index,text
26385,the reproducibility of computational environmental models is an important challenge that calls for open and reusable code and data well documented workflows and controlled environments that allow others to verify published findings this requires an ability to document and share raw datasets data preprocessing scripts model inputs outputs and the specific model code with all associated dependencies hydroshare and geotrust two scientific cyberinfrastructures under development can be used to improve reproducibility in computational hydrology hydroshare is a web based system for sharing hydrologic data and models as digital resources including detailed hydrologic specific resource metadata geotrust provides tools for scientists to efficiently reproduce and share geoscience applications this paper outlines a use case example which focuses on a workflow that uses the modflow model to demonstrate how the cyberinfrastructures hydroshare and geotrust can be integrated in a way that easily and efficiently reproduces computational workflows keywords computational reproducibility hydrologic modeling modflow metadata software availability the software created in this research is free and open source the software information and availability are as follows developers bakinam t essawy daniel voce and wesley zell programming language python bash github link https github com uva hydroinformatics lab aws modflow 1 introduction the challenge of creating more open and reusable code data and formal workflows that allow others to verify published findings is gaining attention in the scientific community borgman 2012 david et al 2016 gorgolewski and poldrack 2016 meng et al 2015 peng 2011 qin et al 2016 reproducibility is important for both verifying previous results as well as building upon the prior computational research of other scientists although we can achieve standard reproducibility for most computational research there are certain cases in which reproducibility remains difficult to achieve this challenge is not caused only by technical barriers but also by limited documentation of the research to be replicated and the potentially complex requirements for how the software is packaged installed and executed piccolo and frampton 2016 recent papers have argued the need and have proposed approaches to improve reproducibility both within geosciences generally and the hydrologic sciences specifically david et al 2016 essawy et al 2016 gil et al 2016 hutton et al 2016 reproducibility of research is said to be achieved if the scientist was able to preserve sufficient computational artifacts in a way that can be replicated in the future meng et al 2015 here we consider reproducibility to be the ability to repeat in the same exact form and then document and share digital resources previously used to complete an analysis these digital resources include 1 initial raw unprocessed datasets 2 data preprocessing scripts used to clean and organize the data 3 model inputs 4 model results and 5 the specific model code along with all of its dependencies fig 1 shows a typical conceptual workflow that needs to be repeated for computational reproducibility these data software and environments are often integrated into workflows as computational experiments that allow scientists to re run an analysis from raw initial datasets and obtain the same model results there are different requirements for reproducibility depending on the nature of the research for example laboratory experiments require capturing descriptive information about protocols and methods leading to empirical reproducibility computational reproducibility on the other hand requires descriptive information about the software and workflow details of model based research stodden 2013 any workflow that is computationally reproducible must be general and able to address the heterogeneous landscape of tools and approaches used within the target scientific community in hydrology scientists use a large variety of computational models many of which have decades of development effort behind them singh et al 2002 computational modeling can often require a significant amount of effort and time to prepare model inputs and to calibrate and validate model parameters depending on the complexity of the system being modeled and the experience of the modeler these aspects can make reproducing computational hydrologic experiments particularly challenging addressing the challenges for achieving reproducibility in computational workflows has been the topic of many studies until now most approaches have either focused on the logical preservation i e sufficient documentation of a workflow and its components to allow for reproduction later on or physical preservation i e workflow conservation by packaging all of its components allowing identical replication santana perez et al 2017 it is hard to achieve a high level of reproducibility while using one of these approaches in isolation rather the integration of both physical and logical preservation is required to achieve a high level of reproducibility some efforts have been made to integrate both logical and physical preservation for computational workflows such as the topology and orchestration specification for cloud applications tosca the tosca framework supports documentation for both the top level structure of the abstract workflow and the execution environment details logical tosca also provides packaging functionality for the workflow physical qasha et al 2016 in a similar way our approach provides both logical and physical preservation however the functionality is extended to allow for automated creation documentation publication and cloud based execution of scientific workflow packages this research presents a solution for achieving a higher level of reproducibility by using geotrust s sciunit cli tool and hydroshare hydroshare http www hydroshare org and geotrust http geotrusthub org are two new cyberinfrastructures under active development that aim to improve reproducibility in computational hydrology the methods described in this paper can be used to assist scientists to more easily repeat reproduce and verify a computational experiment malik 2017 this method goes beyond open source and simply shared by allowing portability in different hardware and software environments and reproducible analyses with different datasets this level of reproducibility is not easily achieved by using hydroshare or geotrust in isolation for example geotrust does not provide a community of users who can verify analyses or the variety of datasets that are required for verification hydroshare however does provide these similarly while hydroshare simplifies the process of sharing code data and descriptive metadata it does not address the challenge of sharing the computational environment required for the workflow and then repeating the computational workflow with different datasets this paper presents the design and implementation of a workflow that takes advantage of the complementary strengths of the two systems hydroshare is used to share key digital resources in the workflow while geotrust is used to capture encapsulate and make portable model execution an example application of the approach is presented using modflow nwt a version of the united states geological survey s groundwater model modflow niswonger et al 2011 the remainder of the paper is organized as follows first additional background on the hydroshare and geotrust projects is provided this background section is meant to orient readers on key aspects of these projects next the methodology section shows the system design and the use case application for the modflow nwt model in the results section the system implementation of the hydroshare and geotrust integration approach is presented and demonstrated by using the use case results as an example application finally a discussion and conclusions section summarizes the key aspects of the approach and outlines opportunities for future research to advance on known limitations of the approach 2 background 2 1 hydroshare hydroshare is an open source web based system developed for hydrologic scientists to easily share collaborate around and publish all types of scientific data and models including detailed hydrologic specific resource metadata tarboton et al 2014a 2014b hydroshare has been developed with the support of the united states national science foundation nsf following the completion of the original nsf grant the consortium of universities for the advancement of hydrologic sciences incorporated cuahsi also funded by the nsf assumed long term support for hydroshare s operation and maintenance in hydroshare digital content is stored and referred to as a resource each resource is a unit used for management and access control within hydroshare every resource has a resource type horsburgh et al 2015 hydroshare assigns a unique identifier for each newly created resource this identifier is known as the resource id the generic resource type supports the dublin core metadata standard weibel et al 1998 and more specific resource types expand on this metadata standard for well defined data types for example model operating system is one of the extended metadata terms for the model program resource type which is used for sharing computational model programs in hydroshare morsy et al 2017 hydroshare provides a representational state transfer rest application program interface api that allows third party applications to interact with hydroshare resources https github com hydroshare hydroshare wiki hydroshare rest api design document developers can create web apps that use hydroshare s rest api to interact with hydroshare resources web app developers can catalogue their apps in hydroshare via the web app resource type swain et al 2016 when a developer creates a web app resource in hydroshare the developer specifies which resource types are relevant to the web app and the url that will be called when the web app is executed from the landing page of the resource that the web app is acting on after a developer adds a web app as a resource in hydroshare hydroshare users can execute that app through hydroshare s web interface to act on relevant resources that they have access to although there are several different resource types supported by hydroshare two of the main resource types relevant to this paper deal with computational models hydroshare divides computational models into two separate but linked resource types a the model program and b the model instance the model program includes the software for executing a specific instance of the model and the model instance are the input files required for executing the model and optionally the output files after a model instance has been executed by a model program horsburgh et al 2015 morsy et al 2014 2017 additionally a model instance resource type can be linked to a model program resource type using the executedby term assisting with reproducibility of the model instance morsy et al 2017 other hydroshare resource types used in this paper include the composite resource type which allows uploading metadata files at both file and resource level the collections resource type which stores any number of individual resources within hydroshare as a single aggregate resource and the web app resource type which is the digital content stored in hydroshare and referred to it as a resource 2 2 geotrust the geotrust project also funded by the nsf through their earthcube program aims to create cyberinfrastructure that assists scientists to efficiently reproduce and share geoscience applications used in research malik et al 2017 the project has done this primarily by developing the concept of a sciunit https sciunit run an efficient lightweight self contained digital package of an ad hoc computational workflow that can be repeated in other environments the sciunit advances the concept of a research object an aggregation of digital artifacts such as code data scripts and temporary experiment results associated with a research paper the sciunit provides an authoritative and far more complete record of a piece of research hai et al 2017 to create maintain and publish sciunits the geotrust project has developed a software tool for linux environments called sciunit cli one of the main advantages of a sciunit is its portability which allows it to be easily run on various computing environments to accomplish this sciunit cli creates sciunits using docker a widely used containerization software docker wraps a piece of software in a complete filesystem that contains everything needed to run the software including code software runtime system tools and system libraries in a docker container owsiak et al 2017 by leveraging docker sciunits are packaged with all of their dependencies in this way any sciunit can be executed in an environment in which both docker and the sciunit cli tool are installed regardless of other computer configurations hai et al 2017 this capability eliminates the burden of configuring a running environment with all software dependencies which can be complex in order to reuse a scientific workflow and reproduce its results in addition to ensuring the portability of sciunits sciunit cli automates some documentation of the workflow packaged into a sciunit including environment dependencies the automation of documenting all code data and environment dependencies alleviates what is typically a burdensome task for scientists importantly sciunit cli also records retrospective provenance of the workflow execution which can be used for re running containers pham et al 2014 because it contains all of the required dependencies the sciunit can be rerun and the outputs reproduced using any other deployment configuration that also has sciunit cli installed when sciunit cli creates a sciunit it includes three types of metadata annotation metadata populated by the user and provenance and version metadata generated automatically by sciunit cli fig 2 shows an example user interaction with the sciunit cli tool the user runs the create command and provides a name model in the example to create a container or a package within the sciunit the user runs the package command and provides the workflow name e g workflow sh along with any inputs for the workflow e g data the user application can be written in any combination of programming languages and many containers can be created within the same sciunit sciunit cli works in a distributed fashion similar to the git version control philosophy such that the sciunits are stored only locally until explicitly shared with a remote repository this method of operation allows distributed collaborators to work offline on the same sciunit when a user is ready to share they can publish the sciunit container to any remote web repository using the publish command to use the publish command the remote repository must be configured within the sciunit cli tool this command line prompts first time users to provide their remote web repository credentials the remote repository reads the container s contents stores the container s digital artifacts in the appropriate remote sciunit and associates the container with an appropriate cloud execution server on which it can potentially re execute in our case we used hydroshare as the remote repository to publish our packaged sciunit in order to use hydroshare s support for rich metadata and its ability to integrate third party applications the latter allowed us to automate the cloud based execution of this packaged sciunit 3 methodology 3 1 system design the combined geotrust and hydroshare system is designed to connect a repeatable computational workflow with its input data in a reproducible way as such both the computational workflow and the data must be stored in a public repository that has extensive metadata support in addition to public accessibility of the data and the computational workflow the execution of the workflow must also be made publicly available to ensure reproducibility and transparency the technology for producing a repeatable computational workflow is provided by the geotrust sciunit cli while the technology for public storage and metadata support is provided by cuahsi s hydroshare therefore the main design aspect of this work consisted of designing a publicly accessible method of execution in which sciunits built with the sciunit cli and stored in hydroshare could be executed using input data also stored in hydroshare this was done in two parts the first was to build in functionality for publishing a sciunit through hydroshare the second part was to automate the execution of a sciunit from hydroshare using hydroshare web apps 3 1 1 integrating sciunit cli with hydroshare fig 3 shows an activity diagram of the system design for integrating geotrust sciunit cli and hydroshare to achieve this integration sciunit cli was extended to support sharing of sciunits through hydroshare this functionality was implemented using hydroshare s rest api to publish their sciunit on hydroshare the user must provide valid hydroshare credentials in the current implementation the sciunit resource is published on hydroshare as a composite resource type once the resource for the sciunit is created within hydroshare the user can log into hydroshare and edit the metadata fields to more fully describe the sciunit resource 3 1 2 automating sciunit execution through hydroshare integrating the cloud based sciunit execution from the hydroshare user interface was done using a hydroshare web app this web app directs hyper text transfer protocol http request to a web server where sciunits can be executed the web app configured to run a particular sciunit can be accessed through the open with button on the landing page for the resource that stores the raw input data when the scientist clicks on the web app button from the open with menu an http request containing the raw input data s resource id will be sent to the server with the resource id the hydroshare rest api can be used to download the raw input data and the sciunit to the server the server can then execute the sciunit using the raw data and return the output to the scientist as a new hydroshare resource fig 4 shows the steps done in a generic form for the integration between the two cyberinfrastructures geotrust and hydroshare to improve reproducibility by automating the execution of the published sciunit the figure shows how the open with app will perform a http get request to a remote server which has already been configured with the sciunit cli this automation process is done using a python script created on the web server machine this python script uses the flask library to act as a web server with nginx https www nginx com used as a proxy to forward all http requests from the user browser to the python script which can handle multiple users simultaneously the python script is using the post request to create a new resource and upload the output generated from running the sciunit on this resource simultaneously a webserver is running on the remote machine which handles the http request and automatically executes a python script this script uses the hydroshare user authentication to download the input data from the resource and downloads the composite resource that includes the sciunit container once both resources are downloaded the resources are unzipped and moved to the working directory for the analysis the sciunit cli executes the downloaded sciunit package after the sciunit is executed a new resource is created in hydroshare and the output from the sciunit cli execution is uploaded into this new resource a new collection resource is also created on hydroshare to group all resources that were included during this execution in this paper we used hydroshare api our python script uses the python client library for the rest api http hs restclient readthedocs io en latest 3 2 use case application a use case application was designed to demonstrate the integration of geotrust sciunit cli and hydroshare this integration allows geotrust to package and publish a sciunit through hydroshare after which hydroshare automates the execution of this sciunit execution of the packaged sciunit through hydroshare was demonstrated using ec2 instances from amazon web services aws a linux based micro sized machine t2 was used for prototyping and demonstration purposes this machine had 1 gb of memory 1 vcpu 32 gb of solid state drive ssd based local instance storage and a 64 bit platform amazon ec2 instances 2015 this use case consisted of a workflow used for preprocessing model input data running a computational model and handling the model outputs the computational model used for the use case was modflow nwt 3 2 1 modflow nwt use case modflow nwt is a standalone version of modflow a commonly used groundwater model niswonger et al 2011 the concept of packages is key to the modularity of the different versions of modflow including modflow nwt packages are input files that define some individual component of the groundwater flow conceptual model or specify the solution method used for the flow equation that is collectively formulated from the individual components for example the basic bas and discretization dis packages define the spatial and temporal framework of the model including the grid dimensions and the location of active and inactive grid cells while the recharge rch package defines the spatial distribution and rate of recharge to the water table for our use case using modflow nwt the newton raphson nwt package defines the variables required to implement the newton raphson solution method for this study modflow nwt was used to simulate the shallow groundwater flow in the james river watershed upstream of richmond va usa the model includes recharge to the water table subsurface flow through the saturated zone and base flow discharge to surface water bodies including the james rivanna and hardware rivers and several smaller order streams depth integrated effective transmissivity was assumed to be constant throughout the active model area and spatially distributed recharge was derived from the national recharge dataset developed by reitz et al 2017 base flow discharge was simulated using the modflow drain drn package with all drain elevations i e the water table elevation required to discharge base flow to a receiving stream extracted from the national elevation dataset the model runs to completion and is unconstrained by calibration as such it is to be only used as an example for the workflow processes described in this paper i e no hydrologic or management conclusions were drawn from the results of the model this workflow could be extended to include calibration fig 1 for example a hydroshare resource for a parameter estimation program such as pest doherty and hunt 2010 could be created and included in the sciunit container similarly the pre processing script could include data retrieval from web services such as the usgs water services api https waterservices usgs gov and the automated generation of pest input files the flopy library was used to create the modflow nwt model from raw input datasets bakker et al 2016 flopy is a library of python modules that allows scripting of the various steps in modflow model development execution and analysis by combining flopy with geotrust and hydroshare the workflow used to create and execute modflow model e g the steps shown in fig 1 can be stored within a reproducible container with descriptive metadata in hydroshare 4 results 4 1 system implementation this section describes how a sciunit package on hydroshare can be deployed from a remote server the system was implemented using the following steps first the script downloads raw input data and the sciunit resources from hydroshare second the script will unzip both the data and sciunit pass the data to the sciunit as an argument this is how the sciunit accepts the input data and then run the sciunit with the downloaded data last after the execution is completed the python script will upload the results to hydroshare by using a post request to create two new resources one for the sciunit output which has the modflow nwt model instance resource type and the other the collection resource that will include all the resources used within the study the script then returns the command status including any errors to the user 4 2 use case results a digital workflow bash script was packaged into a sciunit using the sciunit cli tool the digital workflow runs a python script to prepare the modflow nwt input data files and then executes a single run of the model fig 5 shows the components of the packaged digital workflow fig 6 outlines the first steps taken in the process to start and create a new sciunit through the geotrust sciunit cli tool for the example workflow while fig 7 shows the execution and packaging of the digital workflow into a sciunit package this package command traces all dependencies for the workflow and includes them in a single docker file fig 8 shows how the publish command is used to publish a sciunit package on hydroshare if this is the user s first time connecting to hydroshare sciunit cli will ask for hydroshare user credentials otherwise the credentials stored will be used once the package is published metadata can be provided by the user via the hydroshare graphical user interface gui future implementations of the sciunit cli may expand this functionality by automatically populating more detailed metadata for describing resources the newly created resource on hydroshare is a composite resource type this resource type allows the resource to include multiple files without file format limitations and with metadata associated at a file level within the resource the composite resource contains two files the first is the provenance metadata file created while packaging the workflow this metadata file contains information concerning the creation and version history of the managed data the second file is the zipped package for the sciunit itself once the sciunit is available as a hydroshare resource hydroshare s integration with third party web apps is used to execute the sciunit in order to store data and make it accessible to be used as the input required by the sciunit we made a new model instance type resource titled modflownwtrawdata essawy 2018b we also created a web app resource titled geotrust essawy 2018a this web app pointed to the aws ec2 instance where the sciunit cli tool and our python script were installed the connection between the hydroshare resource and the web server was made by providing the web server s url as the app launching url pattern metadata term in the resource the geotrust web app resource is linked to the modflownwtrawdata resource by the supportedresourcetype metadata property this metadata property was set to include the composite resource type which allowed the web app to appear in a drop down list in the open with menu on the modflownwtrawdata resource landing page fig 9 shows the model instance resource type that includes the raw data and the web apps linked to this resource type to automate the sciunit execution when the geotrust web app on this page is selected the http request is sent to server and the workflow is executed the output is written back to hydroshare as a new resource with the modflow model instance resource type this resource type is used because the resource can be executed by a modflow model program and it allows for adding extended metadata specific to modflow morsy et al 2017 fig 10 presents the activity diagram for the steps that occur when the open with button is clicked and the geotrust app is selected on the modflownwtrawdata resource landing page the geotrust app will perform an http get request to the aws ec2 machine which has already been configured with the sciunit cli the webserver running on the aws ec2 machine handles the http request and automatically executes a python script the script uses the hydroshare user authentication to download both the raw data of the modflownwtrawdata resource and the sciunit container included within the modflownwtsciunit resource essawy 2018c once the modflownwtsciunit and the modflownwtrawdata resources are downloaded the script unzips the resources and moves them to the working directory for the analysis the sciunit cli tool executes the downloaded sciunit package which pre processes the raw input data for the model and executes the modflow nwt model after the model is executed a new resource is created in hydroshare with the modflow model instance resource type named modflownwtsciunitoutput essawy 2018d and the output from the sciunit cli execution is uploaded into this new resource a new collection resource is also created on hydroshare to group all the resources the modflownwtrawdata generic model instance resource the resource type is a generic model instance because the data uploaded have no specific metadata or format that could be tied to a specific resource type the web app geotrust resource the modflownwtsciunit modflow model instance resource the modflownwtsciunit composite resource and the modflownwtsciunitoutput resource that includes the output resulting from executing the sciunit package fig 11 shows hydroshare user my resources page after using the open with action button on the geotrust web app on the modflownwtrawdata resource for the online execution two new resources are created the first resource in the workflow is the modflownwtsciunitoutput resource which includes the input files for the modflow nwt model program that are prepared through the preprocessing script and the output from the model run this resource is given the modflow model instance resource type because the resource has the inputs that are required by the modflow nwt model this resource type allows for extended metadata specific to a modflow model instance the second resource created is the modflownwtcollection resource essawy 2018e which includes all the resources used in the online execution for the modflow nwt this provides a grouping of resources used for an analysis and allows the user to share or download this collection of resources more easily fig 12 shows the output files within modflownwtsciunitoutput resource as viewed on this resource s hydroshare landing page the resource contains the output generated from running the sciunit that prepares the model input for modflow nwt and the output from running the modflow nwt model program itself the modflow model instance resource type includes extended metadata terms specific for modflow in this use case the model has eight packages in addition to the packages already described this model instance includes the output control oc package which specifies how the model output is written the upstream weighting upw groundwater flow package which describes the system properties e g transmissivity conductivity and the one output listing file list which contains all the information about the current run e g stress period time step and the number of active and inactive cells the recharge drains and any errors the name file nam specifies the name of the input and output files for the model instance additional metadata associated with the modflow output resource is divided into four categories 1 authorship 2 related resources 3 resource specific and 4 web apps fig 13 shows the related resources metadata here all resources linked to the modflow output resource through formal relationships are listed in this case the modflow output resource is linked to the modflownwtrawdata resource through the derived from relationship and to the modflow nwt resource through the isexecutedby relationship fig 14 shows the resource specific metadata these are non null metadata terms that apply only to the modflow model instances such as grid attributes solver and boundary condition package choices additional metadata terms not previously populated by the user can be populated later within the edit mode and will appear in this section once populated fig 15 shows details for the resulting modflownwtcollection resource as viewed on this resource s landing page the collection resource contains four sub resources 1 the modflownwtrawdata resource with the raw input data ready to be prepared for the modflow nwt model engine 2 the modflownwtsciunit resource with the sciunit pre processing workflow which also includes running the modflow nwt model 3 the modflownwtsciunitoutput resource which stores the output generated from running the sciunit workflow and 4 the geotrust web app used to perform the online model execution using aws ec2 by organizing all these resources into a single collection it is possible to have one landing page where users can referring back to the stated goals in the introduction of this paper view obtain and execute 1 raw initial datasets 2 data preprocessing scripts used to clean and organize the data 3 model inputs 4 model results and 5 the specific model code along with of all its dependencies used for a computational analysis 5 discussion and conclusions in this paper we demonstrated how hydroshare and geotrust can be integrated to easily and efficiently package share and publish model workflows modlfow nwt was used as an example application to demonstrate the functionality provided by these cyberinfrastructures for creating open reusable data analysis and cloud based model execution services the approach showed how containers built using geotrust tools can be shared as hydroshare resources a cloud based service was created to automatically retrieve raw input data from hydroshare execute a sciunit container that both prepares and runs a modflow nwt model and share the results on hydroshare using a modflow model instance resource type all the resources are aggregated in hydroshare into one collection resource with domain specific metadata the integration of scientific cyberinfrastructures such as the hydroshare and geotrust projects can improve reproducibility in computational hydrology new modflow models can be directly built from unprocessed input data e g land surface dems or stream network shapefiles by running a sciunit container that includes automated data preparation steps implemented using the flopy python package the container is run online using aws resources initiated directly through the hydroshare user interface a particular advantage of this approach is that the geotrust sciunit cli tool provides scientists a method for efficiently creating containers for script driven modeling workflows thus the general approach demonstrated here for the modflow nwt use case could be applied for any workflow that can be automated and that is compatible with docker requirements for example in prior work we have constructed pre and post processing workflows for the variable infiltration capacity vic hydrologic model liang et al 1996 that could directly benefit from this method for packaging sharing and publishing resources billah et al 2016 essawy et al 2016 these containers are efficient lightweight self contained packages of computational experiments that can be repeated or reproduced regardless of deployment configurations in addition to integration with hydroshare for storing and publishing a sciunit cloud resources were used to execute sciunits directly through the hydroshare user interface while only aws was presented we evaluated as part of this work three different cloud computing services earthcube integration and testing environment ecite cyverse and amazon web services aws ecite and cyverse are funded by nsf and both are under active development one main advantage for using ecite or cyverse is that they are free of charge for scientific studies aws though not free does offer a competitive grant program for researchers from our experience the aws platform made the process of obtaining computer resources the simplest when compared to ecite and cyverse the aws user simply logs in to the console selects the type of the machine needed and launches it when using ecite we had to contact the developer and ask for an instance with the required specifications and a short paragraph summarizing the project we are working on to justify the allocation of compute resources we also needed to contact the developer each time we wanted to open a port e g port 22 to ssh or port 80 for http the service did not support elastic ips like aws so each time we restarted an instance and wanted to use ssh to access to the machine we needed to report the ip address used to access the machine to the developer to add this address to the security rules cyverse is a more mature service but allows each user only a certain allocation of computational time once the user exceeds this allocation the instance is suspended and the user needs to request more time from the administrators this feature was problematic for our use case of a continually available cloud based resource for online model execution for these reasons we used aws ec2 for much of the testing work described in this paper but ecite and cyverse are in active development and will likely be good options for this use case in the future while this approach shows great promise it is not without limitations 1 the sciunit cli tool must be installed in order to re execute a sciunit container and 2 hydroshare lacks methods for uniquely identifying and managing web app resources that will be needed as the number of these resources continues to increase regarding the latter limitation without a more organized structure naming conflicts could cause confusion when using the open with button over which app is to be requested also this work does not fully explore computational challenges associated with the proposed methodology using cloud services like aws provides the opportunity for scalability as more users are added for example this solution used small ec2 instances for prototyping future work could explore aws ec2 container service ecs as an alternative for a more scalable solution to support multiple concurrent users data movement between hydroshare and aws is another potential issue as data volumes increase which is not uncommon for hydrologic modeling hydroshare is built on irods integrated rule oriented data system which includes the ability to interface with aws s3 storage resources future work could explore using this functionality to automate the movement of large files between hydroshare and aws to support computation within aws and still maintain access through the hydroshare user interface irods is specifically designed to handle such data federation needs and should provide a robust solution for managing the large data flows common in hydrologic modeling lastly future work should explore scaling of the general approach presented here to use cases in which multiple sciunits are available for execution within a remote cloud based resource in this case a user could select from available sciunits to process input data stored with hydroshare making for a potentially very powerful general approach applicable to many different modeling and analysis use cases that require remote data processing acknowledgements and disclaimer we gratefully acknowledge the national science foundation for support of this work under awards aci 0940841 icer 1343800 and icer 1440323 any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
26385,the reproducibility of computational environmental models is an important challenge that calls for open and reusable code and data well documented workflows and controlled environments that allow others to verify published findings this requires an ability to document and share raw datasets data preprocessing scripts model inputs outputs and the specific model code with all associated dependencies hydroshare and geotrust two scientific cyberinfrastructures under development can be used to improve reproducibility in computational hydrology hydroshare is a web based system for sharing hydrologic data and models as digital resources including detailed hydrologic specific resource metadata geotrust provides tools for scientists to efficiently reproduce and share geoscience applications this paper outlines a use case example which focuses on a workflow that uses the modflow model to demonstrate how the cyberinfrastructures hydroshare and geotrust can be integrated in a way that easily and efficiently reproduces computational workflows keywords computational reproducibility hydrologic modeling modflow metadata software availability the software created in this research is free and open source the software information and availability are as follows developers bakinam t essawy daniel voce and wesley zell programming language python bash github link https github com uva hydroinformatics lab aws modflow 1 introduction the challenge of creating more open and reusable code data and formal workflows that allow others to verify published findings is gaining attention in the scientific community borgman 2012 david et al 2016 gorgolewski and poldrack 2016 meng et al 2015 peng 2011 qin et al 2016 reproducibility is important for both verifying previous results as well as building upon the prior computational research of other scientists although we can achieve standard reproducibility for most computational research there are certain cases in which reproducibility remains difficult to achieve this challenge is not caused only by technical barriers but also by limited documentation of the research to be replicated and the potentially complex requirements for how the software is packaged installed and executed piccolo and frampton 2016 recent papers have argued the need and have proposed approaches to improve reproducibility both within geosciences generally and the hydrologic sciences specifically david et al 2016 essawy et al 2016 gil et al 2016 hutton et al 2016 reproducibility of research is said to be achieved if the scientist was able to preserve sufficient computational artifacts in a way that can be replicated in the future meng et al 2015 here we consider reproducibility to be the ability to repeat in the same exact form and then document and share digital resources previously used to complete an analysis these digital resources include 1 initial raw unprocessed datasets 2 data preprocessing scripts used to clean and organize the data 3 model inputs 4 model results and 5 the specific model code along with all of its dependencies fig 1 shows a typical conceptual workflow that needs to be repeated for computational reproducibility these data software and environments are often integrated into workflows as computational experiments that allow scientists to re run an analysis from raw initial datasets and obtain the same model results there are different requirements for reproducibility depending on the nature of the research for example laboratory experiments require capturing descriptive information about protocols and methods leading to empirical reproducibility computational reproducibility on the other hand requires descriptive information about the software and workflow details of model based research stodden 2013 any workflow that is computationally reproducible must be general and able to address the heterogeneous landscape of tools and approaches used within the target scientific community in hydrology scientists use a large variety of computational models many of which have decades of development effort behind them singh et al 2002 computational modeling can often require a significant amount of effort and time to prepare model inputs and to calibrate and validate model parameters depending on the complexity of the system being modeled and the experience of the modeler these aspects can make reproducing computational hydrologic experiments particularly challenging addressing the challenges for achieving reproducibility in computational workflows has been the topic of many studies until now most approaches have either focused on the logical preservation i e sufficient documentation of a workflow and its components to allow for reproduction later on or physical preservation i e workflow conservation by packaging all of its components allowing identical replication santana perez et al 2017 it is hard to achieve a high level of reproducibility while using one of these approaches in isolation rather the integration of both physical and logical preservation is required to achieve a high level of reproducibility some efforts have been made to integrate both logical and physical preservation for computational workflows such as the topology and orchestration specification for cloud applications tosca the tosca framework supports documentation for both the top level structure of the abstract workflow and the execution environment details logical tosca also provides packaging functionality for the workflow physical qasha et al 2016 in a similar way our approach provides both logical and physical preservation however the functionality is extended to allow for automated creation documentation publication and cloud based execution of scientific workflow packages this research presents a solution for achieving a higher level of reproducibility by using geotrust s sciunit cli tool and hydroshare hydroshare http www hydroshare org and geotrust http geotrusthub org are two new cyberinfrastructures under active development that aim to improve reproducibility in computational hydrology the methods described in this paper can be used to assist scientists to more easily repeat reproduce and verify a computational experiment malik 2017 this method goes beyond open source and simply shared by allowing portability in different hardware and software environments and reproducible analyses with different datasets this level of reproducibility is not easily achieved by using hydroshare or geotrust in isolation for example geotrust does not provide a community of users who can verify analyses or the variety of datasets that are required for verification hydroshare however does provide these similarly while hydroshare simplifies the process of sharing code data and descriptive metadata it does not address the challenge of sharing the computational environment required for the workflow and then repeating the computational workflow with different datasets this paper presents the design and implementation of a workflow that takes advantage of the complementary strengths of the two systems hydroshare is used to share key digital resources in the workflow while geotrust is used to capture encapsulate and make portable model execution an example application of the approach is presented using modflow nwt a version of the united states geological survey s groundwater model modflow niswonger et al 2011 the remainder of the paper is organized as follows first additional background on the hydroshare and geotrust projects is provided this background section is meant to orient readers on key aspects of these projects next the methodology section shows the system design and the use case application for the modflow nwt model in the results section the system implementation of the hydroshare and geotrust integration approach is presented and demonstrated by using the use case results as an example application finally a discussion and conclusions section summarizes the key aspects of the approach and outlines opportunities for future research to advance on known limitations of the approach 2 background 2 1 hydroshare hydroshare is an open source web based system developed for hydrologic scientists to easily share collaborate around and publish all types of scientific data and models including detailed hydrologic specific resource metadata tarboton et al 2014a 2014b hydroshare has been developed with the support of the united states national science foundation nsf following the completion of the original nsf grant the consortium of universities for the advancement of hydrologic sciences incorporated cuahsi also funded by the nsf assumed long term support for hydroshare s operation and maintenance in hydroshare digital content is stored and referred to as a resource each resource is a unit used for management and access control within hydroshare every resource has a resource type horsburgh et al 2015 hydroshare assigns a unique identifier for each newly created resource this identifier is known as the resource id the generic resource type supports the dublin core metadata standard weibel et al 1998 and more specific resource types expand on this metadata standard for well defined data types for example model operating system is one of the extended metadata terms for the model program resource type which is used for sharing computational model programs in hydroshare morsy et al 2017 hydroshare provides a representational state transfer rest application program interface api that allows third party applications to interact with hydroshare resources https github com hydroshare hydroshare wiki hydroshare rest api design document developers can create web apps that use hydroshare s rest api to interact with hydroshare resources web app developers can catalogue their apps in hydroshare via the web app resource type swain et al 2016 when a developer creates a web app resource in hydroshare the developer specifies which resource types are relevant to the web app and the url that will be called when the web app is executed from the landing page of the resource that the web app is acting on after a developer adds a web app as a resource in hydroshare hydroshare users can execute that app through hydroshare s web interface to act on relevant resources that they have access to although there are several different resource types supported by hydroshare two of the main resource types relevant to this paper deal with computational models hydroshare divides computational models into two separate but linked resource types a the model program and b the model instance the model program includes the software for executing a specific instance of the model and the model instance are the input files required for executing the model and optionally the output files after a model instance has been executed by a model program horsburgh et al 2015 morsy et al 2014 2017 additionally a model instance resource type can be linked to a model program resource type using the executedby term assisting with reproducibility of the model instance morsy et al 2017 other hydroshare resource types used in this paper include the composite resource type which allows uploading metadata files at both file and resource level the collections resource type which stores any number of individual resources within hydroshare as a single aggregate resource and the web app resource type which is the digital content stored in hydroshare and referred to it as a resource 2 2 geotrust the geotrust project also funded by the nsf through their earthcube program aims to create cyberinfrastructure that assists scientists to efficiently reproduce and share geoscience applications used in research malik et al 2017 the project has done this primarily by developing the concept of a sciunit https sciunit run an efficient lightweight self contained digital package of an ad hoc computational workflow that can be repeated in other environments the sciunit advances the concept of a research object an aggregation of digital artifacts such as code data scripts and temporary experiment results associated with a research paper the sciunit provides an authoritative and far more complete record of a piece of research hai et al 2017 to create maintain and publish sciunits the geotrust project has developed a software tool for linux environments called sciunit cli one of the main advantages of a sciunit is its portability which allows it to be easily run on various computing environments to accomplish this sciunit cli creates sciunits using docker a widely used containerization software docker wraps a piece of software in a complete filesystem that contains everything needed to run the software including code software runtime system tools and system libraries in a docker container owsiak et al 2017 by leveraging docker sciunits are packaged with all of their dependencies in this way any sciunit can be executed in an environment in which both docker and the sciunit cli tool are installed regardless of other computer configurations hai et al 2017 this capability eliminates the burden of configuring a running environment with all software dependencies which can be complex in order to reuse a scientific workflow and reproduce its results in addition to ensuring the portability of sciunits sciunit cli automates some documentation of the workflow packaged into a sciunit including environment dependencies the automation of documenting all code data and environment dependencies alleviates what is typically a burdensome task for scientists importantly sciunit cli also records retrospective provenance of the workflow execution which can be used for re running containers pham et al 2014 because it contains all of the required dependencies the sciunit can be rerun and the outputs reproduced using any other deployment configuration that also has sciunit cli installed when sciunit cli creates a sciunit it includes three types of metadata annotation metadata populated by the user and provenance and version metadata generated automatically by sciunit cli fig 2 shows an example user interaction with the sciunit cli tool the user runs the create command and provides a name model in the example to create a container or a package within the sciunit the user runs the package command and provides the workflow name e g workflow sh along with any inputs for the workflow e g data the user application can be written in any combination of programming languages and many containers can be created within the same sciunit sciunit cli works in a distributed fashion similar to the git version control philosophy such that the sciunits are stored only locally until explicitly shared with a remote repository this method of operation allows distributed collaborators to work offline on the same sciunit when a user is ready to share they can publish the sciunit container to any remote web repository using the publish command to use the publish command the remote repository must be configured within the sciunit cli tool this command line prompts first time users to provide their remote web repository credentials the remote repository reads the container s contents stores the container s digital artifacts in the appropriate remote sciunit and associates the container with an appropriate cloud execution server on which it can potentially re execute in our case we used hydroshare as the remote repository to publish our packaged sciunit in order to use hydroshare s support for rich metadata and its ability to integrate third party applications the latter allowed us to automate the cloud based execution of this packaged sciunit 3 methodology 3 1 system design the combined geotrust and hydroshare system is designed to connect a repeatable computational workflow with its input data in a reproducible way as such both the computational workflow and the data must be stored in a public repository that has extensive metadata support in addition to public accessibility of the data and the computational workflow the execution of the workflow must also be made publicly available to ensure reproducibility and transparency the technology for producing a repeatable computational workflow is provided by the geotrust sciunit cli while the technology for public storage and metadata support is provided by cuahsi s hydroshare therefore the main design aspect of this work consisted of designing a publicly accessible method of execution in which sciunits built with the sciunit cli and stored in hydroshare could be executed using input data also stored in hydroshare this was done in two parts the first was to build in functionality for publishing a sciunit through hydroshare the second part was to automate the execution of a sciunit from hydroshare using hydroshare web apps 3 1 1 integrating sciunit cli with hydroshare fig 3 shows an activity diagram of the system design for integrating geotrust sciunit cli and hydroshare to achieve this integration sciunit cli was extended to support sharing of sciunits through hydroshare this functionality was implemented using hydroshare s rest api to publish their sciunit on hydroshare the user must provide valid hydroshare credentials in the current implementation the sciunit resource is published on hydroshare as a composite resource type once the resource for the sciunit is created within hydroshare the user can log into hydroshare and edit the metadata fields to more fully describe the sciunit resource 3 1 2 automating sciunit execution through hydroshare integrating the cloud based sciunit execution from the hydroshare user interface was done using a hydroshare web app this web app directs hyper text transfer protocol http request to a web server where sciunits can be executed the web app configured to run a particular sciunit can be accessed through the open with button on the landing page for the resource that stores the raw input data when the scientist clicks on the web app button from the open with menu an http request containing the raw input data s resource id will be sent to the server with the resource id the hydroshare rest api can be used to download the raw input data and the sciunit to the server the server can then execute the sciunit using the raw data and return the output to the scientist as a new hydroshare resource fig 4 shows the steps done in a generic form for the integration between the two cyberinfrastructures geotrust and hydroshare to improve reproducibility by automating the execution of the published sciunit the figure shows how the open with app will perform a http get request to a remote server which has already been configured with the sciunit cli this automation process is done using a python script created on the web server machine this python script uses the flask library to act as a web server with nginx https www nginx com used as a proxy to forward all http requests from the user browser to the python script which can handle multiple users simultaneously the python script is using the post request to create a new resource and upload the output generated from running the sciunit on this resource simultaneously a webserver is running on the remote machine which handles the http request and automatically executes a python script this script uses the hydroshare user authentication to download the input data from the resource and downloads the composite resource that includes the sciunit container once both resources are downloaded the resources are unzipped and moved to the working directory for the analysis the sciunit cli executes the downloaded sciunit package after the sciunit is executed a new resource is created in hydroshare and the output from the sciunit cli execution is uploaded into this new resource a new collection resource is also created on hydroshare to group all resources that were included during this execution in this paper we used hydroshare api our python script uses the python client library for the rest api http hs restclient readthedocs io en latest 3 2 use case application a use case application was designed to demonstrate the integration of geotrust sciunit cli and hydroshare this integration allows geotrust to package and publish a sciunit through hydroshare after which hydroshare automates the execution of this sciunit execution of the packaged sciunit through hydroshare was demonstrated using ec2 instances from amazon web services aws a linux based micro sized machine t2 was used for prototyping and demonstration purposes this machine had 1 gb of memory 1 vcpu 32 gb of solid state drive ssd based local instance storage and a 64 bit platform amazon ec2 instances 2015 this use case consisted of a workflow used for preprocessing model input data running a computational model and handling the model outputs the computational model used for the use case was modflow nwt 3 2 1 modflow nwt use case modflow nwt is a standalone version of modflow a commonly used groundwater model niswonger et al 2011 the concept of packages is key to the modularity of the different versions of modflow including modflow nwt packages are input files that define some individual component of the groundwater flow conceptual model or specify the solution method used for the flow equation that is collectively formulated from the individual components for example the basic bas and discretization dis packages define the spatial and temporal framework of the model including the grid dimensions and the location of active and inactive grid cells while the recharge rch package defines the spatial distribution and rate of recharge to the water table for our use case using modflow nwt the newton raphson nwt package defines the variables required to implement the newton raphson solution method for this study modflow nwt was used to simulate the shallow groundwater flow in the james river watershed upstream of richmond va usa the model includes recharge to the water table subsurface flow through the saturated zone and base flow discharge to surface water bodies including the james rivanna and hardware rivers and several smaller order streams depth integrated effective transmissivity was assumed to be constant throughout the active model area and spatially distributed recharge was derived from the national recharge dataset developed by reitz et al 2017 base flow discharge was simulated using the modflow drain drn package with all drain elevations i e the water table elevation required to discharge base flow to a receiving stream extracted from the national elevation dataset the model runs to completion and is unconstrained by calibration as such it is to be only used as an example for the workflow processes described in this paper i e no hydrologic or management conclusions were drawn from the results of the model this workflow could be extended to include calibration fig 1 for example a hydroshare resource for a parameter estimation program such as pest doherty and hunt 2010 could be created and included in the sciunit container similarly the pre processing script could include data retrieval from web services such as the usgs water services api https waterservices usgs gov and the automated generation of pest input files the flopy library was used to create the modflow nwt model from raw input datasets bakker et al 2016 flopy is a library of python modules that allows scripting of the various steps in modflow model development execution and analysis by combining flopy with geotrust and hydroshare the workflow used to create and execute modflow model e g the steps shown in fig 1 can be stored within a reproducible container with descriptive metadata in hydroshare 4 results 4 1 system implementation this section describes how a sciunit package on hydroshare can be deployed from a remote server the system was implemented using the following steps first the script downloads raw input data and the sciunit resources from hydroshare second the script will unzip both the data and sciunit pass the data to the sciunit as an argument this is how the sciunit accepts the input data and then run the sciunit with the downloaded data last after the execution is completed the python script will upload the results to hydroshare by using a post request to create two new resources one for the sciunit output which has the modflow nwt model instance resource type and the other the collection resource that will include all the resources used within the study the script then returns the command status including any errors to the user 4 2 use case results a digital workflow bash script was packaged into a sciunit using the sciunit cli tool the digital workflow runs a python script to prepare the modflow nwt input data files and then executes a single run of the model fig 5 shows the components of the packaged digital workflow fig 6 outlines the first steps taken in the process to start and create a new sciunit through the geotrust sciunit cli tool for the example workflow while fig 7 shows the execution and packaging of the digital workflow into a sciunit package this package command traces all dependencies for the workflow and includes them in a single docker file fig 8 shows how the publish command is used to publish a sciunit package on hydroshare if this is the user s first time connecting to hydroshare sciunit cli will ask for hydroshare user credentials otherwise the credentials stored will be used once the package is published metadata can be provided by the user via the hydroshare graphical user interface gui future implementations of the sciunit cli may expand this functionality by automatically populating more detailed metadata for describing resources the newly created resource on hydroshare is a composite resource type this resource type allows the resource to include multiple files without file format limitations and with metadata associated at a file level within the resource the composite resource contains two files the first is the provenance metadata file created while packaging the workflow this metadata file contains information concerning the creation and version history of the managed data the second file is the zipped package for the sciunit itself once the sciunit is available as a hydroshare resource hydroshare s integration with third party web apps is used to execute the sciunit in order to store data and make it accessible to be used as the input required by the sciunit we made a new model instance type resource titled modflownwtrawdata essawy 2018b we also created a web app resource titled geotrust essawy 2018a this web app pointed to the aws ec2 instance where the sciunit cli tool and our python script were installed the connection between the hydroshare resource and the web server was made by providing the web server s url as the app launching url pattern metadata term in the resource the geotrust web app resource is linked to the modflownwtrawdata resource by the supportedresourcetype metadata property this metadata property was set to include the composite resource type which allowed the web app to appear in a drop down list in the open with menu on the modflownwtrawdata resource landing page fig 9 shows the model instance resource type that includes the raw data and the web apps linked to this resource type to automate the sciunit execution when the geotrust web app on this page is selected the http request is sent to server and the workflow is executed the output is written back to hydroshare as a new resource with the modflow model instance resource type this resource type is used because the resource can be executed by a modflow model program and it allows for adding extended metadata specific to modflow morsy et al 2017 fig 10 presents the activity diagram for the steps that occur when the open with button is clicked and the geotrust app is selected on the modflownwtrawdata resource landing page the geotrust app will perform an http get request to the aws ec2 machine which has already been configured with the sciunit cli the webserver running on the aws ec2 machine handles the http request and automatically executes a python script the script uses the hydroshare user authentication to download both the raw data of the modflownwtrawdata resource and the sciunit container included within the modflownwtsciunit resource essawy 2018c once the modflownwtsciunit and the modflownwtrawdata resources are downloaded the script unzips the resources and moves them to the working directory for the analysis the sciunit cli tool executes the downloaded sciunit package which pre processes the raw input data for the model and executes the modflow nwt model after the model is executed a new resource is created in hydroshare with the modflow model instance resource type named modflownwtsciunitoutput essawy 2018d and the output from the sciunit cli execution is uploaded into this new resource a new collection resource is also created on hydroshare to group all the resources the modflownwtrawdata generic model instance resource the resource type is a generic model instance because the data uploaded have no specific metadata or format that could be tied to a specific resource type the web app geotrust resource the modflownwtsciunit modflow model instance resource the modflownwtsciunit composite resource and the modflownwtsciunitoutput resource that includes the output resulting from executing the sciunit package fig 11 shows hydroshare user my resources page after using the open with action button on the geotrust web app on the modflownwtrawdata resource for the online execution two new resources are created the first resource in the workflow is the modflownwtsciunitoutput resource which includes the input files for the modflow nwt model program that are prepared through the preprocessing script and the output from the model run this resource is given the modflow model instance resource type because the resource has the inputs that are required by the modflow nwt model this resource type allows for extended metadata specific to a modflow model instance the second resource created is the modflownwtcollection resource essawy 2018e which includes all the resources used in the online execution for the modflow nwt this provides a grouping of resources used for an analysis and allows the user to share or download this collection of resources more easily fig 12 shows the output files within modflownwtsciunitoutput resource as viewed on this resource s hydroshare landing page the resource contains the output generated from running the sciunit that prepares the model input for modflow nwt and the output from running the modflow nwt model program itself the modflow model instance resource type includes extended metadata terms specific for modflow in this use case the model has eight packages in addition to the packages already described this model instance includes the output control oc package which specifies how the model output is written the upstream weighting upw groundwater flow package which describes the system properties e g transmissivity conductivity and the one output listing file list which contains all the information about the current run e g stress period time step and the number of active and inactive cells the recharge drains and any errors the name file nam specifies the name of the input and output files for the model instance additional metadata associated with the modflow output resource is divided into four categories 1 authorship 2 related resources 3 resource specific and 4 web apps fig 13 shows the related resources metadata here all resources linked to the modflow output resource through formal relationships are listed in this case the modflow output resource is linked to the modflownwtrawdata resource through the derived from relationship and to the modflow nwt resource through the isexecutedby relationship fig 14 shows the resource specific metadata these are non null metadata terms that apply only to the modflow model instances such as grid attributes solver and boundary condition package choices additional metadata terms not previously populated by the user can be populated later within the edit mode and will appear in this section once populated fig 15 shows details for the resulting modflownwtcollection resource as viewed on this resource s landing page the collection resource contains four sub resources 1 the modflownwtrawdata resource with the raw input data ready to be prepared for the modflow nwt model engine 2 the modflownwtsciunit resource with the sciunit pre processing workflow which also includes running the modflow nwt model 3 the modflownwtsciunitoutput resource which stores the output generated from running the sciunit workflow and 4 the geotrust web app used to perform the online model execution using aws ec2 by organizing all these resources into a single collection it is possible to have one landing page where users can referring back to the stated goals in the introduction of this paper view obtain and execute 1 raw initial datasets 2 data preprocessing scripts used to clean and organize the data 3 model inputs 4 model results and 5 the specific model code along with of all its dependencies used for a computational analysis 5 discussion and conclusions in this paper we demonstrated how hydroshare and geotrust can be integrated to easily and efficiently package share and publish model workflows modlfow nwt was used as an example application to demonstrate the functionality provided by these cyberinfrastructures for creating open reusable data analysis and cloud based model execution services the approach showed how containers built using geotrust tools can be shared as hydroshare resources a cloud based service was created to automatically retrieve raw input data from hydroshare execute a sciunit container that both prepares and runs a modflow nwt model and share the results on hydroshare using a modflow model instance resource type all the resources are aggregated in hydroshare into one collection resource with domain specific metadata the integration of scientific cyberinfrastructures such as the hydroshare and geotrust projects can improve reproducibility in computational hydrology new modflow models can be directly built from unprocessed input data e g land surface dems or stream network shapefiles by running a sciunit container that includes automated data preparation steps implemented using the flopy python package the container is run online using aws resources initiated directly through the hydroshare user interface a particular advantage of this approach is that the geotrust sciunit cli tool provides scientists a method for efficiently creating containers for script driven modeling workflows thus the general approach demonstrated here for the modflow nwt use case could be applied for any workflow that can be automated and that is compatible with docker requirements for example in prior work we have constructed pre and post processing workflows for the variable infiltration capacity vic hydrologic model liang et al 1996 that could directly benefit from this method for packaging sharing and publishing resources billah et al 2016 essawy et al 2016 these containers are efficient lightweight self contained packages of computational experiments that can be repeated or reproduced regardless of deployment configurations in addition to integration with hydroshare for storing and publishing a sciunit cloud resources were used to execute sciunits directly through the hydroshare user interface while only aws was presented we evaluated as part of this work three different cloud computing services earthcube integration and testing environment ecite cyverse and amazon web services aws ecite and cyverse are funded by nsf and both are under active development one main advantage for using ecite or cyverse is that they are free of charge for scientific studies aws though not free does offer a competitive grant program for researchers from our experience the aws platform made the process of obtaining computer resources the simplest when compared to ecite and cyverse the aws user simply logs in to the console selects the type of the machine needed and launches it when using ecite we had to contact the developer and ask for an instance with the required specifications and a short paragraph summarizing the project we are working on to justify the allocation of compute resources we also needed to contact the developer each time we wanted to open a port e g port 22 to ssh or port 80 for http the service did not support elastic ips like aws so each time we restarted an instance and wanted to use ssh to access to the machine we needed to report the ip address used to access the machine to the developer to add this address to the security rules cyverse is a more mature service but allows each user only a certain allocation of computational time once the user exceeds this allocation the instance is suspended and the user needs to request more time from the administrators this feature was problematic for our use case of a continually available cloud based resource for online model execution for these reasons we used aws ec2 for much of the testing work described in this paper but ecite and cyverse are in active development and will likely be good options for this use case in the future while this approach shows great promise it is not without limitations 1 the sciunit cli tool must be installed in order to re execute a sciunit container and 2 hydroshare lacks methods for uniquely identifying and managing web app resources that will be needed as the number of these resources continues to increase regarding the latter limitation without a more organized structure naming conflicts could cause confusion when using the open with button over which app is to be requested also this work does not fully explore computational challenges associated with the proposed methodology using cloud services like aws provides the opportunity for scalability as more users are added for example this solution used small ec2 instances for prototyping future work could explore aws ec2 container service ecs as an alternative for a more scalable solution to support multiple concurrent users data movement between hydroshare and aws is another potential issue as data volumes increase which is not uncommon for hydrologic modeling hydroshare is built on irods integrated rule oriented data system which includes the ability to interface with aws s3 storage resources future work could explore using this functionality to automate the movement of large files between hydroshare and aws to support computation within aws and still maintain access through the hydroshare user interface irods is specifically designed to handle such data federation needs and should provide a robust solution for managing the large data flows common in hydrologic modeling lastly future work should explore scaling of the general approach presented here to use cases in which multiple sciunits are available for execution within a remote cloud based resource in this case a user could select from available sciunits to process input data stored with hydroshare making for a potentially very powerful general approach applicable to many different modeling and analysis use cases that require remote data processing acknowledgements and disclaimer we gratefully acknowledge the national science foundation for support of this work under awards aci 0940841 icer 1343800 and icer 1440323 any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
26386,empirical rainfall thresholds are commonly used to forecast landslide occurrence in wide areas thresholds are affected by several uncertainties related to the rainfall and the landslide information accuracy the reconstruction of the rainfall responsible for the failure and the method to calculate the thresholds this limits the use of the thresholds in landslide early warning systems to face the problem we developed a comprehensive tool ctrl t calculation of thresholds for rainfall induced landslides tool that automatically and objectively reconstructs rainfall events and the triggering conditions responsible for the failure and calculates rainfall thresholds at different exceedance probabilities ctrl t uses a set of adjustable parameters to account for different morphological and climatic settings we tested ctrl t in liguria region italy which is highly prone to landslides we expect ctrl t has an impact on the definition of rainfall thresholds in italy and elsewhere and on the reduction of the risk posed by rainfall induced landslides graphical abstract image 1 keywords rainfall event rainfall modelling failure evapotranspiration liguria software availability name ctrl t calculation of thresholds for rainfall induced landslides tool developer massimo melillo contact address cnr irpi via madonna alta 126 06128 perugia italy email massimo melillo irpi cnr it program language r software requirement r integrated development environment ide suggest rstudio source code http geomorphology irpi cnr it tools rainfall events and landslides thresholds ctrl algorithm ctrl code ctrl code r 1 introduction landslides are natural and human induced phenomena that affect all continents playing an important role in the evolution of landscapes and posing a serious threat to the population keefer and larsen 2007 nadim et al 2006 2013 petley 2012 rainfall is a recognized trigger of landslides and this explains why there is a vast scientific literature on the relationship between rainfall and landslide occurrence at regional and global scales empirical rainfall thresholds are among the most used tools for the prediction of rainfall induced slope failures several authors have proposed different methods for the calculation of rainfall thresholds through the statistical analysis of empirical distributions of rainfall conditions that have presumably resulted in landslides including cumulated event rainfall vs rainfall duration or mean rainfall intensity vs rainfall duration e g aleotti 2004 guzzetti et al 2007 2008 brunetti et al 2010 berti et al 2012 giannecchini et al 2012 martelloni et al 2012 peruccacci et al 2012 staley et al 2013 segoni et al 2014 rosi et al 2016 galanti et al 2017 some authors have considered both rainfall conditions that have and have not resulted in landslides and have used optimization techniques to define the optimal thresholds e g berti et al 2012 staley et al 2013 peres and cancelliere 2014 on the other hand attempts to predict the occurrence of rainfall induced landslides by means of a physically based approach are present in the scientific literature e g lepore et al 2017 alvioli et al 2014 alvioli and baum 2016 an et al 2016 peres and cancelliere 2016 empirical rainfall thresholds are affected by several uncertainties including i the availability and quality of the rainfall measurements and of the landslide information guzzetti et al 2007 berti et al 2012 peruccacci et al 2012 nikolopoulos et al 2014 gariano et al 2015 marra et al 2017 peres et al 2017 ii the characterization of the rainfall event responsible for the landslides guzzetti et al 2007 iadanza et al 2016 iii the heuristic or statistical methods used to determine the thresholds peruccacci et al 2012 2017 vennari et al 2014 as for the first point uncertainty exists especially in rainfall series containing large data gaps depending on the information source uncertainty can also affect the geographical and the temporal location of the failure gariano et al 2015 found that even a small 1 lack of landslide information can result in a significant decrease in the performance of a threshold based prediction model uncertainty in the characterization of rainfall data has a strong impact on the identification of the thresholds often resulting in their underestimation leading to a high number of false alarms in early warning system applications nikolopoulos et al 2014 marra et al 2017 peres et al 2017 regarding the second point melillo et al 2015 highlighted how standards for measuring landslide triggering rainfall conditions are still lacking or insufficiently formalized in literature indeed how the rainfall responsible for the landslide occurrence is calculated and whether its definition is reliable it is rarely reported thus reducing the possibility of comparing different thresholds concerning the last point the majority of empirical rainfall thresholds available in the literature are calculated using subjective and scarcely repeatable methods only few attempts were recently made to conceive procedures for an objective and reproducible definition of thresholds brunetti et al 2010 martelloni et al 2012 staley et al 2013 segoni et al 2014 vessia et al 2014 melillo et al 2015 piciullo et al 2017 peruccacci et al 2017 we maintain that the quantitative identification of the landslide triggering rainfall and the definition of reliable thresholds are fundamental steps towards a well founded landslide prediction in this work we upgraded the algorithm proposed by melillo et al 2015 2016 introducing new criteria to select automatically i the representative rain gauge i e the most representative measuring station suitable to reconstruct the landslide triggering rainfall and ii the rainfall conditions responsible for landslides the algorithm standardizes the actions performed by an investigator that defines rainfall events starting from series of rainfall records and landslide occurrence dates in addition the algorithm modelling the cumulated event rainfall identifies the rainfall conditions responsible for the observed failures and calculates rainfall thresholds at different exceedance probabilities brunetti et al 2010 peruccacci et al 2012 the algorithm is implemented in a tool ctrl t calculation of thresholds for rainfall induced landslides tool written in r open source software appendix a the paper is organized as follows first we describe the tool and the main upgrades and improvements herein proposed in the algorithm section 2 then we describe data and study area section 3 and some specific parameters section 4 in section 5 we show the results obtained from its application in the study area liguria northwestern italy this is followed in section 6 by a discussion on the main advantages of the tool and its potential application for the reconstruction and assessment of the rainfall conditions that can initiate landslides in wide geographical areas we conclude section 7 summarizing the main findings of the work 2 description of ctrl t ctrl t contains an improved version of the algorithm proposed by melillo et al 2015 2016 the former version of the algorithm allowed i the reconstruction of distinct rainfall events by pre processing and analyzing rainfall measurements and aggregating rainfall sub events ii the identification of multiple rainfall conditions responsible for the triggering of documented landslides iii the definition of rainfall thresholds for possible landslide occurrence the former algorithm used pre defined parameters to account for different seasonal and climatic conditions ctrl t includes new criteria to reconstruct automatically the rainfall conditions responsible for landslides and to define rainfall thresholds at different exceedance probabilities fig 1 illustrates the logical framework of the improved algorithm the input data is composed by i setting parameters of the rainfall events ii rainfall data iii rain gauge locations iv landslide locations and v landslide occurrence times in fig 1 grey bordered boxes represent actions already implemented in the previous version of the algorithm while blue bordered boxes highlighted the new additional actions the algorithm is divided into three main logical blocks fig 1 the block 1 executes the reconstruction of the rainfall events re using the setting parameters and the rainfall series obtained by the rain gauges located in the study area a rainfall event is a period of continuous rainfall or an ensemble of periods of continuous rainfall separated from the preceding and the successive events by dry no rain periods the parameters are selected according to the climate conditions of the area melillo et al 2015 see section 4 using the information on the location of rain gauges and landslides provided by the input section block 2 picks out the rain gauges closest to each landslide details about the selection of rain gauges are reported in section 2 1 in the following action of the same block for each selected rain gauge the algorithm identifies the rainfall event associated with the landslide adopting a modelling of the cumulated event rainfall described in section 2 2 and using criteria reported in melillo et al 2015 the algorithm reconstructs the single or the multiple rainfall conditions mrc likely responsible for each failure mrc can be a d l e l pair of rainfall event duration d l and cumulated event rainfall e l or a set of two or more pairs through an empirical relation that includes the distance between the rain gauge and the landslide d l and e l see section 2 3 a weight w is assigned to each pair of the mrc data set for each landslide among the selected rain gauges the mrc corresponding to the maximum w is that likely responsible for the failure in the block 3 the set of the mrc associated to all the available landslides is used to calculate the rainfall thresholds section 2 4 in the following sections we describe in detail the new actions included in the improved version of the algorithm 2 1 rain gauge selection procedure in the previous release of the algorithm melillo et al 2015 the selection of the representative rain gauge was done manually through multiple queries to the database of rainfall measurements in the new release the algorithm was improved to select automatically the representative rain gauge for a specific landslide from a pool of rain gauges close to the landslide fig 2 the geographical locations of the landslides and of the rainfall stations are a necessary input of ctrl t fig 1 for each failure nearby rain gauges are located in a circular area buffer centered on the landslide location and with a parametrized radius r b in fig 2 r b depends chiefly on the morphological settings of the study area and on the rain gauge density as an example in mountain regions where the altitude changes abruptly the buffer radius should not exceed 5 km whereas in flat regions with low rain gauge density it can increase to 15 km the representative rain gauge used to reconstruct the rainfall responsible of the failure is identified successively after an objective analysis of the reconstructed mrc conditions section 2 3 2 2 modelling the cumulated rainfall responsible for the failure in the previous release of the algorithm the contribution of the rainfall to the landslide occurrence was fully included regardless of the time elapsed since the start of the rain hence any possible effect of the soil water saturation was neglected to overcome this problem ctrl t models the rainfall by applying an arbitrary constant decay factor e g k 0 84 to the cumulated event rainfall e l as proposed by crozier 1999 1 e l e l 0 k e l 1 k 2 e l 2 k n e l n i 0 n k i e l i where e l 0 is the cumulated rainfall in the 24 h before the landslide occurrence time t l e l i is the cumulated rainfall in the 24 h of the i th day before t l and n is the duration of the rainfall event in days i e steps of 24 h as an example the rainfall contribution to the amount of rainfall in the 5th day before the landslide reduces to about one half 49 8 of the measured rain the k decay factor can be changed according to the soil moisture drainage of the local study area 2 3 weight assignement for each landslide the algorithm proposed by melillo et al 2015 identified a variable number n of single n 1 or multiple rainfall conditions likely responsible for the failure all the reconstructed d l e l pairs were considered equally probable i e with the same weight w 1 landslide triggering conditions in a successive release of the algorithm melillo et al 2016 the rainfall conditions were selected with a weight w inversely proportional to n w 1 n with that assumption the weight of a rainfall condition would not depend on the d l and e l variables but only on its multiplicity n w f n here we propose a new empirical meaning of w which is proportional to the inverse square distance between the rain gauge and the landslide d 2 the cumulated rainfall el and the rainfall mean intensity eldl 1 2 w f d e l d l d 2 e l 2 d l 1 the weight is attributed to each d l e l pair of the mrc data set for multiple d l e l pairs i e rainfall conditions with increasing duration and increasing cumulated rainfall when the difference in the cumulated rainfall e l between one pair and the subsequent is less than 10 the weight attributed to the latter pair is null w 0 and this condition is removed from the mrc data set similarly w is null for rainfall conditions having a delay between the rainfall ending time and the landslide occurrence time longer than an assigned time set to 48 h this last requirement prevents the use of wrong information i e incorrectly dated landslides in the threshold definition indeed we expect that the considered landslides are mostly shallow and therefore occur within a short delay after the triggering rainfall for each landslide w is used to identify the representative rain gauge considering both geographical and rainfall features and to determine the probability of the single or multiple rainfall conditions to be adopted for the calculation of rainfall thresholds for a pool of stations enclosed in the radius r b the representative rain gauge is the one for which the d l e l pair has the highest w in case of multiple pairs each w is normalized to the sum of the individual weights whereas is set to one in case of a single condition ctrl t calculates rainfall thresholds for mrc and mprc maximum probability rainfall conditions data sets where mprc is the subset of the d l e l pairs with the highest weights 2 4 definition of rainfall thresholds to define empirical rainfall thresholds and their associated uncertainties ctrl t adopts a bootstrapping statistical technique peruccacci et al 2012 and a frequentist method brunetti et al 2010 by sampling the weighted rainfall conditions that have triggered landslides the general form of the threshold curves is a power law 3 e   d   where e is the cumulated event rainfall in mm d is the rainfall event duration in h  is a scaling constant the intercept  is the shape parameter that defines the slope of the power law curve and  and  represent the uncertainties of  and  respectively in eq 3  and  are the mean values of the parameters resulting from the calculation of thresholds of 5000 synthetic series generated by the algorithm  and  are the standard deviation of  and  respectively each synthetic series contains the same number n of landslides as the original data set but selected randomly with replacement bootstrap technique to calculate the thresholds a single d l e l pair is associated to each landslide for each landslide in the individual synthetic series the algorithm samples randomly with a probability w a single rainfall condition from the mrc data set the extracted d l e l pairs of the n landslide are used to define the thresholds the algorithm also uses the rainfall conditions with the maximum w to define thresholds for the mprc data set the output of the bootstrapping technique consists of 5000 synthetic series of m d l e l pairs analysis of the m synthetic series allows calculating the mean value and the uncertainty associated with the threshold parameters  and  and their respective uncertainties   3 study area and data we tested the tool using rainfall and landslide data available to us for the liguria region 5410 km2 in northwestern italy we used hourly rainfall measurements collected in the 15 year period from march 2001 to december 2014 by a network of 172 rain gauges operated by the osservatorio meteo idrologico della regione liguria omirl fig 3 portrays the geographical distribution of the rain gauges yellow triangles having an average density of about one rain gauge every 31 km2 moreover we exploited geographical and temporal information on 561 rainfall induced landslides occurred in liguria between october 2004 and november 2014 red dots in fig 3 among those there are 73 rock falls 16 mudflows 71 other types of landslides e g earth flows debris blows etc and 401 unspecified shallow landslides on line chronicles and reports of local fire brigades were the exclusively information source for 337 60 and 51 9 landslides respectively the remaining 173 events 31 were reconstructed using information from both sources generally these last events have both a high geographical and temporal accuracy 4 computation of the regional parameters in a previous version of the algorithm applied in the sicily region southern italy melillo et al 2015 used a heuristic approach to separate two consecutive rainfall events they selected a minimum dry i e with no rain interval of 2 days 48 h in the warm period c w april october and a minimum of 4 days 96 h in the cold period c c november march in the same paper the time required for the soil to dry out was assumed inversely proportional to the amount of real evapotranspiration ret and they found a ratio r r e t c w r e t c c 2 however depending on the local seasonal and climatic conditions the length number of months of the c w and c c periods may vary and may lead r to vary accordingly thorp 1986 for this reason among several methods proposed for evaluating the evapotranspiration alley 1984 yates 1996 naoum and tsanis 2003 guo et al 2016 pumo et al 2017 we calculated the r value adopting the monthly soil water balance mswb model thornthwaite 1948 thornthwaite and mather 1957 melillo 2009 di matteo et al 2011 dragoni et al 2015 and we determined the length in months of the c w and c c periods the mswb model is a simplified approach with only few input requirements and is commonly used to obtain reliable estimates of the water balance components e g potential and real evapotranspiration input data of the mswb model are i the latitude ii the average monthly rainfall e m iii the average monthly temperature t m and iv the maximum field capacity or soil water storage sws we calculated e m using data available from the network of 172 rain gauges operated by the omirl in the liguria region see section 3 and t m from data provided by the website of istituto superiore per la protezione e la ricerca ambientale www scia isprambiente it in the period from january 2000 to december 2014 fig 4a shows a graph bagnouls gaussen diagram with the average monthly rainfall grey histogram and the average monthly temperature red line in the liguria region overall the spatial variability of the temperature is about 1 6 c the month with the highest variability is december standard deviation  equal 2 0 c whereas june is the month with the lowest one  1 4 c the spatial variability of the precipitation is 30 3 mm july and november are the months with the lowest  13 6 mm and the highest  51 4 mm spatial variability respectively the sws values were calculated according to the following equation nyvall 2002 4 sws rd awsc where rd is the crop rooting depth m varying from shallow 0 45 m for leaf vegetables to deep 1 20 m for tree fruits and awsc is the available water storage capacity of the soil mm m which ranges from 83 mm m sand textural class to 208 mm m silt textural class since rd and awsc are not available for the entire study area we calculated all the possible sws values table 1 with equation 4 using the mswb model we estimated the average monthly potential evapotranspiration p e t m green curve in fig 4 b and the average monthly real evapotranspiration r e t m corresponding to each sws value the yellow shaded area in fig 4b represents the range of variability of r e t m obtained varying sws the orange curve in the figure is the mean value of r e t m as expected r e t m decreases in the warm period when the rainfall grey histogram in fig 4a decreases we found the c w and c c periods using the average monthly aridity index a i m barrow 1992 5 a i m e m p e t m specifically c w is the period when the soil exhibits a water deficit and is mostly dry e m less than p e t m and a i m 1 and is from may to september red portion of the histogram in fig 4c conversely the c c period is from october to april when a i m 1 green portion of the histogram in fig 4c finally we obtained r 2 4 0 3 calculating the ratio of the total amount of r e t m between may and september c w period and between october and april c c period the uncertainty of ret ratio derives from the variability of r e t m yellow shaded area in fig 4b as a result in liguria r ranges broadly from a value between 2 and 3 which is close to the heuristic value r 2 adopted by melillo et al 2015 in sicily in the following analysis we reported the reconstructed rainfall events rainfall conditions responsible for the failures and the relative rainfall thresholds assuming r 2 which corresponds to a minimum dry interval of 48 h and 96 h in the c w and in the c c period respectively then we assumed r 3 which corresponds to a minimum dry interval of 48 h in the c w and 144 h and in the c c period and we compared the distributions of the rainfall events and of the rainfall conditions and the obtained thresholds 5 results adopting ctrl t and using information presented in the previous sections rainfall events and empirical rainfall thresholds at several exceedance probabilities were determined the empirical cumulative distribution function ecdf of the duration d and of the cumulated rainfall e are shown in fig 5 for the re green curves mrc purple curves and mprc data set orange curves obtained by ctrl t assuming r 2 the corresponding statistics are reported in table 2 for each data set the table contain the total number the minimum and the maximum values and the first second and third quartiles q1 q2 and q3 of the reconstructed rainfall events re and rainfall conditions mrc and mprc fig 5a reveals that more than 70 of the reconstructed rainfall events re green curve have a duration d 100 h and 25 have d 6 h table 2 the shortest values of d are observed in the mprc data set orange curve fig 5b shows the ecdf for the re mrc and mprc data sets in particular the median of the cumulated rainfall e for re is lower 20 than that for mrc and mprc table 2 whereas the duration d is comparable for the three data sets as a consequence the triggering conditions in the mrc and mprc data sets exhibit generally a higher mean rainfall intensity i e d fig 5 c representing the most severe conditions reconstructed for the single landslide the same statistics indicates that the duration d of the mrc are generally longer than that of mprc while the cumulated rainfall e is substantially the same table 2 the delay between the rainfall ending time and the landslide occurrence time is null 0 h for the majority of mprc 286 the delay is 3 h for 69 mprc and is between 4 and 48 h for the remaining 85 mprc using the mrc and mprc data sets and adopting the method proposed by brunetti et al 2010 and peruccacci et al 2012 we determined objective cumulated event rainfall rainfall duration ed thresholds and their associated uncertainties for the study area fig 6 fig 6a shows in log log coordinates the 801 d l e l pairs relative to the mrc data set purple dots together with the ed frequentist threshold at 5 exceedance probability t5 mrc r2 in purple see equation in table 3 and its associated uncertainty shaded area analogously fig 6b shows the 440 d l e l pairs for the mprc data set orange dots the 5 ed frequentist threshold t5 mprc r2 in orange see equation in table 3 and the relative uncertainty shaded area fig 6c shows the same thresholds in linear coordinates and in the range 1 d 120 h we note that t5 mrc r2 is higher than t5 mprc r2 for durations d 10 h conversely for longer durations t5 mprc r2 is getting higher due to the greater angular coefficient the mean value of the difference among the cumulated rainfall e for the two thresholds is 7 mm 12 for 1 d 300 h in particular the e differences varies from values below 2 mm 4 for d 24 h to values that do not exceed the 12 mm 16 for d 300 h to evaluate how r affects the reconstruction of the rainfall events rainfall conditions and the thresholds we conducted a sensitivity analysis assuming r 2 and r 3 solid and dashed curves respectively in the subsequent figures for the purpose we compared the ecdf of d and e for re mrc mprc fig 7 in addition we investigated the differences between the 5 thresholds for mrc and mprc fig 8 below q2 i e d 30 h we found small differences 3 between the ecdf curves of d for the re data sets green curves in fig 7a the differences slowly increase up to 25 at q3 d 100 h analogously in terms of e the differences are between 5 and 10 from q2 to q3 in the range 20 e 65 mm green curves in fig 7b for mrc the percentage difference in the ecdf of d is always 12 up to q3 purple curves in fig 7a while for e the ecdf differences do not exceed 5 purple curves in fig 7b comparing the ecdf curves for the mprc we noted that they overlap for both d and e orange curves in fig 7a and b log log plot in fig 8 shows the 5 ed thresholds calculated using a mrc data set for r 2 and r 3 the obtained thresholds are substantially similar with differences in cumulated rainfall lower than 1 mm 1 at d 24 h the maximum difference is 6 mm 7 for the longest duration d 517 h the 5 thresholds for mprc are coincident orange curves in fig 8 6 discussion 6 1 algorithm improvements ctrl t aims at reducing the computational time of the single steps from the rainfall event reconstruction to the threshold definition the actions performed by ctrl t prove robust and the implementation software do not suffer from computer storage or processing limitations more in details the algorithm improvements reduce the calculation time significantly in the i selection of the available rain gauges ii identification of the representative rain gauge and iii reconstruction and analysis of the rainfall conditions responsible for the failure as for the first two points the long manual procedure to associate a failure to the closest rain gauge is now overcome by the automatic selection of all the available stations within a given distance r b from the landslide r b is parameterized and can vary as a function of the density of the rain gauges ctrl t identifies automatically the representative rain gauge through a weight which is a function of both geographical distance between landslide and rain gauge and hydrological cumulated rainfall and duration of the rainfall event characteristics associated to the selected stations with the proposed weight the closest rain gauges are selected in almost 80 of the cases further improvements might be addressed to the weight evaluation by including orographic and geomorphological parameters such as the elevation difference between landslide and rain gauge the procedure implemented in ctrl t allows running the algorithm also when the rainfall information is available as gridded rainfall maps e g obtained through satellite data in this case each pixel of the map e g 25 km 25 km is treated as a virtual rain gauge otherwise the manual reconstruction of rainfall conditions using gridded rainfall maps would be time consuming regarding the third point the identification and selection of the rainfall conditions responsible for the landslide was previously left to the investigator and it was therefore subjective and time consuming the introduction of the automatic and reproducible selection of rainfall conditions based on weights solves this drawback and tries to face one of the main sources of uncertainties in the threshold definition process an additional improvement is obtained introducing the decay factor k in the previous version all the d l e l pairs were considered equally probable for the activation of the landslide however conditions characterized by long durations and low mean intensity are unlikely to trigger a shallow landslide the decay factor k damps the cumulated rainfall removing those rainfall condition with a small 10 rainfall increase compared to the previous condition in the new version of the algorithm we add an action to calculate thresholds and their uncertainties adopting a frequentist method brunetti et al 2010 peruccacci et al 2012 the procedure that begins from the reconstruction of the rainfall events and ends with the definition of thresholds is now faster than before and completely automatized thus objective and reproducible the structure of the algorithm also allows the implementation and or integration of other threshold calculation methods further details regarding ctrl t e g the required input files and the generated output files are available in appendix a 6 2 parameter settings as described in melillo et al 2015 in the previous version of the algorithm the same values of the parameters were used to reconstruct the rainfall events from rainfall data recorded in all available rain gauges this aspect represents a great limitation given that the rain gauges located in wide areas might record differ rainfall regimes in order to solve this problem ctrl t performs the reconstruction of the rainfall event using an input file see appendix a containing the values of the selectable parameters which can be adjusted for each rain gauge as stated in section 4 we adopted the average monthly aridity index to establish the length of cold and warm periods then we used the real evapotranspiration in the two periods to set the length of the two dry intervals c w and c c the calculation of these parameters is fundamental when using the algorithm in different seasonal and climatic contexts and with various soil types the procedure to determine c w and c c is still not automatized 6 3 rainfall events and thresholds as reported in the previous section the statistics of ecdf curves for re mrc and mprc data sets fig 5 suggest that the triggering conditions selected in the mrc or mprc data sets have a rainfall mean intensity i e d higher than that of re generally to calculate the thresholds it is worth to use the most severe mrc or mprc conditions instead of the whole rainfall event re in order to avoid an underestimation of the rainfall responsible for the landslide note that only in few cases 1 the d e pairs for mrc mprc and re coincide fig 6c and the equations of the two thresholds obtained for the mrc and mprc data sets t5 mrc r2 and t5 mprc r2 in table 3 reveal that the thresholds are statistically indistinguishable if considering the uncertainties associated with the curves fig 6c since the number of rainfall conditions used to determine the thresholds is largely above 75 which is the minimum number to obtain stable  and  parameters peruccacci et al 2017 we acknowledge that the two curves are very similar despite the rather large uncertainties 6 4 sensitivity analysis of the r value the two different values of the r parameter affect the ecdf curves of the reconstructed rainfall events solid and dashed green curves in fig 7 while they are comparable in terms of cumulated rainfall e they are dissimilar regarding the rainfall duration d as the value of the minimum dry period in the cold season c c increases from 96 h r 2 to 144 h r 3 the duration of the rainfall events increases and the total number of re decreases a higher value for the minimum dry period needed to separate two subsequent rainfall events makes more difficult the separation of events consequently the length of the rainfall event increases the duration of the mrc is slightly longer when using r 3 instead of r 2 while the distribution of the cumulated rainfall remains unvaried purple curves in fig 7 the ecdf curves for mprc are overlapping orange curves in fig 7 because the mprc selected by ctrl t in the two cases r 2 and r 3 are coincident this occurs if for each landslide the rainfall events reconstructed using r 2 and r 3 are the same or if the rainfall events for r 2 are included in those reconstructed for r 3 and the additional mrc related to r 3 are not able to vary the selection of mprc however differences are not so evident given that in the cold period c c november march 72 of the rainfall events reconstructed using r 2 and r 3 coincide this means that these events are separated by at least a minimum dry period of 144 h inspection of fig 8 reveals that the four thresholds obtained by using the mrc and mprc data sets and the two r values do not differ much in the study area in particular thresholds for r 2 t 5 m r c r 2 and r 3 t 5 m r c r 3 are completely overlapping while the four thresholds considering the uncertainties associated with them table 3 can be considered statistically indistinguishable therefore the uncertainty introduced using different parameters e g parameters obtained by the proposed ret analysis is negligible and substantially does not affect the threshold calculation 6 5 threshold comparison we compared the results obtained providing as input the same landslide and rainfall data sets to the old version of the algorithm melillo et al 2016 and to the release proposed in this work fig 9 shows the threshold t 5 m r c r 2 obtained using the previous version of the algorithm and the thresholds t 5 m r c r 2 and t 5 m r c r 2 obtained using the new release of the algorithm and considering k 1 and k 0 84 respectively see table 3 for the threshold parameters in particular the t 5 m r c r 2 threshold has the same slope of the t 5 m r c r 2 and a higher intercept the t 5 m r c r 2 threshold has a different slope and a range of duration shorter than the others given that the use of the k decay factor decreases the durations of the rainfall conditions responsible for the failures moreover using the k decay factor the rainfall amount necessary to initiate landslides is lower for duration longer than 10 h see t 5 m r c r 2 and t 5 m r c r 2 in fig 9 the introduction of weights for the rainfall conditions and of a decay factor for the cumulated rainfall made the thresholds more similar to those defined by an expert investigator in fact the earlier version of the algorithm returned thresholds lower and steeper than those determined manually melillo et al 2016 we maintain that ctrl t reproduces better the reconstruction of the rainfall condition responsible for landslide triggering performed by an expert investigator 7 concluding remarks the definition of reliable and accurate rainfall thresholds poses several critical issues and sources of subjectivity the use of a standardized and automatized procedure for the reconstruction of the rainfall conditions responsible for failures and for threshold calculation is necessary for enhancing the objectivity and reproducibility of the thresholds especially for thresholds to be used in landslide early warning systems in this work we faced this problem and updated the algorithm proposed by melillo et al 2015 2016 adding new features and ameliorations ctrl t exploits continuous rainfall measurements and landslide information to 1 reconstruct rainfall events 2 select automatically the representative rain gauges 3 identify multiple rainfall conditions responsible for the failure in terms of d and e modelling the cumulated event rainfall 4 attribute a probability to each rainfall condition and 5 calculate probabilistic rainfall thresholds and their associated uncertainties the main innovations novelties concern i the automatic selection of the representative rain gauge that is chosen by analyzing the distance between the rain gauge and the landslide and the characteristics of the rainfall events recorded by the station and ii the assignment of a probability to the single or multiple rainfall conditions responsible for the landslide in addition ctrl t incorporates the procedures to calculate objective and reproducible thresholds developed by brunetti et al 2010 and peruccacci et al 2012 we maintain that ctrl t enhances the definition of empirical rainfall thresholds for landslide forecasting in particular the use of the algorithm standardizes and accelerates considerably the slow and tedious process of the reconstruction of the rainfall events and reduces the subjectivity inherent in the manual treatment of the rainfall and landslide data this decreases the uncertainty associated with the definition of the rainfall conditions responsible for landslides and allows the possibility of a periodic and reproducible update of the thresholds definitely this has a positive impact on the application of thresholds in early warning systems for operational landslide forecasting acknowledgements this work was financially supported by the italian national department for civil protection dpc accordi di collaborazione 2014 2015 2016 that also provided access to the rainfall database we are grateful to ivan marchesini and massimiliano alvioli cnr irpi who contributed to the calculation of the mean monthly temperatures used in this work massimo melillo was supported by a grant from the italian national department for civil protection accordi di collaborazione 2015 2016 stefano luigi gariano was supported by a grant from nerc in the framework of the landslip project grant number nerc dfid ne p000681 1 anna roccati was supported by a grant from the environment department of the liguria region accordi di collaborazione 2013 2014 we thank the editor and four anonymous reviewers for their criticisms and comments that have helped us to improve the paper appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 024 appendix a software and data we implemented ctrl t calculation of thresholds for rainfall induced landslides tool using the r open source software for advanced statistical computing and graphics release 3 3 3 http www r project org ctrl t was finalized in june 2017 and should work on computers with at least 4 gb of ram the code can be downloaded free of charge at the following website http geomorphology irpi cnr it tools rainfall events and landslides thresholds ctrl algorithm ctrl code ctrl code r the downloadable executable file is 79 kb in addition an example of the input files required by the algorithm can be downloaded free of charge at the following website http geomorphology irpi cnr it tools rainfall events and landslides thresholds ctrl algorithm input demo input zip the downloadable file is 37 55 mb the algorithm is divided into three main blocks fig 1 which perform i the reconstruction of rainfall events ii reconstruction of the rainfall conditions responsible for the landslide and iii the definition of the rainfall thresholds respectively input files here we describe the information provided by the input text files boxes labelled with input in the left upper part of the logical scheme in fig 1 one input file table of raingauges csv contains information on the rain gauges cod area longitude latitude pk sensor id rain gauge and on the parameters used by the algorithm to reconstruct the rainfall events gs p1 c p1 w p2 c p2 w p3 p4 c p4 w sws ews as described in melillo et al 2015 in particular cod area is an identification code of the administrative region where the sensor is located longitude and latitude are the coordinates expressed in decimal degrees and id rain gauge is a unique numeric code derived from the rain gauge geographic coordinates a series of input files named timeseries sensor id rain gauge csv contain the time stamp and the hourly rainfall measurements of each sensor an additional input file named landslides csv contains information on the documented landslides in particular each record reports the i id project which identifies the rainfall event ii id lan which gives the temporal order of the failures associated with the single rainfall event i e a is the 1st landslide b the 2nd etc iii class number which indicates the landslide multiplicity iv class type which is the failure type v longitude and latitude of the landslide vi geo acc which is the level of mapping accuracy peruccacci et al 2012 vii failure occurrence date day month year and time and viii date acc which is the accuracy of the occurrence date t1 t2 and t3 t1 includes landslides for which the exact time of occurrence is known t2 and t3 are used when the part of the day or the day of occurrence are known respectively output files the three blocks of the algorithm boxes in the right part of the logical scheme in fig 1 produce different output files the first block performs the reconstruction of the rainfall events and generates the folder reconstructed rainfall events which contains an output file named rainfall events csv listing the reconstructed rainfall events in particular for each record id rain gauge is the rainfall station code index pos1 and index pos2 are indexes related to the rainfall series of each rain gauge re start date and re end date are the starting and the ending date of each rainfall event the remaining fields are the rainfall duration d e the cumulated rainfall e e the rainfall mean intensity i e the maximum hourly rainfall ip e the maximum cumulated rainfall in 24 h emax24 e and a class a class from 1 to 6 related to the rainfall event classification of alpert et al 2002 after the reconstruction of rainfall events the algorithm calculates the maximum hourly rainfall and the maximum cumulated rainfall in 24 h this allows to check and possibly remove anomalous rainfall events the second block reconstructs the multiple conditions mrc likely responsible for the failures and generates the folder reconstructed rainfall conditions which contains three outputs the first output is the folder individual files containing a single file for each landslide named landslide id project id lan pdf this file contains for each rain gauge included in a buffer centered in the landslide location see section 2 1 the i information about the descriptive statistics of the rain gauge fig 10 a ii distribution of the reconstructed d e e e rainfall pairs in the logarithmic plane fig 10b iii hourly rainfall measurements of the reconstructed rainfall event recorded by the representative rain gauge fig 10c iv multiple d l e l rainfall conditions responsible for the landslide in the logarithmic plane fig 10d the second output is a text file named processing summary report txt this file contains the statistics about the total number of the analyzed reconstructed and discarded landslides we produce a list of the discarded landslides labelled with the id project attribute the third output is a file named mrc csv in this file id project identifies the rainfall event with landslide s id lan lists chronologically the landslides triggered by the same rainfall event date is the failure occurrence time rrg select is the rank of the representative rain gauge based on the distance from the landslide within the circular buffer and rrg distance is the distance between the representative rain gauge and the landslide locations the file lists also information on the rainfall duration d l the cumulated event rainfall e l the rainfall mean intensity i l the rain gauge code id rain gauge the number of rainfall events ren associated with the id rain gauge the maximum hourly rainfall ip l the maximum cumulated rainfall in 24 h emax24 e a class a class from 1 to 6 according to alpert et al 2002 rainfall event classification a binary value ms flag that indicates if the currently rainfall condition have a maximum value of the score w n mrc 1 and the number of multiple rainfall conditions n mrc the third block defines the rainfall thresholds using mrc and mprc data sets see section 2 and generates the folder rainfall thresholds which contains four outputs representing the results of the analysis for each subset the algorithm generates two distinct files the first files boot mrc csv or boot mprc csv contain information on the parameters  and  that define 16 threshold curves at different exceedance probabilities from 0 005 to 50 for each threshold six fields are specified i e the name of the threshold parameters variable the corresponding exceedance probabilities probability the mean value mean obtained through the bootstrap process peruccacci et al 2012 the standard deviation value sigma and the extreme values min max the second files boot mrc pdf or boot mprc pdf contain in the first page the distribution of the d l e l pairs in log log coordinates purple dots in fig 11 a the empirical cumulative distribution functions ecdf of d fig 11b e fig 11c and rrg distance fig 11d moreover the marginal distribution of the representative rain gauge ranking based on the distances between all rain gauges within the circular buffer and the landslide rrg select and of the number of multiple rainfall conditions n mrc are also reported grey and purple bars in fig 11e respectively here we report as example the file related to the mrc data set the one related to the mprc data set is analogous in the subsequent 16 pages are reported the graphs related to i the thresholds at exceedance probabilities from 0 005 to 50 in linear and log log coordinates fig 12 a b respectively and the variation of the values of the  parameter and the related uncertainty  fig 12c d respectively 
26386,empirical rainfall thresholds are commonly used to forecast landslide occurrence in wide areas thresholds are affected by several uncertainties related to the rainfall and the landslide information accuracy the reconstruction of the rainfall responsible for the failure and the method to calculate the thresholds this limits the use of the thresholds in landslide early warning systems to face the problem we developed a comprehensive tool ctrl t calculation of thresholds for rainfall induced landslides tool that automatically and objectively reconstructs rainfall events and the triggering conditions responsible for the failure and calculates rainfall thresholds at different exceedance probabilities ctrl t uses a set of adjustable parameters to account for different morphological and climatic settings we tested ctrl t in liguria region italy which is highly prone to landslides we expect ctrl t has an impact on the definition of rainfall thresholds in italy and elsewhere and on the reduction of the risk posed by rainfall induced landslides graphical abstract image 1 keywords rainfall event rainfall modelling failure evapotranspiration liguria software availability name ctrl t calculation of thresholds for rainfall induced landslides tool developer massimo melillo contact address cnr irpi via madonna alta 126 06128 perugia italy email massimo melillo irpi cnr it program language r software requirement r integrated development environment ide suggest rstudio source code http geomorphology irpi cnr it tools rainfall events and landslides thresholds ctrl algorithm ctrl code ctrl code r 1 introduction landslides are natural and human induced phenomena that affect all continents playing an important role in the evolution of landscapes and posing a serious threat to the population keefer and larsen 2007 nadim et al 2006 2013 petley 2012 rainfall is a recognized trigger of landslides and this explains why there is a vast scientific literature on the relationship between rainfall and landslide occurrence at regional and global scales empirical rainfall thresholds are among the most used tools for the prediction of rainfall induced slope failures several authors have proposed different methods for the calculation of rainfall thresholds through the statistical analysis of empirical distributions of rainfall conditions that have presumably resulted in landslides including cumulated event rainfall vs rainfall duration or mean rainfall intensity vs rainfall duration e g aleotti 2004 guzzetti et al 2007 2008 brunetti et al 2010 berti et al 2012 giannecchini et al 2012 martelloni et al 2012 peruccacci et al 2012 staley et al 2013 segoni et al 2014 rosi et al 2016 galanti et al 2017 some authors have considered both rainfall conditions that have and have not resulted in landslides and have used optimization techniques to define the optimal thresholds e g berti et al 2012 staley et al 2013 peres and cancelliere 2014 on the other hand attempts to predict the occurrence of rainfall induced landslides by means of a physically based approach are present in the scientific literature e g lepore et al 2017 alvioli et al 2014 alvioli and baum 2016 an et al 2016 peres and cancelliere 2016 empirical rainfall thresholds are affected by several uncertainties including i the availability and quality of the rainfall measurements and of the landslide information guzzetti et al 2007 berti et al 2012 peruccacci et al 2012 nikolopoulos et al 2014 gariano et al 2015 marra et al 2017 peres et al 2017 ii the characterization of the rainfall event responsible for the landslides guzzetti et al 2007 iadanza et al 2016 iii the heuristic or statistical methods used to determine the thresholds peruccacci et al 2012 2017 vennari et al 2014 as for the first point uncertainty exists especially in rainfall series containing large data gaps depending on the information source uncertainty can also affect the geographical and the temporal location of the failure gariano et al 2015 found that even a small 1 lack of landslide information can result in a significant decrease in the performance of a threshold based prediction model uncertainty in the characterization of rainfall data has a strong impact on the identification of the thresholds often resulting in their underestimation leading to a high number of false alarms in early warning system applications nikolopoulos et al 2014 marra et al 2017 peres et al 2017 regarding the second point melillo et al 2015 highlighted how standards for measuring landslide triggering rainfall conditions are still lacking or insufficiently formalized in literature indeed how the rainfall responsible for the landslide occurrence is calculated and whether its definition is reliable it is rarely reported thus reducing the possibility of comparing different thresholds concerning the last point the majority of empirical rainfall thresholds available in the literature are calculated using subjective and scarcely repeatable methods only few attempts were recently made to conceive procedures for an objective and reproducible definition of thresholds brunetti et al 2010 martelloni et al 2012 staley et al 2013 segoni et al 2014 vessia et al 2014 melillo et al 2015 piciullo et al 2017 peruccacci et al 2017 we maintain that the quantitative identification of the landslide triggering rainfall and the definition of reliable thresholds are fundamental steps towards a well founded landslide prediction in this work we upgraded the algorithm proposed by melillo et al 2015 2016 introducing new criteria to select automatically i the representative rain gauge i e the most representative measuring station suitable to reconstruct the landslide triggering rainfall and ii the rainfall conditions responsible for landslides the algorithm standardizes the actions performed by an investigator that defines rainfall events starting from series of rainfall records and landslide occurrence dates in addition the algorithm modelling the cumulated event rainfall identifies the rainfall conditions responsible for the observed failures and calculates rainfall thresholds at different exceedance probabilities brunetti et al 2010 peruccacci et al 2012 the algorithm is implemented in a tool ctrl t calculation of thresholds for rainfall induced landslides tool written in r open source software appendix a the paper is organized as follows first we describe the tool and the main upgrades and improvements herein proposed in the algorithm section 2 then we describe data and study area section 3 and some specific parameters section 4 in section 5 we show the results obtained from its application in the study area liguria northwestern italy this is followed in section 6 by a discussion on the main advantages of the tool and its potential application for the reconstruction and assessment of the rainfall conditions that can initiate landslides in wide geographical areas we conclude section 7 summarizing the main findings of the work 2 description of ctrl t ctrl t contains an improved version of the algorithm proposed by melillo et al 2015 2016 the former version of the algorithm allowed i the reconstruction of distinct rainfall events by pre processing and analyzing rainfall measurements and aggregating rainfall sub events ii the identification of multiple rainfall conditions responsible for the triggering of documented landslides iii the definition of rainfall thresholds for possible landslide occurrence the former algorithm used pre defined parameters to account for different seasonal and climatic conditions ctrl t includes new criteria to reconstruct automatically the rainfall conditions responsible for landslides and to define rainfall thresholds at different exceedance probabilities fig 1 illustrates the logical framework of the improved algorithm the input data is composed by i setting parameters of the rainfall events ii rainfall data iii rain gauge locations iv landslide locations and v landslide occurrence times in fig 1 grey bordered boxes represent actions already implemented in the previous version of the algorithm while blue bordered boxes highlighted the new additional actions the algorithm is divided into three main logical blocks fig 1 the block 1 executes the reconstruction of the rainfall events re using the setting parameters and the rainfall series obtained by the rain gauges located in the study area a rainfall event is a period of continuous rainfall or an ensemble of periods of continuous rainfall separated from the preceding and the successive events by dry no rain periods the parameters are selected according to the climate conditions of the area melillo et al 2015 see section 4 using the information on the location of rain gauges and landslides provided by the input section block 2 picks out the rain gauges closest to each landslide details about the selection of rain gauges are reported in section 2 1 in the following action of the same block for each selected rain gauge the algorithm identifies the rainfall event associated with the landslide adopting a modelling of the cumulated event rainfall described in section 2 2 and using criteria reported in melillo et al 2015 the algorithm reconstructs the single or the multiple rainfall conditions mrc likely responsible for each failure mrc can be a d l e l pair of rainfall event duration d l and cumulated event rainfall e l or a set of two or more pairs through an empirical relation that includes the distance between the rain gauge and the landslide d l and e l see section 2 3 a weight w is assigned to each pair of the mrc data set for each landslide among the selected rain gauges the mrc corresponding to the maximum w is that likely responsible for the failure in the block 3 the set of the mrc associated to all the available landslides is used to calculate the rainfall thresholds section 2 4 in the following sections we describe in detail the new actions included in the improved version of the algorithm 2 1 rain gauge selection procedure in the previous release of the algorithm melillo et al 2015 the selection of the representative rain gauge was done manually through multiple queries to the database of rainfall measurements in the new release the algorithm was improved to select automatically the representative rain gauge for a specific landslide from a pool of rain gauges close to the landslide fig 2 the geographical locations of the landslides and of the rainfall stations are a necessary input of ctrl t fig 1 for each failure nearby rain gauges are located in a circular area buffer centered on the landslide location and with a parametrized radius r b in fig 2 r b depends chiefly on the morphological settings of the study area and on the rain gauge density as an example in mountain regions where the altitude changes abruptly the buffer radius should not exceed 5 km whereas in flat regions with low rain gauge density it can increase to 15 km the representative rain gauge used to reconstruct the rainfall responsible of the failure is identified successively after an objective analysis of the reconstructed mrc conditions section 2 3 2 2 modelling the cumulated rainfall responsible for the failure in the previous release of the algorithm the contribution of the rainfall to the landslide occurrence was fully included regardless of the time elapsed since the start of the rain hence any possible effect of the soil water saturation was neglected to overcome this problem ctrl t models the rainfall by applying an arbitrary constant decay factor e g k 0 84 to the cumulated event rainfall e l as proposed by crozier 1999 1 e l e l 0 k e l 1 k 2 e l 2 k n e l n i 0 n k i e l i where e l 0 is the cumulated rainfall in the 24 h before the landslide occurrence time t l e l i is the cumulated rainfall in the 24 h of the i th day before t l and n is the duration of the rainfall event in days i e steps of 24 h as an example the rainfall contribution to the amount of rainfall in the 5th day before the landslide reduces to about one half 49 8 of the measured rain the k decay factor can be changed according to the soil moisture drainage of the local study area 2 3 weight assignement for each landslide the algorithm proposed by melillo et al 2015 identified a variable number n of single n 1 or multiple rainfall conditions likely responsible for the failure all the reconstructed d l e l pairs were considered equally probable i e with the same weight w 1 landslide triggering conditions in a successive release of the algorithm melillo et al 2016 the rainfall conditions were selected with a weight w inversely proportional to n w 1 n with that assumption the weight of a rainfall condition would not depend on the d l and e l variables but only on its multiplicity n w f n here we propose a new empirical meaning of w which is proportional to the inverse square distance between the rain gauge and the landslide d 2 the cumulated rainfall el and the rainfall mean intensity eldl 1 2 w f d e l d l d 2 e l 2 d l 1 the weight is attributed to each d l e l pair of the mrc data set for multiple d l e l pairs i e rainfall conditions with increasing duration and increasing cumulated rainfall when the difference in the cumulated rainfall e l between one pair and the subsequent is less than 10 the weight attributed to the latter pair is null w 0 and this condition is removed from the mrc data set similarly w is null for rainfall conditions having a delay between the rainfall ending time and the landslide occurrence time longer than an assigned time set to 48 h this last requirement prevents the use of wrong information i e incorrectly dated landslides in the threshold definition indeed we expect that the considered landslides are mostly shallow and therefore occur within a short delay after the triggering rainfall for each landslide w is used to identify the representative rain gauge considering both geographical and rainfall features and to determine the probability of the single or multiple rainfall conditions to be adopted for the calculation of rainfall thresholds for a pool of stations enclosed in the radius r b the representative rain gauge is the one for which the d l e l pair has the highest w in case of multiple pairs each w is normalized to the sum of the individual weights whereas is set to one in case of a single condition ctrl t calculates rainfall thresholds for mrc and mprc maximum probability rainfall conditions data sets where mprc is the subset of the d l e l pairs with the highest weights 2 4 definition of rainfall thresholds to define empirical rainfall thresholds and their associated uncertainties ctrl t adopts a bootstrapping statistical technique peruccacci et al 2012 and a frequentist method brunetti et al 2010 by sampling the weighted rainfall conditions that have triggered landslides the general form of the threshold curves is a power law 3 e   d   where e is the cumulated event rainfall in mm d is the rainfall event duration in h  is a scaling constant the intercept  is the shape parameter that defines the slope of the power law curve and  and  represent the uncertainties of  and  respectively in eq 3  and  are the mean values of the parameters resulting from the calculation of thresholds of 5000 synthetic series generated by the algorithm  and  are the standard deviation of  and  respectively each synthetic series contains the same number n of landslides as the original data set but selected randomly with replacement bootstrap technique to calculate the thresholds a single d l e l pair is associated to each landslide for each landslide in the individual synthetic series the algorithm samples randomly with a probability w a single rainfall condition from the mrc data set the extracted d l e l pairs of the n landslide are used to define the thresholds the algorithm also uses the rainfall conditions with the maximum w to define thresholds for the mprc data set the output of the bootstrapping technique consists of 5000 synthetic series of m d l e l pairs analysis of the m synthetic series allows calculating the mean value and the uncertainty associated with the threshold parameters  and  and their respective uncertainties   3 study area and data we tested the tool using rainfall and landslide data available to us for the liguria region 5410 km2 in northwestern italy we used hourly rainfall measurements collected in the 15 year period from march 2001 to december 2014 by a network of 172 rain gauges operated by the osservatorio meteo idrologico della regione liguria omirl fig 3 portrays the geographical distribution of the rain gauges yellow triangles having an average density of about one rain gauge every 31 km2 moreover we exploited geographical and temporal information on 561 rainfall induced landslides occurred in liguria between october 2004 and november 2014 red dots in fig 3 among those there are 73 rock falls 16 mudflows 71 other types of landslides e g earth flows debris blows etc and 401 unspecified shallow landslides on line chronicles and reports of local fire brigades were the exclusively information source for 337 60 and 51 9 landslides respectively the remaining 173 events 31 were reconstructed using information from both sources generally these last events have both a high geographical and temporal accuracy 4 computation of the regional parameters in a previous version of the algorithm applied in the sicily region southern italy melillo et al 2015 used a heuristic approach to separate two consecutive rainfall events they selected a minimum dry i e with no rain interval of 2 days 48 h in the warm period c w april october and a minimum of 4 days 96 h in the cold period c c november march in the same paper the time required for the soil to dry out was assumed inversely proportional to the amount of real evapotranspiration ret and they found a ratio r r e t c w r e t c c 2 however depending on the local seasonal and climatic conditions the length number of months of the c w and c c periods may vary and may lead r to vary accordingly thorp 1986 for this reason among several methods proposed for evaluating the evapotranspiration alley 1984 yates 1996 naoum and tsanis 2003 guo et al 2016 pumo et al 2017 we calculated the r value adopting the monthly soil water balance mswb model thornthwaite 1948 thornthwaite and mather 1957 melillo 2009 di matteo et al 2011 dragoni et al 2015 and we determined the length in months of the c w and c c periods the mswb model is a simplified approach with only few input requirements and is commonly used to obtain reliable estimates of the water balance components e g potential and real evapotranspiration input data of the mswb model are i the latitude ii the average monthly rainfall e m iii the average monthly temperature t m and iv the maximum field capacity or soil water storage sws we calculated e m using data available from the network of 172 rain gauges operated by the omirl in the liguria region see section 3 and t m from data provided by the website of istituto superiore per la protezione e la ricerca ambientale www scia isprambiente it in the period from january 2000 to december 2014 fig 4a shows a graph bagnouls gaussen diagram with the average monthly rainfall grey histogram and the average monthly temperature red line in the liguria region overall the spatial variability of the temperature is about 1 6 c the month with the highest variability is december standard deviation  equal 2 0 c whereas june is the month with the lowest one  1 4 c the spatial variability of the precipitation is 30 3 mm july and november are the months with the lowest  13 6 mm and the highest  51 4 mm spatial variability respectively the sws values were calculated according to the following equation nyvall 2002 4 sws rd awsc where rd is the crop rooting depth m varying from shallow 0 45 m for leaf vegetables to deep 1 20 m for tree fruits and awsc is the available water storage capacity of the soil mm m which ranges from 83 mm m sand textural class to 208 mm m silt textural class since rd and awsc are not available for the entire study area we calculated all the possible sws values table 1 with equation 4 using the mswb model we estimated the average monthly potential evapotranspiration p e t m green curve in fig 4 b and the average monthly real evapotranspiration r e t m corresponding to each sws value the yellow shaded area in fig 4b represents the range of variability of r e t m obtained varying sws the orange curve in the figure is the mean value of r e t m as expected r e t m decreases in the warm period when the rainfall grey histogram in fig 4a decreases we found the c w and c c periods using the average monthly aridity index a i m barrow 1992 5 a i m e m p e t m specifically c w is the period when the soil exhibits a water deficit and is mostly dry e m less than p e t m and a i m 1 and is from may to september red portion of the histogram in fig 4c conversely the c c period is from october to april when a i m 1 green portion of the histogram in fig 4c finally we obtained r 2 4 0 3 calculating the ratio of the total amount of r e t m between may and september c w period and between october and april c c period the uncertainty of ret ratio derives from the variability of r e t m yellow shaded area in fig 4b as a result in liguria r ranges broadly from a value between 2 and 3 which is close to the heuristic value r 2 adopted by melillo et al 2015 in sicily in the following analysis we reported the reconstructed rainfall events rainfall conditions responsible for the failures and the relative rainfall thresholds assuming r 2 which corresponds to a minimum dry interval of 48 h and 96 h in the c w and in the c c period respectively then we assumed r 3 which corresponds to a minimum dry interval of 48 h in the c w and 144 h and in the c c period and we compared the distributions of the rainfall events and of the rainfall conditions and the obtained thresholds 5 results adopting ctrl t and using information presented in the previous sections rainfall events and empirical rainfall thresholds at several exceedance probabilities were determined the empirical cumulative distribution function ecdf of the duration d and of the cumulated rainfall e are shown in fig 5 for the re green curves mrc purple curves and mprc data set orange curves obtained by ctrl t assuming r 2 the corresponding statistics are reported in table 2 for each data set the table contain the total number the minimum and the maximum values and the first second and third quartiles q1 q2 and q3 of the reconstructed rainfall events re and rainfall conditions mrc and mprc fig 5a reveals that more than 70 of the reconstructed rainfall events re green curve have a duration d 100 h and 25 have d 6 h table 2 the shortest values of d are observed in the mprc data set orange curve fig 5b shows the ecdf for the re mrc and mprc data sets in particular the median of the cumulated rainfall e for re is lower 20 than that for mrc and mprc table 2 whereas the duration d is comparable for the three data sets as a consequence the triggering conditions in the mrc and mprc data sets exhibit generally a higher mean rainfall intensity i e d fig 5 c representing the most severe conditions reconstructed for the single landslide the same statistics indicates that the duration d of the mrc are generally longer than that of mprc while the cumulated rainfall e is substantially the same table 2 the delay between the rainfall ending time and the landslide occurrence time is null 0 h for the majority of mprc 286 the delay is 3 h for 69 mprc and is between 4 and 48 h for the remaining 85 mprc using the mrc and mprc data sets and adopting the method proposed by brunetti et al 2010 and peruccacci et al 2012 we determined objective cumulated event rainfall rainfall duration ed thresholds and their associated uncertainties for the study area fig 6 fig 6a shows in log log coordinates the 801 d l e l pairs relative to the mrc data set purple dots together with the ed frequentist threshold at 5 exceedance probability t5 mrc r2 in purple see equation in table 3 and its associated uncertainty shaded area analogously fig 6b shows the 440 d l e l pairs for the mprc data set orange dots the 5 ed frequentist threshold t5 mprc r2 in orange see equation in table 3 and the relative uncertainty shaded area fig 6c shows the same thresholds in linear coordinates and in the range 1 d 120 h we note that t5 mrc r2 is higher than t5 mprc r2 for durations d 10 h conversely for longer durations t5 mprc r2 is getting higher due to the greater angular coefficient the mean value of the difference among the cumulated rainfall e for the two thresholds is 7 mm 12 for 1 d 300 h in particular the e differences varies from values below 2 mm 4 for d 24 h to values that do not exceed the 12 mm 16 for d 300 h to evaluate how r affects the reconstruction of the rainfall events rainfall conditions and the thresholds we conducted a sensitivity analysis assuming r 2 and r 3 solid and dashed curves respectively in the subsequent figures for the purpose we compared the ecdf of d and e for re mrc mprc fig 7 in addition we investigated the differences between the 5 thresholds for mrc and mprc fig 8 below q2 i e d 30 h we found small differences 3 between the ecdf curves of d for the re data sets green curves in fig 7a the differences slowly increase up to 25 at q3 d 100 h analogously in terms of e the differences are between 5 and 10 from q2 to q3 in the range 20 e 65 mm green curves in fig 7b for mrc the percentage difference in the ecdf of d is always 12 up to q3 purple curves in fig 7a while for e the ecdf differences do not exceed 5 purple curves in fig 7b comparing the ecdf curves for the mprc we noted that they overlap for both d and e orange curves in fig 7a and b log log plot in fig 8 shows the 5 ed thresholds calculated using a mrc data set for r 2 and r 3 the obtained thresholds are substantially similar with differences in cumulated rainfall lower than 1 mm 1 at d 24 h the maximum difference is 6 mm 7 for the longest duration d 517 h the 5 thresholds for mprc are coincident orange curves in fig 8 6 discussion 6 1 algorithm improvements ctrl t aims at reducing the computational time of the single steps from the rainfall event reconstruction to the threshold definition the actions performed by ctrl t prove robust and the implementation software do not suffer from computer storage or processing limitations more in details the algorithm improvements reduce the calculation time significantly in the i selection of the available rain gauges ii identification of the representative rain gauge and iii reconstruction and analysis of the rainfall conditions responsible for the failure as for the first two points the long manual procedure to associate a failure to the closest rain gauge is now overcome by the automatic selection of all the available stations within a given distance r b from the landslide r b is parameterized and can vary as a function of the density of the rain gauges ctrl t identifies automatically the representative rain gauge through a weight which is a function of both geographical distance between landslide and rain gauge and hydrological cumulated rainfall and duration of the rainfall event characteristics associated to the selected stations with the proposed weight the closest rain gauges are selected in almost 80 of the cases further improvements might be addressed to the weight evaluation by including orographic and geomorphological parameters such as the elevation difference between landslide and rain gauge the procedure implemented in ctrl t allows running the algorithm also when the rainfall information is available as gridded rainfall maps e g obtained through satellite data in this case each pixel of the map e g 25 km 25 km is treated as a virtual rain gauge otherwise the manual reconstruction of rainfall conditions using gridded rainfall maps would be time consuming regarding the third point the identification and selection of the rainfall conditions responsible for the landslide was previously left to the investigator and it was therefore subjective and time consuming the introduction of the automatic and reproducible selection of rainfall conditions based on weights solves this drawback and tries to face one of the main sources of uncertainties in the threshold definition process an additional improvement is obtained introducing the decay factor k in the previous version all the d l e l pairs were considered equally probable for the activation of the landslide however conditions characterized by long durations and low mean intensity are unlikely to trigger a shallow landslide the decay factor k damps the cumulated rainfall removing those rainfall condition with a small 10 rainfall increase compared to the previous condition in the new version of the algorithm we add an action to calculate thresholds and their uncertainties adopting a frequentist method brunetti et al 2010 peruccacci et al 2012 the procedure that begins from the reconstruction of the rainfall events and ends with the definition of thresholds is now faster than before and completely automatized thus objective and reproducible the structure of the algorithm also allows the implementation and or integration of other threshold calculation methods further details regarding ctrl t e g the required input files and the generated output files are available in appendix a 6 2 parameter settings as described in melillo et al 2015 in the previous version of the algorithm the same values of the parameters were used to reconstruct the rainfall events from rainfall data recorded in all available rain gauges this aspect represents a great limitation given that the rain gauges located in wide areas might record differ rainfall regimes in order to solve this problem ctrl t performs the reconstruction of the rainfall event using an input file see appendix a containing the values of the selectable parameters which can be adjusted for each rain gauge as stated in section 4 we adopted the average monthly aridity index to establish the length of cold and warm periods then we used the real evapotranspiration in the two periods to set the length of the two dry intervals c w and c c the calculation of these parameters is fundamental when using the algorithm in different seasonal and climatic contexts and with various soil types the procedure to determine c w and c c is still not automatized 6 3 rainfall events and thresholds as reported in the previous section the statistics of ecdf curves for re mrc and mprc data sets fig 5 suggest that the triggering conditions selected in the mrc or mprc data sets have a rainfall mean intensity i e d higher than that of re generally to calculate the thresholds it is worth to use the most severe mrc or mprc conditions instead of the whole rainfall event re in order to avoid an underestimation of the rainfall responsible for the landslide note that only in few cases 1 the d e pairs for mrc mprc and re coincide fig 6c and the equations of the two thresholds obtained for the mrc and mprc data sets t5 mrc r2 and t5 mprc r2 in table 3 reveal that the thresholds are statistically indistinguishable if considering the uncertainties associated with the curves fig 6c since the number of rainfall conditions used to determine the thresholds is largely above 75 which is the minimum number to obtain stable  and  parameters peruccacci et al 2017 we acknowledge that the two curves are very similar despite the rather large uncertainties 6 4 sensitivity analysis of the r value the two different values of the r parameter affect the ecdf curves of the reconstructed rainfall events solid and dashed green curves in fig 7 while they are comparable in terms of cumulated rainfall e they are dissimilar regarding the rainfall duration d as the value of the minimum dry period in the cold season c c increases from 96 h r 2 to 144 h r 3 the duration of the rainfall events increases and the total number of re decreases a higher value for the minimum dry period needed to separate two subsequent rainfall events makes more difficult the separation of events consequently the length of the rainfall event increases the duration of the mrc is slightly longer when using r 3 instead of r 2 while the distribution of the cumulated rainfall remains unvaried purple curves in fig 7 the ecdf curves for mprc are overlapping orange curves in fig 7 because the mprc selected by ctrl t in the two cases r 2 and r 3 are coincident this occurs if for each landslide the rainfall events reconstructed using r 2 and r 3 are the same or if the rainfall events for r 2 are included in those reconstructed for r 3 and the additional mrc related to r 3 are not able to vary the selection of mprc however differences are not so evident given that in the cold period c c november march 72 of the rainfall events reconstructed using r 2 and r 3 coincide this means that these events are separated by at least a minimum dry period of 144 h inspection of fig 8 reveals that the four thresholds obtained by using the mrc and mprc data sets and the two r values do not differ much in the study area in particular thresholds for r 2 t 5 m r c r 2 and r 3 t 5 m r c r 3 are completely overlapping while the four thresholds considering the uncertainties associated with them table 3 can be considered statistically indistinguishable therefore the uncertainty introduced using different parameters e g parameters obtained by the proposed ret analysis is negligible and substantially does not affect the threshold calculation 6 5 threshold comparison we compared the results obtained providing as input the same landslide and rainfall data sets to the old version of the algorithm melillo et al 2016 and to the release proposed in this work fig 9 shows the threshold t 5 m r c r 2 obtained using the previous version of the algorithm and the thresholds t 5 m r c r 2 and t 5 m r c r 2 obtained using the new release of the algorithm and considering k 1 and k 0 84 respectively see table 3 for the threshold parameters in particular the t 5 m r c r 2 threshold has the same slope of the t 5 m r c r 2 and a higher intercept the t 5 m r c r 2 threshold has a different slope and a range of duration shorter than the others given that the use of the k decay factor decreases the durations of the rainfall conditions responsible for the failures moreover using the k decay factor the rainfall amount necessary to initiate landslides is lower for duration longer than 10 h see t 5 m r c r 2 and t 5 m r c r 2 in fig 9 the introduction of weights for the rainfall conditions and of a decay factor for the cumulated rainfall made the thresholds more similar to those defined by an expert investigator in fact the earlier version of the algorithm returned thresholds lower and steeper than those determined manually melillo et al 2016 we maintain that ctrl t reproduces better the reconstruction of the rainfall condition responsible for landslide triggering performed by an expert investigator 7 concluding remarks the definition of reliable and accurate rainfall thresholds poses several critical issues and sources of subjectivity the use of a standardized and automatized procedure for the reconstruction of the rainfall conditions responsible for failures and for threshold calculation is necessary for enhancing the objectivity and reproducibility of the thresholds especially for thresholds to be used in landslide early warning systems in this work we faced this problem and updated the algorithm proposed by melillo et al 2015 2016 adding new features and ameliorations ctrl t exploits continuous rainfall measurements and landslide information to 1 reconstruct rainfall events 2 select automatically the representative rain gauges 3 identify multiple rainfall conditions responsible for the failure in terms of d and e modelling the cumulated event rainfall 4 attribute a probability to each rainfall condition and 5 calculate probabilistic rainfall thresholds and their associated uncertainties the main innovations novelties concern i the automatic selection of the representative rain gauge that is chosen by analyzing the distance between the rain gauge and the landslide and the characteristics of the rainfall events recorded by the station and ii the assignment of a probability to the single or multiple rainfall conditions responsible for the landslide in addition ctrl t incorporates the procedures to calculate objective and reproducible thresholds developed by brunetti et al 2010 and peruccacci et al 2012 we maintain that ctrl t enhances the definition of empirical rainfall thresholds for landslide forecasting in particular the use of the algorithm standardizes and accelerates considerably the slow and tedious process of the reconstruction of the rainfall events and reduces the subjectivity inherent in the manual treatment of the rainfall and landslide data this decreases the uncertainty associated with the definition of the rainfall conditions responsible for landslides and allows the possibility of a periodic and reproducible update of the thresholds definitely this has a positive impact on the application of thresholds in early warning systems for operational landslide forecasting acknowledgements this work was financially supported by the italian national department for civil protection dpc accordi di collaborazione 2014 2015 2016 that also provided access to the rainfall database we are grateful to ivan marchesini and massimiliano alvioli cnr irpi who contributed to the calculation of the mean monthly temperatures used in this work massimo melillo was supported by a grant from the italian national department for civil protection accordi di collaborazione 2015 2016 stefano luigi gariano was supported by a grant from nerc in the framework of the landslip project grant number nerc dfid ne p000681 1 anna roccati was supported by a grant from the environment department of the liguria region accordi di collaborazione 2013 2014 we thank the editor and four anonymous reviewers for their criticisms and comments that have helped us to improve the paper appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 024 appendix a software and data we implemented ctrl t calculation of thresholds for rainfall induced landslides tool using the r open source software for advanced statistical computing and graphics release 3 3 3 http www r project org ctrl t was finalized in june 2017 and should work on computers with at least 4 gb of ram the code can be downloaded free of charge at the following website http geomorphology irpi cnr it tools rainfall events and landslides thresholds ctrl algorithm ctrl code ctrl code r the downloadable executable file is 79 kb in addition an example of the input files required by the algorithm can be downloaded free of charge at the following website http geomorphology irpi cnr it tools rainfall events and landslides thresholds ctrl algorithm input demo input zip the downloadable file is 37 55 mb the algorithm is divided into three main blocks fig 1 which perform i the reconstruction of rainfall events ii reconstruction of the rainfall conditions responsible for the landslide and iii the definition of the rainfall thresholds respectively input files here we describe the information provided by the input text files boxes labelled with input in the left upper part of the logical scheme in fig 1 one input file table of raingauges csv contains information on the rain gauges cod area longitude latitude pk sensor id rain gauge and on the parameters used by the algorithm to reconstruct the rainfall events gs p1 c p1 w p2 c p2 w p3 p4 c p4 w sws ews as described in melillo et al 2015 in particular cod area is an identification code of the administrative region where the sensor is located longitude and latitude are the coordinates expressed in decimal degrees and id rain gauge is a unique numeric code derived from the rain gauge geographic coordinates a series of input files named timeseries sensor id rain gauge csv contain the time stamp and the hourly rainfall measurements of each sensor an additional input file named landslides csv contains information on the documented landslides in particular each record reports the i id project which identifies the rainfall event ii id lan which gives the temporal order of the failures associated with the single rainfall event i e a is the 1st landslide b the 2nd etc iii class number which indicates the landslide multiplicity iv class type which is the failure type v longitude and latitude of the landslide vi geo acc which is the level of mapping accuracy peruccacci et al 2012 vii failure occurrence date day month year and time and viii date acc which is the accuracy of the occurrence date t1 t2 and t3 t1 includes landslides for which the exact time of occurrence is known t2 and t3 are used when the part of the day or the day of occurrence are known respectively output files the three blocks of the algorithm boxes in the right part of the logical scheme in fig 1 produce different output files the first block performs the reconstruction of the rainfall events and generates the folder reconstructed rainfall events which contains an output file named rainfall events csv listing the reconstructed rainfall events in particular for each record id rain gauge is the rainfall station code index pos1 and index pos2 are indexes related to the rainfall series of each rain gauge re start date and re end date are the starting and the ending date of each rainfall event the remaining fields are the rainfall duration d e the cumulated rainfall e e the rainfall mean intensity i e the maximum hourly rainfall ip e the maximum cumulated rainfall in 24 h emax24 e and a class a class from 1 to 6 related to the rainfall event classification of alpert et al 2002 after the reconstruction of rainfall events the algorithm calculates the maximum hourly rainfall and the maximum cumulated rainfall in 24 h this allows to check and possibly remove anomalous rainfall events the second block reconstructs the multiple conditions mrc likely responsible for the failures and generates the folder reconstructed rainfall conditions which contains three outputs the first output is the folder individual files containing a single file for each landslide named landslide id project id lan pdf this file contains for each rain gauge included in a buffer centered in the landslide location see section 2 1 the i information about the descriptive statistics of the rain gauge fig 10 a ii distribution of the reconstructed d e e e rainfall pairs in the logarithmic plane fig 10b iii hourly rainfall measurements of the reconstructed rainfall event recorded by the representative rain gauge fig 10c iv multiple d l e l rainfall conditions responsible for the landslide in the logarithmic plane fig 10d the second output is a text file named processing summary report txt this file contains the statistics about the total number of the analyzed reconstructed and discarded landslides we produce a list of the discarded landslides labelled with the id project attribute the third output is a file named mrc csv in this file id project identifies the rainfall event with landslide s id lan lists chronologically the landslides triggered by the same rainfall event date is the failure occurrence time rrg select is the rank of the representative rain gauge based on the distance from the landslide within the circular buffer and rrg distance is the distance between the representative rain gauge and the landslide locations the file lists also information on the rainfall duration d l the cumulated event rainfall e l the rainfall mean intensity i l the rain gauge code id rain gauge the number of rainfall events ren associated with the id rain gauge the maximum hourly rainfall ip l the maximum cumulated rainfall in 24 h emax24 e a class a class from 1 to 6 according to alpert et al 2002 rainfall event classification a binary value ms flag that indicates if the currently rainfall condition have a maximum value of the score w n mrc 1 and the number of multiple rainfall conditions n mrc the third block defines the rainfall thresholds using mrc and mprc data sets see section 2 and generates the folder rainfall thresholds which contains four outputs representing the results of the analysis for each subset the algorithm generates two distinct files the first files boot mrc csv or boot mprc csv contain information on the parameters  and  that define 16 threshold curves at different exceedance probabilities from 0 005 to 50 for each threshold six fields are specified i e the name of the threshold parameters variable the corresponding exceedance probabilities probability the mean value mean obtained through the bootstrap process peruccacci et al 2012 the standard deviation value sigma and the extreme values min max the second files boot mrc pdf or boot mprc pdf contain in the first page the distribution of the d l e l pairs in log log coordinates purple dots in fig 11 a the empirical cumulative distribution functions ecdf of d fig 11b e fig 11c and rrg distance fig 11d moreover the marginal distribution of the representative rain gauge ranking based on the distances between all rain gauges within the circular buffer and the landslide rrg select and of the number of multiple rainfall conditions n mrc are also reported grey and purple bars in fig 11e respectively here we report as example the file related to the mrc data set the one related to the mprc data set is analogous in the subsequent 16 pages are reported the graphs related to i the thresholds at exceedance probabilities from 0 005 to 50 in linear and log log coordinates fig 12 a b respectively and the variation of the values of the  parameter and the related uncertainty  fig 12c d respectively 
26387,many environmental data sets are driven by multiple superimposed periods yet most time series analysis software packages only support single seasonality the objective of this research was to develop a software toolkit utilizing multi seasonal autoregressive integrated msari models a toolkit in matlab was developed for msari based identification estimation forecasting and visualization in the toolkit an adaptive forecasting routine uses a continual event loop for real time data acquisition and parameter re estimation a statistical quality control algorithm monitors model performance and re estimates parameters when necessary a set of visualization tools provide animated graphical representations of forecasts prediction intervals and key performance metrics the toolkit was applied to three case studies electricity demands water demands and sewer flows the analysis of the results demonstrated that the explicit modeling of multi seasonality improved model predictions therefore the msari software presents a promising tool for modeling and predicting real time data series keywords seasonality autocorrelation time series forecasting visualization software and data availability msari toolkit name of software msari toolkit developers jinduan chen and dominic l boccelli contact address 737 engineering research center university of cincinnati cincinnati oh 45221 0012 usa telephone 513 923 0706 e mail jinduan uc gmail com year first available 2014 hardware required pc software required matlab 7 12 0 r2011a or up license gnu general public license version 2 git repository https bitbucket org jinduan tsff programming language matlab program size 1 mb data set 1 name of data hourly ontario and market demands 2002 2013 year first available 2014 form of repository csv file download webpage http www ieso ca pages power data data directory aspx size 2 56 mb data set 2 name of data hourly inflow outflow data of production and storage facilities of the south central water distribution network in hillsborough county fl apr 2012 to dec 2012 year first available 2013 form of repository matlab mat file availability https data mendeley com datasets 4yhprsgjrf 1 size 52 kb data set 3 name of data hourly sewer flows monitored at station s2 in columbus oh jun 1998 to dec 2013 year first available 2014 form of repository matlab mat file availability https data mendeley com datasets 4yhprsgjrf 1 size 371 kb 1 background and objectives many environmental processes are observed in discrete time intervals these time series can be described and predicted by statistical models when physical models are unavailable or too complex to apply traditionally the techniques of time series analysis have been applied to characterize the dynamics of air pollution robeson and steyn 1990 rainfall burlando et al 1993 lake water levels irvine and eberhardt 1992 river flow kachroo and liang 1992 urban water consumption jowitt and xu 1992 salmon production hare and francis 1995 fishery landings koutroumanidis et al 2006 and many others with the recent advances in peripheral sensor and communication technologies real time data services have become more prevalent in environmental applications e g allen et al 2011 however new challenges have been presented to the software tools designed for real time time series analysis first the tools have to adopt high data transfer rates therefore fast algorithms for parameter estimation and forecasting are required second the tools need to run consistently for a long time without human intervention which necessitates self adaptive model structures and or parameter re estimation methods see e g chen et al 1995 third dynamic visualization tools would benefit the perception of forecasting results and performance metrics in addition to these real time software considerations the fundamental challenge associated with time series modeling remains the identification of the appropriate model structure while there are many approaches to time series modeling such as box jenkins time series box and jenkins 1976 wei 2006 artificial neural networks ann hill et al 1996 and support vector machines svm mller et al 1997 our previous studies chen and boccelli 2014 chen 2015 showed that a variant of the classical box jenkins time series named the multi seasonal auto regressive integrated msari model is promising for use within a real time framework for a selection of case studies the empirical auto correlation function acf and partial auto correlation function pacf diagrams showed much stronger characteristics of autoregressive ar processes than moving average ma processes the msari models have also been shown to deliver competitive accuracy compared with more complex models adhikari and agrawal 2013 the relatively simple model structure and reduced number of parameters yield efficient parameter estimation and forecasting algorithms also benefiting from the model structure parameter adaptation algorithms can be implemented in an efficient manner however existing software packages do not have the facilities to specify and utilize the multi seasonal structures and there are few efforts focused on automatic parameter re estimation the objective of the present research is to develop a real time oriented software toolkit with a suite of utilities tailored for msari models at the core of the toolkit is an adaptive forecasting routine by applying the tools on various data series the power and flexibility of the real time analysis tools are demonstrated in this article the second section of the paper reviews the previous work on modeling theory software development and visualization techniques the third section introduces the representation of the multi seasonal structure and algorithms developed around the structure the fourth section discusses the architecture of the toolkit the design of the application programming interface api and the real time visualization tools in the fifth section three case studies are examined the final section summarizes the findings originated from the research and proposes future directions of the study 2 related work 2 1 time series models both linear and non linear models have been suggested to model environmental time series the most popular methods are autoregressive integrated moving average arima models for linear models and ann kaastra and boyd 1996 and svm sapankevych and sankar 2009 for non linear models more recently tiwari and adamowski 2013 proposed a hybrid wavelet bootstrap neural network wbnn model for short term 1 day 2 months urban water demand forecasting the model is presented as an ensemble of several anns to improve forecasts and characterize uncertainties bennett et al 2014 utilized autoregressive integrated moving average with exogenous variables arimax and ann techniques to predict energy use in low voltage distribution networks and suggested a hybrid approach to improve model performance sehgal et al 2014 used wavelet bootstrap multiple linear regression wbmlr to forecast daily river discharges and observed better performance than multiple linear regression mlr and ann models overall the relative predictive power of the linear arima models compared with non linear ann and svm models vary from case to case and the performance for a particular type of model still relies heavily on the empirical selection of the model structure sfetsos 2000 ho et al 2002 wang et al 2009 adhikari and agrawal 2013 this study will focus on the extension and application of traditional time series models because of the compatibility with explicit periodic model structures and adaptive parameter re estimation methods the linear auto correlations in equally intervalled univariate time series were investigated systematically in the fundamental work of box and jenkins 1976 this work introduced a group of linear gaussian models also known as arima models moreover a variant of the box jenkins time series model the seasonal arima sarima model was proposed to characterize periodic behaviors existing in many environmental and socio economic phenomena the single seasonal model was originally proposed to predict air traffic over the years the formulation has been adopted in other applications such as representing monthly lake water levels irvine and eberhardt 1992 monthly river flow kachroo and liang 1992 and hourly road traffic flow williams et al 1998 a multi seasonal arima model has two or more periods incorporated into the model formulation for example caiado 2009 studied the daily urban water demand based on a double seasonal arima model that accounted for weekly and annual relationships in water demands 2 2 time series software three existing software tools for building arima models are investigated in this research the tools include two popular scientific statistical computing platforms r and matlab and one special purpose software package gretl the r packages stats and forecast hyndman et al 2014 include many utility functions for conducting static arima analysis for example functions acf and pacf compute and plot the sample autocorrelation function acf and sample partial autocorrelation function pacf for model identification function arima accepts a data series and estimates the parameters for a non seasonal or single seasonal arima model initial boundary conditions and missing values are handled exactly via a state space representation of the arima process gardner et al 1980 function auto arima searches for the best fitted model structure with a specified information criterion dziak et al 2012 both arima and auto arima return arima objects containing model parameters function predict arima uses an arima object to compute n step ahead forecasts the r package qcc implements the spc methods of shewart quality control charts shewhart 1931 cumulative sum control chart cusum page 1963 and exponentially weighted moving average ewma chart roberts 1959 the tools are designed to detect significant deviations of a quality indicator of the target value and modify the parameters if necessary the same change detection techniques can be potentially used in the series of forecasting residuals to determine the timings of model re estimation however there are no functions available to integrate the quality control methods with the arima forecasting methods in matlab the existing utilities for time series analysis mainly reside in the econometrics toolbox functions autocorr and parcorr calculate and plot the sample autocorrelations and sample partial autocorrelations the arima class is developed to hold the structure and parameters of an arima model the class methods of arima estimate and arima forecast are implemented for parameter estimation and forecasting respectively however the existing toolboxes support only single seasonality in the model specification and the forecasting function is not designed for real time applications with continuous data supply the gnu regression econometrics and time series library gretl is an open source software package for econometrics research cottrell and lucchetti 2014 baiocchi and distaso 2003 the package includes a command line program a graphical user interface and a numerical library with apis written in c gretl provides a set of concise commands for data import export data analysis and graphing the gretl command arma is used for estimating arima parameters using a kalman filter along with the bfgs maximization nocedal and wright 2006 the command fcast produces forecasts gretl also allows connections to a database and retrieving time series data via the open database connectivity odbc standard however the data link can only be accessed manually under the batch mode and there is no support for real time data acquisition and dynamic visualization 2 3 visualization techniques the visualization techniques of time series data have drawn many research interests in box and jenkins 1976 three charts are introduced for assisting the modeling of serial observations 1 the correlograms which include the autocorrelation function acf and partial autocorrelation function pacf diagrams are plots that show the sample autocorrelations and partial autocorrelations versus time lags the correlograms provide visual clues during the identification of arima models 2 the unit root plot shows the real or complex roots of the characteristic polynomial of an arima model and their relative distance to the unit circle this plot is useful in visualizing and detecting the non stationarity of a model 3 the forecast plot shows the observations versus elapsed time in a x y plot the plot also includes multi step ahead forecasts and two lines showing the 95 and 50 prediction intervals for the forecasts modern computer hardware and software has allowed more information about time series to be visually conveyed through the use of color 3 dimensional geometries animations and interactive graphs cleveland 1993 introduced the cycle plot for seasonal time series in which every point in a regular time series plot is replaced with a small subplot showing the inter seasonal trends saito et al 2005 used a two tone pseudo color approach to visualize large multivariate meteorological data sets in a compact fashion tominski et al 2005 used a circular axis to represent seasonal time series for epidemic data stressing the cyclic repetitive nature van wijk and van selow 1999 suggested using a parallel 3d bar graph to visualize time dependent electrical demand data where the heights of the bars represents the values of the observations and the bars are aligned with the time of day and date with this design the seasonal variations were easy to inspect because of the extra dimension the same idea was adopted by wicklin 2011 in the creation of a tile map in which the relative values of the observations were represented in greyscale however despite the many methods for visualizing time series and predictions there have been few research efforts on the visualization of forecasting errors in real time applications the visualization of errors are valuable for monitoring model performance and diagnosing problems from the review of the technical state of the art three features absent in the existing software implementations are identified the support of multi seasonality in the time series models the coupled parameter re estimation and forecasting routine to prevent performance deterioration and animated visualization of predictive errors the present research addresses these three problems in a newly developed time series modeling forecasting toolkit implemented in the matlab programming language 3 methodology 3 1 model representation and parameter estimation the multi seasonal ari models for time series are formally expressed as 1 i 0 n  i b s i s i d i x t a t in which x t is the time series of observations n is the number of seasonal periodicities b is the backward operator defined as b x t x t 1 and treated as a variable applicable for linear transformations s i is the length of the i th period s 0 1 to represent regular non seasonal auto regressions p i and d i are structural parameters that indicate the number of autoregressive ar parameters and the number of differencing levels for the i th period s i d i is the ith seasonal differencing integration operator defined as s i d i 1 b s i d i  i b s i is the ith ar characteristic polynomial defined as  i b s i 1  i 1 b s i  i 2 b 2 s i  i p i b p i s i in which  i j are ar parameters and a t is a series of i i d zero mean gaussian random variables with standard deviation  in msari models both the local auto correlations and the long term periodic behaviors are accounted for via explicit formulation when n 1 equation 1 represents a single seasonal ari ssari model with structural parameters p 0 d 0 p 1 d 1 s 1 and regression parameters  1 1  1 p i  0 1  0 p 0 when n 1 more layers of ar parameters will be included in the model for example for hourly observations the values of s 1 and s 2 can be naturally chosen as 24 and 168 denoting diurnal and weekly periods respectively fig 1 conceptually shows the differences between non seasonal single seasonal and double seasonal ari models the multiplicative parameters  are positioned above the blocks of random variables at the specific time steps the number in the parentheses in the subscript of  indicates the level of seasonality fig 1 a shows that only one historic observation x t 1 contributes to the expression of x t fig 1 b shows a daily seasonal ssari model looking back to time t 24 in predicting x t also x t 25 has non zero multipliers in the expression due to the multiplication of the characteristic polynomials fig 1 c shows that a double seasonal ari model incorporating both daily and weekly seasonality will refer further back into the history for example data received as early as t 193 will be used in the expression of x t even more characteristic polynomials can be added to represent monthly quarterly and annual periods the previously reviewed software packages use one 7 element tuple p d q s p d q to represent the single seasonal model structures the present study focuses on the pure ar models therefore the q parameters associated with the moving average ma component are not used to represent multiple superimposed seasonality the ar parameter structure is extended to a 3 n matrix the structural matrix h is defined as 2 h 1 s 1 s n d 0 d 1 d n p 0 p 1 p n the first second and third row of the matrix represent the lengths of the periods the orders of seasonal integration and the numbers of autoregressive parameters respectively each column represents a seasonal level an important number corresponding to the msari structure is the look back length l b defined as the oldest observation that will appear in the expression of x t from equation 2 the look back length can be conveniently computed as l b i 0 n s i p i d i if a model structure h is known the parameter estimation of the msari models will follow a minimum sum of squared errors mse approach similar to that described in box and jenkins 1976 the optimal values of the regressors  i j can be obtained by an iterative algorithm in which the squared sum of the white noise terms a t is minimized the identification procedure of matrix h is introduced below 3 2 model identification procedure in the present toolkit the identification of msari models i e selecting a proper structural matrix h involves a combination of computation visual inspection and modeler discretion the methodology is a multi seasonal extension to the original box jenkins approach of acf pacf diagram inspection this research developed utilities to assist in the identification of the appropriate structural matrix the first row in h comprises the lengths of the periods s i in practice the periods are usually assumed to be regular calendrical periods such as hours in a day day in a week etc the periods can also be taken from the original acf diagram as periodic spikes in autocorrelations can be observed for seasonal time series the second row in h comprises the orders of integration d i the definition of msari in equation 1 suggests a separation of the seasonal differencing operator i 0 n s i d i and the autoregressive operator i 0 n  i b s i the seasonally differenced series i 0 n s i d i x t is expected to be a stationary autoregressive model identifiable using the pacf diagram according to box and jenkins 1976 such ar models will have decaying spikes of partial autocorrelations located at periodic lag times and partial autocorrelations beyond the lag time i 0 n s i p i should approach zero a program developed during this study applied various choices of d i to the data series to generate pacf diagrams for further inspection on a trial and error basis the differencing operators with the smallest pacf residuals in large lag times were selected for further investigation while higher orders of d i may be helpful our experience suggests that setting d i 0 1 1 will generally result in good removal of pacf residuals and at a minimum is a good initial starting point the third row of h comprises the numbers of parameters on each seasonal level p i two metrics are considered in this study to evaluate the different distributions of parameters given a fixed total number of parameters the first metric is the akaike information criterion aic or bayesian information criterion bic see e g dziak et al 2012 aic is defined as 3 c a 2 l 2 p t and bic is defined as 4 c b 2 l ln n p t in both expressions l represents the maximum log likelihood achievable with the model structure p t i p i represents the total number of parameters and n represents the sample size models with smaller values of aic bic are considered superior to those with greater values these information metrics are designed to assess the trade off between the model accuracy and the parametric parsimony the second metric is the similarity in autocorrelation structures specifically any candidate msari model must correspond to an autocorrelation structure that can be computed with yule walker equations box and jenkins 1976 the euclidean distance in autocorrelations with varied lag times between the observed and synthetic series are then calculated to represent the similarity mathematically 5 s i 1 m c i c i 2 in which i is the lag time of the autocorrelation coefficients m is a large number for lag time beyond which the autocorrelations are considered to be negligible c i and c i are the autocorrelation coefficients for the observed and simulated series respectively the model structure that produces the minimum sum of squared errors in autocorrelations is considered the optimal model fig 2 shows the procedure used in this study to identify the parametric structure for the msari models the first and second steps identify the first and second rows of h by visually inspecting the data series and the correlograms for the original data and the filtered data the correlograms are plotted by a program developed in this study that can handle the multi seasonal structures for the third step i e large dashed box an identification program was developed to enumerate and evaluate each possible configuration of p 0 p 1 p n p i p t for a given configuration the corresponding matrix of h is constructed and the model is fitted to the observations using the standard minimum sum of squared error mse method the performance of any configuration can be evaluated using the metric of either aic bic or acf deviations as described above the configuration with the minimum aic bic or the minimum acf deviations are reported as the optimal choice for the third row of matrix h 3 3 adaptive forecasting if both model structure and parameters are known the least mean squared error forecasts of msari models can be recursively calculated through a formula similar to the one suggested by box and jenkins 1976 6 x  t l 1 i 0 n  i b s i s i d i x  t l l 0 x t l l 0 in which l is the time horizon of the forecast x  t l is the forecast of x t l produced at time t x t l is the observation at time t l the ari operator i 0 n  i b s i s i d i is applied on l instead of t the prediction intervals of the forecasts can be calculated as x  t l   2 v l in which   2 is the 1  2 percentile of the standard normal distribution n 0 1 and v l is a constant that only depends on the time horizon and the parameters of the model detailed derivation of the formula can be found in box and jenkins 1976 the present study also introduces a batch parameter re estimation method to detect changes and update parameters when necessary a moving window of previous observations are saved to re estimate the model parameters table 1 shows the differences between the traditional time series methodology and the proposed methodology the first row of the table shows that in the classical box jenkins method all four major modeling steps are conducted offline after all data are retrieved the second row shows that once an estimated model is supplied with a live data link the forecasting routine can produce predictions right after data retrieval the third row represents the real time forecasting combined with forecasting quality control using this methodology only identification is conducted off line while parameter re estimation forecasting and performance checking all happen in real time this set up is necessary for long running applications in which the underlying dynamics of the series may experience major changes and the model parameters should be monitored and re estimated to maintain the prediction power the quality control method employed in this toolkit is the generalized likelihood ratio glr algorithm basseville and nikiforov 1993 the glr algorithm is a generalized form of the cusum method that uses the ratios of likelihoods to detect significant parameter changes lai 1995 the glr algorithm is expressed as 7 t inf t sup  i t m 1 t log f  x i f 0 x i h in which t is the time of a model change  is a set of model parameters including both standard deviation  of the white noise and autoregressive parameters  i j f x i represents the likelihood of the observations using the error terms a t as i i d normally distributed white noise with variance  as defined in equation 1 for the original subscript 0 and alternate subscript  parameter sets m is the size of the sliding window and h is the pre determined switching threshold conceptually for a fixed msari model structure h the algorithm maintains two sets of parameters at each time step the current set parameters m 0 are carried over from the previous steps and a stand by set of parameters m 1 is estimated from a sliding window of previous observations using the mse approach the likelihoods within the sliding window also named glr scores are calculated for both m 0 and m 1 if the ratio of the two likelihoods exceeds the pre determined threshold the algorithm replaces the m 0 parameters with m 1 i e a model switch the benefits of using the glr algorithm include better forecasts for long running time series and better outputs of prediction intervals both resulting from the adaptive parameters 3 4 performance measures the real time forecasting routine produces both forecasts and probability limits in this study the following measures are utilized to evaluate the model performance 3 4 1 coefficient of determination r 2 the coefficient of determination r 2 represents the ratio of the variability explained by the model and total variance r 2 is defined as 8 r 2 1 i x  i x i 2 i x i x 2 in which x i and x  i are observations and predicted values forecasts and x is the mean value of the observations an r 2 close to 1 means the predictions are close to the actual observations and most of the variance has been accounted for by the model 3 4 2 average absolute relative error aare the average absolute relative error aare is another measure of the forecasting accuracy aare is defined as 9 a a r e i x  i x i x i aare can also be written in the form of percentages smaller aare values mean the predictions are close to observations 3 4 3 prediction interval coverage probability picp the prediction interval coverage probability picp measures the accuracy of the prediction intervals produced by a probabilistic forecasting model shrestha and solomatine 2006 picp is defined as 10 p i c p  i 1 n i w  i  x i u  i  n in which  is the confidence level used when generating the prediction intervals i is the indicator function that equals to 1 when the condition is true and 0 otherwise w   and u   are lower and upper bounds of the prediction interval and n is the size of the test data set a model that produces accurate prediction intervals should have p i c p  close to the selected  value 4 implementation 4 1 control panel fig 3 shows the graphical user interface gui of the control panel the modeling steps of identification estimation and forecasting can be executed from the panel the gui enables options and parameters that can be conveniently adjusted to facilitate the study of the methodology the top pane is used for loading the data file or data link of the data source the pane at the bottom left is used for specifying structural parameters of the model for up to three seasonal levels in addition two utilities can be invoked from the model pane to plot acf pacf correlograms and identify optimal model structures using the methods introduced in the previous section the pane at the bottom right is used for supplying options and parameters to the real time forecasting routine users can choose either fixed parameter static or adaptive parameter forecasting and specify the window size and or threshold of the glr algorithm users can also specify the desired forecasting horizons and visibility for the animated real time visualization 4 2 forecasting event loop fig 4 shows how components in the developed toolkit are integrated specifically a user can press the run stop button in the control panel to enter a forecasting loop with the desired structural parameter inside the loop msari parameters are first estimated by the function findssemin using the training data set the parameters are used to instantiate the class sariforecaster to create a forecasting model next the routine calls the data source to retrieve a new observation emudatasource getanobs then calls the forecaster to update the data buffer and compute forecasts sariforecaster update the current forecasts are sent to the viewer to update the visuals rtviewer update as the final step of an iteration if the parameter adaptation is enabled the differences in likelihood for the current and alternative parameter sets are compared with the preset threshold to determine if a model switch is necessary the selected forecasting model is employed in the next receiving forecasting displaying iteration the loop runs repeatedly until the data source sends an end signal or the user presses run stop button in the gui 4 3 visualization five graphing tools were developed in this study to illustrate important real time performance indicators and diagnostics including raw data forecasts hindcasts forecasting errors and model re estimation indicators for brevity only one graphing tool forecast hindcast view is described in this section detailed discussion about the other four visualization types are provided in the supplemental materials fig 5 shows the forecast hindcast view fhv an animated version of the classical box jenkins forecasting chart the three subplots in the fhv show the variable water demands in this example versus the elapsed time for raw data fig 5a current forecasts fig 5b and previous forecasts or hindcasts fig 5c the forecast hindcast views also include the 50 and 95 prediction intervals when animated the view shows new observations and changing forecasts and lines of prediction intervals the hindcast view is a practical tool for the modelers to review the accuracy of the forecasting model in real time the remainder of the article focuses on the performance of the msari models 5 results and discussions 5 1 hourly electricity demand in ontario canada the first case study of the present research is the electricity demand data set downloaded from the independent electricity system operator ieso public data directory this time series represents the total hourly demands of ontario canada from may 2002 to december 2013 table 2 shows the descriptive statistics of the data set the data set is separated into a training set first 20 of data and a test set last 80 of data the training set is used for identifying the msari model and estimating the initial set of parameters the test set is used for examining the online forecasting algorithm to which data points are supplied sequentially to simulate a real time scenario the forecasting algorithm ran in both static and adaptive modes in static mode the initial parameter estimates remained fixed throughout the simulation in adaptive mode the estimates were allowed to be adjusted dynamically using the glr algorithm described in section 3 3 to identify the structural matrix h the steps described in section 3 2 were followed fig 6 shows the acf and pacf diagrams for up to 250 h of lag time generated from the plotting tools in fig 6 a the initial series shows very strong periodic autocorrelations for lag times of multiples of 24 and the lag 168 autocorrelation coefficient is slightly higher than the adjacent peaks at lag 144 and lag 192 therefore the two periods of the msari models were set as 24 and 168 respectively the pacf diagram in fig 6 a contains many residuals if compared with a typical autoregressive model applying a single seasonal differencing operator 24 to the series yields the acf pacf diagrams shown in fig 6 b this action removed the periodic autocorrelations in the acf chart and the resulting pacf charts have clear decaying periodic spikes in fig 6 c the application of a double seasonal differencing operator 168 24 further reduced the pacf residuals enhancing the traits of the seasonal autoregressive model subsequently the distribution of autoregressive parameters on the diurnal and weekly seasonal levels were determined all possible combinations with less than 6 parameters including 5 autoregressive parameters plus the standard deviation of the white noise were evaluated in terms of aic bic and the acf similarity models with more than 6 parameters had limited improvement by the three metrics section 3 4 and were not considered further table 3 lists the structures and parameters of the different models for comparison purposes non seasonal and single seasonal models were also included in the analysis for the non seasonal model an extra parameter  was included to shift the average value of the series to be around zero to facilitate parameter estimation for seasonal models the parameter  was not needed due to the existence of seasonal differencing operators the identification results show that the seasonal models yielded significantly smaller values of aic bic and acf similarity than the non seasonal models comparing the single and double seasonal models the adoption of double seasonality further reduced the three indicators therefore in the identification step the double seasonal models were recommended with the same levels of seasonality the model structures identified with aic and bic criteria were identical the equivalence of aic and bic can be attributed to the domination of the log likelihood terms in equations 3 and 4 due to the large sample size the acf similarity criterion produced different optimal structures for example the double seasonal model structure that minimized aic was p 0 2 p 1 1 p 2 2 while the structure that minimized acf similarity was p 0 1 p 1 3 p 2 1 however despite the differences in structures and parameters the models minimizing aic bic and acf similarity yield similar spectra of autocorrelation structures and led to similar prediction power in this study the acf similarity metric was used in the following case studies because of the faster computation speed and better differentiation between the models to evaluate the model performance the non single and double seasonal models were used in the forecasting routine described in section 4 both static and adaptive parameter forecasting algorithms were tested the open source software gretl cottrell and lucchetti 2014 was also used for comparison although the capabilities were limited to single seasonal models and static lead 1 forecasting table 4 shows the various performance measures for the models tested the forecasting time horizons shown in the table were for one hour and 24 h representing short term and long term forecasts respectively the results show that the non seasonal model performed worse than seasonal models in terms of the listed measures the seasonal models were more appropriate in forecasting the electricity demand series especially for long term forecasting a single seasonal model generated lead 1 forecasts with 0 92 aare and lead 24 forecasts with 5 36 aare using a double seasonal model improved the results to 0 85 and 4 33 without adding any additional parameters the forecasting results matched the conclusion of the model identification that double seasonal models provide the best fit for the data in terms of the autocorrelation structures the adaptive double seasonal model slightly improved the forecasting results in terms of r 2 lead 24 aare and 50 picp the sliding window size and switching threshold were empirically selected as 300 and 1 for the glr algorithm the adaptive model resulted in tighter prediction intervals than the static counterpart since the standard deviation was adjusted throughout the simulation however the limited improvements in the forecasts seem to suggest that the underlying autocorrelation structure did not experience major changes within the time span of the test dataset in our testing environment we ran the data set on a computer with 2 5 ghz cpu 6 gb memory and matlab version 7 12 the average computational time for non adaptive and adaptive models are 2 93 ms and 605 3 ms respectively although adding the parameter re estimation significantly increased the time for hourly time series data both methods will generate forecasts virtually instantaneously the overall results shown in table 4 suggest that the regional hourly electricity demand can be very accurately modeled and predicted by a double seasonal ari model 5 2 hourly drinking water demand in hillsborough county fl the second case study investigated in this research was the hourly drinking water demands for a water distribution network in hillsborough county fl the series is computed by summing the outflows of all water production and storage facilities in the network inflows to the storage facilities are treated as negative outflows the original data were in 5 min intervals to be consistent with common hydraulic modeling practice the data were aggregated into 1 hour intervals by averaging the 12 values in each hour table 5 shows the descriptive statistics of the demand series compared to the electricity data the water flow measurements are prone to more device errors and maintenance outages see e g allen et al 2011 as a result the series has a significant amount of data missing 2 3 the performance metrics for the missing data points were excluded from the final performance reports following a similar procedure to the first case study the single seasonal model for the water demands was identified as h 1 24 0 1 3 2 with a look back length of 75 and the double seasonal model was identified as h 1 24 168 0 1 1 1 1 3 with a look back length of 721 table 6 shows the forecasting results associated with applying a similar set of models from the first case study likewise the results show that the double seasonal models generated more accurate forecasts than the non seasonal and single seasonal models the improvements in long term forecasts were more significant than short term forecasts according to the table the adaptive double seasonal model produced slightly better results in lead 1 aare and 50 picp than the static model however in other metrics the adaptive models had no significant advantages probably due to the relatively short time span of the test data set on average the 6 parameter double seasonal model was capable of generating one hour ahead forecasts with 6 7 relative errors and one day ahead forecasts with 11 13 relative errors although not as accurate as the electricity demands series the msari models still demonstrated the capacity of predicting future hourly water demand better than the non or single seasonal models 5 3 hourly sewer flow in columbus oh the third data series researched in this study was the hourly sewer flows monitored at a waste water treatment facility in columbus oh the data include 134017 data points collected in 15 years table 7 shows the descriptive statistics of the data set as the data set covers many years of sewer data in this case study a triple seasonal model was also developed in addition to the previously mentioned single and double seasonal models the identified msari models in terms of acf similarity are h 1 24 0 1 1 4 for single seasonal model h 1 24 168 0 1 1 1 2 2 for double seasonal models and h 1 24 168 8760 0 1 1 1 1 0 0 4 for triple seasonal models however the visual inspection of the acf and pacf diagrams showed that the autocorrelation spectra under the double and triple seasonal filters were similar suggesting that the addition of a third seasonal level did not improve model performance the forecasting results on the test data are provided in table 8 among the five models tested in this study the double seasonal models yielded the best results in terms of r 2 and aare the one hour ahead aares for non single double and triple seasonal models were 6 30 8 00 4 80 and 9 48 respectively in terms of online parameter adaptation the addition of glr algorithm resulted in more accurate prediction intervals picp at the cost of minor loss in forecasting accuracy aare which is consistent with the results from the other two case studies adding the yearly period to the model degraded both the lead 1 and lead 24 forecasts the results of model forecasting which were consistent with the acf pacf plots concluded that the triple seasonal model could not properly characterize the observations 6 conclusions the existing software packages implementing the traditional box jenkins time series methodology support at most one level of seasonality however in many environmental studies the observed data series may contain multiple superimposed periods in an attempt to improve modeling accuracy the msari model presented in the paper introduced an explicit formulation associated with multi seasonality the msari models were constructed via the multiplication of the characteristic polynomials at different seasonal levels a structural matrix h was employed to concisely represent a specific multi seasonal model structure to apply the msari models a suite of tools were developed for model identification parameter estimation forecasting and performance checking three examples were studied in this research the hourly electricity demands of a canadian province the hourly tap water demands in a water utility and the hourly sewer inflows of a waste water treatment facility the double seasonal ari models characterized both diurnal and weekly periods and produced more accurate forecasts in the third example a triple seasonal model was also developed in an attempt to model the yearly period however the addition of the third seasonal level did not enhance model outputs the inspection of the autocorrelation and partial autocorrelation spectra concluded that with the same parameter count multi seasonal models in general provided a better match to the observed autocorrelation structures the findings were verified by the forecasting results obtained from the tested data sets in all cases double seasonal models consistently yielded the most accurate forecasts in terms of r 2 and aare overall the multi seasonal technique provides a promising option when modeling environmental time series the time series modeling toolkit implemented in this research was designed for real time applications with live data services the real time forecasting program includes three features the first feature is the multi seasonal model structure focused on auto regressive models future versions will consider a moving average extension as an extension to the box jenkins time series models the msari models still maintains a linear form therefore efficient estimation and forecasting algorithms can be implemented the second feature is the modification of the glr algorithm to perform in a sequential batch mode to maintain the quality of forecasts the glr algorithm monitors the model performance and allows the re estimation of parameters between forecasting time steps therefore the manual procedures of parameter re estimation calibration become unnecessary the third feature is the implementation of dynamic visualizations specific for time series modeling the animations developed in the toolkit visualize raw data multi step ahead forecasts prediction intervals forecasting errors and parameter evaluation re estimation dynamic visualization intuitively conveys the crucial information about the forecaster for better decision supporting from the modeling perspective the msari model with its relatively simple linear form can serve as a foundation for complex domain specific models in many areas of study the temporal variations are traditionally modeled as deterministic patterns replacing the patterns with msari models will enable 1 the uncertainty analysis of the domain specific models and 2 the utilization of online observations for improved predictions for example the electricity demand series can be used in an operational model to optimize scheduling and dispatching the water demand or sewer flow time series model can drive the hydraulic models of the water systems facilitating the forecasting and uncertainty analysis of hydraulic states in the current implementation data services are emulated by a data file reader that generates data points at fixed time intervals to link seamlessly with the existing metering infrastructures future versions of the toolkit will support direct database connection and web based data services the toolkit will also be improved to support multiple time series and other forecasting algorithms acknowledgment this research was funded in part by nsf grant cmmi 09000713 real time distribution system network modeling and fault diagnosis the authors would like to thank john mccary from the public utilities department hillsborough county fl for providing the scada data for the second data set the authors would like to thank bryant mcdonnell for providing the hourly sewer flow series as the third data set appendix a supplementary data the following are the supplementary data related to this article online data online data time series forecasting time series forecasting time series forecasting animation time series forecasting animation raw data raw data raw data animation raw data animation error matrix error matrix error matrix animation error matrix animation appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 034 
26387,many environmental data sets are driven by multiple superimposed periods yet most time series analysis software packages only support single seasonality the objective of this research was to develop a software toolkit utilizing multi seasonal autoregressive integrated msari models a toolkit in matlab was developed for msari based identification estimation forecasting and visualization in the toolkit an adaptive forecasting routine uses a continual event loop for real time data acquisition and parameter re estimation a statistical quality control algorithm monitors model performance and re estimates parameters when necessary a set of visualization tools provide animated graphical representations of forecasts prediction intervals and key performance metrics the toolkit was applied to three case studies electricity demands water demands and sewer flows the analysis of the results demonstrated that the explicit modeling of multi seasonality improved model predictions therefore the msari software presents a promising tool for modeling and predicting real time data series keywords seasonality autocorrelation time series forecasting visualization software and data availability msari toolkit name of software msari toolkit developers jinduan chen and dominic l boccelli contact address 737 engineering research center university of cincinnati cincinnati oh 45221 0012 usa telephone 513 923 0706 e mail jinduan uc gmail com year first available 2014 hardware required pc software required matlab 7 12 0 r2011a or up license gnu general public license version 2 git repository https bitbucket org jinduan tsff programming language matlab program size 1 mb data set 1 name of data hourly ontario and market demands 2002 2013 year first available 2014 form of repository csv file download webpage http www ieso ca pages power data data directory aspx size 2 56 mb data set 2 name of data hourly inflow outflow data of production and storage facilities of the south central water distribution network in hillsborough county fl apr 2012 to dec 2012 year first available 2013 form of repository matlab mat file availability https data mendeley com datasets 4yhprsgjrf 1 size 52 kb data set 3 name of data hourly sewer flows monitored at station s2 in columbus oh jun 1998 to dec 2013 year first available 2014 form of repository matlab mat file availability https data mendeley com datasets 4yhprsgjrf 1 size 371 kb 1 background and objectives many environmental processes are observed in discrete time intervals these time series can be described and predicted by statistical models when physical models are unavailable or too complex to apply traditionally the techniques of time series analysis have been applied to characterize the dynamics of air pollution robeson and steyn 1990 rainfall burlando et al 1993 lake water levels irvine and eberhardt 1992 river flow kachroo and liang 1992 urban water consumption jowitt and xu 1992 salmon production hare and francis 1995 fishery landings koutroumanidis et al 2006 and many others with the recent advances in peripheral sensor and communication technologies real time data services have become more prevalent in environmental applications e g allen et al 2011 however new challenges have been presented to the software tools designed for real time time series analysis first the tools have to adopt high data transfer rates therefore fast algorithms for parameter estimation and forecasting are required second the tools need to run consistently for a long time without human intervention which necessitates self adaptive model structures and or parameter re estimation methods see e g chen et al 1995 third dynamic visualization tools would benefit the perception of forecasting results and performance metrics in addition to these real time software considerations the fundamental challenge associated with time series modeling remains the identification of the appropriate model structure while there are many approaches to time series modeling such as box jenkins time series box and jenkins 1976 wei 2006 artificial neural networks ann hill et al 1996 and support vector machines svm mller et al 1997 our previous studies chen and boccelli 2014 chen 2015 showed that a variant of the classical box jenkins time series named the multi seasonal auto regressive integrated msari model is promising for use within a real time framework for a selection of case studies the empirical auto correlation function acf and partial auto correlation function pacf diagrams showed much stronger characteristics of autoregressive ar processes than moving average ma processes the msari models have also been shown to deliver competitive accuracy compared with more complex models adhikari and agrawal 2013 the relatively simple model structure and reduced number of parameters yield efficient parameter estimation and forecasting algorithms also benefiting from the model structure parameter adaptation algorithms can be implemented in an efficient manner however existing software packages do not have the facilities to specify and utilize the multi seasonal structures and there are few efforts focused on automatic parameter re estimation the objective of the present research is to develop a real time oriented software toolkit with a suite of utilities tailored for msari models at the core of the toolkit is an adaptive forecasting routine by applying the tools on various data series the power and flexibility of the real time analysis tools are demonstrated in this article the second section of the paper reviews the previous work on modeling theory software development and visualization techniques the third section introduces the representation of the multi seasonal structure and algorithms developed around the structure the fourth section discusses the architecture of the toolkit the design of the application programming interface api and the real time visualization tools in the fifth section three case studies are examined the final section summarizes the findings originated from the research and proposes future directions of the study 2 related work 2 1 time series models both linear and non linear models have been suggested to model environmental time series the most popular methods are autoregressive integrated moving average arima models for linear models and ann kaastra and boyd 1996 and svm sapankevych and sankar 2009 for non linear models more recently tiwari and adamowski 2013 proposed a hybrid wavelet bootstrap neural network wbnn model for short term 1 day 2 months urban water demand forecasting the model is presented as an ensemble of several anns to improve forecasts and characterize uncertainties bennett et al 2014 utilized autoregressive integrated moving average with exogenous variables arimax and ann techniques to predict energy use in low voltage distribution networks and suggested a hybrid approach to improve model performance sehgal et al 2014 used wavelet bootstrap multiple linear regression wbmlr to forecast daily river discharges and observed better performance than multiple linear regression mlr and ann models overall the relative predictive power of the linear arima models compared with non linear ann and svm models vary from case to case and the performance for a particular type of model still relies heavily on the empirical selection of the model structure sfetsos 2000 ho et al 2002 wang et al 2009 adhikari and agrawal 2013 this study will focus on the extension and application of traditional time series models because of the compatibility with explicit periodic model structures and adaptive parameter re estimation methods the linear auto correlations in equally intervalled univariate time series were investigated systematically in the fundamental work of box and jenkins 1976 this work introduced a group of linear gaussian models also known as arima models moreover a variant of the box jenkins time series model the seasonal arima sarima model was proposed to characterize periodic behaviors existing in many environmental and socio economic phenomena the single seasonal model was originally proposed to predict air traffic over the years the formulation has been adopted in other applications such as representing monthly lake water levels irvine and eberhardt 1992 monthly river flow kachroo and liang 1992 and hourly road traffic flow williams et al 1998 a multi seasonal arima model has two or more periods incorporated into the model formulation for example caiado 2009 studied the daily urban water demand based on a double seasonal arima model that accounted for weekly and annual relationships in water demands 2 2 time series software three existing software tools for building arima models are investigated in this research the tools include two popular scientific statistical computing platforms r and matlab and one special purpose software package gretl the r packages stats and forecast hyndman et al 2014 include many utility functions for conducting static arima analysis for example functions acf and pacf compute and plot the sample autocorrelation function acf and sample partial autocorrelation function pacf for model identification function arima accepts a data series and estimates the parameters for a non seasonal or single seasonal arima model initial boundary conditions and missing values are handled exactly via a state space representation of the arima process gardner et al 1980 function auto arima searches for the best fitted model structure with a specified information criterion dziak et al 2012 both arima and auto arima return arima objects containing model parameters function predict arima uses an arima object to compute n step ahead forecasts the r package qcc implements the spc methods of shewart quality control charts shewhart 1931 cumulative sum control chart cusum page 1963 and exponentially weighted moving average ewma chart roberts 1959 the tools are designed to detect significant deviations of a quality indicator of the target value and modify the parameters if necessary the same change detection techniques can be potentially used in the series of forecasting residuals to determine the timings of model re estimation however there are no functions available to integrate the quality control methods with the arima forecasting methods in matlab the existing utilities for time series analysis mainly reside in the econometrics toolbox functions autocorr and parcorr calculate and plot the sample autocorrelations and sample partial autocorrelations the arima class is developed to hold the structure and parameters of an arima model the class methods of arima estimate and arima forecast are implemented for parameter estimation and forecasting respectively however the existing toolboxes support only single seasonality in the model specification and the forecasting function is not designed for real time applications with continuous data supply the gnu regression econometrics and time series library gretl is an open source software package for econometrics research cottrell and lucchetti 2014 baiocchi and distaso 2003 the package includes a command line program a graphical user interface and a numerical library with apis written in c gretl provides a set of concise commands for data import export data analysis and graphing the gretl command arma is used for estimating arima parameters using a kalman filter along with the bfgs maximization nocedal and wright 2006 the command fcast produces forecasts gretl also allows connections to a database and retrieving time series data via the open database connectivity odbc standard however the data link can only be accessed manually under the batch mode and there is no support for real time data acquisition and dynamic visualization 2 3 visualization techniques the visualization techniques of time series data have drawn many research interests in box and jenkins 1976 three charts are introduced for assisting the modeling of serial observations 1 the correlograms which include the autocorrelation function acf and partial autocorrelation function pacf diagrams are plots that show the sample autocorrelations and partial autocorrelations versus time lags the correlograms provide visual clues during the identification of arima models 2 the unit root plot shows the real or complex roots of the characteristic polynomial of an arima model and their relative distance to the unit circle this plot is useful in visualizing and detecting the non stationarity of a model 3 the forecast plot shows the observations versus elapsed time in a x y plot the plot also includes multi step ahead forecasts and two lines showing the 95 and 50 prediction intervals for the forecasts modern computer hardware and software has allowed more information about time series to be visually conveyed through the use of color 3 dimensional geometries animations and interactive graphs cleveland 1993 introduced the cycle plot for seasonal time series in which every point in a regular time series plot is replaced with a small subplot showing the inter seasonal trends saito et al 2005 used a two tone pseudo color approach to visualize large multivariate meteorological data sets in a compact fashion tominski et al 2005 used a circular axis to represent seasonal time series for epidemic data stressing the cyclic repetitive nature van wijk and van selow 1999 suggested using a parallel 3d bar graph to visualize time dependent electrical demand data where the heights of the bars represents the values of the observations and the bars are aligned with the time of day and date with this design the seasonal variations were easy to inspect because of the extra dimension the same idea was adopted by wicklin 2011 in the creation of a tile map in which the relative values of the observations were represented in greyscale however despite the many methods for visualizing time series and predictions there have been few research efforts on the visualization of forecasting errors in real time applications the visualization of errors are valuable for monitoring model performance and diagnosing problems from the review of the technical state of the art three features absent in the existing software implementations are identified the support of multi seasonality in the time series models the coupled parameter re estimation and forecasting routine to prevent performance deterioration and animated visualization of predictive errors the present research addresses these three problems in a newly developed time series modeling forecasting toolkit implemented in the matlab programming language 3 methodology 3 1 model representation and parameter estimation the multi seasonal ari models for time series are formally expressed as 1 i 0 n  i b s i s i d i x t a t in which x t is the time series of observations n is the number of seasonal periodicities b is the backward operator defined as b x t x t 1 and treated as a variable applicable for linear transformations s i is the length of the i th period s 0 1 to represent regular non seasonal auto regressions p i and d i are structural parameters that indicate the number of autoregressive ar parameters and the number of differencing levels for the i th period s i d i is the ith seasonal differencing integration operator defined as s i d i 1 b s i d i  i b s i is the ith ar characteristic polynomial defined as  i b s i 1  i 1 b s i  i 2 b 2 s i  i p i b p i s i in which  i j are ar parameters and a t is a series of i i d zero mean gaussian random variables with standard deviation  in msari models both the local auto correlations and the long term periodic behaviors are accounted for via explicit formulation when n 1 equation 1 represents a single seasonal ari ssari model with structural parameters p 0 d 0 p 1 d 1 s 1 and regression parameters  1 1  1 p i  0 1  0 p 0 when n 1 more layers of ar parameters will be included in the model for example for hourly observations the values of s 1 and s 2 can be naturally chosen as 24 and 168 denoting diurnal and weekly periods respectively fig 1 conceptually shows the differences between non seasonal single seasonal and double seasonal ari models the multiplicative parameters  are positioned above the blocks of random variables at the specific time steps the number in the parentheses in the subscript of  indicates the level of seasonality fig 1 a shows that only one historic observation x t 1 contributes to the expression of x t fig 1 b shows a daily seasonal ssari model looking back to time t 24 in predicting x t also x t 25 has non zero multipliers in the expression due to the multiplication of the characteristic polynomials fig 1 c shows that a double seasonal ari model incorporating both daily and weekly seasonality will refer further back into the history for example data received as early as t 193 will be used in the expression of x t even more characteristic polynomials can be added to represent monthly quarterly and annual periods the previously reviewed software packages use one 7 element tuple p d q s p d q to represent the single seasonal model structures the present study focuses on the pure ar models therefore the q parameters associated with the moving average ma component are not used to represent multiple superimposed seasonality the ar parameter structure is extended to a 3 n matrix the structural matrix h is defined as 2 h 1 s 1 s n d 0 d 1 d n p 0 p 1 p n the first second and third row of the matrix represent the lengths of the periods the orders of seasonal integration and the numbers of autoregressive parameters respectively each column represents a seasonal level an important number corresponding to the msari structure is the look back length l b defined as the oldest observation that will appear in the expression of x t from equation 2 the look back length can be conveniently computed as l b i 0 n s i p i d i if a model structure h is known the parameter estimation of the msari models will follow a minimum sum of squared errors mse approach similar to that described in box and jenkins 1976 the optimal values of the regressors  i j can be obtained by an iterative algorithm in which the squared sum of the white noise terms a t is minimized the identification procedure of matrix h is introduced below 3 2 model identification procedure in the present toolkit the identification of msari models i e selecting a proper structural matrix h involves a combination of computation visual inspection and modeler discretion the methodology is a multi seasonal extension to the original box jenkins approach of acf pacf diagram inspection this research developed utilities to assist in the identification of the appropriate structural matrix the first row in h comprises the lengths of the periods s i in practice the periods are usually assumed to be regular calendrical periods such as hours in a day day in a week etc the periods can also be taken from the original acf diagram as periodic spikes in autocorrelations can be observed for seasonal time series the second row in h comprises the orders of integration d i the definition of msari in equation 1 suggests a separation of the seasonal differencing operator i 0 n s i d i and the autoregressive operator i 0 n  i b s i the seasonally differenced series i 0 n s i d i x t is expected to be a stationary autoregressive model identifiable using the pacf diagram according to box and jenkins 1976 such ar models will have decaying spikes of partial autocorrelations located at periodic lag times and partial autocorrelations beyond the lag time i 0 n s i p i should approach zero a program developed during this study applied various choices of d i to the data series to generate pacf diagrams for further inspection on a trial and error basis the differencing operators with the smallest pacf residuals in large lag times were selected for further investigation while higher orders of d i may be helpful our experience suggests that setting d i 0 1 1 will generally result in good removal of pacf residuals and at a minimum is a good initial starting point the third row of h comprises the numbers of parameters on each seasonal level p i two metrics are considered in this study to evaluate the different distributions of parameters given a fixed total number of parameters the first metric is the akaike information criterion aic or bayesian information criterion bic see e g dziak et al 2012 aic is defined as 3 c a 2 l 2 p t and bic is defined as 4 c b 2 l ln n p t in both expressions l represents the maximum log likelihood achievable with the model structure p t i p i represents the total number of parameters and n represents the sample size models with smaller values of aic bic are considered superior to those with greater values these information metrics are designed to assess the trade off between the model accuracy and the parametric parsimony the second metric is the similarity in autocorrelation structures specifically any candidate msari model must correspond to an autocorrelation structure that can be computed with yule walker equations box and jenkins 1976 the euclidean distance in autocorrelations with varied lag times between the observed and synthetic series are then calculated to represent the similarity mathematically 5 s i 1 m c i c i 2 in which i is the lag time of the autocorrelation coefficients m is a large number for lag time beyond which the autocorrelations are considered to be negligible c i and c i are the autocorrelation coefficients for the observed and simulated series respectively the model structure that produces the minimum sum of squared errors in autocorrelations is considered the optimal model fig 2 shows the procedure used in this study to identify the parametric structure for the msari models the first and second steps identify the first and second rows of h by visually inspecting the data series and the correlograms for the original data and the filtered data the correlograms are plotted by a program developed in this study that can handle the multi seasonal structures for the third step i e large dashed box an identification program was developed to enumerate and evaluate each possible configuration of p 0 p 1 p n p i p t for a given configuration the corresponding matrix of h is constructed and the model is fitted to the observations using the standard minimum sum of squared error mse method the performance of any configuration can be evaluated using the metric of either aic bic or acf deviations as described above the configuration with the minimum aic bic or the minimum acf deviations are reported as the optimal choice for the third row of matrix h 3 3 adaptive forecasting if both model structure and parameters are known the least mean squared error forecasts of msari models can be recursively calculated through a formula similar to the one suggested by box and jenkins 1976 6 x  t l 1 i 0 n  i b s i s i d i x  t l l 0 x t l l 0 in which l is the time horizon of the forecast x  t l is the forecast of x t l produced at time t x t l is the observation at time t l the ari operator i 0 n  i b s i s i d i is applied on l instead of t the prediction intervals of the forecasts can be calculated as x  t l   2 v l in which   2 is the 1  2 percentile of the standard normal distribution n 0 1 and v l is a constant that only depends on the time horizon and the parameters of the model detailed derivation of the formula can be found in box and jenkins 1976 the present study also introduces a batch parameter re estimation method to detect changes and update parameters when necessary a moving window of previous observations are saved to re estimate the model parameters table 1 shows the differences between the traditional time series methodology and the proposed methodology the first row of the table shows that in the classical box jenkins method all four major modeling steps are conducted offline after all data are retrieved the second row shows that once an estimated model is supplied with a live data link the forecasting routine can produce predictions right after data retrieval the third row represents the real time forecasting combined with forecasting quality control using this methodology only identification is conducted off line while parameter re estimation forecasting and performance checking all happen in real time this set up is necessary for long running applications in which the underlying dynamics of the series may experience major changes and the model parameters should be monitored and re estimated to maintain the prediction power the quality control method employed in this toolkit is the generalized likelihood ratio glr algorithm basseville and nikiforov 1993 the glr algorithm is a generalized form of the cusum method that uses the ratios of likelihoods to detect significant parameter changes lai 1995 the glr algorithm is expressed as 7 t inf t sup  i t m 1 t log f  x i f 0 x i h in which t is the time of a model change  is a set of model parameters including both standard deviation  of the white noise and autoregressive parameters  i j f x i represents the likelihood of the observations using the error terms a t as i i d normally distributed white noise with variance  as defined in equation 1 for the original subscript 0 and alternate subscript  parameter sets m is the size of the sliding window and h is the pre determined switching threshold conceptually for a fixed msari model structure h the algorithm maintains two sets of parameters at each time step the current set parameters m 0 are carried over from the previous steps and a stand by set of parameters m 1 is estimated from a sliding window of previous observations using the mse approach the likelihoods within the sliding window also named glr scores are calculated for both m 0 and m 1 if the ratio of the two likelihoods exceeds the pre determined threshold the algorithm replaces the m 0 parameters with m 1 i e a model switch the benefits of using the glr algorithm include better forecasts for long running time series and better outputs of prediction intervals both resulting from the adaptive parameters 3 4 performance measures the real time forecasting routine produces both forecasts and probability limits in this study the following measures are utilized to evaluate the model performance 3 4 1 coefficient of determination r 2 the coefficient of determination r 2 represents the ratio of the variability explained by the model and total variance r 2 is defined as 8 r 2 1 i x  i x i 2 i x i x 2 in which x i and x  i are observations and predicted values forecasts and x is the mean value of the observations an r 2 close to 1 means the predictions are close to the actual observations and most of the variance has been accounted for by the model 3 4 2 average absolute relative error aare the average absolute relative error aare is another measure of the forecasting accuracy aare is defined as 9 a a r e i x  i x i x i aare can also be written in the form of percentages smaller aare values mean the predictions are close to observations 3 4 3 prediction interval coverage probability picp the prediction interval coverage probability picp measures the accuracy of the prediction intervals produced by a probabilistic forecasting model shrestha and solomatine 2006 picp is defined as 10 p i c p  i 1 n i w  i  x i u  i  n in which  is the confidence level used when generating the prediction intervals i is the indicator function that equals to 1 when the condition is true and 0 otherwise w   and u   are lower and upper bounds of the prediction interval and n is the size of the test data set a model that produces accurate prediction intervals should have p i c p  close to the selected  value 4 implementation 4 1 control panel fig 3 shows the graphical user interface gui of the control panel the modeling steps of identification estimation and forecasting can be executed from the panel the gui enables options and parameters that can be conveniently adjusted to facilitate the study of the methodology the top pane is used for loading the data file or data link of the data source the pane at the bottom left is used for specifying structural parameters of the model for up to three seasonal levels in addition two utilities can be invoked from the model pane to plot acf pacf correlograms and identify optimal model structures using the methods introduced in the previous section the pane at the bottom right is used for supplying options and parameters to the real time forecasting routine users can choose either fixed parameter static or adaptive parameter forecasting and specify the window size and or threshold of the glr algorithm users can also specify the desired forecasting horizons and visibility for the animated real time visualization 4 2 forecasting event loop fig 4 shows how components in the developed toolkit are integrated specifically a user can press the run stop button in the control panel to enter a forecasting loop with the desired structural parameter inside the loop msari parameters are first estimated by the function findssemin using the training data set the parameters are used to instantiate the class sariforecaster to create a forecasting model next the routine calls the data source to retrieve a new observation emudatasource getanobs then calls the forecaster to update the data buffer and compute forecasts sariforecaster update the current forecasts are sent to the viewer to update the visuals rtviewer update as the final step of an iteration if the parameter adaptation is enabled the differences in likelihood for the current and alternative parameter sets are compared with the preset threshold to determine if a model switch is necessary the selected forecasting model is employed in the next receiving forecasting displaying iteration the loop runs repeatedly until the data source sends an end signal or the user presses run stop button in the gui 4 3 visualization five graphing tools were developed in this study to illustrate important real time performance indicators and diagnostics including raw data forecasts hindcasts forecasting errors and model re estimation indicators for brevity only one graphing tool forecast hindcast view is described in this section detailed discussion about the other four visualization types are provided in the supplemental materials fig 5 shows the forecast hindcast view fhv an animated version of the classical box jenkins forecasting chart the three subplots in the fhv show the variable water demands in this example versus the elapsed time for raw data fig 5a current forecasts fig 5b and previous forecasts or hindcasts fig 5c the forecast hindcast views also include the 50 and 95 prediction intervals when animated the view shows new observations and changing forecasts and lines of prediction intervals the hindcast view is a practical tool for the modelers to review the accuracy of the forecasting model in real time the remainder of the article focuses on the performance of the msari models 5 results and discussions 5 1 hourly electricity demand in ontario canada the first case study of the present research is the electricity demand data set downloaded from the independent electricity system operator ieso public data directory this time series represents the total hourly demands of ontario canada from may 2002 to december 2013 table 2 shows the descriptive statistics of the data set the data set is separated into a training set first 20 of data and a test set last 80 of data the training set is used for identifying the msari model and estimating the initial set of parameters the test set is used for examining the online forecasting algorithm to which data points are supplied sequentially to simulate a real time scenario the forecasting algorithm ran in both static and adaptive modes in static mode the initial parameter estimates remained fixed throughout the simulation in adaptive mode the estimates were allowed to be adjusted dynamically using the glr algorithm described in section 3 3 to identify the structural matrix h the steps described in section 3 2 were followed fig 6 shows the acf and pacf diagrams for up to 250 h of lag time generated from the plotting tools in fig 6 a the initial series shows very strong periodic autocorrelations for lag times of multiples of 24 and the lag 168 autocorrelation coefficient is slightly higher than the adjacent peaks at lag 144 and lag 192 therefore the two periods of the msari models were set as 24 and 168 respectively the pacf diagram in fig 6 a contains many residuals if compared with a typical autoregressive model applying a single seasonal differencing operator 24 to the series yields the acf pacf diagrams shown in fig 6 b this action removed the periodic autocorrelations in the acf chart and the resulting pacf charts have clear decaying periodic spikes in fig 6 c the application of a double seasonal differencing operator 168 24 further reduced the pacf residuals enhancing the traits of the seasonal autoregressive model subsequently the distribution of autoregressive parameters on the diurnal and weekly seasonal levels were determined all possible combinations with less than 6 parameters including 5 autoregressive parameters plus the standard deviation of the white noise were evaluated in terms of aic bic and the acf similarity models with more than 6 parameters had limited improvement by the three metrics section 3 4 and were not considered further table 3 lists the structures and parameters of the different models for comparison purposes non seasonal and single seasonal models were also included in the analysis for the non seasonal model an extra parameter  was included to shift the average value of the series to be around zero to facilitate parameter estimation for seasonal models the parameter  was not needed due to the existence of seasonal differencing operators the identification results show that the seasonal models yielded significantly smaller values of aic bic and acf similarity than the non seasonal models comparing the single and double seasonal models the adoption of double seasonality further reduced the three indicators therefore in the identification step the double seasonal models were recommended with the same levels of seasonality the model structures identified with aic and bic criteria were identical the equivalence of aic and bic can be attributed to the domination of the log likelihood terms in equations 3 and 4 due to the large sample size the acf similarity criterion produced different optimal structures for example the double seasonal model structure that minimized aic was p 0 2 p 1 1 p 2 2 while the structure that minimized acf similarity was p 0 1 p 1 3 p 2 1 however despite the differences in structures and parameters the models minimizing aic bic and acf similarity yield similar spectra of autocorrelation structures and led to similar prediction power in this study the acf similarity metric was used in the following case studies because of the faster computation speed and better differentiation between the models to evaluate the model performance the non single and double seasonal models were used in the forecasting routine described in section 4 both static and adaptive parameter forecasting algorithms were tested the open source software gretl cottrell and lucchetti 2014 was also used for comparison although the capabilities were limited to single seasonal models and static lead 1 forecasting table 4 shows the various performance measures for the models tested the forecasting time horizons shown in the table were for one hour and 24 h representing short term and long term forecasts respectively the results show that the non seasonal model performed worse than seasonal models in terms of the listed measures the seasonal models were more appropriate in forecasting the electricity demand series especially for long term forecasting a single seasonal model generated lead 1 forecasts with 0 92 aare and lead 24 forecasts with 5 36 aare using a double seasonal model improved the results to 0 85 and 4 33 without adding any additional parameters the forecasting results matched the conclusion of the model identification that double seasonal models provide the best fit for the data in terms of the autocorrelation structures the adaptive double seasonal model slightly improved the forecasting results in terms of r 2 lead 24 aare and 50 picp the sliding window size and switching threshold were empirically selected as 300 and 1 for the glr algorithm the adaptive model resulted in tighter prediction intervals than the static counterpart since the standard deviation was adjusted throughout the simulation however the limited improvements in the forecasts seem to suggest that the underlying autocorrelation structure did not experience major changes within the time span of the test dataset in our testing environment we ran the data set on a computer with 2 5 ghz cpu 6 gb memory and matlab version 7 12 the average computational time for non adaptive and adaptive models are 2 93 ms and 605 3 ms respectively although adding the parameter re estimation significantly increased the time for hourly time series data both methods will generate forecasts virtually instantaneously the overall results shown in table 4 suggest that the regional hourly electricity demand can be very accurately modeled and predicted by a double seasonal ari model 5 2 hourly drinking water demand in hillsborough county fl the second case study investigated in this research was the hourly drinking water demands for a water distribution network in hillsborough county fl the series is computed by summing the outflows of all water production and storage facilities in the network inflows to the storage facilities are treated as negative outflows the original data were in 5 min intervals to be consistent with common hydraulic modeling practice the data were aggregated into 1 hour intervals by averaging the 12 values in each hour table 5 shows the descriptive statistics of the demand series compared to the electricity data the water flow measurements are prone to more device errors and maintenance outages see e g allen et al 2011 as a result the series has a significant amount of data missing 2 3 the performance metrics for the missing data points were excluded from the final performance reports following a similar procedure to the first case study the single seasonal model for the water demands was identified as h 1 24 0 1 3 2 with a look back length of 75 and the double seasonal model was identified as h 1 24 168 0 1 1 1 1 3 with a look back length of 721 table 6 shows the forecasting results associated with applying a similar set of models from the first case study likewise the results show that the double seasonal models generated more accurate forecasts than the non seasonal and single seasonal models the improvements in long term forecasts were more significant than short term forecasts according to the table the adaptive double seasonal model produced slightly better results in lead 1 aare and 50 picp than the static model however in other metrics the adaptive models had no significant advantages probably due to the relatively short time span of the test data set on average the 6 parameter double seasonal model was capable of generating one hour ahead forecasts with 6 7 relative errors and one day ahead forecasts with 11 13 relative errors although not as accurate as the electricity demands series the msari models still demonstrated the capacity of predicting future hourly water demand better than the non or single seasonal models 5 3 hourly sewer flow in columbus oh the third data series researched in this study was the hourly sewer flows monitored at a waste water treatment facility in columbus oh the data include 134017 data points collected in 15 years table 7 shows the descriptive statistics of the data set as the data set covers many years of sewer data in this case study a triple seasonal model was also developed in addition to the previously mentioned single and double seasonal models the identified msari models in terms of acf similarity are h 1 24 0 1 1 4 for single seasonal model h 1 24 168 0 1 1 1 2 2 for double seasonal models and h 1 24 168 8760 0 1 1 1 1 0 0 4 for triple seasonal models however the visual inspection of the acf and pacf diagrams showed that the autocorrelation spectra under the double and triple seasonal filters were similar suggesting that the addition of a third seasonal level did not improve model performance the forecasting results on the test data are provided in table 8 among the five models tested in this study the double seasonal models yielded the best results in terms of r 2 and aare the one hour ahead aares for non single double and triple seasonal models were 6 30 8 00 4 80 and 9 48 respectively in terms of online parameter adaptation the addition of glr algorithm resulted in more accurate prediction intervals picp at the cost of minor loss in forecasting accuracy aare which is consistent with the results from the other two case studies adding the yearly period to the model degraded both the lead 1 and lead 24 forecasts the results of model forecasting which were consistent with the acf pacf plots concluded that the triple seasonal model could not properly characterize the observations 6 conclusions the existing software packages implementing the traditional box jenkins time series methodology support at most one level of seasonality however in many environmental studies the observed data series may contain multiple superimposed periods in an attempt to improve modeling accuracy the msari model presented in the paper introduced an explicit formulation associated with multi seasonality the msari models were constructed via the multiplication of the characteristic polynomials at different seasonal levels a structural matrix h was employed to concisely represent a specific multi seasonal model structure to apply the msari models a suite of tools were developed for model identification parameter estimation forecasting and performance checking three examples were studied in this research the hourly electricity demands of a canadian province the hourly tap water demands in a water utility and the hourly sewer inflows of a waste water treatment facility the double seasonal ari models characterized both diurnal and weekly periods and produced more accurate forecasts in the third example a triple seasonal model was also developed in an attempt to model the yearly period however the addition of the third seasonal level did not enhance model outputs the inspection of the autocorrelation and partial autocorrelation spectra concluded that with the same parameter count multi seasonal models in general provided a better match to the observed autocorrelation structures the findings were verified by the forecasting results obtained from the tested data sets in all cases double seasonal models consistently yielded the most accurate forecasts in terms of r 2 and aare overall the multi seasonal technique provides a promising option when modeling environmental time series the time series modeling toolkit implemented in this research was designed for real time applications with live data services the real time forecasting program includes three features the first feature is the multi seasonal model structure focused on auto regressive models future versions will consider a moving average extension as an extension to the box jenkins time series models the msari models still maintains a linear form therefore efficient estimation and forecasting algorithms can be implemented the second feature is the modification of the glr algorithm to perform in a sequential batch mode to maintain the quality of forecasts the glr algorithm monitors the model performance and allows the re estimation of parameters between forecasting time steps therefore the manual procedures of parameter re estimation calibration become unnecessary the third feature is the implementation of dynamic visualizations specific for time series modeling the animations developed in the toolkit visualize raw data multi step ahead forecasts prediction intervals forecasting errors and parameter evaluation re estimation dynamic visualization intuitively conveys the crucial information about the forecaster for better decision supporting from the modeling perspective the msari model with its relatively simple linear form can serve as a foundation for complex domain specific models in many areas of study the temporal variations are traditionally modeled as deterministic patterns replacing the patterns with msari models will enable 1 the uncertainty analysis of the domain specific models and 2 the utilization of online observations for improved predictions for example the electricity demand series can be used in an operational model to optimize scheduling and dispatching the water demand or sewer flow time series model can drive the hydraulic models of the water systems facilitating the forecasting and uncertainty analysis of hydraulic states in the current implementation data services are emulated by a data file reader that generates data points at fixed time intervals to link seamlessly with the existing metering infrastructures future versions of the toolkit will support direct database connection and web based data services the toolkit will also be improved to support multiple time series and other forecasting algorithms acknowledgment this research was funded in part by nsf grant cmmi 09000713 real time distribution system network modeling and fault diagnosis the authors would like to thank john mccary from the public utilities department hillsborough county fl for providing the scada data for the second data set the authors would like to thank bryant mcdonnell for providing the hourly sewer flow series as the third data set appendix a supplementary data the following are the supplementary data related to this article online data online data time series forecasting time series forecasting time series forecasting animation time series forecasting animation raw data raw data raw data animation raw data animation error matrix error matrix error matrix animation error matrix animation appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 034 
26388,in situ field monitoring networks generate vast quantities of continuous data can help to improve the design management operation and maintenance of green infrastructure gi systems however such actions require efficient and reliable quality assurance quality control qaqc in this paper we develop a rule based learning algorithm involving dynamic time warping dtw to investigate the feasibility of detecting anomalous responses from soil moisture probes using data collected from a gi site in milwaukee wi as an enhancement to traditional qaqc methods which rely on individual time steps this method converts the continuous time series into event sequences from which response patterns can be detected association rules are developed on both environmental features and event features the results suggest that this method could be used to identify abnormal change patterns as compared to intra site historical observations though developed for soil moisture this method could easily be extended to apply on other continuous environmental datasets keywords qaqc association rule learning green infrastructure anomalous pattern detection dynamic time warping 1 introduction in situ monitoring with sensors is increasingly being used to improve the design and management of green infrastructure gi systems in urban areas also creating new opportunities for scientific discovery porter et al 2009 at both private and public sites for example digiovanni et al 2010 characterized the hydrologic performance of a small scale green roof with data collected from a custom designed lysimeter a rain gage and soil moisture sensors carson et al 2013 performed a similar study using an onset hobo u30 weather station with a tipping bucket rain gauge and a custom designed weir berretta et al 2014 investigated the et characteristics of a green roof by tracking the soil moisture content and temperature using campbell scientific cs616 water content reflectometer the potential for deriving new insights from these efforts work is however contingent upon good quality data a goal not always easy to achieve given the ease with which data streams can be corrupted hill et al 2009 in the harsh environmental conditions found in gi systems erroneous data frequently occurs in hydro meteorological monitoring sciuto et al 2009 and poor quality data may even significantly hinder analysis inhibit modeling and lead to poor decision making madsen 1989 new approaches to automate quality assurance and quality control qaqc are thus necessary to service the expanding volume of data collected by growing sensors networks national science foundation 2005 overall the purpose of qaqc is to detect the anomalies by both time steps e g an unsensational value and events e g odd response series with strong autocorrelation traditional qaqc protocols emphasize detection of anomalies and outliers for individual time points only and can broadly classified into three categories the first is the record limit check method which compares individual sensor values to a pair of upper and lower limits for example durre et al 2010 used global temperature precipitation snowfall and other extremes to flag sensor data that was physically impossible because it fell outside the range of values observed anywhere on the earth the second includes a family of statistical methods that define outliers as sensor values that fall in the far head or tail of the frequency distribution at a specific location and time a z score derived from the mean and standard deviation of the normalized data values can be used for example to estimate how likely a sensor value is to be an outlier by setting a specific confidence threshold z score exceedances can be used to flag outliers hubbard and you 2005 kunkel et al 2005 such methods are extensions of traditional statistical process control techniques and assume that the variability in the underlying phenomena is constant over time e g stationary such that deviations from historical distributions indicate anomalous behaviors such assumption do not always hold in gi systems where natural processes such as soil erosion or human included perturbation such as irrigation can lead to gradual and abrupt changes in the field the third and most recent category of approaches are machine learning methods unsupervised machine learning methods such as k means have become very popular for outlier detection bolton and hand 2001 bayesian methods such as dynamic bayesian networks dbns identify anomalies by dynamically tracking changes in the historical data hill et al 2009 supervised learning methods such as neural networks kozma et al 1994 lpez lineros et al 2014 support vector machines svm buluta et al 2005 and decision trees john 1995 can classify sensor values as either valid or invalid based on the properties of a training dataset to date application of these methods has principally focused on each time stamped data point frequently underestimating the autocorrelation inherent to the time series data as mentioned previously qaqc methods should scrutinize both individual points and point series in an event one short come of traditional machine learning methods is lack the ability to identify autocorrelation in patterns of change within and between events to enhance the traditional methods to complete qaqc similarity comparisons on patterns or shapes need to be applied across the time series events generated by sensors using gi monitoring as an example the qaqc process should align with our physical understanding of a gi site s rainfall runoff response infiltration characteristics as well as the surrounding microclimatic limits of evapotranspiration a first step would confirm whether the raw observations are responding consistently to historical change patterns few if any existing qaqc methods in hydrology monitoring address this need this paper aims to improve data quality in terms of detecting unusual change series patterns in situations where historical observations are of inadequate quality the approach includes a rule based machine learning algorithm to investigate the feasibility of detecting anomalous response patterns from soil moisture probes within a single site and across multiple sites with similar configurations the data used to demonstrate the technique are collected at a gi site in milwaukee wi to enhance traditional qaqc methods which focus on outlier detection on a single time step without considering the autocorrelation across multiple time points this new method splits the whole data set into chunks based on precipitation events and employs dynamic time warping dtw to align and compare events at different lengths the paired event distances calculated by dtw are generalized to categorize the similarity between events other more stable observed site phenomena are also included in the model to reflect the physical conditions that underlie the association rules 2 methodology 2 1 data and site the monitored gi site is a green roof located in uwm golda meir library in milwaukee wi the site is about 1350 m2 and has a growth media depth of 0 1524 m it is monitored by optirtc optirtc com using soil moisture probes installed at the bottom of the growth media decagon ec 5 soil moisture sensor with hobo link data logger three soil moisture probes are installed at different locations at the site see fig 1 instant precipitation and running median temperature are also recorded by a climate station located in the same site these data were collected at 5 min intervals from april 2016 to january 2017 a polynomial calibration curve was developed to scale the raw data assuming the most saturated observations were 100 saturated while the least were considered 0 saturated the time period used to perform the calibration represents only a small segment of the entire time series allowing us to adjust the calibration ranges later on the overall goal is to maximize the value that can be obtained from imperfect soil moisture data sets data quality varied between the different sensors overall the probe located at south center location yielded the most consistent change pattern while the responses of the other two probes displayed a significant drift as shown in fig 2 raw values obtained from the probe installed at north center dropped suddenly in january 2017 beginning in august 2016 the probe located at the south side shifted response pattern the analysis utilizes all data from the south center probe the data gathered prior to december 2016 from the north center probe and the data gathered before august 2016 from the south side probe to derive the rules from which pattern similarity is evaluated the remainder of the data will be used to test the accuracy of abnormal pattern detection because many physical e g flow directions and anthropogenic e g irrigation regime factors are not known for this site the learning algorithms cannot be tested against a water mass balance for the site however one key benefit of the approach is that such information is not needed the algorithm detects anomalies in the data based solely on similarity between the tested data with patterns observed in the training set 2 2 method process the processes forming up the algorithm are illustrated in fig 3 the raw soil moisture data will be firstly smoothed to mitigate all noises to maximize the quality in the following processes the results are then separated into soil moisture change event smce series based on an inter event dry period each smce will be centralized by its mean value to mitigate the bias from initial soil moisture status in calculating the following dtw distances for all possible smce pairs these distances are then used as a basis to categorize the similarity of each smce pair meanwhile a paired event feature difference pefd is generated for each smce pair finally both the similarity category and pefd of all smce pairs are used to conduct an apriori algorithm to investigate the association rules from pefd to similarity 2 3 noise mitigation the phenomena we are ultimately interested in is the change in soil moisture however soil moisture changes due to diurnal climatic fluxes such as evapotranspiration and condensation can under the right conditions mask smaller amplitude fluctuations due to precipitation events in this context regular diurnal soil moisture variability can be conceived as noise that must first be removed from the time series to extract the soil moisture response we are interested here especially with a sampling interval of five minutes such noise could cover up the principle change pattern for short fluctuation events when evaluating time series similarity to maximize accuracy of the approach noise from all sources must be mitigated prior to attempting to align events moving average method kendall et al 1983 is used for noise mitigation the effectiveness of this approach is shown in fig 4 where the blue curve depicts the raw observation and the red one shows the smoothed trend the red curve is more stable and represents a more general pattern of soil moisture change next the whole series is split into discrete wet and dry events using a six hour inter event dry period driscoll et al 1989 each precipitation event and its successive dry period is then combined as a single smce in fig 4 the starting point of each smce is identified with a dot 2 4 series alignment and distance calculation distance based methods such as dtw have been used to compare the similarity between two time series in multiple dimensions zhuang and chen 2006 as an intermediate step for aligning time series in different lengths hu et al 2003 dtw berndt and clifford 1994 finds the optimal alignment between different time series this approach is widely used in speech processing keogh 2002 sections in the sequences for comparison are considered stretchable and compressible although the optimal alignment between these sequences searched by dtw are one to one between sections time steps are usually aligned as one to many based on the optimal alignment a distance measure is calculated to score the similarity between two sequences fig 5 illustrates the point to point soil moisture alignment between two sample smces in the monitored data the initial segments of the two series are aligned linked but begin to diverge further as time progresses becoming open unlinked this divergence occurs because the first point of each smce pair always follows the dry period of the previous event whereas during the subject rain events soil moisture patterns differ due to the intra storm distribution of rainfall moreover because different storms have different dry period durations there is no justification for linking the end points of different smce pairs this phenomenon is demonstrated graphically in fig 5 where the end point of the black line fell somewhere in the middle of the red line the similarity distance as computed by dtw is defined by the sum of the lengths of all the point to point alignment lines the length of these lines is influenced by both the temporal pattern of the smce response and the initial soil moisture value at the beginning of the event two smce s could have the same temporal pattern but still have a large similarity distance if one occurs after a long dry period and the other after a short dry period similarly two smces with different temporal patterns could have short similarity distances because they both began at similar moisture status time steps to correct for these tendencies in the procedure all smces in the training data set were compared to one another after a pre process of centralization by mean and scaled by standard deviation so that the similarity of the general change patterns can be scored using the distance calculated by dtw these dtw results are then summarized and analyzed to estimate a threshold for defining similarity all these distances were analyzed to categorize the relationship extent of similarity between different smces as shown in fig 6 the distance unit less distribution of the generally matched smce pairs is lower in both mean and standard deviation of the distance than for the unmatched pairs however for the generally unmatched pairs the distance distribution has a higher average and standard deviation any smce pairs with an intermedium average or high standard deviation could be classified either matched or unmatched and are thus flagged as ambiguous starting with each smce as a reference the procedure next computes the standard deviation of the distances between that smce and all others as well as the average their values are plotted against one another in fig 7 the generally matched low mean low standard deviation and generally unmatched high mean low standard deviation relationships are displayed at the lower left corner and upper left corner respectively principle component analysis pca is then used to extract the first two principle components pc along these directions both relationships can be determined by reaching the far end points given few reference a 5  value is selected as a threshold percentile to differentiate these relationships from the ambiguous ones finally the averages of the dtw distances of these two categories labeled in fig 7 are employed as the thresholds to define the paired event relationships if the dtw distance between any two smces are less than the average dtw distance of the general matched category their relationship is defined as match on the other hand if this distance is larger than the average dtw distance of the general unmatched category their relationship is defined as unmatched the region between these two thresholds defines events classified as ambiguous 2 5 paired event feature difference pefd besides the time series distance computed by dtw other physical state and event phenomena such as event duration air temperature and precipitation intensity and quantity enhance the algorithm s skill in evaluating the similarity of any two events although the pre processing conducted prior to dtw mitigates the potential for bias in the soil moisture values the nonsensical possibility to match a winter smce to a summer smce with similar change pattern remains to invalidate such a match a paired event feature difference pefd approach is invoked where julian date average temperature and the other physical characteristics listed below are included as the pefd between two smces for training event duration rain period duration dry period duration event precipitation amount soil moisture time series first principle component to characterize inter smce similarity each event in the training period is again compared with all others once an event is chosen as a reference all physical characteristics differences except julian date and soil moisture pc are categorized into five bins based on their quantile 20 40 60 80 and 100 the granularity of the categories is a function of the sample size research topic result application etc in this case 20 percentile bins were sufficient to demonstrate the method the julian date information is specifically added for seasonality consideration a season is defined as 90 days given four seasons a year if the difference of julian date between two smces is larger than 45 days it is defined as out of season otherwise in season smces that meet this pefd are extracted for further analysis different from other environmental features soil moisture time series first pc indicates the general change direction which is a feature of the smce itself given the difference of soil moisture pc is generally an indicator of the dtw distance absolute pc difference is categorized based on the whole population additionally the absolute soil moisture pc differences are divided evenly into 5 categories because the long tails in its distribution may be underestimated in quantile bins the impacts of both environmental features and event features on association rules will be investigated the following is an example of how pefd criteria are used to find similar smces with environmental features as shown in table 1 this sample pefd includes average temperature difference below 20 percentile dry period duration difference between 40 and 60 percentile event duration difference between 40 and 60 percentile rain period duration difference above 80 percentile and event precipitation difference above 80 percentile as shown in fig 8 a targeted smce is colored pink in all charts fig 8a e illustrate the difference categories of all features of its nearby in season smces the feature difference categories of the targeted smce and its paired smces contain 0 20 20 40 and 40 60 for average temperature 40 60 and 60 80 for dry period and event duration 80 100 for event precipitation and rain period based on the sample pefd the smces to which the targeted smce matches are colored in purple in fig 8f while the blue ones do not meet the pefd criteria it should be noted that even some close in terms of time and similar in terms of event precipitation difference and rain period difference nearby smces may not always be linked to the targeted smce due to the other pefd criteria for example the events prior to the targeted smce in fig 8f does not match the pefd in the features of average temperature and event duration although its change pattern looks similar 2 6 association rule learning association rule learning is a popular data mining method used intensively in marketing analysis for discovering interesting relations between different variables in large databases the apriori algorithm is a classic approach for performing association rule learning it works by mining frequent item sets agrawal and srikant 1994 market analysts use it to find the transaction rules e g 80 of customers who bought wine also bought cheese in hydrology related fields nandagopal et al 2010 used this approach to predict weather conditions using multi station atmospheric data guo et al 2004 forecasted disaster weather by using the algorithm to investigate the relationship between weather elements and natural events for the present study the apriori algorithm is employed to find the association rules determining the likelihood that two smces are match unmatched or ambiguous categories the algorithm generates three important output values support confidence and lift the support value indicates the portion of events in the whole training population that meet the specific pefd the confidence shows the true probability of this association rule given a specific pefd it is important to note that a pefd is a complete set of all pefd criteria defined previously in other words a pefd containing only a portion of all criteria is not considered appropriate to evaluate smce similarity the lift value compares the conditional match probability to the probability of this similarity in the whole population specifically with a unity lift the condition of interest does nothing better to strengthen the association rule than the whole general population an association is strong only when its lift greater than 1 3 results the results are presented in three stages using rules based on the environmental features only the skill of the apriori algorithm to find matches for single probe data will be presented first to illustrate how the pefd is associated with smce similarity data from multiple probes in similar configurations will be then applied to demonstrate the broad applicability of this method finally soil moisture pc an event feature will be added to strengthen the results 3 1 single probe results the historical data from south center probe is split into 134 events and generates 3968 in season pefd criteria after applying the apriori algorithm 965 association rules were found some examples of the results are listed in table 2 the first sample same pefd as introduced in section 2 4 is interpreted as that such a pefd has an 81 8 probability to find a match this association rule occurred 0 226 out of 965 that is if a comparison of two smces fits this pefd there is an 81 8 probability the two smces are a matched pair which indicates they show a similar pattern of change similarly the second sample indicates that all relationships under its pefd can be categorized as unmatched the third pefd is 75 probability to identify a matched smce pair based on the lift values in this example sample 2 is the strongest rule the first pefd events are visualized in fig 9 the larger points indicate the start of different smces the series between two start points are the soil moisture observations for a single smce the green smce is the targeted event from which the pefd with all other events are generated see fig 8 matched events are colored as red since the targeted event is formed by an initial rise a peak and a long recession all the matched events are in a similar change phase there are two events in orange classified as ambiguous the one in 2015 only contains a rise while the one in 2016 has a relative long and extra intensive drop the grey events indicate the smces either out of season or not in the pefd of interest in fig 10 the targeted event can be described as a monotonic increase and the events of the pefd 2 are all colored as unmatched in blue because they are either monotonic decrease or have a longer decrease phase in fig 11 the events of pefd 3 have all categories exhibiting a monotonic decrease targeted smce around apr 8 the unmatched events are almost all monotonic increase except one of them changes relatively flat the short smce precedent to may 1 all the other events with heavy weight on drop are categorized as match the only ambiguous event does have a long drop phase while its increase phase is too significant to be ignored 3 2 multiple probes results the data from all probes used in this method includes 370 smces which generates 1177 association by the apriori algorithm rules of pefd 1 in multi probe scenario are listed in table 3 with larger sample size by including data from multiple probes the association rules involved all relationships instead of only two in fig 9 in addition the accuracy shown in fig 12 seems improved with larger size of training data for example the smce of south center probe around 2015 10 28 were labeled unmatched instead of ambiguous in the single probe analysis in fig 9 this event is a monotonic increase that is completely different from the target by including more data from other probes the similarity of this smce to the targeted smce has been downgraded which matches human understanding better 3 3 event feature pc the goal of this method is to find the pefd with high confidence or associated with smces in similar pattern however in fig 12 the smces around 2015 10 28 bear a monotonic increase that is very different from other matched events with only environmental features it is hard to distinguish these events from others thus confidence in its association is low nevertheless with the addition of event features such as pc multiple granular pefds with higher confidence can be generated for example in fig 13 those monotonic increase smces are found to obtain a pc difference higher than 20 percentile this dimension increase in pefd brings a better association to the event similarity than only use environmental features 4 discussion the association rules reveal a useful relationship between different smces the change phase in the targeted events could be identified clearly from the match events and starting fading out from the ambiguous ones and eventually under weighed or even disappeared from the unmatched events the framework of matching smces by tracking the association rules mined from the pefd is practically informative to announcing the relationships the confidence value returned by the apriori method represents the probability of the interested pefd to be linked with the associated similarity based on the historical records more specifically if a pefd has a high confidence value association rule to be match it could be comprehended in two ways one if the targeted smce is known valid the match events are more likely to be valid on another hand if a new event matched with any valid historical event by such a pefd it is more likely to be a valid event thus a new event could be judged as valid or not by only checking its pefd to all historical events without calculating the dtw distance which is computationally costly a future area of exploration to improve the performance of this method would be including more features into pefd such as autocorrelation between adjacent events yet more factors could lead to more scattered pefds which could lower the support value of an association rule for example in fig 13 pefd 1 was broken into two granular ones with less occurrences for both with a low support value it is hard to conclude an association rules are statistically reliable even its confidence value is high this suggests that the support value is also important to determine if an association rule is practically usable this could be achieved by setting up a threshold of support value in the apriori method but this value may vary based on different sample size and number of factors in pefd a strategy to use the results of this method could be established in a conservative way a threshold of confidence value and certain relationship could be pre defined in the apriori algorithm so that only the high confidence association rules would be saved in the results these results paired with a confidence threshold allow for the determination of either valid or invalid smces with configurable precision and recall it could be seen from the examples shown previously pefd 1 and 2 were both with confidence value higher than 80 and no opposite relationship among their matched event pairs however with confidence value 75 in pefd 3 there exists multiple events not reflecting the change phase of the targeted event and falling in the category of unmatched it would be much easier and more accurate to just use pefd 1 and 2 in determining the relationship of an event pair than including pefd 3 by setting up a confidence threshold of 80 a minimum lift value can also be used to only consider strong associations by ruling out the pefd conditions not better than the whole population a demonstration of such strategy is applied to the test dataset shown in fig 14 the association rules linking pefds to match mined from the training data is selected by confidence higher than 60 and lift higher than 1 5 the test smces other than the grey ones are found to correspond to different pefd the unconfident to match events grey show that unusual characteristics such as flat shape e g events at the tail of the test data uncommon soil moisture change in the middle of an smce e g test event start near 2016 12 01 granular event partitioning by precipitation e g events near 2016 9 01 could result in difficulties finding a corresponding pefd yet high confident matches show little trend to indicate if match confidence is positively related to event characteristics e g length fluctuation or value etc for example a typical soil moisture change responding to a rain event can be found between 2016 and 07 15 and 2016 08 01 with 75 confidence blue an 85 confidence event purple with a long flat tail sits between 2016 11 01 and 2016 12 01 and a 70 confidence event green with a significant drop is located near 2017 01 01 this is because the method focuses on comparing the general shape between smces regardless of the general position of the value fig 15 is presented to show this shape comparison by focusing on certain typical smces fig 15 a as references to reprocess all calculations these events begin with a short and quick soil moisture increase triggered by precipitation followed by a long slow decrease during a dry period fig 15 b illustrates the pefd match confidence of test events due to fewer pefds generated by only referencing certain typical events the confidence of pefd match is different from fig 14 and more unconfident to match events grey can be found similarity comparison is conceptually visualized by zooming into some sample periods two events observed by the south probe between 2016 and 07 20 and 2016 07 28 are shown against to the references in terms of standardized values in fig 15 c although with differences in event length and soil moisture change intensity the general reference event shape can be found from both events event south100 observed prior to south101 changes more intensely in a short period but follows the reference event shape more closely event south101 has an increasing tail end which is only represented by one of the three reference events since this increasing tail end weighs more in south101 than in the reference event the confidence of south101 is lowered moreover two events observed by the northctr probe between 2016 and 12 25 and 2017 1 8 are compared to the references in fig 15 d it should be noted that all events are standardized to emphasize their general shapes without value bias thus despite the significant drop in fig 15 b event northctr130 can be represented by a z shape closing to northctr129 in fig 15 d since the drying of the soil in the reference events could be generally embodied in both northctr129 and northctr130 in fig 15 d their confidences are also high such shape match can be found from many events after the standardization process and is helpful in comparing the general shape between smces which is also a key point in this paper the example in fig 15 also indicates that instead of using all historical data in qaqc training for detecting abnormal changes certain typical smce change patterns can be refined to build a model bearing higher accuracy in detecting uncommon patterns this could be more practically useful for a real project since it saves the degree of model complexity and reduces training time from the perspective of applying qaqc on real time in data monitoring this method is computationally more efficient than only use time series comparison methods such as dtw embedded in the association rules and their corresponding support confidence and lift values the relationship between the physical based pefd and the dtw distance resulted by the apriori method could be statistically interpreted once the sample size of the historical data is larger enough this statistical relationship would be reliable in determining validity of a new event for example in fig 12 the data from multiple probes helped downgrade the similarity relationships of the ambiguous smces of the single probe scenario to unmatched which is closer to the reality given the limited data this method can be treated as a pilot experiment however once applying this method on a site with multiple sensors or a monitoring network with multiple sites the restriction of sample size could be overcome by lumping all data together with nearby location and similar configurations the soil moisture data even logged by different sensors in different locations can be learned to extract the association rules that are generally applicable different from other black box machine learning algorithms such as decision trees svm and neural networks etc the association rules provide the similarity between two smces under a combination of multiple feature conditions with explainable statistical results these rules are also portable to other sensors or locations and more understandable to engineers and decision makers than black box models overall the similarity score calculated by dtw could be used as a shape likelihood indicator in comparing two events by learning the associations between dtw scores and other environmental and event features in the historical data the similarity between two events could be predicted without performing dtw calculation which is computational intensive although not in the scope of this method the value difference between events could be identified using traditional qaqc methods by focusing on individual points hence when combining traditional qaqc methods with the method introduced in this paper the overall qaqc can be complete 5 conclusion in this paper the apriori association rule learning method is applied to detecting the soil moisture probe anomaly response pattern in a green roof located in uwm golda meir library in milwaukee wi the sequential soil moisture data and other climatic characteristics are split into smces and converted into physical based pefd the similarity between two smces are defined by all dtw distances and their summaries the apriori algorithm is then employed to find the association rules between pefd and events similarity the results show that with a good balance between the number of features and the thresholds of support and confidence values reliable accurate and comprehensive association rules can be found in all the association rule learning algorithm outputs understandable statistical results in detecting soil moisture probe anomaly response patterns from historical observations and physical characteristics not only for single probe data this method could be potentially broadly applicable to multi sensor or sensor network data as well this method could also apply on other time series information to improve the data quality further improvement and applicability of the method in performing a qaqc on time series data are also discussed in this paper the key of getting high reliability accuracy and applicability of this method is to achieve a balance of selecting factors in the pefd and the thresholds of support and confidence values in apriori method a secondary paper about the extensive usage of this method is under preparation it will include an sensitivity analysis to optimize the feature selection and information categorization by using different methods to categorize feature difference using different thresholds in labeling dtw scores into matched and unmatched and testing the performance of different feature combinations the results will be applied in other machine learning algorithms to explore its adaptability acknowledgements this research was supported by optirtc inc we thank them for sharing us the data that initiated this study the insights and expertise they provided greatly assisted in building the method we also thank for the comments that greatly improved the manuscript appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 04 001 
26388,in situ field monitoring networks generate vast quantities of continuous data can help to improve the design management operation and maintenance of green infrastructure gi systems however such actions require efficient and reliable quality assurance quality control qaqc in this paper we develop a rule based learning algorithm involving dynamic time warping dtw to investigate the feasibility of detecting anomalous responses from soil moisture probes using data collected from a gi site in milwaukee wi as an enhancement to traditional qaqc methods which rely on individual time steps this method converts the continuous time series into event sequences from which response patterns can be detected association rules are developed on both environmental features and event features the results suggest that this method could be used to identify abnormal change patterns as compared to intra site historical observations though developed for soil moisture this method could easily be extended to apply on other continuous environmental datasets keywords qaqc association rule learning green infrastructure anomalous pattern detection dynamic time warping 1 introduction in situ monitoring with sensors is increasingly being used to improve the design and management of green infrastructure gi systems in urban areas also creating new opportunities for scientific discovery porter et al 2009 at both private and public sites for example digiovanni et al 2010 characterized the hydrologic performance of a small scale green roof with data collected from a custom designed lysimeter a rain gage and soil moisture sensors carson et al 2013 performed a similar study using an onset hobo u30 weather station with a tipping bucket rain gauge and a custom designed weir berretta et al 2014 investigated the et characteristics of a green roof by tracking the soil moisture content and temperature using campbell scientific cs616 water content reflectometer the potential for deriving new insights from these efforts work is however contingent upon good quality data a goal not always easy to achieve given the ease with which data streams can be corrupted hill et al 2009 in the harsh environmental conditions found in gi systems erroneous data frequently occurs in hydro meteorological monitoring sciuto et al 2009 and poor quality data may even significantly hinder analysis inhibit modeling and lead to poor decision making madsen 1989 new approaches to automate quality assurance and quality control qaqc are thus necessary to service the expanding volume of data collected by growing sensors networks national science foundation 2005 overall the purpose of qaqc is to detect the anomalies by both time steps e g an unsensational value and events e g odd response series with strong autocorrelation traditional qaqc protocols emphasize detection of anomalies and outliers for individual time points only and can broadly classified into three categories the first is the record limit check method which compares individual sensor values to a pair of upper and lower limits for example durre et al 2010 used global temperature precipitation snowfall and other extremes to flag sensor data that was physically impossible because it fell outside the range of values observed anywhere on the earth the second includes a family of statistical methods that define outliers as sensor values that fall in the far head or tail of the frequency distribution at a specific location and time a z score derived from the mean and standard deviation of the normalized data values can be used for example to estimate how likely a sensor value is to be an outlier by setting a specific confidence threshold z score exceedances can be used to flag outliers hubbard and you 2005 kunkel et al 2005 such methods are extensions of traditional statistical process control techniques and assume that the variability in the underlying phenomena is constant over time e g stationary such that deviations from historical distributions indicate anomalous behaviors such assumption do not always hold in gi systems where natural processes such as soil erosion or human included perturbation such as irrigation can lead to gradual and abrupt changes in the field the third and most recent category of approaches are machine learning methods unsupervised machine learning methods such as k means have become very popular for outlier detection bolton and hand 2001 bayesian methods such as dynamic bayesian networks dbns identify anomalies by dynamically tracking changes in the historical data hill et al 2009 supervised learning methods such as neural networks kozma et al 1994 lpez lineros et al 2014 support vector machines svm buluta et al 2005 and decision trees john 1995 can classify sensor values as either valid or invalid based on the properties of a training dataset to date application of these methods has principally focused on each time stamped data point frequently underestimating the autocorrelation inherent to the time series data as mentioned previously qaqc methods should scrutinize both individual points and point series in an event one short come of traditional machine learning methods is lack the ability to identify autocorrelation in patterns of change within and between events to enhance the traditional methods to complete qaqc similarity comparisons on patterns or shapes need to be applied across the time series events generated by sensors using gi monitoring as an example the qaqc process should align with our physical understanding of a gi site s rainfall runoff response infiltration characteristics as well as the surrounding microclimatic limits of evapotranspiration a first step would confirm whether the raw observations are responding consistently to historical change patterns few if any existing qaqc methods in hydrology monitoring address this need this paper aims to improve data quality in terms of detecting unusual change series patterns in situations where historical observations are of inadequate quality the approach includes a rule based machine learning algorithm to investigate the feasibility of detecting anomalous response patterns from soil moisture probes within a single site and across multiple sites with similar configurations the data used to demonstrate the technique are collected at a gi site in milwaukee wi to enhance traditional qaqc methods which focus on outlier detection on a single time step without considering the autocorrelation across multiple time points this new method splits the whole data set into chunks based on precipitation events and employs dynamic time warping dtw to align and compare events at different lengths the paired event distances calculated by dtw are generalized to categorize the similarity between events other more stable observed site phenomena are also included in the model to reflect the physical conditions that underlie the association rules 2 methodology 2 1 data and site the monitored gi site is a green roof located in uwm golda meir library in milwaukee wi the site is about 1350 m2 and has a growth media depth of 0 1524 m it is monitored by optirtc optirtc com using soil moisture probes installed at the bottom of the growth media decagon ec 5 soil moisture sensor with hobo link data logger three soil moisture probes are installed at different locations at the site see fig 1 instant precipitation and running median temperature are also recorded by a climate station located in the same site these data were collected at 5 min intervals from april 2016 to january 2017 a polynomial calibration curve was developed to scale the raw data assuming the most saturated observations were 100 saturated while the least were considered 0 saturated the time period used to perform the calibration represents only a small segment of the entire time series allowing us to adjust the calibration ranges later on the overall goal is to maximize the value that can be obtained from imperfect soil moisture data sets data quality varied between the different sensors overall the probe located at south center location yielded the most consistent change pattern while the responses of the other two probes displayed a significant drift as shown in fig 2 raw values obtained from the probe installed at north center dropped suddenly in january 2017 beginning in august 2016 the probe located at the south side shifted response pattern the analysis utilizes all data from the south center probe the data gathered prior to december 2016 from the north center probe and the data gathered before august 2016 from the south side probe to derive the rules from which pattern similarity is evaluated the remainder of the data will be used to test the accuracy of abnormal pattern detection because many physical e g flow directions and anthropogenic e g irrigation regime factors are not known for this site the learning algorithms cannot be tested against a water mass balance for the site however one key benefit of the approach is that such information is not needed the algorithm detects anomalies in the data based solely on similarity between the tested data with patterns observed in the training set 2 2 method process the processes forming up the algorithm are illustrated in fig 3 the raw soil moisture data will be firstly smoothed to mitigate all noises to maximize the quality in the following processes the results are then separated into soil moisture change event smce series based on an inter event dry period each smce will be centralized by its mean value to mitigate the bias from initial soil moisture status in calculating the following dtw distances for all possible smce pairs these distances are then used as a basis to categorize the similarity of each smce pair meanwhile a paired event feature difference pefd is generated for each smce pair finally both the similarity category and pefd of all smce pairs are used to conduct an apriori algorithm to investigate the association rules from pefd to similarity 2 3 noise mitigation the phenomena we are ultimately interested in is the change in soil moisture however soil moisture changes due to diurnal climatic fluxes such as evapotranspiration and condensation can under the right conditions mask smaller amplitude fluctuations due to precipitation events in this context regular diurnal soil moisture variability can be conceived as noise that must first be removed from the time series to extract the soil moisture response we are interested here especially with a sampling interval of five minutes such noise could cover up the principle change pattern for short fluctuation events when evaluating time series similarity to maximize accuracy of the approach noise from all sources must be mitigated prior to attempting to align events moving average method kendall et al 1983 is used for noise mitigation the effectiveness of this approach is shown in fig 4 where the blue curve depicts the raw observation and the red one shows the smoothed trend the red curve is more stable and represents a more general pattern of soil moisture change next the whole series is split into discrete wet and dry events using a six hour inter event dry period driscoll et al 1989 each precipitation event and its successive dry period is then combined as a single smce in fig 4 the starting point of each smce is identified with a dot 2 4 series alignment and distance calculation distance based methods such as dtw have been used to compare the similarity between two time series in multiple dimensions zhuang and chen 2006 as an intermediate step for aligning time series in different lengths hu et al 2003 dtw berndt and clifford 1994 finds the optimal alignment between different time series this approach is widely used in speech processing keogh 2002 sections in the sequences for comparison are considered stretchable and compressible although the optimal alignment between these sequences searched by dtw are one to one between sections time steps are usually aligned as one to many based on the optimal alignment a distance measure is calculated to score the similarity between two sequences fig 5 illustrates the point to point soil moisture alignment between two sample smces in the monitored data the initial segments of the two series are aligned linked but begin to diverge further as time progresses becoming open unlinked this divergence occurs because the first point of each smce pair always follows the dry period of the previous event whereas during the subject rain events soil moisture patterns differ due to the intra storm distribution of rainfall moreover because different storms have different dry period durations there is no justification for linking the end points of different smce pairs this phenomenon is demonstrated graphically in fig 5 where the end point of the black line fell somewhere in the middle of the red line the similarity distance as computed by dtw is defined by the sum of the lengths of all the point to point alignment lines the length of these lines is influenced by both the temporal pattern of the smce response and the initial soil moisture value at the beginning of the event two smce s could have the same temporal pattern but still have a large similarity distance if one occurs after a long dry period and the other after a short dry period similarly two smces with different temporal patterns could have short similarity distances because they both began at similar moisture status time steps to correct for these tendencies in the procedure all smces in the training data set were compared to one another after a pre process of centralization by mean and scaled by standard deviation so that the similarity of the general change patterns can be scored using the distance calculated by dtw these dtw results are then summarized and analyzed to estimate a threshold for defining similarity all these distances were analyzed to categorize the relationship extent of similarity between different smces as shown in fig 6 the distance unit less distribution of the generally matched smce pairs is lower in both mean and standard deviation of the distance than for the unmatched pairs however for the generally unmatched pairs the distance distribution has a higher average and standard deviation any smce pairs with an intermedium average or high standard deviation could be classified either matched or unmatched and are thus flagged as ambiguous starting with each smce as a reference the procedure next computes the standard deviation of the distances between that smce and all others as well as the average their values are plotted against one another in fig 7 the generally matched low mean low standard deviation and generally unmatched high mean low standard deviation relationships are displayed at the lower left corner and upper left corner respectively principle component analysis pca is then used to extract the first two principle components pc along these directions both relationships can be determined by reaching the far end points given few reference a 5  value is selected as a threshold percentile to differentiate these relationships from the ambiguous ones finally the averages of the dtw distances of these two categories labeled in fig 7 are employed as the thresholds to define the paired event relationships if the dtw distance between any two smces are less than the average dtw distance of the general matched category their relationship is defined as match on the other hand if this distance is larger than the average dtw distance of the general unmatched category their relationship is defined as unmatched the region between these two thresholds defines events classified as ambiguous 2 5 paired event feature difference pefd besides the time series distance computed by dtw other physical state and event phenomena such as event duration air temperature and precipitation intensity and quantity enhance the algorithm s skill in evaluating the similarity of any two events although the pre processing conducted prior to dtw mitigates the potential for bias in the soil moisture values the nonsensical possibility to match a winter smce to a summer smce with similar change pattern remains to invalidate such a match a paired event feature difference pefd approach is invoked where julian date average temperature and the other physical characteristics listed below are included as the pefd between two smces for training event duration rain period duration dry period duration event precipitation amount soil moisture time series first principle component to characterize inter smce similarity each event in the training period is again compared with all others once an event is chosen as a reference all physical characteristics differences except julian date and soil moisture pc are categorized into five bins based on their quantile 20 40 60 80 and 100 the granularity of the categories is a function of the sample size research topic result application etc in this case 20 percentile bins were sufficient to demonstrate the method the julian date information is specifically added for seasonality consideration a season is defined as 90 days given four seasons a year if the difference of julian date between two smces is larger than 45 days it is defined as out of season otherwise in season smces that meet this pefd are extracted for further analysis different from other environmental features soil moisture time series first pc indicates the general change direction which is a feature of the smce itself given the difference of soil moisture pc is generally an indicator of the dtw distance absolute pc difference is categorized based on the whole population additionally the absolute soil moisture pc differences are divided evenly into 5 categories because the long tails in its distribution may be underestimated in quantile bins the impacts of both environmental features and event features on association rules will be investigated the following is an example of how pefd criteria are used to find similar smces with environmental features as shown in table 1 this sample pefd includes average temperature difference below 20 percentile dry period duration difference between 40 and 60 percentile event duration difference between 40 and 60 percentile rain period duration difference above 80 percentile and event precipitation difference above 80 percentile as shown in fig 8 a targeted smce is colored pink in all charts fig 8a e illustrate the difference categories of all features of its nearby in season smces the feature difference categories of the targeted smce and its paired smces contain 0 20 20 40 and 40 60 for average temperature 40 60 and 60 80 for dry period and event duration 80 100 for event precipitation and rain period based on the sample pefd the smces to which the targeted smce matches are colored in purple in fig 8f while the blue ones do not meet the pefd criteria it should be noted that even some close in terms of time and similar in terms of event precipitation difference and rain period difference nearby smces may not always be linked to the targeted smce due to the other pefd criteria for example the events prior to the targeted smce in fig 8f does not match the pefd in the features of average temperature and event duration although its change pattern looks similar 2 6 association rule learning association rule learning is a popular data mining method used intensively in marketing analysis for discovering interesting relations between different variables in large databases the apriori algorithm is a classic approach for performing association rule learning it works by mining frequent item sets agrawal and srikant 1994 market analysts use it to find the transaction rules e g 80 of customers who bought wine also bought cheese in hydrology related fields nandagopal et al 2010 used this approach to predict weather conditions using multi station atmospheric data guo et al 2004 forecasted disaster weather by using the algorithm to investigate the relationship between weather elements and natural events for the present study the apriori algorithm is employed to find the association rules determining the likelihood that two smces are match unmatched or ambiguous categories the algorithm generates three important output values support confidence and lift the support value indicates the portion of events in the whole training population that meet the specific pefd the confidence shows the true probability of this association rule given a specific pefd it is important to note that a pefd is a complete set of all pefd criteria defined previously in other words a pefd containing only a portion of all criteria is not considered appropriate to evaluate smce similarity the lift value compares the conditional match probability to the probability of this similarity in the whole population specifically with a unity lift the condition of interest does nothing better to strengthen the association rule than the whole general population an association is strong only when its lift greater than 1 3 results the results are presented in three stages using rules based on the environmental features only the skill of the apriori algorithm to find matches for single probe data will be presented first to illustrate how the pefd is associated with smce similarity data from multiple probes in similar configurations will be then applied to demonstrate the broad applicability of this method finally soil moisture pc an event feature will be added to strengthen the results 3 1 single probe results the historical data from south center probe is split into 134 events and generates 3968 in season pefd criteria after applying the apriori algorithm 965 association rules were found some examples of the results are listed in table 2 the first sample same pefd as introduced in section 2 4 is interpreted as that such a pefd has an 81 8 probability to find a match this association rule occurred 0 226 out of 965 that is if a comparison of two smces fits this pefd there is an 81 8 probability the two smces are a matched pair which indicates they show a similar pattern of change similarly the second sample indicates that all relationships under its pefd can be categorized as unmatched the third pefd is 75 probability to identify a matched smce pair based on the lift values in this example sample 2 is the strongest rule the first pefd events are visualized in fig 9 the larger points indicate the start of different smces the series between two start points are the soil moisture observations for a single smce the green smce is the targeted event from which the pefd with all other events are generated see fig 8 matched events are colored as red since the targeted event is formed by an initial rise a peak and a long recession all the matched events are in a similar change phase there are two events in orange classified as ambiguous the one in 2015 only contains a rise while the one in 2016 has a relative long and extra intensive drop the grey events indicate the smces either out of season or not in the pefd of interest in fig 10 the targeted event can be described as a monotonic increase and the events of the pefd 2 are all colored as unmatched in blue because they are either monotonic decrease or have a longer decrease phase in fig 11 the events of pefd 3 have all categories exhibiting a monotonic decrease targeted smce around apr 8 the unmatched events are almost all monotonic increase except one of them changes relatively flat the short smce precedent to may 1 all the other events with heavy weight on drop are categorized as match the only ambiguous event does have a long drop phase while its increase phase is too significant to be ignored 3 2 multiple probes results the data from all probes used in this method includes 370 smces which generates 1177 association by the apriori algorithm rules of pefd 1 in multi probe scenario are listed in table 3 with larger sample size by including data from multiple probes the association rules involved all relationships instead of only two in fig 9 in addition the accuracy shown in fig 12 seems improved with larger size of training data for example the smce of south center probe around 2015 10 28 were labeled unmatched instead of ambiguous in the single probe analysis in fig 9 this event is a monotonic increase that is completely different from the target by including more data from other probes the similarity of this smce to the targeted smce has been downgraded which matches human understanding better 3 3 event feature pc the goal of this method is to find the pefd with high confidence or associated with smces in similar pattern however in fig 12 the smces around 2015 10 28 bear a monotonic increase that is very different from other matched events with only environmental features it is hard to distinguish these events from others thus confidence in its association is low nevertheless with the addition of event features such as pc multiple granular pefds with higher confidence can be generated for example in fig 13 those monotonic increase smces are found to obtain a pc difference higher than 20 percentile this dimension increase in pefd brings a better association to the event similarity than only use environmental features 4 discussion the association rules reveal a useful relationship between different smces the change phase in the targeted events could be identified clearly from the match events and starting fading out from the ambiguous ones and eventually under weighed or even disappeared from the unmatched events the framework of matching smces by tracking the association rules mined from the pefd is practically informative to announcing the relationships the confidence value returned by the apriori method represents the probability of the interested pefd to be linked with the associated similarity based on the historical records more specifically if a pefd has a high confidence value association rule to be match it could be comprehended in two ways one if the targeted smce is known valid the match events are more likely to be valid on another hand if a new event matched with any valid historical event by such a pefd it is more likely to be a valid event thus a new event could be judged as valid or not by only checking its pefd to all historical events without calculating the dtw distance which is computationally costly a future area of exploration to improve the performance of this method would be including more features into pefd such as autocorrelation between adjacent events yet more factors could lead to more scattered pefds which could lower the support value of an association rule for example in fig 13 pefd 1 was broken into two granular ones with less occurrences for both with a low support value it is hard to conclude an association rules are statistically reliable even its confidence value is high this suggests that the support value is also important to determine if an association rule is practically usable this could be achieved by setting up a threshold of support value in the apriori method but this value may vary based on different sample size and number of factors in pefd a strategy to use the results of this method could be established in a conservative way a threshold of confidence value and certain relationship could be pre defined in the apriori algorithm so that only the high confidence association rules would be saved in the results these results paired with a confidence threshold allow for the determination of either valid or invalid smces with configurable precision and recall it could be seen from the examples shown previously pefd 1 and 2 were both with confidence value higher than 80 and no opposite relationship among their matched event pairs however with confidence value 75 in pefd 3 there exists multiple events not reflecting the change phase of the targeted event and falling in the category of unmatched it would be much easier and more accurate to just use pefd 1 and 2 in determining the relationship of an event pair than including pefd 3 by setting up a confidence threshold of 80 a minimum lift value can also be used to only consider strong associations by ruling out the pefd conditions not better than the whole population a demonstration of such strategy is applied to the test dataset shown in fig 14 the association rules linking pefds to match mined from the training data is selected by confidence higher than 60 and lift higher than 1 5 the test smces other than the grey ones are found to correspond to different pefd the unconfident to match events grey show that unusual characteristics such as flat shape e g events at the tail of the test data uncommon soil moisture change in the middle of an smce e g test event start near 2016 12 01 granular event partitioning by precipitation e g events near 2016 9 01 could result in difficulties finding a corresponding pefd yet high confident matches show little trend to indicate if match confidence is positively related to event characteristics e g length fluctuation or value etc for example a typical soil moisture change responding to a rain event can be found between 2016 and 07 15 and 2016 08 01 with 75 confidence blue an 85 confidence event purple with a long flat tail sits between 2016 11 01 and 2016 12 01 and a 70 confidence event green with a significant drop is located near 2017 01 01 this is because the method focuses on comparing the general shape between smces regardless of the general position of the value fig 15 is presented to show this shape comparison by focusing on certain typical smces fig 15 a as references to reprocess all calculations these events begin with a short and quick soil moisture increase triggered by precipitation followed by a long slow decrease during a dry period fig 15 b illustrates the pefd match confidence of test events due to fewer pefds generated by only referencing certain typical events the confidence of pefd match is different from fig 14 and more unconfident to match events grey can be found similarity comparison is conceptually visualized by zooming into some sample periods two events observed by the south probe between 2016 and 07 20 and 2016 07 28 are shown against to the references in terms of standardized values in fig 15 c although with differences in event length and soil moisture change intensity the general reference event shape can be found from both events event south100 observed prior to south101 changes more intensely in a short period but follows the reference event shape more closely event south101 has an increasing tail end which is only represented by one of the three reference events since this increasing tail end weighs more in south101 than in the reference event the confidence of south101 is lowered moreover two events observed by the northctr probe between 2016 and 12 25 and 2017 1 8 are compared to the references in fig 15 d it should be noted that all events are standardized to emphasize their general shapes without value bias thus despite the significant drop in fig 15 b event northctr130 can be represented by a z shape closing to northctr129 in fig 15 d since the drying of the soil in the reference events could be generally embodied in both northctr129 and northctr130 in fig 15 d their confidences are also high such shape match can be found from many events after the standardization process and is helpful in comparing the general shape between smces which is also a key point in this paper the example in fig 15 also indicates that instead of using all historical data in qaqc training for detecting abnormal changes certain typical smce change patterns can be refined to build a model bearing higher accuracy in detecting uncommon patterns this could be more practically useful for a real project since it saves the degree of model complexity and reduces training time from the perspective of applying qaqc on real time in data monitoring this method is computationally more efficient than only use time series comparison methods such as dtw embedded in the association rules and their corresponding support confidence and lift values the relationship between the physical based pefd and the dtw distance resulted by the apriori method could be statistically interpreted once the sample size of the historical data is larger enough this statistical relationship would be reliable in determining validity of a new event for example in fig 12 the data from multiple probes helped downgrade the similarity relationships of the ambiguous smces of the single probe scenario to unmatched which is closer to the reality given the limited data this method can be treated as a pilot experiment however once applying this method on a site with multiple sensors or a monitoring network with multiple sites the restriction of sample size could be overcome by lumping all data together with nearby location and similar configurations the soil moisture data even logged by different sensors in different locations can be learned to extract the association rules that are generally applicable different from other black box machine learning algorithms such as decision trees svm and neural networks etc the association rules provide the similarity between two smces under a combination of multiple feature conditions with explainable statistical results these rules are also portable to other sensors or locations and more understandable to engineers and decision makers than black box models overall the similarity score calculated by dtw could be used as a shape likelihood indicator in comparing two events by learning the associations between dtw scores and other environmental and event features in the historical data the similarity between two events could be predicted without performing dtw calculation which is computational intensive although not in the scope of this method the value difference between events could be identified using traditional qaqc methods by focusing on individual points hence when combining traditional qaqc methods with the method introduced in this paper the overall qaqc can be complete 5 conclusion in this paper the apriori association rule learning method is applied to detecting the soil moisture probe anomaly response pattern in a green roof located in uwm golda meir library in milwaukee wi the sequential soil moisture data and other climatic characteristics are split into smces and converted into physical based pefd the similarity between two smces are defined by all dtw distances and their summaries the apriori algorithm is then employed to find the association rules between pefd and events similarity the results show that with a good balance between the number of features and the thresholds of support and confidence values reliable accurate and comprehensive association rules can be found in all the association rule learning algorithm outputs understandable statistical results in detecting soil moisture probe anomaly response patterns from historical observations and physical characteristics not only for single probe data this method could be potentially broadly applicable to multi sensor or sensor network data as well this method could also apply on other time series information to improve the data quality further improvement and applicability of the method in performing a qaqc on time series data are also discussed in this paper the key of getting high reliability accuracy and applicability of this method is to achieve a balance of selecting factors in the pefd and the thresholds of support and confidence values in apriori method a secondary paper about the extensive usage of this method is under preparation it will include an sensitivity analysis to optimize the feature selection and information categorization by using different methods to categorize feature difference using different thresholds in labeling dtw scores into matched and unmatched and testing the performance of different feature combinations the results will be applied in other machine learning algorithms to explore its adaptability acknowledgements this research was supported by optirtc inc we thank them for sharing us the data that initiated this study the insights and expertise they provided greatly assisted in building the method we also thank for the comments that greatly improved the manuscript appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 04 001 
26389,a promising way to address the growing demand for water supply and improve the liveability of cities is to invest in decentralised multifunctional urban water technologies however the adoption of multifunctional water technologies is a complex issue that requires cross disciplinary approaches this paper uses an agent based model that integrates economic and environmental factors to explore and simulate the decision making and interactions of two types of agents a regulator and households the model is applied to evaluate strategies to increase the adoption of rainwater tanks in a suburb of melbourne a city that has often suffered from severe droughts the model was able to replicate the uptake of rainwater tanks by households for 2005 2014 the period known as the millennium drought results indicate that using economic instruments alone may have been insufficient to promote the adoption of rainwater tanks and that water restrictions have had a major impact on the uptake keywords agent based model sustainable urban water systems environmental policies rainwater harvesting technology adoption software availability name of software dynamind toolbox v0 11 5 developers dr christian urich contact details dr christian urich address 23 college walk bld 60 clayton campus clayton vic 3800 australia email christian urich monash edu phone 61 3 990 51562 availability online for free download at https github com iut ibk dynamind toolbox cost free year first available 2013 hardware required 32 bit x86 or 64 bit x64 processor software required c python gdal programming language c python program size 37 2 mb 1 introduction by 2050 66 of the world population is expected to live in cities compared to 30 in 1950 united nations 2014 the number of urban residents affected by perennial water shortage is likely to reach 1 1 billion by 2050 from an estimated 150 million in 2000 due to climate change land use change and demographic growth mcdonald et al 2011 one in four large cities globally are currently water stressed due to geographical and financial limitations mcdonald et al 2014 and cities in dry areas are likely to suffer from lower rainfall and greater need for infrastructure due to the combined effect of climate change and population growth mcdonald et al 2011 green infrastructure and decentralised water solutions can provide a reliable and fit for purpose additional source of water while delivering additional benefits such as flood reduction heat mitigation and amenity de haan et al 2014 however the diffusion of decentralised water technologies is hampered by various barriers such as uncertainty around costs and benefits for both policy makers and technology adopters fragmentation of government responsibilities and resistance to change roy et al 2008 for this reason there is a need to investigate the behaviour of stakeholders involved in the installation of technologies there is also a need for better evaluation of the dynamics of the urban water socio technical system modelling of the dynamics can improve the understanding of the impact of different policy interventions e g incentives or regulations to facilitate technology diffusion agent based modelling has been used to explore stakeholder interactions and decision making mller et al 2013 in particular such models have been applied to investigate the processes of adoption of novel technologies kiesling et al 2012 rai and robinson 2015 jensen and chappin 2017 and to simulate transition of socio technical systems bergman et al 2008 lopolito et al 2013 holtz et al 2015 in the urban water sector agent based models have mostly been used to simulate urban water demand and supply tillman et al 1999 2001 ducrot et al 2004 athanasiadis et al 2005 zellner 2007 moglia et al 2010 kanta and zechman 2013 linkola et al 2013 yuan et al 2014 koutiva and makropoulos 2016 darbandsari et al 2017 mashhadi ali et al 2017 and to explore the adoption of water appliances chu et al 2009 galn et al 2009 jensen and chappin 2017 but few authors have investigated the adoption of decentralised water supply or stormwater technologies schwarz and ernst 2009 montalto et al 2013 giacomoni and berglund 2015 kandiah et al 2017 and the modelled results have yet to be compared with observed uptake the impact of policy interventions on the adoption of household scale technologies has been explored in other fields for instance on the uptake of energy efficient vehicles schwoon 2006 mueller and de haan 2009 querini and benetto 2014 silvia and krause 2016 decentralised energy supply technologies faber et al 2010 zhao et al 2011 sopha et al 2013 palmer et al 2015 or smart meters zhang and nuttall 2011 rixen and weigand 2014 vasiljevska et al 2017 for decentralised water technologies only montalto et al 2013 explored the impact of economic incentives on the adoption of green roofs and raingardens to address these gaps and better understand the influence of different policy interventions on the uptake of decentralised water technologies we present in this paper an empirically grounded model to explore the decision making and interactions of two types of stakeholders i e households and a regulator with the following objectives to develop a model able to simulate the uptake of decentralised and multifunctional water technologies focusing in the first instance on rainwater tanks to validate the model using the observed data on installation of rainwater tanks in a suburb of melbourne australia to understand the sensitivity of model results to the variation of uncertain parameters to explore the main drivers key factors in the uptake of rainwater tanks the paper is organised as follows first we provide the rationale for developing the model and review the main determinants of rainwater tank uptake then we describe the model components and processes as well as the method used for validating the model finally we present the performance of the model and key findings before concluding with a discussion on the advantages limitations and potential future applications of the model 2 case study 2 1 melbourne and the millennium drought the model stems from a severe drought that occurred in melbourne victoria known as the millennium drought which lasted from 1995 until 2009 record low rainfall and dam inflow to the main reservoir for several years consecutively triggered the search for alternative water resources low et al 2015 this included seawater desalination recycling of harvested stormwater and recycling of treated wastewater ferguson et al 2013 the state government committed 10 million aud over 4 years to provide an incentive for victorians connected to a reticulated water supply system to conserve water khastagir and jayasuriya 2011 for instance the victorian government has recognised the importance of plumbing rainwater tanks for non potable use e g to toilet and or washing machine and offered rebates for the installation of rainwater tanks connected to houses media coverage and awareness campaigns promoting reduction of residential water use were initiated in an attempt to curb residential water demand hurlimann and dolnicar 2012 ferguson et al 2013 which accounts for 65 of total water demand in the city melbourne water 2016b however two years prior to the end of the drought the construction of a desalination plant was announced for a price of 4 9 billion aud to provide 150 gl of potable water per year porter et al 2015 the drought ended with high rainfall and flash flooding events occurred in several parts of the city in the following years and the desalination plant was not used ferguson et al 2013 following the end of the drought the expensive desalination plant construction highlighted the need to improve the understanding of the impact of different policy instruments on the decision making of households regarding the uptake of flexible and multifunctional decentralised water technologies to avoid technological lock in 2 2 rainwater tanks uptake several factors may influence the decision of households to install a rainwater tank economic factors including the water savings benefits the costs the net benefit and payback period of tanks have been investigated in several cities tam et al 2010 rahman et al 2012 devkota et al 2015 however households may not act entirely upon economic factors other social cultural and behavioural factors such as risk and threat perception trust and education may influence the acceptance and adoption of decentralised water systems mankad and tapsuwan 2011 the main reasons for installing a rainwater tank highlighted by the last census australian bureau of statistics 2013 were to save water avoid water restrictions on mains water and to save on water costs from a survey with 1425 households in the illawarra region australia delaney and fam 2015 found that water restrictions changed the emotional experience of using alternative water sources for outdoor uses mankad et al 2013 concluded that the main determinants for installing a rainwater tank as a response to water shortage in south east queensland were the efficacy of tanks to address the water shortage threat the appraisal of water shortage as a threat and the costs related to tanks this response to water shortage depends not only on the intensity of water shortage but also on the water supply options available for example lindsay et al 2016 found that residents in melbourne perth and brisbane reacted differently despite being similarly affected by water shortage and restrictions the behaviour of households regarding water conservation triggered by a sense of water crisis was observed in brisbane and melbourne whereas water conservation behaviour was negligible in perth due to access to water sources from desalination plants and household bores lindsay et al 2016 as a result rainwater tank uptake reached 31 and 47 in 2013 in melbourne and brisbane respectively compared to only 9 3 in perth australian bureau of statistics 2013 normative factors or social influence which have been used to simulate the diffusion of innovation through social networks in previous agent based models rai and robinson 2015 manson et al 2016 were not found to be a major factor of adoption australian bureau of statistics 2010 2013 mankad et al 2013 based on this set of literature the main determinants for the installation of a rainwater tank in this model are the water savings benefits the costs of tanks and the willingness to pay wtp of households to avoid water restrictions depending on the restriction level these three factors can be influenced by the regulator agent through different policy instruments 3 model development we present in this section the model developed to represent the influence of different policy interventions on the behaviour of households regarding rainwater tank uptake the agent based modelling approach was chosen over other types of technology diffusion models such as equation based rao and kishore 2010 or system dynamics models tigabu et al 2015 because of the capacity to define decision rules at the household level an 2012 and the disaggregated nature of building scale technology adoption this heterogeneity facilitates the inclusion of the main determinants of rainwater tank uptake as reviewed in the last section in the decision making process of households and reproduces the impact of policy interventions on these key drivers of uptake and the decision of households furthermore agent based models offer a flexible approach to integrate socio demographic dynamics with biophysical models filatova et al 2013 in the case of water technology adoption this approach facilitates the linkage of sub models of households water demand rain tank water balance and households decision making although we concluded from the review of literature that social influence or normative factors were not a main influence of rainwater tank uptake in this particular case a spatial agent based modelling approach offers a flexible tool to implement such social interaction model if it is considered as an important factor in a different context e g samaddar et al 2014 for the description of the model we follow an adapted version of the odd overview design concepts and details protocol grimm et al 2006 2010 we start with a description of the key assumptions and equations used in the model i e the agents state variables and rules followed by the modelling platform and process overview finally the initialisation and input data are presented in the following section see appendix 1 table a 1 for an in depth description of the model following the odd protocol 3 1 agents variables and rules 3 1 1 type of agents the model simulates the behaviour of two types of agents that are key to the adoption of rainwater tanks as discussed below the regulator agent an umbrella agent encompassing different entities with an objective to provide public benefits such as reliable water supply through regulation and economic instruments in the model the regulator includes the water utility and the victorian department of sustainability and environment which can regulate on water restrictions and water pricing and offer rebates on rainwater tanks respectively the household agent the decision making process of the household consists in evaluating the monetary benefit from water savings the costs after rebates of different options of tanks as well as to which extent a tank can reduce the threat of water shortage or restrictions the decision of households to install a rainwater tank is based on the net present value npv of tanks which results from these three key variables described in the next section to measure the npv for different types of tanks for each household the costs and benefits are measured for six options of tanks based on literature and government policies consisting of three volumes 2 5 and 10 kl and two types of uses tanks for outdoor use tou i e tanks used only for garden irrigation and tanks for combined outdoor indoor use tcu i e for toilet laundry and outdoor uses together the benefit of tanks is measured as a sum of the water saving benefit and the willingness to pay wtp to avoid water restrictions whereas costs include total costs of tanks less the available rebates the equations and rules used for calculation of npv and wtp are discussed below 3 1 2 key variables the installation costs of tou and tcu ic u v y include the costs of the tank c v y plumbing pb y and installation inst y adjusted for each simulation year y and volume v using the consumer price index cpi and depend on the use u of the tank u 0 for outdoor usage and u 1 for combined usage the ic u v y of tcu also includes the cost of a pump pp y necessary to provide pressure for non potable uses such as toilet and laundry we assume that a pump is not necessary for tanks used for outdoor purposes based on tam et al 2010 although we acknowledge that pumps can occasionally be used for garden irrigation it is estimated that the pump needs to be replaced after 10 years of the tank installation tam et al 2010 therefore we add a second pump discounted at the 10th year following the tank installation because tcu are likely to require more work for plumbing and installation marsden jacob associates 2009 a coefficient for installation and plumbing cip is included to lower these two costs for tou 1 i c u v y c v y i n s t y p b y c i p u 0 c v y i n s t y p b y p p y p p y d 10 u 1 a discount factor d n is calculated for each year n of the life time of the tank t using the discount rate r 2 d n n 1 t 1 1 r n the maintenance costs are estimated separately for different types of tanks for outdoor tanks the discounted maintenance cost of tou mc u y only includes annual maintenance cost amc y which may include different activities such as cleaning gutters checking water quality or removing sediments in tank moglia et al 2014 previous studies estimating the npv of tanks have used a fixed operational cost typically 0 05 aud kl additional to the annual maintenance cost for tanks plumbed to the house tam et al 2010 khastagir and jayasuriya 2011 gato trinidad and gan 2014 however more recent work has decomposed this operational cost to include energy usage energy cost and water use from tanks gurung et al 2016 we therefore measure the maintenance cost for tcu mc i u y for u 1 with the following parameters to assess the additional pump operation cost annual non potable and outdoor water savings for each household i ws i u y energy cost ec y at year y and energy use e 3 m c i u y n 1 t d n a m c y u 0 n 1 t d n a m c y d n w s i u y e c y e u 1 this means that as opposed to the maintenance cost of tou which is fixed the maintenance cost of tcu varies according to the households water savings and on energy price the present value of tank costs pvc i u v y is measured as the sum of the installation cost and the annual maintenance cost for each volume and use at year y 4 p v c i u v y i c u v y m c i u y the present value of water savings benefits pvb i u v y was estimated to include the potential water saved from rainwater tanks ws i u v y the price of potable water p y and the wtp of households to avoid water restrictions wtp i 5 p v b i u v y n 1 t w s i u v y p y d n w t p i d n the water savings from tou ws i u v y for u 0 only include outdoor water savings whereas savings provided by tcu ws i u v y for u 1 include outdoor and non potable water savings additionally to the economic benefit of water savings rainwater tanks act as a protection from water shortage threat mankad et al 2013 to estimate the value of this attribute of protection against water shortage we use the benefit function transfer methodology this method consists in estimating economic values of a good or service by transferring a function calibrated with the attributes of people in a previous study kaul et al 2013 we estimate the wtp to avoid water restriction wtp i l y as a share s l of the total annual water bill of each household b i y based on the restriction level in place l by transferring a function developed by hensher et al 2006 in canberra australia an assumption was made that observations made in canberra are valid for application in other cities in australia according to their survey of 211 residential respondents households are only willing to pay to avoid restrictions that matter i e restrictions that apply all year every day of the week and for restriction level l 3 or higher 6 w t p i l y b i y s l l 3 0 l 3 a water demand sub model was developed to calculate the water use and savings of households fully based on roberts et al 2011 the sub model that follows a rather standard approach used in water supply modelling practice takes the parcel garden and roof areas the number of persons living in the household and daily rainfall events as input parameters outdoor and indoor non potable and potable water demands are returned as outputs of the sub model and used to assess the potential water savings from the six rainwater tank options greywater and black water generated by each household are also used to estimate the sewer usage charge to measure the total annual water bill for more details on the estimation of the total water bill in melbourne see appendix 2 3 1 3 decision rule for uptake of rainwater tanks the npv of tanks npv i u v y is then measured as the difference between the present values of the benefits and the costs 8 n p v i u v y p v b i u v y p v c i u v y finally the total value on which households take their decision total npv i u v y is based on the sum of the npv of tanks and the rebates offered by the regulator 9 t o t a l n p v i u v y n p v i u v y r u y v households decide to install a tank if the total npv i u v y of one of the six options is above 0 and will choose the option with the highest value 3 2 modelling platform and process overview the model was developed within the dance4water dynamic adaptation for enabling city evolution for water framework and is implemented in the dynamind software dynamind allows the integration of modules working tasks or building blocks within a simulation and facilitates the simulation of urban water systems in time and space urich et al 2012 general modules are written in c and python whereas the modules referring to the regulator and households decision making are exclusively written in python there are three main modules in the simulation 1 a water demand sub model 2 a policy instruments module and 3 a households decision module fig 1 an additional module representing the feedback of tank uptake on the water balance and the regulator s decision to apply water restrictions was developed but not implemented for the validation of rainwater tank uptake since historical restrictions levels were used for the simulation the water demand is first calculated for all households taking into account daily rainfall and the attributes of households and parcels the six tank options are created for each household with the potential water savings measured from the water demand and roof attributes in the next module the regulator agent updates the policy instruments according to observed policies in melbourne the three policies available to the regulator agent water prices restrictions rebates influence the three key variables for the decision making of households water savings wtp and tank costs respectively considering the updated policies households measure the npv of each tank option and install the tank with the highest npv if this npv is above 0 the attributes of tank options and installed tanks are written in sqlite database files before the simulation starts again at the following simulation step the computing time required for one model run is approximately 2 hours on a dell intel core i5 4210u 2 40 ghz processor and simulations were carried out using the cloud computing services at monash university 4 validation of the model for a suburb of melbourne validation of agent based models is critical to insure the reliability of model results to inform decision making rai and henry 2016 well known methods to validate or characterise the performance of models include using residuals by comparing and analysing observed data and model results bennett et al 2013 however a recurrent issue with agent based simulations of technology adoption is the lack of observed data to measure the property of residuals given the limited amount of data we aim to validate the model by comparing model results on tank uptake with available data and by conducting uncertainty analysis to assess the range of results 4 1 the scotchman s creek catchment the model was applied to the scotchman s creek catchment the location of the case study can be found in appendix 3 fig a 1 southeast of melbourne cbd for the period 2005 2014 when most of the rainwater tank uptake occurred in the city the catchment has an area of approximately 10 36 km2 and a population of around 25 000 residents the model includes 10 187 household agents which represents a scaling down of approximately 1 136 considering the total number of households across the city of melbourne 1 391 900 australian bureau of statistics 2016a household agents were initialised using 2011 census data from the australian bureau of statistics we assume that there has not been a significant development in this area and therefore the urban development within the case study location was not simulated the number of people living in each household was estimated by distributing the number of people living in each statistical area to the number of buildings in these areas the initial percentage of households with a rainwater tank was estimated at 10 from the earliest estimates from data on melbourne 13 in 2007 australian bureau of statistics 2013 and to account for a slight increase from 2005 to 2007 as already discussed to simulate the wtp to avoid water restrictions we used the share of the total annual water bill according to the restriction level from hensher et al 2006 following their study in canberra australia however there were differences between the way restrictions have been implemented in the two cities the city of canberra had 5 levels of water restrictions at the time of the research whereas melbourne had 4 levels southeast water corporation 2012 during the simulation period table 1 outlines how the model was applied for melbourne 4 2 model set up and inputs 4 2 1 policy instruments policy instruments in place in melbourne for the simulation period were used to define the regulator s behaviour fig 2 restriction levels are decided by the water utility melbourne water and have a range of 0 4 melbourne water 2016a an additional level 3a was added in 2007 but level 4 the highest restriction level has never been implemented fig 2a the restriction levels followed the storage level and rainfall patterns fig 2d annual rainfall was especially low from 2006 to 2009 at which time it remained under 500 mm per year compared to the long term annual average of ca 650 mm per year yarra valley water is the corporation responsible for providing water supply and sewerage services in the case study area water supply charges are composed of three levels of water consumption per household per day less than 440 l between 440 and 880 l and greater than 880 l fig 2b furthermore yarra valley water 2016 determines the sewerage drainage and park charges that are used to measure the total annual water bill of households rebates were set up according to two rebate programs offered by the victorian department of sustainability and environment the water smart gardens and homes rebate scheme wsgh from 2003 to 2011 victorian government department of sustainability and environment 2007 and the living victoria water rebate program that ran from 2012 to 2015 state government of victoria 2015 the wsgh scheme included rebates for tanks of any size of 150 aud starting in 2003 with an additional 150 aud if the tank was connected to a toilet system and was terminated in 2007 the second phase of the wsgh scheme from 2007 to 2011 consisted in rebates for tanks plumbed to houses only with different rebates depending on the volumes of tanks 500 aud for less than 5 kl if connected to toilet and or laundry 900 aud for 5 kl and above connected to toilet or laundry and 1000 aud for 5 kl and above connected to toilet and laundry because we looked at tanks plumbed to houses without distinction of the indoor uses the rebates for 5 kl and 10 kl plumbed tanks were averaged to 950 aud fig 2c the living victoria water rebate program increased the rebates of smaller tanks less than 5 kl to 850 aud if connected to toilet and or laundry and larger tanks 5 kl and above to 1300 aud if connected to toilet or laundry and 1500 aud if connected to toilet and laundry again the rebates for large plumbed tanks were averaged to 1400 aud 4 2 2 rainwater tanks variables the cost of tanks was derived from 486 tanks of various shapes slimline round and underground and materials plastic steel and polyethylene on sale from the websites of five manufacturers in melbourne and the cost of pumps was obtained from the websites of four retailers the costs of tanks and pumps advertised are in 2016 aud and were therefore adjusted for each year of the simulation using the cpi for melbourne australian bureau of statistics 2016b more details on the costs of tanks and pumps can be found in appendix 4 fig a 3 other ancillary tank expenses installation plumbing and maintenance were obtained from tam et al 2010 table 2 marsden jacob associates 2009 estimated the installation and plumbing costs of tou at between 40 and 70 cheaper than tcu we therefore used 0 55 for the initial value of cip for the base case simulation the lifespan of rainwater tanks was assumed to be 20 years and the interest rate for discounting was estimated at 5 tam et al 2010 energy usage charges were taken from united energy origin energy 2016 an electricity provider for the case study area and adjusted with the cpi of electricity in melbourne australian bureau of statistics 2016b energy consumption from water pumps for non potable indoor uses can vary widely a literature review conducted by vieira et al 2014 showed that pumping energy intensity for rainwater harvesting ranged from 0 12 to 2 10 kwh kl for single storey detached residential buildings over 10 studies with a median of 1 40 kwh kl we therefore used this median value for the initialisation of the model 4 3 evaluation of the model 4 3 1 comparison to the available data on the uptake of rainwater tanks the observed percentage of households with a rainwater tank in the city of melbourne is available for three years 2007 2010 and 2013 from the australian bureau of statistics 2013 and provides the most reliable variable to compare with model results the percentage of households that installed a tank in the case study area was thus compared with the observed percentage in melbourne the latest census 2013 also includes the proportion of rainwater tanks that are plumbed to the house i e that are used for indoor and outdoor purposes although the sizes of tanks are not included in the census data results from a previous survey carried out with 413 respondents in melbourne moglia et al 2014 were used to compare with model results the authors divided installed tanks among 13 categories of sizes from less than 0 5 kl to greater than 20 kl which were used to compare with the installation of small 2 kl medium 5 kl and large 10 kl tanks from the model we present the model results on the proportion of rainwater tanks that are plumbed to the house and the sizes of tanks but these two variables are not used to validate the model as more observed data would be needed to compare model results with observed patterns 4 3 2 uncertainty assessment we conducted monte carlo mc sampling to run 5000 simulations to assess the uncertainty of model results and their sensitivity to variation in input values additionally to uncertain tank parameters we ran simulations with a range of the following parameters initial rainwater tank percentage init rwht wtp as a share of the water bill s l and energy use e table 2 because of the lack of knowledge around the distribution of these parameters we sampled from uniform distributions for initialisation in order to measure the sensitivity and uncertainty of the model to rainfall and water demand a coefficient was added to simulations for non potable demand outdoor demand and rainfall table 2 a value of 1 was used for the base case and a range of 0 8 1 2 for mc simulations 5 results and discussion 5 1 model validation of rainwater tank uptake the base case simulation with initial values is able to replicate the two main uptake periods seen from the observed data in 2010 and in 2013 fig 3 a major uptake occurs in 2009 due to the implementation of level 3 restrictions and a higher wtp to avoid water restrictions the second uptake in 2013 can be attributed to higher water prices combined with high rebates on tcu the median value of the monte carlo simulations 12 1 25 9 and 28 4 of households with a rainwater tank for years 2007 2010 and 2013 follows a similar pattern to the base case simulation 11 24 8 27 6 reproducing the pattern of observed values 11 6 28 2 and 31 1 for the same years however the uncertainty of results is relatively large with 90 between 5th and 95th percentiles of results in 2013 ranging from 12 9 to 71 8 of uptake 5 2 simulation of rainwater tanks characteristics the model was used to simulate the different types of tanks installed i e the usage and volumes due to the lack of observed data to validate the types of tanks we do not use these results for validating the model but to provide additional information on the functionality of the model if sufficient data become available for validation 5 2 1 tank usage the proportion of installed tcu was measured from the simulations and was compared to only one observation 29 in 2013 available from the australian bureau of statistics 2013 fig 4 shows the proportion of tanks plumbed to houses the base case simulation provides an accurate estimate of the proportion in 2013 32 3 of tanks plumbed to houses whereas the median value from simulations overestimates the proportion of plumbed tanks 35 8 however it should be noted that these model results concern only tanks installed after the start of the simulations i e from 2005 whereas the observed datum from the australian bureau of statistics includes all tanks i e tanks installed before 2005 estimated to 33 of total installed tanks and after the percentage of tanks plumbed to houses installed during the simulation is thus likely to range between 0 assuming that all tanks were plumbed prior to the simulation period and 50 assuming that none of the tanks were plumbed prior to the simulation period due to low rebates and lower water price it is more likely that a low proportion of tanks were plumbed to houses prior to the simulation the range of model outputs is very large with 90 of results ranging from between 0 62 and 96 of plumbed tanks in 2013 this is likely due to the uncertainty in the parameter cip coefficient for installation and plumbing that captures the difference between installation and plumbing costs of tcu compared to tou coefficient for installation and plumbing because of a lack of information about this cost difference we used a fairly large range 40 70 for this parameter based on marsden jacob associates 2009 which can explain the resulting high uncertainty in the type of tanks installed therefore there is a high uncertainty related to the ability of the model to replicate the proportion of tanks plumbed to houses 5 2 2 tank volumes the volumes of installed tanks indicate that smaller tanks 2 kl are preferred to larger tanks 5 and 10 kl fig 5 there is no official data for the volumes of installed tanks but model results are consistent with the observation from a previous study conducted by moglia et al 2014 who undertook a survey of 417 households that installed a tank across melbourne it was found that smaller tanks 0 5 3 kl accounted for a majority of tanks 64 followed by medium sized tanks 3 7 kl 25 and larger tanks 7 kl 11 this pattern is represented in the current model as the mean fraction of 2 kl tanks at the end of the simulation 2014 is 54 of all installed tanks compared to 32 for 5 kl tanks and 14 for 10 kl tanks the location of parcels with a tank installed shows that the first tanks were installed in 2007 but the major uptake occurs in 2009 at which time a majority of tanks are 2 kl tanks the spatial location of installed tanks of different volumes is represented in appendix 3 fig a 2 5 3 model sensitivity we conducted a sensitivity assessment using scatter plots fig 6 to investigate the correlation between the various uncertain factors and output variables table 3 the correlation between the coefficient to assess the difference in additional tanks costs installation and plumbing between tou and tcu cip and the proportion of tcu is especially high at 0 67 a higher value of the cip parameter smaller difference in costs makes tou less competitive and leads to a greater proportion of tanks for combined use prop combined this high correlation between the two parameters explains the high uncertainty of the tank usage observed in fig 4 the cip parameter is also positively correlated with the proportion of larger tanks prop 5 kl and prop 10 kl and negatively correlated with the proportion of smaller tanks prop 2 kl thus with a low cip value the cost of tanks becomes an important factor of decision making and cheaper usage outdoor use and volume 2 kl are preferred table 3 the wtp of households to avoid water restrictions as the share of the total water bill s i l has the opposite impact on model results it has the highest correlation 0 42 with the tank uptake percentage perc rwht of all non policy parameters which means that a higher share of the total water bill leads to a higher wtp to avoid water restrictions and a greater uptake it is also positively correlated with smaller tanks prop 2 kl and negatively correlated with the proportion of tcu prop combined and the proportion of larger tanks prop 5 kl and prop 10 kl this may be caused by the wtp being activated in early years of the simulation 2007 2009 at which time rebates and water prices are less advantageous for tcu than the last years of the simulation mainly 2012 2014 indoor demand outdoor demand rainfall and most costs parameters have low correlations with all output variables only pump costs pp are negatively correlated with the proportion of tcu and 5 kl tanks 0 31 for both and positively correlated with 2 kl tanks 0 26 5 4 the main drivers for rainwater tank uptake the npv for the three volumes of tank options i e tanks that have not yet been installed and the two uses combined and outdoor use only over the length of the base case simulation is represented in fig 7 a this npv includes water savings and the wtp to avoid water restrictions to measure rainwater tank benefits but excludes rebates a significant increase of the npv occurring during the most severe time of the drought 2007 2009 is caused by the wtp of households to avoid water restrictions as the restriction level was raised from 2 to 3 in 2007 the present value shows that without rebates tank uptake would only occur during years of water restriction equal to or above 3 earlier years of the simulation show a consistently more beneficial npv for tou than for tcu in the second half of the simulation the npv for tcu raises compared to tou due to higher prices of water and therefore higher water savings benefit when rebates are included in the npv fig 7b the gap between larger 5 and 10 kl tcu and tou is reduced from the first rebate program that started in 2007 and larger tcu become more beneficial from 2009 onwards from the last rebate scheme introduced in 2012 a significant gap separates the two types making larger tanks plumbed to the house preferable 5 4 1 willingness to pay to avoid restrictions as demonstrated by the npv of tanks during years of restrictions of level 3 the wtp to avoid water restrictions has an important impact on the behaviour of households the wtp is based on the total annual water bill fig 8 a and is only taken into account in the decision making for three years of the simulation from 2007 to 2009 fig 8b the total annual water bill is composed of water supply and sewer charges both fixed and per usage as well as fixed park and drainage charges fig 8c for a description of the measurement of total water bill see appendix 2 the median total annual water bill increases at an average annual growth of 6 4 from 2005 433 aud to 2008 522 9 aud and doubles by 2014 1080 aud with a strong annual growth rate 12 8 as a result the median wtp of households increases from 80 5 to 105 5 aud from 2007 to 2009 the willingness to avoid water restrictions was found to be an important factor of tank installation which is in agreement with previous surveys australian bureau of statistics 2013 mankad et al 2013 moglia et al 2014 previous findings from the literature suggest a large range of values for the wtp to avoid water restrictions respondents from a study in california indicated a wtp between 140 and 203 annually to avoid an occurrence of water restrictions in 1993 us dollars depending on the frequency and severity of the restrictions koss and khawaja 2001 griffin and mjelde 2000 examined customers preferences using contingent valuation methods and found that respondents from seven texan cities were willing to pay between 304 08 and 412 68 in 1997 us dollars on average to avoid water restrictions in australia studies have shown a significantly lower wtp for unrestricted water use for instance hurlimann and mckay 2005 found that the mean wtp of respondents in adelaide to be exempt from water restrictions was 8 6 aud annually with a standard deviation of 25 6 although the results included a high response rate of 0 in canberra residents were willing to pay a very small amount 10 aud to prevent a 10 per cent reduction in water use in 1997 australian dollars gordon et al 2001 thus the results from the current model show values below findings from united states studies but above australian studies 5 4 2 water savings the simulated water savings provided by installed tanks show higher results than previous findings from the literature the median water savings for households that installed a tcu of 5 kl was 87 kl and 113 3 kl for households with a 10 kl tank whereas no tcu of 2 kl were installed results from previous empirical and modelling studies have estimated the annual savings based for combined usage toilet laundry and irrigation from rainwater tanks between 38 5 burns et al 2014 and 59 5 kl per year rahman et al 2012 for a tank of 5 kl in this study the higher values from the model can be explained by the fact that households with the highest levels of water savings will choose the tank option with a npv above 0 which means the highest water saving furthermore although the median value for total annual water savings from the model is significantly higher than previous findings it is similar to an empirical study conducted in a nearby area in melbourne with 4391 households that have used a rainwater tank rebate during the drought gato trinidad and gan 2014 by comparing average annual water consumption per household from mains before the installation of tanks of 5 kl and larger with their water consumption after the tank installation the authors found that the average annual savings was 120 5 kl per household although other factors than rainwater tanks may have influenced this saving that occurred from july 2005 to june 2010 such as more efficient end use appliances water conservation behaviour and water restrictions rainwater tanks are likely to have had a large impact on the saving the median water saved for 2 kl tanks across the two uses corresponds to 19 of households total water demand 44 5 kl tanks and 59 for 10 kl fig 9 these figures are within the range of previous studies for example as reported by souza and ghisi 2012 i e up to 50 water saved from a 5 kl tank and up to 60 from a 10 kl tank depending on water demand roof area and annual rainfall 6 conclusions this paper described the development of an agent based model to replicate the uptake of decentralised water solutions and its application and validation in the context of rainwater harvesting in melbourne the tool was able to replicate the uptake of rainwater tanks regarding the number of installed tanks however model results showed a high uncertainty related to the percentage of tanks plumbed to houses due to the large difference in the installation and plumbing costs and more data would be needed to assess the capacity of the model to accurately replicate the type of tanks as a result the model may not be adequate with the currently available data to predict the impact of policy instruments on the types of installed tanks the model showed that for the major uptake to occur the three policy instruments had to be implemented at the same time or two policies at a high level high water price and high rebates on tcu furthermore the wtp to avoid water restrictions was only triggered at restrictions levels at which the use of sprinklers and the watering of lawns were not permitted therefore lower levels of restrictions may help to reduce water use but only have a marginal effect on the value of tanks as a water shortage protection tool for households this behaviour highlights the need for more than one policy instrument to be used in combination to trigger the uptake of residential rainwater harvesting systems this is significant in the context of melbourne where the rebate scheme has been terminated in 2015 the agent based approach allowed us to examine the impact of different policies on the behaviour of households because of its use of spatial data the model could be applied to test spatially targeted policy interventions as opposed to a majority of agent based models we have decided to omit the simulation of the interaction among households based on our literature review however because of the flexibility of the model and its spatial representation such interactions could be implemented if it is found as an important determinant of rainwater tank uptake the model could provide information for policy makers to enable the uptake of comparable decentralised technologies in developing cities that are likely to suffer from increasing drought frequency groundwater depletion and saltwater intrusion the technical suitability of rainwater tanks has been examined in several countries with arid climate by looking for example at the optimal sizing of tanks for the local climate but the behaviour of households regarding the installation of tanks is often overlooked the model could provide valuable insights into the combination of economic and regulatory instruments required to influence the behaviour of households by changing the value of tanks and thus trigger decentralised technology adoption and shape water management transition appendix 1 description of the model following the odd protocol table a1 odd protocol elements table a1 odd element implementation in the model purpose the purpose of the model is to simulate the uptake of rainwater tanks based on the cost of tanks the willingness to pay to avoid water restrictions and the water savings of households and with the influence of different policy instruments available to the policy maker entities there are two agents represented in the model households and a regulator encompassing the victorian department of sustainability and environment and the water utility the environment is represented by rainfall time series and there are no collectives of agents scales the temporal and spatial scales are described in section 4 1 process overview and scheduling the process overview and scheduling are presented in fig 1 basic principles it is assumed that households make their decisions based on the main determinants of rainwater tank adoption found from the australian australian bureau of statistics 2010 2013 mankad et al 2013 i e to save water to avoid water restrictions and to save on water costs the willingness to avoid water restrictions on mains water is estimated by the willingness to pay to avoid water restrictions calculated with the use of benefit transfer function from a study conducted in a similar setting in canberra australia hensher et al 2006 emergence the key results analysed from the emerging behaviour of households is the total number and the proportions of types and sizes of tanks installed after a given year adaptation household agents are not changing their decision making process during the simulation or not given adaptive traits objectives the objective of the household agent is to save money and protect itself from water shortage over time by evaluating the money saved from water savings the costs after rebates of different options of tanks as well as to which extent a tank can reduce the threat of water shortage learning the agents base their decision on the current year information and do not change their decision making process over time prediction households do not predict future savings but they assume that their water savings and maintenance costs will remain the same for the service life of the tank when they consider tank options sensing the decision making process of households is dependent on the three policies available to the regulator agent water price restriction rebates and the varying rainfall a module was developed in which the regulator adjusts the restriction level based on the water storage influenced by storage inflow and total water demand but the model was kept idle for the historical uptake households sense the variations in rainfall when measuring the water savings from the tank interaction there are no interaction among household agents as we have concluded from our literature review that the main determinants of tank uptake are to save water avoid water restrictions on mains water and to save on water costs as indicated in section 2 2 stochasticity stochasticity is introduced through the monte carlo sampling by sampling from uniform distribution for the uncertain variables shown in table 2 collectives there are no collectives of agents observation the output data are collected in sqlite files as indicated in fig 1 results on rainwater tank options for each household are used for analysis mainly npv water savings water bills and willingness to pay results are assessed for each household initialization initialisation is described in section 4 1 input data input data are described in table 2 sub models we use a water demand sub model described in section 3 1 2 appendix 2 measurement of total water bill in melbourne the components of total annual water bills vary among cities in melbourne the total bill b i y comprises a water usage charge measured as the total household water use wu i y multiplied by the water price p y and a sewer usage charge calculated as the annual amount of wastewater produced by each household sd i y multiplied by the treatment price of wastewater sp y the water bill also includes fixed annual charges for water supply sewerage drainage and park maintenance fw y fs y fd y fp y b i y w u i y p y s d i y s p y f w y f s y f d y f p y appendix 3 case study location fig a1 location of scotchman s creek catchment east of melbourne cbd fig a1 fig a2 spatial location of parcels with a rainwater tank in the case study location following the three main years of uptake 2007 2009 and 2013 fig a2 appendix 4 measurement of tank and pump costs fig a3 distribution of pump and tank costs for the three volumes of tanks used in the model boxplots of costs are shown for the five tank retailers and overall the median of all values is used for initialisation of the base case simulation for each volume tank costs were obtained from the websites of five retailers in melbourne asc water tanks tankworld kerrimuir rainwater products and g store n 267 the median value over all retailers is used for the three volumes 2 kl n 70 5 kl n 53 10 kl n 17 pump costs were obtained from the websites of four retailers in melbourne asc water tanks tankworld kerrimuir rainwater products oz tanks and g store n 534 with a median value of 600 aud fig a3 appendix 5 sensitivity of model results to input parameters table a2 spearman s rho for all parameters columns represent output variables and rows represent input parameters table a2 parameter perc rwht prop combined prop 2 kl prop 5 kl prop 10 kl y 0 65 0 15 0 00 0 10 0 21 l 0 17 0 12 0 01 0 07 0 17 r1 2 0 59 0 14 0 02 0 08 0 09 r1 5 0 59 0 14 0 02 0 08 0 09 r0 0 61 0 08 0 06 0 09 0 26 p 0 65 0 15 0 00 0 10 0 21 c2 0 01 0 02 0 02 0 02 0 02 c5 0 00 0 00 0 00 0 00 0 01 c10 0 00 0 00 0 00 0 00 0 00 pp 0 03 0 31 0 26 0 31 0 09 pb 0 14 0 01 0 05 0 01 0 14 amc 0 11 0 12 0 14 0 11 0 16 inst 0 09 0 02 0 04 0 01 0 09 cip 0 26 0 67 0 66 0 65 0 50 s 0 42 0 46 0 52 0 44 0 60 t 0 01 0 01 0 01 0 01 0 01 r 0 02 0 11 0 08 0 10 0 02 id 0 06 0 18 0 12 0 07 0 15 od 0 10 0 11 0 08 0 09 0 00 rf 0 01 0 03 0 03 0 03 0 02 init rwht 0 20 0 00 0 00 0 01 0 01 p 0 001 p 0 01 p 0 05 
26389,a promising way to address the growing demand for water supply and improve the liveability of cities is to invest in decentralised multifunctional urban water technologies however the adoption of multifunctional water technologies is a complex issue that requires cross disciplinary approaches this paper uses an agent based model that integrates economic and environmental factors to explore and simulate the decision making and interactions of two types of agents a regulator and households the model is applied to evaluate strategies to increase the adoption of rainwater tanks in a suburb of melbourne a city that has often suffered from severe droughts the model was able to replicate the uptake of rainwater tanks by households for 2005 2014 the period known as the millennium drought results indicate that using economic instruments alone may have been insufficient to promote the adoption of rainwater tanks and that water restrictions have had a major impact on the uptake keywords agent based model sustainable urban water systems environmental policies rainwater harvesting technology adoption software availability name of software dynamind toolbox v0 11 5 developers dr christian urich contact details dr christian urich address 23 college walk bld 60 clayton campus clayton vic 3800 australia email christian urich monash edu phone 61 3 990 51562 availability online for free download at https github com iut ibk dynamind toolbox cost free year first available 2013 hardware required 32 bit x86 or 64 bit x64 processor software required c python gdal programming language c python program size 37 2 mb 1 introduction by 2050 66 of the world population is expected to live in cities compared to 30 in 1950 united nations 2014 the number of urban residents affected by perennial water shortage is likely to reach 1 1 billion by 2050 from an estimated 150 million in 2000 due to climate change land use change and demographic growth mcdonald et al 2011 one in four large cities globally are currently water stressed due to geographical and financial limitations mcdonald et al 2014 and cities in dry areas are likely to suffer from lower rainfall and greater need for infrastructure due to the combined effect of climate change and population growth mcdonald et al 2011 green infrastructure and decentralised water solutions can provide a reliable and fit for purpose additional source of water while delivering additional benefits such as flood reduction heat mitigation and amenity de haan et al 2014 however the diffusion of decentralised water technologies is hampered by various barriers such as uncertainty around costs and benefits for both policy makers and technology adopters fragmentation of government responsibilities and resistance to change roy et al 2008 for this reason there is a need to investigate the behaviour of stakeholders involved in the installation of technologies there is also a need for better evaluation of the dynamics of the urban water socio technical system modelling of the dynamics can improve the understanding of the impact of different policy interventions e g incentives or regulations to facilitate technology diffusion agent based modelling has been used to explore stakeholder interactions and decision making mller et al 2013 in particular such models have been applied to investigate the processes of adoption of novel technologies kiesling et al 2012 rai and robinson 2015 jensen and chappin 2017 and to simulate transition of socio technical systems bergman et al 2008 lopolito et al 2013 holtz et al 2015 in the urban water sector agent based models have mostly been used to simulate urban water demand and supply tillman et al 1999 2001 ducrot et al 2004 athanasiadis et al 2005 zellner 2007 moglia et al 2010 kanta and zechman 2013 linkola et al 2013 yuan et al 2014 koutiva and makropoulos 2016 darbandsari et al 2017 mashhadi ali et al 2017 and to explore the adoption of water appliances chu et al 2009 galn et al 2009 jensen and chappin 2017 but few authors have investigated the adoption of decentralised water supply or stormwater technologies schwarz and ernst 2009 montalto et al 2013 giacomoni and berglund 2015 kandiah et al 2017 and the modelled results have yet to be compared with observed uptake the impact of policy interventions on the adoption of household scale technologies has been explored in other fields for instance on the uptake of energy efficient vehicles schwoon 2006 mueller and de haan 2009 querini and benetto 2014 silvia and krause 2016 decentralised energy supply technologies faber et al 2010 zhao et al 2011 sopha et al 2013 palmer et al 2015 or smart meters zhang and nuttall 2011 rixen and weigand 2014 vasiljevska et al 2017 for decentralised water technologies only montalto et al 2013 explored the impact of economic incentives on the adoption of green roofs and raingardens to address these gaps and better understand the influence of different policy interventions on the uptake of decentralised water technologies we present in this paper an empirically grounded model to explore the decision making and interactions of two types of stakeholders i e households and a regulator with the following objectives to develop a model able to simulate the uptake of decentralised and multifunctional water technologies focusing in the first instance on rainwater tanks to validate the model using the observed data on installation of rainwater tanks in a suburb of melbourne australia to understand the sensitivity of model results to the variation of uncertain parameters to explore the main drivers key factors in the uptake of rainwater tanks the paper is organised as follows first we provide the rationale for developing the model and review the main determinants of rainwater tank uptake then we describe the model components and processes as well as the method used for validating the model finally we present the performance of the model and key findings before concluding with a discussion on the advantages limitations and potential future applications of the model 2 case study 2 1 melbourne and the millennium drought the model stems from a severe drought that occurred in melbourne victoria known as the millennium drought which lasted from 1995 until 2009 record low rainfall and dam inflow to the main reservoir for several years consecutively triggered the search for alternative water resources low et al 2015 this included seawater desalination recycling of harvested stormwater and recycling of treated wastewater ferguson et al 2013 the state government committed 10 million aud over 4 years to provide an incentive for victorians connected to a reticulated water supply system to conserve water khastagir and jayasuriya 2011 for instance the victorian government has recognised the importance of plumbing rainwater tanks for non potable use e g to toilet and or washing machine and offered rebates for the installation of rainwater tanks connected to houses media coverage and awareness campaigns promoting reduction of residential water use were initiated in an attempt to curb residential water demand hurlimann and dolnicar 2012 ferguson et al 2013 which accounts for 65 of total water demand in the city melbourne water 2016b however two years prior to the end of the drought the construction of a desalination plant was announced for a price of 4 9 billion aud to provide 150 gl of potable water per year porter et al 2015 the drought ended with high rainfall and flash flooding events occurred in several parts of the city in the following years and the desalination plant was not used ferguson et al 2013 following the end of the drought the expensive desalination plant construction highlighted the need to improve the understanding of the impact of different policy instruments on the decision making of households regarding the uptake of flexible and multifunctional decentralised water technologies to avoid technological lock in 2 2 rainwater tanks uptake several factors may influence the decision of households to install a rainwater tank economic factors including the water savings benefits the costs the net benefit and payback period of tanks have been investigated in several cities tam et al 2010 rahman et al 2012 devkota et al 2015 however households may not act entirely upon economic factors other social cultural and behavioural factors such as risk and threat perception trust and education may influence the acceptance and adoption of decentralised water systems mankad and tapsuwan 2011 the main reasons for installing a rainwater tank highlighted by the last census australian bureau of statistics 2013 were to save water avoid water restrictions on mains water and to save on water costs from a survey with 1425 households in the illawarra region australia delaney and fam 2015 found that water restrictions changed the emotional experience of using alternative water sources for outdoor uses mankad et al 2013 concluded that the main determinants for installing a rainwater tank as a response to water shortage in south east queensland were the efficacy of tanks to address the water shortage threat the appraisal of water shortage as a threat and the costs related to tanks this response to water shortage depends not only on the intensity of water shortage but also on the water supply options available for example lindsay et al 2016 found that residents in melbourne perth and brisbane reacted differently despite being similarly affected by water shortage and restrictions the behaviour of households regarding water conservation triggered by a sense of water crisis was observed in brisbane and melbourne whereas water conservation behaviour was negligible in perth due to access to water sources from desalination plants and household bores lindsay et al 2016 as a result rainwater tank uptake reached 31 and 47 in 2013 in melbourne and brisbane respectively compared to only 9 3 in perth australian bureau of statistics 2013 normative factors or social influence which have been used to simulate the diffusion of innovation through social networks in previous agent based models rai and robinson 2015 manson et al 2016 were not found to be a major factor of adoption australian bureau of statistics 2010 2013 mankad et al 2013 based on this set of literature the main determinants for the installation of a rainwater tank in this model are the water savings benefits the costs of tanks and the willingness to pay wtp of households to avoid water restrictions depending on the restriction level these three factors can be influenced by the regulator agent through different policy instruments 3 model development we present in this section the model developed to represent the influence of different policy interventions on the behaviour of households regarding rainwater tank uptake the agent based modelling approach was chosen over other types of technology diffusion models such as equation based rao and kishore 2010 or system dynamics models tigabu et al 2015 because of the capacity to define decision rules at the household level an 2012 and the disaggregated nature of building scale technology adoption this heterogeneity facilitates the inclusion of the main determinants of rainwater tank uptake as reviewed in the last section in the decision making process of households and reproduces the impact of policy interventions on these key drivers of uptake and the decision of households furthermore agent based models offer a flexible approach to integrate socio demographic dynamics with biophysical models filatova et al 2013 in the case of water technology adoption this approach facilitates the linkage of sub models of households water demand rain tank water balance and households decision making although we concluded from the review of literature that social influence or normative factors were not a main influence of rainwater tank uptake in this particular case a spatial agent based modelling approach offers a flexible tool to implement such social interaction model if it is considered as an important factor in a different context e g samaddar et al 2014 for the description of the model we follow an adapted version of the odd overview design concepts and details protocol grimm et al 2006 2010 we start with a description of the key assumptions and equations used in the model i e the agents state variables and rules followed by the modelling platform and process overview finally the initialisation and input data are presented in the following section see appendix 1 table a 1 for an in depth description of the model following the odd protocol 3 1 agents variables and rules 3 1 1 type of agents the model simulates the behaviour of two types of agents that are key to the adoption of rainwater tanks as discussed below the regulator agent an umbrella agent encompassing different entities with an objective to provide public benefits such as reliable water supply through regulation and economic instruments in the model the regulator includes the water utility and the victorian department of sustainability and environment which can regulate on water restrictions and water pricing and offer rebates on rainwater tanks respectively the household agent the decision making process of the household consists in evaluating the monetary benefit from water savings the costs after rebates of different options of tanks as well as to which extent a tank can reduce the threat of water shortage or restrictions the decision of households to install a rainwater tank is based on the net present value npv of tanks which results from these three key variables described in the next section to measure the npv for different types of tanks for each household the costs and benefits are measured for six options of tanks based on literature and government policies consisting of three volumes 2 5 and 10 kl and two types of uses tanks for outdoor use tou i e tanks used only for garden irrigation and tanks for combined outdoor indoor use tcu i e for toilet laundry and outdoor uses together the benefit of tanks is measured as a sum of the water saving benefit and the willingness to pay wtp to avoid water restrictions whereas costs include total costs of tanks less the available rebates the equations and rules used for calculation of npv and wtp are discussed below 3 1 2 key variables the installation costs of tou and tcu ic u v y include the costs of the tank c v y plumbing pb y and installation inst y adjusted for each simulation year y and volume v using the consumer price index cpi and depend on the use u of the tank u 0 for outdoor usage and u 1 for combined usage the ic u v y of tcu also includes the cost of a pump pp y necessary to provide pressure for non potable uses such as toilet and laundry we assume that a pump is not necessary for tanks used for outdoor purposes based on tam et al 2010 although we acknowledge that pumps can occasionally be used for garden irrigation it is estimated that the pump needs to be replaced after 10 years of the tank installation tam et al 2010 therefore we add a second pump discounted at the 10th year following the tank installation because tcu are likely to require more work for plumbing and installation marsden jacob associates 2009 a coefficient for installation and plumbing cip is included to lower these two costs for tou 1 i c u v y c v y i n s t y p b y c i p u 0 c v y i n s t y p b y p p y p p y d 10 u 1 a discount factor d n is calculated for each year n of the life time of the tank t using the discount rate r 2 d n n 1 t 1 1 r n the maintenance costs are estimated separately for different types of tanks for outdoor tanks the discounted maintenance cost of tou mc u y only includes annual maintenance cost amc y which may include different activities such as cleaning gutters checking water quality or removing sediments in tank moglia et al 2014 previous studies estimating the npv of tanks have used a fixed operational cost typically 0 05 aud kl additional to the annual maintenance cost for tanks plumbed to the house tam et al 2010 khastagir and jayasuriya 2011 gato trinidad and gan 2014 however more recent work has decomposed this operational cost to include energy usage energy cost and water use from tanks gurung et al 2016 we therefore measure the maintenance cost for tcu mc i u y for u 1 with the following parameters to assess the additional pump operation cost annual non potable and outdoor water savings for each household i ws i u y energy cost ec y at year y and energy use e 3 m c i u y n 1 t d n a m c y u 0 n 1 t d n a m c y d n w s i u y e c y e u 1 this means that as opposed to the maintenance cost of tou which is fixed the maintenance cost of tcu varies according to the households water savings and on energy price the present value of tank costs pvc i u v y is measured as the sum of the installation cost and the annual maintenance cost for each volume and use at year y 4 p v c i u v y i c u v y m c i u y the present value of water savings benefits pvb i u v y was estimated to include the potential water saved from rainwater tanks ws i u v y the price of potable water p y and the wtp of households to avoid water restrictions wtp i 5 p v b i u v y n 1 t w s i u v y p y d n w t p i d n the water savings from tou ws i u v y for u 0 only include outdoor water savings whereas savings provided by tcu ws i u v y for u 1 include outdoor and non potable water savings additionally to the economic benefit of water savings rainwater tanks act as a protection from water shortage threat mankad et al 2013 to estimate the value of this attribute of protection against water shortage we use the benefit function transfer methodology this method consists in estimating economic values of a good or service by transferring a function calibrated with the attributes of people in a previous study kaul et al 2013 we estimate the wtp to avoid water restriction wtp i l y as a share s l of the total annual water bill of each household b i y based on the restriction level in place l by transferring a function developed by hensher et al 2006 in canberra australia an assumption was made that observations made in canberra are valid for application in other cities in australia according to their survey of 211 residential respondents households are only willing to pay to avoid restrictions that matter i e restrictions that apply all year every day of the week and for restriction level l 3 or higher 6 w t p i l y b i y s l l 3 0 l 3 a water demand sub model was developed to calculate the water use and savings of households fully based on roberts et al 2011 the sub model that follows a rather standard approach used in water supply modelling practice takes the parcel garden and roof areas the number of persons living in the household and daily rainfall events as input parameters outdoor and indoor non potable and potable water demands are returned as outputs of the sub model and used to assess the potential water savings from the six rainwater tank options greywater and black water generated by each household are also used to estimate the sewer usage charge to measure the total annual water bill for more details on the estimation of the total water bill in melbourne see appendix 2 3 1 3 decision rule for uptake of rainwater tanks the npv of tanks npv i u v y is then measured as the difference between the present values of the benefits and the costs 8 n p v i u v y p v b i u v y p v c i u v y finally the total value on which households take their decision total npv i u v y is based on the sum of the npv of tanks and the rebates offered by the regulator 9 t o t a l n p v i u v y n p v i u v y r u y v households decide to install a tank if the total npv i u v y of one of the six options is above 0 and will choose the option with the highest value 3 2 modelling platform and process overview the model was developed within the dance4water dynamic adaptation for enabling city evolution for water framework and is implemented in the dynamind software dynamind allows the integration of modules working tasks or building blocks within a simulation and facilitates the simulation of urban water systems in time and space urich et al 2012 general modules are written in c and python whereas the modules referring to the regulator and households decision making are exclusively written in python there are three main modules in the simulation 1 a water demand sub model 2 a policy instruments module and 3 a households decision module fig 1 an additional module representing the feedback of tank uptake on the water balance and the regulator s decision to apply water restrictions was developed but not implemented for the validation of rainwater tank uptake since historical restrictions levels were used for the simulation the water demand is first calculated for all households taking into account daily rainfall and the attributes of households and parcels the six tank options are created for each household with the potential water savings measured from the water demand and roof attributes in the next module the regulator agent updates the policy instruments according to observed policies in melbourne the three policies available to the regulator agent water prices restrictions rebates influence the three key variables for the decision making of households water savings wtp and tank costs respectively considering the updated policies households measure the npv of each tank option and install the tank with the highest npv if this npv is above 0 the attributes of tank options and installed tanks are written in sqlite database files before the simulation starts again at the following simulation step the computing time required for one model run is approximately 2 hours on a dell intel core i5 4210u 2 40 ghz processor and simulations were carried out using the cloud computing services at monash university 4 validation of the model for a suburb of melbourne validation of agent based models is critical to insure the reliability of model results to inform decision making rai and henry 2016 well known methods to validate or characterise the performance of models include using residuals by comparing and analysing observed data and model results bennett et al 2013 however a recurrent issue with agent based simulations of technology adoption is the lack of observed data to measure the property of residuals given the limited amount of data we aim to validate the model by comparing model results on tank uptake with available data and by conducting uncertainty analysis to assess the range of results 4 1 the scotchman s creek catchment the model was applied to the scotchman s creek catchment the location of the case study can be found in appendix 3 fig a 1 southeast of melbourne cbd for the period 2005 2014 when most of the rainwater tank uptake occurred in the city the catchment has an area of approximately 10 36 km2 and a population of around 25 000 residents the model includes 10 187 household agents which represents a scaling down of approximately 1 136 considering the total number of households across the city of melbourne 1 391 900 australian bureau of statistics 2016a household agents were initialised using 2011 census data from the australian bureau of statistics we assume that there has not been a significant development in this area and therefore the urban development within the case study location was not simulated the number of people living in each household was estimated by distributing the number of people living in each statistical area to the number of buildings in these areas the initial percentage of households with a rainwater tank was estimated at 10 from the earliest estimates from data on melbourne 13 in 2007 australian bureau of statistics 2013 and to account for a slight increase from 2005 to 2007 as already discussed to simulate the wtp to avoid water restrictions we used the share of the total annual water bill according to the restriction level from hensher et al 2006 following their study in canberra australia however there were differences between the way restrictions have been implemented in the two cities the city of canberra had 5 levels of water restrictions at the time of the research whereas melbourne had 4 levels southeast water corporation 2012 during the simulation period table 1 outlines how the model was applied for melbourne 4 2 model set up and inputs 4 2 1 policy instruments policy instruments in place in melbourne for the simulation period were used to define the regulator s behaviour fig 2 restriction levels are decided by the water utility melbourne water and have a range of 0 4 melbourne water 2016a an additional level 3a was added in 2007 but level 4 the highest restriction level has never been implemented fig 2a the restriction levels followed the storage level and rainfall patterns fig 2d annual rainfall was especially low from 2006 to 2009 at which time it remained under 500 mm per year compared to the long term annual average of ca 650 mm per year yarra valley water is the corporation responsible for providing water supply and sewerage services in the case study area water supply charges are composed of three levels of water consumption per household per day less than 440 l between 440 and 880 l and greater than 880 l fig 2b furthermore yarra valley water 2016 determines the sewerage drainage and park charges that are used to measure the total annual water bill of households rebates were set up according to two rebate programs offered by the victorian department of sustainability and environment the water smart gardens and homes rebate scheme wsgh from 2003 to 2011 victorian government department of sustainability and environment 2007 and the living victoria water rebate program that ran from 2012 to 2015 state government of victoria 2015 the wsgh scheme included rebates for tanks of any size of 150 aud starting in 2003 with an additional 150 aud if the tank was connected to a toilet system and was terminated in 2007 the second phase of the wsgh scheme from 2007 to 2011 consisted in rebates for tanks plumbed to houses only with different rebates depending on the volumes of tanks 500 aud for less than 5 kl if connected to toilet and or laundry 900 aud for 5 kl and above connected to toilet or laundry and 1000 aud for 5 kl and above connected to toilet and laundry because we looked at tanks plumbed to houses without distinction of the indoor uses the rebates for 5 kl and 10 kl plumbed tanks were averaged to 950 aud fig 2c the living victoria water rebate program increased the rebates of smaller tanks less than 5 kl to 850 aud if connected to toilet and or laundry and larger tanks 5 kl and above to 1300 aud if connected to toilet or laundry and 1500 aud if connected to toilet and laundry again the rebates for large plumbed tanks were averaged to 1400 aud 4 2 2 rainwater tanks variables the cost of tanks was derived from 486 tanks of various shapes slimline round and underground and materials plastic steel and polyethylene on sale from the websites of five manufacturers in melbourne and the cost of pumps was obtained from the websites of four retailers the costs of tanks and pumps advertised are in 2016 aud and were therefore adjusted for each year of the simulation using the cpi for melbourne australian bureau of statistics 2016b more details on the costs of tanks and pumps can be found in appendix 4 fig a 3 other ancillary tank expenses installation plumbing and maintenance were obtained from tam et al 2010 table 2 marsden jacob associates 2009 estimated the installation and plumbing costs of tou at between 40 and 70 cheaper than tcu we therefore used 0 55 for the initial value of cip for the base case simulation the lifespan of rainwater tanks was assumed to be 20 years and the interest rate for discounting was estimated at 5 tam et al 2010 energy usage charges were taken from united energy origin energy 2016 an electricity provider for the case study area and adjusted with the cpi of electricity in melbourne australian bureau of statistics 2016b energy consumption from water pumps for non potable indoor uses can vary widely a literature review conducted by vieira et al 2014 showed that pumping energy intensity for rainwater harvesting ranged from 0 12 to 2 10 kwh kl for single storey detached residential buildings over 10 studies with a median of 1 40 kwh kl we therefore used this median value for the initialisation of the model 4 3 evaluation of the model 4 3 1 comparison to the available data on the uptake of rainwater tanks the observed percentage of households with a rainwater tank in the city of melbourne is available for three years 2007 2010 and 2013 from the australian bureau of statistics 2013 and provides the most reliable variable to compare with model results the percentage of households that installed a tank in the case study area was thus compared with the observed percentage in melbourne the latest census 2013 also includes the proportion of rainwater tanks that are plumbed to the house i e that are used for indoor and outdoor purposes although the sizes of tanks are not included in the census data results from a previous survey carried out with 413 respondents in melbourne moglia et al 2014 were used to compare with model results the authors divided installed tanks among 13 categories of sizes from less than 0 5 kl to greater than 20 kl which were used to compare with the installation of small 2 kl medium 5 kl and large 10 kl tanks from the model we present the model results on the proportion of rainwater tanks that are plumbed to the house and the sizes of tanks but these two variables are not used to validate the model as more observed data would be needed to compare model results with observed patterns 4 3 2 uncertainty assessment we conducted monte carlo mc sampling to run 5000 simulations to assess the uncertainty of model results and their sensitivity to variation in input values additionally to uncertain tank parameters we ran simulations with a range of the following parameters initial rainwater tank percentage init rwht wtp as a share of the water bill s l and energy use e table 2 because of the lack of knowledge around the distribution of these parameters we sampled from uniform distributions for initialisation in order to measure the sensitivity and uncertainty of the model to rainfall and water demand a coefficient was added to simulations for non potable demand outdoor demand and rainfall table 2 a value of 1 was used for the base case and a range of 0 8 1 2 for mc simulations 5 results and discussion 5 1 model validation of rainwater tank uptake the base case simulation with initial values is able to replicate the two main uptake periods seen from the observed data in 2010 and in 2013 fig 3 a major uptake occurs in 2009 due to the implementation of level 3 restrictions and a higher wtp to avoid water restrictions the second uptake in 2013 can be attributed to higher water prices combined with high rebates on tcu the median value of the monte carlo simulations 12 1 25 9 and 28 4 of households with a rainwater tank for years 2007 2010 and 2013 follows a similar pattern to the base case simulation 11 24 8 27 6 reproducing the pattern of observed values 11 6 28 2 and 31 1 for the same years however the uncertainty of results is relatively large with 90 between 5th and 95th percentiles of results in 2013 ranging from 12 9 to 71 8 of uptake 5 2 simulation of rainwater tanks characteristics the model was used to simulate the different types of tanks installed i e the usage and volumes due to the lack of observed data to validate the types of tanks we do not use these results for validating the model but to provide additional information on the functionality of the model if sufficient data become available for validation 5 2 1 tank usage the proportion of installed tcu was measured from the simulations and was compared to only one observation 29 in 2013 available from the australian bureau of statistics 2013 fig 4 shows the proportion of tanks plumbed to houses the base case simulation provides an accurate estimate of the proportion in 2013 32 3 of tanks plumbed to houses whereas the median value from simulations overestimates the proportion of plumbed tanks 35 8 however it should be noted that these model results concern only tanks installed after the start of the simulations i e from 2005 whereas the observed datum from the australian bureau of statistics includes all tanks i e tanks installed before 2005 estimated to 33 of total installed tanks and after the percentage of tanks plumbed to houses installed during the simulation is thus likely to range between 0 assuming that all tanks were plumbed prior to the simulation period and 50 assuming that none of the tanks were plumbed prior to the simulation period due to low rebates and lower water price it is more likely that a low proportion of tanks were plumbed to houses prior to the simulation the range of model outputs is very large with 90 of results ranging from between 0 62 and 96 of plumbed tanks in 2013 this is likely due to the uncertainty in the parameter cip coefficient for installation and plumbing that captures the difference between installation and plumbing costs of tcu compared to tou coefficient for installation and plumbing because of a lack of information about this cost difference we used a fairly large range 40 70 for this parameter based on marsden jacob associates 2009 which can explain the resulting high uncertainty in the type of tanks installed therefore there is a high uncertainty related to the ability of the model to replicate the proportion of tanks plumbed to houses 5 2 2 tank volumes the volumes of installed tanks indicate that smaller tanks 2 kl are preferred to larger tanks 5 and 10 kl fig 5 there is no official data for the volumes of installed tanks but model results are consistent with the observation from a previous study conducted by moglia et al 2014 who undertook a survey of 417 households that installed a tank across melbourne it was found that smaller tanks 0 5 3 kl accounted for a majority of tanks 64 followed by medium sized tanks 3 7 kl 25 and larger tanks 7 kl 11 this pattern is represented in the current model as the mean fraction of 2 kl tanks at the end of the simulation 2014 is 54 of all installed tanks compared to 32 for 5 kl tanks and 14 for 10 kl tanks the location of parcels with a tank installed shows that the first tanks were installed in 2007 but the major uptake occurs in 2009 at which time a majority of tanks are 2 kl tanks the spatial location of installed tanks of different volumes is represented in appendix 3 fig a 2 5 3 model sensitivity we conducted a sensitivity assessment using scatter plots fig 6 to investigate the correlation between the various uncertain factors and output variables table 3 the correlation between the coefficient to assess the difference in additional tanks costs installation and plumbing between tou and tcu cip and the proportion of tcu is especially high at 0 67 a higher value of the cip parameter smaller difference in costs makes tou less competitive and leads to a greater proportion of tanks for combined use prop combined this high correlation between the two parameters explains the high uncertainty of the tank usage observed in fig 4 the cip parameter is also positively correlated with the proportion of larger tanks prop 5 kl and prop 10 kl and negatively correlated with the proportion of smaller tanks prop 2 kl thus with a low cip value the cost of tanks becomes an important factor of decision making and cheaper usage outdoor use and volume 2 kl are preferred table 3 the wtp of households to avoid water restrictions as the share of the total water bill s i l has the opposite impact on model results it has the highest correlation 0 42 with the tank uptake percentage perc rwht of all non policy parameters which means that a higher share of the total water bill leads to a higher wtp to avoid water restrictions and a greater uptake it is also positively correlated with smaller tanks prop 2 kl and negatively correlated with the proportion of tcu prop combined and the proportion of larger tanks prop 5 kl and prop 10 kl this may be caused by the wtp being activated in early years of the simulation 2007 2009 at which time rebates and water prices are less advantageous for tcu than the last years of the simulation mainly 2012 2014 indoor demand outdoor demand rainfall and most costs parameters have low correlations with all output variables only pump costs pp are negatively correlated with the proportion of tcu and 5 kl tanks 0 31 for both and positively correlated with 2 kl tanks 0 26 5 4 the main drivers for rainwater tank uptake the npv for the three volumes of tank options i e tanks that have not yet been installed and the two uses combined and outdoor use only over the length of the base case simulation is represented in fig 7 a this npv includes water savings and the wtp to avoid water restrictions to measure rainwater tank benefits but excludes rebates a significant increase of the npv occurring during the most severe time of the drought 2007 2009 is caused by the wtp of households to avoid water restrictions as the restriction level was raised from 2 to 3 in 2007 the present value shows that without rebates tank uptake would only occur during years of water restriction equal to or above 3 earlier years of the simulation show a consistently more beneficial npv for tou than for tcu in the second half of the simulation the npv for tcu raises compared to tou due to higher prices of water and therefore higher water savings benefit when rebates are included in the npv fig 7b the gap between larger 5 and 10 kl tcu and tou is reduced from the first rebate program that started in 2007 and larger tcu become more beneficial from 2009 onwards from the last rebate scheme introduced in 2012 a significant gap separates the two types making larger tanks plumbed to the house preferable 5 4 1 willingness to pay to avoid restrictions as demonstrated by the npv of tanks during years of restrictions of level 3 the wtp to avoid water restrictions has an important impact on the behaviour of households the wtp is based on the total annual water bill fig 8 a and is only taken into account in the decision making for three years of the simulation from 2007 to 2009 fig 8b the total annual water bill is composed of water supply and sewer charges both fixed and per usage as well as fixed park and drainage charges fig 8c for a description of the measurement of total water bill see appendix 2 the median total annual water bill increases at an average annual growth of 6 4 from 2005 433 aud to 2008 522 9 aud and doubles by 2014 1080 aud with a strong annual growth rate 12 8 as a result the median wtp of households increases from 80 5 to 105 5 aud from 2007 to 2009 the willingness to avoid water restrictions was found to be an important factor of tank installation which is in agreement with previous surveys australian bureau of statistics 2013 mankad et al 2013 moglia et al 2014 previous findings from the literature suggest a large range of values for the wtp to avoid water restrictions respondents from a study in california indicated a wtp between 140 and 203 annually to avoid an occurrence of water restrictions in 1993 us dollars depending on the frequency and severity of the restrictions koss and khawaja 2001 griffin and mjelde 2000 examined customers preferences using contingent valuation methods and found that respondents from seven texan cities were willing to pay between 304 08 and 412 68 in 1997 us dollars on average to avoid water restrictions in australia studies have shown a significantly lower wtp for unrestricted water use for instance hurlimann and mckay 2005 found that the mean wtp of respondents in adelaide to be exempt from water restrictions was 8 6 aud annually with a standard deviation of 25 6 although the results included a high response rate of 0 in canberra residents were willing to pay a very small amount 10 aud to prevent a 10 per cent reduction in water use in 1997 australian dollars gordon et al 2001 thus the results from the current model show values below findings from united states studies but above australian studies 5 4 2 water savings the simulated water savings provided by installed tanks show higher results than previous findings from the literature the median water savings for households that installed a tcu of 5 kl was 87 kl and 113 3 kl for households with a 10 kl tank whereas no tcu of 2 kl were installed results from previous empirical and modelling studies have estimated the annual savings based for combined usage toilet laundry and irrigation from rainwater tanks between 38 5 burns et al 2014 and 59 5 kl per year rahman et al 2012 for a tank of 5 kl in this study the higher values from the model can be explained by the fact that households with the highest levels of water savings will choose the tank option with a npv above 0 which means the highest water saving furthermore although the median value for total annual water savings from the model is significantly higher than previous findings it is similar to an empirical study conducted in a nearby area in melbourne with 4391 households that have used a rainwater tank rebate during the drought gato trinidad and gan 2014 by comparing average annual water consumption per household from mains before the installation of tanks of 5 kl and larger with their water consumption after the tank installation the authors found that the average annual savings was 120 5 kl per household although other factors than rainwater tanks may have influenced this saving that occurred from july 2005 to june 2010 such as more efficient end use appliances water conservation behaviour and water restrictions rainwater tanks are likely to have had a large impact on the saving the median water saved for 2 kl tanks across the two uses corresponds to 19 of households total water demand 44 5 kl tanks and 59 for 10 kl fig 9 these figures are within the range of previous studies for example as reported by souza and ghisi 2012 i e up to 50 water saved from a 5 kl tank and up to 60 from a 10 kl tank depending on water demand roof area and annual rainfall 6 conclusions this paper described the development of an agent based model to replicate the uptake of decentralised water solutions and its application and validation in the context of rainwater harvesting in melbourne the tool was able to replicate the uptake of rainwater tanks regarding the number of installed tanks however model results showed a high uncertainty related to the percentage of tanks plumbed to houses due to the large difference in the installation and plumbing costs and more data would be needed to assess the capacity of the model to accurately replicate the type of tanks as a result the model may not be adequate with the currently available data to predict the impact of policy instruments on the types of installed tanks the model showed that for the major uptake to occur the three policy instruments had to be implemented at the same time or two policies at a high level high water price and high rebates on tcu furthermore the wtp to avoid water restrictions was only triggered at restrictions levels at which the use of sprinklers and the watering of lawns were not permitted therefore lower levels of restrictions may help to reduce water use but only have a marginal effect on the value of tanks as a water shortage protection tool for households this behaviour highlights the need for more than one policy instrument to be used in combination to trigger the uptake of residential rainwater harvesting systems this is significant in the context of melbourne where the rebate scheme has been terminated in 2015 the agent based approach allowed us to examine the impact of different policies on the behaviour of households because of its use of spatial data the model could be applied to test spatially targeted policy interventions as opposed to a majority of agent based models we have decided to omit the simulation of the interaction among households based on our literature review however because of the flexibility of the model and its spatial representation such interactions could be implemented if it is found as an important determinant of rainwater tank uptake the model could provide information for policy makers to enable the uptake of comparable decentralised technologies in developing cities that are likely to suffer from increasing drought frequency groundwater depletion and saltwater intrusion the technical suitability of rainwater tanks has been examined in several countries with arid climate by looking for example at the optimal sizing of tanks for the local climate but the behaviour of households regarding the installation of tanks is often overlooked the model could provide valuable insights into the combination of economic and regulatory instruments required to influence the behaviour of households by changing the value of tanks and thus trigger decentralised technology adoption and shape water management transition appendix 1 description of the model following the odd protocol table a1 odd protocol elements table a1 odd element implementation in the model purpose the purpose of the model is to simulate the uptake of rainwater tanks based on the cost of tanks the willingness to pay to avoid water restrictions and the water savings of households and with the influence of different policy instruments available to the policy maker entities there are two agents represented in the model households and a regulator encompassing the victorian department of sustainability and environment and the water utility the environment is represented by rainfall time series and there are no collectives of agents scales the temporal and spatial scales are described in section 4 1 process overview and scheduling the process overview and scheduling are presented in fig 1 basic principles it is assumed that households make their decisions based on the main determinants of rainwater tank adoption found from the australian australian bureau of statistics 2010 2013 mankad et al 2013 i e to save water to avoid water restrictions and to save on water costs the willingness to avoid water restrictions on mains water is estimated by the willingness to pay to avoid water restrictions calculated with the use of benefit transfer function from a study conducted in a similar setting in canberra australia hensher et al 2006 emergence the key results analysed from the emerging behaviour of households is the total number and the proportions of types and sizes of tanks installed after a given year adaptation household agents are not changing their decision making process during the simulation or not given adaptive traits objectives the objective of the household agent is to save money and protect itself from water shortage over time by evaluating the money saved from water savings the costs after rebates of different options of tanks as well as to which extent a tank can reduce the threat of water shortage learning the agents base their decision on the current year information and do not change their decision making process over time prediction households do not predict future savings but they assume that their water savings and maintenance costs will remain the same for the service life of the tank when they consider tank options sensing the decision making process of households is dependent on the three policies available to the regulator agent water price restriction rebates and the varying rainfall a module was developed in which the regulator adjusts the restriction level based on the water storage influenced by storage inflow and total water demand but the model was kept idle for the historical uptake households sense the variations in rainfall when measuring the water savings from the tank interaction there are no interaction among household agents as we have concluded from our literature review that the main determinants of tank uptake are to save water avoid water restrictions on mains water and to save on water costs as indicated in section 2 2 stochasticity stochasticity is introduced through the monte carlo sampling by sampling from uniform distribution for the uncertain variables shown in table 2 collectives there are no collectives of agents observation the output data are collected in sqlite files as indicated in fig 1 results on rainwater tank options for each household are used for analysis mainly npv water savings water bills and willingness to pay results are assessed for each household initialization initialisation is described in section 4 1 input data input data are described in table 2 sub models we use a water demand sub model described in section 3 1 2 appendix 2 measurement of total water bill in melbourne the components of total annual water bills vary among cities in melbourne the total bill b i y comprises a water usage charge measured as the total household water use wu i y multiplied by the water price p y and a sewer usage charge calculated as the annual amount of wastewater produced by each household sd i y multiplied by the treatment price of wastewater sp y the water bill also includes fixed annual charges for water supply sewerage drainage and park maintenance fw y fs y fd y fp y b i y w u i y p y s d i y s p y f w y f s y f d y f p y appendix 3 case study location fig a1 location of scotchman s creek catchment east of melbourne cbd fig a1 fig a2 spatial location of parcels with a rainwater tank in the case study location following the three main years of uptake 2007 2009 and 2013 fig a2 appendix 4 measurement of tank and pump costs fig a3 distribution of pump and tank costs for the three volumes of tanks used in the model boxplots of costs are shown for the five tank retailers and overall the median of all values is used for initialisation of the base case simulation for each volume tank costs were obtained from the websites of five retailers in melbourne asc water tanks tankworld kerrimuir rainwater products and g store n 267 the median value over all retailers is used for the three volumes 2 kl n 70 5 kl n 53 10 kl n 17 pump costs were obtained from the websites of four retailers in melbourne asc water tanks tankworld kerrimuir rainwater products oz tanks and g store n 534 with a median value of 600 aud fig a3 appendix 5 sensitivity of model results to input parameters table a2 spearman s rho for all parameters columns represent output variables and rows represent input parameters table a2 parameter perc rwht prop combined prop 2 kl prop 5 kl prop 10 kl y 0 65 0 15 0 00 0 10 0 21 l 0 17 0 12 0 01 0 07 0 17 r1 2 0 59 0 14 0 02 0 08 0 09 r1 5 0 59 0 14 0 02 0 08 0 09 r0 0 61 0 08 0 06 0 09 0 26 p 0 65 0 15 0 00 0 10 0 21 c2 0 01 0 02 0 02 0 02 0 02 c5 0 00 0 00 0 00 0 00 0 01 c10 0 00 0 00 0 00 0 00 0 00 pp 0 03 0 31 0 26 0 31 0 09 pb 0 14 0 01 0 05 0 01 0 14 amc 0 11 0 12 0 14 0 11 0 16 inst 0 09 0 02 0 04 0 01 0 09 cip 0 26 0 67 0 66 0 65 0 50 s 0 42 0 46 0 52 0 44 0 60 t 0 01 0 01 0 01 0 01 0 01 r 0 02 0 11 0 08 0 10 0 02 id 0 06 0 18 0 12 0 07 0 15 od 0 10 0 11 0 08 0 09 0 00 rf 0 01 0 03 0 03 0 03 0 02 init rwht 0 20 0 00 0 00 0 01 0 01 p 0 001 p 0 01 p 0 05 
