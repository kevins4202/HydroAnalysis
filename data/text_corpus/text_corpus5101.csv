index,text
25505,modeling transient flow in water transport networks wtns is characterized by hyperbolic partial differential equations existing commercial and open source software for transient modeling in wtns have limitations such as lack of scalability and compatibility with high performance computers difficulty to systematically execute simulations and analyze results this work proposes a novel open source python package that relies on vectorization and distributed computing to overcome the limitations of existing software the proposed library the parallel transient simulator for water networks ptsnet surpasses in computational performance existing modeling tools and incorporates novel analytics functionalities ptsnet has been tested on wtns composed of tens hundreds and thousands of hydraulic elements using a personal and a supercomputer running from tens to hundreds of processors we show through rigorous analyses that ptsnet is scalable accurate and significantly speeds up simulations with sufficiently dense numerical grids keywords transient flow networked water systems parallel computing high performance computing message passing interface data availability all data and codes are available in a public github repository that was referenced in the manuscript https github com gandresr ptsnet software availability name of software ptsnet software requirements python 3 6 3 7 3 8 3 9 and python dependencies i e numpy tqdm scipy h5py pandas wntr mpi4py numba availability ptsnet source codes are available from a github repository at https github com gandresr ptsnet 1 introduction modeling hydraulic processes in environmental systems is critical for management operation and control of water transport networks wtn in natural and urban environments particularly modeling transient scenarios such as the surcharge of water mains in drainage systems or sudden pressure changes in water distribution systems due to the operation of pumps is essential to prevent failures that may result in high economic losses boulos et al 2005 failures attributed to rapid flow accelerations and drastic changes in pressure include floods system fatigue or pipe ruptures pump and device malfunctioning and intrusion of pathogens and contaminants through leaks cracks and other defects boulos et al 2005 lechevallier et al 2003 fox et al 2014 this paper focuses on modeling transients in wtns which involves solving partial differential equations pdes that describe the water transport phenomenon in terms of conservation of mass and momentum relationships pdes for transient flow modeling can be solved using a variety of numerical methods including the method of characteristics moc wylie et al 1993 nault et al 2018 finite difference methods chaudhry and hussaini 1985 kiuchi 1994 blanco et al 2015 verdugo et al 2019 cao et al 2020 or finite volume methods zhao and ghidaoui 2004 castro et al 2006 fernández pato and garcía navarro 2014 mesgari sohani and ghidaoui 2019 overall moc has been predominantly used over other numerical schemes due to its ease of implementation and accuracy however since most wtns are composed of thousands of hydraulic elements and cover extensive areas modeling transient phenomena for realistic wtns can be intractable with current simulation tools recent advances in computational fluid dynamics demonstrate that distributed computing and vectorization can significantly speed up computations for modeling water transport in environmental and urban systems in lin and zhang 2021 the authors propose a methodology that divides hydrologic and hydraulic zones for a 2d flood simulation computing the hydrodynamic behavior of the zones across multiple processors the model runs 10 times faster than traditional sequential implementations yet sacrifices accuracy for computational performance similarly in burger et al 2014 the authors present a parallel version of the epa s storm water management model swmm that runs six to ten times faster than sequential swmm on a twelve processor system in carlotto et al 2021 the authors developed sw2d gpu an open source 2d shallow water model that parallelizes computations across multiple processing units in a general purpose graphic processing units gpgpus achieving 36 times faster running times similarly anguita et al 2015 proposes a 3d semi implicit hydrodynamic model for shallow water that can be executed in parallel in all the previous works the improvement in computational efficiency is typically greatest when running highly dense numerical grids which are sometimes necessary to capture nonlinearities in large scale simulations abhyankar et al 2020 likewise there is a generalized adoption of partitioning algorithms and distributed computing to accelerate models in water systems zhu et al 2019 xu et al 2021 tiernan and hodges 2022 for example there have been significant advancements in the parallelization of shallow water equations which are essential for open flow modeling e g river and drainage modeling speeding up simulations several orders of magnitude carlotto et al 2021 burger et al 2014 however fewer works have focused on studying the problem of parallelization to solve transient flow in pressurized wtns in the context of urban water systems packages for transient simulation in wtns include commercial software such as bentley hammer bentley systems 2019 kypipe kypipe 2019 infosurge innovyze 2019 and transam mcinnis and karney 1998 commercial packages share similar functionalities and are of limited use for large scale applications commercial packages come with limitations in computational efficiency such as lack of scalability and compatibility with high performance computers moreover the usability of commercial packages is limited since commercial codes are delivered to users as black boxes and thus do not provide opportunities for improving state of the art models modifying or extending its functionalities nor running transient simulations systematically i e through program commands rather than interacting with a graphical user interface gui therefore it is difficult to extract analyze and visualize numerical results on the other hand open source packages can be categorized as transient focused and general purpose transient focused packages which solve the specific system of hyperbolic pdes that models transient flow in wtns include tsnet xing and sela 2020 and the free open source implementation of moc presented in kjerrumgaard jensen et al 2018 transient focused packages are computationally inefficient with respect to commercial packages when modeling medium and large size wtns conversely general purpose packages allow users to solve a systems of hyperbolic pdes defined by the user some of the available general purpose packages include modelica fritzson 2011 openfoam jasak et al 2007 fenicsx scroggs et al 2021 and petsc s dmnetwork abhyankar et al 2020 general purpose packages incorporate state of the art methods to run distributed computations across multiple processors but are typically based on implicit numerical schemes whose solution is found via adjoint or iterative methods e g conjugate gradients which have proven to be computationally inefficient when running in parallel for inherently sparse systems such as wtns burger et al 2016 abhyankar et al 2020 more critically the usability of general purpose packages can be cumbersome for users that need to run transient simulations of wtns because users need to manually define the topology of the wtn and its properties to formulate the system of pdes and its boundary conditions defining simulation parameters for a large scale simulation can be highly time consuming and prone to errors given the multitude of boundary conditions that are present in wtns this study presents and demonstrates a new open source software package the parallel transient simulation in networks ptsnet that addresses the limitations of computational efficiency and code usability of both commercial and open source packages for computational efficiency we build upon our dv moc work in riaño briceño et al 2021a which uses vectorization and distributed computing for efficient computation in riaño briceño et al 2021a we demonstrate 1 a new vectorized formulation of the transient flow equations which is specifically designed to fit an efficient memory allocation scheme and 2 a parallel algorithm that distributes the load among processors to further speed up vectorization and adopts to general wtns topologies and many boundary conditions the numerical solution uses an explicit time marching scheme to reduce the computational overhead of parallel communications the readers are referred to riaño briceño et al 2021a for the theoretical derivations this paper contributes to the 1 definition of the software architecture to support the novel vectorized and parallel scheme for general wtns and 2 development of new analytics tools such as automated input file processing selection of the best time step and number of processors for a transient simulation running in parallel the code is written in python using openmpi and hdf5 libraries that are compatible with high performance computing systems for code usability we equip ptsnet with functionalities to automatically extract the wtn properties from standardized input files i e the epanet inp file rossman 1994 which have been widely adopted in both industry and academia sela and housh 2019 sela et al 2019 in addition ptsnet contains a set of functionalities that allows users to extract and analyze simulation data both quantitatively and visually facilitating parameter selection and the storage retrieval and analysis of results the remainder of the paper is organized as follows section 2 provides an overview of the dv moc transient flow model and ptsnet functionalities section 3 describes the underlying architecture of the library and its main data structures for users interested in building new functions sections 4 and 5 focus on explaining model setup and execution through code snippets in section 6 the package is validated for modeling medium and large size networks by comparing simulation results with widely accepted computational packages for transient modeling additionally ptsnet s performance is compared with that of its counterparts showing that the dv moc offers significantly better simulation times and extends the capabilities of currently available software finally in section 7 conclusions and future directions are provided 2 software description ptsnet is an open source python package capable of computing the dynamics of flowrates q x t and hydraulic heads h x t in time t and space x for a pressurized wtn under the effect of transient scenarios such as valve opening and closure pump start up and shut off and bursts modeling wtn dynamics in ptsnet requires defining the wtn properties using the industry standard epanet inp format defined by rossman 1994 input files can be generated systematically using code scripts based on the wntr package klise et al 2018 or via epanet s gui rossman 1994 ptsnet relies on other open source python packages that need to be installed through a conda environment following the instructions provided in ptsnet s repository riaño briceño et al 2022 in particular we chose the wntr package over other available libraries that can systematically extract information from general text files because wntr is supported and maintained by the us environmental protection agency epa that is responsible for defining the inp file standard moreover we use mpi4py to handle the open message passing interface openmpi standard which enables distributed programming and is broadly used in industry and academia graham et al 2005 once all the dependencies are installed ptsnet s installation is done via the pypi repository using the pip install ptsnet terminal command as described in riaño briceño et al 2022 we note that since ptsnet is continuously updated by using the previous command users will install the latest version of the package to install the version presented in this paper users should use pip install ptsnet 0 1 7 ptsnet s functionalities can be categorized into four distinct groups 1 model setup 2 execution of simulation 3 extraction of results and 4 analytics as illustrated in fig 1 the first three groups of functionalities allow users to run a transient simulation using one or multiple processors and create workspaces to save simulation results the fourth group are functionalities to help with model setup and results analysis the remainder of this section presents a description of the underlying transient flow model and describes ptsnet s functionalities 2 1 transient flow model transient flow within pipes is characterized by a system of hyperbolic pdes that describes conservation of mass and momentum relationships wylie et al 1993 these equations are prognostic since they predict the value of flowrates and hydraulic heads for some time in the future based on current time conditions holton 1973 when pipes are connected to other wtn elements such as valves pumps leaks bursts surge protections reservoirs dead ends and demands transient flow is modeled by coupling prognostic and diagnostic equations the latter being equations that link flowrates and or heads at identical times holton 1973 ptsnet is based on the distributed and vectorized method of characteristics dv moc of riaño briceño et al 2021a which has advantages for accuracy ease of implementation and suitability for large scale simulations with significantly lower computational times than sequential implementations details of the dv moc method are found in riaño briceño et al 2021a with an abbreviated description provided below the dv moc time marching scheme can be executed by one or multiple processors to solve conservation of mass and momentum equations wylie et al 1993 1 h x t t ω 2 g a q x t x 0 2 h x t x 1 g a q x t t f x t 0 where h and q represent head and flowrate respectively a is the cross section area of a pipe ω is the pressure wave celerity referred to as wave speed from now on and g is the gravitational acceleration the term f x t in eq 2 represents the friction head loss per unit length estimated for steady quasi steady and unsteady conditions nault et al 2018 in this work we solve eq 2 using a steady friction model where f x t q x t q x t f 2 g d a 2 d is the pipe diameter and f is the darcy weisbach dimensionless friction factor for pipes derivations of prognostic eqs 1 and 2 and their numerical solution can be found in riaño briceño et al 2021a and riaño briceño et al 2021b the formulation of diagnostic and prognostic equations for hydraulic devices e g valves pumps is described in section s1 of the supporting information si the dv moc numerical scheme is guaranteed to be linearly stable for model time steps satisfying the courant friedrichs lewy cfl condition courant et al 1967 this behavior is achieved by discretizing longer pipes lengths into segments and adjusting the local wave speed values for the global time step to ensure that the cfl number remains less than 1 for every pipe segment in the network this local adjustment of wave speed is a standard approach for moc solutions wylie et al 1993 the discrete versions of eqs 1 and 2 are solved at the solution points which can be classified as interior and boundary points depending on their association with the wtn elements while interior points are those within pipes boundary points are those where the coupling between eqs 1 and 2 with boundary conditions occur riaño briceño et al 2021a to illustrate fig 2 shows a wtn formed by several pipes in series connecting an upstream reservoir through a pump to supply a downstream demand regulated by a valve the flow moves across the x axis from left to right downstream the pump there is a surge protection device and upstream the valve there is a leak the pipes of the wtn in fig 2 are discretized into pipe segments of length δ forming interior points marked with black dots and boundary points associated with the reservoir general junction valve pump and surge protection device marked at the ends of the pipes with icons that differentiate the types of the boundary points the relationship between the system and its model abstraction as illustrated in fig 2 is useful in understanding how simulation settings time step wave speed and the δ segment length will affect both the computational time and model error smaller time steps faster wave speeds and smaller pipe segments will provide a more precise solution but at the cost of increased computational time we further discuss these settings in section 4 2 2 functionalities using ptsnet involves using the functionalities presented in fig 1 to set up a transient model determine simulation settings such as time step and number of processors execute a parallel simulation and extract results a simulation can be initialized using model setup functionalities which extract information from the input file allocate memory for the model compute initial conditions define transient scenarios and run a compatibility check the compatibility check validates that the wtn meets ptsnet s connectivity requirements as follows valves and pumps must be connected to a single pipe upstream and downstream reservoirs can only be connected to valves or pumps through a pipe no leaks or demands are allowed at valves or pumps demand values cannot be negative and valves cannot have zero head loss if their initial flowrate is greater than zero as part of the model setup users must define the number of processors and the simulation time step either manually or using analytics functionalities as described in sections 5 1 and 5 2 once the model is set up users run a ptsnet simulation read and plot results after completion using execution and extraction functionalities respectively note that model setup and execution functionalities are invoked in the same python script and separate scripts can be used to invoke extraction and analytics functionalities the next section describes the software architecture and its underlying data structures followed by demonstration of ptsnet functionalities through snippets of code 3 software architecture this section describes the underlying mechanisms of vectorization and distributed parallel computing that run within ptsnet including specifications of the data structures that enable these mechanisms using the small scale example described below and presented in fig 3 we show how vector operations take place when solving eqs 1 and 2 numerically how tasks are divided between processors and how information is exchanged between processors to compute an moc step furthermore we describe how simulation files are stored and organized within workspaces and how array data structures are stored in memory using parallel hdf5 example small scale system the small scale test case presented on the top left corner of fig 3 comprises a reservoir nine pipes five general junctions and a valve that regulates water demand downstream as described in section 2 1 each pipe in the system is divided into segments such that heads and flowrates are computed at solution points which are enumerated from 0 to 32 boundary points at the extreme of the pipes are marked with squares and interior points within the pipes are marked with ticks for illustration purposes the network data is partitioned among three processors fig 3 bottom thus creating information dependencies between processors fig 3 top right 3 1 vectorization modern central processing units cpus have single instruction multiple data simd functionalities grama et al 2003 which allow simultaneous computations of basic operations such as addition and multiplication this capability referred to as vectorization is executed when a loop is replaced in the code by an simd instruction so that instead of computing a single operation for a single solution point at every time step multiple solution points can be processed simultaneously in a fraction of the time grama et al 2003 the first step towards vectorization consists of expressing the numerical solution of eqs 1 and 2 in terms of vector operations to illustrate we show how flowrates are computed for a partition of the small scale example based on the dv moc framework flowrates at interior points located at x and time t are computed based on the previous solution in time at time t minus the time step τ as follows 3 q x t w x t τ h x t u x t τ c h x t d x t τ b x t τ c w x t τ d x t τ b x t τ u x t τ where b x t ω g a b x f δ 2 g d a 2 r x abs q x δ t d x t h x δ t b x q x δ t u x t b x r x abs q x δ t w x t h x δ t b x q x δ t the information associated with each interior point is stored in custom vector data structures that inherit the properties of a numpy array in addition we store the indexes associated with interior points on a separate array referred to as selector in the case of the small scale example if processor 3 is in charge of computing the points in the network that are marked in blue the selector of interior points for processor 3 is given by 22 23 24 25 26 27 28 31 once the selector is defined eq 3 can be solved either with sequential or vectorized programming as shown in fig 4 when the sequential code is executed in a cpu with eight pipelines the computation of flowrate vector entries takes eight iterations one iteration per pipeline sequentially whereas the vectorized computation takes a single iteration eight pipelines computed simultaneously vectorization is executed via numpy s fancy indexing functionality as shown in fig 4 to create new vectors with contiguously allocated data of w b d and u based on the corresponding selector once the vectors are created vector operations take place and flowrates and heads are updated note that the number of simultaneous operations is limited and depends on the architecture of the cpu for example for intel s xeon phi x200 knights landing cpus which we used to produce some of the results presented in section 6 the maximum number of simultaneous fused multiply add operations per cycle is eight when using 64 bit floating point data mccalpin 2022 one of the main challenges of implementing vectorization and making it practical for efficient numerical operations for networked systems is the proper definition of selectors since vector operations are invoked recurrently within ptsnet we decided to pre compute and pre allocate all the necessary selectors to replace any for loop with array instructions by doing so we avoid allocating and deallocating selectors at every iteration this is possible given that the structure of the numerical grid remains constant throughout the simulation table s1 in the si lists the selectors for points and nodes that can be accessed via the ptsnetsimulation worker where dictionary the ptsnetsimulation worker where data structure is broadly used within the simulation funcs module as well as within the results module facilitating not only the computations of results but also the extraction of results data that are used in vectorized operations are stored within a ptsnettable the ptsnettable was designed to allow data extraction either by label or by a collection of indexes users will mostly interact with two types of ptsnettable data structures i e system states and results system states refer to a collection of tables that store properties of different system elements specifically these are nodes pipes valves pumps open and closed surge tanks each element type is associated with a specific set of states which can be constant or variable as defined in the source code see the simulation constants module table properties are stored in ordered arrays such that every position in the array is associated with a specific element label since element labels cannot be used to extract information out of ordered arrays in python tables internally translate element labels into array indexes as shown in fig 5 table indexing is done through a python dictionary that operates as a hash function f k z taking a label k k and mapping it to a positive index the associated collections of states are stored within the ptsnetsimulation ss dictionary in order to extract information about system states users need to use a special syntax to illustrate reading the initial head ihead at a specific node involves calling the ptsnettable extracting the ihead property and using a numerical index or a label to extract the value as shown in fig 5 in the example the initial head at the node junction 1 is extracted 3 2 distributed parallel computing ptsnet leverages the computational power of multiple processors using distributed parallel computing in practice the overall moc problem is divided in smaller parts such that every processor concurrently computes a smaller vectorized moc problem when distributed parallel computing is used each processor has its own private memory making it necessary to exchange information among processors once information is passed processors perform computations independently from each other spending different amounts of time to run a single time step therefore processors need to synchronize with each other at the end of each time step to ensure that all the solutions of heads and flowrates at time t are available to compute the solutions at time t τ hence given k processors the networked numerical grid must be partitioned such that the time spent in communication between processors is minimized and the load on each processor is balanced in general we assume that boundary conditions can only be computed by one processor and because of this boundary points belonging to the same boundary condition are assigned to a single processor for example in fig 3 top left we show the corresponding subgraphs that result from a three way heuristic partitioning the graph that represents the networked numerical grid consists of thirty three points that are divided into three sub graphs each of them containing 16 18 and 12 points respectively moreover processors 1 and 2 exchange information from points 11 14 and 17 coming from processor 2 and points 10 13 and 16 coming from processor 1 therefore information dependencies are bidirectional given that processors not only receive information from their neighbors but also send information to them the partitioning algorithm included within ptsnet achieves balance of work between processors by dividing the number of solution points into approximately equal parts for example creating a simulation with the small scale network using a time step of 0 07 s produces a numerical grid with 33 solution points this simulation is partitioned to be executed by three processors hence each processor allocates memory for the points it is in charge of plus the external information dependencies necessary to compute the moc equations each point in the network receives a global index used to determine information dependencies and export results and a local index to facilitate local vector operations and information retrieval local operations are those executed by an individual processor for a specific region of the wtn e g computation of specific solution points in contrast global operations are those executed for the entire wtn by multiple processors e g storing results for the example presented in fig 3 notice that processor 1 is in charge of computing points 0 11 13 and 16 however to compute the solution of boundary points 8 13 and 16 it is necessary to use information from solution points 7 14 and 17 therefore processor 2 needs to communicate information from points 14 and 17 to processor 1 and processor 1 needs to allocate additional space for the incoming information indexes pointing to local spaces in memory for the incoming information dependencies are referred to as send and receive buffers and can be accessed through the ptsnetsimulation worker send buffer and ptsnetsimulation worker recv buffer statements buffers are used to build the information dependencies graph idg which is a special data structure created with the openmpi standard that establishes connections between processors to exchange information based on the idg ptsnet builds communicator data structures that define groups of processors that need to exchange data groups of processors locally exchange data using the neighbor all to all communication protocol grama et al 2003 in summary each processor independently computes dv moc as presented in riaño briceño et al 2021a first the program is initialized and then computations of solution points are distributed among processors by performing the partitioning of the networked numerical grid once the partitioning is defined the idg is built in a distributed fashion i e each processor receives a portion of the network and defines its dependency to other processors afterwards all the processors start running their part of the vectorized step synchronizing and exchanging information at the end of the time step this process is repeated iteratively until the duration of the simulation is completed 3 3 file management results are not centrally stored in either memory or written to a file as the simulation is executed instead results on each processor are stored in memory that only that processor can access however ptsnet uses the hdf5 library to unify simulation results in a single binary file at the end of the simulation collette 2013 this approach is more efficient than writing either a unified file or separate results files during runtime however it has the disadvantage that a model crash will generally produce no written output of results before the crash the unified output file is concurrently processed and written using the functionalities of the hdf5 library collette 2013 this single standardized binary file facilitates read operations and reduces the complexity of parsing data when extracting results for analyses at the end of a simulation ptsnet stores the results within the flowrate h5 demand flow h5 head h5 and leak flow h5 files as presented in fig 6 other workspace files include initial conditions profiler and simulation properties note that within ptsnet jargon initial conditions refers to data extracted from the epanet input file but rearranged in ptsnet s data structures profiler files store information of running times for the initialization computation of interior and boundary points and communication between processors finally simulation properties refers to all the constant values that are used throughout the simulation in order to compute the dv moc method the files composing a workspace are locally stored in the directory from which the user executes the simulation users extract data via ptsnet s persistent mode which accesses i initial conditions computed with epanet ii simulation times iii flowrates at the start and end of pipes iv demand and leak flowrates at nodes and iv the head at every node in the network users do not have to directly manipulate these files instead the workspace data is accessed through ptsnet objects whose information is internally loaded from the files if the persistent mode is active using this approach results are extracted invoking the ptsnetsimulation object initial conditions are found within the system ptsnetsimulation ss dictionary and running times are stored in the ptsnetsimulation worker profiler note that information stored in pkl files is internally used by ptsnet and stored in the ptsnetsimulation the workspace folders can occupy a large block of memory during the simulation depending on the size of the problem therefore users should carefully consider how much simulation data to store in the workspace ptsnet provides flexible control of storage through functionalities to read results list workspace information and delete a specific workspace these functionalities are found within the results workspaces module to allocate time series results ptsnet uses the ptsnettable2d data structure which is an extension of a ptsnettable with time series as table entries results are allocated within the sim results dictionary and then unified and updated at the end of the simulation through the ptsnetsimulation object internal manipulations simplify the syntax to extract results for example whenever ptsnetsimulation node is invoked information is either loaded from ram or disk and extracted from the ptsnetsimulation results node data structure results can be extracted for nodes and pipe extremes i e pipe start and end segments in the case of nodes users can extract heads leak and demand flowrates and for pipes users can extract flowrates pipe flowrates will always be given with positive values if users require the sign of the flowrate to match the flow convention of the epanet model used as input the flowrate time series can be multiplied by sim ss pipe direction note that a ptsnet simulation can run out of memory if the results for all time steps and all solution points are stored to minimize the use of memory resources and ensure scalability the time marching algorithm of ptsnet routinely stores point data for the minimum number of time steps necessary to advance in time i e two consecutive time levels these are referred to as the memory pool of points mem pool points and their data are stored in ptsnettable2d as the simulation advances ptsnet switches between table rows in order to compute the next time step i e initial conditions at time t 0 are stored in the first row of the table then a step is taken and the solution of time t τ is stored on the second row when the third step t 2 τ is computed the solution is stored on the first row using the solution of t τ therefore accessing high resolution point data at every time step requires manually extracting the necessary results from the memory pool of points making use of proper selectors such as the ones described in section s2 of the si even though point data is only stored for two time steps data at the extreme of pipes is available for every time step results at pipe extremes can be accessed either directly from ram or from the hdf5 disk file via the ptsnetsimulation as shown in fig 7 4 model setup this section illustrates different use cases through snippets of code that exercise ptsnet s functionalities these snippets can be used to model specific transient scenarios extract results and determine time step and number of processors additional example codes for using ptsnet are available in riaño briceño et al 2022 setting up a transient model with ptsnet involves i defining simulation settings in the form of a python dictionary ii creating transient model and iii defining the transient scenario as described next defining simulation settings ptsnet provides default settings see lines 7 through 20 in fig 8 users can adjust these values to increase the temporal and spatial resolution of the model extend the duration of the simulation set up wave speed values for each pipe turn on and off secondary processes such as displaying messages on the terminal running a compatibility check measuring simulation times and saving results secondary processes see lines 15 through 19 in fig 8 can also be turned off to reduce simulation times for instance once ptsnet determines that a wtn is compatible with the transient flow model there is no need to run the compatibility check again if the same input file is executed multiple times the temporal settings such as the time step and duration should be given in seconds ptsnet transient analysis initiates from a steady state condition computed by epanet in the event that the epanet inp file includes an extended period simulation eps i e the steady state simulation spans multiple hours users need to select the time of the steady state condition to initialize the transient simulation the period setting is the index associated with the time period within the eps that will be used to calculate the initial condition for the transient simulation to define the wave speed values for pipes users can either i set the same value for all pipes using the default wave speed setting or ii define specific wave speed values using a text file containing pipe labels on the first column and wave speed values on the second the disk path to access the text file with wave speed information must be defined via the wave speed path setting note that in any moc method wave speed values are adjusted by the moc algorithm by a factor ϕ i e an adjusted wave speed ω ˆ i ϕ i ω i is applied for each i pipe thus the user input values or the default values are merely starting points for the adjusted wave speeds three common wave speed adjustment methods are included in ptsnet the user bentley systems 2022a the critical wylie et al 1993 and the optimal misiūnas 2008 methods note that regardless of the method at least two segments n i are required for each pipe to obtain a valid numerical grid the user method prioritizes the user s choice of simulation time step τ and sets the number of segments for the i th pipe as required by the moc method for that τ specifically n i round n i where n i ℓ i ω ˆ i τ and ℓ i is the length of the pipe the critical method allows the user to identify a critical pipe i e the user or default wave speed values would require the smallest time step to meet the cfl condition then the simulation time step is computed based on the critical pipe i as τ ℓ i 2 ω i with two segments and the number of segments for all the other pipes in the wtn is computed as in the user method for the user and the critical methods the wave speed adjustment is given by ϕ i n i n i the optimal method simultaneously adjusts the wave speeds for all the pipes in wtn by solving the least squares problem first the number of pipe segments for each pipe are computed based on the smallest permissible time step that satisfies the cfl condition i e τ min i ℓ i 2 ω i second the wave speed adjustments and the time step are computed by minimizing the sum of squared wave speed adjustments which has an analytical solution based on the normal equation i e ϕ τ argmin ϕ τ ϕ 2 τ ℓ i ω i ϕ i n i where ϕ denotes the vector of wave speed adjustments for every pipe in the wtn more details about the optimal method can be found in misiūnas 2008 users select the wave speed adjustment method using the wave speed method setting creating transient model defining a transient model requires creating a ptsnetsimulation object whose constructor function is imported in line 1 and executed in lines 22 through 27 in fig 8 when the ptsnetsimulation is created ptsnet creates a workspace folder in the current working directory which is identified by the parameter workspace name as shown in line 22 in fig 8 while files associated with results of a specific simulation are allocated in the workspace folder ptsnet internally allocates ram memory for the simulation and extracts wtn properties from the input file the path to the input file can be declared as shown in line 24 in fig 8 either explicitly or using the function get example path to select one of the 12 examples included in the library defining transient scenarios after the ptsnetsimulation is created users can define the transient scenario transient scenarios must be defined after the creation of the ptsnetsimulation object and before the simulation is executed i e between lines 28 and 36 in fig 8 for example in lines 30 and 31 in fig 8 a valve closure operation is defined using the define valve operation function which takes as inputs the valve label the initial and final settings and the maneuver start and end times in seconds valve settings are defined as fractional opening from zero closed to one fully open note that setting changes are linear from start to end times users requiring more control over behaviors can define custom operational maneuvers using time series specifying setting values y for specific times x as shown in lines 32 and 33 in fig 8 however time series operations are slightly different than standard linear open close operations in that a time series is interpreted as a step function i e setting changes occur instantaneously at a given time rather than linearly between times other transient scenarios that can be modeled with ptsnet include the operation of pumps pipe bursts leaks variable demands and surge protections the commands to operate pumps see fig 9 a are related to pump speed and are defined similarly to valve settings pump setting equal to one represents a pump operating at its full initial speed and zero means that the pump is off pump settings can be defined as linear functions with start and end times using the define pump operation command or with time series with the define pump settings function analogous to the define valve settings functionality the operation of the pump will be determined based on the characteristic pump curve defined in the inp input file which is used as a diagnostic equation in the model the equations for modeling transient scenarios are based on riaño briceño et al 2021a and presented in section s1 in the si users can also add a burst to the model using the add burst command shown in fig 9 b bursts in ptsnet are modeled using the orifice equation with outflow q ˆ κ h z where κ is the time varying discharge coefficient and z is the elevation at the location of the burst wylie et al 1993 the outflow associated with a burst is computed by coupling the orifice equation with the general junction equations introduced in riaño briceño et al 2021a see section s1 2 in the si the size of the orifice produced by the burst grows linearly between start and end times until it reaches the discharge coefficient defined in the add burst command orifices with constant discharge coefficients referred to as leaks can also be modeled with ptsnet yet they are not introduced through ptsnet commands instead users must define leaks in the epanet inp input file before executing the simulation with ptsnet and adjust the value of the emitter coefficient in the input file rossman 1994 ptsnet can also model variable demands as shown in fig 9 c by changing the discharge coefficient associated with the demand at a specific node variable demands need to be changed iteratively thus it is necessary to execute the simulation using a while loop step by step finally surge protection devices can also be modeled in ptsnet as shown in fig 9 d surge protection devices include open and closed surge tanks that absorb the wave shocks by compressing the air contained in the vessel in general open surge tanks are open to the atmosphere have infinite storage capacity and uniform cross section area which needs to be specified when modeling open surge tanks users need to specify the location and the cross section area of the tank as shown in line 2 of fig 9 d closed surge tanks are covered on top and have limited storage capacity when modeling the closed surge tank users need to specify the location of the tank its cross section area and height and the initial water level inside the tank as shown in line 4 of fig 9 d the equations for surge tanks are based on larock et al 1999 and presented in section s1 5 of the si 5 model execution a ptsnet simulation can be executed with one or multiple processors by saving a script similar to the one presented in fig 8 and then using the terminal command mpiexec n 2 python main script py where the n flag is used to specify the number of processors that will execute the ptsnet simulation in this case two within the script the execution is triggered either by invoking the sim run command as shown in lines 38 and 39 in fig 8 or using a while loop that advances the simulation step by step with the command sim run step as shown in line 6 in fig 9 c with respect to the initial conditions ptsnet always starts the wtn transient solution from a steady state condition developed using epanet with the users network configuration and initial settings ptsnet automatically runs the epanet model prior to the transient simulation without any prompting from the user however users should be aware that the epanet steady state solution has some limitations relative to the discretization of the dv moc network in particular since epanet only provides heads at the extremes of pipes and a single flowrate per pipe heads for the smaller pipe segments of dv moc are linearly interpolated from the pipe end values and a single flowrate is enforced along all segments of a single pipe when running ptsnet in parallel a single processor executes the epanet simulation and broadcasts steady state simulation results to the other processors once the processors receive the steady state results from epanet processors interpolate initial heads and populate initial flowrate values for the pipe segments assigned to them only after ptsnet has setup the initial conditions using epanet the dv moc transient simulation of heads and flowrates is computed from the user specified start to end time note that results are only extracted by setting save results to true in which case results will be extracted at every time step an will be temporarily allocated in a data structure during the simulation the results are converted and saved permanently in hdf5 files at the end of the simulation as shown in line 14 in fig 8 recall that hdf5 files are stored within a workspace folder whose name is defined when creating the ptsnetsimulation object as done in line 22 in fig 8 the jupyter notebook 1 simulate scenarios available on github demonstrates how to setup execute and extract simulation results for analytics functionalities that involve multiprocessing such as those that allow time step selection and determining the number of processors the execution requires a two step process first the function is invoked using a single processor then ptsnet automatically generates a script that is used in step two for running a suite of simulations and prints a line of code that needs to be executed in the terminal by the user finally the user executes the command in the terminal which triggers the suite of simulations via the openmpi standard the two step execution process for analytics functionalities is necessary in order to execute customized parallel simulations given that simulations with multiple processors can only be executed through the command line the jupyter notebook 3 analytics available on github demonstrates the analytics functionalities including analyzing simulation time step number of processors and wave speeds 5 1 time step selection the simulation time step determines the resolution accuracy and computational burden of a ptsnet model this is because the time step determines how much the wave speeds need to be adjusted to fulfill the cfl condition therefore the wave speed adjustment i e ω ˆ i ω i 1 can be used as a proxy to estimate the effect of the time step selection on simulation results ptsnet incorporates functionalities within the graphics module that facilitate the users to visualize the network topology and wave speed adjustments more specifically the plot wave speed error functionality in the graphics static module generates a plot of the network and creates a colored map with the wave speed adjustment values in order to execute this functionality users must previously define a ptsnetsimulation object and specify the name of the file for saving the plot as shown in lines 1 27 in fig 8 and line 8 in fig 10 respectively this method is executed prior to the transient simulation and only takes a few seconds to run even for networks with thousands of pipes hence it provides the users with a rapid selection of a time step that fits their modeling needs 5 2 determining the number of processors the analytics functionalities in ptsnet allow users to determine the optimal number of processors for their simulations considering that the performance of the library depends on the characteristics of the transient model e g model resolution duration number of boundary conditions and hardware specifications riaño briceño et al 2021a estimating the number of processors that maximize speedup and from which there is no improvement in computational performance is not straightforward as more processors are used in a simulation less time is spent on computing simulation steps but the time spent on communication increases also the sequential part of the program limits the speedup as shown in riaño briceño et al 2021a furthermore running a simulation multiple times using a different number of processors might become cumbersome when using a supercomputer or when a simulation takes too long to run to help the user manage these issues ptsnet includes functionalities to i estimate computational times and ii determine the optimal number of processors for a specific application first the compute simulation times functionality shown in line 7 in fig 10 allow users to run a set of simulations with different time step values and a given number of processors such that only a fraction of the simulation is executed and profiled based on the average computation time per step the compute simulation times functionality estimates the total computation time that will be required to execute the simulation second the compute num processors functionality shown in line 6 in fig 10 allows users to determine the number of processors that best fits their application by running a fraction of the simulation with various processor numbers and a fixed time step defined by the user users can specify the number of steps that they want to compute for the partial simulations and the average running times per step are computed for each processor this allows the user to determine the number of processors that provides the minimum running time per step for their application the resulting computational times are stored in temporary binary files within the workspace such that users can consult them even after the simulation is terminated see section 3 3 for more details 5 3 exporting results at the end of the simulation users can access simulation results including head and discharge at the nodes and flowrates at the start and end nodes of the links for all time steps of the simulation results associated with nodes and pipe extremes can be extracted from the sim node sim pipe start and sim pipe end data structures respectively see section 3 3 when operating with results users can use ptsnet in normal or persistent modes depending on the mode results will be retrieved from ram or disk memory under the normal mode users can extract results directly from ram after the transient simulation is completed without closing the current python session i e calling the results data structures after line 37 in fig 8 using lines 7 9 in fig 11 users operating under persistent mode can read results saved from previous simulations to use the persistent mode a copy of the workspace and results are saved in hdf5 files via simulation settings line 14 in fig 8 the persistent mode is activated by opening a simulation using python s with statement which ensures a safe manipulation of the workspace files as shown in line 6 in fig 11 in the code presented in fig 11 results are extracted via persistent mode by invoking the ptsnetsimulation constructor which retrieves data from the workspace saved with the name passed as an argument as a result the head for junction 23 is plotted as shown at the bottom of fig 11 workspaces persist in memory only if the user specifies so through the simulation settings when simulating for the first time under the normal mode after that users can assign a name to the workspace to differentiate results from future simulations the jupyter notebook 2 get results available on github demonstrates how to load results saved in previous simulations 6 results we assess the performance of the ptsnet package by i comparing the accuracy and computational times with other transient software using different transient scenarios ii performing scalability tests with large scale wtn and analyze the performance on a personal computer pc and high performance computer hpc and iii showcase the implementation of analytics functionality to explore wave speed adjustment and the effect of time step selection 6 1 transient software comparison the following example demonstrates the capability of ptsnet to model a pump shut off scenario in a mid scale network the simulation results of the ptsnet library executed on a single processor are compared against the tsnet python package an open source library xing and sela 2020 and bentley hammer v8i bentley systems 2019 a commercial software broadly used in both academia an industry to analyze transient phenomena in pipes we validate the physical results of the model by comparing the transient response of the system to different scenarios in terms of heads at a set of junctions in different locations in the network labeled in fig 12 the jupyter notebook 4 si figures available on github demonstrates how to replicate the results in this section and in the si example mid scale system consider the bwsn i network presented in fig 12 and adapted from ostfeld et al 2008 bwsn i comprises 126 nodes one reservoir 168 pipes two pumps and eight valves different transient events are generated including pump shut off burst and valve closure resulting pressure waves that propagate throughout the entire wtn for illustration purposes all pipes are assumed to have a wave speed value of 1200 m s and the transient event is simulated for 20 s using a time step of 5 ms 6 1 1 numerical results the pump shut off scenario illustrates how ptsnet models a transient event resulting from a controlled pump shut off at pump 172 by decreasing the pump rotational speed to zero starting at 0 and until 1 s into the simulation fig 13 shows the pressure at five different nodes in the network in response to pump shut off where a b and c correspond to simulation results using ptsnet hammer and tsnet respectively the results from the three solvers closely resemble attenuation and phase shift throughout the simulation period capturing down and up surges in pressure the minor discrepancies are attributable to the different wave speed adjustment schemes and boundary condition computation methods adopted by the three packages e g ptsnet adopts flow based equations whereas tsnet are velocity based and hammer s wave speed adjustment method differs from that of both ptsnet and tsnet overall all the methods show pressure waves generated by the pump propagating through the system as waves propagate their amplitude and shape changes according to the network topology the amplitude of waves attenuates or amplifies as a result of wave reflections and transmissions and the transient pressure observed at different locations of a wtn is an aggregated signal of multiple pressure waves hence locations farther from the origin of the transient do not necessarily exhibit lower amplitudes for example in fig 13 junction 30 senses the transient first while junction 16 experiences it last the node with the largest change in pressure is junction 90 which experiences a pressure drop of over 50 m after 7 s indicating that the pump shut off can generate significant transients in the wtn when operated quickly therefore it is essential to evaluate the impacts of and design appropriate procedures to guide pumping operations additional results comparing simulation results between the three solvers for burst valve closure and pump shut off with open and closed surge tanks are shown in figs s1 s4 in the si 6 1 2 computational performance results table 1 lists the running times for the different test cases executed by each package i e pump shut off valve closure and burst it is evident that ptsnet surpasses in performance both tsnet and hammer packages on average simulating the mid scale network takes around 6 s with ptsnet 700 s with tsnet and 45 s with hammer overall ptsnet ran roughly seven times faster than hammer in all the test cases and 116 times faster than tsnet notably for a fair comparison ptsnet simulations were executed using a single processor hence the speedups are largely attributed to vectorization of the moc equations 6 2 scalability test in this section we show performance tests for a large scale test case to illustrate the benefits of ptsnet over other transient modeling tools we focus on the computational aspect of the software showing simulation times when executing ptsnet in parallel we introduce a large scale test case using bwsn ii water system which is a well known benchmark in the water systems research community ostfeld et al 2008 and compare running times for three different numerical grid resolutions we also compare average running times per time step for the denser numerical grid using both pc and hpc systems we show that the optimal number of processors depends on simulation properties and on hardware specifications all the reported running times are wall clock times grama et al 2003 pc simulations ran on a 16 gb ram computer with an amd ryzen 7 5700u 4 5 ghz processor simulations performed on a supercomputer ran on the stampede 2 system stanzione et al 2017 and its intel xeon phi 7250 1 6 ghz computing nodes whose hardware specifications can be found in tacc texas advanced computing center 2020 the results presented in this section are not compared with commercial or open source software since it was not possible to execute the simulations for the large scale system initializing the bwsn ii network with tsnet takes several hours and hammer v8i crashes when executing the simulation with small time steps example large scale system the bwsn ii network is adapted from ostfeld et al 2008 bwsn ii comprises 12 526 nodes two reservoirs 14 824 pipes a single pump and six valves a transient event was generated by rapidly closing a valve that controls the flow in one of the main pipelines in the system within 10 s thus producing a pressure wave that propagates throughout the entire piped network for illustration purposes we assume that all pipes have a wave speed value of 1000 m s 6 2 1 comparing different numerical grid resolutions we model the large scale network using three different numerical grid resolutions to study the performance of ptsnet when dealing with highly computationally expensive simulations in general the model resolution is highly sensitive to the time step value given that small changes in time step result in large changes in problem size due to the cfl condition considering that the problem size grows exponentially for high resolution models simulating with ptsnet using a single processor can be computationally restrictive due to memory and running time limitations since ptsnet adopts the dv moc framework it ensures scalability for high resolution models by dividing the transient flow problem among multiple processors we report detailed results for three numerical grid resolutions table 2 lists the time steps and the corresponding number of solution points in the numerical grid and the number of time steps in the simulation in fig 14 a we present running times from 1 22 and 64 processors with simulation times categorized by subprocess i e initialization computation of interior points computation of boundary conditions and communication among processors with a single core system the simulation takes 879 2815 and 10 188 s for the numerical grid resolutions given by τ 1 τ 2 and τ 3 respectively overall as the number of processors increases the marginal improvement in running times decreases until no further improvement is observed analyzing the computational time invested in computing the different subprocesses we observe that the majority of the time running simulations with one processor is spent computing interior points for all the different numerical grid resolutions however once the number of processors increases the time spent in communication dominates the simulation time the computational time on interior points decreases since the number of interior points per processor decreases as the number of processors increases thus reducing the computational burden per processor however as the number of processors increases more partitions are generated thus increasing the number of information dependencies between processors which increases the cost of communication regardless of the time step it is demonstrated that communication becomes the bottleneck of the simulation for any sufficiently large set of processors simultaneously the time spend in the computation of interior and boundary points becomes minimal as the number of processors increases for a problem of fixed size note that initialization is almost negligible compared to the rest of the subprocesses across all cases furthermore the time spent computing boundary conditions accounts for roughly 5 of the total simulation time and remains virtually constant for 22 cores or more a speedup metric can be defined as s t 1 t k where t 1 is the time spent simulating with dv moc on a single processor and t k is the time spent simulating with k processors in general the speedups are more significant as the number of interior points in the numerical grid increases if we compare the running times for 1 and 64 processors when the time step is equal to τ 3 the speedup is 3 the speedup with τ 2 is 1 85 and with τ 1 the speedup is 1 43 thus it is not as efficient to use multiple processors for τ 1 since the communication overhead outweighs the reduction in the computation of solution points additionally higher speedups are achieved with denser numerical grids which can be attributed to the fact that cache mechanisms operate more efficiently as the number of contiguously allocated solution points increases riaño briceño et al 2021a 6 2 2 comparing pc with hpc systems in fig 14 b we show the performance in terms of average running time per time step for the large scale system bwsn ii running on both pc and hpc systems simulations running on both systems were identical with respect to simulation settings numerical grid resolution and the number of processors specifically we ran 2500 time steps with a time step of τ 3 0 0005 s we measured running times using ptsnet s profiler module and averaged running times per step including computation of solution points and communication between processors as evidenced by fig 14 b running times per time step differ significantly between the pc and hpc simulations the pc running times are faster than the hpc as long as the number of processors is less than 8 additionally for the hpc the computational time decreases as the number of processors increases however for pc an initial decrease in computational times is observed when increasing the number of processors from one but as more processors are added the overall computational times increase the latter can be attributed to the different computational burdens between pc and hpc systems it might be surprising that the pc runs faster than the hpc yet if we compare the core specifications of both machines the pc cores run almost three times faster than the supercomputer cores due to the differences in frequencies i e 4 5 ghz vs 1 6 ghz nevertheless the hpc performance surpasses the pc as the number of processors is increased given that the hpc architecture has been optimized to minimize the communication burden between processors communication rapidly becomes a bottleneck for the pc with far fewer cores than for the hpc system the optimal number of processors for the pc is three while the optimal number of processors for the hpc is estimated to be eight using the elbow of the curve as the selection criterion see fig 14 b the elbow point is a commonly used heuristic to determine the optimal balance between computational cost and speedup grama et al 2003 6 3 wave speed error referring back to section 5 1 the simulation time step determines the resolution accuracy and computational burden of a transient model selecting a time step that results in a grid resolution that ensures high accuracy and fast running times is ideal yet these goals are conflicting finer grid resolutions provide higher accuracy but also demand higher computational resources and are likely to lead to communication bottlenecks and slower execution ptsnet provides functionalities to estimate the time step automatically balancing the trade off between accuracy and computational burden bentley systems 2022b for example for large scale networks hammer v8i assigns more weight to the computational cost hence the automatic time step selection results in high simulation errors in such cases users need to manually tune the time step testing different grid resolutions by iteratively running very long simulations to overcome this manual process we propose using the wave speed adjustment error as a proxy for the simulation error allowing the user to visualize the error before running the simulation visualizing the wave speed error produced by the resolution of the numerical grid facilitates identifying parts of the network where the error will be higher and determining whether or not a time step selection is satisfactory for a particular application for example a user might find it acceptable to have a time step that results in acceptable wave speed error in the study region and localized high wave speed error far from the area of study also users can define a time step that minimizes the problem size such that the majority of the network pipes remain below a certain wave speed error threshold to illustrate we used ptsnet analytics functionalities to plot the wave speed error for two time steps τ 1 0 01 s and τ 2 0 1 s the resulting numerical grids have 199 923 and 46 642 solution points for τ 1 and τ 2 respectively thus τ 1 provides a higher resolution and accuracy compared to τ 2 in fig 15 we show the relative wave speed error defined as in riaño briceño et al 2021a for both time steps zooming on a section of the bwsn ii network the plot shows that error remains below 10 for almost every pipe in the network when using the smaller time step for τ 1 whereas for τ 2 the error is greater than 10 for almost every pipe in the system even though the number of solution points became 3 2 times greater for τ 1 compared to τ 2 which can result in longer simulation times users may prefer using the smaller time step and exploit ptsnet parallel capabilities to run this test case 7 conclusion this paper presents ptsnet an open source python package for parallel transient simulation in wtns all the source code software documentation and multiple examples including input files and codes are provided within the package and can be downloaded from the github repository riaño briceño et al 2022 the capability and user interaction with ptsnet are demonstrated through the detailed simulation examples of pump shut off valve closure burst and surge tank the ptsnet package is proven to be computationally efficient compared to other open source and commercial packages unlike other packages we demonstrate that ptsnet is scalable and capable of running large scale transient simulations providing significant speedups to its users thanks to the adoption of vectorization and distributed computing in addition the package offers essential analytic tools for users to quickly determine the best time step and number of processors for their application ptsnet does not include all the modeling capabilities of the commercial software instead it is designed to provide simulation capabilities for transient modeling in wtns for the research community that is currently not available in open source software including petsc abhyankar et al 2020 and tsnet xing and sela 2020 ptsnet is under continuous maintenance improvement and development future work includes extending the solver to generalize to other pde based models e g open flow channel modeling 2d and 3d dynamics li and hodges 2021 morales hernández et al 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors acknowledge the texas advanced computing center tacc at the university of texas at austin for providing hpc resources that have contributed to the research results reported within this publication this work was supported in part by the national science foundation us under award 2015658 and cooperative agreement no 83595001 awarded by the u s environmental protection agency to the university of texas at austin it has not been formally reviewed by epa the views expressed in this presentation are solely those of the authors and do not necessarily reflect those of the agency epa does not endorse any products or commercial services mentioned in this publication appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2022 105554 appendix a supplementary data the following is the supplementary material related to this article mmc s1 it includes dv moc equations selectors to compute the dv moc with ptsnet and a comparison of software packages 
25505,modeling transient flow in water transport networks wtns is characterized by hyperbolic partial differential equations existing commercial and open source software for transient modeling in wtns have limitations such as lack of scalability and compatibility with high performance computers difficulty to systematically execute simulations and analyze results this work proposes a novel open source python package that relies on vectorization and distributed computing to overcome the limitations of existing software the proposed library the parallel transient simulator for water networks ptsnet surpasses in computational performance existing modeling tools and incorporates novel analytics functionalities ptsnet has been tested on wtns composed of tens hundreds and thousands of hydraulic elements using a personal and a supercomputer running from tens to hundreds of processors we show through rigorous analyses that ptsnet is scalable accurate and significantly speeds up simulations with sufficiently dense numerical grids keywords transient flow networked water systems parallel computing high performance computing message passing interface data availability all data and codes are available in a public github repository that was referenced in the manuscript https github com gandresr ptsnet software availability name of software ptsnet software requirements python 3 6 3 7 3 8 3 9 and python dependencies i e numpy tqdm scipy h5py pandas wntr mpi4py numba availability ptsnet source codes are available from a github repository at https github com gandresr ptsnet 1 introduction modeling hydraulic processes in environmental systems is critical for management operation and control of water transport networks wtn in natural and urban environments particularly modeling transient scenarios such as the surcharge of water mains in drainage systems or sudden pressure changes in water distribution systems due to the operation of pumps is essential to prevent failures that may result in high economic losses boulos et al 2005 failures attributed to rapid flow accelerations and drastic changes in pressure include floods system fatigue or pipe ruptures pump and device malfunctioning and intrusion of pathogens and contaminants through leaks cracks and other defects boulos et al 2005 lechevallier et al 2003 fox et al 2014 this paper focuses on modeling transients in wtns which involves solving partial differential equations pdes that describe the water transport phenomenon in terms of conservation of mass and momentum relationships pdes for transient flow modeling can be solved using a variety of numerical methods including the method of characteristics moc wylie et al 1993 nault et al 2018 finite difference methods chaudhry and hussaini 1985 kiuchi 1994 blanco et al 2015 verdugo et al 2019 cao et al 2020 or finite volume methods zhao and ghidaoui 2004 castro et al 2006 fernández pato and garcía navarro 2014 mesgari sohani and ghidaoui 2019 overall moc has been predominantly used over other numerical schemes due to its ease of implementation and accuracy however since most wtns are composed of thousands of hydraulic elements and cover extensive areas modeling transient phenomena for realistic wtns can be intractable with current simulation tools recent advances in computational fluid dynamics demonstrate that distributed computing and vectorization can significantly speed up computations for modeling water transport in environmental and urban systems in lin and zhang 2021 the authors propose a methodology that divides hydrologic and hydraulic zones for a 2d flood simulation computing the hydrodynamic behavior of the zones across multiple processors the model runs 10 times faster than traditional sequential implementations yet sacrifices accuracy for computational performance similarly in burger et al 2014 the authors present a parallel version of the epa s storm water management model swmm that runs six to ten times faster than sequential swmm on a twelve processor system in carlotto et al 2021 the authors developed sw2d gpu an open source 2d shallow water model that parallelizes computations across multiple processing units in a general purpose graphic processing units gpgpus achieving 36 times faster running times similarly anguita et al 2015 proposes a 3d semi implicit hydrodynamic model for shallow water that can be executed in parallel in all the previous works the improvement in computational efficiency is typically greatest when running highly dense numerical grids which are sometimes necessary to capture nonlinearities in large scale simulations abhyankar et al 2020 likewise there is a generalized adoption of partitioning algorithms and distributed computing to accelerate models in water systems zhu et al 2019 xu et al 2021 tiernan and hodges 2022 for example there have been significant advancements in the parallelization of shallow water equations which are essential for open flow modeling e g river and drainage modeling speeding up simulations several orders of magnitude carlotto et al 2021 burger et al 2014 however fewer works have focused on studying the problem of parallelization to solve transient flow in pressurized wtns in the context of urban water systems packages for transient simulation in wtns include commercial software such as bentley hammer bentley systems 2019 kypipe kypipe 2019 infosurge innovyze 2019 and transam mcinnis and karney 1998 commercial packages share similar functionalities and are of limited use for large scale applications commercial packages come with limitations in computational efficiency such as lack of scalability and compatibility with high performance computers moreover the usability of commercial packages is limited since commercial codes are delivered to users as black boxes and thus do not provide opportunities for improving state of the art models modifying or extending its functionalities nor running transient simulations systematically i e through program commands rather than interacting with a graphical user interface gui therefore it is difficult to extract analyze and visualize numerical results on the other hand open source packages can be categorized as transient focused and general purpose transient focused packages which solve the specific system of hyperbolic pdes that models transient flow in wtns include tsnet xing and sela 2020 and the free open source implementation of moc presented in kjerrumgaard jensen et al 2018 transient focused packages are computationally inefficient with respect to commercial packages when modeling medium and large size wtns conversely general purpose packages allow users to solve a systems of hyperbolic pdes defined by the user some of the available general purpose packages include modelica fritzson 2011 openfoam jasak et al 2007 fenicsx scroggs et al 2021 and petsc s dmnetwork abhyankar et al 2020 general purpose packages incorporate state of the art methods to run distributed computations across multiple processors but are typically based on implicit numerical schemes whose solution is found via adjoint or iterative methods e g conjugate gradients which have proven to be computationally inefficient when running in parallel for inherently sparse systems such as wtns burger et al 2016 abhyankar et al 2020 more critically the usability of general purpose packages can be cumbersome for users that need to run transient simulations of wtns because users need to manually define the topology of the wtn and its properties to formulate the system of pdes and its boundary conditions defining simulation parameters for a large scale simulation can be highly time consuming and prone to errors given the multitude of boundary conditions that are present in wtns this study presents and demonstrates a new open source software package the parallel transient simulation in networks ptsnet that addresses the limitations of computational efficiency and code usability of both commercial and open source packages for computational efficiency we build upon our dv moc work in riaño briceño et al 2021a which uses vectorization and distributed computing for efficient computation in riaño briceño et al 2021a we demonstrate 1 a new vectorized formulation of the transient flow equations which is specifically designed to fit an efficient memory allocation scheme and 2 a parallel algorithm that distributes the load among processors to further speed up vectorization and adopts to general wtns topologies and many boundary conditions the numerical solution uses an explicit time marching scheme to reduce the computational overhead of parallel communications the readers are referred to riaño briceño et al 2021a for the theoretical derivations this paper contributes to the 1 definition of the software architecture to support the novel vectorized and parallel scheme for general wtns and 2 development of new analytics tools such as automated input file processing selection of the best time step and number of processors for a transient simulation running in parallel the code is written in python using openmpi and hdf5 libraries that are compatible with high performance computing systems for code usability we equip ptsnet with functionalities to automatically extract the wtn properties from standardized input files i e the epanet inp file rossman 1994 which have been widely adopted in both industry and academia sela and housh 2019 sela et al 2019 in addition ptsnet contains a set of functionalities that allows users to extract and analyze simulation data both quantitatively and visually facilitating parameter selection and the storage retrieval and analysis of results the remainder of the paper is organized as follows section 2 provides an overview of the dv moc transient flow model and ptsnet functionalities section 3 describes the underlying architecture of the library and its main data structures for users interested in building new functions sections 4 and 5 focus on explaining model setup and execution through code snippets in section 6 the package is validated for modeling medium and large size networks by comparing simulation results with widely accepted computational packages for transient modeling additionally ptsnet s performance is compared with that of its counterparts showing that the dv moc offers significantly better simulation times and extends the capabilities of currently available software finally in section 7 conclusions and future directions are provided 2 software description ptsnet is an open source python package capable of computing the dynamics of flowrates q x t and hydraulic heads h x t in time t and space x for a pressurized wtn under the effect of transient scenarios such as valve opening and closure pump start up and shut off and bursts modeling wtn dynamics in ptsnet requires defining the wtn properties using the industry standard epanet inp format defined by rossman 1994 input files can be generated systematically using code scripts based on the wntr package klise et al 2018 or via epanet s gui rossman 1994 ptsnet relies on other open source python packages that need to be installed through a conda environment following the instructions provided in ptsnet s repository riaño briceño et al 2022 in particular we chose the wntr package over other available libraries that can systematically extract information from general text files because wntr is supported and maintained by the us environmental protection agency epa that is responsible for defining the inp file standard moreover we use mpi4py to handle the open message passing interface openmpi standard which enables distributed programming and is broadly used in industry and academia graham et al 2005 once all the dependencies are installed ptsnet s installation is done via the pypi repository using the pip install ptsnet terminal command as described in riaño briceño et al 2022 we note that since ptsnet is continuously updated by using the previous command users will install the latest version of the package to install the version presented in this paper users should use pip install ptsnet 0 1 7 ptsnet s functionalities can be categorized into four distinct groups 1 model setup 2 execution of simulation 3 extraction of results and 4 analytics as illustrated in fig 1 the first three groups of functionalities allow users to run a transient simulation using one or multiple processors and create workspaces to save simulation results the fourth group are functionalities to help with model setup and results analysis the remainder of this section presents a description of the underlying transient flow model and describes ptsnet s functionalities 2 1 transient flow model transient flow within pipes is characterized by a system of hyperbolic pdes that describes conservation of mass and momentum relationships wylie et al 1993 these equations are prognostic since they predict the value of flowrates and hydraulic heads for some time in the future based on current time conditions holton 1973 when pipes are connected to other wtn elements such as valves pumps leaks bursts surge protections reservoirs dead ends and demands transient flow is modeled by coupling prognostic and diagnostic equations the latter being equations that link flowrates and or heads at identical times holton 1973 ptsnet is based on the distributed and vectorized method of characteristics dv moc of riaño briceño et al 2021a which has advantages for accuracy ease of implementation and suitability for large scale simulations with significantly lower computational times than sequential implementations details of the dv moc method are found in riaño briceño et al 2021a with an abbreviated description provided below the dv moc time marching scheme can be executed by one or multiple processors to solve conservation of mass and momentum equations wylie et al 1993 1 h x t t ω 2 g a q x t x 0 2 h x t x 1 g a q x t t f x t 0 where h and q represent head and flowrate respectively a is the cross section area of a pipe ω is the pressure wave celerity referred to as wave speed from now on and g is the gravitational acceleration the term f x t in eq 2 represents the friction head loss per unit length estimated for steady quasi steady and unsteady conditions nault et al 2018 in this work we solve eq 2 using a steady friction model where f x t q x t q x t f 2 g d a 2 d is the pipe diameter and f is the darcy weisbach dimensionless friction factor for pipes derivations of prognostic eqs 1 and 2 and their numerical solution can be found in riaño briceño et al 2021a and riaño briceño et al 2021b the formulation of diagnostic and prognostic equations for hydraulic devices e g valves pumps is described in section s1 of the supporting information si the dv moc numerical scheme is guaranteed to be linearly stable for model time steps satisfying the courant friedrichs lewy cfl condition courant et al 1967 this behavior is achieved by discretizing longer pipes lengths into segments and adjusting the local wave speed values for the global time step to ensure that the cfl number remains less than 1 for every pipe segment in the network this local adjustment of wave speed is a standard approach for moc solutions wylie et al 1993 the discrete versions of eqs 1 and 2 are solved at the solution points which can be classified as interior and boundary points depending on their association with the wtn elements while interior points are those within pipes boundary points are those where the coupling between eqs 1 and 2 with boundary conditions occur riaño briceño et al 2021a to illustrate fig 2 shows a wtn formed by several pipes in series connecting an upstream reservoir through a pump to supply a downstream demand regulated by a valve the flow moves across the x axis from left to right downstream the pump there is a surge protection device and upstream the valve there is a leak the pipes of the wtn in fig 2 are discretized into pipe segments of length δ forming interior points marked with black dots and boundary points associated with the reservoir general junction valve pump and surge protection device marked at the ends of the pipes with icons that differentiate the types of the boundary points the relationship between the system and its model abstraction as illustrated in fig 2 is useful in understanding how simulation settings time step wave speed and the δ segment length will affect both the computational time and model error smaller time steps faster wave speeds and smaller pipe segments will provide a more precise solution but at the cost of increased computational time we further discuss these settings in section 4 2 2 functionalities using ptsnet involves using the functionalities presented in fig 1 to set up a transient model determine simulation settings such as time step and number of processors execute a parallel simulation and extract results a simulation can be initialized using model setup functionalities which extract information from the input file allocate memory for the model compute initial conditions define transient scenarios and run a compatibility check the compatibility check validates that the wtn meets ptsnet s connectivity requirements as follows valves and pumps must be connected to a single pipe upstream and downstream reservoirs can only be connected to valves or pumps through a pipe no leaks or demands are allowed at valves or pumps demand values cannot be negative and valves cannot have zero head loss if their initial flowrate is greater than zero as part of the model setup users must define the number of processors and the simulation time step either manually or using analytics functionalities as described in sections 5 1 and 5 2 once the model is set up users run a ptsnet simulation read and plot results after completion using execution and extraction functionalities respectively note that model setup and execution functionalities are invoked in the same python script and separate scripts can be used to invoke extraction and analytics functionalities the next section describes the software architecture and its underlying data structures followed by demonstration of ptsnet functionalities through snippets of code 3 software architecture this section describes the underlying mechanisms of vectorization and distributed parallel computing that run within ptsnet including specifications of the data structures that enable these mechanisms using the small scale example described below and presented in fig 3 we show how vector operations take place when solving eqs 1 and 2 numerically how tasks are divided between processors and how information is exchanged between processors to compute an moc step furthermore we describe how simulation files are stored and organized within workspaces and how array data structures are stored in memory using parallel hdf5 example small scale system the small scale test case presented on the top left corner of fig 3 comprises a reservoir nine pipes five general junctions and a valve that regulates water demand downstream as described in section 2 1 each pipe in the system is divided into segments such that heads and flowrates are computed at solution points which are enumerated from 0 to 32 boundary points at the extreme of the pipes are marked with squares and interior points within the pipes are marked with ticks for illustration purposes the network data is partitioned among three processors fig 3 bottom thus creating information dependencies between processors fig 3 top right 3 1 vectorization modern central processing units cpus have single instruction multiple data simd functionalities grama et al 2003 which allow simultaneous computations of basic operations such as addition and multiplication this capability referred to as vectorization is executed when a loop is replaced in the code by an simd instruction so that instead of computing a single operation for a single solution point at every time step multiple solution points can be processed simultaneously in a fraction of the time grama et al 2003 the first step towards vectorization consists of expressing the numerical solution of eqs 1 and 2 in terms of vector operations to illustrate we show how flowrates are computed for a partition of the small scale example based on the dv moc framework flowrates at interior points located at x and time t are computed based on the previous solution in time at time t minus the time step τ as follows 3 q x t w x t τ h x t u x t τ c h x t d x t τ b x t τ c w x t τ d x t τ b x t τ u x t τ where b x t ω g a b x f δ 2 g d a 2 r x abs q x δ t d x t h x δ t b x q x δ t u x t b x r x abs q x δ t w x t h x δ t b x q x δ t the information associated with each interior point is stored in custom vector data structures that inherit the properties of a numpy array in addition we store the indexes associated with interior points on a separate array referred to as selector in the case of the small scale example if processor 3 is in charge of computing the points in the network that are marked in blue the selector of interior points for processor 3 is given by 22 23 24 25 26 27 28 31 once the selector is defined eq 3 can be solved either with sequential or vectorized programming as shown in fig 4 when the sequential code is executed in a cpu with eight pipelines the computation of flowrate vector entries takes eight iterations one iteration per pipeline sequentially whereas the vectorized computation takes a single iteration eight pipelines computed simultaneously vectorization is executed via numpy s fancy indexing functionality as shown in fig 4 to create new vectors with contiguously allocated data of w b d and u based on the corresponding selector once the vectors are created vector operations take place and flowrates and heads are updated note that the number of simultaneous operations is limited and depends on the architecture of the cpu for example for intel s xeon phi x200 knights landing cpus which we used to produce some of the results presented in section 6 the maximum number of simultaneous fused multiply add operations per cycle is eight when using 64 bit floating point data mccalpin 2022 one of the main challenges of implementing vectorization and making it practical for efficient numerical operations for networked systems is the proper definition of selectors since vector operations are invoked recurrently within ptsnet we decided to pre compute and pre allocate all the necessary selectors to replace any for loop with array instructions by doing so we avoid allocating and deallocating selectors at every iteration this is possible given that the structure of the numerical grid remains constant throughout the simulation table s1 in the si lists the selectors for points and nodes that can be accessed via the ptsnetsimulation worker where dictionary the ptsnetsimulation worker where data structure is broadly used within the simulation funcs module as well as within the results module facilitating not only the computations of results but also the extraction of results data that are used in vectorized operations are stored within a ptsnettable the ptsnettable was designed to allow data extraction either by label or by a collection of indexes users will mostly interact with two types of ptsnettable data structures i e system states and results system states refer to a collection of tables that store properties of different system elements specifically these are nodes pipes valves pumps open and closed surge tanks each element type is associated with a specific set of states which can be constant or variable as defined in the source code see the simulation constants module table properties are stored in ordered arrays such that every position in the array is associated with a specific element label since element labels cannot be used to extract information out of ordered arrays in python tables internally translate element labels into array indexes as shown in fig 5 table indexing is done through a python dictionary that operates as a hash function f k z taking a label k k and mapping it to a positive index the associated collections of states are stored within the ptsnetsimulation ss dictionary in order to extract information about system states users need to use a special syntax to illustrate reading the initial head ihead at a specific node involves calling the ptsnettable extracting the ihead property and using a numerical index or a label to extract the value as shown in fig 5 in the example the initial head at the node junction 1 is extracted 3 2 distributed parallel computing ptsnet leverages the computational power of multiple processors using distributed parallel computing in practice the overall moc problem is divided in smaller parts such that every processor concurrently computes a smaller vectorized moc problem when distributed parallel computing is used each processor has its own private memory making it necessary to exchange information among processors once information is passed processors perform computations independently from each other spending different amounts of time to run a single time step therefore processors need to synchronize with each other at the end of each time step to ensure that all the solutions of heads and flowrates at time t are available to compute the solutions at time t τ hence given k processors the networked numerical grid must be partitioned such that the time spent in communication between processors is minimized and the load on each processor is balanced in general we assume that boundary conditions can only be computed by one processor and because of this boundary points belonging to the same boundary condition are assigned to a single processor for example in fig 3 top left we show the corresponding subgraphs that result from a three way heuristic partitioning the graph that represents the networked numerical grid consists of thirty three points that are divided into three sub graphs each of them containing 16 18 and 12 points respectively moreover processors 1 and 2 exchange information from points 11 14 and 17 coming from processor 2 and points 10 13 and 16 coming from processor 1 therefore information dependencies are bidirectional given that processors not only receive information from their neighbors but also send information to them the partitioning algorithm included within ptsnet achieves balance of work between processors by dividing the number of solution points into approximately equal parts for example creating a simulation with the small scale network using a time step of 0 07 s produces a numerical grid with 33 solution points this simulation is partitioned to be executed by three processors hence each processor allocates memory for the points it is in charge of plus the external information dependencies necessary to compute the moc equations each point in the network receives a global index used to determine information dependencies and export results and a local index to facilitate local vector operations and information retrieval local operations are those executed by an individual processor for a specific region of the wtn e g computation of specific solution points in contrast global operations are those executed for the entire wtn by multiple processors e g storing results for the example presented in fig 3 notice that processor 1 is in charge of computing points 0 11 13 and 16 however to compute the solution of boundary points 8 13 and 16 it is necessary to use information from solution points 7 14 and 17 therefore processor 2 needs to communicate information from points 14 and 17 to processor 1 and processor 1 needs to allocate additional space for the incoming information indexes pointing to local spaces in memory for the incoming information dependencies are referred to as send and receive buffers and can be accessed through the ptsnetsimulation worker send buffer and ptsnetsimulation worker recv buffer statements buffers are used to build the information dependencies graph idg which is a special data structure created with the openmpi standard that establishes connections between processors to exchange information based on the idg ptsnet builds communicator data structures that define groups of processors that need to exchange data groups of processors locally exchange data using the neighbor all to all communication protocol grama et al 2003 in summary each processor independently computes dv moc as presented in riaño briceño et al 2021a first the program is initialized and then computations of solution points are distributed among processors by performing the partitioning of the networked numerical grid once the partitioning is defined the idg is built in a distributed fashion i e each processor receives a portion of the network and defines its dependency to other processors afterwards all the processors start running their part of the vectorized step synchronizing and exchanging information at the end of the time step this process is repeated iteratively until the duration of the simulation is completed 3 3 file management results are not centrally stored in either memory or written to a file as the simulation is executed instead results on each processor are stored in memory that only that processor can access however ptsnet uses the hdf5 library to unify simulation results in a single binary file at the end of the simulation collette 2013 this approach is more efficient than writing either a unified file or separate results files during runtime however it has the disadvantage that a model crash will generally produce no written output of results before the crash the unified output file is concurrently processed and written using the functionalities of the hdf5 library collette 2013 this single standardized binary file facilitates read operations and reduces the complexity of parsing data when extracting results for analyses at the end of a simulation ptsnet stores the results within the flowrate h5 demand flow h5 head h5 and leak flow h5 files as presented in fig 6 other workspace files include initial conditions profiler and simulation properties note that within ptsnet jargon initial conditions refers to data extracted from the epanet input file but rearranged in ptsnet s data structures profiler files store information of running times for the initialization computation of interior and boundary points and communication between processors finally simulation properties refers to all the constant values that are used throughout the simulation in order to compute the dv moc method the files composing a workspace are locally stored in the directory from which the user executes the simulation users extract data via ptsnet s persistent mode which accesses i initial conditions computed with epanet ii simulation times iii flowrates at the start and end of pipes iv demand and leak flowrates at nodes and iv the head at every node in the network users do not have to directly manipulate these files instead the workspace data is accessed through ptsnet objects whose information is internally loaded from the files if the persistent mode is active using this approach results are extracted invoking the ptsnetsimulation object initial conditions are found within the system ptsnetsimulation ss dictionary and running times are stored in the ptsnetsimulation worker profiler note that information stored in pkl files is internally used by ptsnet and stored in the ptsnetsimulation the workspace folders can occupy a large block of memory during the simulation depending on the size of the problem therefore users should carefully consider how much simulation data to store in the workspace ptsnet provides flexible control of storage through functionalities to read results list workspace information and delete a specific workspace these functionalities are found within the results workspaces module to allocate time series results ptsnet uses the ptsnettable2d data structure which is an extension of a ptsnettable with time series as table entries results are allocated within the sim results dictionary and then unified and updated at the end of the simulation through the ptsnetsimulation object internal manipulations simplify the syntax to extract results for example whenever ptsnetsimulation node is invoked information is either loaded from ram or disk and extracted from the ptsnetsimulation results node data structure results can be extracted for nodes and pipe extremes i e pipe start and end segments in the case of nodes users can extract heads leak and demand flowrates and for pipes users can extract flowrates pipe flowrates will always be given with positive values if users require the sign of the flowrate to match the flow convention of the epanet model used as input the flowrate time series can be multiplied by sim ss pipe direction note that a ptsnet simulation can run out of memory if the results for all time steps and all solution points are stored to minimize the use of memory resources and ensure scalability the time marching algorithm of ptsnet routinely stores point data for the minimum number of time steps necessary to advance in time i e two consecutive time levels these are referred to as the memory pool of points mem pool points and their data are stored in ptsnettable2d as the simulation advances ptsnet switches between table rows in order to compute the next time step i e initial conditions at time t 0 are stored in the first row of the table then a step is taken and the solution of time t τ is stored on the second row when the third step t 2 τ is computed the solution is stored on the first row using the solution of t τ therefore accessing high resolution point data at every time step requires manually extracting the necessary results from the memory pool of points making use of proper selectors such as the ones described in section s2 of the si even though point data is only stored for two time steps data at the extreme of pipes is available for every time step results at pipe extremes can be accessed either directly from ram or from the hdf5 disk file via the ptsnetsimulation as shown in fig 7 4 model setup this section illustrates different use cases through snippets of code that exercise ptsnet s functionalities these snippets can be used to model specific transient scenarios extract results and determine time step and number of processors additional example codes for using ptsnet are available in riaño briceño et al 2022 setting up a transient model with ptsnet involves i defining simulation settings in the form of a python dictionary ii creating transient model and iii defining the transient scenario as described next defining simulation settings ptsnet provides default settings see lines 7 through 20 in fig 8 users can adjust these values to increase the temporal and spatial resolution of the model extend the duration of the simulation set up wave speed values for each pipe turn on and off secondary processes such as displaying messages on the terminal running a compatibility check measuring simulation times and saving results secondary processes see lines 15 through 19 in fig 8 can also be turned off to reduce simulation times for instance once ptsnet determines that a wtn is compatible with the transient flow model there is no need to run the compatibility check again if the same input file is executed multiple times the temporal settings such as the time step and duration should be given in seconds ptsnet transient analysis initiates from a steady state condition computed by epanet in the event that the epanet inp file includes an extended period simulation eps i e the steady state simulation spans multiple hours users need to select the time of the steady state condition to initialize the transient simulation the period setting is the index associated with the time period within the eps that will be used to calculate the initial condition for the transient simulation to define the wave speed values for pipes users can either i set the same value for all pipes using the default wave speed setting or ii define specific wave speed values using a text file containing pipe labels on the first column and wave speed values on the second the disk path to access the text file with wave speed information must be defined via the wave speed path setting note that in any moc method wave speed values are adjusted by the moc algorithm by a factor ϕ i e an adjusted wave speed ω ˆ i ϕ i ω i is applied for each i pipe thus the user input values or the default values are merely starting points for the adjusted wave speeds three common wave speed adjustment methods are included in ptsnet the user bentley systems 2022a the critical wylie et al 1993 and the optimal misiūnas 2008 methods note that regardless of the method at least two segments n i are required for each pipe to obtain a valid numerical grid the user method prioritizes the user s choice of simulation time step τ and sets the number of segments for the i th pipe as required by the moc method for that τ specifically n i round n i where n i ℓ i ω ˆ i τ and ℓ i is the length of the pipe the critical method allows the user to identify a critical pipe i e the user or default wave speed values would require the smallest time step to meet the cfl condition then the simulation time step is computed based on the critical pipe i as τ ℓ i 2 ω i with two segments and the number of segments for all the other pipes in the wtn is computed as in the user method for the user and the critical methods the wave speed adjustment is given by ϕ i n i n i the optimal method simultaneously adjusts the wave speeds for all the pipes in wtn by solving the least squares problem first the number of pipe segments for each pipe are computed based on the smallest permissible time step that satisfies the cfl condition i e τ min i ℓ i 2 ω i second the wave speed adjustments and the time step are computed by minimizing the sum of squared wave speed adjustments which has an analytical solution based on the normal equation i e ϕ τ argmin ϕ τ ϕ 2 τ ℓ i ω i ϕ i n i where ϕ denotes the vector of wave speed adjustments for every pipe in the wtn more details about the optimal method can be found in misiūnas 2008 users select the wave speed adjustment method using the wave speed method setting creating transient model defining a transient model requires creating a ptsnetsimulation object whose constructor function is imported in line 1 and executed in lines 22 through 27 in fig 8 when the ptsnetsimulation is created ptsnet creates a workspace folder in the current working directory which is identified by the parameter workspace name as shown in line 22 in fig 8 while files associated with results of a specific simulation are allocated in the workspace folder ptsnet internally allocates ram memory for the simulation and extracts wtn properties from the input file the path to the input file can be declared as shown in line 24 in fig 8 either explicitly or using the function get example path to select one of the 12 examples included in the library defining transient scenarios after the ptsnetsimulation is created users can define the transient scenario transient scenarios must be defined after the creation of the ptsnetsimulation object and before the simulation is executed i e between lines 28 and 36 in fig 8 for example in lines 30 and 31 in fig 8 a valve closure operation is defined using the define valve operation function which takes as inputs the valve label the initial and final settings and the maneuver start and end times in seconds valve settings are defined as fractional opening from zero closed to one fully open note that setting changes are linear from start to end times users requiring more control over behaviors can define custom operational maneuvers using time series specifying setting values y for specific times x as shown in lines 32 and 33 in fig 8 however time series operations are slightly different than standard linear open close operations in that a time series is interpreted as a step function i e setting changes occur instantaneously at a given time rather than linearly between times other transient scenarios that can be modeled with ptsnet include the operation of pumps pipe bursts leaks variable demands and surge protections the commands to operate pumps see fig 9 a are related to pump speed and are defined similarly to valve settings pump setting equal to one represents a pump operating at its full initial speed and zero means that the pump is off pump settings can be defined as linear functions with start and end times using the define pump operation command or with time series with the define pump settings function analogous to the define valve settings functionality the operation of the pump will be determined based on the characteristic pump curve defined in the inp input file which is used as a diagnostic equation in the model the equations for modeling transient scenarios are based on riaño briceño et al 2021a and presented in section s1 in the si users can also add a burst to the model using the add burst command shown in fig 9 b bursts in ptsnet are modeled using the orifice equation with outflow q ˆ κ h z where κ is the time varying discharge coefficient and z is the elevation at the location of the burst wylie et al 1993 the outflow associated with a burst is computed by coupling the orifice equation with the general junction equations introduced in riaño briceño et al 2021a see section s1 2 in the si the size of the orifice produced by the burst grows linearly between start and end times until it reaches the discharge coefficient defined in the add burst command orifices with constant discharge coefficients referred to as leaks can also be modeled with ptsnet yet they are not introduced through ptsnet commands instead users must define leaks in the epanet inp input file before executing the simulation with ptsnet and adjust the value of the emitter coefficient in the input file rossman 1994 ptsnet can also model variable demands as shown in fig 9 c by changing the discharge coefficient associated with the demand at a specific node variable demands need to be changed iteratively thus it is necessary to execute the simulation using a while loop step by step finally surge protection devices can also be modeled in ptsnet as shown in fig 9 d surge protection devices include open and closed surge tanks that absorb the wave shocks by compressing the air contained in the vessel in general open surge tanks are open to the atmosphere have infinite storage capacity and uniform cross section area which needs to be specified when modeling open surge tanks users need to specify the location and the cross section area of the tank as shown in line 2 of fig 9 d closed surge tanks are covered on top and have limited storage capacity when modeling the closed surge tank users need to specify the location of the tank its cross section area and height and the initial water level inside the tank as shown in line 4 of fig 9 d the equations for surge tanks are based on larock et al 1999 and presented in section s1 5 of the si 5 model execution a ptsnet simulation can be executed with one or multiple processors by saving a script similar to the one presented in fig 8 and then using the terminal command mpiexec n 2 python main script py where the n flag is used to specify the number of processors that will execute the ptsnet simulation in this case two within the script the execution is triggered either by invoking the sim run command as shown in lines 38 and 39 in fig 8 or using a while loop that advances the simulation step by step with the command sim run step as shown in line 6 in fig 9 c with respect to the initial conditions ptsnet always starts the wtn transient solution from a steady state condition developed using epanet with the users network configuration and initial settings ptsnet automatically runs the epanet model prior to the transient simulation without any prompting from the user however users should be aware that the epanet steady state solution has some limitations relative to the discretization of the dv moc network in particular since epanet only provides heads at the extremes of pipes and a single flowrate per pipe heads for the smaller pipe segments of dv moc are linearly interpolated from the pipe end values and a single flowrate is enforced along all segments of a single pipe when running ptsnet in parallel a single processor executes the epanet simulation and broadcasts steady state simulation results to the other processors once the processors receive the steady state results from epanet processors interpolate initial heads and populate initial flowrate values for the pipe segments assigned to them only after ptsnet has setup the initial conditions using epanet the dv moc transient simulation of heads and flowrates is computed from the user specified start to end time note that results are only extracted by setting save results to true in which case results will be extracted at every time step an will be temporarily allocated in a data structure during the simulation the results are converted and saved permanently in hdf5 files at the end of the simulation as shown in line 14 in fig 8 recall that hdf5 files are stored within a workspace folder whose name is defined when creating the ptsnetsimulation object as done in line 22 in fig 8 the jupyter notebook 1 simulate scenarios available on github demonstrates how to setup execute and extract simulation results for analytics functionalities that involve multiprocessing such as those that allow time step selection and determining the number of processors the execution requires a two step process first the function is invoked using a single processor then ptsnet automatically generates a script that is used in step two for running a suite of simulations and prints a line of code that needs to be executed in the terminal by the user finally the user executes the command in the terminal which triggers the suite of simulations via the openmpi standard the two step execution process for analytics functionalities is necessary in order to execute customized parallel simulations given that simulations with multiple processors can only be executed through the command line the jupyter notebook 3 analytics available on github demonstrates the analytics functionalities including analyzing simulation time step number of processors and wave speeds 5 1 time step selection the simulation time step determines the resolution accuracy and computational burden of a ptsnet model this is because the time step determines how much the wave speeds need to be adjusted to fulfill the cfl condition therefore the wave speed adjustment i e ω ˆ i ω i 1 can be used as a proxy to estimate the effect of the time step selection on simulation results ptsnet incorporates functionalities within the graphics module that facilitate the users to visualize the network topology and wave speed adjustments more specifically the plot wave speed error functionality in the graphics static module generates a plot of the network and creates a colored map with the wave speed adjustment values in order to execute this functionality users must previously define a ptsnetsimulation object and specify the name of the file for saving the plot as shown in lines 1 27 in fig 8 and line 8 in fig 10 respectively this method is executed prior to the transient simulation and only takes a few seconds to run even for networks with thousands of pipes hence it provides the users with a rapid selection of a time step that fits their modeling needs 5 2 determining the number of processors the analytics functionalities in ptsnet allow users to determine the optimal number of processors for their simulations considering that the performance of the library depends on the characteristics of the transient model e g model resolution duration number of boundary conditions and hardware specifications riaño briceño et al 2021a estimating the number of processors that maximize speedup and from which there is no improvement in computational performance is not straightforward as more processors are used in a simulation less time is spent on computing simulation steps but the time spent on communication increases also the sequential part of the program limits the speedup as shown in riaño briceño et al 2021a furthermore running a simulation multiple times using a different number of processors might become cumbersome when using a supercomputer or when a simulation takes too long to run to help the user manage these issues ptsnet includes functionalities to i estimate computational times and ii determine the optimal number of processors for a specific application first the compute simulation times functionality shown in line 7 in fig 10 allow users to run a set of simulations with different time step values and a given number of processors such that only a fraction of the simulation is executed and profiled based on the average computation time per step the compute simulation times functionality estimates the total computation time that will be required to execute the simulation second the compute num processors functionality shown in line 6 in fig 10 allows users to determine the number of processors that best fits their application by running a fraction of the simulation with various processor numbers and a fixed time step defined by the user users can specify the number of steps that they want to compute for the partial simulations and the average running times per step are computed for each processor this allows the user to determine the number of processors that provides the minimum running time per step for their application the resulting computational times are stored in temporary binary files within the workspace such that users can consult them even after the simulation is terminated see section 3 3 for more details 5 3 exporting results at the end of the simulation users can access simulation results including head and discharge at the nodes and flowrates at the start and end nodes of the links for all time steps of the simulation results associated with nodes and pipe extremes can be extracted from the sim node sim pipe start and sim pipe end data structures respectively see section 3 3 when operating with results users can use ptsnet in normal or persistent modes depending on the mode results will be retrieved from ram or disk memory under the normal mode users can extract results directly from ram after the transient simulation is completed without closing the current python session i e calling the results data structures after line 37 in fig 8 using lines 7 9 in fig 11 users operating under persistent mode can read results saved from previous simulations to use the persistent mode a copy of the workspace and results are saved in hdf5 files via simulation settings line 14 in fig 8 the persistent mode is activated by opening a simulation using python s with statement which ensures a safe manipulation of the workspace files as shown in line 6 in fig 11 in the code presented in fig 11 results are extracted via persistent mode by invoking the ptsnetsimulation constructor which retrieves data from the workspace saved with the name passed as an argument as a result the head for junction 23 is plotted as shown at the bottom of fig 11 workspaces persist in memory only if the user specifies so through the simulation settings when simulating for the first time under the normal mode after that users can assign a name to the workspace to differentiate results from future simulations the jupyter notebook 2 get results available on github demonstrates how to load results saved in previous simulations 6 results we assess the performance of the ptsnet package by i comparing the accuracy and computational times with other transient software using different transient scenarios ii performing scalability tests with large scale wtn and analyze the performance on a personal computer pc and high performance computer hpc and iii showcase the implementation of analytics functionality to explore wave speed adjustment and the effect of time step selection 6 1 transient software comparison the following example demonstrates the capability of ptsnet to model a pump shut off scenario in a mid scale network the simulation results of the ptsnet library executed on a single processor are compared against the tsnet python package an open source library xing and sela 2020 and bentley hammer v8i bentley systems 2019 a commercial software broadly used in both academia an industry to analyze transient phenomena in pipes we validate the physical results of the model by comparing the transient response of the system to different scenarios in terms of heads at a set of junctions in different locations in the network labeled in fig 12 the jupyter notebook 4 si figures available on github demonstrates how to replicate the results in this section and in the si example mid scale system consider the bwsn i network presented in fig 12 and adapted from ostfeld et al 2008 bwsn i comprises 126 nodes one reservoir 168 pipes two pumps and eight valves different transient events are generated including pump shut off burst and valve closure resulting pressure waves that propagate throughout the entire wtn for illustration purposes all pipes are assumed to have a wave speed value of 1200 m s and the transient event is simulated for 20 s using a time step of 5 ms 6 1 1 numerical results the pump shut off scenario illustrates how ptsnet models a transient event resulting from a controlled pump shut off at pump 172 by decreasing the pump rotational speed to zero starting at 0 and until 1 s into the simulation fig 13 shows the pressure at five different nodes in the network in response to pump shut off where a b and c correspond to simulation results using ptsnet hammer and tsnet respectively the results from the three solvers closely resemble attenuation and phase shift throughout the simulation period capturing down and up surges in pressure the minor discrepancies are attributable to the different wave speed adjustment schemes and boundary condition computation methods adopted by the three packages e g ptsnet adopts flow based equations whereas tsnet are velocity based and hammer s wave speed adjustment method differs from that of both ptsnet and tsnet overall all the methods show pressure waves generated by the pump propagating through the system as waves propagate their amplitude and shape changes according to the network topology the amplitude of waves attenuates or amplifies as a result of wave reflections and transmissions and the transient pressure observed at different locations of a wtn is an aggregated signal of multiple pressure waves hence locations farther from the origin of the transient do not necessarily exhibit lower amplitudes for example in fig 13 junction 30 senses the transient first while junction 16 experiences it last the node with the largest change in pressure is junction 90 which experiences a pressure drop of over 50 m after 7 s indicating that the pump shut off can generate significant transients in the wtn when operated quickly therefore it is essential to evaluate the impacts of and design appropriate procedures to guide pumping operations additional results comparing simulation results between the three solvers for burst valve closure and pump shut off with open and closed surge tanks are shown in figs s1 s4 in the si 6 1 2 computational performance results table 1 lists the running times for the different test cases executed by each package i e pump shut off valve closure and burst it is evident that ptsnet surpasses in performance both tsnet and hammer packages on average simulating the mid scale network takes around 6 s with ptsnet 700 s with tsnet and 45 s with hammer overall ptsnet ran roughly seven times faster than hammer in all the test cases and 116 times faster than tsnet notably for a fair comparison ptsnet simulations were executed using a single processor hence the speedups are largely attributed to vectorization of the moc equations 6 2 scalability test in this section we show performance tests for a large scale test case to illustrate the benefits of ptsnet over other transient modeling tools we focus on the computational aspect of the software showing simulation times when executing ptsnet in parallel we introduce a large scale test case using bwsn ii water system which is a well known benchmark in the water systems research community ostfeld et al 2008 and compare running times for three different numerical grid resolutions we also compare average running times per time step for the denser numerical grid using both pc and hpc systems we show that the optimal number of processors depends on simulation properties and on hardware specifications all the reported running times are wall clock times grama et al 2003 pc simulations ran on a 16 gb ram computer with an amd ryzen 7 5700u 4 5 ghz processor simulations performed on a supercomputer ran on the stampede 2 system stanzione et al 2017 and its intel xeon phi 7250 1 6 ghz computing nodes whose hardware specifications can be found in tacc texas advanced computing center 2020 the results presented in this section are not compared with commercial or open source software since it was not possible to execute the simulations for the large scale system initializing the bwsn ii network with tsnet takes several hours and hammer v8i crashes when executing the simulation with small time steps example large scale system the bwsn ii network is adapted from ostfeld et al 2008 bwsn ii comprises 12 526 nodes two reservoirs 14 824 pipes a single pump and six valves a transient event was generated by rapidly closing a valve that controls the flow in one of the main pipelines in the system within 10 s thus producing a pressure wave that propagates throughout the entire piped network for illustration purposes we assume that all pipes have a wave speed value of 1000 m s 6 2 1 comparing different numerical grid resolutions we model the large scale network using three different numerical grid resolutions to study the performance of ptsnet when dealing with highly computationally expensive simulations in general the model resolution is highly sensitive to the time step value given that small changes in time step result in large changes in problem size due to the cfl condition considering that the problem size grows exponentially for high resolution models simulating with ptsnet using a single processor can be computationally restrictive due to memory and running time limitations since ptsnet adopts the dv moc framework it ensures scalability for high resolution models by dividing the transient flow problem among multiple processors we report detailed results for three numerical grid resolutions table 2 lists the time steps and the corresponding number of solution points in the numerical grid and the number of time steps in the simulation in fig 14 a we present running times from 1 22 and 64 processors with simulation times categorized by subprocess i e initialization computation of interior points computation of boundary conditions and communication among processors with a single core system the simulation takes 879 2815 and 10 188 s for the numerical grid resolutions given by τ 1 τ 2 and τ 3 respectively overall as the number of processors increases the marginal improvement in running times decreases until no further improvement is observed analyzing the computational time invested in computing the different subprocesses we observe that the majority of the time running simulations with one processor is spent computing interior points for all the different numerical grid resolutions however once the number of processors increases the time spent in communication dominates the simulation time the computational time on interior points decreases since the number of interior points per processor decreases as the number of processors increases thus reducing the computational burden per processor however as the number of processors increases more partitions are generated thus increasing the number of information dependencies between processors which increases the cost of communication regardless of the time step it is demonstrated that communication becomes the bottleneck of the simulation for any sufficiently large set of processors simultaneously the time spend in the computation of interior and boundary points becomes minimal as the number of processors increases for a problem of fixed size note that initialization is almost negligible compared to the rest of the subprocesses across all cases furthermore the time spent computing boundary conditions accounts for roughly 5 of the total simulation time and remains virtually constant for 22 cores or more a speedup metric can be defined as s t 1 t k where t 1 is the time spent simulating with dv moc on a single processor and t k is the time spent simulating with k processors in general the speedups are more significant as the number of interior points in the numerical grid increases if we compare the running times for 1 and 64 processors when the time step is equal to τ 3 the speedup is 3 the speedup with τ 2 is 1 85 and with τ 1 the speedup is 1 43 thus it is not as efficient to use multiple processors for τ 1 since the communication overhead outweighs the reduction in the computation of solution points additionally higher speedups are achieved with denser numerical grids which can be attributed to the fact that cache mechanisms operate more efficiently as the number of contiguously allocated solution points increases riaño briceño et al 2021a 6 2 2 comparing pc with hpc systems in fig 14 b we show the performance in terms of average running time per time step for the large scale system bwsn ii running on both pc and hpc systems simulations running on both systems were identical with respect to simulation settings numerical grid resolution and the number of processors specifically we ran 2500 time steps with a time step of τ 3 0 0005 s we measured running times using ptsnet s profiler module and averaged running times per step including computation of solution points and communication between processors as evidenced by fig 14 b running times per time step differ significantly between the pc and hpc simulations the pc running times are faster than the hpc as long as the number of processors is less than 8 additionally for the hpc the computational time decreases as the number of processors increases however for pc an initial decrease in computational times is observed when increasing the number of processors from one but as more processors are added the overall computational times increase the latter can be attributed to the different computational burdens between pc and hpc systems it might be surprising that the pc runs faster than the hpc yet if we compare the core specifications of both machines the pc cores run almost three times faster than the supercomputer cores due to the differences in frequencies i e 4 5 ghz vs 1 6 ghz nevertheless the hpc performance surpasses the pc as the number of processors is increased given that the hpc architecture has been optimized to minimize the communication burden between processors communication rapidly becomes a bottleneck for the pc with far fewer cores than for the hpc system the optimal number of processors for the pc is three while the optimal number of processors for the hpc is estimated to be eight using the elbow of the curve as the selection criterion see fig 14 b the elbow point is a commonly used heuristic to determine the optimal balance between computational cost and speedup grama et al 2003 6 3 wave speed error referring back to section 5 1 the simulation time step determines the resolution accuracy and computational burden of a transient model selecting a time step that results in a grid resolution that ensures high accuracy and fast running times is ideal yet these goals are conflicting finer grid resolutions provide higher accuracy but also demand higher computational resources and are likely to lead to communication bottlenecks and slower execution ptsnet provides functionalities to estimate the time step automatically balancing the trade off between accuracy and computational burden bentley systems 2022b for example for large scale networks hammer v8i assigns more weight to the computational cost hence the automatic time step selection results in high simulation errors in such cases users need to manually tune the time step testing different grid resolutions by iteratively running very long simulations to overcome this manual process we propose using the wave speed adjustment error as a proxy for the simulation error allowing the user to visualize the error before running the simulation visualizing the wave speed error produced by the resolution of the numerical grid facilitates identifying parts of the network where the error will be higher and determining whether or not a time step selection is satisfactory for a particular application for example a user might find it acceptable to have a time step that results in acceptable wave speed error in the study region and localized high wave speed error far from the area of study also users can define a time step that minimizes the problem size such that the majority of the network pipes remain below a certain wave speed error threshold to illustrate we used ptsnet analytics functionalities to plot the wave speed error for two time steps τ 1 0 01 s and τ 2 0 1 s the resulting numerical grids have 199 923 and 46 642 solution points for τ 1 and τ 2 respectively thus τ 1 provides a higher resolution and accuracy compared to τ 2 in fig 15 we show the relative wave speed error defined as in riaño briceño et al 2021a for both time steps zooming on a section of the bwsn ii network the plot shows that error remains below 10 for almost every pipe in the network when using the smaller time step for τ 1 whereas for τ 2 the error is greater than 10 for almost every pipe in the system even though the number of solution points became 3 2 times greater for τ 1 compared to τ 2 which can result in longer simulation times users may prefer using the smaller time step and exploit ptsnet parallel capabilities to run this test case 7 conclusion this paper presents ptsnet an open source python package for parallel transient simulation in wtns all the source code software documentation and multiple examples including input files and codes are provided within the package and can be downloaded from the github repository riaño briceño et al 2022 the capability and user interaction with ptsnet are demonstrated through the detailed simulation examples of pump shut off valve closure burst and surge tank the ptsnet package is proven to be computationally efficient compared to other open source and commercial packages unlike other packages we demonstrate that ptsnet is scalable and capable of running large scale transient simulations providing significant speedups to its users thanks to the adoption of vectorization and distributed computing in addition the package offers essential analytic tools for users to quickly determine the best time step and number of processors for their application ptsnet does not include all the modeling capabilities of the commercial software instead it is designed to provide simulation capabilities for transient modeling in wtns for the research community that is currently not available in open source software including petsc abhyankar et al 2020 and tsnet xing and sela 2020 ptsnet is under continuous maintenance improvement and development future work includes extending the solver to generalize to other pde based models e g open flow channel modeling 2d and 3d dynamics li and hodges 2021 morales hernández et al 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors acknowledge the texas advanced computing center tacc at the university of texas at austin for providing hpc resources that have contributed to the research results reported within this publication this work was supported in part by the national science foundation us under award 2015658 and cooperative agreement no 83595001 awarded by the u s environmental protection agency to the university of texas at austin it has not been formally reviewed by epa the views expressed in this presentation are solely those of the authors and do not necessarily reflect those of the agency epa does not endorse any products or commercial services mentioned in this publication appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2022 105554 appendix a supplementary data the following is the supplementary material related to this article mmc s1 it includes dv moc equations selectors to compute the dv moc with ptsnet and a comparison of software packages 
25506,agent based models abms are an increasingly popular choice for simulating large systems of interacting components and have been applied across a wide variety of natural and environmental systems however abms can be incredibly disparate and often opaque in their formulation implementation and analysis this can impede critical assessment and re implementation and jeopardize the reproducibility and conclusions of abm studies in this review we survey recent work towards standardization in abm methodology in several aspects model description and documentation model implementation and model analysis and inference based on a critical review of the literature focused on abms of environmental and natural systems we describe a recurrent trade off between flexibility and standardization in abm methodology we find that standard protocols for model documentation are beginning to establish although their uptake by the abm community is inhibited by their sometimes excessive level of detail we highlight how implementation options now exist at all points along a spectrum from ad hoc from scratch implementations to specific software offering off the shelf abm implementations we outline how the main focal points of abm analysis behavioural and inferential analysis are facing similar issues with similar approaches while this active development of abm analysis techniques brings additional methods to our analysis toolbox it does not contribute to the development of a standardized framework since the performance and design of these methods tends to be highly problem specific we therefore recommend that agent based modellers should consider multiple approaches simultaneously when analysing their model well documented software packages and critical comparative reviews of such will be important facilitators in these advances abms can additionally make better use of developments in other fields working with high dimensional problems such as bayesian statistics and machine learning keywords agent based models simulation model analysis inference calibration data availability no data was used for the research described in the article 1 introduction agent based models abms are gaining in popularity for modelling complex systems of interacting components and have become established in an ever growing number of disciplines including ecology biology social sciences economics urban planning and many more grimm and berger 2016 o sullivan et al 2016 sun et al 2016 abms model the properties activities and interactions of each agent or individual within a population grimm and railsback 2013 population or community level properties then emerge as the result of the collective actions of the agents these models are ideal for understanding how behaviours and actions at the individual level lead to the population level patterns that determine the stability and behaviour of the system as a whole railsback et al 2020 gallagher et al 2021 the flexibility of and diversity within the abm paradigm poses both an enormous advantage and a significant barrier to full acceptance of abms as a scientific tool for instance agent based modelling studies can be incredibly disparate and often opaque in their formulation execution and analysis this can often make critical assessment and re implementation cornerstones of good scientific practice very difficult for abm based studies and in this way jeopardize their reproducibility and ultimately their conclusions hence there have been many calls to standardize or unify agent based modelling approaches so as to introduce the same kind of coherence formalism and structure as classical population level modelling paradigms like systems of partial differential equations grimm and berger 2016 fulton et al 2019 this is an extremely ambitious goal with multiple intertwined strands however as similar issues are encountered in other simulation based modelling approaches opportunities for adaptation and cross pollination arise cranmer et al 2020 in this review we highlight recent work towards standardization within abm methodology along several lines i model description and documentation ii model implementation and iii model analysis and inference we close the review by summarizing the main findings and providing recommendations to guide users in the choice operation and interpretation of abms 2 model description a first important aspect of a unifying framework is a standardized protocol for describing abms as the model description conveys the findings from the model development cycle and the resulting abm to the reader it is crucial for the interpretation and reproducibility of the results in this section we briefly summarize the most common standard protocols for both model description and model development given that abms can involve on the one hand very few or even no equations and on the other hand significant numbers of parameters rules subroutines and submodels a clear and concise format for their description is crucial for peers to be able to understand and reproduce an abm study abm descriptions in published literature are often ad hoc and can therefore vary substantially in their level of detail for example a survey of published simulation based studies found that the majority 59 did not include model equations only 7 contained partial equations and only a minority 34 of the explored studies reported the complete model rahmandad and sterman 2012 similarly only 30 of the surveyed articles included all parameter values needed to replicate the baseline simulation scenario and less than 20 included complete units for all equations rahmandad and sterman 2012 clearly failing to report the underlying model equations and or parameters renders a modelling study unreproducible and hence weakens confidence in any conclusion drawn from it and at the other extreme of total transparency a survey of more than 2000 scientific articles published before 2014 and involving abms found that less than 10 made their source code available janssen 2017 against this varied background it is clear that a standardized framework for describing abms has not yet been adopted by the field at large however one approach stands above others in terms of the significant and growing number of studies that use it the odd overview design concepts and details protocol consists of seven elements grouped into overview purpose state variables and scales process overview and scheduling design concepts and details initialization input submodels it was proposed grimm et al 2006 in 2006 updated grimm et al 2010 in 2010 and once more grimm et al 2020 in 2020 in 2007 the year after the first version of the odd protocol was published it was cited by 1 8 of all published abm studies 9 of 509 articles grimm et al 2020 this proportion quickly increased to 7 7 of all abm studies 78 of 1017 articles by 2011 and in recent years has stabilized around 9 10 in 2018 172 of 1735 published studies 9 9 cited the odd protocol grimm et al 2020 the protocol has been adopted by modellers in a number of different fields predominantly the life sciences and social sciences in 2018 these fields accounted for 60 and 17 of the odd protocol s total citations respectively grimm et al 2020 the protocol has also been cited in studies from the physical and formal sciences e g computer science engineering mathematics these fields accounted for 4 and 19 of the protocol s citations in 2018 respectively while the odd protocol provides a consistent and fairly logical template for reporting abm implementations some issues remain descriptions still tend to run on the lengthy side often up to several pages and yet some important details can still be overlooked within the protocol s structure so while the odd protocol provides perhaps the best currently available basis for a standard description of abms a number of extensions and modifications to the protocol have been proposed these generally involve the addition or removal of sections and or sub sections to suit the needs of a specific field or application examples include the odd d protocol for abms which include human decision making müller et al 2013 the odd 2d protocol which formalizes the use of empirical data within abms laatabi et al 2018 the odd p protocol which also provides provenance information on how the abm was generated reinhardt et al 2018 and the oddox protocol which documents not only the model itself but also the source code topping et al 2010 however these extensions have not yet been adopted anywhere near as widely as the original odd protocol only 212 8 3 and 58 articles can be found on web of science respectively as of april 2022 compared to 1608 citations for the odd protocol in its original inception from 2006 the design and development of abms require numerous choices on the part of the modeller often some of these choices are not explicitly discussed or even mentioned when it comes time to describe the model for the purpose of dissemination hence there have been a growing number of calls in the abm literature for more transparent and extensive description of the model development process itself one of the most frequently cited is trace transparent and comprehensive model evaludation grimm et al 2014 where evaludation refers to the assessment of a model s quality and reliability thereby combining the processes of evaluation and validation trace was proposed as a standard protocol for the documentation of an abm s development and testing the documentation is extensive and detailed and therefore provides an excellent record and justification of the model s design and workings thereby greatly facilitating re implementation by others however compiling such an extensive document requires a significant effort on the modeller s part which may discourage some authors from selecting trace but instead choosing a more concise documentation format both the odd protocol and trace result in detailed and therefore lengthy descriptions of the abm while such an elaborate description certainly benefits the model s reproducibility it does not necessarily aid in its interpretation indeed the main ideas behind the abm and its key features may become lost in the details we believe that the introduction of a standardized graphical representation could overcome this and consequently benefit the field of agent based modelling this would result in a concise description of the abm in the main text of an article while the detailed descriptions could accompany the annotated source code in the supplementary materials efforts to establish standard protocols for the description of abms and their design have focused on the need to stimulate and support a culture of good modelling practices in the abm field augusiak et al 2014 such practices already common in other fields such as software development e g software development life cycle models davis et al 1988 have several benefits first they provide a basic level of quality assurance by ensuring that all model design choices are not only described but also justified grimm et al 2014 second they contribute to increased confidence in any given abm improving its likelihood of being used by other modellers in particular newcomers to the field and avoiding the need to redevelop the same model over and over again third they can help to standardize the model development process itself by avoiding the need to repeat testing or analyses before deployment of a given model 3 model implementation once a model has been conceptualized and its key processes formalized as mathematical equations or heuristic algorithms it must be implemented in some programming language for the purpose of simulation here as with other aspects of agent based modelling a tension emerges between the flexibility and freedom that the paradigm affords modellers and the lack of standardization inherent in this freedom hence many abms are implemented on an ad hoc basis subject only to the whims and fancies of the particular modeller for example two abms may have the very same underlying rules and mechanistic processes but differ in the order in which equations are solved or in which individuals characteristics are updated after interactions occur muelder and filatova 2018 for example the choice of a synchronous or asynchronous schedule for updates during a simulation can lead to different results even for very simple abms this divergence becomes more acute as population densities and interaction complexity increase caron lormier et al 2008 while this freedom of implementation has the benefit of permitting the emergence of novel techniques and creative solutions it also leads to an extraordinarily large number of implementations of similar if not identical environmental processes and algorithmic procedures furthermore those users who do not wish to implement their model from scratch may instead turn to a wide variety of software tools and platforms designed specifically for abm development and implementation to give an idea of the breadth of this fast growing body of work a recent review of this literature found 85 different software tools for abm development and implementation abar et al 2017 each of the numerous available platforms typically takes its own approach to the implementation of the abm s routines and processes increasing even further the suite of myriad options available to the novice modeller these options for abm implementation run the gamut from entirely from scratch implementations in a general purpose programming language gppl that are ad hoc and case specific to modular combinations of standard submodels wrapped in a novel case specific framework to off the shelf models that require the user only to plug and play fig 1 the first approach is clearly the least standardized and hence depends entirely on the modeller to make appropriate and well justified design and programming choices however it also allows for novel approaches to emerge by taking advantage of the extreme flexibility inherent to abms unfortunately these worthy examples can easily be lost in the flood of new abms so that the insights and knowledge they bring are not picked up by the scientific and modelling community this build up of abms without a corresponding increase in our body of knowledge has been referred to only half jokingly as yet another model syndrome o sullivan et al 2016 and is a key factor in the increasing fatigue felt in some domains for the abm paradigm on the other side of the spectrum stand alone programmes like netlogo wilensky 1999 mason multi agent simulator of neighbourhoods or networks luke et al 2005 and repast north et al 2013 provide general abm environments that can greatly simplify model development and implementation however with the growing number of such development platforms it has become a tedious task to select the one that serves the developer s needs novice researchers will often not be able to find the right tool for their specific model or lose a significant amount of time searching for it given that each programme has its own syntax and semantics that will suit some situations and not others moreover these programmes are limited by the included tools for example those needed for model analysis and inference see section 4 hence many researchers still prefer to implement new and stand alone models taking advantage of abms flexibility to construct a model carefully tailored for their specific simulation and analysis needs between these two extremes abm packages have been developed for several programming languages e g the mesa package in python kazil et al 2020 or agents jl in julia datseris et al 2022 as well as extensions of existing abm platforms to general purpose programming languages e g extensions between netlogo and r thiele 2014 salecker et al 2019 or python jaxa rozen and kwakkel 2018 as illustrated in fig 1 in this way modellers can take advantage of established submodels or frameworks as modular building blocks within their own abm this reduces the modelling effort relative to implementing an abm entirely from scratch but is still less restrictive than the other extreme of an off the shelf model other implementations focus on developing more robust abm frameworks that unify modelling approaches within a specific field or context this allows researchers to focus on the development and refinement of analytical tools for a generic process based framework that is readily applicable to different models within the field one example is the work of christensen et al who developed a framework to couple biological abms and physical based models of oceanographic processes christensen et al 2018 this strategy allows researchers to swap out either the physical or biological components so that they can compare different submodels and take advantage of the cross scale capabilities of the abm approach in a flexible manner this transition is taking place in various disciplines for example the biomedical field is confronted with the problem of converting mechanistic knowledge of individual level processes to higher level outcomes that are clinically relevant and particularly doing so within a single cohesive model this has motivated the recent development of multi scale abms such as spark simple platform for agent based representation of knowledge one example of a framework for developing systems level biomedical abms that can admit data across multiple scales solovyev et al 2010 cilfone et al review this type of approach for models of biomedical systems and provide a generic framework for linking simulating and analysing such hybrid multi scale models cilfone et al 2015 another example is the emod platform bershteyn et al 2018 although restricted to epidemiological modelling this agent based framework can be used to model multiple diseases since the majority of the abm processes are relevant to infectious disease modelling in general this takes advantage of similar interaction mechanisms i e disease transmission to allow researchers to build on a shared model framework and again swap out different modules that are specific to certain diseases for example airborne diseases versus vector borne diseases given the wide variety of approaches to abm implementation a natural question for the novice programmer is which is best of course the flexibility of abms means there is no set answer to this question but instead a value judgement about which features of a model are most important to a given research setting and question however recent studies have begun to pay more attention to the fact that there is indeed sometimes a right way and a wrong way to implement an abm of a given process or system pietzsch et al 2020 taghikhah et al 2021 model verification or model checking is the domain concerned with the assessment of a model s behaviour with the goal of ensuring that its formulation and implementation were appropriately designed and correctly programmed unfortunately formal verification techniques typically depend on applications of formal logic that are often beyond the interest and knowledge of most modellers liu et al 2021 advances in the model verification field are therefore needed to improve users confidence in abms as doing what it says on the tin capturing the key system characteristics and processes at the appropriate level of detail without resorting to obfuscating black box approaches 4 model analysis and inference 4 1 modern models require modern tools of analysis in order to meet its scientific purpose an implemented model must be subjected to a thorough analysis while such an analysis is typically seen as forming a calibration and validation step of the modelling cycle we will approach this from another viewpoint an analysis can either target an understanding of the model behaviour e g stability bifurcation and sensitivity analysis or the model simulations can be compared to observational data e g parameter estimation model selection which is commonly referred to as inference a calibration step involves an inferential analysis but validation is closely related to the research question and as such problem specific however the results of both the behaviour and inferential analysis are used in validation the flexibility of abms comes here with yet another cost the loss of analytical tractability while abms are capable of simulating complex emergent behaviour it is notoriously difficult to infer interpret or explain the rules that govern agents actions and lead to these emergent behaviours bodine et al 2020 indeed as most abms are inherently stochastic the actions and responses of agents in the system occur stochastically conversely it is also often the case that different rules and parameter settings can lead to the same model output a phenomenon known as equifinality graebner 2018 despite the huge increase in computational power over the last few decades exploring the entire parameter space of an abm is still practically infeasible hence abm analyses are generally based on heuristics that rely on many abm simulations with varying assumptions and parameter sets lee et al 2015 these heuristics often involve ad hoc decisions rendering the results hard to compare reproduce or generalize in contrast a major advantage of more established models like those based on differential equations is the availability of standardized analytical tools in absence of such tools a modelling paradigm is bound to remain restricted to the theoretical margins and without consequential applications it is therefore not surprising that more and more studies are working towards the development of standardized methodological frameworks for abms analogous to those for classical models when it comes to modelling natural and environmental systems pattern oriented modelling has been proposed as such a standardized framework for abms in which the focus lies on reproducing observed patterns grimm et al 2005 the latter is certainly a creditable attempt to guide modellers in developing and analysing abms but it has become more of a modelling philosophy and hence remains too vague to be a truly standard methodology while the popularity of abms continues to increase the explicit use of pattern oriented modelling has stagnated over the last years gallagher et al 2021 conversely analytical methods rooted in dynamical systems theory and statistics have been increasingly used for abms it is important to note that the problem of analytically intractable simulation based analysis is of course not limited to abms as this issue presents itself in many areas where computationally expensive models arise cranmer et al 2020 indeed the methods discussed in the remainder of this section were typically developed in other research domains and only later adopted by the abm community generally two main strategies can be distinguished for developing such standardized tools 1 adapt or extend existing analytical tools so they can be applied to abms or 2 approximate abms by tractable models that permit the use of established analytical tools from other fields both of these strategies have resulted in novel methods for achieving the two main analysis goals before reviewing these methods we briefly discuss these goals in relation to abms 4 2 goals of model analysis 4 2 1 behaviour analysis since a model s behaviour strongly depends on the values of its parameters a quantitative analysis of this dependency is crucial typically this modelling step comprises some sort of sensitivity analysis with the aim of identifying the most and least thus possibly redundant model parameters saltelli et al 2007 sensitivity analysis methods for abms are well established with readily available tutorials e g thiele et al 2014 ten broeke et al 2016 borgonovo et al 2022 and software e g sensitivity package in r sensitivity analysis library salib package in python behaviorspace in netlogo however criticism has recently been levelled at how such sensitivity analyses are being performed in practice noting that many do not sufficiently explore the parameter space saltelli et al 2019 moreover it has been pointed out that local sensitivity analyses are based on local linearity and should not be used for highly nonlinear models as abms typically are when studying such highly nonlinear systems the analysis of steady state behaviour can provide key insights into the system s dynamics for example a bifurcation diagram where the equilibrium points of the system and their stability and or basin of attraction are plotted as functions of a varying model parameter can be used to identify and quantify tipping points ashwin et al 2012 of the system where transitions between behavioural regimes occur due to external forcing in natural systems for example such tipping points are typically linked to major system level events such as ecosystem collapse or the catastrophic failure of complex networks scheffer 2009 lacking the analytical tools for finding the equilibria of the model abm users typically resort to simulations over long time periods while manually tuning model parameters in order to observe these dynamical shifts woods et al 2005 however this approach again quickly becomes computationally prohibitive for all but the simplest abms especially when considering the effect of multiple parameters simultaneously moreover as the stochastic nature of most abms complicates even the basic definition of an equilibrium or bifurcation van nes et al 2016 ad hoc operational definitions are instead required colon et al 2015 for example used monte carlo singular spectrum analysis to detect statistically significant oscillatory patterns in abm outputs given a certain value of a model parameter θ the detection of such patterns in more than 50 of the abm simulations was subsequently defined as a transition to a limit cycle regime i e a hopf bifurcation in the parameter θ 4 2 2 inferential analysis when observational data are available to compare with simulated data we bump into another analysis problem typically such an analysis focuses on retrieving meaningful model parameters and is hence often referred to as inference parameter estimation model calibration or inverse modelling in all these cases the purpose is to determine the most appropriate values for the model parameters based on some comparison to the observational data the latter can aim for example to reproduce key demographic or dynamical patterns or obtain model predictions that match the observed data as closely as possible as in model behaviour analysis manual exploration of the parameter space quickly becomes infeasible hence agent based modellers often turn to numerical optimization methods such as simulated annealing or genetic algorithms hartig et al 2011 grimm and railsback 2013 thiele et al 2014 the goal of the latter is to find one optimal set of parameter values for which the abm simulations agree to some desired level with the observations however keeping in mind the fundamental issues of equifinality and stochasticity the benefit of finding such an optimal parameter set seems questionable gallagher et al 2021 calls for more rigorous inference and uncertainty quantification have thus drawn attention to suitable methods from inferential statistics as these methods can rely on the foundations of probability theory they are apt to be standardized and suitable for robust decision making however their application to abms is often thwarted by the fact that an analytically tractable likelihood function cannot be obtained for most abms this problem precludes the direct application of a large body of frequentist and bayesian statistics cranmer et al 2020 and forces modellers turn to more novel approaches in the next section we review a number of such approaches 4 3 adaptation of existing tools 4 3 1 adapted behavioural analysis tools a first approach for analysing abms is to adapt established tools from other fields so that they can be applied to abms such methods regard abms as black box models that produce a set of simulated outputs given a particular set of inputs i e the values of the model parameters and initial conditions this model often termed a generative model or simulator depending on the application is then employed in some type of numerical scheme inspired by an established analysis for example the equation free framework kevrekidis et al 2004 was developed for performing numerical steady state analysis of the emergent patterns of a simulator in this context an abm this framework is based on the separation of time scales between the dynamics of the microscopic simulator and its emergent macroscopic behaviour assuming that the former will quickly converge to the latter the main idea is that under these circumstances the equation free time stepper can jump between the microscopic and macroscopic state spaces in practice this boils down to consecutive short bursts of abm simulations followed by the derivation and extrapolation of the emergent patterns and careful re initialization of the simulator by performing the extrapolation with a numerical bifurcation scheme it would be possible to efficiently construct numerical bifurcation diagrams or at least do so more efficiently than with a brute force or manual approach however switching from the low dimensional macroscopic space to the high dimensional microscopic space in equation free terminology the lifting step is a non trivial task kevrekidis and samaey 2009 lifting requires the re initialization of the abm based on aggregated states and is thus problem specific and often computationally intensive vandekerckhove et al 2011 willers et al 2020 hence in spite of some attempts to generalize this method for abms thomas et al 2016 its application has so far been limited to very specific cases siekmann 2015 martin and thomas 2016 4 3 2 adapted inference tools in contrast to behavioural analysis tools the development of likelihood free methods for statistical parameter inference has enjoyed more success in particular approximate bayesian computation abc has become an established tool in the abm domain and beyond sisson et al 2018 cranmer et al 2020 several abc implementations such as the r package abc csilléry et al 2012 are readily available for use with abms van der vaart et al 2016 pietzsch et al 2020 as its name suggests abc belongs to the field of bayesian statistics where prior knowledge on the parameters is updated with new information from observations using the likelihood function gelman et al 2013 the resulting posterior distribution forms the inferential basis of the bayesian analysis abc comprises a family of methods in which the likelihood function is approximated by repeatedly comparing abm simulations with observations beaumont et al 2002 as is illustrated in fig 2 this comparison is based on some discrepancy measure which is often just the euclidean distance applied to the observations and a set of summary statistics of the simulated model outputs the simulation based likelihood approximation is then passed to a sampling algorithm such as rejection sampling markov chain monte carlo or sequential monte carlo which favours parameter sets resulting in the lowest discrepancy measure values and as such generates a sample from the approximated posterior distribution hartig et al 2011 the use of summary statistics in abc fits with the idea of pattern oriented modelling as they arise when simulated patterns are quantitatively compared with observations gallagher et al 2021 provide an overview of such patterns for models in natural systems it should be noted however that the performance of abc critically depends on the choice of summary statistics prangle 2018 because summary statistics reduce the dimensionality of the problem similar to the macroscopic space in the equation free method this reduction generally entails information loss that can affect the inference quality whether the latter is the case is a matter of sufficiency a sufficient summary statistic preserves all the information in the data to infer a specific parameter as analytical intractability precludes the construction of rigorous proofs hartig et al 2011 sufficiency of summary statistics must be tested empirically for most abc inference problems several strategies have been proposed for selecting abc summary statistics reviewed in blum et al 2013 prangle 2018 and beaumont 2019 but this critical dependency on user defined summary statistics is seen as a major shortcoming of abc cranmer et al 2020 moreover it often requires so many simulations that the computational overhead becomes prohibitively large especially because simulations must be repeated for every new scenario or parameter setting 4 4 approximating abms 4 4 1 model behaviour analysis using abm approximations the purely simulation based methods described in section 4 3 involve formulating a wrapper method around the abm which repeatedly runs the model with different parameter sets a second broad approach for analysing abms makes use of some abm approximation fig 3 the latter can involve a purely data driven approximation trained on the abm outputs which is often called a surrogate model emulator or meta model such a surrogate model can be used as a computationally cheap alternative of the abm in an adapted behaviour analysis an example hereof is the use of support vector machines trained on some classification of abm outputs to construct bifurcation diagrams van strien et al 2019 ten broeke et al 2021 another approach is to construct an analytically tractable model in parallel to the abm nardini et al 2021 this relaxes the dichotomy between simulation based abms and more tractable paradigms by seeing them as complementary tools the hope is that one can have the best of both modelling worlds where the abm provides accurate simulations of reality and the tractable model s provide insight into the emergent behaviour of the abm for example colon et al 2015 use a tractable approximative model to guide the construction of a bifurcation diagram for the abm in some cases the tractable model can be a set of algebraic equations hinkelmann et al 2011 but this approach is restricted to deterministic abms ruling out its application for the majority of abms of natural systems a more general approach is to instead derive an approximate model based on differential equations nardini et al 2021 typically such an approximation is based on the mean field assumptions where the properties of agents are averaged out by use of local densities a example hereof is the work of reichenbach et al 2007 which assesses the effect of species mobility on biodiversity using a simple agent based model and its counterpart based on partial differential equations pdes more recently bernoff et al 2020 modelled the resource dependency of locust movement based on an abm and then constructed a pde based model to further investigate the emergent patterns in a sensitivity analysis yet another strategy is to recast an abm as a spatio temporal point process ovaskainen et al 2014 where agents are represented by points in space and the dis appearance and movements of these points therefore represent the system s demographic processes birth death and dispersal this approach was recently used by cornell et al to derive a more generic framework for abm analysis cornell et al 2019 however deriving an appropriate set of p des is often challenging and methods such as symbolic regression have been developed to automate this formulation from data schmidt and lipson 2009 brunton et al 2016 the latter typically starts from a set of analytical building blocks which are combined in a regression analysis nardini et al 2021 reviewed how such an automated de discovery can also be applied to abm simulations these authors demonstrate that this method can help bridge the gap between abms and p de based models by revealing the cases in which the assumptions underlying the latter do or do not hold automated de discovery hence seems a promising tool for future model development and automated equation derivation functionalities are planned to be incorporated into new software such as the agents jl package in julia datseris et al 2022 4 4 2 inference using abm approximations a recent review by cranmer et al 2020 highlighted a similar shift in simulation based inference methodology there is now a growing interest in amortizing the computationally expensive simulations performed by a generative model the amortization is again achieved by employing some approximative part of the inference scheme e g the generative model the likelihood function or the posterior distribution this boils down to a manifestation of the bias variance trade off the use of an approximation introduces some bias but if this approximation reduces computational overhead then more samples can be drawn so that the variance and monte carlo error is reduced classical methods in this category aim to find an analytically tractable approximation of the likelihood function which is often referred to as the synthetic or pseudo likelihood hartig et al 2011 this approach can be parametric if the parameters of some theoretical distribution are estimated from the generative model outputs wood 2010 hartig et al 2014 for example use a multivariate normal approximation for the likelihood function of the parameters in an agent based forest model on the other hand non parametric kernel density estimation has long been proposed for approximating the likelihood function diggle and gratton 1984 but is rarely used because of an extra limiting computational overhead particularly for high dimensional inference problems grazzini et al 2017 shiono 2021 novel simulation based inference approaches tackle such high dimensional problems more efficiently by relying on recent advances in machine learning cranmer et al 2020 these methods construct approximations using flexible data driven models again trained on the simulated outputs called surrogates or emulators for example alden et al 2020 trained five types of machine learning models to emulate an agent based simulator of lymphoid tissue organogenesis they found that the ensemble predictions of these surrogate models were highly accurate and could be used to reproduce a previously reported sensitivity analysis with far less computational overhead moreover the ensemble surrogate was then used to perform an abc analysis which would have been computationally infeasible with the abm itself similarly reiker et al 2021 calibrated an abm of malaria using a gaussian process emulator this emulator was trained progressively on the abm s simulations using bayesian optimization and subsequently used for a global sensitivity analysis an example of a posterior surrogate was explored by shiono 2021 for a standard macroeconomic abm this method termed bayesflow radev et al 2022 involves training a conditional invertible neural network on a set of abm simulations obtained with a small set of samples from the prior distribution when compared to the kernel density approximation synthetic likelihood method bayesflow showed promising results in the sense that the computational overhead was drastically reduced especially when the trained surrogate could be re used moreover bayesflow provided accurate parameter estimations that were robust to bifurcations as these modern simulation based inference methods show promising results with abm applications we envision them being increasingly adopted by the abm community in the near future 4 5 standardization of abm analysis methods it should be clear that methods for both behaviour and inferential analysis of abms are facing similar issues regarding tractability and computational demand as the main development strategy is shifting from adapting existing analysis methods to approximating abms by tractable and computationally cheaper models we expect more proof of concept papers that apply new analysis methods to specific abms while such papers certainly help to expand the abm analysis toolbox they do not contribute to a standardized abm analysis framework moreover the added complexity of new methods may not necessarily lead to better results as the performance and design of these methods tends to be highly problem specific agent based modellers should consider multiple approaches simultaneously when analysing their model carrella 2021 similarly to occam s razor we would recommend including simpler more established methods in such an analysis ensemble 5 conclusions and perspectives abm standardization is more than a matter of simple housekeeping it is important to and necessary for ongoing progress in the development of both our theoretical knowledge and our methodological toolbox it can allow us to establish a consensus on options for model structure analysis and data requirements but we should also keep in the mind that the flexibility of abms is one of their main advantages in this sense attempting to standardize abms to an extreme degree is counterproductive and misses the point of the freedom inherent in their design and analysis instead a carefully maintained and above all consciously considered balance between standardization and flexibility will bring the most benefit to the future prospects of abms as a modelling tool this trade off is a recurrent theme in the different stages of the agent based modelling cycle a clear description of abms is crucial for their reproducibility and their part in the scientific discussion and this requires a common tongue i e a standardized way of communicating the modelling cycle in this respect odd and trace are important advances for the abm field however due to abms flexibility they often result in lengthy documents and therefore a significant amount of extra work on the part of modellers reviewers and readers to keep communication concise but comprehensive we envision these documents as guides through a well annotated and openly available source code for example the comses openabm library offers a collection of tutorials on agent based modelling and its associated techniques as well as a model library that allows modellers to share the design and implementation of their abms in order to facilitate reproducibility and transparency janssen et al 2008 moreover as the abm field moves towards increasing use of standardized and reusable submodels grimm and berger 2016 these documents can increasingly refer to these well known submodels the latter trend will also move the field away from the need for every study to implement most or all of its code from scratch this will allow researchers to benefit from more consensus on the appropriate data needs computational methods and analytical approaches zhang and robinson 2021 however the analytical toolbox for abms is still very much under construction and so abm platforms need to be compatible with novel analytical methods we therefore expect an expansion of abm packages in general purpose scientific programming languages as well as more extensions for abm specific software this will increase the options available to modellers so that they have more freedom to select the tool most suited to their own particular needs at the same time transparent documentation and critical comparative reviews will be needed to avoid the choice overload that can leave modellers overwhelmed and unsure as to the best choice of implementation platform here the abm field can learn from best practices in software development and other computer science applications studies aiming for either behaviour or inferential analysis face similar issues due to the analytically intractable nature of abms more specifically abm analyses are plagued by the curse of dimensionality when exploring the model s parameter space and comparing high dimensional outputs or observations rather than trying to tackle these issues on their own the abm community can make better use of the work being done in other fields concerned with high dimensional problems for example machine learning such practices will only become widely adopted if users find their way to comparative reviews of these methods baker et al 2022 and well documented software packages that are readily usable with abms declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25506,agent based models abms are an increasingly popular choice for simulating large systems of interacting components and have been applied across a wide variety of natural and environmental systems however abms can be incredibly disparate and often opaque in their formulation implementation and analysis this can impede critical assessment and re implementation and jeopardize the reproducibility and conclusions of abm studies in this review we survey recent work towards standardization in abm methodology in several aspects model description and documentation model implementation and model analysis and inference based on a critical review of the literature focused on abms of environmental and natural systems we describe a recurrent trade off between flexibility and standardization in abm methodology we find that standard protocols for model documentation are beginning to establish although their uptake by the abm community is inhibited by their sometimes excessive level of detail we highlight how implementation options now exist at all points along a spectrum from ad hoc from scratch implementations to specific software offering off the shelf abm implementations we outline how the main focal points of abm analysis behavioural and inferential analysis are facing similar issues with similar approaches while this active development of abm analysis techniques brings additional methods to our analysis toolbox it does not contribute to the development of a standardized framework since the performance and design of these methods tends to be highly problem specific we therefore recommend that agent based modellers should consider multiple approaches simultaneously when analysing their model well documented software packages and critical comparative reviews of such will be important facilitators in these advances abms can additionally make better use of developments in other fields working with high dimensional problems such as bayesian statistics and machine learning keywords agent based models simulation model analysis inference calibration data availability no data was used for the research described in the article 1 introduction agent based models abms are gaining in popularity for modelling complex systems of interacting components and have become established in an ever growing number of disciplines including ecology biology social sciences economics urban planning and many more grimm and berger 2016 o sullivan et al 2016 sun et al 2016 abms model the properties activities and interactions of each agent or individual within a population grimm and railsback 2013 population or community level properties then emerge as the result of the collective actions of the agents these models are ideal for understanding how behaviours and actions at the individual level lead to the population level patterns that determine the stability and behaviour of the system as a whole railsback et al 2020 gallagher et al 2021 the flexibility of and diversity within the abm paradigm poses both an enormous advantage and a significant barrier to full acceptance of abms as a scientific tool for instance agent based modelling studies can be incredibly disparate and often opaque in their formulation execution and analysis this can often make critical assessment and re implementation cornerstones of good scientific practice very difficult for abm based studies and in this way jeopardize their reproducibility and ultimately their conclusions hence there have been many calls to standardize or unify agent based modelling approaches so as to introduce the same kind of coherence formalism and structure as classical population level modelling paradigms like systems of partial differential equations grimm and berger 2016 fulton et al 2019 this is an extremely ambitious goal with multiple intertwined strands however as similar issues are encountered in other simulation based modelling approaches opportunities for adaptation and cross pollination arise cranmer et al 2020 in this review we highlight recent work towards standardization within abm methodology along several lines i model description and documentation ii model implementation and iii model analysis and inference we close the review by summarizing the main findings and providing recommendations to guide users in the choice operation and interpretation of abms 2 model description a first important aspect of a unifying framework is a standardized protocol for describing abms as the model description conveys the findings from the model development cycle and the resulting abm to the reader it is crucial for the interpretation and reproducibility of the results in this section we briefly summarize the most common standard protocols for both model description and model development given that abms can involve on the one hand very few or even no equations and on the other hand significant numbers of parameters rules subroutines and submodels a clear and concise format for their description is crucial for peers to be able to understand and reproduce an abm study abm descriptions in published literature are often ad hoc and can therefore vary substantially in their level of detail for example a survey of published simulation based studies found that the majority 59 did not include model equations only 7 contained partial equations and only a minority 34 of the explored studies reported the complete model rahmandad and sterman 2012 similarly only 30 of the surveyed articles included all parameter values needed to replicate the baseline simulation scenario and less than 20 included complete units for all equations rahmandad and sterman 2012 clearly failing to report the underlying model equations and or parameters renders a modelling study unreproducible and hence weakens confidence in any conclusion drawn from it and at the other extreme of total transparency a survey of more than 2000 scientific articles published before 2014 and involving abms found that less than 10 made their source code available janssen 2017 against this varied background it is clear that a standardized framework for describing abms has not yet been adopted by the field at large however one approach stands above others in terms of the significant and growing number of studies that use it the odd overview design concepts and details protocol consists of seven elements grouped into overview purpose state variables and scales process overview and scheduling design concepts and details initialization input submodels it was proposed grimm et al 2006 in 2006 updated grimm et al 2010 in 2010 and once more grimm et al 2020 in 2020 in 2007 the year after the first version of the odd protocol was published it was cited by 1 8 of all published abm studies 9 of 509 articles grimm et al 2020 this proportion quickly increased to 7 7 of all abm studies 78 of 1017 articles by 2011 and in recent years has stabilized around 9 10 in 2018 172 of 1735 published studies 9 9 cited the odd protocol grimm et al 2020 the protocol has been adopted by modellers in a number of different fields predominantly the life sciences and social sciences in 2018 these fields accounted for 60 and 17 of the odd protocol s total citations respectively grimm et al 2020 the protocol has also been cited in studies from the physical and formal sciences e g computer science engineering mathematics these fields accounted for 4 and 19 of the protocol s citations in 2018 respectively while the odd protocol provides a consistent and fairly logical template for reporting abm implementations some issues remain descriptions still tend to run on the lengthy side often up to several pages and yet some important details can still be overlooked within the protocol s structure so while the odd protocol provides perhaps the best currently available basis for a standard description of abms a number of extensions and modifications to the protocol have been proposed these generally involve the addition or removal of sections and or sub sections to suit the needs of a specific field or application examples include the odd d protocol for abms which include human decision making müller et al 2013 the odd 2d protocol which formalizes the use of empirical data within abms laatabi et al 2018 the odd p protocol which also provides provenance information on how the abm was generated reinhardt et al 2018 and the oddox protocol which documents not only the model itself but also the source code topping et al 2010 however these extensions have not yet been adopted anywhere near as widely as the original odd protocol only 212 8 3 and 58 articles can be found on web of science respectively as of april 2022 compared to 1608 citations for the odd protocol in its original inception from 2006 the design and development of abms require numerous choices on the part of the modeller often some of these choices are not explicitly discussed or even mentioned when it comes time to describe the model for the purpose of dissemination hence there have been a growing number of calls in the abm literature for more transparent and extensive description of the model development process itself one of the most frequently cited is trace transparent and comprehensive model evaludation grimm et al 2014 where evaludation refers to the assessment of a model s quality and reliability thereby combining the processes of evaluation and validation trace was proposed as a standard protocol for the documentation of an abm s development and testing the documentation is extensive and detailed and therefore provides an excellent record and justification of the model s design and workings thereby greatly facilitating re implementation by others however compiling such an extensive document requires a significant effort on the modeller s part which may discourage some authors from selecting trace but instead choosing a more concise documentation format both the odd protocol and trace result in detailed and therefore lengthy descriptions of the abm while such an elaborate description certainly benefits the model s reproducibility it does not necessarily aid in its interpretation indeed the main ideas behind the abm and its key features may become lost in the details we believe that the introduction of a standardized graphical representation could overcome this and consequently benefit the field of agent based modelling this would result in a concise description of the abm in the main text of an article while the detailed descriptions could accompany the annotated source code in the supplementary materials efforts to establish standard protocols for the description of abms and their design have focused on the need to stimulate and support a culture of good modelling practices in the abm field augusiak et al 2014 such practices already common in other fields such as software development e g software development life cycle models davis et al 1988 have several benefits first they provide a basic level of quality assurance by ensuring that all model design choices are not only described but also justified grimm et al 2014 second they contribute to increased confidence in any given abm improving its likelihood of being used by other modellers in particular newcomers to the field and avoiding the need to redevelop the same model over and over again third they can help to standardize the model development process itself by avoiding the need to repeat testing or analyses before deployment of a given model 3 model implementation once a model has been conceptualized and its key processes formalized as mathematical equations or heuristic algorithms it must be implemented in some programming language for the purpose of simulation here as with other aspects of agent based modelling a tension emerges between the flexibility and freedom that the paradigm affords modellers and the lack of standardization inherent in this freedom hence many abms are implemented on an ad hoc basis subject only to the whims and fancies of the particular modeller for example two abms may have the very same underlying rules and mechanistic processes but differ in the order in which equations are solved or in which individuals characteristics are updated after interactions occur muelder and filatova 2018 for example the choice of a synchronous or asynchronous schedule for updates during a simulation can lead to different results even for very simple abms this divergence becomes more acute as population densities and interaction complexity increase caron lormier et al 2008 while this freedom of implementation has the benefit of permitting the emergence of novel techniques and creative solutions it also leads to an extraordinarily large number of implementations of similar if not identical environmental processes and algorithmic procedures furthermore those users who do not wish to implement their model from scratch may instead turn to a wide variety of software tools and platforms designed specifically for abm development and implementation to give an idea of the breadth of this fast growing body of work a recent review of this literature found 85 different software tools for abm development and implementation abar et al 2017 each of the numerous available platforms typically takes its own approach to the implementation of the abm s routines and processes increasing even further the suite of myriad options available to the novice modeller these options for abm implementation run the gamut from entirely from scratch implementations in a general purpose programming language gppl that are ad hoc and case specific to modular combinations of standard submodels wrapped in a novel case specific framework to off the shelf models that require the user only to plug and play fig 1 the first approach is clearly the least standardized and hence depends entirely on the modeller to make appropriate and well justified design and programming choices however it also allows for novel approaches to emerge by taking advantage of the extreme flexibility inherent to abms unfortunately these worthy examples can easily be lost in the flood of new abms so that the insights and knowledge they bring are not picked up by the scientific and modelling community this build up of abms without a corresponding increase in our body of knowledge has been referred to only half jokingly as yet another model syndrome o sullivan et al 2016 and is a key factor in the increasing fatigue felt in some domains for the abm paradigm on the other side of the spectrum stand alone programmes like netlogo wilensky 1999 mason multi agent simulator of neighbourhoods or networks luke et al 2005 and repast north et al 2013 provide general abm environments that can greatly simplify model development and implementation however with the growing number of such development platforms it has become a tedious task to select the one that serves the developer s needs novice researchers will often not be able to find the right tool for their specific model or lose a significant amount of time searching for it given that each programme has its own syntax and semantics that will suit some situations and not others moreover these programmes are limited by the included tools for example those needed for model analysis and inference see section 4 hence many researchers still prefer to implement new and stand alone models taking advantage of abms flexibility to construct a model carefully tailored for their specific simulation and analysis needs between these two extremes abm packages have been developed for several programming languages e g the mesa package in python kazil et al 2020 or agents jl in julia datseris et al 2022 as well as extensions of existing abm platforms to general purpose programming languages e g extensions between netlogo and r thiele 2014 salecker et al 2019 or python jaxa rozen and kwakkel 2018 as illustrated in fig 1 in this way modellers can take advantage of established submodels or frameworks as modular building blocks within their own abm this reduces the modelling effort relative to implementing an abm entirely from scratch but is still less restrictive than the other extreme of an off the shelf model other implementations focus on developing more robust abm frameworks that unify modelling approaches within a specific field or context this allows researchers to focus on the development and refinement of analytical tools for a generic process based framework that is readily applicable to different models within the field one example is the work of christensen et al who developed a framework to couple biological abms and physical based models of oceanographic processes christensen et al 2018 this strategy allows researchers to swap out either the physical or biological components so that they can compare different submodels and take advantage of the cross scale capabilities of the abm approach in a flexible manner this transition is taking place in various disciplines for example the biomedical field is confronted with the problem of converting mechanistic knowledge of individual level processes to higher level outcomes that are clinically relevant and particularly doing so within a single cohesive model this has motivated the recent development of multi scale abms such as spark simple platform for agent based representation of knowledge one example of a framework for developing systems level biomedical abms that can admit data across multiple scales solovyev et al 2010 cilfone et al review this type of approach for models of biomedical systems and provide a generic framework for linking simulating and analysing such hybrid multi scale models cilfone et al 2015 another example is the emod platform bershteyn et al 2018 although restricted to epidemiological modelling this agent based framework can be used to model multiple diseases since the majority of the abm processes are relevant to infectious disease modelling in general this takes advantage of similar interaction mechanisms i e disease transmission to allow researchers to build on a shared model framework and again swap out different modules that are specific to certain diseases for example airborne diseases versus vector borne diseases given the wide variety of approaches to abm implementation a natural question for the novice programmer is which is best of course the flexibility of abms means there is no set answer to this question but instead a value judgement about which features of a model are most important to a given research setting and question however recent studies have begun to pay more attention to the fact that there is indeed sometimes a right way and a wrong way to implement an abm of a given process or system pietzsch et al 2020 taghikhah et al 2021 model verification or model checking is the domain concerned with the assessment of a model s behaviour with the goal of ensuring that its formulation and implementation were appropriately designed and correctly programmed unfortunately formal verification techniques typically depend on applications of formal logic that are often beyond the interest and knowledge of most modellers liu et al 2021 advances in the model verification field are therefore needed to improve users confidence in abms as doing what it says on the tin capturing the key system characteristics and processes at the appropriate level of detail without resorting to obfuscating black box approaches 4 model analysis and inference 4 1 modern models require modern tools of analysis in order to meet its scientific purpose an implemented model must be subjected to a thorough analysis while such an analysis is typically seen as forming a calibration and validation step of the modelling cycle we will approach this from another viewpoint an analysis can either target an understanding of the model behaviour e g stability bifurcation and sensitivity analysis or the model simulations can be compared to observational data e g parameter estimation model selection which is commonly referred to as inference a calibration step involves an inferential analysis but validation is closely related to the research question and as such problem specific however the results of both the behaviour and inferential analysis are used in validation the flexibility of abms comes here with yet another cost the loss of analytical tractability while abms are capable of simulating complex emergent behaviour it is notoriously difficult to infer interpret or explain the rules that govern agents actions and lead to these emergent behaviours bodine et al 2020 indeed as most abms are inherently stochastic the actions and responses of agents in the system occur stochastically conversely it is also often the case that different rules and parameter settings can lead to the same model output a phenomenon known as equifinality graebner 2018 despite the huge increase in computational power over the last few decades exploring the entire parameter space of an abm is still practically infeasible hence abm analyses are generally based on heuristics that rely on many abm simulations with varying assumptions and parameter sets lee et al 2015 these heuristics often involve ad hoc decisions rendering the results hard to compare reproduce or generalize in contrast a major advantage of more established models like those based on differential equations is the availability of standardized analytical tools in absence of such tools a modelling paradigm is bound to remain restricted to the theoretical margins and without consequential applications it is therefore not surprising that more and more studies are working towards the development of standardized methodological frameworks for abms analogous to those for classical models when it comes to modelling natural and environmental systems pattern oriented modelling has been proposed as such a standardized framework for abms in which the focus lies on reproducing observed patterns grimm et al 2005 the latter is certainly a creditable attempt to guide modellers in developing and analysing abms but it has become more of a modelling philosophy and hence remains too vague to be a truly standard methodology while the popularity of abms continues to increase the explicit use of pattern oriented modelling has stagnated over the last years gallagher et al 2021 conversely analytical methods rooted in dynamical systems theory and statistics have been increasingly used for abms it is important to note that the problem of analytically intractable simulation based analysis is of course not limited to abms as this issue presents itself in many areas where computationally expensive models arise cranmer et al 2020 indeed the methods discussed in the remainder of this section were typically developed in other research domains and only later adopted by the abm community generally two main strategies can be distinguished for developing such standardized tools 1 adapt or extend existing analytical tools so they can be applied to abms or 2 approximate abms by tractable models that permit the use of established analytical tools from other fields both of these strategies have resulted in novel methods for achieving the two main analysis goals before reviewing these methods we briefly discuss these goals in relation to abms 4 2 goals of model analysis 4 2 1 behaviour analysis since a model s behaviour strongly depends on the values of its parameters a quantitative analysis of this dependency is crucial typically this modelling step comprises some sort of sensitivity analysis with the aim of identifying the most and least thus possibly redundant model parameters saltelli et al 2007 sensitivity analysis methods for abms are well established with readily available tutorials e g thiele et al 2014 ten broeke et al 2016 borgonovo et al 2022 and software e g sensitivity package in r sensitivity analysis library salib package in python behaviorspace in netlogo however criticism has recently been levelled at how such sensitivity analyses are being performed in practice noting that many do not sufficiently explore the parameter space saltelli et al 2019 moreover it has been pointed out that local sensitivity analyses are based on local linearity and should not be used for highly nonlinear models as abms typically are when studying such highly nonlinear systems the analysis of steady state behaviour can provide key insights into the system s dynamics for example a bifurcation diagram where the equilibrium points of the system and their stability and or basin of attraction are plotted as functions of a varying model parameter can be used to identify and quantify tipping points ashwin et al 2012 of the system where transitions between behavioural regimes occur due to external forcing in natural systems for example such tipping points are typically linked to major system level events such as ecosystem collapse or the catastrophic failure of complex networks scheffer 2009 lacking the analytical tools for finding the equilibria of the model abm users typically resort to simulations over long time periods while manually tuning model parameters in order to observe these dynamical shifts woods et al 2005 however this approach again quickly becomes computationally prohibitive for all but the simplest abms especially when considering the effect of multiple parameters simultaneously moreover as the stochastic nature of most abms complicates even the basic definition of an equilibrium or bifurcation van nes et al 2016 ad hoc operational definitions are instead required colon et al 2015 for example used monte carlo singular spectrum analysis to detect statistically significant oscillatory patterns in abm outputs given a certain value of a model parameter θ the detection of such patterns in more than 50 of the abm simulations was subsequently defined as a transition to a limit cycle regime i e a hopf bifurcation in the parameter θ 4 2 2 inferential analysis when observational data are available to compare with simulated data we bump into another analysis problem typically such an analysis focuses on retrieving meaningful model parameters and is hence often referred to as inference parameter estimation model calibration or inverse modelling in all these cases the purpose is to determine the most appropriate values for the model parameters based on some comparison to the observational data the latter can aim for example to reproduce key demographic or dynamical patterns or obtain model predictions that match the observed data as closely as possible as in model behaviour analysis manual exploration of the parameter space quickly becomes infeasible hence agent based modellers often turn to numerical optimization methods such as simulated annealing or genetic algorithms hartig et al 2011 grimm and railsback 2013 thiele et al 2014 the goal of the latter is to find one optimal set of parameter values for which the abm simulations agree to some desired level with the observations however keeping in mind the fundamental issues of equifinality and stochasticity the benefit of finding such an optimal parameter set seems questionable gallagher et al 2021 calls for more rigorous inference and uncertainty quantification have thus drawn attention to suitable methods from inferential statistics as these methods can rely on the foundations of probability theory they are apt to be standardized and suitable for robust decision making however their application to abms is often thwarted by the fact that an analytically tractable likelihood function cannot be obtained for most abms this problem precludes the direct application of a large body of frequentist and bayesian statistics cranmer et al 2020 and forces modellers turn to more novel approaches in the next section we review a number of such approaches 4 3 adaptation of existing tools 4 3 1 adapted behavioural analysis tools a first approach for analysing abms is to adapt established tools from other fields so that they can be applied to abms such methods regard abms as black box models that produce a set of simulated outputs given a particular set of inputs i e the values of the model parameters and initial conditions this model often termed a generative model or simulator depending on the application is then employed in some type of numerical scheme inspired by an established analysis for example the equation free framework kevrekidis et al 2004 was developed for performing numerical steady state analysis of the emergent patterns of a simulator in this context an abm this framework is based on the separation of time scales between the dynamics of the microscopic simulator and its emergent macroscopic behaviour assuming that the former will quickly converge to the latter the main idea is that under these circumstances the equation free time stepper can jump between the microscopic and macroscopic state spaces in practice this boils down to consecutive short bursts of abm simulations followed by the derivation and extrapolation of the emergent patterns and careful re initialization of the simulator by performing the extrapolation with a numerical bifurcation scheme it would be possible to efficiently construct numerical bifurcation diagrams or at least do so more efficiently than with a brute force or manual approach however switching from the low dimensional macroscopic space to the high dimensional microscopic space in equation free terminology the lifting step is a non trivial task kevrekidis and samaey 2009 lifting requires the re initialization of the abm based on aggregated states and is thus problem specific and often computationally intensive vandekerckhove et al 2011 willers et al 2020 hence in spite of some attempts to generalize this method for abms thomas et al 2016 its application has so far been limited to very specific cases siekmann 2015 martin and thomas 2016 4 3 2 adapted inference tools in contrast to behavioural analysis tools the development of likelihood free methods for statistical parameter inference has enjoyed more success in particular approximate bayesian computation abc has become an established tool in the abm domain and beyond sisson et al 2018 cranmer et al 2020 several abc implementations such as the r package abc csilléry et al 2012 are readily available for use with abms van der vaart et al 2016 pietzsch et al 2020 as its name suggests abc belongs to the field of bayesian statistics where prior knowledge on the parameters is updated with new information from observations using the likelihood function gelman et al 2013 the resulting posterior distribution forms the inferential basis of the bayesian analysis abc comprises a family of methods in which the likelihood function is approximated by repeatedly comparing abm simulations with observations beaumont et al 2002 as is illustrated in fig 2 this comparison is based on some discrepancy measure which is often just the euclidean distance applied to the observations and a set of summary statistics of the simulated model outputs the simulation based likelihood approximation is then passed to a sampling algorithm such as rejection sampling markov chain monte carlo or sequential monte carlo which favours parameter sets resulting in the lowest discrepancy measure values and as such generates a sample from the approximated posterior distribution hartig et al 2011 the use of summary statistics in abc fits with the idea of pattern oriented modelling as they arise when simulated patterns are quantitatively compared with observations gallagher et al 2021 provide an overview of such patterns for models in natural systems it should be noted however that the performance of abc critically depends on the choice of summary statistics prangle 2018 because summary statistics reduce the dimensionality of the problem similar to the macroscopic space in the equation free method this reduction generally entails information loss that can affect the inference quality whether the latter is the case is a matter of sufficiency a sufficient summary statistic preserves all the information in the data to infer a specific parameter as analytical intractability precludes the construction of rigorous proofs hartig et al 2011 sufficiency of summary statistics must be tested empirically for most abc inference problems several strategies have been proposed for selecting abc summary statistics reviewed in blum et al 2013 prangle 2018 and beaumont 2019 but this critical dependency on user defined summary statistics is seen as a major shortcoming of abc cranmer et al 2020 moreover it often requires so many simulations that the computational overhead becomes prohibitively large especially because simulations must be repeated for every new scenario or parameter setting 4 4 approximating abms 4 4 1 model behaviour analysis using abm approximations the purely simulation based methods described in section 4 3 involve formulating a wrapper method around the abm which repeatedly runs the model with different parameter sets a second broad approach for analysing abms makes use of some abm approximation fig 3 the latter can involve a purely data driven approximation trained on the abm outputs which is often called a surrogate model emulator or meta model such a surrogate model can be used as a computationally cheap alternative of the abm in an adapted behaviour analysis an example hereof is the use of support vector machines trained on some classification of abm outputs to construct bifurcation diagrams van strien et al 2019 ten broeke et al 2021 another approach is to construct an analytically tractable model in parallel to the abm nardini et al 2021 this relaxes the dichotomy between simulation based abms and more tractable paradigms by seeing them as complementary tools the hope is that one can have the best of both modelling worlds where the abm provides accurate simulations of reality and the tractable model s provide insight into the emergent behaviour of the abm for example colon et al 2015 use a tractable approximative model to guide the construction of a bifurcation diagram for the abm in some cases the tractable model can be a set of algebraic equations hinkelmann et al 2011 but this approach is restricted to deterministic abms ruling out its application for the majority of abms of natural systems a more general approach is to instead derive an approximate model based on differential equations nardini et al 2021 typically such an approximation is based on the mean field assumptions where the properties of agents are averaged out by use of local densities a example hereof is the work of reichenbach et al 2007 which assesses the effect of species mobility on biodiversity using a simple agent based model and its counterpart based on partial differential equations pdes more recently bernoff et al 2020 modelled the resource dependency of locust movement based on an abm and then constructed a pde based model to further investigate the emergent patterns in a sensitivity analysis yet another strategy is to recast an abm as a spatio temporal point process ovaskainen et al 2014 where agents are represented by points in space and the dis appearance and movements of these points therefore represent the system s demographic processes birth death and dispersal this approach was recently used by cornell et al to derive a more generic framework for abm analysis cornell et al 2019 however deriving an appropriate set of p des is often challenging and methods such as symbolic regression have been developed to automate this formulation from data schmidt and lipson 2009 brunton et al 2016 the latter typically starts from a set of analytical building blocks which are combined in a regression analysis nardini et al 2021 reviewed how such an automated de discovery can also be applied to abm simulations these authors demonstrate that this method can help bridge the gap between abms and p de based models by revealing the cases in which the assumptions underlying the latter do or do not hold automated de discovery hence seems a promising tool for future model development and automated equation derivation functionalities are planned to be incorporated into new software such as the agents jl package in julia datseris et al 2022 4 4 2 inference using abm approximations a recent review by cranmer et al 2020 highlighted a similar shift in simulation based inference methodology there is now a growing interest in amortizing the computationally expensive simulations performed by a generative model the amortization is again achieved by employing some approximative part of the inference scheme e g the generative model the likelihood function or the posterior distribution this boils down to a manifestation of the bias variance trade off the use of an approximation introduces some bias but if this approximation reduces computational overhead then more samples can be drawn so that the variance and monte carlo error is reduced classical methods in this category aim to find an analytically tractable approximation of the likelihood function which is often referred to as the synthetic or pseudo likelihood hartig et al 2011 this approach can be parametric if the parameters of some theoretical distribution are estimated from the generative model outputs wood 2010 hartig et al 2014 for example use a multivariate normal approximation for the likelihood function of the parameters in an agent based forest model on the other hand non parametric kernel density estimation has long been proposed for approximating the likelihood function diggle and gratton 1984 but is rarely used because of an extra limiting computational overhead particularly for high dimensional inference problems grazzini et al 2017 shiono 2021 novel simulation based inference approaches tackle such high dimensional problems more efficiently by relying on recent advances in machine learning cranmer et al 2020 these methods construct approximations using flexible data driven models again trained on the simulated outputs called surrogates or emulators for example alden et al 2020 trained five types of machine learning models to emulate an agent based simulator of lymphoid tissue organogenesis they found that the ensemble predictions of these surrogate models were highly accurate and could be used to reproduce a previously reported sensitivity analysis with far less computational overhead moreover the ensemble surrogate was then used to perform an abc analysis which would have been computationally infeasible with the abm itself similarly reiker et al 2021 calibrated an abm of malaria using a gaussian process emulator this emulator was trained progressively on the abm s simulations using bayesian optimization and subsequently used for a global sensitivity analysis an example of a posterior surrogate was explored by shiono 2021 for a standard macroeconomic abm this method termed bayesflow radev et al 2022 involves training a conditional invertible neural network on a set of abm simulations obtained with a small set of samples from the prior distribution when compared to the kernel density approximation synthetic likelihood method bayesflow showed promising results in the sense that the computational overhead was drastically reduced especially when the trained surrogate could be re used moreover bayesflow provided accurate parameter estimations that were robust to bifurcations as these modern simulation based inference methods show promising results with abm applications we envision them being increasingly adopted by the abm community in the near future 4 5 standardization of abm analysis methods it should be clear that methods for both behaviour and inferential analysis of abms are facing similar issues regarding tractability and computational demand as the main development strategy is shifting from adapting existing analysis methods to approximating abms by tractable and computationally cheaper models we expect more proof of concept papers that apply new analysis methods to specific abms while such papers certainly help to expand the abm analysis toolbox they do not contribute to a standardized abm analysis framework moreover the added complexity of new methods may not necessarily lead to better results as the performance and design of these methods tends to be highly problem specific agent based modellers should consider multiple approaches simultaneously when analysing their model carrella 2021 similarly to occam s razor we would recommend including simpler more established methods in such an analysis ensemble 5 conclusions and perspectives abm standardization is more than a matter of simple housekeeping it is important to and necessary for ongoing progress in the development of both our theoretical knowledge and our methodological toolbox it can allow us to establish a consensus on options for model structure analysis and data requirements but we should also keep in the mind that the flexibility of abms is one of their main advantages in this sense attempting to standardize abms to an extreme degree is counterproductive and misses the point of the freedom inherent in their design and analysis instead a carefully maintained and above all consciously considered balance between standardization and flexibility will bring the most benefit to the future prospects of abms as a modelling tool this trade off is a recurrent theme in the different stages of the agent based modelling cycle a clear description of abms is crucial for their reproducibility and their part in the scientific discussion and this requires a common tongue i e a standardized way of communicating the modelling cycle in this respect odd and trace are important advances for the abm field however due to abms flexibility they often result in lengthy documents and therefore a significant amount of extra work on the part of modellers reviewers and readers to keep communication concise but comprehensive we envision these documents as guides through a well annotated and openly available source code for example the comses openabm library offers a collection of tutorials on agent based modelling and its associated techniques as well as a model library that allows modellers to share the design and implementation of their abms in order to facilitate reproducibility and transparency janssen et al 2008 moreover as the abm field moves towards increasing use of standardized and reusable submodels grimm and berger 2016 these documents can increasingly refer to these well known submodels the latter trend will also move the field away from the need for every study to implement most or all of its code from scratch this will allow researchers to benefit from more consensus on the appropriate data needs computational methods and analytical approaches zhang and robinson 2021 however the analytical toolbox for abms is still very much under construction and so abm platforms need to be compatible with novel analytical methods we therefore expect an expansion of abm packages in general purpose scientific programming languages as well as more extensions for abm specific software this will increase the options available to modellers so that they have more freedom to select the tool most suited to their own particular needs at the same time transparent documentation and critical comparative reviews will be needed to avoid the choice overload that can leave modellers overwhelmed and unsure as to the best choice of implementation platform here the abm field can learn from best practices in software development and other computer science applications studies aiming for either behaviour or inferential analysis face similar issues due to the analytically intractable nature of abms more specifically abm analyses are plagued by the curse of dimensionality when exploring the model s parameter space and comparing high dimensional outputs or observations rather than trying to tackle these issues on their own the abm community can make better use of the work being done in other fields concerned with high dimensional problems for example machine learning such practices will only become widely adopted if users find their way to comparative reviews of these methods baker et al 2022 and well documented software packages that are readily usable with abms declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25507,complex systems simulations can support collaborative water planning by allowing stakeholders to jointly see hidden effects of land and water use decisions on groundwater flow we adopted a participatory modeling progression where stakeholders learned to modify and use increasingly sophisticated models to assess policy impacts on groundwater levels stakeholders shared understanding of the problem and the novelty concreteness and richness of proposed solutions evolved alongside the models degree of realism but up to a certain point more realistic models became a distraction and stymied efforts to plan for water shortages the reflective learning required to plan for complex environmental problems is best supported by models that strike a balance between representational fidelity and end user intelligibility complicated models and high resolution data may overwhelm model users preventing them from acting on the useful planning insights they derived from the exploratory modeling particularly within social contexts that exhibit strong power dynamics and favor prediction keywords participatory modeling complex systems modeling environmental planning computer supported collaborative learning data availability the data that has been used is confidential 1 introduction the role of models in supporting participatory environmental planning has been studied extensively in natural resource management and in planning support systems literature see geertman and stillwell 2020 and will et al 2021 zellner et al 2012 for reviews most of these studies describe cases in which a particular model was developed or adapted to inform collective management and planning decisions assessment of the use of such models emphasize user satisfaction as a key for a successful participatory activity pelzer 2017 russo et al 2018 te brömmelstroet 2017 fewer studies however conduct a systematic assessment of how these models support collaborative decision making goodspeed 2016b jones et al 2009 milz 2019 in other words how they inform and improve the judgments of participants about the specific problem they are seeking to address and how they address it arciniegas et al 2013 gray et al 2018 hedelin et al 2021 inman et al 2011 jordan et al 2019 milz 2019 salter et al 2009 it is often assumed that non expert stakeholders have more confidence in expert crafted predictive models and that their confidence is linked to how accurately models represent features in the system and the resolution of the spatial and or temporal data on which it runs goodspeed 2016a however it is not clear that more complicated and detailed models of complex systems can improve the quality and impact of environmental planning and policy making in practice hoch et al 2015 zellner et al 2012 complicated detailed models are expensive and time consuming to create tune and validate such labor intensive tools are effective and useful to advance scientific knowledge but may impede effective group deliberations stakeholders end up either relying on expert interpretations or dedicating the bulk of their energy to learning how the model works klosterman 2013 thus they are never able to fully integrate insights gained from the models into the planning process in response some researchers have built models that rely on simple abstractions for the purposes of enabling stakeholder learning about the underlying dynamics of a complex problem and supporting better decision making rather than producing accurate predictions lempert 2019 in this way simple models may help planners and stakeholders hone their intuitive sense of how a system would respond to different plans and policies helping them engage in more robust planning while these modeling tools tend to be more user friendly than sophisticated models used for scientific research and prediction they typically lack the concreteness of real world details of the problems faced by stakeholders in their own specific communities e g communities of neighborhood residents farmers etc thus stakeholders have to learn for themselves how to translate insights from the simple models to the messy dynamics of their real problems it is not always clear that what stakeholders learn is applicable to their needs and adequate for their issues therefore they may lack confidence in the solutions that emerge from using such overly simplistic abstractions to interrogate the tension between simple versus realistic models in informing environmental planning we combined the advantages of each via a theory grounded approach to structuring planning sessions that we refer to as a participatory model progression in this article we explore how a progression of agent based models that started with stylized models of land use and groundwater interactions leading to more detailed and geographically relevant ones supported collective deliberation and planning about groundwater depletion in a community in northeast illinois usa we designed our approach and modeling tools in line with theories drawn from the learning sciences goldstone and son 2005 goldstone and wilensky 2008 and prior planning studies that employed models with different degrees of detail see zellner et al 2012 for examples we hypothesized that the gradual incorporation of more detailed information and higher levels of geographic correspondence between the model and the stakeholders community would enhance their understanding of interactions between land use and water use decisions and their effects on groundwater flow in other words stakeholder learning and deliberative quality would scale with the detail of the model we also hoped over time to see stakeholders develop trust in the insights they derived from using the models a trust based not on expert endorsement but instead on their own growing collective understanding of systems here we present the approach we developed and the analysis of the interactions and policy discussions that ensued identifying the aspects that supported and hampered learning and innovation analysis of meeting dialogue demonstrates that stakeholder learning improved as the models became more sophisticated up to a point we find that a model that balanced simplicity and representational accuracy effectively supported deliberative conversations about groundwater policy and practices however when presented with a detailed realistic model of their community participants shifted from discussing the problem to talking about the tool as predicted by lee 1973 and progress toward concrete policy deliberation was lost in the next section we provide an overview of the literature covering the tension between simple and complicated modeling for natural resource management and environmental planning and propose a framework for understanding this tension we follow with a description of our case and methodology for participatory modeling and for assessment of the activities we discuss our findings on how participants learned from the experience with different kinds of models followed by a discussion of the barriers encountered and conclude with implications for participatory modeling to support environmental planning particularly in light of the growing proliferation of gaming and data visualization tools promoted for this purpose 2 background 2 1 simple versus complicated models for planning and policy making often overlooked in lee s 1973 famous critique of the comprehensive urban simulations of the mid 20th century is the tension modelers face between the size and complexity of the models and the type and amount of knowledge gained from using them for lee the knowledge gained about policy and urban structure peaked with the development and use of small simple models and diminished rapidly as the models increased in size and complexity see lee 1973 p 173 even moderately complicated models led to rapidly diminishing returns in terms of useful knowledge for planning shifting to knowledge about the model instead this tension and its implications for planning and policy making has been discussed elsewhere in the literature e g will et al 2021 with scholars often advocating for the collective use of simple models with novice users e g bankes 1993 zellner 2008 here we sharpen and codify this distinction into a framework for exploring the utility of models for policy and planning deliberation represented in fig 1 our framework includes two dimensions model sophistication and model representativeness model sophistication refers to the relative simplicity or complication of a model as it appears to the novice user models can be characterized by where they fall on a spectrum between complicated and simple complicated models include a large number of mechanistic features that can be set by the model user more simplistic models reduce or eliminate the number of features that can be adjusted by a user model representativeness on the other hand refers to the degree to which the model replicates reality models can be characterized by where they fall on the spectrum between stylized i e low representativeness and detailed i e higher representativeness thus this framework contains four broad classes of models i toy models that are stylized and simple ii complicated theoretical models that are highly stylized they may not be geographically or demographically relevant but include all the elements of the dynamic interactions iii data dependent models that represent relationships in a simple way masking complexity from the end user e g regression models machine learning approaches and iv expert models that are both complicated and realistic our study described in detail in the methodology section was intentionally designed within this framework to follow a progression from toy i to expert iv models expert and data intensive models tend to be favored in the field yet are unwieldy to use and expensive to build dennis et al 1984 and the large number of parameters makes calibration difficult due to long run times and increased instability doherty and christensen 2011 a toy model can run faster and fit observations with fewer parameters reducing the risk of overfitting brown et al 2005 the tradeoff is that expert knowledge e g field observations and measurements complex and cross scale interactions does not readily map into the simple model s parameters and mechanisms sun et al 2016 this problem only becomes important if the simple model misses critical drivers of the system or if it underestimates the uncertainty present in the model simplified versions however could be designed as stepping stones towards understanding the more complicated expert models edmonds et al 2019 smaldino 2017 and because they are less expensive to build and learn from they can be used to rapidly test scenarios to highlight relevant issues and policy tradeoffs combining the use of both complicated and simple models with a well defined association between the two can establish credibility in the simpler model and support relevant and meaningful exchange of ideas and provide more robust support to decision making than either one alone while the lack of accuracy of simpler models could weaken their role in politically sensitive decisions it could also make it easier to rely on the more complicated models when higher levels of accuracy are required in other words different levels of detail are necessary for robust decision making but over emphasis on detail and accuracy may be misplaced often models are critiqued based on how faithfully users believe they represent reality more realistic models may be more influential for planning and policy making however a model deserves criticism only when it fails to achieve quantification of uncertainty and maximum reduction of uncertainty through optimal processing of environmental data picture perfect rarely equates to optimality in either respect doherty 2011 p 455 representational accuracy for example may become a distraction and a barrier to identifying key interactions salter et al 2009 because the cognitive effort required to understand these interactions and their effects varies with the amount level of detail and format of presented information arciniegas et al 2013 more qualitative and simple models tend to allow for more exploration and innovation in solutions because the simplicity of the model facilitates its active use for brainstorming and collective problem solving arciniegas et al 2013 le page and perrotton 2017 smaldino 2017 twiston davies et al 2021 will et al 2021 goldstone and son 2005 found that juxtaposing a visually detailed and a visually simple model worked better than using either alone following a process of concreteness fading from the former to the latter to identify most salient features le page and perrotton 2017 propose a kilt keep it a learning tool approach to strike a balance between abstract theoretical simple models and empirical descriptive realistic models neither of which are adequate to support social learning and collaboration among stakeholders addressing complex problems nevertheless there is a cultural bias against simpler and more abstract models that do not reproduce the reality as stakeholders perceive it doherty 2011 models of different levels of detail may also be more or less useful given the context after all it is not just the model that affects collective decision making but also the social context and purpose for which it is used edmonds et al 2019 grimm et al 2020 sun et al 2016 zellner et al 2012 for example online models for public use one potential way to engage more public participation in planning may need to use simpler models and adopt additional strategies like a game like structure to encourage and sustain participation among a broader population of stakeholders poplin 2012 while expert users may question the realism and the visibility of the calculations of a game making the game more realistic would defeat the purpose of lowering the barrier to public participation moreover more detailed models tend to perform worse and be valued less than more stylized and qualitative approaches due to increased decision stress i e the aspects of decision making that make it difficult e g the amount of effort entailed or the confidence in the decision and the consequent perception of decision inaccuracy inman et al 2011 on the contrary simplicity facilitates a more intense and stimulating use of models which correlates with users effective use of information ability to explore and innovate solutions and the quality of overall outcome inman et al 2011 the act of deciding which parameters are included and which are excluded in a model the act of building a model itself is fundamental to coming to understand a modeled system thus participatory modeling approaches often recommend that models be co constructed with participants gray et al 2018 sterling et al 2019 a demonstration of the power of model co construction can be found in lane et al 2011 who studied an approach to flood risk management where models knowledge and policy recommendations were co produced in a parallel distributed and dynamic process involving anyone concerned with an issue either through their profession their interest or their experience simpler and less expensive and data intensive models were coded by the scientists and trusted by the locals because they were more consistent with local data and collective knowledge collective engagement from conceptual development to critical assessment of the tool rather than model sophistication and detail gave the group a sense of competence to intervene in a situation where negotiations had stalled and engage in practice of a decision that broke the status quo lane et al 2011 2 2 toward a more effective practice of participatory modeling within a policy context the value of complicated models lies squarely in their ability to predict outcomes and often win the trust of planning participants whose beliefs align with these outcomes whereas the value of simpler models seems to lie in their explanatory power non experts merely consume or reject the results of complicated models but can engage much more richly with simpler models through for example not only scenario exploration where the focus is on the model interface but also model co construction where the focus is on model structure and function building a deeper understanding of salient variables and how these variables affect others in other words they can begin to understand the system that drives the outcomes which can translate into reasoning more flexibly while planning collectively a skill that is direly needed to support community adaptation to complex problems zellner 2008 one major open question though is how to effectively build models with the appropriate level of detail and realism within environmental planning processes defining the right level might differ across contexts so how can we generalize the process by which we find this level how do we address the widespread bias against simpler and stylized models we have little evidence for what might constitute effective collaborative model building strategies with stakeholders for obvious logistical reasons few existing studies of model sophistication have been able to follow through and examine the impact of model use on the actual community s plans and actions lane et al 2011 did not set out to study this aspect per se but they were able to show that engaging non experts in the model construction process led to a shared deep understanding of the planning challenges and in turn to real change in the community to get beyond a collection of empirical examples of how models could be used in planning and towards shared guidelines for effective participatory modeling in planning the practice of collaborative model building should be grounded in theory there is an abundance of theoretically grounded research in the domain of the learning sciences that studies how models can be used to support knowledge building and reasoning model construction is bolstered by the pedagogical theory of constructionism which claims that both motivation and knowledge are enhanced when learners get a chance to construct artifacts that express their knowledge papert 1980 resnick and kafai 1996 while constructionism can be used as a general pedagogy e g to guide the design of programming environments for children and youth maloney et al 2010 there is rich empirical evidence demonstrating how building simulation models can help people learn about complicated real world systems e g barab et al 2000 blikstein and wilensky 2009 jonassen et al 2005 klopfer 2003 klopfer et al 2005 louca and zacharia 2012 smetana and bell 2012 stieff and wilensky 2003 wilensky and reisman 1999 wilensky and resnick 1999 although there can be challenges learners often need scaffolding to engage in the modeling process sins et al 2005 moreover research in learning sciences supports observations that increased realism and detail in models and representations can distract from core system features goldstone and wilensky 2008 note that realism in models is sometimes disadvantageous p 507 and instead recommend that non essential aspects of a complex system be removed or idealized to allow learners to focus their attention on the core phenomena that underpin the system s emergent properties based on the above literature and our prior work zellner 2008 zellner et al 2012 we study how a constructionist strategy of model progression might help communities address complex environmental planning problems fig 2 participants with diverse expertise would begin with a very simplified model of core aspects of the environmental problem i e a toy model and then co construct with the researchers more detailed models that would include their specialized local knowledge towards the center of fig 2 doherty 2011 lane et al 2011 le page and perrotton 2017 the simpler models should capture the relevant system features and serve as appropriate mirrors of more complicated and focused models dennis et al 1984 doherty 2011 goldstone and wilensky 2008 the use of such combined models must be sensitive to the social context zellner et al 2012 and attempt to remove barriers to entry poplin 2012 to prevent users from perceiving the model to be difficult to use and in turn affecting their use of the model arciniegas et al 2013 inman et al 2011 salter et al 2009 while our progression is inverse to the concreteness fading suggested by goldstone and son 2005 and sun et al 2016 our participants were not learners in the traditional sense but professionals and community representatives who are experts on the complex problem being addressed and whose goal was to jointly plan for it in this context we wanted participants to help us co define the elements in the more detailed model versions so that it would better reflect their concerns hopefully by exposing participants to the construction process building on a simple model to develop a more detailed and representative one we could support participants understanding of the iterative and simplifying nature of model development grimm and railsback 2012 and how to appropriately use modeling to gain policy relevant insights bankes 1993 zellner 2008 zellner and campbell 2015 we seek to contribute to planning and participatory modeling literature with an evaluation of this theory grounded progression strategy of increasing model detail within planning meetings by systematically evaluating the ways in which different types of models support deliberation toward policy innovation we can begin to devise guidelines for how modeling should be conducted within such collaborative planning efforts as much as the models are important so is the facilitation and participation structure we seek to study how simple models can work if built as part of a collective endeavor where trust can grow in the group s learning and decision making competence as well as in the modeling tools that emerge from the collective generation of knowledge most planning research assesses outcomes of the modeling and planning process by using metrics like participant satisfaction bryson et al 2013 deyle and slotterback 2009 uddin and alam 2021 which can be misleading users may express satisfaction with models that are visually appealing and easy to manipulate for example yet are a poor or obscure representation of the underlying socio ecological complexity preventing the necessary learning and reasoning that can increase decision quality we focus here on documenting and understanding the process of collaborative model building and use in planning 3 the case a us supreme court decree wisconsin v illinois 1967 and the great lakes compact s j res 45 2008 limits the amount of water communities in northeastern illinois can withdraw from lake michigan annin 2006 many must rely on aquifers for their water supplies as a result and decades of unsustainable growth and development have depleted their groundwater supplies we partnered with an organization representing several exurban communities in northeast illinois usa that have all been experiencing water supply threats the organization s mission is to promote regional cooperation and to develop and implement a comprehensive plan for development and conservation having long recognized the importance of water resources they created a formal initiative in 2001 to study their resources and promote deliberation on how to best use and sustain their water supplies we learned through our early conversations that their concerns focused on the effects of droughts and unconstrained urbanization occurring around them exogenous development was drawing down the aquifers beneath these communities and droughts are reducing aquifer recharge moreover real estate developers had been pressuring these communities to grow which they had resisted their efforts eroded the trust with neighboring communities developers the court system and external experts who might have undermined their efforts to protect their water supply with models that do not adequately represent the area s natural and socio economic environment or the communities concerns as a result the organization was keen to protect their groundwater resource from neighboring communities hoping to isolate themselves from the effect of surrounding development in 2011 we worked with their committee for water resources as they were working on a formal groundwater plan they had been working for 11 years to develop a data grounded depiction of their groundwater resource but were unsure on how to make sense of it for planning nevertheless they had already settled on several policy recommendations including development restrictions groundwater recharge protection areas data and monitoring standards a public education program water pricing conservation design injection wells and other water conservation practices while providing policy guidelines this list lacked the concreteness required for implementation which would have been ideally built on the hydrological information they had gathered however they continued to struggle to meaningfully connect science and policy the purpose for our collaborative modeling was jointly defined to help the organization visualize the interaction between land use and water decisions and their effects on groundwater flow and to use the understanding of these interactions and effects to explore policy and planning options that would support the sustainability of groundwater resources the goal was explicitly set as an exploratory rather than a predictive modeling process 4 methodology 4 1 model progression we designed a participatory modeling process that included four meetings with a group of 18 representative and diverse stakeholders including hydrologists engineers planners environmental advocates and ecologists the experience and expertise of the group varied with some having explicit training and experience in water resources management while others lacked that technical training the group was also composed of decision makers e g town administrators and elected officials professional planners and consultants and non technical stakeholders additionally the research team actively participated in the meetings as technical facilitators to support the use of models as exploratory devices to derive insights and understanding of the complexity of land and water interactions the principal investigator served as the lead facilitator co investigators participated in large group discussion and shared technical expertise and research assistants provided technical support gathered data and captured observational notes the overall process was designed to allow participants to progress from simple to more complicated agent based models and in parallel to allow the research team to tailor a generic model to fit the geographic context of this community table 1 and fig 2 so that participants could use them to develop and test policies meetings were convened every other week in june and july 2011 and lasted 2 h each at the start of each meeting the group would review the purpose of the collaborative modeling process and the role of modeling for exploration and collective learning followed by a summary of what had been learned until that point and an overview of the models and activities that were planned for the day participants expectations for each meeting were also discussed the rest of each meeting involved facilitated exploration with the corresponding models for that day in pairs or small groups around a shared laptop a large group discussion at the end of each meeting allowed for people to compare notes and collectively discuss findings and insights reflect on their experience and make suggestions for model modifications and policy scenarios for the research team to work on in between meetings meeting 1 involved an initial demonstration of agent based models exposing participants to two simple models in the netlogo library the fire model and the traffic basic model wilensky 1997a 1997b and a very simple stylized land use model that built on the some model rand et al 2002 model 1 in figs 2 and 3 this allowed participants to become familiarized with the netlogo software application and to begin exploring how localized interactions may lead to system wide effects the core definition of complexity from meeting 2 onwards the group worked with a coupled model of groundwater flow and land use and land cover change for exploration of behavioral and policy scenarios models 2 through 5 in figs 2 and 3 the departure point for this model progression was a stylized version of the wulum model zellner 2007 that was prepared ahead of time we had designed the progression of meetings so that participants data and feedback on the conceptual understanding of the problem could be gradually fed into each new model version by the research team in between meetings the research team would incorporate the data and feedback provided to build in the features and mechanisms discussed in the prior meeting conduct verification and sensitivity testing and try out the scenarios proposed by the stakeholders the model modifications new interface features and scenario results were presented at the start of the following meeting for collective inspection and discussion before engaging in systematic collaborative explorations in small groups as the models became more detailed and slower to run the researchers provided participants with scaled model versions with reduced granularity e g by aggregating agents and cells to speed up run times and enable in meeting exploration and discussion 4 2 data collection and analysis we sought to answer questions regarding how the modeling tools mediated the conversations towards developing a collective understanding of the complex problem shifting from a localized to a systems view of the problem and solutions innovating on policy and planning approaches that acknowledge that complexity and building legitimacy around the process and proposed solutions using these models we used a method adapted from the learning sciences milz 2015 2018 2019 radinsky et al 2017 to systematically identify patterns of talk in the meeting interactions that had not been obvious to us solely from our firsthand experience and to describe how participants used models to inform planning and policy discussions each meeting was audio video recorded using a tripod mounted high definition camcorder the camcorder was positioned to capture all group dialogue and interactions during the meetings in total we gathered approximately 500 min of video recorded data from four individual meetings recordings were reviewed and transcribed by a team of five research assistants finished transcripts included individual speaking turns as in your turn versus my turn to talk and were labeled according to the pseudonym of the speaker and intended audience pseudonyms were used to ensure the anonymity of the meeting participants a team of five research assistants coded the complete set of transcripts by applying one or more qualitative labels to describe the content of the statement speaking turn and how the statement functioned within the discussion miles et al 2013 we coded for two features of dialogue which we called moves and targets moves referred to the conversational function of each statement and included 1 observations 2 objections 3 agreements 4 requests for clarification 5 clarifications 6 requests for explanation and 7 and explanations targets referred to the subject matter of the dialogue and included 1 model 2 software 3 model software use 4 policy 5 participatory modeling process and 6 the real world see radinsky et al 2017 for details on this methodology including specific definitions of each coding construct each transcript was coded by two research assistants working independently construct validity was established through a process of convergence in which pairs of researchers coded a document separately then met to discuss discrepancies discrepancies were used to refine the definitions of the codes listed above until independent coding passes were over 90 matching for each code coded transcripts were visualized as timelines and frequency distributions fig 4 the frequency distributions allowed us to inspect the distribution of codes across each meeting and the timelines allowed us to visualize the flow of conversations within each meeting by looking for different patterns in the coded data we observed moments in the meetings where codes might have co occurred been absent or where the conversation shifted from one code or set of codes to another working up from the data we were able to observe the changes in how the participants worked together and with the models over time 5 findings meeting the goals of a model progression the ultimate goal of the model progression strategy was to support plan making where 1 the simple explanatory models would help stakeholders develop a deeper intuition for the important interactions processes and systemic feedback at play in their region 2 the gradual incorporation of detail into the model would help stakeholders link the models to their real world problem space and 3 the deeper understanding and closer relevance would help participants trust their collective capability to draw on their complex systems understanding to build novel robust and widely supported planning solutions we were particularly interested in how stakeholders understanding and attitudes changed as the model detail and sophistication was increased in the following subsections we describe the findings in each of the three dimensions outlined above overall we observed that stakeholders definition of the problem and the novelty concreteness richness and projected effectiveness of proposed solutions did in fact evolve with the use of increasingly detailed and sophisticated models but only up to a certain point that evolution appeared to peak with model 4 which was introduced in the third stakeholder workshop meeting 3 and declined in the final meeting meeting 4 with the more sophisticated and detailed model 5 despite the evidence of learning with direct implications for planning and policy innovation relative to the initial solutions they had proposed for their community the general tenor of the discussion in the last meeting was of uncertainty and distrust in the lessons they had learned through modeling expressed by some as concerns about the inaccuracy of the models these reactions had the effect of widespread disengagement we focus in this section on the evidence of various aspects of participants learning about the complex environmental problem they faced and how they used what they learned to innovate solutions that recognize and build on that complexity in the discussion we focus on implications for participatory modeling particularly to address the barriers encountered 5 1 supporting complex systems learning we expected that in the early stages of the modeling process participants would become comfortable working with the models and understanding concepts related to complex systems and the agent based modeling paradigm e g emergence thresholds feedbacks etc as the meetings progressed we expected the more sophisticated modeling tools would help the stakeholders connect their understanding about complexity and the tools in order to start planning together evidence from the coded dialogue from across the four meetings suggests that participants did comprehend the complex relationships that drive groundwater change in their area and that comprehension did improve as the models became more complicated and realistic however those gains were limited by the overwhelming detail of the final models in the progression and the multiple elements to manipulate on their interface the code frequencies fig 4 demonstrate that during the initial meeting participants focused their efforts on understanding netlogo and figuring out how to manipulate the model interface the models also demonstrated concepts of complex systems e g interaction effects feedback loops emergence etc for the participants as well as they became comfortable working with the models during the second half of meeting 2 their conversations shifted to include references that were more relevant to understanding complex environmental phenomena and applying them to groundwater issues in their community i e references to the world however the quantity and diversity of their references peaked in the third meeting before dissipating during the fourth and final meeting also their references were primarily concerned with the model and underlying data during the fourth meeting instead of focusing on planning and policy making in other words their growing comprehension and application of complex systems reasoning hit a wall as the models became overwhelmingly complicated for example when switching to model 2 the greatly simplified land use model of exurban development and ecological deterioration in meeting 2 participants became more aware of how suburban sprawl and environmental decline were emergent phenomena one participant stan noted how individual decisions lead to system wide effects in some early models households are trying to find ecological quality and in doing so they toasted the environment another participant quorra followed but couldn t quite explain the connections between individual behavior and system wide change stating that if she had been a statistician she might have been able to explain the results she believed her limitation was one of mathematical statistical expertise rather than of conceptualization of interactions and emergence this increasing awareness was a new feature to their deliberations previously most of the discussion focused on reporting at a low level i e without local to regional explanations because some of the important causal relationships in their problem space remained invisible to them leo could understand how a small local change could lead to system wide effects just a sense of how everything interacted together a sense for what magnitude of change such as a small change in an independent variable made a large difference in the results the more complicated models also allowed the group to develop causal connections between growth i e urbanization and groundwater consumption for instance stan used model 3 in meeting 2 to explore the implications of changing consumption rates he understood that they could delay groundwater deficit into the future by reducing household water consumption but the deficit would still happen if growth continued nicole agreed with the point of limiting consumption collectively they seemed to start converging on the idea that growth was a problem to the sustainability of groundwater supplies a notion that was not entirely new to them but that they could start to explain in causal terms in the third meeting we introduced participants to a new version of the urbanization and groundwater model that incorporated more of the community s characteristics model 4 figs 2 and 3 with more realistic features and a more complicated interface the concrete relationship between model 4 and their community allowed them to realize that the eastern portion of communities surrounding their area was connected to lake michigan water so that those communities decisions would not be affecting groundwater levels in their area however model 4 assumed that those communities were groundwater consumers which overestimated their impact on aquifer drawdown this consideration spurred a discussion about neighboring communities and the relationship between water users and their various water sources in other words the stakeholders adopted a broader geographic view of the problem that allowed them to characterize the regional interdependencies the more complicated and realistic model allowed them to understand and mentally simulate scenarios connecting the location of pumping the direction of groundwater flow and the emergent spatial impacts even without the computer simulation in front of them thus we observe how the participants learned about complex systems both through the use of demonstration models and by interacting with increasingly realistic models of their community they used concepts related to emergence and interaction effects estimated regional impacts by land use change to groundwater consumption and collectively developed conceptual models that allowed them to develop counterfactual explanations of groundwater impacts in the next section we describe how they applied this learning to testing policy ideas 5 2 policy innovation testing and relevance to the real world in addition to learning about complex systems we also expected the participants to play with the models to test current policy proposals and identify new ones most participants found this a challenge because relating the modeling to the real world and using it to inform planning decisions was not something that everyone in the group grasped intuitively we expected that increasing the representativeness of models in the progression would help address this challenge because the models would gradually become a more authentic representation of the participants community in fact one of the main participants critiques along the way was that the models were not an accurate enough portrayal of their community the coded dialogue demonstrates that the participants were able to use the models to develop and test policy proposals the code frequencies show that in the second half of the first meeting the middle of the second meeting and early in the third meeting the participants made frequent references to the world and policy during the same conversations as they were interacting with the models in front of them fig 4 however by the fourth and final meeting their references to the world and policy were infrequent and most of their attention was drawn back to the model in meeting 2 participants started to formulate and test hypotheses about how changes in different model parameters might lead to different development patterns participants became aware that the models were limited and did not reflect aspects of reality but they did not see the simplified model as a way to produce generalizable policy insights for instance we showed how a simple land use model like model 2 could illustrate how preferences of people for ecological quality may drive widespread ecological degradation suggesting that action promoting changes in preferences might alter this outcome this understanding could inform policy even when the model was not a detailed replication of the real world in spite of these illustrations one participant luke strongly opposed the use of such models because they were not a perfect representation of reality so any insights the group would generate from it were meaningless he shared that he was conducting a hydrological study that showed there were no groundwater problems in the region several others challenged his position however referring to their own experience and to cumulative effects in the system that his empirical study could not capture quorra asked luke why communities were involved in conservation practices if there was no problem using her personal knowledge of strategies in the real world supported by model results luke acknowledged that the uncertainty around groundwater flows makes conservation a reasonable practice kaitlin wrapped up this discussion by reminding him of cumulative effects over time due to population growth and excessive use especially during droughts allowing the group to reframe the problem for their region luke continued to challenge the model and its assumptions and thus limiting the discussion around complexity by using strongly dismissive language e g referring to the simulations as ridiculous the third meeting supported the richest discussion around policy after a first round of participant feedback was incorporated in the models fig 4 during the small group interaction pairs of participants were assigned to explore a particular set of policy scenarios with model 4 e g preserving recharge areas from development implementing growth management policies within and outside their area water conservation explorations with model 4 sparked the increasing awareness that despite their desire for independence interdependence with neighboring communities would prevent them from addressing groundwater supply if they only approached conservation and enhanced recharge within their boundaries and that regional coordination was needed the explorations also allowed participants to refine the idea of connecting to lake michigan water and to understand how downstream communities could not improve their situation even if they did switch to lake water this aspect was subsequently dropped from the policy discussion as a result of the insights derived from the modeling the realization that the causes of groundwater drawdown were outside their local control overwhelmed the participants they were quite discouraged that none of the policies they had proposed prior to the meetings worked in the way they had envisioned olivia they were all kind of hopeless where s the good news scenario nicole whatever we do here is not enough laura the things that matter most to us we don t have control over this hopelessness might be one explanation for the disengagement in the last meeting in that meeting the presentation of the most detailed model of the progression model 5 figs 2 and 3 allowed the group to further refine where enhanced recharge should be located whereas our model placed them randomly quorra pointed out that it would make more sense to locate them in places with high permeability or recharge rates i e she was using the models to gradually build concreteness towards an implementation plan for enhancing recharge olivia suggested the model implies that we re fine but not those around us kaitlin who was sitting with her elaborated that from the planner s perspective we re not just fine they affect us the conclusion was that actions they took locally would not have much of an effect regionally larry reminded the group that the drawdown impacts were likely overestimated because the municipal wells were drawing from an aquifer different from the ones private wells used showing an ability to extract the insight despite the lack of specific data the realization motivated the group to think further on what could be done to preserve the water resource emphasizing the need for regional coordination and pushing beyond the solutions that had been commonly discussed for example olivia do we create giant recharge areas that will assist the whole region i mean what are we learning from this laura i think we re learning that regional water management is essential for all of us to which most of the participants nodded in agreement olivia conceded and commented on the need to coordinate with their neighbors as the last meeting came to an end individual reflections showed that the concern for water sustainability had grown over the participatory modeling experience but the sense of uncertainty and helplessness took over resulting in a general dismissal of and disappointment in the modeling process and the lessons learned the modeling introduced uncertainty and confusion leading to hesitation about possible directions the conversation was more sparse and focused relatively more on making sense of the model and its outputs and less on the real world implications while the facilitated policy discussion failed to build on the lessons learned through their collaborative modeling fig 4 the group instead advocated for collecting more data to address the uncertainty in line with dennis et al 1984 the lack of accuracy weakened the role of exploration with simple models in what was a politically sensitive process 5 3 planning with models finally we observed how the participants used the models and their knowledge of complexity to consider the future impacts that growth and development might have on their groundwater supply and develop a robust set of solutions to ensure water sustainability the participants initial desire for autonomy and independence was gradually replaced by the recognition of interdependence with the communities around them and the need for coordination and collaboration technical solutions also took more concrete shape focusing on details like the location and magnitude of artificial groundwater recharge sites needed to counteract the effect of development and with a more nuanced understanding of spatial interactions of withdrawals relative to direction of groundwater flow however the limitations they experienced as they interacted with the more detailed models caused them to question their learning and how they could use it for planning the code frequencies show evidence that the participants used the models to talk about planning issues especially toward the end of the second meeting and at the beginning of the third meeting fig 4 during these periods we note that the participants were referring to the model the world and policy simultaneously by the fourth meeting their references to the model were segregated from their fleeting references to policy and world this reflects the challenges experienced in using model 5 the most detailed model for planning and policy making it is important to note that they raised these challenges even while recognizing that the models had revealed useful insights for addressing future groundwater shortages the concept that simple even if unrealistic models could be useful to generate policy insights was difficult for the group to assimilate in their work in the first meeting participants had a chance to discuss the various solutions to groundwater depletion that the water resources initiative was pursuing the group came together to define the focus for themselves as water quality quantity and conservation there was great concern for the relevance of the entire exercise to the specific conditions of their region particularly since the community had dedicated considerable efforts in collecting data to characterize their aquifer one of the participants stan proposed that if the model did not look like their community then it would not be as helpful while others insisted on the use of the hydrological data that had been collected this became a theme throughout the rest of the meetings which in retrospect led the process away from deriving generalizable planning insights from an understanding of interactions as was originally intended and discussed when participants first used model 2 a simple urbanization model in the second meeting figs 2 and 3 they understood how location preferences could drive urban and environmental patterns despite realizing the importance of land use preferences most of their planning focus remained on water conservation and on physical aspects of the landscape a top down perspective rather than on the behavioral emergent urbanization patterns and their indirect effects on water supply nevertheless the group quickly realized that they only had power over their own community and not over their neighbors the question started to shift towards recharge within their community and to what extent this could compensate for the drawdowns from surrounding communities the group continued to make progress through the third meeting working in small groups leo and larry effectively played with model 4 to test questions about interactions doing systematic parameter sweeps and trying to make sense of the outputs quorra struggled but started to shift from clarification questions to explanation of the emerging patterns although quickly shifting to hopelessness we re already hosed the hopelessness then turned into frustration with the models and the modeling process and disengagement with the work in progress nature of exploratory modeling the research team attempted to address the frustration by incorporating as much feedback and data as possible before the last meeting participants were then forced to make sense of the complexity of interactions and to comprehend model 5 with the highest degree of representativeness they had seen so far participants who struggled with model 5 had trouble designing and making sense of policy scenarios that were meaningful to them overall there was much more reference to what participants were seeing on their screen suggesting that the new detail made it more concrete but also so complicated that they could not generalize beyond the instance in front of them while the first three meetings resulted in rich discussions insights and novel policy propositions in the fourth meeting participants were noticeably less vocal fig 4 and the tenor of the conversation was more around hopelessness and uncertainty about what to plan for followed by questions about the validity of the experience and the request for more data to reduce the uncertainty rather than looking for robust approaches to the problem e g reduce consumption levels whether wells were shallow or deep the group hesitated to commit to next steps of action based on what they had learned and focused the discussion on the additional data needed to make decisions this position however left the group powerless they were not aware of how the detailed data had contributed to making the modeling intractable to them for example quorra and kaitlin commented on how long it took to play with the model both to make sense of it and to run it quorra believed that adding data would make the model better however she did not see the relationship between increasing the level of complication and the slowing down of both sense making and run times the more detail that was added the better the model got i still do not think there were enough factors in there to truly get a representation the community had already dedicated significant resources to gather data about the aquifers but water levels were harder to come by did this mean that they could not make decisions how much data is necessary to make decisions how much uncertainty is tolerable the end result was significant hesitancy and disorientation while we were hoping for more concreteness sense of direction and empowerment during the final reflection several participants expressed that the ultimate test for the model s usefulness for planning was how close it was to reality olivia in particular stressed that if it was not accurate then it is of no value the models were useless unless they were really telling her what the story is stan and quorra showed more willingness to reason with the models about the problem the former because of his level of comfort with modeling the latter because of her curiosity despite the struggles with the model 6 discussion barriers to participatory modeling for environmental planning based on our evidence we argue that the increased level of detail hampered the effective use of modeling within the participatory modeling process goldstone and wilensky 2008 the progression of models enabled a rich discussion about the complex problem the communities were facing and of the implications for policy and the real world but the increasing complication and representativeness became an impediment to productive discussions and appropriation of the insights after meeting 3 other aspects of the meetings such as the instructional setting the power relationships among the participants hoch et al 2015 and a culture that favors outsourcing sophisticated predictive modeling to external consultants zellner 2008 likely contributed to this breakdown as well we suspect however that simpler models might have helped overcome these obstacles we expand on the barriers encountered and their possible causes below 6 1 exploration versus prediction supporting playfulness and collaborative ownership there is an inbuilt assumption among many scientists and decision makers that predictability establishes validity zellner 2008 participants in our case wanted to establish faith in the results at the end of our progression through a predictive model that used their data while we clarified expectations early on and throughout the process several participants still expected predictability kaitlin for example wanted to be able to make sense of the data to inform decision making and was interested in new ways of modeling for that purpose however her expectations were not met truthfully i expected to learn something different than i learned and it was a challenge to scale down my expectations i didn t learn what i wanted to learn but i learned some other things she also discussed the unmet expectations of other participants and saw her role as helping others be satisfied with the experience and understand that the modeling was exploratory more than predictive people came in with expectations of a predictive model i know people were frustrated and talked to me about it in between sessions i wanted very much for us all to give you all an open mind and to bring in other information about other studies that have been done etc about the area she expressed how participants didn t know how to make sense of the outputs if they weren t predictions that is what people are used to seeing and used to getting if that wasn t where we were going what are we getting out of it larry was among those who agreed it s not a predictive model so that s a limitation models are used for various things but primarily the objective is to be predictive if a model isn t equipped to be predictive then that s a limitation that people are gonna perceive as being significant you have to first go back to the original intention of the model if they couldn t use a model for predictive purposes then they wouldn t build it the model you built can bring attention to various things but it would be a whole lot better if it had what it needed to be predictive in contrast quorra came into the meetings without the modeling expertise and expectations others had i guess i went into this without really any particular expectations just more as an educational process my first expectation is education and understanding the idea that we re gonna have a model for our community to follow and policies from that model would be way too much to expect perhaps this expectation is what allowed her to play with the models despite the difficulty that complicated interfaces and outputs posed this difficulty negatively affected her perception of the usefulness of the tools after the modeling experience i do not think i ever got an idea of just what this model can do i am not convinced the model is useful while quorra believed in participatory planning it is always better she also believed that modeling is too complicated and that it should be outsourced to consultants she could not see the benefit of engaging in the process of modeling as she was more used to studying outputs produced by modeling professionals quorra had effectively built on the modeling experience to sharpen her ideas for solutions which were taken up by the whole group but her sense of uncertainty prevented her from feeling empowered by the process instead suggesting outsourcing the modeling to professionals 6 2 prosthetics for planning versus cognitive overload the power of simplifications and examination participants tried to make sense of the detailed model s features and outputs but expressed considerable difficulty in doing so the models complication significantly increased the cognitive load required to understand indirect effects and tradeoffs which made it increasingly difficult to keep track of their progress or lack thereof towards sustainability goals with the ensuing consequences described above uncertainty and frustration demanding for more accuracy to increase their confidence in a specific policy approach arciniegas et al 2013 dennis et al 1984 inman et al 2011 sun et al 2016 kaitlin did concede that the simpler less realistic models were clearer because simulations with the more realistic models ended up in the same place so you couldn t see what difference each slider made and that was discouraging i do not think it the model has the value in its current form to do the kind of work that we want it to do the difficulty of making sense of more complicated models led to perceptions that the problem was the simplification t he model is grossly oversimplified we shouldn t be drawing these conclusions w e don t have enough to make an educated policy decision i would not want to establish a policy unless the data really supported it we need to have compelling scientific evidence in order to convince people of needed actions throughout the process we had demonstrated how simplifications and assumptions are a necessary part of complex systems modeling and testing such assumptions e g with empirical or hypothetical data can help us explore the problem space understand simulation outputs and innovate and test solutions accordingly set up in this way such models become prosthetic devices for thinking and planning with complexity hoch et al 2015 when prompted participants didn t seek to test alternative assumptions to see how much difference they would make the modeling progression that the group had worked through was not enough to show how one can handle such erroneous simplifications with systematic testing instead it fed the notion that just one mistake in parameterization could render an entire modeling process meaningless 6 3 instruction versus collaboration ensuring productive and equitable knowledge co production in each engagement the ability of the group to use the models to collectively reflect on the world improved with each version of the progression but we observed discrepancies in how participants transferred what they learned from the modeling to the real world power struggles emerged around the legitimacy of knowledge and solutions with the modeling effort at its core the differences in expertise contributed to power imbalances that impacted group deliberations as the views of the decision makers and disciplinary experts were often given more weight by the other members of the group participants often resolved differences in understanding and power imbalances among themselves sometimes using the modeling to support their claims hoch et al 2015 the instructional setting we had designed had the unintended effect of pitting the researchers expertise against the participants making it hard to support an enriching collaboration instead some participants contributed some insights about this setting olivia i felt that there were a lot of assumptions ahead of time that included the research team s assumptions made more than the practitioners did practitioners know more about day to day reality of water and users very cursory assumption i recognize that real people sometimes get disconnected from data felt it was education based not real life the model assumptions were made explicit so that participants could modify them to increase the correspondence to their community olivia had a hard time translating her expertise into model assumptions however and thus engaging in the collaborative exercise an excerpt from meeting 1 illustrates this tension moira if you want to just use the map and point to things like the bigger map olivia well the map doesn t really say where the recharge areas are moira right but do you have an idea of where they are olivia yes moira okay places the map close to the group olivia do i have to moira no you don t have to olivia i m just kidding the recharge areas were never drawn on the map stan commented that the meeting structure was limited that we either needed more meetings or needed to arrive to the point sooner the series of meetings had been designed in sets of four to support a gradual uptake of participatory agent based modeling that they could own and expand on beyond the fourth meeting such a setup did not provide the closure that participants needed instead ending abruptly for them a lot had yet to be resolved or understood as the last meeting came to an end the sense of uncertainty and helplessness took over leading to a general disappointment in the modeling experience a lack of excitement about the lessons they had collectively learned and the useful planning implications that their explorations had revealed and disinterest in pursuing further collaborative modeling 7 recommendations for participatory modeling in environmental planning our model progression strategy supported planning of complex sustainability problems the stakeholders used the modeling progression to comprehend complex social and ecological systems to test various policy options some of which had been previously considered and some of which were new to the group participants could discuss both in meetings and after meetings about how local land use and water use decisions could lead to widespread depletion in their community they also used the models to rethink their strategies for addressing potential future water shortages and the interdependencies that drove water consumption in the larger region they could also see how they could not isolate themselves from surrounding communities and they also advanced in their design of more concrete approaches to enhancing groundwater recharge in other words they modeled learned and planned together the process was hampered by the lack of confidence in their insights as the models became more detailed for the reasons described above we focus here on two aspects that may better support participatory modeling in such situations slowing down model complication and designing facilitation strategies that ensure that meaningful insights can be derived in each meeting while there is support in the learning sciences literature for the use of models with different representations of detail the cognitive load associated with managing multiple interactions and tradeoffs is taxing for participants models need to support a basic understanding of interactions extract main points and focus further modeling on those points e g recharge access to lake michigan water simpler models help participants overcome the difficulty of using models for systematic exploration difficult decisions especially those involving many interactions and tradeoffs trigger the need for accuracy as cognitive load increases arciniegas et al 2013 dennis et al 1984 inman et al 2011 the degree of model complication triggered this need in our case prompting the request for accuracy and unwittingly contributing to decision stress what our participants thought they wanted certainty and what the research team attempted to deliver trust took us further away from where we could collectively learn about complexity and design robust planning and policy approaches to address it other experiences have had similar outcomes schmitt olabisi et al 2010 such overemphasis on accuracy is not helpful simple models are useful to have meaningful conversations around more sophisticated models dennis et al 1984 further work is thus needed to study the ways in which a modeling progression can help use the basic understanding gained with simpler models to support appropriate inspection of and meaningful simulations with more detailed models even though we had a good justification for our model progression past a certain point the representativeness of the model became unhelpful in contrast with other studies we found that this had less to do with whether the users could make appropriate inferences about what was happening within the system our data shows people in fact could do this but rather with how people s cultural expectations shaped their ability to trust the inferences they were making the models allowed the group to test solutions fail and modify their ideas their expectation was that it would be easy and obvious to show how their favored solutions would work but it turned out to be less so especially with the most detailed models they expected that clarity would emerge when the model incorporated all the data they had given us the models became too complicated for them to understand why they kept failing and the collaboration became a tense negotiation between participants and researchers and frustration with the exploratory nature of the modeling hoch et al 2015 interfaces model structures and facilitation structures need to be tailored to different kinds of user groups to support proper interpretation of modeling outputs especially when they are dissonant with expectations pettit et al 2011 stakeholders needed more opportunities for synthesis and reflection after a carefully designed sequence of tasks of increasing difficulty white and pea 2011 this is as much about model progressions as it is about meeting design and facilitation the four meeting sequence gave a sense of anticipation for the end where a big realization was supposed to happen but they did not get to a good news scenario that they could hold on to with hope e g antle et al 2014 poplin 2012 participants needed extra support from facilitators to productively explore with the models they helped develop keep track of changing variables of interest and reflect on how to make these variables change in more desirable ways zellner et al 2020 the instructional approach also had a profound impact on participants expectations and their disappointment the conceptual culture in planning is learning about complexity through precedent what works elsewhere rather than through models that can help develop explanations for what could work here the culture of model use is often linked to external consultants who deliver outputs for decision makers as consumers rather than co creators of an exploratory modeling process popular planning models tend to be easier to use and more general in application but there is still a barrier from modeler to planning user triantakonstantis and mountrakis 2012 the key bottlenecks are related to lack of transparency of model assumptions and inner workings low communication value with planners politicians and stakeholders and difficulty of tool interaction for exploration tebrömmelstroet 2010 will et al 2021 stakeholders must be engaged in product development rather than be mere consumers pettit et al 2011 for this to happen modeling skills must be taught to a broader audience so that they can become aware of its benefits will et al 2021 our attempts to address these recommendations were not enough however while the modeling progression supported learning and solution building the simplifications needed to be led by the participants as we responded to their demands for accuracy we were making at times too many modeling implementations for them their learning increased but their trust in the models decreased as our simplifications and variables were foreign to them we could have instead negotiated the simplifying assumptions collectively rather than instructing them on how it could be done a facilitated discussion with a skilled community facilitator as proposed by e g hovmand 2013 lane et al 2011 schmitt olabisi et al 2010 would have been helpful in ways we hadn t thought of supporting imagination and projection quantification strengthening relationships defusing conflict staying engaged reducing entrenched trust or distrust and avoiding dualistic approaches i e co producing knowledge through activity in a cognitive apprenticeship brown et al 1989 models are embedded in culture and thus should be learned and developed within that culture this is precisely the reason why models should be built within the authentic planning activity and not extraneous to it so an instructional setting where the models are taught to participants was not conducive to building a robust shared knowledge the rich experience gained in this trial allowed us to refine our participatory modeling approach for environmental planning first models need to be kept simple to work better in supporting complex systems learning and translation into policy and planning this keeps the cognitive load at a manageable level while allowing models to run fast within a meeting in this way stakeholders can quickly examine a range of scenarios and gain a fuller understanding of the problem and the multi dimensional and stochastic effects of proposed solutions second rather than a sequence of instructional sessions we contend that an open ended series of meetings each one of them self contained and centered on interactive exploratory activities both among participants including researchers and between participants and the modeling tools is more conducive to engaged exploration the self contained design ensures that there is at least one aspect that participants can take away from each meeting such lessons could also be structured around game like tasks e g friendly competition among participants the open ended structure motivates participants to join in ongoing endeavors as they raise the need for further modifications and participate more directly in the simplifications as more complication is reached and the cognitive load increases stakeholders need to be engaged in determining what factors and mechanisms to leave out in favor of new ones that they deem necessary to include third participants must be supported in ways that help them systematically explore the problem and solution space keep track of how outputs vary in each scenario and assess these outcomes against the collective preference for specific tradeoffs initial work on tangible and mobile interfaces and facilitated learning structures developed for this purpose shows promising results zellner et al 2020 finally in addition to expert modelers and modeling facilitators community facilitators ideally among the group of stakeholders are critical in enhancing communication and collaboration among all participants community members and researchers both within and in between meetings hovmand 2013 lane et al 2011 participatory modeling is useful for environmental planning in representing diverse knowledge inspiring insightful conversations fostering strong and equitable relationships and supporting transformative solution building and agency it requires however significant time and resource commitment to develop and refine its guidelines for practice ongoing work must focus as much on the collaborative model development and social learning process as on the cultural social and institutional context in which this process is embedded hedelin et al 2021 here we showed how and to what extent a model progression could work for this purpose perhaps more importantly we shared how and under what conditions this progression stopped being helpful identifying the barriers to address and future steps needed to support a more widespread adoption of participatory modeling as an established planning practice finding finding the balance between simplicity and realism in participatory modeling for environmental planning declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work would not have been possible without the support of several research assistants over the years kelsey pudlock jennifer weizeorick miller april schnider lisa domoracki and priscilla jimenez we would also like to thank dean massey for invaluable editorial support the research was funded by the department of urban planning and policy the great cities institute the institute for public and civic engagement and the chancellor discovery fund at the university of illinois at chicago the national science foundation oci program 1135572 and drl reese program 1020065 and the northeastern university college of social science and humanities we are grateful to the stakeholders who participated in this project and to three anonymous reviewers appendix a supplementary data the following is the supplementary data to this article figs1 figs1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105481 
25507,complex systems simulations can support collaborative water planning by allowing stakeholders to jointly see hidden effects of land and water use decisions on groundwater flow we adopted a participatory modeling progression where stakeholders learned to modify and use increasingly sophisticated models to assess policy impacts on groundwater levels stakeholders shared understanding of the problem and the novelty concreteness and richness of proposed solutions evolved alongside the models degree of realism but up to a certain point more realistic models became a distraction and stymied efforts to plan for water shortages the reflective learning required to plan for complex environmental problems is best supported by models that strike a balance between representational fidelity and end user intelligibility complicated models and high resolution data may overwhelm model users preventing them from acting on the useful planning insights they derived from the exploratory modeling particularly within social contexts that exhibit strong power dynamics and favor prediction keywords participatory modeling complex systems modeling environmental planning computer supported collaborative learning data availability the data that has been used is confidential 1 introduction the role of models in supporting participatory environmental planning has been studied extensively in natural resource management and in planning support systems literature see geertman and stillwell 2020 and will et al 2021 zellner et al 2012 for reviews most of these studies describe cases in which a particular model was developed or adapted to inform collective management and planning decisions assessment of the use of such models emphasize user satisfaction as a key for a successful participatory activity pelzer 2017 russo et al 2018 te brömmelstroet 2017 fewer studies however conduct a systematic assessment of how these models support collaborative decision making goodspeed 2016b jones et al 2009 milz 2019 in other words how they inform and improve the judgments of participants about the specific problem they are seeking to address and how they address it arciniegas et al 2013 gray et al 2018 hedelin et al 2021 inman et al 2011 jordan et al 2019 milz 2019 salter et al 2009 it is often assumed that non expert stakeholders have more confidence in expert crafted predictive models and that their confidence is linked to how accurately models represent features in the system and the resolution of the spatial and or temporal data on which it runs goodspeed 2016a however it is not clear that more complicated and detailed models of complex systems can improve the quality and impact of environmental planning and policy making in practice hoch et al 2015 zellner et al 2012 complicated detailed models are expensive and time consuming to create tune and validate such labor intensive tools are effective and useful to advance scientific knowledge but may impede effective group deliberations stakeholders end up either relying on expert interpretations or dedicating the bulk of their energy to learning how the model works klosterman 2013 thus they are never able to fully integrate insights gained from the models into the planning process in response some researchers have built models that rely on simple abstractions for the purposes of enabling stakeholder learning about the underlying dynamics of a complex problem and supporting better decision making rather than producing accurate predictions lempert 2019 in this way simple models may help planners and stakeholders hone their intuitive sense of how a system would respond to different plans and policies helping them engage in more robust planning while these modeling tools tend to be more user friendly than sophisticated models used for scientific research and prediction they typically lack the concreteness of real world details of the problems faced by stakeholders in their own specific communities e g communities of neighborhood residents farmers etc thus stakeholders have to learn for themselves how to translate insights from the simple models to the messy dynamics of their real problems it is not always clear that what stakeholders learn is applicable to their needs and adequate for their issues therefore they may lack confidence in the solutions that emerge from using such overly simplistic abstractions to interrogate the tension between simple versus realistic models in informing environmental planning we combined the advantages of each via a theory grounded approach to structuring planning sessions that we refer to as a participatory model progression in this article we explore how a progression of agent based models that started with stylized models of land use and groundwater interactions leading to more detailed and geographically relevant ones supported collective deliberation and planning about groundwater depletion in a community in northeast illinois usa we designed our approach and modeling tools in line with theories drawn from the learning sciences goldstone and son 2005 goldstone and wilensky 2008 and prior planning studies that employed models with different degrees of detail see zellner et al 2012 for examples we hypothesized that the gradual incorporation of more detailed information and higher levels of geographic correspondence between the model and the stakeholders community would enhance their understanding of interactions between land use and water use decisions and their effects on groundwater flow in other words stakeholder learning and deliberative quality would scale with the detail of the model we also hoped over time to see stakeholders develop trust in the insights they derived from using the models a trust based not on expert endorsement but instead on their own growing collective understanding of systems here we present the approach we developed and the analysis of the interactions and policy discussions that ensued identifying the aspects that supported and hampered learning and innovation analysis of meeting dialogue demonstrates that stakeholder learning improved as the models became more sophisticated up to a point we find that a model that balanced simplicity and representational accuracy effectively supported deliberative conversations about groundwater policy and practices however when presented with a detailed realistic model of their community participants shifted from discussing the problem to talking about the tool as predicted by lee 1973 and progress toward concrete policy deliberation was lost in the next section we provide an overview of the literature covering the tension between simple and complicated modeling for natural resource management and environmental planning and propose a framework for understanding this tension we follow with a description of our case and methodology for participatory modeling and for assessment of the activities we discuss our findings on how participants learned from the experience with different kinds of models followed by a discussion of the barriers encountered and conclude with implications for participatory modeling to support environmental planning particularly in light of the growing proliferation of gaming and data visualization tools promoted for this purpose 2 background 2 1 simple versus complicated models for planning and policy making often overlooked in lee s 1973 famous critique of the comprehensive urban simulations of the mid 20th century is the tension modelers face between the size and complexity of the models and the type and amount of knowledge gained from using them for lee the knowledge gained about policy and urban structure peaked with the development and use of small simple models and diminished rapidly as the models increased in size and complexity see lee 1973 p 173 even moderately complicated models led to rapidly diminishing returns in terms of useful knowledge for planning shifting to knowledge about the model instead this tension and its implications for planning and policy making has been discussed elsewhere in the literature e g will et al 2021 with scholars often advocating for the collective use of simple models with novice users e g bankes 1993 zellner 2008 here we sharpen and codify this distinction into a framework for exploring the utility of models for policy and planning deliberation represented in fig 1 our framework includes two dimensions model sophistication and model representativeness model sophistication refers to the relative simplicity or complication of a model as it appears to the novice user models can be characterized by where they fall on a spectrum between complicated and simple complicated models include a large number of mechanistic features that can be set by the model user more simplistic models reduce or eliminate the number of features that can be adjusted by a user model representativeness on the other hand refers to the degree to which the model replicates reality models can be characterized by where they fall on the spectrum between stylized i e low representativeness and detailed i e higher representativeness thus this framework contains four broad classes of models i toy models that are stylized and simple ii complicated theoretical models that are highly stylized they may not be geographically or demographically relevant but include all the elements of the dynamic interactions iii data dependent models that represent relationships in a simple way masking complexity from the end user e g regression models machine learning approaches and iv expert models that are both complicated and realistic our study described in detail in the methodology section was intentionally designed within this framework to follow a progression from toy i to expert iv models expert and data intensive models tend to be favored in the field yet are unwieldy to use and expensive to build dennis et al 1984 and the large number of parameters makes calibration difficult due to long run times and increased instability doherty and christensen 2011 a toy model can run faster and fit observations with fewer parameters reducing the risk of overfitting brown et al 2005 the tradeoff is that expert knowledge e g field observations and measurements complex and cross scale interactions does not readily map into the simple model s parameters and mechanisms sun et al 2016 this problem only becomes important if the simple model misses critical drivers of the system or if it underestimates the uncertainty present in the model simplified versions however could be designed as stepping stones towards understanding the more complicated expert models edmonds et al 2019 smaldino 2017 and because they are less expensive to build and learn from they can be used to rapidly test scenarios to highlight relevant issues and policy tradeoffs combining the use of both complicated and simple models with a well defined association between the two can establish credibility in the simpler model and support relevant and meaningful exchange of ideas and provide more robust support to decision making than either one alone while the lack of accuracy of simpler models could weaken their role in politically sensitive decisions it could also make it easier to rely on the more complicated models when higher levels of accuracy are required in other words different levels of detail are necessary for robust decision making but over emphasis on detail and accuracy may be misplaced often models are critiqued based on how faithfully users believe they represent reality more realistic models may be more influential for planning and policy making however a model deserves criticism only when it fails to achieve quantification of uncertainty and maximum reduction of uncertainty through optimal processing of environmental data picture perfect rarely equates to optimality in either respect doherty 2011 p 455 representational accuracy for example may become a distraction and a barrier to identifying key interactions salter et al 2009 because the cognitive effort required to understand these interactions and their effects varies with the amount level of detail and format of presented information arciniegas et al 2013 more qualitative and simple models tend to allow for more exploration and innovation in solutions because the simplicity of the model facilitates its active use for brainstorming and collective problem solving arciniegas et al 2013 le page and perrotton 2017 smaldino 2017 twiston davies et al 2021 will et al 2021 goldstone and son 2005 found that juxtaposing a visually detailed and a visually simple model worked better than using either alone following a process of concreteness fading from the former to the latter to identify most salient features le page and perrotton 2017 propose a kilt keep it a learning tool approach to strike a balance between abstract theoretical simple models and empirical descriptive realistic models neither of which are adequate to support social learning and collaboration among stakeholders addressing complex problems nevertheless there is a cultural bias against simpler and more abstract models that do not reproduce the reality as stakeholders perceive it doherty 2011 models of different levels of detail may also be more or less useful given the context after all it is not just the model that affects collective decision making but also the social context and purpose for which it is used edmonds et al 2019 grimm et al 2020 sun et al 2016 zellner et al 2012 for example online models for public use one potential way to engage more public participation in planning may need to use simpler models and adopt additional strategies like a game like structure to encourage and sustain participation among a broader population of stakeholders poplin 2012 while expert users may question the realism and the visibility of the calculations of a game making the game more realistic would defeat the purpose of lowering the barrier to public participation moreover more detailed models tend to perform worse and be valued less than more stylized and qualitative approaches due to increased decision stress i e the aspects of decision making that make it difficult e g the amount of effort entailed or the confidence in the decision and the consequent perception of decision inaccuracy inman et al 2011 on the contrary simplicity facilitates a more intense and stimulating use of models which correlates with users effective use of information ability to explore and innovate solutions and the quality of overall outcome inman et al 2011 the act of deciding which parameters are included and which are excluded in a model the act of building a model itself is fundamental to coming to understand a modeled system thus participatory modeling approaches often recommend that models be co constructed with participants gray et al 2018 sterling et al 2019 a demonstration of the power of model co construction can be found in lane et al 2011 who studied an approach to flood risk management where models knowledge and policy recommendations were co produced in a parallel distributed and dynamic process involving anyone concerned with an issue either through their profession their interest or their experience simpler and less expensive and data intensive models were coded by the scientists and trusted by the locals because they were more consistent with local data and collective knowledge collective engagement from conceptual development to critical assessment of the tool rather than model sophistication and detail gave the group a sense of competence to intervene in a situation where negotiations had stalled and engage in practice of a decision that broke the status quo lane et al 2011 2 2 toward a more effective practice of participatory modeling within a policy context the value of complicated models lies squarely in their ability to predict outcomes and often win the trust of planning participants whose beliefs align with these outcomes whereas the value of simpler models seems to lie in their explanatory power non experts merely consume or reject the results of complicated models but can engage much more richly with simpler models through for example not only scenario exploration where the focus is on the model interface but also model co construction where the focus is on model structure and function building a deeper understanding of salient variables and how these variables affect others in other words they can begin to understand the system that drives the outcomes which can translate into reasoning more flexibly while planning collectively a skill that is direly needed to support community adaptation to complex problems zellner 2008 one major open question though is how to effectively build models with the appropriate level of detail and realism within environmental planning processes defining the right level might differ across contexts so how can we generalize the process by which we find this level how do we address the widespread bias against simpler and stylized models we have little evidence for what might constitute effective collaborative model building strategies with stakeholders for obvious logistical reasons few existing studies of model sophistication have been able to follow through and examine the impact of model use on the actual community s plans and actions lane et al 2011 did not set out to study this aspect per se but they were able to show that engaging non experts in the model construction process led to a shared deep understanding of the planning challenges and in turn to real change in the community to get beyond a collection of empirical examples of how models could be used in planning and towards shared guidelines for effective participatory modeling in planning the practice of collaborative model building should be grounded in theory there is an abundance of theoretically grounded research in the domain of the learning sciences that studies how models can be used to support knowledge building and reasoning model construction is bolstered by the pedagogical theory of constructionism which claims that both motivation and knowledge are enhanced when learners get a chance to construct artifacts that express their knowledge papert 1980 resnick and kafai 1996 while constructionism can be used as a general pedagogy e g to guide the design of programming environments for children and youth maloney et al 2010 there is rich empirical evidence demonstrating how building simulation models can help people learn about complicated real world systems e g barab et al 2000 blikstein and wilensky 2009 jonassen et al 2005 klopfer 2003 klopfer et al 2005 louca and zacharia 2012 smetana and bell 2012 stieff and wilensky 2003 wilensky and reisman 1999 wilensky and resnick 1999 although there can be challenges learners often need scaffolding to engage in the modeling process sins et al 2005 moreover research in learning sciences supports observations that increased realism and detail in models and representations can distract from core system features goldstone and wilensky 2008 note that realism in models is sometimes disadvantageous p 507 and instead recommend that non essential aspects of a complex system be removed or idealized to allow learners to focus their attention on the core phenomena that underpin the system s emergent properties based on the above literature and our prior work zellner 2008 zellner et al 2012 we study how a constructionist strategy of model progression might help communities address complex environmental planning problems fig 2 participants with diverse expertise would begin with a very simplified model of core aspects of the environmental problem i e a toy model and then co construct with the researchers more detailed models that would include their specialized local knowledge towards the center of fig 2 doherty 2011 lane et al 2011 le page and perrotton 2017 the simpler models should capture the relevant system features and serve as appropriate mirrors of more complicated and focused models dennis et al 1984 doherty 2011 goldstone and wilensky 2008 the use of such combined models must be sensitive to the social context zellner et al 2012 and attempt to remove barriers to entry poplin 2012 to prevent users from perceiving the model to be difficult to use and in turn affecting their use of the model arciniegas et al 2013 inman et al 2011 salter et al 2009 while our progression is inverse to the concreteness fading suggested by goldstone and son 2005 and sun et al 2016 our participants were not learners in the traditional sense but professionals and community representatives who are experts on the complex problem being addressed and whose goal was to jointly plan for it in this context we wanted participants to help us co define the elements in the more detailed model versions so that it would better reflect their concerns hopefully by exposing participants to the construction process building on a simple model to develop a more detailed and representative one we could support participants understanding of the iterative and simplifying nature of model development grimm and railsback 2012 and how to appropriately use modeling to gain policy relevant insights bankes 1993 zellner 2008 zellner and campbell 2015 we seek to contribute to planning and participatory modeling literature with an evaluation of this theory grounded progression strategy of increasing model detail within planning meetings by systematically evaluating the ways in which different types of models support deliberation toward policy innovation we can begin to devise guidelines for how modeling should be conducted within such collaborative planning efforts as much as the models are important so is the facilitation and participation structure we seek to study how simple models can work if built as part of a collective endeavor where trust can grow in the group s learning and decision making competence as well as in the modeling tools that emerge from the collective generation of knowledge most planning research assesses outcomes of the modeling and planning process by using metrics like participant satisfaction bryson et al 2013 deyle and slotterback 2009 uddin and alam 2021 which can be misleading users may express satisfaction with models that are visually appealing and easy to manipulate for example yet are a poor or obscure representation of the underlying socio ecological complexity preventing the necessary learning and reasoning that can increase decision quality we focus here on documenting and understanding the process of collaborative model building and use in planning 3 the case a us supreme court decree wisconsin v illinois 1967 and the great lakes compact s j res 45 2008 limits the amount of water communities in northeastern illinois can withdraw from lake michigan annin 2006 many must rely on aquifers for their water supplies as a result and decades of unsustainable growth and development have depleted their groundwater supplies we partnered with an organization representing several exurban communities in northeast illinois usa that have all been experiencing water supply threats the organization s mission is to promote regional cooperation and to develop and implement a comprehensive plan for development and conservation having long recognized the importance of water resources they created a formal initiative in 2001 to study their resources and promote deliberation on how to best use and sustain their water supplies we learned through our early conversations that their concerns focused on the effects of droughts and unconstrained urbanization occurring around them exogenous development was drawing down the aquifers beneath these communities and droughts are reducing aquifer recharge moreover real estate developers had been pressuring these communities to grow which they had resisted their efforts eroded the trust with neighboring communities developers the court system and external experts who might have undermined their efforts to protect their water supply with models that do not adequately represent the area s natural and socio economic environment or the communities concerns as a result the organization was keen to protect their groundwater resource from neighboring communities hoping to isolate themselves from the effect of surrounding development in 2011 we worked with their committee for water resources as they were working on a formal groundwater plan they had been working for 11 years to develop a data grounded depiction of their groundwater resource but were unsure on how to make sense of it for planning nevertheless they had already settled on several policy recommendations including development restrictions groundwater recharge protection areas data and monitoring standards a public education program water pricing conservation design injection wells and other water conservation practices while providing policy guidelines this list lacked the concreteness required for implementation which would have been ideally built on the hydrological information they had gathered however they continued to struggle to meaningfully connect science and policy the purpose for our collaborative modeling was jointly defined to help the organization visualize the interaction between land use and water decisions and their effects on groundwater flow and to use the understanding of these interactions and effects to explore policy and planning options that would support the sustainability of groundwater resources the goal was explicitly set as an exploratory rather than a predictive modeling process 4 methodology 4 1 model progression we designed a participatory modeling process that included four meetings with a group of 18 representative and diverse stakeholders including hydrologists engineers planners environmental advocates and ecologists the experience and expertise of the group varied with some having explicit training and experience in water resources management while others lacked that technical training the group was also composed of decision makers e g town administrators and elected officials professional planners and consultants and non technical stakeholders additionally the research team actively participated in the meetings as technical facilitators to support the use of models as exploratory devices to derive insights and understanding of the complexity of land and water interactions the principal investigator served as the lead facilitator co investigators participated in large group discussion and shared technical expertise and research assistants provided technical support gathered data and captured observational notes the overall process was designed to allow participants to progress from simple to more complicated agent based models and in parallel to allow the research team to tailor a generic model to fit the geographic context of this community table 1 and fig 2 so that participants could use them to develop and test policies meetings were convened every other week in june and july 2011 and lasted 2 h each at the start of each meeting the group would review the purpose of the collaborative modeling process and the role of modeling for exploration and collective learning followed by a summary of what had been learned until that point and an overview of the models and activities that were planned for the day participants expectations for each meeting were also discussed the rest of each meeting involved facilitated exploration with the corresponding models for that day in pairs or small groups around a shared laptop a large group discussion at the end of each meeting allowed for people to compare notes and collectively discuss findings and insights reflect on their experience and make suggestions for model modifications and policy scenarios for the research team to work on in between meetings meeting 1 involved an initial demonstration of agent based models exposing participants to two simple models in the netlogo library the fire model and the traffic basic model wilensky 1997a 1997b and a very simple stylized land use model that built on the some model rand et al 2002 model 1 in figs 2 and 3 this allowed participants to become familiarized with the netlogo software application and to begin exploring how localized interactions may lead to system wide effects the core definition of complexity from meeting 2 onwards the group worked with a coupled model of groundwater flow and land use and land cover change for exploration of behavioral and policy scenarios models 2 through 5 in figs 2 and 3 the departure point for this model progression was a stylized version of the wulum model zellner 2007 that was prepared ahead of time we had designed the progression of meetings so that participants data and feedback on the conceptual understanding of the problem could be gradually fed into each new model version by the research team in between meetings the research team would incorporate the data and feedback provided to build in the features and mechanisms discussed in the prior meeting conduct verification and sensitivity testing and try out the scenarios proposed by the stakeholders the model modifications new interface features and scenario results were presented at the start of the following meeting for collective inspection and discussion before engaging in systematic collaborative explorations in small groups as the models became more detailed and slower to run the researchers provided participants with scaled model versions with reduced granularity e g by aggregating agents and cells to speed up run times and enable in meeting exploration and discussion 4 2 data collection and analysis we sought to answer questions regarding how the modeling tools mediated the conversations towards developing a collective understanding of the complex problem shifting from a localized to a systems view of the problem and solutions innovating on policy and planning approaches that acknowledge that complexity and building legitimacy around the process and proposed solutions using these models we used a method adapted from the learning sciences milz 2015 2018 2019 radinsky et al 2017 to systematically identify patterns of talk in the meeting interactions that had not been obvious to us solely from our firsthand experience and to describe how participants used models to inform planning and policy discussions each meeting was audio video recorded using a tripod mounted high definition camcorder the camcorder was positioned to capture all group dialogue and interactions during the meetings in total we gathered approximately 500 min of video recorded data from four individual meetings recordings were reviewed and transcribed by a team of five research assistants finished transcripts included individual speaking turns as in your turn versus my turn to talk and were labeled according to the pseudonym of the speaker and intended audience pseudonyms were used to ensure the anonymity of the meeting participants a team of five research assistants coded the complete set of transcripts by applying one or more qualitative labels to describe the content of the statement speaking turn and how the statement functioned within the discussion miles et al 2013 we coded for two features of dialogue which we called moves and targets moves referred to the conversational function of each statement and included 1 observations 2 objections 3 agreements 4 requests for clarification 5 clarifications 6 requests for explanation and 7 and explanations targets referred to the subject matter of the dialogue and included 1 model 2 software 3 model software use 4 policy 5 participatory modeling process and 6 the real world see radinsky et al 2017 for details on this methodology including specific definitions of each coding construct each transcript was coded by two research assistants working independently construct validity was established through a process of convergence in which pairs of researchers coded a document separately then met to discuss discrepancies discrepancies were used to refine the definitions of the codes listed above until independent coding passes were over 90 matching for each code coded transcripts were visualized as timelines and frequency distributions fig 4 the frequency distributions allowed us to inspect the distribution of codes across each meeting and the timelines allowed us to visualize the flow of conversations within each meeting by looking for different patterns in the coded data we observed moments in the meetings where codes might have co occurred been absent or where the conversation shifted from one code or set of codes to another working up from the data we were able to observe the changes in how the participants worked together and with the models over time 5 findings meeting the goals of a model progression the ultimate goal of the model progression strategy was to support plan making where 1 the simple explanatory models would help stakeholders develop a deeper intuition for the important interactions processes and systemic feedback at play in their region 2 the gradual incorporation of detail into the model would help stakeholders link the models to their real world problem space and 3 the deeper understanding and closer relevance would help participants trust their collective capability to draw on their complex systems understanding to build novel robust and widely supported planning solutions we were particularly interested in how stakeholders understanding and attitudes changed as the model detail and sophistication was increased in the following subsections we describe the findings in each of the three dimensions outlined above overall we observed that stakeholders definition of the problem and the novelty concreteness richness and projected effectiveness of proposed solutions did in fact evolve with the use of increasingly detailed and sophisticated models but only up to a certain point that evolution appeared to peak with model 4 which was introduced in the third stakeholder workshop meeting 3 and declined in the final meeting meeting 4 with the more sophisticated and detailed model 5 despite the evidence of learning with direct implications for planning and policy innovation relative to the initial solutions they had proposed for their community the general tenor of the discussion in the last meeting was of uncertainty and distrust in the lessons they had learned through modeling expressed by some as concerns about the inaccuracy of the models these reactions had the effect of widespread disengagement we focus in this section on the evidence of various aspects of participants learning about the complex environmental problem they faced and how they used what they learned to innovate solutions that recognize and build on that complexity in the discussion we focus on implications for participatory modeling particularly to address the barriers encountered 5 1 supporting complex systems learning we expected that in the early stages of the modeling process participants would become comfortable working with the models and understanding concepts related to complex systems and the agent based modeling paradigm e g emergence thresholds feedbacks etc as the meetings progressed we expected the more sophisticated modeling tools would help the stakeholders connect their understanding about complexity and the tools in order to start planning together evidence from the coded dialogue from across the four meetings suggests that participants did comprehend the complex relationships that drive groundwater change in their area and that comprehension did improve as the models became more complicated and realistic however those gains were limited by the overwhelming detail of the final models in the progression and the multiple elements to manipulate on their interface the code frequencies fig 4 demonstrate that during the initial meeting participants focused their efforts on understanding netlogo and figuring out how to manipulate the model interface the models also demonstrated concepts of complex systems e g interaction effects feedback loops emergence etc for the participants as well as they became comfortable working with the models during the second half of meeting 2 their conversations shifted to include references that were more relevant to understanding complex environmental phenomena and applying them to groundwater issues in their community i e references to the world however the quantity and diversity of their references peaked in the third meeting before dissipating during the fourth and final meeting also their references were primarily concerned with the model and underlying data during the fourth meeting instead of focusing on planning and policy making in other words their growing comprehension and application of complex systems reasoning hit a wall as the models became overwhelmingly complicated for example when switching to model 2 the greatly simplified land use model of exurban development and ecological deterioration in meeting 2 participants became more aware of how suburban sprawl and environmental decline were emergent phenomena one participant stan noted how individual decisions lead to system wide effects in some early models households are trying to find ecological quality and in doing so they toasted the environment another participant quorra followed but couldn t quite explain the connections between individual behavior and system wide change stating that if she had been a statistician she might have been able to explain the results she believed her limitation was one of mathematical statistical expertise rather than of conceptualization of interactions and emergence this increasing awareness was a new feature to their deliberations previously most of the discussion focused on reporting at a low level i e without local to regional explanations because some of the important causal relationships in their problem space remained invisible to them leo could understand how a small local change could lead to system wide effects just a sense of how everything interacted together a sense for what magnitude of change such as a small change in an independent variable made a large difference in the results the more complicated models also allowed the group to develop causal connections between growth i e urbanization and groundwater consumption for instance stan used model 3 in meeting 2 to explore the implications of changing consumption rates he understood that they could delay groundwater deficit into the future by reducing household water consumption but the deficit would still happen if growth continued nicole agreed with the point of limiting consumption collectively they seemed to start converging on the idea that growth was a problem to the sustainability of groundwater supplies a notion that was not entirely new to them but that they could start to explain in causal terms in the third meeting we introduced participants to a new version of the urbanization and groundwater model that incorporated more of the community s characteristics model 4 figs 2 and 3 with more realistic features and a more complicated interface the concrete relationship between model 4 and their community allowed them to realize that the eastern portion of communities surrounding their area was connected to lake michigan water so that those communities decisions would not be affecting groundwater levels in their area however model 4 assumed that those communities were groundwater consumers which overestimated their impact on aquifer drawdown this consideration spurred a discussion about neighboring communities and the relationship between water users and their various water sources in other words the stakeholders adopted a broader geographic view of the problem that allowed them to characterize the regional interdependencies the more complicated and realistic model allowed them to understand and mentally simulate scenarios connecting the location of pumping the direction of groundwater flow and the emergent spatial impacts even without the computer simulation in front of them thus we observe how the participants learned about complex systems both through the use of demonstration models and by interacting with increasingly realistic models of their community they used concepts related to emergence and interaction effects estimated regional impacts by land use change to groundwater consumption and collectively developed conceptual models that allowed them to develop counterfactual explanations of groundwater impacts in the next section we describe how they applied this learning to testing policy ideas 5 2 policy innovation testing and relevance to the real world in addition to learning about complex systems we also expected the participants to play with the models to test current policy proposals and identify new ones most participants found this a challenge because relating the modeling to the real world and using it to inform planning decisions was not something that everyone in the group grasped intuitively we expected that increasing the representativeness of models in the progression would help address this challenge because the models would gradually become a more authentic representation of the participants community in fact one of the main participants critiques along the way was that the models were not an accurate enough portrayal of their community the coded dialogue demonstrates that the participants were able to use the models to develop and test policy proposals the code frequencies show that in the second half of the first meeting the middle of the second meeting and early in the third meeting the participants made frequent references to the world and policy during the same conversations as they were interacting with the models in front of them fig 4 however by the fourth and final meeting their references to the world and policy were infrequent and most of their attention was drawn back to the model in meeting 2 participants started to formulate and test hypotheses about how changes in different model parameters might lead to different development patterns participants became aware that the models were limited and did not reflect aspects of reality but they did not see the simplified model as a way to produce generalizable policy insights for instance we showed how a simple land use model like model 2 could illustrate how preferences of people for ecological quality may drive widespread ecological degradation suggesting that action promoting changes in preferences might alter this outcome this understanding could inform policy even when the model was not a detailed replication of the real world in spite of these illustrations one participant luke strongly opposed the use of such models because they were not a perfect representation of reality so any insights the group would generate from it were meaningless he shared that he was conducting a hydrological study that showed there were no groundwater problems in the region several others challenged his position however referring to their own experience and to cumulative effects in the system that his empirical study could not capture quorra asked luke why communities were involved in conservation practices if there was no problem using her personal knowledge of strategies in the real world supported by model results luke acknowledged that the uncertainty around groundwater flows makes conservation a reasonable practice kaitlin wrapped up this discussion by reminding him of cumulative effects over time due to population growth and excessive use especially during droughts allowing the group to reframe the problem for their region luke continued to challenge the model and its assumptions and thus limiting the discussion around complexity by using strongly dismissive language e g referring to the simulations as ridiculous the third meeting supported the richest discussion around policy after a first round of participant feedback was incorporated in the models fig 4 during the small group interaction pairs of participants were assigned to explore a particular set of policy scenarios with model 4 e g preserving recharge areas from development implementing growth management policies within and outside their area water conservation explorations with model 4 sparked the increasing awareness that despite their desire for independence interdependence with neighboring communities would prevent them from addressing groundwater supply if they only approached conservation and enhanced recharge within their boundaries and that regional coordination was needed the explorations also allowed participants to refine the idea of connecting to lake michigan water and to understand how downstream communities could not improve their situation even if they did switch to lake water this aspect was subsequently dropped from the policy discussion as a result of the insights derived from the modeling the realization that the causes of groundwater drawdown were outside their local control overwhelmed the participants they were quite discouraged that none of the policies they had proposed prior to the meetings worked in the way they had envisioned olivia they were all kind of hopeless where s the good news scenario nicole whatever we do here is not enough laura the things that matter most to us we don t have control over this hopelessness might be one explanation for the disengagement in the last meeting in that meeting the presentation of the most detailed model of the progression model 5 figs 2 and 3 allowed the group to further refine where enhanced recharge should be located whereas our model placed them randomly quorra pointed out that it would make more sense to locate them in places with high permeability or recharge rates i e she was using the models to gradually build concreteness towards an implementation plan for enhancing recharge olivia suggested the model implies that we re fine but not those around us kaitlin who was sitting with her elaborated that from the planner s perspective we re not just fine they affect us the conclusion was that actions they took locally would not have much of an effect regionally larry reminded the group that the drawdown impacts were likely overestimated because the municipal wells were drawing from an aquifer different from the ones private wells used showing an ability to extract the insight despite the lack of specific data the realization motivated the group to think further on what could be done to preserve the water resource emphasizing the need for regional coordination and pushing beyond the solutions that had been commonly discussed for example olivia do we create giant recharge areas that will assist the whole region i mean what are we learning from this laura i think we re learning that regional water management is essential for all of us to which most of the participants nodded in agreement olivia conceded and commented on the need to coordinate with their neighbors as the last meeting came to an end individual reflections showed that the concern for water sustainability had grown over the participatory modeling experience but the sense of uncertainty and helplessness took over resulting in a general dismissal of and disappointment in the modeling process and the lessons learned the modeling introduced uncertainty and confusion leading to hesitation about possible directions the conversation was more sparse and focused relatively more on making sense of the model and its outputs and less on the real world implications while the facilitated policy discussion failed to build on the lessons learned through their collaborative modeling fig 4 the group instead advocated for collecting more data to address the uncertainty in line with dennis et al 1984 the lack of accuracy weakened the role of exploration with simple models in what was a politically sensitive process 5 3 planning with models finally we observed how the participants used the models and their knowledge of complexity to consider the future impacts that growth and development might have on their groundwater supply and develop a robust set of solutions to ensure water sustainability the participants initial desire for autonomy and independence was gradually replaced by the recognition of interdependence with the communities around them and the need for coordination and collaboration technical solutions also took more concrete shape focusing on details like the location and magnitude of artificial groundwater recharge sites needed to counteract the effect of development and with a more nuanced understanding of spatial interactions of withdrawals relative to direction of groundwater flow however the limitations they experienced as they interacted with the more detailed models caused them to question their learning and how they could use it for planning the code frequencies show evidence that the participants used the models to talk about planning issues especially toward the end of the second meeting and at the beginning of the third meeting fig 4 during these periods we note that the participants were referring to the model the world and policy simultaneously by the fourth meeting their references to the model were segregated from their fleeting references to policy and world this reflects the challenges experienced in using model 5 the most detailed model for planning and policy making it is important to note that they raised these challenges even while recognizing that the models had revealed useful insights for addressing future groundwater shortages the concept that simple even if unrealistic models could be useful to generate policy insights was difficult for the group to assimilate in their work in the first meeting participants had a chance to discuss the various solutions to groundwater depletion that the water resources initiative was pursuing the group came together to define the focus for themselves as water quality quantity and conservation there was great concern for the relevance of the entire exercise to the specific conditions of their region particularly since the community had dedicated considerable efforts in collecting data to characterize their aquifer one of the participants stan proposed that if the model did not look like their community then it would not be as helpful while others insisted on the use of the hydrological data that had been collected this became a theme throughout the rest of the meetings which in retrospect led the process away from deriving generalizable planning insights from an understanding of interactions as was originally intended and discussed when participants first used model 2 a simple urbanization model in the second meeting figs 2 and 3 they understood how location preferences could drive urban and environmental patterns despite realizing the importance of land use preferences most of their planning focus remained on water conservation and on physical aspects of the landscape a top down perspective rather than on the behavioral emergent urbanization patterns and their indirect effects on water supply nevertheless the group quickly realized that they only had power over their own community and not over their neighbors the question started to shift towards recharge within their community and to what extent this could compensate for the drawdowns from surrounding communities the group continued to make progress through the third meeting working in small groups leo and larry effectively played with model 4 to test questions about interactions doing systematic parameter sweeps and trying to make sense of the outputs quorra struggled but started to shift from clarification questions to explanation of the emerging patterns although quickly shifting to hopelessness we re already hosed the hopelessness then turned into frustration with the models and the modeling process and disengagement with the work in progress nature of exploratory modeling the research team attempted to address the frustration by incorporating as much feedback and data as possible before the last meeting participants were then forced to make sense of the complexity of interactions and to comprehend model 5 with the highest degree of representativeness they had seen so far participants who struggled with model 5 had trouble designing and making sense of policy scenarios that were meaningful to them overall there was much more reference to what participants were seeing on their screen suggesting that the new detail made it more concrete but also so complicated that they could not generalize beyond the instance in front of them while the first three meetings resulted in rich discussions insights and novel policy propositions in the fourth meeting participants were noticeably less vocal fig 4 and the tenor of the conversation was more around hopelessness and uncertainty about what to plan for followed by questions about the validity of the experience and the request for more data to reduce the uncertainty rather than looking for robust approaches to the problem e g reduce consumption levels whether wells were shallow or deep the group hesitated to commit to next steps of action based on what they had learned and focused the discussion on the additional data needed to make decisions this position however left the group powerless they were not aware of how the detailed data had contributed to making the modeling intractable to them for example quorra and kaitlin commented on how long it took to play with the model both to make sense of it and to run it quorra believed that adding data would make the model better however she did not see the relationship between increasing the level of complication and the slowing down of both sense making and run times the more detail that was added the better the model got i still do not think there were enough factors in there to truly get a representation the community had already dedicated significant resources to gather data about the aquifers but water levels were harder to come by did this mean that they could not make decisions how much data is necessary to make decisions how much uncertainty is tolerable the end result was significant hesitancy and disorientation while we were hoping for more concreteness sense of direction and empowerment during the final reflection several participants expressed that the ultimate test for the model s usefulness for planning was how close it was to reality olivia in particular stressed that if it was not accurate then it is of no value the models were useless unless they were really telling her what the story is stan and quorra showed more willingness to reason with the models about the problem the former because of his level of comfort with modeling the latter because of her curiosity despite the struggles with the model 6 discussion barriers to participatory modeling for environmental planning based on our evidence we argue that the increased level of detail hampered the effective use of modeling within the participatory modeling process goldstone and wilensky 2008 the progression of models enabled a rich discussion about the complex problem the communities were facing and of the implications for policy and the real world but the increasing complication and representativeness became an impediment to productive discussions and appropriation of the insights after meeting 3 other aspects of the meetings such as the instructional setting the power relationships among the participants hoch et al 2015 and a culture that favors outsourcing sophisticated predictive modeling to external consultants zellner 2008 likely contributed to this breakdown as well we suspect however that simpler models might have helped overcome these obstacles we expand on the barriers encountered and their possible causes below 6 1 exploration versus prediction supporting playfulness and collaborative ownership there is an inbuilt assumption among many scientists and decision makers that predictability establishes validity zellner 2008 participants in our case wanted to establish faith in the results at the end of our progression through a predictive model that used their data while we clarified expectations early on and throughout the process several participants still expected predictability kaitlin for example wanted to be able to make sense of the data to inform decision making and was interested in new ways of modeling for that purpose however her expectations were not met truthfully i expected to learn something different than i learned and it was a challenge to scale down my expectations i didn t learn what i wanted to learn but i learned some other things she also discussed the unmet expectations of other participants and saw her role as helping others be satisfied with the experience and understand that the modeling was exploratory more than predictive people came in with expectations of a predictive model i know people were frustrated and talked to me about it in between sessions i wanted very much for us all to give you all an open mind and to bring in other information about other studies that have been done etc about the area she expressed how participants didn t know how to make sense of the outputs if they weren t predictions that is what people are used to seeing and used to getting if that wasn t where we were going what are we getting out of it larry was among those who agreed it s not a predictive model so that s a limitation models are used for various things but primarily the objective is to be predictive if a model isn t equipped to be predictive then that s a limitation that people are gonna perceive as being significant you have to first go back to the original intention of the model if they couldn t use a model for predictive purposes then they wouldn t build it the model you built can bring attention to various things but it would be a whole lot better if it had what it needed to be predictive in contrast quorra came into the meetings without the modeling expertise and expectations others had i guess i went into this without really any particular expectations just more as an educational process my first expectation is education and understanding the idea that we re gonna have a model for our community to follow and policies from that model would be way too much to expect perhaps this expectation is what allowed her to play with the models despite the difficulty that complicated interfaces and outputs posed this difficulty negatively affected her perception of the usefulness of the tools after the modeling experience i do not think i ever got an idea of just what this model can do i am not convinced the model is useful while quorra believed in participatory planning it is always better she also believed that modeling is too complicated and that it should be outsourced to consultants she could not see the benefit of engaging in the process of modeling as she was more used to studying outputs produced by modeling professionals quorra had effectively built on the modeling experience to sharpen her ideas for solutions which were taken up by the whole group but her sense of uncertainty prevented her from feeling empowered by the process instead suggesting outsourcing the modeling to professionals 6 2 prosthetics for planning versus cognitive overload the power of simplifications and examination participants tried to make sense of the detailed model s features and outputs but expressed considerable difficulty in doing so the models complication significantly increased the cognitive load required to understand indirect effects and tradeoffs which made it increasingly difficult to keep track of their progress or lack thereof towards sustainability goals with the ensuing consequences described above uncertainty and frustration demanding for more accuracy to increase their confidence in a specific policy approach arciniegas et al 2013 dennis et al 1984 inman et al 2011 sun et al 2016 kaitlin did concede that the simpler less realistic models were clearer because simulations with the more realistic models ended up in the same place so you couldn t see what difference each slider made and that was discouraging i do not think it the model has the value in its current form to do the kind of work that we want it to do the difficulty of making sense of more complicated models led to perceptions that the problem was the simplification t he model is grossly oversimplified we shouldn t be drawing these conclusions w e don t have enough to make an educated policy decision i would not want to establish a policy unless the data really supported it we need to have compelling scientific evidence in order to convince people of needed actions throughout the process we had demonstrated how simplifications and assumptions are a necessary part of complex systems modeling and testing such assumptions e g with empirical or hypothetical data can help us explore the problem space understand simulation outputs and innovate and test solutions accordingly set up in this way such models become prosthetic devices for thinking and planning with complexity hoch et al 2015 when prompted participants didn t seek to test alternative assumptions to see how much difference they would make the modeling progression that the group had worked through was not enough to show how one can handle such erroneous simplifications with systematic testing instead it fed the notion that just one mistake in parameterization could render an entire modeling process meaningless 6 3 instruction versus collaboration ensuring productive and equitable knowledge co production in each engagement the ability of the group to use the models to collectively reflect on the world improved with each version of the progression but we observed discrepancies in how participants transferred what they learned from the modeling to the real world power struggles emerged around the legitimacy of knowledge and solutions with the modeling effort at its core the differences in expertise contributed to power imbalances that impacted group deliberations as the views of the decision makers and disciplinary experts were often given more weight by the other members of the group participants often resolved differences in understanding and power imbalances among themselves sometimes using the modeling to support their claims hoch et al 2015 the instructional setting we had designed had the unintended effect of pitting the researchers expertise against the participants making it hard to support an enriching collaboration instead some participants contributed some insights about this setting olivia i felt that there were a lot of assumptions ahead of time that included the research team s assumptions made more than the practitioners did practitioners know more about day to day reality of water and users very cursory assumption i recognize that real people sometimes get disconnected from data felt it was education based not real life the model assumptions were made explicit so that participants could modify them to increase the correspondence to their community olivia had a hard time translating her expertise into model assumptions however and thus engaging in the collaborative exercise an excerpt from meeting 1 illustrates this tension moira if you want to just use the map and point to things like the bigger map olivia well the map doesn t really say where the recharge areas are moira right but do you have an idea of where they are olivia yes moira okay places the map close to the group olivia do i have to moira no you don t have to olivia i m just kidding the recharge areas were never drawn on the map stan commented that the meeting structure was limited that we either needed more meetings or needed to arrive to the point sooner the series of meetings had been designed in sets of four to support a gradual uptake of participatory agent based modeling that they could own and expand on beyond the fourth meeting such a setup did not provide the closure that participants needed instead ending abruptly for them a lot had yet to be resolved or understood as the last meeting came to an end the sense of uncertainty and helplessness took over leading to a general disappointment in the modeling experience a lack of excitement about the lessons they had collectively learned and the useful planning implications that their explorations had revealed and disinterest in pursuing further collaborative modeling 7 recommendations for participatory modeling in environmental planning our model progression strategy supported planning of complex sustainability problems the stakeholders used the modeling progression to comprehend complex social and ecological systems to test various policy options some of which had been previously considered and some of which were new to the group participants could discuss both in meetings and after meetings about how local land use and water use decisions could lead to widespread depletion in their community they also used the models to rethink their strategies for addressing potential future water shortages and the interdependencies that drove water consumption in the larger region they could also see how they could not isolate themselves from surrounding communities and they also advanced in their design of more concrete approaches to enhancing groundwater recharge in other words they modeled learned and planned together the process was hampered by the lack of confidence in their insights as the models became more detailed for the reasons described above we focus here on two aspects that may better support participatory modeling in such situations slowing down model complication and designing facilitation strategies that ensure that meaningful insights can be derived in each meeting while there is support in the learning sciences literature for the use of models with different representations of detail the cognitive load associated with managing multiple interactions and tradeoffs is taxing for participants models need to support a basic understanding of interactions extract main points and focus further modeling on those points e g recharge access to lake michigan water simpler models help participants overcome the difficulty of using models for systematic exploration difficult decisions especially those involving many interactions and tradeoffs trigger the need for accuracy as cognitive load increases arciniegas et al 2013 dennis et al 1984 inman et al 2011 the degree of model complication triggered this need in our case prompting the request for accuracy and unwittingly contributing to decision stress what our participants thought they wanted certainty and what the research team attempted to deliver trust took us further away from where we could collectively learn about complexity and design robust planning and policy approaches to address it other experiences have had similar outcomes schmitt olabisi et al 2010 such overemphasis on accuracy is not helpful simple models are useful to have meaningful conversations around more sophisticated models dennis et al 1984 further work is thus needed to study the ways in which a modeling progression can help use the basic understanding gained with simpler models to support appropriate inspection of and meaningful simulations with more detailed models even though we had a good justification for our model progression past a certain point the representativeness of the model became unhelpful in contrast with other studies we found that this had less to do with whether the users could make appropriate inferences about what was happening within the system our data shows people in fact could do this but rather with how people s cultural expectations shaped their ability to trust the inferences they were making the models allowed the group to test solutions fail and modify their ideas their expectation was that it would be easy and obvious to show how their favored solutions would work but it turned out to be less so especially with the most detailed models they expected that clarity would emerge when the model incorporated all the data they had given us the models became too complicated for them to understand why they kept failing and the collaboration became a tense negotiation between participants and researchers and frustration with the exploratory nature of the modeling hoch et al 2015 interfaces model structures and facilitation structures need to be tailored to different kinds of user groups to support proper interpretation of modeling outputs especially when they are dissonant with expectations pettit et al 2011 stakeholders needed more opportunities for synthesis and reflection after a carefully designed sequence of tasks of increasing difficulty white and pea 2011 this is as much about model progressions as it is about meeting design and facilitation the four meeting sequence gave a sense of anticipation for the end where a big realization was supposed to happen but they did not get to a good news scenario that they could hold on to with hope e g antle et al 2014 poplin 2012 participants needed extra support from facilitators to productively explore with the models they helped develop keep track of changing variables of interest and reflect on how to make these variables change in more desirable ways zellner et al 2020 the instructional approach also had a profound impact on participants expectations and their disappointment the conceptual culture in planning is learning about complexity through precedent what works elsewhere rather than through models that can help develop explanations for what could work here the culture of model use is often linked to external consultants who deliver outputs for decision makers as consumers rather than co creators of an exploratory modeling process popular planning models tend to be easier to use and more general in application but there is still a barrier from modeler to planning user triantakonstantis and mountrakis 2012 the key bottlenecks are related to lack of transparency of model assumptions and inner workings low communication value with planners politicians and stakeholders and difficulty of tool interaction for exploration tebrömmelstroet 2010 will et al 2021 stakeholders must be engaged in product development rather than be mere consumers pettit et al 2011 for this to happen modeling skills must be taught to a broader audience so that they can become aware of its benefits will et al 2021 our attempts to address these recommendations were not enough however while the modeling progression supported learning and solution building the simplifications needed to be led by the participants as we responded to their demands for accuracy we were making at times too many modeling implementations for them their learning increased but their trust in the models decreased as our simplifications and variables were foreign to them we could have instead negotiated the simplifying assumptions collectively rather than instructing them on how it could be done a facilitated discussion with a skilled community facilitator as proposed by e g hovmand 2013 lane et al 2011 schmitt olabisi et al 2010 would have been helpful in ways we hadn t thought of supporting imagination and projection quantification strengthening relationships defusing conflict staying engaged reducing entrenched trust or distrust and avoiding dualistic approaches i e co producing knowledge through activity in a cognitive apprenticeship brown et al 1989 models are embedded in culture and thus should be learned and developed within that culture this is precisely the reason why models should be built within the authentic planning activity and not extraneous to it so an instructional setting where the models are taught to participants was not conducive to building a robust shared knowledge the rich experience gained in this trial allowed us to refine our participatory modeling approach for environmental planning first models need to be kept simple to work better in supporting complex systems learning and translation into policy and planning this keeps the cognitive load at a manageable level while allowing models to run fast within a meeting in this way stakeholders can quickly examine a range of scenarios and gain a fuller understanding of the problem and the multi dimensional and stochastic effects of proposed solutions second rather than a sequence of instructional sessions we contend that an open ended series of meetings each one of them self contained and centered on interactive exploratory activities both among participants including researchers and between participants and the modeling tools is more conducive to engaged exploration the self contained design ensures that there is at least one aspect that participants can take away from each meeting such lessons could also be structured around game like tasks e g friendly competition among participants the open ended structure motivates participants to join in ongoing endeavors as they raise the need for further modifications and participate more directly in the simplifications as more complication is reached and the cognitive load increases stakeholders need to be engaged in determining what factors and mechanisms to leave out in favor of new ones that they deem necessary to include third participants must be supported in ways that help them systematically explore the problem and solution space keep track of how outputs vary in each scenario and assess these outcomes against the collective preference for specific tradeoffs initial work on tangible and mobile interfaces and facilitated learning structures developed for this purpose shows promising results zellner et al 2020 finally in addition to expert modelers and modeling facilitators community facilitators ideally among the group of stakeholders are critical in enhancing communication and collaboration among all participants community members and researchers both within and in between meetings hovmand 2013 lane et al 2011 participatory modeling is useful for environmental planning in representing diverse knowledge inspiring insightful conversations fostering strong and equitable relationships and supporting transformative solution building and agency it requires however significant time and resource commitment to develop and refine its guidelines for practice ongoing work must focus as much on the collaborative model development and social learning process as on the cultural social and institutional context in which this process is embedded hedelin et al 2021 here we showed how and to what extent a model progression could work for this purpose perhaps more importantly we shared how and under what conditions this progression stopped being helpful identifying the barriers to address and future steps needed to support a more widespread adoption of participatory modeling as an established planning practice finding finding the balance between simplicity and realism in participatory modeling for environmental planning declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work would not have been possible without the support of several research assistants over the years kelsey pudlock jennifer weizeorick miller april schnider lisa domoracki and priscilla jimenez we would also like to thank dean massey for invaluable editorial support the research was funded by the department of urban planning and policy the great cities institute the institute for public and civic engagement and the chancellor discovery fund at the university of illinois at chicago the national science foundation oci program 1135572 and drl reese program 1020065 and the northeastern university college of social science and humanities we are grateful to the stakeholders who participated in this project and to three anonymous reviewers appendix a supplementary data the following is the supplementary data to this article figs1 figs1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105481 
25508,integrated distributed hydrologic models leverage advances in computational power and data accessibility to improve predictive understanding of the water cycle while impressive advances in this area of environmental modeling have been accomplished such models are still rarely used partially because of difficulty integrating model and data this research describes the release of watershed workflow version 1 2 a new library aiming to automate and enable complex workflows defining inputs to high resolution integrated distributed hydrologic models watershed workflow provides tools enabling the discovery acquisition mapping and coordination of watershed geometry land cover soil properties and meteorological data it enables the construction of unstructured meshes that incorporate this data and provides tools for automating a first simulation on any watershed in the united states we present the design of the workflow tool and describe best practices for its usage culminating in a final example from watershed specification to simulation at the coweeta hydrologic laboratory graphical abstract keywords integrated hydrologic modeling modeling workflow model parameterization and setup data availability data will be made available on request 1 introduction water is a crucial resource required for life yet that resource is threatened globally under a changing climate vorosmarty 2000 predicting and understanding threats to water quality and quantity is greatly benefited by relevant accurate simulations of the water cycle bierkens 2015 the desire to understand the processes and process interactions governing the water cycle over climatalogical time scales has motivated a community push toward so called hyperresolution hydrologic modeling simulations on full river basins the natural unit of simulation of water on land at 100 m resolution or smaller the scale at which macrotopography is resolved and models can capture lateral flow wood et al 2011 bierkens et al 2015 routine application of such simulations remains a goal of the community today meeting this goal requires building from ideas and capabilities in two related modeling communities the integrated hydrologic modeling community e g paniconi and putti 2015 whose models resolve smaller watersheds at the needed resolution and often include lateral flow through 3d representations of richards equation and the regional and global earth system modeling community e g sood and smakhtin 2015 whose models capture the needed extent but at coarser resolution and often with simpler representations of hydrologic flow processes progress toward hyperresolution models enabled by ongoing trends of increasingly available computational resources and high resolution spatially complete data has included several early successes maxwell and condon 2016 yucel et al 2015 ko et al 2019 however such models are still relatively coarse resolution and often ignore or simplify subsurface lateral flow and surface water groundwater interactions furthermore these successes are still few and far between integrated distributed models are not yet widely used at large scales one reason for this is because of the difficulty of parameterizing calibrating and evaluating these models beven and cloke 2012 ebel and loague 2006 parameterizing these models alone requires a large collection of datasets including accurate elevation models land cover maps subsurface structure models soil and plant properties and meteorological forcing data this work seeks to address this need by providing a tool to automate and enable complex workflows defining all needed inputs to integrated hydrologic models on unstructured meshes several previous efforts have developed software to synthesize data for use in hydrologic models gis platforms provide many of the needed algorithms for data mapping and integration and hydrologists have leveraged these tools extensively for years devantier and feldman 1993 maidment 1996 gis based tools have been developed to integrate data with specific models and often function as graphical user interfaces guis for those models olivera et al 2006 granell et al 2010 examples include the regional hydro ecologic simulation system rhessys workflows tool which builds on the ecohydrolib library to provides a python based suite of tools for preparing data for modeling carbon water and nutrient fluxes at the watershed scale tague and band 2004 nativi et al 2013 demonstrates a service based approach for connecting observations from a data archive to the soil and water assessment tool swat hydrologic model gardner et al 2018 developed a suite of python scripts for the proprietary arcgis software which maps raster based datasets into consistent input for the gsflow hydrologic model models which route surface water explicitly like gsflow must ensure that the stream network and dems are consistent and this tool focuses on that mapping process additionally recent u s operational efforts in flood prediction have created workflows and cyberinfrastructure for generating 30 day flood predictions in watersheds across the nation souffront alcantara et al 2018 the national water model used in these products leverages 250 m resolution raster based datasets within a variant of the wrf hydro model powers et al 2017 which includes vertical infiltration and lateral surface routing of these only gardner et al 2018 includes fully distributed representations of the subsurface and provides subsurface soil structure and properties none of these tools include the discrete differential geometry needed for generating unstructured meshes which are often used in high performance computing simulators in the case of integrated hydrologic models that work for unstructured meshes one other workflow tool has been developed and published pihmgis www pihm psu edu is a workflow tool that uses the hydroterre leonard and duffy 2014 archive of synthesized data products to set up simulations for the penn state integrated hydrologic model pihm bhatt et al 2014 pihmgis uses a tightly coupled approach where the workflow tool and hydrologic model share a common data model and geodatabase file format the hydroterre data service synthesizes and redistributes a single set of terrestrial variables and data sources for watersheds at the huc 12 level allowing an automated model setup process this approach is limited in its extensibility to other models other data sources and other strategies for parameterizing the model here we look to address this gap through the release of a new tool watershed workflow watershed workflow consists of a python library that provides crucial capabilities for model parameterization including 1 a common set of interfaces for finding downloading and synthesizing datasets on both the land surface and in the subsurface 2 functionality to integrate and rescale those datasets onto a common unstructured mesh and 3 an api for backends that write these datasets into the computational meshes and the accompanying files needed by integrated distributed hydrologic models example jupyter notebooks are provided that leverage these capabilities to form a reproducible and automated workflow for model experiment driven efforts the tools themselves are largely scale agnostic but the default data sources typically range between 10 m and 1 km resolution and have been used on watersheds ranging from a few square kilometers to full river basins watershed workflow automates the formation of a first simulation anywhere in the united states while providing sufficient tools to allow users to extend or improve that model as data permits including user provided data this workflow was developed to generate inputs for the advanced terrestrial simulator amanzi ats coon et al 2019 2020 but the underlying toolset is designed to be as model agnostic as possible and has been used to develop input files for pflotran lichtner et al 2015 as well conceptually similar codes such as parflow kollet et al 2010 hydrogeosphere brunner and simmons 2012 and pihm qu and duffy 2007 could directly use this tool with the addition of backends that would write their corresponding mesh and data file formats explicitly routed integrated distributed models such as gsflow markstrom et al 2008 could leverage this capability with additions to ensure consistency of the river network and dem gardner et al 2018 while watershed workflow does a similar job as pihmgis they are also significantly different first the tightly coupled nature of pihmgis requires both the data provisioner hydroterre and the hydrologic model pihm to adopt a common data structure while watershed workflow prefers to duplicate data and map between data models as needed trading performance for flexibility second pihmgis is fundamentally a gis tool focused around a gui for enabling point and click access to a modeling platform watershed workflow takes a different approach as a set of tools enabling the development of scripts and notebooks to enable custom yet reproducible models while like pihmgis watershed workflow provides a default model setup it is more focused around providing the needed utilities to allow researchers to automate script and configure their own workflow for their own simulation plan this makes watershed workflow more useful as a tool for enabling extensible research as it allows both more complex workflows and composing this workflow with other python based workflow libraries also as a python based library watershed workflow is easily scripted to enable ensembles of simulations or simulations across many watersheds furthermore all dependencies are open source and freely distributed making it more accessible than many gis based tools in the remainder of this paper we describe the data needs of integrated distributed hydrologic models and how datasets addressing each need are found and downloaded section 2 we show the tools used for mapping that data into a model domain and mesh section 3 we then discuss the deployment and design of the workflow focusing on our approach for a robust and extensible capability section 4 finally we present a worked example showing an example workflow section 5 2 data sources and management watershed workflow s goal of providing a first simulation anywhere in the united states is made possible by a large suite of open data sources provided by governmental agencies and served through many assorted web services unfortunately there is not a single source or data server for all of these and web services differ across agencies therefore a key contribution of this library is the collection and standardization of access to many data repositories in a single package furthermore this suite of tools for reading apis would be useful in extending this workflow for use outside of the u s where datasets are available the tools provided by this workflow are applicable to accomplish this watershed workflow introduces the concept of a source manager which given a hydrologic unit code huc or polygonal shape representing a watershed queries downloads or otherwise acquires a dataset or datasets that cover that watershed unzips the file if necessary and stores the resulting files under a standardized naming convention in a data library to enable reuse reads metadata and properties providing shapes or rasters and metadata and does this via a standardized interface to allow users to swap data sources easily source managers frequently use representational state transfer application programming interfaces rest apis to discover specific downloadable files or directly download the data itself via a get push request in this section we enumerate the diverse datasets needed to inform a single simulation and describe the source manager or managers used to acquire that data 2 1 watershed geometry watershed boundary datasets and hydrography datasets together form the geographic structure of a watershed watershed boundary datasets are typically delineated through the analysis of elevation datasets defining a watershed as all parts of the land surface which drain to a common point typically in the stream network watersheds are hierarchical ranging in scale from small primary watersheds which drain into first order streams to full river basins which drain into an ocean in the united states the united states geological survey usgs formally calculates hydrologic units and identifies them using hydrologic unit codes or hucs which respect this hierarchy huc 2 regions e g the upper colorado river or the tennessee river basin are the largest in areal extent while huc 12 s or sub watersheds are the smallest commonly provided representing on the order of 100 square kilometers watershed workflow uses hucs as an organizing unit for working with data primarily because most hydrologic datasets in the united states are organized by the huc but also because they form physically meaningful domains for simulation should groundwater divides be desired instead of the default surface water divides shapefiles must be provided by the user and otherwise the process is identical the huc containing the given shape is determined automatically and all data is restricted to the provided shape hydrography datasets provide surveys of river networks which form the drainage network of watersheds and are where most of the fast time scale dynamics occur some hydrologic models for instance river routing models dam operations management models and many flood models directly use the river network as their simulation domain while others for instance the class of integrated distributed models described here can use the river network to refine meshes near the rivers and therefore improve resolution where fast dynamics are occurring watershed boundary and hydrography datasets are typically available as gis shapefiles where each watershed boundary or reach is represented as a shape watershed workflow leverages the watershed boundary dataset wbd the national hydrography dataset nhd and nhdplus high resolution nhdplus hr available at multiple resolutions at 1 24 000 scale or better to represent united states watersheds simley and carswell 2009 by default the nhd datasets are used but all resolutions are accessible through the source manager data is discovered through the national map s rest api u s geological survey 2021 which allows querying for data files organized by huc and resolution via http post requests providing direct download urls files are downloaded on first request unzipped and stored in the data library for future use currently files are indexed by 2 digit wbd 4 digit nhdplus hr and 8 digit nhd hucs which serves as the upper limit of the huc levels for request 2 2 digital elevation models digital elevation models or digital terrain models dems or dtms hereafter we use dem to indicate either are necessary to provided elevation of the ground surface the national elevation dataset and ongoing work in the usgs 3d elevation program 3dep provide rasters which cover the united states in 1 degree tiles gesch et al 2009 these data products are available at a variety of scales but are available uniformly across the entire u s at 30 m resolution and across the conterminous u s at 10 m resolution the location and filename which differ across the u s are discovered using the national map s rest api and are downloaded stitched together if needed and returned as a single dataset and metadata profile note that the above nhd nhdplus hr and wbd delineations are based on data from the 3dep program consistent elevation maps and watershed delineations are used to ensure consistent watershed simulations 2 3 land cover given a watershed land cover is required to define surficial processes including runoff through manning s coefficient or other model parameters evapotranspiration through leaf area index or other model inputs and impervious surfaces most distributed hydrologic models link these parameters to a land cover index or plant functional type which correlates with land cover the national land cover database nlcd homer et al 2012 provides a map at 30 m resolution of land cover across the nation at several different time slices this dataset is based on landsat remotely sensed imagery and classifies each pixel into a set of indexed land cover types typical parameter values are chosen from the literature and associated with each land cover type the nlcd raster is a single 16 gb file download that covers the conterminous u s and additional rasters for each for alaska puerto rico and hawaii hosted at static urls the raster is downloaded unzipped and stored in the data library on first request watershed workflow in addition to standard capabilities required below provides colormaps and labels for the various nlcd indices allowing for plotting using standard nlcd colors 2 4 subsurface structure and properties perhaps the most uncertain yet important parameters required by an integrated hydrologic model based on richards equation are the subsurface structure and properties soil horizons depth to bedrock and soil and geologic properties e g permeability porosity and water retention curves control the rate of infiltration water storage and other key processes of the water cycle these are also the most difficult properties to attain in standard ways across basins rarely are they observable through remote sensing leaving ground based surveys direct sampling and other local measurement approaches to inform model parameters as a result integrated distributed hydrologic models are often limited by the availability of accurate subsurface structure and parameter data fortunately synthetic aggregated datasets of soil structure and properties are becoming more common dai et al 2019 and at least for the top few meters of soil are relatively well constrained depth to bedrock and deeper information is correspondingly harder to find but global products are available dai et al 2019 modelers must choose both what products to build from and also at what resolution and structural complexity reasonable modeling studies might choose simple models and few parameters or complex models with varying horizon thicknesses and properties as fits the modeling question being asked in many watersheds individual soil pits or boreholes might provide extremely detailed information in exchange for being localized in space individual users should incorporate this data where possible here following the general concept of watershed workflow we provide a workflow defining a default soil structure that can be constructed everywhere but also provide the ability to use other datasets as chosen by the user in this way we can ensure that there is a well posed problem that can be automatically formed everywhere in the conterminous u s however this may be either too complex by introducing too many free parameters for calibration for instance or too simple by not considering important fractured bedrock or other subsurface formations for a given modeling study therefore we emphasize that it is crucial for a user to choose subsurface structure and properties carefully and understand the sensitivity of their result to these choices 2 4 1 global hydrology maps products intended for earth system models such as global hydrology maps version 2 0 glhymps huscroft et al 2018a provide coarse resolution information of aggregated properties providing a single porosity and permeability for all soil and geology including fractured bedrock above the bedrock glhymps 2 0 builds on a high resolution lithology map that includes fine and coarse sediments sedimentary rocks weathered sediments bedrock and improvements relative to the original product in differentiating consolidated and unconsolidated sediments glhymps is available through a data doi huscroft et al 2018b but requires a web form to be completed out of respect to the author we therefore require users to manually download this dataset rather than bypassing the form while glhymps is complete in the information it provides it is not sufficient as it does not include water retention curves through van genuchten or other parameters 2 4 2 nrcs soils more detailed survey products such as the u s national resources conservation service s web soil survey ssurgo and statsgo2 datasets provide near surface soil information soil survey staff 2021 these products are based on extensive u s and state geological surveys and cover the entirety of the conterminous u s soil survey files identify a set of map unit shapefiles that cover the ground identifying multiple components of the soil in each map unit and multiple soil horizons in each component the soil survey includes porosity permeability bulk density and sand silt and clay compositional percentages for several horizons in each component watershed workflow provides tools to average these values both vertically across horizons and then horizontally across components by area fraction to determine a single set of properties for each map unit note that permeabilities are averaged in log space then watershed workflow calls the rosetta version 3 beta zhang and schaap 2017 model for pedotransfer functions allowing the calculation of water retention curves in the form of van genuchten parameters van genuchten 1980 as a function of these texture properties the nrcs web soil survey provides several ways to access its data here we leverage the sql based query structure of the provided web tools natural resources conservation service 2018 these allow users to post requests based on a bounding box of the area of interest downloading collections of shapefiles that include map unit keys these files along with corresponding sql queries for soil properties by map unit key are saved to disk and potential used to color a raster we note that the gridded gssurgo product is available by state this product while potentially useful to users is served in a cloud based service whose software toolkit packages require purchased access while a user may manually click through links to download gssurgo it is not possible for an automated workflow to discover the url meaning that the rest api is more robust for an automated application watershed workflow does allow the use of gssurgo or other raster files but the user must download them manually while these datasets are some of the best information available the dataset as a whole is limited to the top two meters and is not complete in that some formations do not have sufficient data to infer all needed soil properties where these datasets are insufficient by default the above glhymps data is used in the near surface as well as in the deeper geologic layer 2 4 3 soilgrids finally the soilgrids project takes a different approach toward soil properties in hengl et al 2017 a machine learning based approach was used to infer soil properties from a wide range of surface climatological and other properties the neural network was trained using over 150 000 soil profile observations and over 150 remotely sensed observations assumed to covary with soil properties the resulting neural network is used to predict soil properties at seven soil layers globally at a variety of resolutions the result is a dataset that is complete globally but is also a field for each property as opposed to the geologic unit approach of glhymps and nrcs soils datasets using such a field based approach is not currently supported by watershed workflow but the structural aspects of soilgrids are still useful specifically soilgrids 2017 includes a depth to bedrock field that can be used to provide the structure of the bottommost layer of simulations soilgrids 2017 depth to bedrock field is downloaded globally at 250 m resolution as a single raster shangguan et al 2017 2 5 meteorological data given surface and subsurface structural data and properties the crucial remaining model input is meteorological data typically including precipitation air temperature relative humidity and radiation fluxes these variables are used to drive the transient conditions of a hydrologic model determining water sources and sinks e g precipitation evapotranspiration etc daymet version 4 thornton et al 2020 provides daily meteorological data at 1 km resolution across the entirety of north america it synthesizes daily weather stations datasets from the global historical climate network daily database durre et al 2010 and spatially and temporally interpolates to create a gridded product given the model domain watershed workflow leverages the daymet rest api services to download for each year and variable a file containing all pixel values in a bounding box covering the model domain the library then merges these into a single file for use by the model code this is typically preferred to interpolating the gridded product directly onto the mesh due to file size concerns a hyperresolution simulation may consist of millions of surface cells saving a data point for every cell at every day of a given simulation may be prohibitively expensive instead it is expected that the interpolation on the mesh can be handled inside of a simulator from the raster the resulting data is saved in either netcdf4 or hdf5 file formats for use by the simulator 2 6 user provided data finally it is recognized that these data products while sufficient to generate a first model may not be the first choice for a given watershed problem or resolution in addition to being designed to be extensible see section 4 generic source managers are provided for using user provided files including both shapefile based products e g watershed boundary and raster based products e g dem these files can be loaded and mapped for use in the workflow using the source agnostic tools below 3 data mapping each of the above data products must be standardized and placed on a common mesh for use in a hydrologic model watershed workflow leverages a variety of open source libraries to accomplish this goal wrapping and adapting them to provide a toolset for data mapping the first step of a typical workflow is to use geometric information to compute and define a surface mesh in two dimensions then elevate that mesh to a 2d surface submanifold in 3d using a dem and extrude that mesh to compute a terrain following 3d volume mesh the mesh is refined near the river network to capture the fast changing dynamics in that area given that mesh each of the above datasets are transformed into the correct coordinate system potentially up or down scaled to resolutions appropriate for the mesh and projected onto the mesh labels are generated to map material properties to collections of entities within the mesh each volumetric cell is placed in an element block corresponding to its soil or geologic material type and each face on the top surface of the mesh is placed in a labeled set indicating the land cover type additional labeled sets may be used to indicate target areas for observations wells contributing areas to gages and other application specific usages finally the output of the workflow consists of this mesh file which describes the geometry and topology of the unstructured mesh along with these labels and a set of comma separated values csv files containing the parameter values themselves backends may be written to write these files in formats native to a given simulator 3 1 coordinate reference system transformation and standardization each dataset is provided in its own coordinate reference system crs as described by either well known text wkt format strings or epsg geodetic parameter dataset code watershed workflow uses the proj proj contributors 2021 the library formerly called proj4 python library s crs as its underlying crs object and provides functions for converting this object to similar objects used by a variety of other python libraries including shapely gillies et al 2007 fiona gillies et al 2011 rasterio gillies et al 2013 and cartopy met office 2010 it then provides a series of wrappers to the proj library to change the coordinate system of both shapes and raster objects users can choose to work in whichever crs makes the most sense for their watershed but typically it is recommended to use the crs of the meteorological dataset this is because interpolating from the meteorological data raster onto the unstructured mesh is much more computationally efficient if the mesh and the meteorological data are in the same coordinate system and reprojecting raster data between arbitrary coordinate systems can introduce artifacts and biases 3 2 domain manipulation and scale transformation while boundary domain and hydrography datasets are available from a variety of sources these domains may or may not be appropriate for direct simulation a key consideration is scale watershed workflow wraps a variety of tools to manipulate the domain of interest frequently coarsening it from higher resolution to the resolution desired by the user additionally tools are provided to filter and smooth rasters allowing high resolution rasters to be restricted to the mesh cells in a variety of ways watershed boundaries and river reaches are loaded from shapefiles into a list of polygons and polylines respectively polygons are loaded into a split data structure where overlapping huc boundaries are combined into collections of linestrings this allows the topological and geometric manipulation of these shapes to guarantee consistency of boundaries between hucs ensuring that geometric transformations cannot lose or gain area reaches are processed into a list of tree based data structures with one for each outlet which terminates on or within the watershed boundary this processing is done using a k dimensional tree algorithm on reach endpoints while this is unnecessary for nhdplus datasets which include hydrosequence connectivity information it is helpful for nhd datasets which do not while a tree assumes that rivers only merge as they move downstream this is sufficient for our current use case of models without explicit routing the stream network is used only for mesh refinement and features that are not included in this network can still arise in the surface flow system if the dem resolves them optionally river networks are pruned if they include too few reaches or do not exit the watershed because of this tree network it is straightforward to accumulate and analyze river network properties from reach properties provided by the dataset such as accumulated drainage area or other values these can be used in a workflow to prune or otherwise change the network such as removing all reaches whose contributing areas are less than a given fraction of the total watershed s area given these data structures a user may wish to coarsen from the native resolution to a resolution more tractable for simulation as an example nhd datasets provide watershed boundaries whose shapes consist of points that are often within 10 m of each other using this shape to generate a mesh would result mesh elements that include edges of that same size this is typically too small to be computationally feasible for watershed scale simulations both watershed boundaries and river networks may be simplified using shapely s standard gis based simplify algorithm which coarsens the typical edge length this is done in a way to ensure that the hucs still partition the domain not allowing overlap or disjoint shapes similarly river networks are simplified in a way that preserves the topological relationships intersections of rivers and huc boundaries additionally many raster images are now available at 30 m or higher resolution but this may be too high for full river basin simulations algorithms are wrapped in a standard interface to resample the domain onto a coarser raster some of these are provided from rasterio nearest neighbor median etc and others are provided from scipy gaussian filters and other image processing filters finally a simple unstructured pit filling algorithm based on a marching method may be used on either the primary mesh for finite difference or galerkin finite element methods or a dual mesh for finite volume or mixed finite element methods once a dem has been mapped onto an unstructured mesh this is necessary for kinematic wave approximation solutions of overland flow and reduces the effect of discrete errors introduced by sampling regions with large curvature in the dem 3 3 mesh generation given a watershed boundary and a dem on that boundary a mesh must be formed to cover that domain for use by distributed hydrologic models as the primary target code considered here is an unstructured code we leverage the triangle library shewchuck 1996 to triangulate the map view surface triangle forms delaunay triangulations which are preferred for simulation accuracy and allow user provided refinement functions through the meshpy wrapper which is used to call triangle a c code from python a variety of standard functions are provided including options for refining by triangle area minimum angle and distance from the river network the river network may be directly included within the mesh ensuring that the network exists as edges in the mesh or may be used solely to refine the triangles in the river corridor while optimal meshes for simulation are an area of active research özgen xian et al 2020 it is intuitive to refine the mesh near the river network this results in higher resolution where faster dynamics are occurring and decreases the minimum stream width resolved in an integrated simulation an example surface mesh resulting from this workflow is seen in section 5 finally given a user provided strategy for discretizing soil structure in the vertical capability is provided to extrude the surface triangles in the vertical providing terrain following triangular prism 3d cells the resulting mesh which includes the topology and geometry of this 3d mesh labeled material sets for each soil map unit and bedrock and labeled face sets for each land cover type are written to the exodus ii format sjaardema 1993 3 4 plotting finally we find it is crucial to be able to visualize these watershed products effectively with such complicated workflows data products and geographic information quality imagery makes it much easier for both model users and stakeholders to understand and internalize model results watershed workflow provides a large collection of plotting utilities that leverage matplotlib for base plotting capability and cartopy to ensure watersheds are plotted accurately in their native crs projection and to provide underlying basemaps for geographic and political boundaries defaults and standard styles are used to ensure efficient communication of this information across watersheds and model simulations 4 software design and deployment workflow software is notorious for being one off scripts with poor documentation and software practices this is changing as more complex computational workflows are becoming the norm in science fields where much of the work is in all aspects of model data integration jupyter notebooks have revolutionized the field of model data integration thanks to their iterated development model integrated markdown text and code integrated graphics and overall ease of use perez and granger 2015 however they are also potentially fraught with confusing bugs as they allow for out of order execution unexecuted cells untested code and other pitfalls pimentel et al 2019 grus 2018 motivating the need for community developed best practices rule et al 2019 watershed workflow is designed for use in a two tiered approach to adhere to best practices of using notebooks all underlying functionality including all capabilities described above are implemented in standard python modules this allows testing and other advantages of standard python library development users typically choose to execute the workflow in jupyter notebooks and all examples and demo problems are provided in notebooks these notebooks are expected to be executed in order from a freshly restarted kernel and exactly once while ignoring these conventions is common in development users should take care to ensure these norms are followed with their final product to ensure traceable workflows to ensure a robust and usable capability watershed workflow adopts several software development best practices including version control issue tracking and pull requests through github and continuous integration using github actions to enable tests to be run on new commits and pull requests documentation is placed in restructured text files in code docstrings and jupyter notebook examples and sphinx is used to generate html documentation that is served through a github page because of the fairly complex set of third party libraries users may choose to download a docker container hosted at docker hub that includes layers for jupyter lab all third party libraries and the watershed workflow package itself alternatively the anaconda package manager may be used to directly install dependencies using provided environment files but the exodus ii library must be installed independently instructions for both approaches are in the online documentation finally watershed workflow is designed as much as possible to be extensible specifically the package is easily added to for new data sources and we expect that the list of available sources provided in the distribution will grow over time additionally the current workflow is documented and used with a single unstructured simulator the advanced terrestrial simulator amanzi ats coon et al 2019 2020 however tools are designed to be easily extended to other similar codes we welcome collaborations with teams of users and would support extending the tool for other codes mesh backends and data structures are designed to be where possible agnostic to the final file format and writers could easily be developed for other mesh and data file formats structured backends are also possible 5 a worked example the coweeta hydrologic laboratory to understand a typical workflow developed using this tool it is useful to consider an example here we describe a complete workflow from watershed specification via shapefile to the simulation of hydrologic flow and stream discharge using amanzi ats the simulation is of the coweeta hydrologic laboratory a u s national forest service station in the u s southeast see fig 1 note that this section is itself a complete jupyter notebook and is maintained and updated with the code in the watershed workflow repository see the example in https github com ecoon watershed workflow blob master examples mesh coweeta ipynb the first step of the workflow is to select a collection of data source managers to download each type of data sources are chosen for watershed and stream network shapefiles dem land cover soil properties and meteorological data in this example we use the defaults as described in section 2 selecting nhd hydrography and the 30 m ned dataset the soil structure is as described in section 2 4 these data source managers share a common interface allowing them to be substituted as needed with other data sources given these sources the watershed geometry is specified specifically a shapefile is provided for the watershed boundary and the stream geometry is downloaded from nhd datasets by first detecting which huc this watershed is in then downloading the appropriate file and finally projecting the stream data from the nhd native crs to the target crs given this geometry we then simplify the geometry requiring that all edges in both features are of length greater than 30 m next a flat 2d triangular mesh is built via delaunay triangulation of the watershed boundary data the horizontal resolution is chosen to be variable with triangles refined until their area is less than 1000 m 2 when within 500 m of the stream network and less than 5000 m 2 when at least 1 km from the stream network additionally the mesh is refined to enforce a minimum angle of 32 this helps ensure accuracy of the flow calculation then we download the 1 tile of digital elevation map from the national elevation dataset that covers this domain the dem is smoothed slightly using a gaussian filter to assist in coarsening from the 30 m raster to the larger unstructured mesh prior to interpolation and bilinearly interpolate the elevation raster to the 2d triangulation to form a 3d surface mesh finally the mesh is conditioned to remove internal local minima in the elevation field pit filling the resulting mesh including the elevation field is shown in fig 2 and consists of 16 000 triangles for land surface properties needed in the hydrologic simulation values are chosen for each land cover type as indexed by the nlcd index therefore we must determine the land cover index for each surface grid cell the nlcd raster is resolved on the mesh see fig 3 note that several land cover types are merged into an other index this index consists of those typically small areas that share common hydrologic properties in the model e g water covered or developed land the actual hydrologic model parameters used for each land cover type are provided in the referenced example in this example we use the default soil structure as described in section 2 4 this consists of a bottom most bedrock layer whose upper boundary is determined using the soilgrids 2017 product a geologic layer given by the glhymps dataset and a top most soil layer given by the ssurgo dataset which is contained to the top 2 m of the domain each of these datasets were downloaded and projected onto the mesh the ssurgo map units and porosity and soilgrids depth to bedrock are shown in figs 4 6 next a 3d volumetric mesh is extruded from the surficial mesh a vertical resolution profile is chosen that telescopes from 5 cm at the top surface to 2 m at the bottom of the bedrock with a total formation thickness of 45 m and a total of 20 layers in the vertical this terrain following mesh is labeled according to the above structural model and labeled sets for each of the bedrock geologic formations and soil map units are saved in the resulting exodusii file all integer labels identifying sets of entities in exodus files must be unique this includes boundary faces land cover types and soil structural formations to avoid conflict we renumber all formations according to a common numbering scheme labels are chosen to be 1 the bottom boundary faces of the domain 2 the top surface boundary faces of the domain 3 all other boundary faces side boundaries 4 9 reserved for user provided sets 10 99 land cover side sets typically nlcd indices are used directly 100 geologic layer material ids mapped from glhymps formation id 999 reserved for bedrock 1000 9999 soil layer material ids mapped from ssurgo map unit key the complete set of soil properties for each formation is provided in the referenced example lastly the workflow is used to download sections of the daymet meteorological dataset for use in driving a hydrologic model typically precipitation both rain and snow air temperature relative humidity and incoming radiation or corresponding quantities are needed this data is downloaded and saved into hdf5 files as described above this structured data is interpolated onto the unstructured mesh by the simulation code to save disk space with this complete set of model inputs including the mesh file associated tables of soil and land cover properties and meteorological forcing dataset a simulation is performed here to demonstrate the type of results generated results from running amanzi ats for one year the water year 2020 with no spinup this simulation includes both subsurface and surface flow solving richards and the diffusion wave equations respectively as described in coon et al 2020 and in a similar analysis to shuai et al 2021 land surface processes are represented through evapotranspiration as calculated by the priestley taylor equation priestley and taylor 1972 the first year of this simulation was run on 32 cores of a standard cluster and an image of the resulting soil saturation and ponded depth are shown in fig 7 in an integrated distributed 3d richards equation model such as amanzi ats streams arise naturally from the coupled surface and subsurface flow equations and the complex subsurface structure and properties and land cover result in spatial variations in soil moisture this demonstrates the viability of watershed workflow for providing all needed inputs for integrated hydrologic simulations in a real world example 6 conclusions and future work as integrated hydrologic modeling becomes more commonly used in understanding the water cycle at local to regional scales there is a need for tools allowing the efficient formation of models informed by quality data products that data is increasingly available through a wide range of remote sensing observational networks and synthesis work supported by governmental and academic agencies globally coordinating this data for use in hydrologic models remains a challenge for model users this work aims to bridge the gap between data and models by providing an open source set of workflow tools for developing hydrologic models that are spatially explicit and include the best available data for a given site furthermore it focuses around ensuring that the workflow is self documented and reproducible extensible automated and scalable ensuring that it can be used on large modeling efforts and allowing research teams to efficiently work in many watersheds the availability efficiency reproducibility and extensibility of watershed workflow make it a novel contribution to the integrated distributed hydrologic modeling community previously modelers have done each step of this process in an ad hoc model specific data source specific way for the most part however this process has been done by hand in a one off way by individual researchers or research groups such approaches consume many hours of researcher time each time a new watershed is studied as users of an integrated hydrologic model we find that this tool has taken a job that used to take an experienced researcher days to weeks to execute for a given watershed and allowed it to be completed in minutes to hours as developers of an integrated hydrologic model we find that this tool greatly eases the burden on adopting and modifying such models by both new and experienced users future work on watershed workflow aims to expand the number of data products available in automated form especially through the inclusion of other soil and geologic structure datasets other meteorological datasets and other land surface datasets for quantities such as leaf area index soil moisture and others additionally we hope to better integrate this workflow to individual models adding back ends that better automate input file formation for amanzi ats and other hydrologic simulators from this workflow credit authorship contribution statement ethan t coon conceptualization software writing original draft editing pin shuai software writing review editing declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests ethan coon reports financial support was provided by oak ridge national laboratory ethan coon reports financial support was provided by us department of energy office of science pin shuai reports financial support was provided by us department of energy office of science acknowledgments funding this work was supported by the oak ridge national laboratory s laboratory directed research development ldrd drd lois 8872 and by the u s department of energy biological and ecological research program under the exasheds project software availability software name watershed workflow v1 2 developers ethan coon pin shuai year first official release 2021 hardware requirements pc system requirements windows linux mac program language python3 program size 2 mb availability https github com ecoon watershed workflow license bsd 3 clause documentation user guide api documentation and examples hosted at https ecoon github io watershed workflow 
25508,integrated distributed hydrologic models leverage advances in computational power and data accessibility to improve predictive understanding of the water cycle while impressive advances in this area of environmental modeling have been accomplished such models are still rarely used partially because of difficulty integrating model and data this research describes the release of watershed workflow version 1 2 a new library aiming to automate and enable complex workflows defining inputs to high resolution integrated distributed hydrologic models watershed workflow provides tools enabling the discovery acquisition mapping and coordination of watershed geometry land cover soil properties and meteorological data it enables the construction of unstructured meshes that incorporate this data and provides tools for automating a first simulation on any watershed in the united states we present the design of the workflow tool and describe best practices for its usage culminating in a final example from watershed specification to simulation at the coweeta hydrologic laboratory graphical abstract keywords integrated hydrologic modeling modeling workflow model parameterization and setup data availability data will be made available on request 1 introduction water is a crucial resource required for life yet that resource is threatened globally under a changing climate vorosmarty 2000 predicting and understanding threats to water quality and quantity is greatly benefited by relevant accurate simulations of the water cycle bierkens 2015 the desire to understand the processes and process interactions governing the water cycle over climatalogical time scales has motivated a community push toward so called hyperresolution hydrologic modeling simulations on full river basins the natural unit of simulation of water on land at 100 m resolution or smaller the scale at which macrotopography is resolved and models can capture lateral flow wood et al 2011 bierkens et al 2015 routine application of such simulations remains a goal of the community today meeting this goal requires building from ideas and capabilities in two related modeling communities the integrated hydrologic modeling community e g paniconi and putti 2015 whose models resolve smaller watersheds at the needed resolution and often include lateral flow through 3d representations of richards equation and the regional and global earth system modeling community e g sood and smakhtin 2015 whose models capture the needed extent but at coarser resolution and often with simpler representations of hydrologic flow processes progress toward hyperresolution models enabled by ongoing trends of increasingly available computational resources and high resolution spatially complete data has included several early successes maxwell and condon 2016 yucel et al 2015 ko et al 2019 however such models are still relatively coarse resolution and often ignore or simplify subsurface lateral flow and surface water groundwater interactions furthermore these successes are still few and far between integrated distributed models are not yet widely used at large scales one reason for this is because of the difficulty of parameterizing calibrating and evaluating these models beven and cloke 2012 ebel and loague 2006 parameterizing these models alone requires a large collection of datasets including accurate elevation models land cover maps subsurface structure models soil and plant properties and meteorological forcing data this work seeks to address this need by providing a tool to automate and enable complex workflows defining all needed inputs to integrated hydrologic models on unstructured meshes several previous efforts have developed software to synthesize data for use in hydrologic models gis platforms provide many of the needed algorithms for data mapping and integration and hydrologists have leveraged these tools extensively for years devantier and feldman 1993 maidment 1996 gis based tools have been developed to integrate data with specific models and often function as graphical user interfaces guis for those models olivera et al 2006 granell et al 2010 examples include the regional hydro ecologic simulation system rhessys workflows tool which builds on the ecohydrolib library to provides a python based suite of tools for preparing data for modeling carbon water and nutrient fluxes at the watershed scale tague and band 2004 nativi et al 2013 demonstrates a service based approach for connecting observations from a data archive to the soil and water assessment tool swat hydrologic model gardner et al 2018 developed a suite of python scripts for the proprietary arcgis software which maps raster based datasets into consistent input for the gsflow hydrologic model models which route surface water explicitly like gsflow must ensure that the stream network and dems are consistent and this tool focuses on that mapping process additionally recent u s operational efforts in flood prediction have created workflows and cyberinfrastructure for generating 30 day flood predictions in watersheds across the nation souffront alcantara et al 2018 the national water model used in these products leverages 250 m resolution raster based datasets within a variant of the wrf hydro model powers et al 2017 which includes vertical infiltration and lateral surface routing of these only gardner et al 2018 includes fully distributed representations of the subsurface and provides subsurface soil structure and properties none of these tools include the discrete differential geometry needed for generating unstructured meshes which are often used in high performance computing simulators in the case of integrated hydrologic models that work for unstructured meshes one other workflow tool has been developed and published pihmgis www pihm psu edu is a workflow tool that uses the hydroterre leonard and duffy 2014 archive of synthesized data products to set up simulations for the penn state integrated hydrologic model pihm bhatt et al 2014 pihmgis uses a tightly coupled approach where the workflow tool and hydrologic model share a common data model and geodatabase file format the hydroterre data service synthesizes and redistributes a single set of terrestrial variables and data sources for watersheds at the huc 12 level allowing an automated model setup process this approach is limited in its extensibility to other models other data sources and other strategies for parameterizing the model here we look to address this gap through the release of a new tool watershed workflow watershed workflow consists of a python library that provides crucial capabilities for model parameterization including 1 a common set of interfaces for finding downloading and synthesizing datasets on both the land surface and in the subsurface 2 functionality to integrate and rescale those datasets onto a common unstructured mesh and 3 an api for backends that write these datasets into the computational meshes and the accompanying files needed by integrated distributed hydrologic models example jupyter notebooks are provided that leverage these capabilities to form a reproducible and automated workflow for model experiment driven efforts the tools themselves are largely scale agnostic but the default data sources typically range between 10 m and 1 km resolution and have been used on watersheds ranging from a few square kilometers to full river basins watershed workflow automates the formation of a first simulation anywhere in the united states while providing sufficient tools to allow users to extend or improve that model as data permits including user provided data this workflow was developed to generate inputs for the advanced terrestrial simulator amanzi ats coon et al 2019 2020 but the underlying toolset is designed to be as model agnostic as possible and has been used to develop input files for pflotran lichtner et al 2015 as well conceptually similar codes such as parflow kollet et al 2010 hydrogeosphere brunner and simmons 2012 and pihm qu and duffy 2007 could directly use this tool with the addition of backends that would write their corresponding mesh and data file formats explicitly routed integrated distributed models such as gsflow markstrom et al 2008 could leverage this capability with additions to ensure consistency of the river network and dem gardner et al 2018 while watershed workflow does a similar job as pihmgis they are also significantly different first the tightly coupled nature of pihmgis requires both the data provisioner hydroterre and the hydrologic model pihm to adopt a common data structure while watershed workflow prefers to duplicate data and map between data models as needed trading performance for flexibility second pihmgis is fundamentally a gis tool focused around a gui for enabling point and click access to a modeling platform watershed workflow takes a different approach as a set of tools enabling the development of scripts and notebooks to enable custom yet reproducible models while like pihmgis watershed workflow provides a default model setup it is more focused around providing the needed utilities to allow researchers to automate script and configure their own workflow for their own simulation plan this makes watershed workflow more useful as a tool for enabling extensible research as it allows both more complex workflows and composing this workflow with other python based workflow libraries also as a python based library watershed workflow is easily scripted to enable ensembles of simulations or simulations across many watersheds furthermore all dependencies are open source and freely distributed making it more accessible than many gis based tools in the remainder of this paper we describe the data needs of integrated distributed hydrologic models and how datasets addressing each need are found and downloaded section 2 we show the tools used for mapping that data into a model domain and mesh section 3 we then discuss the deployment and design of the workflow focusing on our approach for a robust and extensible capability section 4 finally we present a worked example showing an example workflow section 5 2 data sources and management watershed workflow s goal of providing a first simulation anywhere in the united states is made possible by a large suite of open data sources provided by governmental agencies and served through many assorted web services unfortunately there is not a single source or data server for all of these and web services differ across agencies therefore a key contribution of this library is the collection and standardization of access to many data repositories in a single package furthermore this suite of tools for reading apis would be useful in extending this workflow for use outside of the u s where datasets are available the tools provided by this workflow are applicable to accomplish this watershed workflow introduces the concept of a source manager which given a hydrologic unit code huc or polygonal shape representing a watershed queries downloads or otherwise acquires a dataset or datasets that cover that watershed unzips the file if necessary and stores the resulting files under a standardized naming convention in a data library to enable reuse reads metadata and properties providing shapes or rasters and metadata and does this via a standardized interface to allow users to swap data sources easily source managers frequently use representational state transfer application programming interfaces rest apis to discover specific downloadable files or directly download the data itself via a get push request in this section we enumerate the diverse datasets needed to inform a single simulation and describe the source manager or managers used to acquire that data 2 1 watershed geometry watershed boundary datasets and hydrography datasets together form the geographic structure of a watershed watershed boundary datasets are typically delineated through the analysis of elevation datasets defining a watershed as all parts of the land surface which drain to a common point typically in the stream network watersheds are hierarchical ranging in scale from small primary watersheds which drain into first order streams to full river basins which drain into an ocean in the united states the united states geological survey usgs formally calculates hydrologic units and identifies them using hydrologic unit codes or hucs which respect this hierarchy huc 2 regions e g the upper colorado river or the tennessee river basin are the largest in areal extent while huc 12 s or sub watersheds are the smallest commonly provided representing on the order of 100 square kilometers watershed workflow uses hucs as an organizing unit for working with data primarily because most hydrologic datasets in the united states are organized by the huc but also because they form physically meaningful domains for simulation should groundwater divides be desired instead of the default surface water divides shapefiles must be provided by the user and otherwise the process is identical the huc containing the given shape is determined automatically and all data is restricted to the provided shape hydrography datasets provide surveys of river networks which form the drainage network of watersheds and are where most of the fast time scale dynamics occur some hydrologic models for instance river routing models dam operations management models and many flood models directly use the river network as their simulation domain while others for instance the class of integrated distributed models described here can use the river network to refine meshes near the rivers and therefore improve resolution where fast dynamics are occurring watershed boundary and hydrography datasets are typically available as gis shapefiles where each watershed boundary or reach is represented as a shape watershed workflow leverages the watershed boundary dataset wbd the national hydrography dataset nhd and nhdplus high resolution nhdplus hr available at multiple resolutions at 1 24 000 scale or better to represent united states watersheds simley and carswell 2009 by default the nhd datasets are used but all resolutions are accessible through the source manager data is discovered through the national map s rest api u s geological survey 2021 which allows querying for data files organized by huc and resolution via http post requests providing direct download urls files are downloaded on first request unzipped and stored in the data library for future use currently files are indexed by 2 digit wbd 4 digit nhdplus hr and 8 digit nhd hucs which serves as the upper limit of the huc levels for request 2 2 digital elevation models digital elevation models or digital terrain models dems or dtms hereafter we use dem to indicate either are necessary to provided elevation of the ground surface the national elevation dataset and ongoing work in the usgs 3d elevation program 3dep provide rasters which cover the united states in 1 degree tiles gesch et al 2009 these data products are available at a variety of scales but are available uniformly across the entire u s at 30 m resolution and across the conterminous u s at 10 m resolution the location and filename which differ across the u s are discovered using the national map s rest api and are downloaded stitched together if needed and returned as a single dataset and metadata profile note that the above nhd nhdplus hr and wbd delineations are based on data from the 3dep program consistent elevation maps and watershed delineations are used to ensure consistent watershed simulations 2 3 land cover given a watershed land cover is required to define surficial processes including runoff through manning s coefficient or other model parameters evapotranspiration through leaf area index or other model inputs and impervious surfaces most distributed hydrologic models link these parameters to a land cover index or plant functional type which correlates with land cover the national land cover database nlcd homer et al 2012 provides a map at 30 m resolution of land cover across the nation at several different time slices this dataset is based on landsat remotely sensed imagery and classifies each pixel into a set of indexed land cover types typical parameter values are chosen from the literature and associated with each land cover type the nlcd raster is a single 16 gb file download that covers the conterminous u s and additional rasters for each for alaska puerto rico and hawaii hosted at static urls the raster is downloaded unzipped and stored in the data library on first request watershed workflow in addition to standard capabilities required below provides colormaps and labels for the various nlcd indices allowing for plotting using standard nlcd colors 2 4 subsurface structure and properties perhaps the most uncertain yet important parameters required by an integrated hydrologic model based on richards equation are the subsurface structure and properties soil horizons depth to bedrock and soil and geologic properties e g permeability porosity and water retention curves control the rate of infiltration water storage and other key processes of the water cycle these are also the most difficult properties to attain in standard ways across basins rarely are they observable through remote sensing leaving ground based surveys direct sampling and other local measurement approaches to inform model parameters as a result integrated distributed hydrologic models are often limited by the availability of accurate subsurface structure and parameter data fortunately synthetic aggregated datasets of soil structure and properties are becoming more common dai et al 2019 and at least for the top few meters of soil are relatively well constrained depth to bedrock and deeper information is correspondingly harder to find but global products are available dai et al 2019 modelers must choose both what products to build from and also at what resolution and structural complexity reasonable modeling studies might choose simple models and few parameters or complex models with varying horizon thicknesses and properties as fits the modeling question being asked in many watersheds individual soil pits or boreholes might provide extremely detailed information in exchange for being localized in space individual users should incorporate this data where possible here following the general concept of watershed workflow we provide a workflow defining a default soil structure that can be constructed everywhere but also provide the ability to use other datasets as chosen by the user in this way we can ensure that there is a well posed problem that can be automatically formed everywhere in the conterminous u s however this may be either too complex by introducing too many free parameters for calibration for instance or too simple by not considering important fractured bedrock or other subsurface formations for a given modeling study therefore we emphasize that it is crucial for a user to choose subsurface structure and properties carefully and understand the sensitivity of their result to these choices 2 4 1 global hydrology maps products intended for earth system models such as global hydrology maps version 2 0 glhymps huscroft et al 2018a provide coarse resolution information of aggregated properties providing a single porosity and permeability for all soil and geology including fractured bedrock above the bedrock glhymps 2 0 builds on a high resolution lithology map that includes fine and coarse sediments sedimentary rocks weathered sediments bedrock and improvements relative to the original product in differentiating consolidated and unconsolidated sediments glhymps is available through a data doi huscroft et al 2018b but requires a web form to be completed out of respect to the author we therefore require users to manually download this dataset rather than bypassing the form while glhymps is complete in the information it provides it is not sufficient as it does not include water retention curves through van genuchten or other parameters 2 4 2 nrcs soils more detailed survey products such as the u s national resources conservation service s web soil survey ssurgo and statsgo2 datasets provide near surface soil information soil survey staff 2021 these products are based on extensive u s and state geological surveys and cover the entirety of the conterminous u s soil survey files identify a set of map unit shapefiles that cover the ground identifying multiple components of the soil in each map unit and multiple soil horizons in each component the soil survey includes porosity permeability bulk density and sand silt and clay compositional percentages for several horizons in each component watershed workflow provides tools to average these values both vertically across horizons and then horizontally across components by area fraction to determine a single set of properties for each map unit note that permeabilities are averaged in log space then watershed workflow calls the rosetta version 3 beta zhang and schaap 2017 model for pedotransfer functions allowing the calculation of water retention curves in the form of van genuchten parameters van genuchten 1980 as a function of these texture properties the nrcs web soil survey provides several ways to access its data here we leverage the sql based query structure of the provided web tools natural resources conservation service 2018 these allow users to post requests based on a bounding box of the area of interest downloading collections of shapefiles that include map unit keys these files along with corresponding sql queries for soil properties by map unit key are saved to disk and potential used to color a raster we note that the gridded gssurgo product is available by state this product while potentially useful to users is served in a cloud based service whose software toolkit packages require purchased access while a user may manually click through links to download gssurgo it is not possible for an automated workflow to discover the url meaning that the rest api is more robust for an automated application watershed workflow does allow the use of gssurgo or other raster files but the user must download them manually while these datasets are some of the best information available the dataset as a whole is limited to the top two meters and is not complete in that some formations do not have sufficient data to infer all needed soil properties where these datasets are insufficient by default the above glhymps data is used in the near surface as well as in the deeper geologic layer 2 4 3 soilgrids finally the soilgrids project takes a different approach toward soil properties in hengl et al 2017 a machine learning based approach was used to infer soil properties from a wide range of surface climatological and other properties the neural network was trained using over 150 000 soil profile observations and over 150 remotely sensed observations assumed to covary with soil properties the resulting neural network is used to predict soil properties at seven soil layers globally at a variety of resolutions the result is a dataset that is complete globally but is also a field for each property as opposed to the geologic unit approach of glhymps and nrcs soils datasets using such a field based approach is not currently supported by watershed workflow but the structural aspects of soilgrids are still useful specifically soilgrids 2017 includes a depth to bedrock field that can be used to provide the structure of the bottommost layer of simulations soilgrids 2017 depth to bedrock field is downloaded globally at 250 m resolution as a single raster shangguan et al 2017 2 5 meteorological data given surface and subsurface structural data and properties the crucial remaining model input is meteorological data typically including precipitation air temperature relative humidity and radiation fluxes these variables are used to drive the transient conditions of a hydrologic model determining water sources and sinks e g precipitation evapotranspiration etc daymet version 4 thornton et al 2020 provides daily meteorological data at 1 km resolution across the entirety of north america it synthesizes daily weather stations datasets from the global historical climate network daily database durre et al 2010 and spatially and temporally interpolates to create a gridded product given the model domain watershed workflow leverages the daymet rest api services to download for each year and variable a file containing all pixel values in a bounding box covering the model domain the library then merges these into a single file for use by the model code this is typically preferred to interpolating the gridded product directly onto the mesh due to file size concerns a hyperresolution simulation may consist of millions of surface cells saving a data point for every cell at every day of a given simulation may be prohibitively expensive instead it is expected that the interpolation on the mesh can be handled inside of a simulator from the raster the resulting data is saved in either netcdf4 or hdf5 file formats for use by the simulator 2 6 user provided data finally it is recognized that these data products while sufficient to generate a first model may not be the first choice for a given watershed problem or resolution in addition to being designed to be extensible see section 4 generic source managers are provided for using user provided files including both shapefile based products e g watershed boundary and raster based products e g dem these files can be loaded and mapped for use in the workflow using the source agnostic tools below 3 data mapping each of the above data products must be standardized and placed on a common mesh for use in a hydrologic model watershed workflow leverages a variety of open source libraries to accomplish this goal wrapping and adapting them to provide a toolset for data mapping the first step of a typical workflow is to use geometric information to compute and define a surface mesh in two dimensions then elevate that mesh to a 2d surface submanifold in 3d using a dem and extrude that mesh to compute a terrain following 3d volume mesh the mesh is refined near the river network to capture the fast changing dynamics in that area given that mesh each of the above datasets are transformed into the correct coordinate system potentially up or down scaled to resolutions appropriate for the mesh and projected onto the mesh labels are generated to map material properties to collections of entities within the mesh each volumetric cell is placed in an element block corresponding to its soil or geologic material type and each face on the top surface of the mesh is placed in a labeled set indicating the land cover type additional labeled sets may be used to indicate target areas for observations wells contributing areas to gages and other application specific usages finally the output of the workflow consists of this mesh file which describes the geometry and topology of the unstructured mesh along with these labels and a set of comma separated values csv files containing the parameter values themselves backends may be written to write these files in formats native to a given simulator 3 1 coordinate reference system transformation and standardization each dataset is provided in its own coordinate reference system crs as described by either well known text wkt format strings or epsg geodetic parameter dataset code watershed workflow uses the proj proj contributors 2021 the library formerly called proj4 python library s crs as its underlying crs object and provides functions for converting this object to similar objects used by a variety of other python libraries including shapely gillies et al 2007 fiona gillies et al 2011 rasterio gillies et al 2013 and cartopy met office 2010 it then provides a series of wrappers to the proj library to change the coordinate system of both shapes and raster objects users can choose to work in whichever crs makes the most sense for their watershed but typically it is recommended to use the crs of the meteorological dataset this is because interpolating from the meteorological data raster onto the unstructured mesh is much more computationally efficient if the mesh and the meteorological data are in the same coordinate system and reprojecting raster data between arbitrary coordinate systems can introduce artifacts and biases 3 2 domain manipulation and scale transformation while boundary domain and hydrography datasets are available from a variety of sources these domains may or may not be appropriate for direct simulation a key consideration is scale watershed workflow wraps a variety of tools to manipulate the domain of interest frequently coarsening it from higher resolution to the resolution desired by the user additionally tools are provided to filter and smooth rasters allowing high resolution rasters to be restricted to the mesh cells in a variety of ways watershed boundaries and river reaches are loaded from shapefiles into a list of polygons and polylines respectively polygons are loaded into a split data structure where overlapping huc boundaries are combined into collections of linestrings this allows the topological and geometric manipulation of these shapes to guarantee consistency of boundaries between hucs ensuring that geometric transformations cannot lose or gain area reaches are processed into a list of tree based data structures with one for each outlet which terminates on or within the watershed boundary this processing is done using a k dimensional tree algorithm on reach endpoints while this is unnecessary for nhdplus datasets which include hydrosequence connectivity information it is helpful for nhd datasets which do not while a tree assumes that rivers only merge as they move downstream this is sufficient for our current use case of models without explicit routing the stream network is used only for mesh refinement and features that are not included in this network can still arise in the surface flow system if the dem resolves them optionally river networks are pruned if they include too few reaches or do not exit the watershed because of this tree network it is straightforward to accumulate and analyze river network properties from reach properties provided by the dataset such as accumulated drainage area or other values these can be used in a workflow to prune or otherwise change the network such as removing all reaches whose contributing areas are less than a given fraction of the total watershed s area given these data structures a user may wish to coarsen from the native resolution to a resolution more tractable for simulation as an example nhd datasets provide watershed boundaries whose shapes consist of points that are often within 10 m of each other using this shape to generate a mesh would result mesh elements that include edges of that same size this is typically too small to be computationally feasible for watershed scale simulations both watershed boundaries and river networks may be simplified using shapely s standard gis based simplify algorithm which coarsens the typical edge length this is done in a way to ensure that the hucs still partition the domain not allowing overlap or disjoint shapes similarly river networks are simplified in a way that preserves the topological relationships intersections of rivers and huc boundaries additionally many raster images are now available at 30 m or higher resolution but this may be too high for full river basin simulations algorithms are wrapped in a standard interface to resample the domain onto a coarser raster some of these are provided from rasterio nearest neighbor median etc and others are provided from scipy gaussian filters and other image processing filters finally a simple unstructured pit filling algorithm based on a marching method may be used on either the primary mesh for finite difference or galerkin finite element methods or a dual mesh for finite volume or mixed finite element methods once a dem has been mapped onto an unstructured mesh this is necessary for kinematic wave approximation solutions of overland flow and reduces the effect of discrete errors introduced by sampling regions with large curvature in the dem 3 3 mesh generation given a watershed boundary and a dem on that boundary a mesh must be formed to cover that domain for use by distributed hydrologic models as the primary target code considered here is an unstructured code we leverage the triangle library shewchuck 1996 to triangulate the map view surface triangle forms delaunay triangulations which are preferred for simulation accuracy and allow user provided refinement functions through the meshpy wrapper which is used to call triangle a c code from python a variety of standard functions are provided including options for refining by triangle area minimum angle and distance from the river network the river network may be directly included within the mesh ensuring that the network exists as edges in the mesh or may be used solely to refine the triangles in the river corridor while optimal meshes for simulation are an area of active research özgen xian et al 2020 it is intuitive to refine the mesh near the river network this results in higher resolution where faster dynamics are occurring and decreases the minimum stream width resolved in an integrated simulation an example surface mesh resulting from this workflow is seen in section 5 finally given a user provided strategy for discretizing soil structure in the vertical capability is provided to extrude the surface triangles in the vertical providing terrain following triangular prism 3d cells the resulting mesh which includes the topology and geometry of this 3d mesh labeled material sets for each soil map unit and bedrock and labeled face sets for each land cover type are written to the exodus ii format sjaardema 1993 3 4 plotting finally we find it is crucial to be able to visualize these watershed products effectively with such complicated workflows data products and geographic information quality imagery makes it much easier for both model users and stakeholders to understand and internalize model results watershed workflow provides a large collection of plotting utilities that leverage matplotlib for base plotting capability and cartopy to ensure watersheds are plotted accurately in their native crs projection and to provide underlying basemaps for geographic and political boundaries defaults and standard styles are used to ensure efficient communication of this information across watersheds and model simulations 4 software design and deployment workflow software is notorious for being one off scripts with poor documentation and software practices this is changing as more complex computational workflows are becoming the norm in science fields where much of the work is in all aspects of model data integration jupyter notebooks have revolutionized the field of model data integration thanks to their iterated development model integrated markdown text and code integrated graphics and overall ease of use perez and granger 2015 however they are also potentially fraught with confusing bugs as they allow for out of order execution unexecuted cells untested code and other pitfalls pimentel et al 2019 grus 2018 motivating the need for community developed best practices rule et al 2019 watershed workflow is designed for use in a two tiered approach to adhere to best practices of using notebooks all underlying functionality including all capabilities described above are implemented in standard python modules this allows testing and other advantages of standard python library development users typically choose to execute the workflow in jupyter notebooks and all examples and demo problems are provided in notebooks these notebooks are expected to be executed in order from a freshly restarted kernel and exactly once while ignoring these conventions is common in development users should take care to ensure these norms are followed with their final product to ensure traceable workflows to ensure a robust and usable capability watershed workflow adopts several software development best practices including version control issue tracking and pull requests through github and continuous integration using github actions to enable tests to be run on new commits and pull requests documentation is placed in restructured text files in code docstrings and jupyter notebook examples and sphinx is used to generate html documentation that is served through a github page because of the fairly complex set of third party libraries users may choose to download a docker container hosted at docker hub that includes layers for jupyter lab all third party libraries and the watershed workflow package itself alternatively the anaconda package manager may be used to directly install dependencies using provided environment files but the exodus ii library must be installed independently instructions for both approaches are in the online documentation finally watershed workflow is designed as much as possible to be extensible specifically the package is easily added to for new data sources and we expect that the list of available sources provided in the distribution will grow over time additionally the current workflow is documented and used with a single unstructured simulator the advanced terrestrial simulator amanzi ats coon et al 2019 2020 however tools are designed to be easily extended to other similar codes we welcome collaborations with teams of users and would support extending the tool for other codes mesh backends and data structures are designed to be where possible agnostic to the final file format and writers could easily be developed for other mesh and data file formats structured backends are also possible 5 a worked example the coweeta hydrologic laboratory to understand a typical workflow developed using this tool it is useful to consider an example here we describe a complete workflow from watershed specification via shapefile to the simulation of hydrologic flow and stream discharge using amanzi ats the simulation is of the coweeta hydrologic laboratory a u s national forest service station in the u s southeast see fig 1 note that this section is itself a complete jupyter notebook and is maintained and updated with the code in the watershed workflow repository see the example in https github com ecoon watershed workflow blob master examples mesh coweeta ipynb the first step of the workflow is to select a collection of data source managers to download each type of data sources are chosen for watershed and stream network shapefiles dem land cover soil properties and meteorological data in this example we use the defaults as described in section 2 selecting nhd hydrography and the 30 m ned dataset the soil structure is as described in section 2 4 these data source managers share a common interface allowing them to be substituted as needed with other data sources given these sources the watershed geometry is specified specifically a shapefile is provided for the watershed boundary and the stream geometry is downloaded from nhd datasets by first detecting which huc this watershed is in then downloading the appropriate file and finally projecting the stream data from the nhd native crs to the target crs given this geometry we then simplify the geometry requiring that all edges in both features are of length greater than 30 m next a flat 2d triangular mesh is built via delaunay triangulation of the watershed boundary data the horizontal resolution is chosen to be variable with triangles refined until their area is less than 1000 m 2 when within 500 m of the stream network and less than 5000 m 2 when at least 1 km from the stream network additionally the mesh is refined to enforce a minimum angle of 32 this helps ensure accuracy of the flow calculation then we download the 1 tile of digital elevation map from the national elevation dataset that covers this domain the dem is smoothed slightly using a gaussian filter to assist in coarsening from the 30 m raster to the larger unstructured mesh prior to interpolation and bilinearly interpolate the elevation raster to the 2d triangulation to form a 3d surface mesh finally the mesh is conditioned to remove internal local minima in the elevation field pit filling the resulting mesh including the elevation field is shown in fig 2 and consists of 16 000 triangles for land surface properties needed in the hydrologic simulation values are chosen for each land cover type as indexed by the nlcd index therefore we must determine the land cover index for each surface grid cell the nlcd raster is resolved on the mesh see fig 3 note that several land cover types are merged into an other index this index consists of those typically small areas that share common hydrologic properties in the model e g water covered or developed land the actual hydrologic model parameters used for each land cover type are provided in the referenced example in this example we use the default soil structure as described in section 2 4 this consists of a bottom most bedrock layer whose upper boundary is determined using the soilgrids 2017 product a geologic layer given by the glhymps dataset and a top most soil layer given by the ssurgo dataset which is contained to the top 2 m of the domain each of these datasets were downloaded and projected onto the mesh the ssurgo map units and porosity and soilgrids depth to bedrock are shown in figs 4 6 next a 3d volumetric mesh is extruded from the surficial mesh a vertical resolution profile is chosen that telescopes from 5 cm at the top surface to 2 m at the bottom of the bedrock with a total formation thickness of 45 m and a total of 20 layers in the vertical this terrain following mesh is labeled according to the above structural model and labeled sets for each of the bedrock geologic formations and soil map units are saved in the resulting exodusii file all integer labels identifying sets of entities in exodus files must be unique this includes boundary faces land cover types and soil structural formations to avoid conflict we renumber all formations according to a common numbering scheme labels are chosen to be 1 the bottom boundary faces of the domain 2 the top surface boundary faces of the domain 3 all other boundary faces side boundaries 4 9 reserved for user provided sets 10 99 land cover side sets typically nlcd indices are used directly 100 geologic layer material ids mapped from glhymps formation id 999 reserved for bedrock 1000 9999 soil layer material ids mapped from ssurgo map unit key the complete set of soil properties for each formation is provided in the referenced example lastly the workflow is used to download sections of the daymet meteorological dataset for use in driving a hydrologic model typically precipitation both rain and snow air temperature relative humidity and incoming radiation or corresponding quantities are needed this data is downloaded and saved into hdf5 files as described above this structured data is interpolated onto the unstructured mesh by the simulation code to save disk space with this complete set of model inputs including the mesh file associated tables of soil and land cover properties and meteorological forcing dataset a simulation is performed here to demonstrate the type of results generated results from running amanzi ats for one year the water year 2020 with no spinup this simulation includes both subsurface and surface flow solving richards and the diffusion wave equations respectively as described in coon et al 2020 and in a similar analysis to shuai et al 2021 land surface processes are represented through evapotranspiration as calculated by the priestley taylor equation priestley and taylor 1972 the first year of this simulation was run on 32 cores of a standard cluster and an image of the resulting soil saturation and ponded depth are shown in fig 7 in an integrated distributed 3d richards equation model such as amanzi ats streams arise naturally from the coupled surface and subsurface flow equations and the complex subsurface structure and properties and land cover result in spatial variations in soil moisture this demonstrates the viability of watershed workflow for providing all needed inputs for integrated hydrologic simulations in a real world example 6 conclusions and future work as integrated hydrologic modeling becomes more commonly used in understanding the water cycle at local to regional scales there is a need for tools allowing the efficient formation of models informed by quality data products that data is increasingly available through a wide range of remote sensing observational networks and synthesis work supported by governmental and academic agencies globally coordinating this data for use in hydrologic models remains a challenge for model users this work aims to bridge the gap between data and models by providing an open source set of workflow tools for developing hydrologic models that are spatially explicit and include the best available data for a given site furthermore it focuses around ensuring that the workflow is self documented and reproducible extensible automated and scalable ensuring that it can be used on large modeling efforts and allowing research teams to efficiently work in many watersheds the availability efficiency reproducibility and extensibility of watershed workflow make it a novel contribution to the integrated distributed hydrologic modeling community previously modelers have done each step of this process in an ad hoc model specific data source specific way for the most part however this process has been done by hand in a one off way by individual researchers or research groups such approaches consume many hours of researcher time each time a new watershed is studied as users of an integrated hydrologic model we find that this tool has taken a job that used to take an experienced researcher days to weeks to execute for a given watershed and allowed it to be completed in minutes to hours as developers of an integrated hydrologic model we find that this tool greatly eases the burden on adopting and modifying such models by both new and experienced users future work on watershed workflow aims to expand the number of data products available in automated form especially through the inclusion of other soil and geologic structure datasets other meteorological datasets and other land surface datasets for quantities such as leaf area index soil moisture and others additionally we hope to better integrate this workflow to individual models adding back ends that better automate input file formation for amanzi ats and other hydrologic simulators from this workflow credit authorship contribution statement ethan t coon conceptualization software writing original draft editing pin shuai software writing review editing declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests ethan coon reports financial support was provided by oak ridge national laboratory ethan coon reports financial support was provided by us department of energy office of science pin shuai reports financial support was provided by us department of energy office of science acknowledgments funding this work was supported by the oak ridge national laboratory s laboratory directed research development ldrd drd lois 8872 and by the u s department of energy biological and ecological research program under the exasheds project software availability software name watershed workflow v1 2 developers ethan coon pin shuai year first official release 2021 hardware requirements pc system requirements windows linux mac program language python3 program size 2 mb availability https github com ecoon watershed workflow license bsd 3 clause documentation user guide api documentation and examples hosted at https ecoon github io watershed workflow 
25509,coastal and riverine flooding is one of the most common environmental hazards that affect billions of people worldwide a coupled hydrologic and coastal storm surge simulation is required to develop an improved understanding of the individual and collective mechanisms that can cause flooding within watersheds these simulations are dependent on an accurate digital elevation model dem however it is a challenge to include numerical model resolution as fine as contemporary dems due to the enormous computational cost therefore significant vertical features vfs such as roadbeds levees railroads and natural ridges must be identified and considered in developing the model representation of the dem since the vfs can affect flow propagation pyvf is an open source program to extract significant vfs from a high resolution bare earth lidar derived dem automatically this paper introduces the methods and shows the automated extraction of vfs for a coastal urban mountain and beach area keywords vertical features automated extraction digital elevation model surface hydrology compound flooding storm surge 1 introduction flooding in coastal regions can be caused by i riverine flooding from extreme rainfall runoff ii coastal surges driven by tropical cyclones or strong onshore winds or iii a compounding of both processes occurring simultaneous or in close succession bevacqua et al 2019 bilskie and hagen 2018 santiago collazo et al 2019 zheng et al 2013 2014 accurate representation of the bathymetry i e water depth topography i e land elevation and inundation barriers e g levees raised roadways and natural ridges is fundamental to accurately simulating floods bilskie 2012 dube et al 2010 gallien et al 2018 westerink et al 2008 however a critical challenge for predictive flood modeling is the geometric complexities of the terrain gallien et al 2014 xie et al 2019 model performance in low gradient coastal regions is particularly susceptible to inaccurate topographical representation within computational models since the land elevation variation can be as few centimeters colby and dobson 2010 van de sande et al 2012 a prerequisite to the numerical flood model is the generation of high quality structured or unstructured meshes that permit an accurate representation of complex domain geometry finite element and volume based models typically employ unstructured triangular meshes that are capable of resolving complex coastal domains chen et al 2003 ham et al 2005 namin et al 2004 pain et al 2005 shen et al 2006 xie et al 2019 yoon and kang 2004 the unstructured triangular mesh allows users to refine the mesh in critical areas and use coarse resolution in less sensitive regions such as in deeper bathymetries while maintaining a given computational cost bern and plassmann 2000 hagen et al 2001 kim et al 2014 marsh et al 2018 mcguigan et al 2015 in recent years airborne light detection and ranging lidar technology has grown more precise and high resolution 10 m data have become increasingly available for supporting multi dimensional flood modeling research bateset al 2003 noaa 2007 although the increasing terrain data resolution may permit an improved description of the bare earth topography the unstructured meshes are restricted to a minimum resolution to minimize computational cost and numerical instabilities e g courant friedrichs lewy condition bilskie et al 2015 limiting the resolution results in smoothing out the elevation of natural barriers and anthropogenic features e g levees and raised roadbeds which can alter the path of simulated inundation and result in an inaccurate solution bilskie et al 2015 horritt and bates 2002 kim et al 2014 sofia et al 2014 purvis et al 2008 recognized this shortcoming in model resolution and manually digitized significant terrain features from uk ordnance survey maps to include their peak elevation within the lisflood fp inundation model bunya et al 2010 applied the federal levees defined by usace mvn surveys and the road and railroad crown heights taken from atlas lidar surveys into high resolution adcirc hurricane models they concluded that the model accuracy is dependent on the high level grid resolution of the terrain surface their efforts demonstrated that including long and narrow raised features are critical to build accurate flood inundation models however there are limitations in these methods these limitations include the amount of hand digitizing and editing low accuracy in the horizontal placement of crest elevations a high number of person hours and the potential for errors in sum these findings motivate the automated extraction of significantly raised linear features i e vertical features from high resolution terrain data vertical features vf are raised linear features such as roadbeds railroads levees floodwalls and natural features that can block flow conduct flow and redirect flow the wetting and drying of an inundation front may differ depending on the unstructured mesh model with or without vertical features bilskie et al 2015 there have been many studies aimed at automatically extracting ridge features from high resolution topographic data roberts 2004 applied a method to take the point where the maximum gradient in a region is steep enough as vf points from lidar point cloud data then anomalous points were manually cleaned and employed as raised feature points these raised feature points were incorporated into the coastal flooding analysis coggin 2008 developed an automatic method for extracting vfs from watershed boundaries generated from lidar dem data for inclusion in an adcirc finite element mesh with special parameters bilskie et al 2015 followed and expanded the work of coggin 2008 and roberts 2004 to extract vfs and fix them as polylines in an unstructured mesh that was employed in the simulation of shallow water hydrodynamics for hurricane katrina the research results of coggin 2008 and bilskie et al 2015 2020 show that simulations of flood extents and depths are more accurate when using an unstructured mesh that includes vfs vfs play an essential role in the simulation of storm surge and hydrological flow routing in hydrology watersheds typically define land units which have boundaries delineated from topographic high points edwards et al 2015 zhang et al 2016 obtained ridges by extracting the watersheds of the river network since the watersheds correspond to the ridges in many real scenarios most hydrological models e g swat annagnps hspf gssha delineate watershed boundaries and topographic characteristics of watersheds using dems parajuli and ouyang 2013 wang et al 2011 predicted the spatial patterns of water yield by a swat road model and a swat noroad model the conclusion is that hydrologic effects of raised roads are important for accurately simulating runoff within a low relief watershed alzahrani 2017 manually added vfs into a hec ras 2d model and kept water away from the dry side of a vf until the water surface elevation was higher than the vf s elevation griffiths 2010 represented vfs as embankment arcs that alter the overland surface flow characteristics of a watershed along with grid cell edges or elevated grid cells to simulate overbank flow in the gssha model thus it is well documented in previous efforts that the inclusion of vertical terrain features is critical for accurate surface water flow in coastal and riverine floodplains most techniques for extracting raised linear features from lidar data are concentrated on breaklines coggin 2008 in surface modeling breaklines are linear features used to represent a sudden or abrupt change in the terrain s smoothness and continuity abdullah 2017 or an otherwise string of connected points that should be honored by the data triangulation they are commonly extracted from airborne laser scanner als point cloud data digital orthophotos or ground surveyed cross sections requiring cumbersome manual work bodoque et al 2016 briese 2004 brugelmann 2000 wang et al 2018 yang et al 2016 breaklines are often located at the toe and shoulder of levees and highways rather than along the highest point of the protruding feature where the vfs should be located in unstructured mesh generation vertical features are breaklines considered as polylines or series of edges connected by triangular elements vertical features are used as a reference to form breaklines in unstructured mesh generation to improve the terrain description in contrast to breaklines in surface modeling vertical features are extracted from dems rather than lidar point clouds they have special requirements including being tall enough to affect flood propagation long enough to span the edges of at least one element and having an appropriate spatial distribution for the horizontal scale of the unstructured grid to be designed bilskie et al 2015 coggin 2008 the work of bilskie et al 2015 and coggin 2008 show the usefulness of the inclusion of vf for flood models however they do point to some shortcomings that should be overcome first their methods require over a dozen parameters that should be manuall adjusted based on the surrounding terrain bilskie et al 2015 states that future work should focus on parameter section for varying landscapes and vf extraction sensitity to hydrodynamic model resolution in addition since their vf extraction methods begin with watershed boundaries the final vf lines reside along dem cell edges rather down the centroid we aim to address these shortcomings through a revised vf extraction algorithm that minimizes the number of parameters while considering various topographic landscapes from mountains to coastal regions this prevous research led to developing an automated vf delineation algorithm method based on a lidar derived dem for inclusion in flood inundation models section 2 describes the capability of the developed algorithm and software called pyvf and how vfs are delineated in section 3 examples of extracted vfs are presented for four study areas in different types of geography to illustrate the capacity of pyvf for a variety of terrains section 4 contains a discussion of the pyvf method and section 5 summarizes the research and conclusions 2 methods pyvf is written as a python version 2 7 script to take advantage of arcgis functions through the arcpy module all of the geoprocessing functions of arcgis such as data analysis data conversion and data management can be accessed through python using arcpy which is a python site package that integrates arcgis with python esri 2016 pyvf also includes numpy walt et al 2011 for data manipulation and the python standard math library lundh 2001 the design of pyvf is divided into four main tasks 1 batch processing sub dems 2 vf target recognition 3 vf delineation and 4 post processing of potential vfs pyvf produces vfs as a shapefile and two raster images with attributes described in sections 2 3 and 2 4 pyvf like the method proposed in coggin 2008 and bilskie et al 2015 aims to extract vfs that are high enough and long enough their method evaluates the relative elevation by comparing the height of a vertex on the watershed boundary line with the height at two perpendicular distances from the vertex according to the height difference each vertex is declared as significant or continue if the significant vertex is below a ratio e g 35 the watershed line is eliminated the extracted vfs from this previous method are a subset of the watershed boundaries the path of the watershed boundary is along the edge of the dem cells and so are the extracted vfs this can lead to large height errors when placing mesh nodes on narrow vfs and coarser dems to reduce the height error in the model and provide more meaningful parameters an iterative increasing size moving window method is employed to search for potential vfs cells by calculating the height trend of all dem cells in eight directions the extracted vfs with meaningful attributes are along the center of the cells the flowchart in fig 1 provides a general description of the pyvf algorithm the two inputs of the algorithm are a dem and a target unstructured mesh element size es the es also can be replaced with a constant value first the dem is split into sub dem tiles to efficiently utilize computer memory useful for large domains with small cell sizes by batch processing next two rasters which have the value of r and d h for all tiles are extracted through the target recognition tr method two thresholds of r and d h m i n r and m i n d h are used for reducing the vf cell candidates to avoid weak vf cells and extraneous cell noise they are defined through the 1 5 x iqr interquartile range rule and explained in the following section the dem also serves as the input to watershed boundary delineation the extracted watershed boundaries are considered ridges the reduced vf cells that coincide with watershed boundaries are potential vf raster then the potential vf cells continue to be deleted to create a linear feature of a single cell width by a thinning approach this is in preparation to convert the linear raster to vertical feature polylines finally post processing is performed based on the constraints of the individual modeling study that will utilize the extracted vfs for example when vertical features are applied to an unstructured mesh model the element size is required to determine the appropriate length of the final vertical feature polylines 2 1 batch processing the high resolution raster datasets across large domains result in large amounts of data and processing challenges due to the computer memory limitation batch processing is a common method for overcoming memory limitations there are two methods provided for dividing the large raster into tiles tiles are read into memory processed and written to disk one by one until the task is complete the two methods have a common purpose to split the dem so that the data volume of each sub dem region can reside in memory and minimize discontinuities or gaps in the final mosaicked raster image the first batch processing approach is based on customized rectangular polygons shapefile the grid polygons used in this research are a net of square polygons the large region dem is clipped into sub dem by the polygon shapefile it should be noted that the side length of each tile in the polygon shapefile must be a multiple of the dem cell size otherwise there will be gaps between each tile the sub dem region is called the recognition area as a minimum unit for the following target recognition method another dem decomposition approach is dividing the dem into many tiles from the upper left corner according to the dem coordinate and using coordinates i and j with 0 0 denoting the upper left corner of the dem the minimum unit i e the recognition region is a number of rectangular i j tiles where i is the number of rows j is the number of columns each tile is processed in the target recognition algorithm individually one by one this method is better for manipulating the entire dem region without creating a grid polygon the target recognition method in this research applies a moving window approach since the raster edge cells e g the cells in the top and bottom rows and the left and right columns do not have sufficient neighbors for example in fig 2 the center of the moving window is on the cell at the left edge the lack of data in the moving window will affect the calculation if the edge cells of each sub dem cannot be calculated as non edge cells apparent discontinuities will result along the edges of each tile in the final mosaicked image hence a buffer distance around the recognition area is determined to define an effective area effective areas polygon tiles at a specified distance around recognition areas are determined to solve this problem the role of effective areas is that when the moving window traverses each recognition area there is no null value inside the window range the following section will introduce the method of searching potential vf cells from each recognition area 2 2 target recognition the vf cell recognition method in this research adopts a circular moving window approach which has an advantage in directional uniformity over a square moving window chang and sinha 2007 chang et al 1998 koike et al 1995 the centroid of the circular moving window is placed at the center of each cell the circular moving window is divided into eight sectors representing eight directions e g a1 is north b1 is northeast fig 3 a the target recognition method traverses each target dem cell and aims to find the highest cells in at least one symmetry direction the higher elevation cells are identified as potential vf candidates two parameters r and d h are calculated by the target recognition method the first parameter r is the radius of the moving window moving windows can use a fixed size or iteratively increasing size casas et al 2012 proposed a method for assessing the structural integrity of levees this method relies on the slope calculation based on a 3 by 3 moving window however small fixed moving windows are not always the best choice lin et al 2013 in this research an iteratively increasing circular moving window is applied hence the circular moving window expands as the r value increases the initial value of r is 1 5 times the cell size the second parameter d h is the difference in height between the value of the checked cell with that of the lowest cell for example fig 3 b shows an assumed cross profile of a north south vf and the check cell is located on the vf to the west of check cell the lowest cell can be found when r is equal to 3 5 cell size the r in the west and east direction are represented by r w and r e the d h in the west direction is d h w in the east of check cell the lowest cell is found when r is equal to 5 5 cell size the d h in the west direction is d h e a different value of r will lead to a different value of d h this example illustrates that a variable window size is more appropriate for extracting vfs than a constant size since the width of the vfs is not fixed for example fig 4 shows a hypothetical large scale cross section spanning multiple raised features to illustrate the desired target cell location for this study because the actual terrain is very complex there are many possible elevation relationships between the potential vf cells and their surrounding points on the same cross section 1 the transverse profile is approximately symmetric and vertex position is very clear i e inversed v shape fig 4 b d 2 the transverse profile is approximately symmetric and top is wide i e inversed u shape fig 4 c 3 the transverse profile is asymmetrical and vertex position is very clear fig 4 a 4 the transverse profile is asymmetrical and the top is wide fig 4 e the black dots are non target points and hollow dots are target points in fig 4 although the transverse profiles have different forms the common feature of the target cells is that they must be the highest point within a certain distance in a symmetric direction the flowchart for the vf recognition method in the e w direction is presented in fig 5 the variables and their descriptions are summarized in table 1 first the target recognition algorithm aims to find the highest cells in at least one symmetrical direction this is determined by comparing the elevation of the checked point h 0 with the elevation around the checked points e g h w i h e i that is d h w and d h e in the e w direction when i is larger than 1 the d h w and d h e is computed from h 0 minus m h w i and m h e i respectively when the elevation values in one of the symmetrical directions stop getting lower that is d m h w i i e m h w i m h w i 1 or d m h e i i e m h e i m h e i 1 is less than 0 m each direction must be checked separately to ensure that this situation in fig 4 a e is not overlooked that is in fig 4 a e starting from the check point after a certain distance of elevation drop the height remains stable or even rises and then continues to fall if the calculation loop terminates when the height does not drop further a smaller d h is obtained than the d h returned by continuing the calculation the check point with small d h value is likely to be filtered out due to the insignificant height difference a short ascent is allowed to ensure that the d h is closer to the actual situation and avoid deletion of important vf points taking west direction as an example this process is achieved by setting the thresholds to two parameters t and d m h w i i e m a x t and m a x d m h the m a x t is employed to limit the distance of ascent so that the height does not continuously decrease with increasing r the m a x d m h limits the height of each ascent the outputs of this portion of the framework are two target recognition raster files and an attribute table the values of the two generated rasters are the maximum radius r and the maximum height difference d h of the eight directions the attribute table includes r and d h for each direction for vf cells r represents how wide and d h represents how high users must define two thresholds m i n d h i e the minimum d h and m i n r i e the minimum r to filter raster cells for their study that is all remaining vf cells have d h values greater than m i n d h and r values greater than m i n r these vf cells are the input for the following target delineation method the m i n d h is determined by the 1 5 iqr rule and m i n r is recommend to be 2 5 cell size the details of the two thresholds and the recommended values for different study areas will be provided in section 3 2 3 target delineation next the vf raster is converted to feature polylines the potential vf cells identified from target recognition are more than the desired vf cells as this is the first phase in vf delineation raster cells of potential vf points that do not meet required criteria i e not high enough are removed based on values of r and d h the remaining potential vf cells form wide and linear raster cells or even blocks of raster cells specific examples and values of r and dh are discussed in the following sections there is an assumption that significant barriers to surge propagation will be captured as watershed boundaries bilskie et al 2015 some programs e g taudem gdal esri s archydro extension were developed to delineate watershed boundaries from dem kraemer and panda 2009 tarboton 2005 through establishing flow direction linking flow path and calculating flow accumulation based on a dem cells with a flow accumulation value of zero generally correspond to watershed boundaries also it is impossible to point out which of the two adjacent cells that share the watershed boundary is higher hence the watershed boundaries are buffered by a distance of one cell size on both sides however the potential raster cells covered by the buffered watershed boundary is desired at this stage the width of the linear potential vf raster is two cell sizes at a minimum when generating vf polylines it is necessary to reduce the number of cells to create a linear feature of a single cell width this is accomplished by using a thinning i e skeletonization approach davies and plummer 1981 naccache and shinghal 1984 zhan 1993 zhang and suen 1984 the thinned linear raster can then be converted to polyline by arcgis or similar gis software therefore the vf polylines reside along the centroid of raster cells and not on the edge 2 4 post processing post processing is required depending on the research objective the main purpose of our vfs extraction method is to help guide the unstructured finite element mesh generation for a flood inundation model therefore post processing focuses on retaining sufficiently long vfs relative to the element size including removing shorter vfs and bridging the gap between vfs this is to facilitate the placement of nodes and element edges along the vf lines for example the advanced circulation adcirc model employs unstructured finite element meshes that are widely used for predicting storm surge generated coastal inundation across normally dry regions bhaskaran et al 2014 bilskie et al 2014 2016 bunya et al 2010 dietrich et al 2011 gayathri et al 2016 luettich et al 1992 if the length of the extracted vf is less than the length of the desired local element size it is not possible to directly include it in the mesh therefore vf lines with lengths less than the desired local mesh size must be removed additionally there may be small gaps in the vfs that should be connected further details are described in the following section 3 applications of pyvf this section highlights some applications of the pyvf methods pyvf is employed to extract vfs from four distinct landforms low gradient coastal region urban region mountain region and beach region for all regions the values of the parameters in target recognition and target delineation methods are recommended table 2 summarizes the description and values of the parameters in the four study areas the application of batch processing is described in the mountain region the cases requiring post processing are illustrated in the low gradient coastal region 3 1 low gradient coastal region area low gradient coastal areas have a higher probability of flooding from storm surges and prolonged torrential precipitation than regions with higher elevation gradient terrains moreover especially in low gradient regions the potential of more destructive flooding from compound events is often higher than the occurrence of a single event bevacqua et al 2019 ikeuchi et al 2017 ipcc 2013 moftakhari et al 2017 nicholls et al 2007 this region is difficult to model due to the complicated flows of coastal storm surges rainfall runoff and fluvial flooding that can occur in combination bilskie and hagen 2018 santiago collazo et al 2019 the low gradient coastal study area displayed in fig 6 is a part of the lake maurepas watershed in southeastern louisiana fig 6 shows the aerial imagery draped over a 3 m resolution lidar derived topo bathymetric dem provided by u s geological survey usgs some roadbeds and natural barriers that can alter the path of flood flow are shown in the figure the elevation in this area ranges from 0 5 m to 11 7 m navd88 the elevation difference d h which is a result of the target recognition method is shown in fig 7 a a the darker the color the larger the elevation difference the vfs of interest have a larger elevation difference however there is a large amount of vf cells with small elevation difference i e d h these cells are not regarded as potential vertical features hence m i n d h is the major parameter to provide a threshold to identify the vf cells some geomorphometric parameters such as slope curvature elevation residual and entropy are used in terrain analysis to extract vertical features hiller and smith 2008 sofia et al 2014 tarolli et al 2010 the m i n d h has the similar core idea with elevation residual er that is to filter low relief plains in local scale the er is calculated as following equation 1 e r e d e m e m e a n r where e m e a n r is the average elevation of cells within a moving window with a fixed size m e a n r which is the average r of the entire study area and e d e m is the elevation of the cell in the center of the moving window the statistical iqr is used to define the threshold value of geomorphometric parameters hiller and smith 2008 sofia et al 2014 therefore the iqr is feasible to analyze d h the vf cells can be regarded as the outliers of the entire dem cells the m i n d h should satisfy the condition 2 i q r q 3 q 1 3 m i n d h q 3 n i q r where q1 is the first quartile q3 is the third quartile and n is a parameter defined by users 1 5 iqr i e n 1 5 is the commonly used rule to define outliers in this region the m i n d h in 1 5 iqr rule is about 0 2 m fig 13 a value greater than 0 2 can be considered as a m i n d h value and the upper limit is recommended not to exceed the average value of outliers 0 6 m since the width of vfs in this study is not regarded as an important indicator for vf extraction the selection of minimum r is only used to delete those discrete local high cells hence the value of minimum r is related to the resolution of dem which is generally 2 5 i e initial r 1 times the cell size the potential vf raster image using 0 2 m m i n d h and 7 5 m m i n r i e 2 5 cell size 3 m is shown in fig 7 b b to avoid noise the value of the parameter m i n d h is selected as 0 5 m and m i n r is 7 5 m fig 7 c c as previously mentioned the extracted cells are wide and difficult to convert to lines the potential vf cells are delineated with the aid of watershed boundaries fig 8 a threshold t that represents the minimum contributing cells in the drainage network needs to be selected if the threshold is too small the flow accumulation will be too short resulting in more watersheds and short watershed boundaries conversely if the threshold is too large some important watershed boundaries will be omitted the process of selecting the threshold is conducted through trial and error while iterating on target recognition results four thresholds are chosen 20000 cells 10000 cells 5000 cells and 2000 cells the watershed boundaries generated by the accumulated flow threshold of 5000 is determined suitable for the vf extraction the delineated watershed boundaries are along the edge of the cells there is no method to determine whether the watershed boundary lies to the right or left of the potential vf cells so the watershed boundaries polylines are buffered by one cell size distance on both sides the potential vf cells that overlap with the watershed boundary buffer polygons are extracted then a linear feature of a single cell width for generating polyline by a thinning approach the polylines converted from the thinned raster cells are the initial set of vf lines fig 9 for inclusion in the mesh generation processes the vfs should be further processed to remove vf lines shorter than the minimum element size and to connect small gaps for example the vfs shown in fig 9 a are shorter than the surrounding desired mesh element size and therefore deleted additionally there may be small gaps in the vfs that should be connected fig 9 c gaps should remain intact when a river flows through a vf fig 9 d in addition vf polylines that have a large bend at the end are not conducive to mesh generation fig 9 e also parallel vf lines that have a separation distance within a given element size should be compared to decide which should be kept fig 9 b closed loops caused by thinning are cleaned fig 9 f as a result of the automated post processing routines a cleaner and more meaningful set of vf lines are produced for mesh generation fig 10 a furthermore the location of the extracted vfs is along the centroid of raster cells as opposed to watershed boundaries that are on the edges of raster cells fig 10 b 3 2 urban area vf extraction using pyvf was also tested for an urban area port allen and baton rouge louisiana fig 11 a the mississippi river passes through the study region the usgs 3 m resolution lidar topo bathymetric model was used as the source dem fig 11 b the elevation of port allen is substantially lower than that of baton rouge however the average elevation differences within each city are small the 1 5 iqr of the d h is about 0 66 m fig 14 fig 11 c shows that there are many potential vf cells in the mississippi river region then the m i n d h and m i n r are set to 1 m and 7 5 m in other words the value of recognized potential raster cells with an elevation greater than 1 m and the r is greater than 7 5 m fig 11 d the watershed boundaries are generated with an accumulated flow threshold of 1000 fig 11 e through post processing the vfs derived from pyvf with lengths greater than 200 m are retained the black dotted line shown in fig 11 f g is the mississippi river east and west bank levees obtained from nation levee database it is obvious that levees are extracted and there are many non levee vfs that can impact flow path in this area especially in the port allen area 3 3 mountain area the mountain study area is a region of north georgia and is about 20 by 40 km fig 12 a the 10 m dem in this area was obtained from the nation elevation dataset ned assembled by the usgs fig 12 b this site is larger than the previous two so batch processing the dem was necessary the area is divided into six titles and pyvf is run individually on each tile to obtain potential vf raster cells for the entire mountain area the watershed boundaries are generated with an accumulated flow threshold of 10 000 fig 12 c the 1 5 iqr of the d h is about 50 m fig 14 this value is reasonable since the characteristics of mountain areas are substantially higher than the surrounding terrain and includes large slopes the minimum d h and r are set 50 m and 25 m fig 12 d the vfs with a minimum length of 200 m and 1000 m are shown in fig 12 e f there are many short branches using 200 m as the minimum length pyvf provides users with the option of customizing the minimum length to meet a variety of research objectives 3 4 beach area the fourth study area is a coastal area located in virginia beach fig 13 a according to the 10 m dem supported by the noaa fig 13 b there is a natural barrier i e sandy beach dune beach dunes are important for ecosystems habitats and coastal protection by reducing the impact of extreme coastal hazards such as wave and storm surge ranwell and rosalind 1986 roelvink et al 2009 van der meulen and salman 1996 in this area the 1 5 iqr of the d h is about 1 5 m fig 14 since this area is small the accumulated flow threshold used for extracting watershed boundaries is 100 fig 13 d and the minimum d h and r are set 1 5 m and 25 m i e 2 5 cell size fig 12 c the vfs with a minimum length of 50 m are shown in fig 12 e the longest vf is the beach dune pyvf is shown as able to extract raised features from the beach area 4 discussion since the extracted vertical features are required to be high enough polylines the vf extraction method in this research combines two methods to achieve the requirement they are the target recognition method and the target delineation method the target recognition methods use the threshold of parameters d h i e min d h to ensure the detected cells with high elevation difference tribe 1992 after that with the target delineation method based on the accumulated flow threshold t i e target delineation ai 2007 the potential vertical feature cells can be converted to the potential vertical feature polylines the values of these thresholds will affect the number of extracted vfs in this section the focus is to discuss the considerations when selecting thresholds for these parameters finally the advantages compared to the previous method and the current limitation are elaborated the target recognition in this research applies an iterative increasing size moving window to detect each cell within the dem range this method is superior to previous methods of fixed size moving windows because the importance of each vf can be automatically assessed by the variable d h and r compared with the fixed size approach the output vf height d h identified by the moving window with the increasing radius r is closer to the real height though it may need additional computing time also the radius r could be regarded as the width of a vf d h and r can be considered as vertical and horizontal increments to calculate the slope of vfs the four case studies presented highlight that the minimum values of parameter d h and r can be determined based on the iqr and the resolution of dem they also prove that the min d h has a high relationship with the type of terrain for a given study area for instance in the area with large elevation variations such as the mountain region a larger min d h is used to filter out less significant vfs on the contrary in the low gradient area the smaller min d h is more appropriate due to the minor land elevation variations and small surface slope since the watershed boundary delineation method effectively provides the location of ridges it naturally serves to use the watershed boundary as vfs the selection of the threshold t also has an impact on the vf delineation if a large threshold is selected less output watershed boundaries may cause the loss of significant vfs if a small threshold is selected there will be more watershed boundaries and of course the intersection of the watershed boundaries will also be significantly increased using a small threshold increases computation time and many weak vfs are extracted and the thinned vfs have numerous spurs this requires additional post processing hence the threshold selection should be an iterative process from large to small to determine an appropriate threshold the work of coggin 2008 and bilskie et al 2015 start from the watershed delineation and considers the watershed boundaries that meet three criteria by special parameters as significant vfs the method greatly reduces the number of watershed boundaries and can warrant the importance of the extracted vfs i e portions of the watershed boundaries however there is no weighting among the extracted vfs in unstructured mesh design vfs with close spacing may face trade offs the parameter d h can provide the vertically significant order of each vf additionally the watershed boundaries are along the edges of the grid cell instead of the centroid when the vfs are narrow the nodes of triangular elements could be positioned on the surrounding lower terrain by accident the vfs extracted by pyvf are located along the center of raster cells to avoid careless element node placement there are however some limitations in pyvf first vfs rely on the position of watershed boundaries when potential vf cells and watershed boundaries cannot coincide there may be gaps in the vfs that should be continuous this condition requires post processing to compensate in addition the accuracy of vfs extracted with pyvf will be affected by the quality of the dem pyvf is more effective if a quality dem is available third since pyvf applies some arcgis functions in the target delineation method the users must have access to an arcgis license the last limitation that needs to be overcome is to speed up pyvf the computer system environments used to run pyvf for the four study is listed in table 3 the running duration for the four areas were list in table 4 in addition to computer performance the running duration depends on the number of potential vertical features in other words an area of the same size with more vf will take longer 5 summary and conclusion in this paper the problem of extracting vfs from dem is presented pyvf is written in python to solve this problem using the target recognition and target delineation algorithm the target recognition aims to extract the potential vf cells applying a circular moving window with an iterative increasing size rather than a fixed size the objective of the target delineation method is to convert the potential vf cells to vf polylines the two main algorithms are mainly based on window size r height parameter dh and accumulated flow threshold t also post processing could be required for cleaning up vfs that are insignificant to the research objective pyvf is employed to extract vfs in four different landform areas low gradient coastal area urban area mountain area and beach area the vfs such as roadbed levees mountain ridges and beach dunes in these areas are delineated according to different landforms and research objectives the appropriate values of parameters are changeable the results of the four study areas demonstrate automatic vf delineation from disparate dems our future work will combine the pyvf tool with a local mesh scaling algorithm to extend the delineation of vfs beyond the geometric based approach to include flow properties computer code availability software name pyvf version 1 2 availability all python code and testing data for pyvf associated with the current submission is available through https github com shugao7 pyvf git or https doi org 10 5281 zenodo 4291027 under the gnu general public license v3 0 any updates will also be published on github and zenodo authorship shu gao developed the algorithm wrote code and prepared the initial draft of the manuscript matthew v bilskie aided conceptual algorithm development wrote code and revised the manuscript scott c hagen pi conceived the initial idea aided conceptual algorithm development and revised the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank the feedback provided by drs peter bacopoulos jin ikeda and félix l santiago collazo during the preparation of this article this research was funded in part by the water institute of the gulf under project award number cpra 2015 coe mb this research was paid for in part with federal funding from the department of the treasury through the louisiana coastal protection and restoration authority s center of excellence research grants program under the resources and ecosystems sustainability tourist opportunities and revived economies of the gulf coast states act of 2012 restore act and the louisiana sea grant laborde chair the statements findings conclusions and recommendations are those of the author s and do not necessarily reflect the views of the department of the treasury cpra the water institute of the gulf or the louisiana sea grant college program 
25509,coastal and riverine flooding is one of the most common environmental hazards that affect billions of people worldwide a coupled hydrologic and coastal storm surge simulation is required to develop an improved understanding of the individual and collective mechanisms that can cause flooding within watersheds these simulations are dependent on an accurate digital elevation model dem however it is a challenge to include numerical model resolution as fine as contemporary dems due to the enormous computational cost therefore significant vertical features vfs such as roadbeds levees railroads and natural ridges must be identified and considered in developing the model representation of the dem since the vfs can affect flow propagation pyvf is an open source program to extract significant vfs from a high resolution bare earth lidar derived dem automatically this paper introduces the methods and shows the automated extraction of vfs for a coastal urban mountain and beach area keywords vertical features automated extraction digital elevation model surface hydrology compound flooding storm surge 1 introduction flooding in coastal regions can be caused by i riverine flooding from extreme rainfall runoff ii coastal surges driven by tropical cyclones or strong onshore winds or iii a compounding of both processes occurring simultaneous or in close succession bevacqua et al 2019 bilskie and hagen 2018 santiago collazo et al 2019 zheng et al 2013 2014 accurate representation of the bathymetry i e water depth topography i e land elevation and inundation barriers e g levees raised roadways and natural ridges is fundamental to accurately simulating floods bilskie 2012 dube et al 2010 gallien et al 2018 westerink et al 2008 however a critical challenge for predictive flood modeling is the geometric complexities of the terrain gallien et al 2014 xie et al 2019 model performance in low gradient coastal regions is particularly susceptible to inaccurate topographical representation within computational models since the land elevation variation can be as few centimeters colby and dobson 2010 van de sande et al 2012 a prerequisite to the numerical flood model is the generation of high quality structured or unstructured meshes that permit an accurate representation of complex domain geometry finite element and volume based models typically employ unstructured triangular meshes that are capable of resolving complex coastal domains chen et al 2003 ham et al 2005 namin et al 2004 pain et al 2005 shen et al 2006 xie et al 2019 yoon and kang 2004 the unstructured triangular mesh allows users to refine the mesh in critical areas and use coarse resolution in less sensitive regions such as in deeper bathymetries while maintaining a given computational cost bern and plassmann 2000 hagen et al 2001 kim et al 2014 marsh et al 2018 mcguigan et al 2015 in recent years airborne light detection and ranging lidar technology has grown more precise and high resolution 10 m data have become increasingly available for supporting multi dimensional flood modeling research bateset al 2003 noaa 2007 although the increasing terrain data resolution may permit an improved description of the bare earth topography the unstructured meshes are restricted to a minimum resolution to minimize computational cost and numerical instabilities e g courant friedrichs lewy condition bilskie et al 2015 limiting the resolution results in smoothing out the elevation of natural barriers and anthropogenic features e g levees and raised roadbeds which can alter the path of simulated inundation and result in an inaccurate solution bilskie et al 2015 horritt and bates 2002 kim et al 2014 sofia et al 2014 purvis et al 2008 recognized this shortcoming in model resolution and manually digitized significant terrain features from uk ordnance survey maps to include their peak elevation within the lisflood fp inundation model bunya et al 2010 applied the federal levees defined by usace mvn surveys and the road and railroad crown heights taken from atlas lidar surveys into high resolution adcirc hurricane models they concluded that the model accuracy is dependent on the high level grid resolution of the terrain surface their efforts demonstrated that including long and narrow raised features are critical to build accurate flood inundation models however there are limitations in these methods these limitations include the amount of hand digitizing and editing low accuracy in the horizontal placement of crest elevations a high number of person hours and the potential for errors in sum these findings motivate the automated extraction of significantly raised linear features i e vertical features from high resolution terrain data vertical features vf are raised linear features such as roadbeds railroads levees floodwalls and natural features that can block flow conduct flow and redirect flow the wetting and drying of an inundation front may differ depending on the unstructured mesh model with or without vertical features bilskie et al 2015 there have been many studies aimed at automatically extracting ridge features from high resolution topographic data roberts 2004 applied a method to take the point where the maximum gradient in a region is steep enough as vf points from lidar point cloud data then anomalous points were manually cleaned and employed as raised feature points these raised feature points were incorporated into the coastal flooding analysis coggin 2008 developed an automatic method for extracting vfs from watershed boundaries generated from lidar dem data for inclusion in an adcirc finite element mesh with special parameters bilskie et al 2015 followed and expanded the work of coggin 2008 and roberts 2004 to extract vfs and fix them as polylines in an unstructured mesh that was employed in the simulation of shallow water hydrodynamics for hurricane katrina the research results of coggin 2008 and bilskie et al 2015 2020 show that simulations of flood extents and depths are more accurate when using an unstructured mesh that includes vfs vfs play an essential role in the simulation of storm surge and hydrological flow routing in hydrology watersheds typically define land units which have boundaries delineated from topographic high points edwards et al 2015 zhang et al 2016 obtained ridges by extracting the watersheds of the river network since the watersheds correspond to the ridges in many real scenarios most hydrological models e g swat annagnps hspf gssha delineate watershed boundaries and topographic characteristics of watersheds using dems parajuli and ouyang 2013 wang et al 2011 predicted the spatial patterns of water yield by a swat road model and a swat noroad model the conclusion is that hydrologic effects of raised roads are important for accurately simulating runoff within a low relief watershed alzahrani 2017 manually added vfs into a hec ras 2d model and kept water away from the dry side of a vf until the water surface elevation was higher than the vf s elevation griffiths 2010 represented vfs as embankment arcs that alter the overland surface flow characteristics of a watershed along with grid cell edges or elevated grid cells to simulate overbank flow in the gssha model thus it is well documented in previous efforts that the inclusion of vertical terrain features is critical for accurate surface water flow in coastal and riverine floodplains most techniques for extracting raised linear features from lidar data are concentrated on breaklines coggin 2008 in surface modeling breaklines are linear features used to represent a sudden or abrupt change in the terrain s smoothness and continuity abdullah 2017 or an otherwise string of connected points that should be honored by the data triangulation they are commonly extracted from airborne laser scanner als point cloud data digital orthophotos or ground surveyed cross sections requiring cumbersome manual work bodoque et al 2016 briese 2004 brugelmann 2000 wang et al 2018 yang et al 2016 breaklines are often located at the toe and shoulder of levees and highways rather than along the highest point of the protruding feature where the vfs should be located in unstructured mesh generation vertical features are breaklines considered as polylines or series of edges connected by triangular elements vertical features are used as a reference to form breaklines in unstructured mesh generation to improve the terrain description in contrast to breaklines in surface modeling vertical features are extracted from dems rather than lidar point clouds they have special requirements including being tall enough to affect flood propagation long enough to span the edges of at least one element and having an appropriate spatial distribution for the horizontal scale of the unstructured grid to be designed bilskie et al 2015 coggin 2008 the work of bilskie et al 2015 and coggin 2008 show the usefulness of the inclusion of vf for flood models however they do point to some shortcomings that should be overcome first their methods require over a dozen parameters that should be manuall adjusted based on the surrounding terrain bilskie et al 2015 states that future work should focus on parameter section for varying landscapes and vf extraction sensitity to hydrodynamic model resolution in addition since their vf extraction methods begin with watershed boundaries the final vf lines reside along dem cell edges rather down the centroid we aim to address these shortcomings through a revised vf extraction algorithm that minimizes the number of parameters while considering various topographic landscapes from mountains to coastal regions this prevous research led to developing an automated vf delineation algorithm method based on a lidar derived dem for inclusion in flood inundation models section 2 describes the capability of the developed algorithm and software called pyvf and how vfs are delineated in section 3 examples of extracted vfs are presented for four study areas in different types of geography to illustrate the capacity of pyvf for a variety of terrains section 4 contains a discussion of the pyvf method and section 5 summarizes the research and conclusions 2 methods pyvf is written as a python version 2 7 script to take advantage of arcgis functions through the arcpy module all of the geoprocessing functions of arcgis such as data analysis data conversion and data management can be accessed through python using arcpy which is a python site package that integrates arcgis with python esri 2016 pyvf also includes numpy walt et al 2011 for data manipulation and the python standard math library lundh 2001 the design of pyvf is divided into four main tasks 1 batch processing sub dems 2 vf target recognition 3 vf delineation and 4 post processing of potential vfs pyvf produces vfs as a shapefile and two raster images with attributes described in sections 2 3 and 2 4 pyvf like the method proposed in coggin 2008 and bilskie et al 2015 aims to extract vfs that are high enough and long enough their method evaluates the relative elevation by comparing the height of a vertex on the watershed boundary line with the height at two perpendicular distances from the vertex according to the height difference each vertex is declared as significant or continue if the significant vertex is below a ratio e g 35 the watershed line is eliminated the extracted vfs from this previous method are a subset of the watershed boundaries the path of the watershed boundary is along the edge of the dem cells and so are the extracted vfs this can lead to large height errors when placing mesh nodes on narrow vfs and coarser dems to reduce the height error in the model and provide more meaningful parameters an iterative increasing size moving window method is employed to search for potential vfs cells by calculating the height trend of all dem cells in eight directions the extracted vfs with meaningful attributes are along the center of the cells the flowchart in fig 1 provides a general description of the pyvf algorithm the two inputs of the algorithm are a dem and a target unstructured mesh element size es the es also can be replaced with a constant value first the dem is split into sub dem tiles to efficiently utilize computer memory useful for large domains with small cell sizes by batch processing next two rasters which have the value of r and d h for all tiles are extracted through the target recognition tr method two thresholds of r and d h m i n r and m i n d h are used for reducing the vf cell candidates to avoid weak vf cells and extraneous cell noise they are defined through the 1 5 x iqr interquartile range rule and explained in the following section the dem also serves as the input to watershed boundary delineation the extracted watershed boundaries are considered ridges the reduced vf cells that coincide with watershed boundaries are potential vf raster then the potential vf cells continue to be deleted to create a linear feature of a single cell width by a thinning approach this is in preparation to convert the linear raster to vertical feature polylines finally post processing is performed based on the constraints of the individual modeling study that will utilize the extracted vfs for example when vertical features are applied to an unstructured mesh model the element size is required to determine the appropriate length of the final vertical feature polylines 2 1 batch processing the high resolution raster datasets across large domains result in large amounts of data and processing challenges due to the computer memory limitation batch processing is a common method for overcoming memory limitations there are two methods provided for dividing the large raster into tiles tiles are read into memory processed and written to disk one by one until the task is complete the two methods have a common purpose to split the dem so that the data volume of each sub dem region can reside in memory and minimize discontinuities or gaps in the final mosaicked raster image the first batch processing approach is based on customized rectangular polygons shapefile the grid polygons used in this research are a net of square polygons the large region dem is clipped into sub dem by the polygon shapefile it should be noted that the side length of each tile in the polygon shapefile must be a multiple of the dem cell size otherwise there will be gaps between each tile the sub dem region is called the recognition area as a minimum unit for the following target recognition method another dem decomposition approach is dividing the dem into many tiles from the upper left corner according to the dem coordinate and using coordinates i and j with 0 0 denoting the upper left corner of the dem the minimum unit i e the recognition region is a number of rectangular i j tiles where i is the number of rows j is the number of columns each tile is processed in the target recognition algorithm individually one by one this method is better for manipulating the entire dem region without creating a grid polygon the target recognition method in this research applies a moving window approach since the raster edge cells e g the cells in the top and bottom rows and the left and right columns do not have sufficient neighbors for example in fig 2 the center of the moving window is on the cell at the left edge the lack of data in the moving window will affect the calculation if the edge cells of each sub dem cannot be calculated as non edge cells apparent discontinuities will result along the edges of each tile in the final mosaicked image hence a buffer distance around the recognition area is determined to define an effective area effective areas polygon tiles at a specified distance around recognition areas are determined to solve this problem the role of effective areas is that when the moving window traverses each recognition area there is no null value inside the window range the following section will introduce the method of searching potential vf cells from each recognition area 2 2 target recognition the vf cell recognition method in this research adopts a circular moving window approach which has an advantage in directional uniformity over a square moving window chang and sinha 2007 chang et al 1998 koike et al 1995 the centroid of the circular moving window is placed at the center of each cell the circular moving window is divided into eight sectors representing eight directions e g a1 is north b1 is northeast fig 3 a the target recognition method traverses each target dem cell and aims to find the highest cells in at least one symmetry direction the higher elevation cells are identified as potential vf candidates two parameters r and d h are calculated by the target recognition method the first parameter r is the radius of the moving window moving windows can use a fixed size or iteratively increasing size casas et al 2012 proposed a method for assessing the structural integrity of levees this method relies on the slope calculation based on a 3 by 3 moving window however small fixed moving windows are not always the best choice lin et al 2013 in this research an iteratively increasing circular moving window is applied hence the circular moving window expands as the r value increases the initial value of r is 1 5 times the cell size the second parameter d h is the difference in height between the value of the checked cell with that of the lowest cell for example fig 3 b shows an assumed cross profile of a north south vf and the check cell is located on the vf to the west of check cell the lowest cell can be found when r is equal to 3 5 cell size the r in the west and east direction are represented by r w and r e the d h in the west direction is d h w in the east of check cell the lowest cell is found when r is equal to 5 5 cell size the d h in the west direction is d h e a different value of r will lead to a different value of d h this example illustrates that a variable window size is more appropriate for extracting vfs than a constant size since the width of the vfs is not fixed for example fig 4 shows a hypothetical large scale cross section spanning multiple raised features to illustrate the desired target cell location for this study because the actual terrain is very complex there are many possible elevation relationships between the potential vf cells and their surrounding points on the same cross section 1 the transverse profile is approximately symmetric and vertex position is very clear i e inversed v shape fig 4 b d 2 the transverse profile is approximately symmetric and top is wide i e inversed u shape fig 4 c 3 the transverse profile is asymmetrical and vertex position is very clear fig 4 a 4 the transverse profile is asymmetrical and the top is wide fig 4 e the black dots are non target points and hollow dots are target points in fig 4 although the transverse profiles have different forms the common feature of the target cells is that they must be the highest point within a certain distance in a symmetric direction the flowchart for the vf recognition method in the e w direction is presented in fig 5 the variables and their descriptions are summarized in table 1 first the target recognition algorithm aims to find the highest cells in at least one symmetrical direction this is determined by comparing the elevation of the checked point h 0 with the elevation around the checked points e g h w i h e i that is d h w and d h e in the e w direction when i is larger than 1 the d h w and d h e is computed from h 0 minus m h w i and m h e i respectively when the elevation values in one of the symmetrical directions stop getting lower that is d m h w i i e m h w i m h w i 1 or d m h e i i e m h e i m h e i 1 is less than 0 m each direction must be checked separately to ensure that this situation in fig 4 a e is not overlooked that is in fig 4 a e starting from the check point after a certain distance of elevation drop the height remains stable or even rises and then continues to fall if the calculation loop terminates when the height does not drop further a smaller d h is obtained than the d h returned by continuing the calculation the check point with small d h value is likely to be filtered out due to the insignificant height difference a short ascent is allowed to ensure that the d h is closer to the actual situation and avoid deletion of important vf points taking west direction as an example this process is achieved by setting the thresholds to two parameters t and d m h w i i e m a x t and m a x d m h the m a x t is employed to limit the distance of ascent so that the height does not continuously decrease with increasing r the m a x d m h limits the height of each ascent the outputs of this portion of the framework are two target recognition raster files and an attribute table the values of the two generated rasters are the maximum radius r and the maximum height difference d h of the eight directions the attribute table includes r and d h for each direction for vf cells r represents how wide and d h represents how high users must define two thresholds m i n d h i e the minimum d h and m i n r i e the minimum r to filter raster cells for their study that is all remaining vf cells have d h values greater than m i n d h and r values greater than m i n r these vf cells are the input for the following target delineation method the m i n d h is determined by the 1 5 iqr rule and m i n r is recommend to be 2 5 cell size the details of the two thresholds and the recommended values for different study areas will be provided in section 3 2 3 target delineation next the vf raster is converted to feature polylines the potential vf cells identified from target recognition are more than the desired vf cells as this is the first phase in vf delineation raster cells of potential vf points that do not meet required criteria i e not high enough are removed based on values of r and d h the remaining potential vf cells form wide and linear raster cells or even blocks of raster cells specific examples and values of r and dh are discussed in the following sections there is an assumption that significant barriers to surge propagation will be captured as watershed boundaries bilskie et al 2015 some programs e g taudem gdal esri s archydro extension were developed to delineate watershed boundaries from dem kraemer and panda 2009 tarboton 2005 through establishing flow direction linking flow path and calculating flow accumulation based on a dem cells with a flow accumulation value of zero generally correspond to watershed boundaries also it is impossible to point out which of the two adjacent cells that share the watershed boundary is higher hence the watershed boundaries are buffered by a distance of one cell size on both sides however the potential raster cells covered by the buffered watershed boundary is desired at this stage the width of the linear potential vf raster is two cell sizes at a minimum when generating vf polylines it is necessary to reduce the number of cells to create a linear feature of a single cell width this is accomplished by using a thinning i e skeletonization approach davies and plummer 1981 naccache and shinghal 1984 zhan 1993 zhang and suen 1984 the thinned linear raster can then be converted to polyline by arcgis or similar gis software therefore the vf polylines reside along the centroid of raster cells and not on the edge 2 4 post processing post processing is required depending on the research objective the main purpose of our vfs extraction method is to help guide the unstructured finite element mesh generation for a flood inundation model therefore post processing focuses on retaining sufficiently long vfs relative to the element size including removing shorter vfs and bridging the gap between vfs this is to facilitate the placement of nodes and element edges along the vf lines for example the advanced circulation adcirc model employs unstructured finite element meshes that are widely used for predicting storm surge generated coastal inundation across normally dry regions bhaskaran et al 2014 bilskie et al 2014 2016 bunya et al 2010 dietrich et al 2011 gayathri et al 2016 luettich et al 1992 if the length of the extracted vf is less than the length of the desired local element size it is not possible to directly include it in the mesh therefore vf lines with lengths less than the desired local mesh size must be removed additionally there may be small gaps in the vfs that should be connected further details are described in the following section 3 applications of pyvf this section highlights some applications of the pyvf methods pyvf is employed to extract vfs from four distinct landforms low gradient coastal region urban region mountain region and beach region for all regions the values of the parameters in target recognition and target delineation methods are recommended table 2 summarizes the description and values of the parameters in the four study areas the application of batch processing is described in the mountain region the cases requiring post processing are illustrated in the low gradient coastal region 3 1 low gradient coastal region area low gradient coastal areas have a higher probability of flooding from storm surges and prolonged torrential precipitation than regions with higher elevation gradient terrains moreover especially in low gradient regions the potential of more destructive flooding from compound events is often higher than the occurrence of a single event bevacqua et al 2019 ikeuchi et al 2017 ipcc 2013 moftakhari et al 2017 nicholls et al 2007 this region is difficult to model due to the complicated flows of coastal storm surges rainfall runoff and fluvial flooding that can occur in combination bilskie and hagen 2018 santiago collazo et al 2019 the low gradient coastal study area displayed in fig 6 is a part of the lake maurepas watershed in southeastern louisiana fig 6 shows the aerial imagery draped over a 3 m resolution lidar derived topo bathymetric dem provided by u s geological survey usgs some roadbeds and natural barriers that can alter the path of flood flow are shown in the figure the elevation in this area ranges from 0 5 m to 11 7 m navd88 the elevation difference d h which is a result of the target recognition method is shown in fig 7 a a the darker the color the larger the elevation difference the vfs of interest have a larger elevation difference however there is a large amount of vf cells with small elevation difference i e d h these cells are not regarded as potential vertical features hence m i n d h is the major parameter to provide a threshold to identify the vf cells some geomorphometric parameters such as slope curvature elevation residual and entropy are used in terrain analysis to extract vertical features hiller and smith 2008 sofia et al 2014 tarolli et al 2010 the m i n d h has the similar core idea with elevation residual er that is to filter low relief plains in local scale the er is calculated as following equation 1 e r e d e m e m e a n r where e m e a n r is the average elevation of cells within a moving window with a fixed size m e a n r which is the average r of the entire study area and e d e m is the elevation of the cell in the center of the moving window the statistical iqr is used to define the threshold value of geomorphometric parameters hiller and smith 2008 sofia et al 2014 therefore the iqr is feasible to analyze d h the vf cells can be regarded as the outliers of the entire dem cells the m i n d h should satisfy the condition 2 i q r q 3 q 1 3 m i n d h q 3 n i q r where q1 is the first quartile q3 is the third quartile and n is a parameter defined by users 1 5 iqr i e n 1 5 is the commonly used rule to define outliers in this region the m i n d h in 1 5 iqr rule is about 0 2 m fig 13 a value greater than 0 2 can be considered as a m i n d h value and the upper limit is recommended not to exceed the average value of outliers 0 6 m since the width of vfs in this study is not regarded as an important indicator for vf extraction the selection of minimum r is only used to delete those discrete local high cells hence the value of minimum r is related to the resolution of dem which is generally 2 5 i e initial r 1 times the cell size the potential vf raster image using 0 2 m m i n d h and 7 5 m m i n r i e 2 5 cell size 3 m is shown in fig 7 b b to avoid noise the value of the parameter m i n d h is selected as 0 5 m and m i n r is 7 5 m fig 7 c c as previously mentioned the extracted cells are wide and difficult to convert to lines the potential vf cells are delineated with the aid of watershed boundaries fig 8 a threshold t that represents the minimum contributing cells in the drainage network needs to be selected if the threshold is too small the flow accumulation will be too short resulting in more watersheds and short watershed boundaries conversely if the threshold is too large some important watershed boundaries will be omitted the process of selecting the threshold is conducted through trial and error while iterating on target recognition results four thresholds are chosen 20000 cells 10000 cells 5000 cells and 2000 cells the watershed boundaries generated by the accumulated flow threshold of 5000 is determined suitable for the vf extraction the delineated watershed boundaries are along the edge of the cells there is no method to determine whether the watershed boundary lies to the right or left of the potential vf cells so the watershed boundaries polylines are buffered by one cell size distance on both sides the potential vf cells that overlap with the watershed boundary buffer polygons are extracted then a linear feature of a single cell width for generating polyline by a thinning approach the polylines converted from the thinned raster cells are the initial set of vf lines fig 9 for inclusion in the mesh generation processes the vfs should be further processed to remove vf lines shorter than the minimum element size and to connect small gaps for example the vfs shown in fig 9 a are shorter than the surrounding desired mesh element size and therefore deleted additionally there may be small gaps in the vfs that should be connected fig 9 c gaps should remain intact when a river flows through a vf fig 9 d in addition vf polylines that have a large bend at the end are not conducive to mesh generation fig 9 e also parallel vf lines that have a separation distance within a given element size should be compared to decide which should be kept fig 9 b closed loops caused by thinning are cleaned fig 9 f as a result of the automated post processing routines a cleaner and more meaningful set of vf lines are produced for mesh generation fig 10 a furthermore the location of the extracted vfs is along the centroid of raster cells as opposed to watershed boundaries that are on the edges of raster cells fig 10 b 3 2 urban area vf extraction using pyvf was also tested for an urban area port allen and baton rouge louisiana fig 11 a the mississippi river passes through the study region the usgs 3 m resolution lidar topo bathymetric model was used as the source dem fig 11 b the elevation of port allen is substantially lower than that of baton rouge however the average elevation differences within each city are small the 1 5 iqr of the d h is about 0 66 m fig 14 fig 11 c shows that there are many potential vf cells in the mississippi river region then the m i n d h and m i n r are set to 1 m and 7 5 m in other words the value of recognized potential raster cells with an elevation greater than 1 m and the r is greater than 7 5 m fig 11 d the watershed boundaries are generated with an accumulated flow threshold of 1000 fig 11 e through post processing the vfs derived from pyvf with lengths greater than 200 m are retained the black dotted line shown in fig 11 f g is the mississippi river east and west bank levees obtained from nation levee database it is obvious that levees are extracted and there are many non levee vfs that can impact flow path in this area especially in the port allen area 3 3 mountain area the mountain study area is a region of north georgia and is about 20 by 40 km fig 12 a the 10 m dem in this area was obtained from the nation elevation dataset ned assembled by the usgs fig 12 b this site is larger than the previous two so batch processing the dem was necessary the area is divided into six titles and pyvf is run individually on each tile to obtain potential vf raster cells for the entire mountain area the watershed boundaries are generated with an accumulated flow threshold of 10 000 fig 12 c the 1 5 iqr of the d h is about 50 m fig 14 this value is reasonable since the characteristics of mountain areas are substantially higher than the surrounding terrain and includes large slopes the minimum d h and r are set 50 m and 25 m fig 12 d the vfs with a minimum length of 200 m and 1000 m are shown in fig 12 e f there are many short branches using 200 m as the minimum length pyvf provides users with the option of customizing the minimum length to meet a variety of research objectives 3 4 beach area the fourth study area is a coastal area located in virginia beach fig 13 a according to the 10 m dem supported by the noaa fig 13 b there is a natural barrier i e sandy beach dune beach dunes are important for ecosystems habitats and coastal protection by reducing the impact of extreme coastal hazards such as wave and storm surge ranwell and rosalind 1986 roelvink et al 2009 van der meulen and salman 1996 in this area the 1 5 iqr of the d h is about 1 5 m fig 14 since this area is small the accumulated flow threshold used for extracting watershed boundaries is 100 fig 13 d and the minimum d h and r are set 1 5 m and 25 m i e 2 5 cell size fig 12 c the vfs with a minimum length of 50 m are shown in fig 12 e the longest vf is the beach dune pyvf is shown as able to extract raised features from the beach area 4 discussion since the extracted vertical features are required to be high enough polylines the vf extraction method in this research combines two methods to achieve the requirement they are the target recognition method and the target delineation method the target recognition methods use the threshold of parameters d h i e min d h to ensure the detected cells with high elevation difference tribe 1992 after that with the target delineation method based on the accumulated flow threshold t i e target delineation ai 2007 the potential vertical feature cells can be converted to the potential vertical feature polylines the values of these thresholds will affect the number of extracted vfs in this section the focus is to discuss the considerations when selecting thresholds for these parameters finally the advantages compared to the previous method and the current limitation are elaborated the target recognition in this research applies an iterative increasing size moving window to detect each cell within the dem range this method is superior to previous methods of fixed size moving windows because the importance of each vf can be automatically assessed by the variable d h and r compared with the fixed size approach the output vf height d h identified by the moving window with the increasing radius r is closer to the real height though it may need additional computing time also the radius r could be regarded as the width of a vf d h and r can be considered as vertical and horizontal increments to calculate the slope of vfs the four case studies presented highlight that the minimum values of parameter d h and r can be determined based on the iqr and the resolution of dem they also prove that the min d h has a high relationship with the type of terrain for a given study area for instance in the area with large elevation variations such as the mountain region a larger min d h is used to filter out less significant vfs on the contrary in the low gradient area the smaller min d h is more appropriate due to the minor land elevation variations and small surface slope since the watershed boundary delineation method effectively provides the location of ridges it naturally serves to use the watershed boundary as vfs the selection of the threshold t also has an impact on the vf delineation if a large threshold is selected less output watershed boundaries may cause the loss of significant vfs if a small threshold is selected there will be more watershed boundaries and of course the intersection of the watershed boundaries will also be significantly increased using a small threshold increases computation time and many weak vfs are extracted and the thinned vfs have numerous spurs this requires additional post processing hence the threshold selection should be an iterative process from large to small to determine an appropriate threshold the work of coggin 2008 and bilskie et al 2015 start from the watershed delineation and considers the watershed boundaries that meet three criteria by special parameters as significant vfs the method greatly reduces the number of watershed boundaries and can warrant the importance of the extracted vfs i e portions of the watershed boundaries however there is no weighting among the extracted vfs in unstructured mesh design vfs with close spacing may face trade offs the parameter d h can provide the vertically significant order of each vf additionally the watershed boundaries are along the edges of the grid cell instead of the centroid when the vfs are narrow the nodes of triangular elements could be positioned on the surrounding lower terrain by accident the vfs extracted by pyvf are located along the center of raster cells to avoid careless element node placement there are however some limitations in pyvf first vfs rely on the position of watershed boundaries when potential vf cells and watershed boundaries cannot coincide there may be gaps in the vfs that should be continuous this condition requires post processing to compensate in addition the accuracy of vfs extracted with pyvf will be affected by the quality of the dem pyvf is more effective if a quality dem is available third since pyvf applies some arcgis functions in the target delineation method the users must have access to an arcgis license the last limitation that needs to be overcome is to speed up pyvf the computer system environments used to run pyvf for the four study is listed in table 3 the running duration for the four areas were list in table 4 in addition to computer performance the running duration depends on the number of potential vertical features in other words an area of the same size with more vf will take longer 5 summary and conclusion in this paper the problem of extracting vfs from dem is presented pyvf is written in python to solve this problem using the target recognition and target delineation algorithm the target recognition aims to extract the potential vf cells applying a circular moving window with an iterative increasing size rather than a fixed size the objective of the target delineation method is to convert the potential vf cells to vf polylines the two main algorithms are mainly based on window size r height parameter dh and accumulated flow threshold t also post processing could be required for cleaning up vfs that are insignificant to the research objective pyvf is employed to extract vfs in four different landform areas low gradient coastal area urban area mountain area and beach area the vfs such as roadbed levees mountain ridges and beach dunes in these areas are delineated according to different landforms and research objectives the appropriate values of parameters are changeable the results of the four study areas demonstrate automatic vf delineation from disparate dems our future work will combine the pyvf tool with a local mesh scaling algorithm to extend the delineation of vfs beyond the geometric based approach to include flow properties computer code availability software name pyvf version 1 2 availability all python code and testing data for pyvf associated with the current submission is available through https github com shugao7 pyvf git or https doi org 10 5281 zenodo 4291027 under the gnu general public license v3 0 any updates will also be published on github and zenodo authorship shu gao developed the algorithm wrote code and prepared the initial draft of the manuscript matthew v bilskie aided conceptual algorithm development wrote code and revised the manuscript scott c hagen pi conceived the initial idea aided conceptual algorithm development and revised the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank the feedback provided by drs peter bacopoulos jin ikeda and félix l santiago collazo during the preparation of this article this research was funded in part by the water institute of the gulf under project award number cpra 2015 coe mb this research was paid for in part with federal funding from the department of the treasury through the louisiana coastal protection and restoration authority s center of excellence research grants program under the resources and ecosystems sustainability tourist opportunities and revived economies of the gulf coast states act of 2012 restore act and the louisiana sea grant laborde chair the statements findings conclusions and recommendations are those of the author s and do not necessarily reflect the views of the department of the treasury cpra the water institute of the gulf or the louisiana sea grant college program 
