index,text
26180,the increasing impact of flooding urges more effective flood management strategies to guarantee sustainable ecosystem development recent catastrophes underline the importance of avoiding local flood management but characterizing large scale basin wide approaches for systemic flood risk management here we introduce an information theoretic portfolio decision model ipdm for the optimization of a systemic ecosystem value at the basin scale by evaluating all potential flood risk mitigation plans ipdm calculates the ecosystem value predicted by all feasible combinations of flood control structures fcs considering environmental social and economical asset criteria a multi criteria decision analytical model evaluates the benefits of all fcs portfolios at the basin scale weighted by stakeholder preferences for assets criteria as ecosystem services the risk model is based on a maximum entropy model maxent that predicts the flood susceptibility the risk of floods based on the exceedance probability distribution and its most important drivers information theoretic global sensitivity and uncertainty analysis is used to select the simplest and most accurate model based on a flood return period a stochastic optimization algorithm optimizes the ecosystem value constrained to the budget available and provides pareto frontiers of optimal fcs plans for any budget level pareto optimal solutions maximize fcs diversity and minimize the criticality of floods manifested by the scaling exponent of the pareto distribution of flood size that links management and hydrogeomorphological patterns the proposed model is tested on the 17 000 km2 tiber river basin in italy ipdm allows stakeholders to identify optimal fcs plans in river basins for a comprehensive evaluation of flood effects under future ecosystem trajectories graphical abstract image 1 keywords river basin management floods systemic risk maxent portfolio decision model mcda 1 introduction 1 1 flood protection and distribution of floods flooding is a major natural hazard affecting some 520 million people every year claiming the lives of about 25 000 worldwide and causing global economic losses between usd 50 and 60 billion annually as a result it is essential that we seek to manage the risk of flooding in an effective and appropriate way also in consideration of climate change the assessment of changing dynamics of flood impacts is a public concern beyond many other climate change effects on populations liu et al 2019 the latest assessment of the intergovernmental panel on climate change ipcc ipcc 2014 on observed changes and future projections of floods was provided in chapter 3 of the ipcc special report on extremes often called the ipcc srex report1 a summary on projected flooding in this report stated that overall there is low confidence in projections of changes in fluvial floods this confidence is low due to limited and uncertain evidence of ecosystem drivers and because of the complexity of these causes at the regional changes that is not necessarily addressed quantitatively speaking thus a model that considers these heterogeneities such as hydrological land use cover change and socio political dynamical complexity and their uncertainty is needed for effective decision making aimed to systemic flood protection as for climate change in general the major changes are expected into a varied return period rather than average magnitude of the events considered including floods in particular the return period or frequency equivalently of extreme phenomena is varying because the underlying conditions influencing climate are changing or have been changing for long time in a diverse way most people and the literature talks about changes in magnitude of events that is not necessarily affected primarily rather the accumulation of unusually close events even of small or medium magnitude can bring to an intensification of extreme events resulting in extreme floods examples are about what happened recently in europe and in rome specifically where hydrologic changes driven by human induced land development have largely enhanced flooding nardi et al 2015 extreme flood events of such kind are like the ones in paris in the spring of 2016 and the 2018 european floods paprotny et al 2018 floods and other hydrology related extremes caused economic losses of euro 453 billion between 1980 and 2017 claiming the lives of more than 115 000 people across europe see zanardo et al 2019 for recent estimates of european floods similar patterns of floods and losses are observed in the americas see quinn et al 2019 for usa asia and australia infrastructures are typically designed for withstanding extreme events with a predetermined return period such as for the peak runoff of 100 years i e the runoff expected every 100 years on average volpi et al 2015 however recurrence periods are decreasing considering climatic changes therefore many extreme floods are occurring with higher frequency than in the past due to the inability of flood control infrastructure to contain them this is also associated to poorly planned and maintained urban water management systems i e storm water and sewer drainage originally designed for a 10 30 years frequency scenario that are now inefficiently managing even frequently expected rainfall events yet in general the probability distribution of events is changing with a tendency toward a longer power law distribution of events rather than an increase in the mean or the variance of these events bak et al 1988 fonstad and marcus 2003 van de wiel and coulthard 2010 convertino et al 2012 this trend has also a tendency to increase the criticality of flood events by decreasing the power law exponent regulating the flood size this implies increased persistency of flood phenomena bak et al 1988 convertino et al 2012 for this motivation that leads to high interdependencies between geographical areas beyond administrative boundaries the development of adaptive and systemic flood management models is needed in order to avoid local fragmentation of decisions that implies large sustained flood risks at the basin scale 1 2 current flood management models various models have been used to characterize the risk of extreme events to large urban and more localized infrastructure systems merz et al 2010 but very few models have addressed multiscale and multiasset risk assessment problems little has been done for ecological systems and far less considering societal impacts on populations affected by floods current models include probabilistic risk analysis and decision analysis considered as separate models meyer et al 2009 merz et al 2010 probabilistic models including bayesian analysis have been developed for situations in which there are sparse data of historical frequencies of extreme events such as extreme floods decision analysis has been used in the performance assessment of infrastructure policies and investments for instance see meyer et al 2009 for a multi criteria decision analytical application for flood risk there is a considerable literature that addresses the costs and benefits of investment in protecting infrastructure systems from the threat of extreme events in particular bier et al 1999 discusses several approaches for assessing and managing the risk of extremes which has inspired to focus on the exceedance probabilities of catastrophic flood losses these methods can be categorized as asset management methods where the assets considered were water infrastructure and the flood risk was calculated a posteriori based on the likelihood of a flood to occur and asset conditions usace 2012 other hydraulics hydrologic methods have been used to characterized the very local flooding risk without much consideration of infrastructure criticality and feasible multiasset spatial arrangements nardi et al 2015 all these studies are somewhat risk centered in a unidimensional way in other words these models do not provide indications of optimal management solutions integration of risk and decision science models considering socio environmental and economic outcomes nor the incorporation of spatial heterogeneities in a spatially explicit way lastly these methods are considering exceedance probability values rather than their distribution in a self organized criticality perspective bak et al 1988 fonstad and marcus 2003 and therefore missing the systemic probabilistic characterization of the patterns analyzed recently efforts have explored the applicability of financial analysis models to flood protection in particular modern portfolio theory markowitz 1952 was explored for the diversification of flood infrastructure investments financial portfolio analysis has been widely used in project selection and choice management tasca et al 2017 there is significant work applying such analysis to the financial risk associated to extreme events valverde and andrews 2006 haldane and may 2011a park et al 2012 convertino and james valverde 2013 including the use of metrics like value at risk var and conditional value at risk covar results of these models describe that markowitz type diversification markowitz 1952 can generate the highest expected economic return under an acceptable systemic risk threshold for a desired flood protection systemic risk is a risk that considers non linear functional and structural interdependencies among systems components leading to cascading effects haldane and may 2011b battiston and caldarelli 2013 helbing 2013 tasca and battiston 2014 battiston et al 2016 burkholz et al 2016 this is a signature of the self organized criticality of complex systems bak et al 1988 in flood management these interdependent features are for instance stakeholders type and distribution of flood control structures river network and landscape features which collectively determine non linear flood events as an example applications to the netherlands are significative aerts et al 2008 considering their pressing flood security issues in aerts et al 2008 through a systematic combination of four different types of flood protection assets excluding the no action alternative portfolios of asset types were constructed to reduce the variance of expected losses the variance reduction was greater with more types of assets compared with portfolios that only contained one asset these past efforts leave open the question about how such diversification might influence other population endpoints beyond the traditional economic metrics of performance of flood protection additionally the use of exceedance probabilities epdfs of extreme losses definable via a multicriteria function rather than just using mean variance portfolio approaches is missing we believe that there are significant simplifications to using average metrics or mean variance optimization of flood risk that often lacks of a spatial component characterization such as in zhou et al 2012 for designing flood protection plans considering extreme events first no study has yet suggested that variance minimization is appropriate as a performance objective for multicriteria projects in particular variance may not always be an appropriate component of the objective function in engineering decision making where return period design should be prevalent for example for coastal protection and flood investment for engineering design a mean variance approach does not represent extreme events because variance of events whose probability distribution can be a power law distribution with infinite variance for the power law exponent of the flood size does not characterize enough the severity of these events decision makers are concerned with low probability catastrophic events along with more frequently occurring and less severe events rather than mean variance optimization exceedance probabilities and return periods at several levels of systemic loss should be used as metrics of the risk associated to extreme events in hydrologic and ecological engineering yet this paper will explore how diversification evaluated using a multicriteria metric of flood protection alternatives might be effective to reduce the exceedance probabilities of extreme systemic losses under several scenarios of floods here we focus on flood management scenarios but any scenario can be considered for instance related to climate change it is hypothesized that there are conditions in which diversification could reduce the risk of extreme losses and be effective to address the nonsystematic portion of risks associated with climate change in other words good adaptive management can overcome unpredictable extrinsic fluctuations convertino et al 2014b this risk is non systematic because extrinsic and potentially unpredictable versus the risk associated to management the risk model determines a probability distribution of quantified flood losses in which average covariances and any other statistical moment for several types of joint flood protection assets are the output of numerical simulations the spectrum of return periods is selected to measure the risk reduction of selected flood protection portfolios among all feasible ones including the status quo yet the epdf can be considered as a macro indicator of flood protection of a basin where neutrally balanced bottom up basin heterogeneities and top down systemic risk control are considered by the portfolio model 1 3 how is the portfolio approach different the systemic perspective of the information theoretic portfolio decision model ipdm considering spatial temporal aggregated or explicit and decisional complexities is the main conceptual difference with respect to traditional models of flood management invoking static and discrete cost benefit analyses see for example zhou et al 2012 the underlying idea of the portfolio model is to integrate all information in the least complex way and formulate a decision that is comprehensive in terms of local heterogeneities and outcomes considered for any possible management plan convertino and james valverde 2013 the model leverages any information coming from any data or model such as hydrological and ecological impact models and guides further scenarios by exploring all management plans fig 1 illustrates the computational steps and information integration of ipdm the truly novel aspect of the paper is the integration of all models into ipdm via information theoretic analytics additionally ipdm can also guide data collection and define site importance in providing data considering the analysis of information entropy complexity sensitivity and diversity more technically the main innovative and integrated models of the paper and ipdm are the following flood susceptibility prediction and flood delineation the flood susceptibility is determined by the maxent model phillips et al 2006 phillips and miroslav 2008 convertino et al 2013b based on observed flood occurrences dependent on fcs plans land cover use and hydrogeomorphological variables the integrated probability distribution function that is associated to a spatial distribution of floods constitutes the basis for the calculation of the systemic flood risk sfr that also considers criticality and efficiency of fcs the exceedance probability distribution defines the arrangement and likelihood of floods the flood delineation method establishes a threshold on the maxent flood susceptibility for calibration based on historical floods different thresholds allow stakeholders the selection of a flood size distribution with a desired maximum return period multi criteria decision analytical model and pareto optimization for flood plan design the inclusion of a multi criteria decision analytical model mcda for assessing the systemic flood risk sfr to minimize a systemic value to optimize can be formulated by considering social environmental and economic outcomes related to floods an optimization model is used to determine pareto frontiers that define optimal flood control plans for the whole basin the optimal plan is defined as the one that optimize the systemic value after exploring all potential combinations of flood infrastructure portfolios this optimal sfr is not necessarily the absolute minimum if other socio economical and environmental criteria need to be maximized thus the non linearity between susceptibility risk and value is highlighted by the model if time is considered explicitly fcs strategies should be selected as optimal combination of single time point fcs plans in an adaptive management framework convertino et al 2014b information theoretic global sensitivity and uncertainty analyses the adoption of an information theoretic global sensitivity and uncertainty analyses igsua niklas lüdtke et al 2008 to characterize all input factors model parameters and state variables probabilistically and propagate their uncertainty to outputs predicted floods igsua also allows to select the most accurate and least complex predictive model within the possible maxent models where accuracy is about the distance between predictions and data and complexity is about the number and amount of predictors used in the model in a broad occam s razor purview grimm et al 2005 and for a selected return period of floods gsua also allows a better probabilistic characterization of the epdf of floods that can be used as reliable macroecological indicator of systemic hydrologic risk 2 materials and methods 2 1 the tiber river basin the tiber basin figs 3 and 4 is the largest river basin in central italy second largest river in italy draining towards the mediterranean sea and covering a land area of 17 500 km2 approximately various studies have investigated the hydrological behavior of the basin fiseha et al 2013 these studies were conducted considering the hydraulic and hydrologic processes taking place in the basin ecosystem among others flood forecasting see for instance calenda et al 2000 calvo and savi 2009 napolitano et al 2010 flood routing franchini et al 2011 and soil moisture assessment brocca et al 2009 on selected parts of the tiber river piccolroaz et al 2015 2016 di baldassarre et al 2017 the mathematical models used in these studies were of different nature from process based hydrogeomorphological models to machine learning models such as artificial neural networks much attention has been given to the issue of flood risk as the tiber river passes through many historical places in the region s urban areas including the historical core of the city of rome calenda et al 2005 rome is subjected to a non negligible risk of inundation when extreme floods propagate along the tiber river natale and savi 2007 severe floods characterized by a return period slightly lower than 200 years although 100 years floods are also very severe volpi et al 2015 can in fact overtop both the left and right river banks and inundate the northern outskirts of rome while extreme events characterized by a return period of about 1000 years can submerge large parts of the monumental center of rome the overtopping of river banks is predicted to occur for the 200 year design flood event near ponte milvio the oldest roman bridge in rome in a short reach 200 300 m long where the levees elevation is slightly depressed causing the potential overtopping of the retaining walls muraglioni extreme events characterized by a return period of about 1000 years can overtop the levees in different segments submerging large parts of the monumental center of rome the tributary network of the tiber is also characterized by elevated flood risk for the significant impact of land use changes related to transportation networks channel road intersections impact the water flow capacity and conveyance during extreme precipitation events causing frequent inundation conditions nardi et al 2015 despite the large heterogeneities of the basin most of the flooding studies in rome are associated to single hydrologic extreme responses rather than focusing on the whole river system flood patterns yet the latter are not mapped to the underlying physiographic climate and flood control infrastructure features that are potentially responsible for their occurrence yet the tiber basin provides an ideal setting for testing the proposed portfolio management model considering the magnitude and diversity of flood related forcing factors socio ecological actors and the increasing hydrologic change and urbanization trends that characterize such large basin 2 2 stochastic portfolio decision model in this paper the portfolio model in convertino and james valverde 2013 is extended to the context of floods in river systems and used via information theoretic models such as maxent and igsua fig 1 shows the conceptual diagram of the interlinked models whose connection is related via input output relationships the maximum entropy model maxent dudik et al 2007 phillips and miroslav 2008 elith et al 2010 merow et al 2013 based on the maximum entropy principle jaynes 1957 1988 was previously used in convertino et al 2013b and convertino et al 2014a for the dynamical prediction of landslides and avian species habitat in large scale ecosystems at a variety of different resolutions recently the model has been extended to microbial ecosystems li and convertino 2019 furthermore here ipdm is enriched by a fully stochastic characterization of all state variables and model parameters in an information theoretic framework such as in niklas lüdtke et al 2008 igsua is used for this purpose such framework describes the variability of the predicted patterns in a pattern oriented modeling purview grimm et al 2005 where in this case the exceedance probability distribution of flooded areas and their geographical distribution are the patterns to predict driven by the variability of local and systemic river network features i e for instance fcs efficiency and drainage connections systemic features are always exerted by the river network that defines the spatial dependencies among different areas resulting into a interconnected flood dynamics even in absence of atmospherical dependencies among rainfall patterns see e g steinhaeuser et al 2012 for climate teleconnections these geomorphic features are well known to influence many river dependent processes such as soil saturation sediment transport hydrochory and the spread of passive and reactive agents rinaldo et al 2018 agents are particles in a modeling sense whose intrinsic and extrinsic dynamics the latter more deeply dependent on the environment than the former determines the patterns of interest in the case of flood patterns local environmental conditions and systemic networks drainage flood control structure interdependency and green infrastructure networks determine flood patterns in a more general decision theoretic sense agents are stakeholders that have local and global effects on the emergence of floods via their actions and decision making at multiple scales of river basin ecosystems the ipdm modeling approach is conceptually and theoretically equivalent to the neutral metacommunity model of muneepeerakul et al 2008 and convertino 2011 that is essentially describing patterns formation of ecosystems resulting from the collective behavior of species agents generally speaking driven by local and systemic factors in this context we adopt the same philosophy but we disregard the dynamics over time yet we focus on the local generating function in this case the drainage area that is the scaling function governing the runoff in fact under homogenous conditions the drainage area is the precise surrogate of the runoff rather than the habitat suitability as in muneepeerakul et al 2008 and convertino 2011 that is inferred via maxent and the network is considered as a static network however network cumulated features are considered i e the drainage area is dependent on the network on each site further analyses about the evolution of the network via information theoretic models for forecasting the dynamics of runoff are worth exploring and undergoing however flood management is more concerned about incorporating long term dynamics therefore this approach that neglects the evolution of runoff at short time scales is very oriented toward long term management and design of water infrastructure in river basins vs real time forecasting of floods 2 3 2d maxent flooding risk model in a ipdm context a payoff function is defined as the balance between total benefits or systemic value dependent on the reduction of systemic flooded area causing socio environmental damages and the cost of flood control structures fcs this is in analogy to other systemic management models such as the enhanced adaptive management models in convertino et al 2014b where the difference in payoff is evaluated for different scenarios altering local and systemic conditions and as a function of data quality measured as value of information i e the difference in payoff for different data quality states more technically the payoff is a multicriteria function that considers multiple asset arrangements and balances benefits and fcs costs see eq 4 because cost is considered as a constraint in a pareto optimization we prefer to talk about systemic value rather than payoff flooded areas are here identified by a layer of occurrences and other numerically determined socio ecohydrological layers dependent on their importance for flood prediction land cover use flood control structures drainage area and the topographic index which are socio ecological infrastructural and geomorphic variables which are rasterized information provided to the model fig 4 the dynamics of the systemic value whether time is explicitly included in the model can be described for example by a langevin based model when values are considered or by a fokker planck model if the probability distribution is considered li and convertino 2019 shows a temporal application of the model where pdfs are propagating macroecological values in a microbial ecosystem and different ecosystem states are detected the same can be done for flood where transitions occur during the time varying hydrological dynamics of a basin in this dynamics criticality occurs when the propagating effects are maximized that is when cascading effects such as floods or information transmission in general are maximized note that criticality is something desired in biological systems vs something not desired in the case of floods in an anthropocentric purview here the dynamics is not taken into account explicitly and the systemic value is predicted as a static average pattern dependent on the average flood pattern associated to a return period of 200 years the 200 years floods were obtained from validated floodplain zoning model grounded on geomorphic theories for large scale identification of floods using digital topographic datasets nardi et al 2006 2013 2015 floodplain maps are available for the tiber river basin as well as at the global scale see nardi et al 2019 and https github com fnardi gfplain using the global floodplain gfplain geomorphic model s manfreda et al 2014 morrison et al 2018 nardi et al 2018 annis et al 2019 nardi et al 2019 kara scheel et al 2019 the probability of a flood at time t note that here we adopt a macroecological characterization of the probability convertino et al 2013b i e an average susceptibility of a community but a temporal assessment is possible for time varying conditions p y 1 c t is calculated as 1 p y 1 c t f c t exp η c t p y 1 f c t where f c is the probability density of covariates c across the river basin and η c α ρ h c α is a normalizing constant that ensures that f 1 c integrates to one f 1 being the probability density function pdf of the flooded area occurrences and ρ is the constant lagrangian multiplier of the maxent features h c phillips et al 2006 phillips and miroslav 2008 elith et al 2010 merow et al 2013 the lagrangian multiplier that multiplies all environmental features finds the optimal trade off between model complexity defined as the number of environmental variables used as predictors and model accuracy that is the distance between predictions and data convertino et al 2013b merow et al 2013 the set of parameters is identified by minimizing the prediction error between the observed and modeled floods features are transformations of the covariates in the covariate space and this allows a faster and more precise computation rather than operating in the geographical space elith et al 2010 convertino et al 2013b further details of the model can be found in convertino et al 2013b that was the first application of the model in hydro geomorphology the susceptibility of floods in eq 1 is calculated for each pixel defined by the digital elevation model fig 4 this susceptibility can be later averaged at the subbasin scale or any scale of interest for the mcda and pareto optimization model calculations the probability distribution of a flood is recalculated for each fcs plan fcs plans are matrices that alter the probability of occurrence of a flood these matrices are populated with the fcs efficiency that is the factor determining the local influence of a fcs to decrease the likelihood of a flood to occur eq 1 this efficiency can be estimated from literature or inferred ex post the calculation of floods dependent only on one fcs typology at a time thus the covariate c and fcs plans allow one to predict the multiplex flood susceptibility where multiple networked layers are interdependent from each other the fcs matrix is a multilayer matrix in case a diverse set of fcs are used the collectivity of fcs in terms of their diversity and spatial positioning determines the fsc effectiveness and systemic risk section 2 6 describes in details how the systemic risk is calculated predictions of floods may be dependent on the number of observations maxent predicts the flood susceptibility i e the probability to have a site flooded of each pixel nearby the centers of mass of an observed flood fig 2 as a function of the relationships inferred between the environmental variables and the centers of mass of randomly selected observed floods background points at the basin scale for this reason we used a procedure similar to coarse graining widely adopted in hydrology see for example convertino et al 2007 to understand how flood predictions are dependent on the number of observed floods considered as inputs of the model fig s2 note that this is analogous to a coarse graining analysis since zooming out preserves the average features of the variables considered but reduces the computational needs related to the data in input yet it is a data reduction approach that aims to preserve the predictive power of the model fig s3 shows the probability of flood susceptibility conditional to the selected predictors also called response curves of maxent and divided by likelihood of susceptibility in the random fcs scenario in the predictive scale invariant region of the model that provides the highest information content servadio and convertino 2018 li and convertino 2019 2 4 flood delineation and fractal characterization the flood delineation method is based on a threshold assigned on the maxent flood susceptibility and calibrated on observed or predicted floods with a certain return period such method allows easily the selection of the flood size distribution with a desired maximum return period nearest neighbor pixels according to the von neumann criteria or 4 neighborhood whose flood susceptibility is higher than the calibrated flood susceptibility threshold are categorized as flooded areas fig 2 the validation of the maxent model is based on reproducing the floods with the specified return period associated to a certain susceptibility threshold additional explanation of this delineation method shown in fig 2 is contained in convertino et al 2013b in the context of landslides the fractal distribution of observed and predicted flooded areas is used as a macroecological indicator of hydrological risk this distribution is captured with a varying degree of accuracy by both the box counting method see supplementary information and the korcak s law method that define flooded areas as patches these areas define the epdf shown in fig 5 the box counting method fig s1 overestimates the fractal dimension with respect to the korcak s law method as shown in convertino et al 2013c and does not precisely characterize finite size effects along the end of the tail of the distribution thus we consider the korcak s law method to determine the more likely shape and power law slope of the exceedance distribution of flooded areas that is related to the flood pattern fractal dimension the probability of exceedance of the flood areas that can be considered as the equivalent of the patch size in an ecological context known in literature as korcak s law korcak 1940 nikora et al 1999 convertino et al 2013c is 2 p s s c s ε f s s c where s is the flood area c is a constant f is a homogeneity function that depends on a characteristic flood size s c dependent on the basin size and hydrological dynamics and ε d 2 is the scaling exponent of the pareto epdf considering self affinity of floods korcak 1940 mandelbrot 1982 convertino et al 2013c d is the fractal dimension of floods that is derivable from both the epdf based on the 1d vector of flood sizes and the spatial pattern of floods via different methods note that the relationship ε d 2 is only valid within the critical regime when system s aggregates are considered as patches as according the korcak s law convertino et al 2013c otherwise for general self affine systems ε 2 d f 3 d f for which the exponent defining the criticality of the system is larger the higher the fractal dimension the true fractal dimension is related to the hurst exponent h via d f 2 h that manifests the persistency of stochastic processes convertino et al 2013c the more fractal a process is the higher the fluctuations that occur and the lower h the probability of exceedance exhibits a power law behavior although exponential behavior is observed for small flooded areas a transition in pdf epdf is observed for different fcs plans either different heterogeneous fcs plans and homogeneous fcs plans for one fcs typology at a time the transition is related to both the spatial distribution of fcs which determines the plan effectiveness and the fcs local efficiency see section 2 6 the probability distribution of the flood size is used to validate the maxent model and the box counting estimates on the real occurrences the fit of the predicted distribution of patches is performed using a maximum likelihood estimation technique mle convertino et al 2013c in our case study for the current flood distribution status quo ε d 2 0 84 the absolute value d is 1 83 from the box counting method 2 from the space filling counts and 1 67 from the stability analysis and the korcak s law the last value is considered for calculating ε because it is calculated directly on the mapped floods and it is verified to be the most reliable estimate convertino et al 2013c 2 5 stochastic mcda and pareto optimization model ipdm adopts several foundational concepts drawn from modern portfolio theory mpt including the concept of pareto optimality pareto 1971 whereas mpt focuses primarily on the task of developing computationally tractable means by which to allocate resources among various expenditures and investments over time our primary focus is on developing a holistic integrated bottom up top down model to flood management with the use of machine learning models informed by macrophysical processes one may think that these models such as maxent can be replaced by detailed flood mapping hydraulics models at fine resolution however it is much less obvious about how to use the information coming from these models solely for guiding optimal management plans here the purpose is to expose ipdm for the first time and to demonstrate how this model can be used for regional planning and detailed flood mapping with selected information that is also potentially provided by other models whose predictions e g hydraulic mapping of observed floods with a certain return period can be used to calibrated and validate ipdm and in particular maxent one of the submodels of ipdm is the mcda model that defines the necessary information for characterizing ecosystem values specifically the mcda model used for evaluating each flood control structure fcs plan for all assets is a linear multi attribute value theory mavt model convertino et al 2013a in this paper assets are the areas or communities impacted by floods thus characterized by a flood size a more detailed characterization of assets can be done if additional information is available the size of this information is unlimited potentially but only certain information is relevant for the predicted patterns servadio and convertino 2018 here a fcs plan is constituted by the current or all other possible sets of fcs attributed to each community of the river network considering their efficiency for flood protection as we discuss later in more detail our community scale is the subbasin scale the mcda model ranks fcs by scoring them considering a value determined as a linear combination of criteria values and weights the mcda model calculates the local value of fcs plans v i j f c s with respect to asset j in area community i tables s1 s3 shows all input factors of the mcda model in a broader mcda framework it is better to assess a systemic value rather than a systemic risk to emphasize the resilience approach that should be considered in landscape management this approach identifies the most valuable portfolios of alternatives that optimize the systemic value the systemic value does not focus only on the maximization of the hazard dependent inverse risk but on the maximization of all ecosystem services given all feasible hazard controlled scenarios and their social and economic outcomes we assume implicitly the dependence on time of the predicted values via maxent that characterizes the average pattern of flood areas within a management period defined by the return time this approach is a macroecological approach that is widely used in ecology and population sciences for characterizing average or persistent patterns the mcda value or systemic value generally speaking the inverse of the systemic risk sfr if social and economical values are not considered that is ultimately the risk of assets at the basin scale given a fcs plan is calculated as 3 v i j f c s 1 ν j f c s f m j f c s m j i v i j f c s where v i j f c s k w j k x j k i f c s x j k i are the values of criteria k in the mcda model for asset j and area i in our case study all criteria are minimized so the optimization problem is actually a minimization problem however the mcda model allows for multiple criteria to minimize and maximize yet the value vs risk problem formulation is more general than the one presented the weights w j k are stakeholder preferences for the asset j and criteria k that are typically independent of the area i note that here we use the portfolio considering areas as assets but more explicitly the portfolio problem is a fcs problem because a fcs can be assigned to each area and we are only characterizing the effects of fcs in each area but more precisely multiple assets j can exist in the same or different areas i these assets protected by a set of fcs m can be human populations infrastructure and other human resources or ecosystem services broadly defined such as agricultural fields with a certain crop yield and vulnerable species in ecosystems see for example convertino and james valverde 2013 for an ecological application in this paper we just consider the flood area as a criteria for assessing flood risk however the systemic risk is not perfectly equivalent to the flood area both locally i e at the subbasin scale and at the basin scale because the former considers criticality and efficiency of fcs related to their life cycle status and ability to withstand floods in isolation as a function of their structure respectively see tables s2 and s3 as well as the non linear predicted flood area determined by the combination of fcs here criticality refers to the fcs vulnerability but a more complex multicriteria function can consider a more comprehensive vulnerability for all communities considered the inverse of the systemic risk is used to calculate the systemic value that ideally considers other ecosystem services the inverse of the flood area is the local fcs effectiveness that incorporates local fcs criticality and efficiency in a purely mathematical definition the convolution of systemic risk and value is equal to one if no other ecosystem services are considered in the formulation of value the monetary quantification of the risk or value is certainly interesting and necessary for practical management however here we focus only on proposing the model able to perform such comprehensive assessment of ecosystem services table s4 shows for instance economic and social services as an example also because of the unavailability of other data thus we leave the economic evaluation of floods to subsequent applications of the model the local value v i j f c s of a fcs plan is adjusted by the probability of success of flood protection given by the vulnerability of each asset under a fcs plan ν j f c s as later described this is more precisely the criticality of a fcs in this case study and the effectiveness of the fcs plan f m j f c s m j i considering all fcs arrangements that is dependent on the intrinsic efficiency of all fcs locally f m j potentially asset specific for example a flood prone area protects well urban areas but may not protect agricultural areas and the location with respect to all other fcs on the basin i e f c s m j i at the basin scale the effect of all efficiencies estimated from fcs flood protection potential gathered from data and the spatial location of fcs constitutes the overall effectiveness of the portfolio set at the basin scale here the unknown vulnerability is considered related to the structural criticality of each fcs evaluated by experts tables s1 and s2 the more critical a fcs is the higher ν j f c s and the lower its potential to protect against floods this structural criticality can be related to infrastructure age maintenance and other factors the systemic value at the basin scale of the whole fcs plan v t f c s is calculated as a euclidian distance where the components of the distance are the local value of areas dependent on a fcs plan weighted by stakeholder preferences w i j for areas and assets the systemic value that is ultimately the value of a population given a fcs plan is 4 v t f c s i 1 l j 1 j v i j f c s w i j 2 table s4 shows a snapshot of an ideal mcda model where ecosystem services based on the maxent inferred floods can be assessed for any area and asset in this paper we calculate v as the inverse of the flood size to maximize the size of floods everywhere in the basin is dependent on the fcs plan that determines the pdf of floods the pdf is the pattern on which the model is calibrated and validated the total number of portfolio plans or fcs plans is m l that is the number of possible fcs for each area raised to a power exponent equal to the number of areas each area can be a pixel in the basin dictated by the digital elevation model but any different delineation of communities can be decided the baseline model that reproduces the 200 years return period pdf of floods the 200 years floods are 10 larger than the 100 years ones on average is considered as the status quo for the current flood management approach after exploring all fcs plans the pareto optimal set is identified as the one that maximizes v t f c s constrained or unconstrained to the available resources convertino and james valverde 2013 here we explore all potential resources that are identified by the number of potential fcs one for each community of the drainage network the optimization model of ipdm is a linear mixed integer optimization algorithm that explores all possible combinations of fcs alternatives for each asset community one alternative at a time with their expected value and cost at the local scale the maximization of the systemic value or alternatively the minimization of sfr if economic and social endpoints are not considered is performed with and without the constraint of the available resources b these resources are the one available for flood management and here they are taken proportional to the number of fcs considering the unavailability of resource information note that in this context optimization coincides with maximization since all criteria are maximized in principle the number of portfolio combinations is dictated by the number of channelized cells at the lowest scale i e 11 321 for the tiber basin and the number of fcs i e 5 including the no action alternative specifically the number of portfolio plans is the product of the number of possible fcs for each community raised to a power exponent equal to the number of communities where each fcs is present due to the large number of streams where a fcs can be ideally placed we decided more reasonably to consider the number of subbasins i e 2308 for the extracted network with a threshold of 5 km2 as communities thus the total number of portfolio plans is 52308 small scales of subbasins allows stakeholders to have a more granular control of floods but large scales may be more effective this raises the question about which is the optimal scale for flood control this question is however postponed to further studies table s1 reports the fcs considered in this case study and table s3 reports other potential fcs such as expansion areas or flood prone areas e g floodplains riparian areas vegetative buffer strips and gauging stations that can be evaluated in basin flood management and flood related services these fcs are green infrastructure that also support other ecosystem services such as promotion of species habitat and dispersal filtering of chemicals and sediments and pathogen spreading abatement in the portfolio constrained case the cost of the fcs plan c f c s m 1 m i 1 l j 1 j c m f c s m j i cannot exceed the resources b in the case of the pareto optimization unconstrained by budget if v t f c s 1 v t f c s 2 and c f c s 1 c f c s 2 then the portfolio solution f c s 1 dominates f c s 2 thus all fcs envisioned by f c s 1 are selected those are the ones that shift the tail of the pdf of the flooded area to the left the most in the budget constrained pareto optimization if v t f c s 1 v t f c s 2 then the portfolio combination f c s 1 dominates f c s 2 it can be easily proven that a payoff function i v t f c s c f c s to maximize can be used to obtain the same pareto frontiers table s6 reports the inputs of the pareto optimization model and a ideal output 2 6 metamodeling for maxent model selection and testing metamodeling is the art to detect the model with the highest information power for a stated objective saltelli et al 2004 gu et al 2012 servadio and convertino 2018 this is in line with the information theoretic principle of optimal model deign servadio and convertino 2018 the optimal set of input factors is defined as the one that simultaneously minimizes the error e ˆ in reproducing the observed floods and maximizes model accuracy that can for instance measured by the area under the curve auc zweig and campbell 1993 the auc is solely dependent on the reported flood occurrences while the error is dependent on the delineation of predicted landslides based on the predicted flood susceptibility therefore both must be considered when assessing the model and this is one of our strength with respect to classical applications of maxent the auc is evaluated considering the receiver operator characteristic curve roc phillips et al 2006 phillips and miroslav 2008 the roc is a graphical plot of the sensitivity or true positives i e the percentage of predicted flood occurrences that match the observed ones versus the complement function that is the specificity or false positives i e the percentage of predicted flood occurrences which do not match any observation note that this comparison is done at the pixel scale before any flood delineation the auc compares the likelihood that a random flood occurrence site has a higher predicted value in the model than a random site where no flood occurs thus the higher the auc the better the prediction in the jackknife test that is used as the metamodeling design test each variable is excluded in turn from the maxent model run on instantiation and a model is created with the remaining variables then a model is created using each variable in isolation in addition a run is created using all available variables when only one variable is used in the prediction the auc measures the absolute importance of the variable in predicting flood patterns yet this is the first order sensitivity index see section 2 7 the difference between the auc with all variables and the auc for the single variable prediction is a proxy of the sum of interactions or interdependencies between the variable considered in isolation and all others yet this is the second order sensitivity index see section 2 7 thus the jackknife can be used also as a global sensitivity method for the model saltelli et al 2004 beyond being a model deign test to define which variable to incorporate after the calibration on the historical flood pattern for which the error e ˆ is minimized it is possible to retain the variables for which the jackknife test shows an auc greater than 0 5 that is a standard threshold in establishing the importance of the environmental variables considered auc greater than 0 5 is meaningful that any prediction is beyond a random pattern prediction where flood are randomly predicted over space the error e ˆ is observed to be proportional to 1 n p n h where n p is the predicted number of flooded pixels over the total number of historical flooded pixels n h the calculation of 1 n p n h is conditional to the flood occurrences thus this calculation is conditional to the delineation of floods fig 2 2 7 global sensitivity and uncertainty analyses in a variance decomposition framework saltelli et al 2004 servadio and convertino 2018 the variance of the systemic value the payoff i e the systemic value minus the fcs plan cost or of any other selected function e g the flood area is attributed to the intrinsic local variability of socio environmental determinants and systemic network determinants this formulation is considering space explicitly but the variance of the systemic value can lump together spatially defined variables and the variability can be attributed to the intrinsic variability of single variables and their interactions servadio and convertino 2018 li and convertino 2019 this variance based approach has been adopted in convertino et al 2013b for the analysis of fundamental determinants of landslide patterns in an information theoretic approach we use a broader application of gsua via the consideration of the entropy of the payoff that is defined as the necessary and sufficient information contained in its probability distribution and derived from underlying variables a fully probabilistic information theoretic gsua that considers all probability distribution functions pdfs of variables and their entropy as in niklas lüdtke et al 2008 and servadio and convertino 2018 li and convertino 2019 more recently is adopted in this context we aim to determine the relative importance of variables used in predicting flood susceptibility considering the payoff its entropy can be seen as the sum of the intrinsic variability conditional to all other areas or variables m i y i y j i e the mutual information and the entropy h y c that is how much the variability of the underlying determinants contribute to the total local variability thus when finding the total entropy of the payoff the information balance equation that defines the total entropy is given by sum of the shannon entropies of all input variables lumped or not over space as considered alone for the variability of the payoff as well as the sum of their transfer entropies that is assessing the directional variable interdependence see li and convertino 2019 for a biological application of igsua the two quantities assess the variability of the payoff considering local and systemic features thus the total entropy of the flood susceptibility y can be written as 5 h y i h x i i j i t e i x i x j σ y where x i denote the i s variables covariate c in eq 1 that contribute to the payoff y in eq 5 i denotes a variable rather than an area as in eq 4 in this equation h denotes shannon entropy and t e denotes transfer entropy from the first variable to the second variable li and convertino 2019 eq 5 represents a fundamental concept of shannon s information theory and forms the general basis of sensitivity analyses servadio and convertino 2018 li and convertino 2019 the sum of the absolute values of transfer entropies is a proxy of the mutual information m i i p x j x i log 2 p x j x i p x j p x i servadio and convertino 2018 thus it considers the whole set of variable interdependencies here we do not consider the sign of variable interaction as in servadio and convertino 2018 because we focus on absolute flood predictability and because time dynamics of floods is not available explicitly σ y is a noise term that captures the unexplained variability of y related to variables not considered and discretization factors of the model e g due to numerical solvers and pdf inference algorithms eq 5 can also be extended in space variable first order importance and interaction for reproducing the pdf of floods are then calculated as mutual information indices mii niklas lüdtke et al 2008 that are defined by the mutual information normalized by the entropy of the output variable considering one variable or pairs of variables servadio and convertino 2018 these indices are respectively s i m i x i y h y and s i j m i x i x j y h y where x i is any covariate and y is the flooded area or payoff in this study we use igsua for the flood susceptibility that is the fundamental predicted pattern influencing the payoff the use of the transfer entropy whether temporal data are available can give further information about the directionality of the causality between variables in a predictive sense of the model in space and time and the time lag of their causality 3 results and discussion the main impact of fcs is assessed considering the flood size tables s2 s5 although the model can handle any information about losses on assets via the mcda component of the model table s4 the exceedance probability distribution function epdf of the predicted flood size is shown in fig 5 the epdf is computed for the predicted flood size after thresholding the maxent susceptibility to delineate floods this epdf is computed considering the size of flooded areas defined as the ensemble of pixels with probability higher than 0 85 this flood susceptibility threshold on the flood susceptibility is suitable to reproduce the 200 years return time floods figs 2 and 3 the calibration of the threshold constitutes the validation of the model on the reported floods fig 3 fig 2 shows the criteria and procedure used for delineating floods in analogy to the landslide delineation method proposed in convertino et al 2013b data about the 200 years return time floods were constructed from scenario based efforts using standard hydrological models and mapping procedures nardi et al 2006 2013 2015 in these hydrologic models major rivers are integrated with hydrogeomorphic floodplain mapping for extending the flood prone area information at the basin scale other methods for flood delineation have been used by the hydrology community and proposed the use of predictors that are also used in this study such as the topographic index salvatore manfreda et al 2014 2014 samela et al 2015 2017 in flood risk assessment calibration is only performed on low frequency hydrologic data since 30 50 years of observations are available for river flows longer data series are available for precipitations only water infrastructure need to be designed for extreme events which may have never occurred thus hydraulics and hydrologic scenario modeling serve the purpose to determine those extreme events which are the inputs of ipdm interestingly we find that the scaling exponent of the flood size distribution ε d 2 0 84 is about double than the exponent of the runoff or drainage area i e 0 43 rodríguez iturbe and rinaldo 2001 this may shed some light on the relationship between these two random variables drainage area and flood size that are physically dependent on the river network structure the latter forces the propagation of runoff and floods along the scale free drainage pattern and cross river elongation features of the two phenomena floods and runoff may just be responsible for the different power law exponents a power law exponent that is more than double than the one for floods has been observed for landslides i e 1 92 see convertino et al 2013c and convertino et al 2013b hydrologically triggered landslides are of course related to the drainage network and dependent on the same hydroclimatological extremes triggering floods however landslides occurs on hillslopes of basins and yet their scaling exponent is higher than floods which dictates their more limited size the perimeter area relationship for floods shows an exponent of about 0 75 that leaving aside numerical corrections related to the calculation methods is in accordance to the estimate of the box counting the slope of the epdf of the flooded area provides an exponent equal to 0 78 these differences in the slope of the epdf of the flood areas are only related to the different methods used to calculate such distribution see methods section however all these methods are linked to each other as shown in convertino et al 2013c the higher scaling exponent of floods shows the lower criticality in a self organized criticality perspective bak et al 1988 and persistency of floods vs the runoff as expected this implies lower probability of floods to occur considering the variability of flood patterns on fcs plans the epdf is proposed as a macroindicator of phase transitions in flood dynamics the no fcs scenario may correspond to the unaltered ecosystem scenario this scenario shows an exponent ε that is lower than the basin regulated scenario which implies a higher probability for the same flood size on average moreover the no fcs flood size distribution has a much more exponential behavior for small floods that represent the random uncorrelated dynamics of frequent inundation events it is interesting to see that the regulated basin e g the random and current fcs plans has a larger maximum flood than the unregulated basin this is actually confirming the empirical evidence of many river basins where too much suboptimal flood control causes bigger floods the randomly flood regulated basin scenario is quite worrisome because of the extent of the power law regime that is much wider and with very large floods in this case the exponential cutoff for finite size effects in the tail of the distribution highlights that the maximum flood is just limited by the size of the basin but larger floods may occur in very extreme hydrological conditions in principle the maximum flood size is weakly constrained by the basin size and more sensitive to extreme hydrometeorological forcings all other epdfs different than the random epdf are truncated pareto distribution without finite size effects therefore the maximum flood size is much smaller than the one dictated by the size of the landscape if we define α 1 ε where ε is the slope of the epdf of the flood size then α 1 83 for the current fcs plan it is known that when 1 α 2 the first moment of the distribution the mean or average is infinite along with all the higher moments when 2 α 3 such as for the pareto optimal fcs plan the first moment is finite yet more controllable but the second the variance and higher moments are infinite thus some care needs to be placed in monitoring the optimal fcs plan despite the maximum predicted flood is much smaller than the current regulated state for the pareto optimal plan α is 2 2 for the drainage area α 1 43 since ε 0 43 thus also the drainage area has infinite mean theoretically which highlight the fact that extremely large runoff phenomena occur but they are not necessarily determining floods in fig 6 we show the pareto optimal plans of fcs along the pareto frontier in green after considering all potential combinations of fcs tested on the river basin all potential fcs are placed at the subbasin scale fig s4 table s7 shows an example of a pareto optimal solution each optimal plan is located along the green pareto optimal frontier generated by considering the number of fcs as the constraint function this maximum number is in principle equal to the number of all subbasins if they are all regulated however this is never the case of course the maximum number of fcs considered in fig 6 is an example based on some more fcs beyond the current number of fcs see fig 3 and inset in fig 6 the current fcs plan 16 fcs is shown against the optimal fcs plan 15 fcs in the pareto chart the inset in fig 6 is showing over space the newly suggested fcs according to the pareto solution the current fcs that are not envisioned in the pareto plan removed and the ones to keep maintain this result shows the importance of ipdm in detecting the best spatial arrangement of fcs also in consideration of current and future needs of flood protection which can be also related to socio ecological services dependent on fcs such as the promotion of species dispersal and the efficient production of hydropower roy et al 2018 kareiva 2012 and ziv et al 2012 are all examples of potential multi ecosystem service problems that ipdm can accommodate all these needs can be included into the mcda model and be traded off with the flood protection needs the flood susceptibility and epdf over space of the pareto optimal solution as well as of all other scenarios considered is shown in fig 7 the total number of possible communities where to place a fcs are 2308 that corresponds to the subbasins for the 5 km2 thresholded stream network fig s4 the corresponding epdf of flooded areas in the pareto scenario is reported in fig 5 note that here the efficiency is identifying the intrinsic structural potential to retain floods of a fcs table s3 and the effectiveness is assessing the true field capacity of a fcs plan to retain extreme events causing floods this capacity is learnt based on data of floods and environmental data note that floods are also the outcomes of the fcs spatial arrangement beyond the socio environmental factors characterizing a basin yet pareto solutions are those that maximize the systemic effectiveness of a portfolio or set of fcs we believe that this concept of interconnected effects is often overlooked in water resource management versus traditional approaches to contain floods in specified locations without thinking in details about the effects in downstream areas the difference in systemic risk between the no fcs and the current scenario first and second plots is actually really large despite the epdf of these two scenarios is somewhat similar fig 5 such difference is due to the mcda model that considers fcs efficiency and criticality tables s2 s5 which have a profound impact in the determination of the systemic risk if social and economical criteria are also included in the calculation of the systemic risk we would have a large difference with respect to the risk determined solely by the flood size distribution the formulation of the systemic value in eq 3 that is the inverse of the systemic risk if only floods are considered in the mcda model somehow reflects the traditional framework of risk assessment where hazard vulnerability and exposures are calculated and combined together in our case the hazard is constituted by the occurrence of a flood or not exposure is determined by the predicted flood size dependent on all fcs effectiveness i e the systemic driver and vulnerability is related to both the specific local criticality and the efficiency of fcs to protect against floods the it should be noted that the systemicity of the risk is highly dependent on the joint effect of all fcs arrangements in the basin which alters the susceptibility calculated by eq 1 this in turns determines the extent and connectivity of floods fig 7 top plots the predicted floods extracted from the flood probability higher than 0 85 the flood susceptibility and the cumulative output of maxent are shown in fig 7 in maxent the log output is simply a logistic transformation of the raw output which indicates a relative probability of occurrence of a flood this is shown in the bottom plots of fig 7 in the cumulative output middle plots the value of a site is the sum of all the raw outputs from the sites with equal or lower values times 100 this represents the percentage of the potential distribution that is contained within all the sites that are not more suitable than the one considered in other words the logistic output or flood susceptibility output in the bottom plots provides the local probability value of observing a flood in a site or better a potential part of a flood at the pixel scale while the cumulative output of the model is equivalent to the exceedance probability distribution that considers the ability to get that value of flood probability or higher with respect to all other pixels in an information theoretic framework the local probability can be thought as the local response or action local generation of each site to local environmental features in generating floods while the epdf can be thought as the systemic response or reaction systemic response of each site considering surrounding sites and their propensity in generating a flood see gutiérrez roig et al 2016 for a broader view of such information theoretical purview recently gao et al 2016 formulated a general model for resilience where local and systemic network variability are decoupled in assessing the systemic response here we do somewhat the same by decoupling those local flood propensity features dictated by heterogeneous environmental features and network features dictated by the river network and fcs arrangement in the middle plots of fig 7 the more red a site is the higher the contribution of many other sites in determining the flood susceptibility in that point yet the more that considered point is connected with all others thus from the epdf in space it is possible to determine clusters of interdependent points that are likely forming floods the random fcs flood scenario is the one with most of the areas in red which are largely interdependent and giving rise to the power law distribution of flood size it is interesting to note that the pareto optimal management solution decrease the pareto criticality of floods the far right distribution of floods in fig 7 shows that floods are limited in size and disconnected which avoids cascading effects down the drainage network in the pareto optimal solution both flood susceptibility and epdf are much smaller than in any other scenario middle and bottom plots of fig 7 this gives much higher flood protection to the ecosystem as a whole and to critical cities such as rome and perugia located in the south west and north regions of the basin respectively see fig 3 the distribution of fcs is shown by the red dots in the top plots of fig 7 the pareto fcs are distributed much more homogeneously than in other scenarios yet this underlines the fact that structural distribution of assets is very different than their functional distribution manifested by floods in terms of flood size and susceptibility the worst scenario is the random fcs plan the random placement of fsc causes more harm than benefits by likely destroying the natural formation of floods along the drainage network however the suboptimal no fcs scenario is also relatively worrisome considering the more widespread distribution of medium value flood susceptibility than the random scenario the random scenario is more extreme in terms of clustered areas at high flood susceptibility including around the city of rome while the no fcs scenario is worse for distributed medium flood susceptibility this is emphasized by the epdf at the basin scale in fig 5 the way in which the normalized systemic value of fcs for flood protection increases with fcs typological diversity is shown in fig 8 in our case the diversity of fcs protecting assets is not very high since the maximum number of diverse assets is five table s1 lists all fcs plus the no action alternative here we show four groups of assets since two of them i e earth dam with mantle and nucleus and gravity brick dam and filled with soil spurs see table s1 can be grouped in the same structural category earth dam the cost is here taken as a percentage of each infrastructure type selected by ipdm at the basin scale with respect to the feasible total number of fcs in the basin in real settings the cost function of fcs is determined considering construction and maintenance costs but it can also integrate other monetary functions that are forming the basin payoff function together with the reduced flood risk they contributed to generate see fig 8 the reader should note how for high fcs typological diversity the cost of the most expensive assets concrete dams is decreasing with respect to low diversity scenarios in favor of other assets this is likely because this massive high cost fcs are disregarded in favor of other soft fcs optimally organized throughout the basin this high resource optimal diversification pattern is typically seen in other human and natural system portfolio contexts such as in stock markets and ecosystems haldane and may 2011a where optimality is reached via collective organization scale free and diversification for instance zhou et al 2012 has evidenced this in the context of flood protection it is known that optimal solutions are more stable and more diverse as well as less complex considering the balanced mutual interdependencies of assets may 2013 and their scale free distribution of values that make systems more predictable li and convertino 2019 li and convertino 2019 analyzed the diversity complexity stability landscape of microbial ecosystems and derived universal organization rules for complex systems anchored to the pareto optimality principle stability is here considered as ecosystem occurrence in an optimal state as well as the ability to move to better states or to bounce back if perturbed yet it is a broader view of resilience convertino and valverde 2019 li and convertino 2019 in ecological literature many papers have speculated the positive diversity health relationship that here we see in the context of flood protection health is here an ecosystem indicator measured as a continuous function by the systemic value that is the inverse of the number of flooded areas if other socio economical outcomes are not considered the higher the likelihood of floods the higher the health risk considering both morbidity and mortality outcomes of populations this is particularly true in developing countries where floods spread infectious and toxic agents but also in extreme settings such as in europe and usa where anthropogenic pressure and climate change are exacerbating flood frequency and their effects which can lead to infrastructure and human losses in a broader sense the diversity resilience or diversity health hypothesis formulated for socio ecological systems is reflected by the findings of fig 8 in a more engineering sense with respect to our aforementioned perspective resilience is a broad concept underlying positive feedback mechanisms leading to optimal and stable or increasing system s performance after a sequence of perturbations valverde and convertino 2019 that can be achieved via activated or automated heterogenous management such as a re arrangement of system s controls e g fcs in this case the difference in systemic value is defined as the value of information voi convertino et al 2014b that is the amount of information a decision maker can be willing to pay before making a decision in our case study this information can be economically quantified and measures in more depth as the δ between the optimal solution and the status quo or any other state being evaluated for the basin in terms of flood protection it is interesting to note that for a very diversified portfolio the normalized cost of optimal fcs plans is lower this is likely related to the better distribution of resources allowed by diversely priced fcs to underline the importance of diverse portfolio approaches we re emphasize how the susceptibility of maxent fig 7 is different than the systemic risk fig 6 even when social and economical outcomes are not considered in fact the risk is based on the flood size that is calculated a posteriori the definition of a threshold on the flood susceptibility to represent the 200 year return time floods additionally the systemic risk is not the inverse function of the systemic value because the latter is based on a more complex multicriteria function containing social and economical variables if included eq 4 this underlines the non linear relationships between susceptibility risk and value and emphasizes the need of resilience focused approaches based on long term trajectories of ecosystem values valverde and convertino 2019 convertino and james valverde 2019 the systemic uncertainty evaluation of the model is shown in fig 9 in particular the roc to measure prediction accuracy is show as well as the igsua of all variables introduced in the prediction in the form of jackknife test phillips et al 2006 phillips and miroslav 2008 convertino et al 2013b and functional network of interdependency among predictors the igsua results are shown for the optimal version of the model with the highest information theoretic power i e the highest entropy in eq 5 this high entropy state corresponds to the lowest complexity and highest predictive accuracy see also li and convertino 2019 for an application to microbial ecosystems 89 of the observed floods are reproduced which translates into a auc 0 89 all variables whose regularized training gain that is the variable specific prediction importance is lower than 0 5 are disregarded from the model the roc curve shows that the smallest floods are not reproduced well considering the small values of the sensitivity for small floods this is a general result of maxent that needs to calibrate on some event sizes that are larger than small event sizes in our case study these are the larger floods that encompass a wide range of environmental variability however it is majorly important to represent extreme events better than small ones because they contribute more to forming the extreme risk the roc curve shows an exponential increase in sensitivity after small size events while it is certainly true that small high frequency events can carry substantial economic losses big events can be catastrophic events that destroy completely the areas being flooded with cascading outcomes such as diseases and generalized transport of pollutants the jackknife test bottom bar plot in fig 9 provides an estimate of the first and second order sensitivity indices s i and s i j respectively for predicting the observed landslides see section 2 7 the functional interaction network shows s i and s i j proportional to the diameter of nodes and the width of links respectively elevation is the most important predictor of flood susceptibility and it majorly affects the topographic index as expected since ti is derived from the elevation information ti also known as the wetness index wi is a steady state wetness index that is commonly used to quantify topographic or geomorphological control on hydrological processes it is calculated based on the upslope area and the local slope convertino et al 2013b ideally in presence of dynamical information this index can also be dependent on time and contribute to almost real time forecast of floods the elevation is also affecting in a minor way all other predictors and the directionality of the functional dependence is expected to be from the elevation variable to all others whether transfer entropy is used to assess that functional variable dependence note that in this case study all these variables are static variables thus it is not possible to assess the direction of their functional dependency see section 2 7 the low importance of drainage directions was expected because floods can occur for any value of drainage directions floodplains are low laying flat areas with undefined local flow directions in fluvial corridors flood waves move by cutting meanders and local flow patterns are progressing downstream along a main direction as a result flood prone areas are characterized by almost all values of drainage directions independently of the methods used to calculate them rodríguez iturbe and rinaldo 2001 nardi et al 2008 2019 the proposed ipdm is also suited to evaluate the impact of green infrastructure solutions for flood risk management vs standard grey infrastructure temmerman et al 2013 keesstra et al 2018 standard fcs are usually adopted for the lack of a quantitative measure of the multi sectorial social economical and environmental beneficial effect of non structural and green infrastructure solutions additionally there is currently no model that tells where to locate these green infrastructure in river basins optimally nevertheless the socio economic benefit of nature based solutions can be quantitatively evaluated by means of the proposed ipdm which supports innovative decision making towards large scale and long term green infrastructure strategies a quantitative data driven decision model could support the much needed cultural change for moving decision making towards river basin hydrology driven risk management based on diverse fcs and systemic basin scale actions this would pave the way to a novel flood management paradigm based on the combination of green and grey solutions or any other diverse solution recognizing the complexity of intertwined natural and human systems that optimize human and natural resources including life safety and ecosystem productivity lastly it is crucial to mention that ipdm is open to mental modeling that is the mapping and quantitative incorporation of the whole decision space of stakeholders involved in the problem kolkman et al 2005 wood et al 2012a b convertino et al 2016 in this case mental modeling would be focused on flood protection and all its dependent ecosystem services of biological social and economic nature rinaldo et al 2018 this kind of modeling grimm et al 2005 embraces the socio hydrology paradigm embracing sustainability that is currently relatively widespread in the hydrological community blöschl and montanari 2010 viglione et al 2014 blöschl et al 2017 risk management decisions that are informed by and address decision makers and stakeholder risk perceptions and behavior are in fact essential for effective risk management policy and ultimately environmental planning therefore mental modeling is completely fitting in the systemic risk purview of ipdm via the inclusion of the socio ecological aspects of the problem e g weights in the value of eqs 3 and 4 handled beyond the traditional hydraulics and hydrologic models 4 conclusions an information theoretic portfolio decision model is proposed as an optimal model and operational process to design optimal fcs plans at the basin scale this paper proposes a case study for the tiber basin and traditional fcs types but any basin and fcs can be considered by the portfolio model as for fcs types for instance flood prone riparian areas that embrace a more engineering with nature management approach can be considered overall the portfolio approach supports a participatory transparent and rigorous decision making proposition about environmental planning that focuses on the optimization of the systemic value of fcs services for the whole basin ecosystem the model can operationalize detailed flood mapping models based on hydraulic processes and can consider any scenario such as related to climate and land use land cover change coupled to all possible management plans a flood control strategy should consider different fcs plans over time considering the changing basin conditions for simplicity in this paper we show the average static application of ipdm anchored to floods with an extreme return period the flood susceptibility is proportional to the risk although in a non linear way considering the complexity of river basins dictated by land features geomorphological features infrastructure heterogeneities and diverse stakeholder preferences such as for the tiber basin considered the truly innovative part of the study is the portfolio framework and model that brings together several submodels aiming to predict characterize predict and control flood patterns beyond the novel introduction of ipdm the following results are worth mentioning the pareto management optimal solution is shown to remove critical cascading effects related to propagation of big floods downstream this can be seen by the broken spatial distribution of floods along the network that appeared more isolated one from another additionally the pareto optimal solution decreases consistently the maximum size of the largest flood yet it reduces the pareto power law regime or critical regime of the flood size distribution in other words the optimal paretian management plan reduces the risk related to the naturally self organized floods this pareto management corresponds to a neutral state where local conflicting small scale heterogeneities are well balanced into a maximim systemic value see li and convertino 2019 for a biological application where the neutral state corresponds to the critical state we propose the shape and slope in case of a power law distribution of the exceedance distribution of flooded areas that is related to the fractal dimension of floods in 2d as a macrohydrological indicator of phase transitions in flood dynamics the random fcs plan is worse than no fcs in terms of flood size distribution because fcs produce cascading effects in terms of connected floods that can be highly dangerous if the spatial arrangement maximizes flood propagation the no fcs solution determines the natural scale free distribution of floods if no action is taken this is the critical state that is undesired because persistent and with maximum flood size when considering criticality and effectiveness of fcs via the mcda model the systemic risk of the random fcs plan is lower than the no fcs plan this risk is different because it considers socio ecological endpoints beyond the flood size after the calculation of flood susceptibility the flood delineation method based on the predicted maxent flood susceptibility and the von neumann criteria on the pixel flood susceptibility is a novel information theoretic model that can be used more easily than more complex hydraulic process oriented models such as hydrogeomorphic floodplain delineation models or hydrodynamics models in calculating the flood area and yet the flood risk based on the flood size epdf the flood size epdf is a geomorphic function indicating phase transitions in flood patterns maxent is introduced from ecological sciences as a suitable model to predict flood susceptibility in this context it is found that elevation is the most important predictor of susceptibility the second most important predictor is the topographic index that is also known as the wetness index and this can be used as a dynamical factor when predicting flood susceptibility over time second order predictors are land cover use drainage area and drainage directions ipdm allows decision makers to create a baseline value of systemic risk or vice versa of systemic ecosystem value of the basin in terms of flood safety and socio economical outcomes yet any future investment can be compared to that baseline value via the use of the pareto frontier that shows the departure of any basin state from the optimal solution the pareto frontier also shows the necessary and sufficient investment for flood protection before reaching the plateau where additional investments in resources do not increase the systemic value significantly or does not decrease the systemic flood risk equivalently if only losses in terms of flood area are considered we show that the normalized systemic value of assets increases when the diversity of fcs increases but the spatial arrangement of fcs must be pareto optimal distribution this results shows the non linearity between structural features of the basin such as river network structure and fcs arrangement and functional features such as flood size and systemic value distributions the difference in systemic value can be considered as the value of information before reaching a decision about a fcs plan information that has high socio economical and environmental value that decision makers should be willing to pay for maintaining or increasing ecosystem value authors contribution m c formulated the original idea of ipdm for flood management designed the model performed the calculations and wrote the paper a a helped in structuring the data for the model and provided all the gis support f n provided the hydrological data and expertise and contributed to the writing of the paper acknowledgments m c acknowledges the funding from the gi core station for big data cybersecurity at hokkaido university sapporo jp the microsoft ai for earth program grant bio hydro geo dynamics mapping systemic earth risk https www microsoft com en us ai ai for earth grantsg and the nsf srn project srn integrated urban infrastructure solutions for environmentally sustainable healthy and livable cities http www sustainablehealthycities org f n and a a acknowledge the support of the water resources research and documentation center warredoc of università per stranieri di perugia and the financial support of regione lazio grant number a11598 research grant media valle del fiume tevere and the autorità di bacino del fiume tevere working group flood map ps5 updating for the city of rome by means of gis and 2d hydraulic modeling dr james valverde jr department of energy is highly acknowledged for comments to the paper and the original formulation of the portfolio model with m c all data have been made available https github com matteoconvertino floodteveremaxent glossary sfr systemic flood risk ipdm information theoretic portfolio decision model fcs flood control structure igsua information theoretic global sensitivity and uncertainty analyses maxent maximum entropy mcda multi criteria decision analysis ti topographic index dem digital elevation model lc lu land cover land use auc area under the curve mi mutual information mii mutual information indices appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 06 013 
26180,the increasing impact of flooding urges more effective flood management strategies to guarantee sustainable ecosystem development recent catastrophes underline the importance of avoiding local flood management but characterizing large scale basin wide approaches for systemic flood risk management here we introduce an information theoretic portfolio decision model ipdm for the optimization of a systemic ecosystem value at the basin scale by evaluating all potential flood risk mitigation plans ipdm calculates the ecosystem value predicted by all feasible combinations of flood control structures fcs considering environmental social and economical asset criteria a multi criteria decision analytical model evaluates the benefits of all fcs portfolios at the basin scale weighted by stakeholder preferences for assets criteria as ecosystem services the risk model is based on a maximum entropy model maxent that predicts the flood susceptibility the risk of floods based on the exceedance probability distribution and its most important drivers information theoretic global sensitivity and uncertainty analysis is used to select the simplest and most accurate model based on a flood return period a stochastic optimization algorithm optimizes the ecosystem value constrained to the budget available and provides pareto frontiers of optimal fcs plans for any budget level pareto optimal solutions maximize fcs diversity and minimize the criticality of floods manifested by the scaling exponent of the pareto distribution of flood size that links management and hydrogeomorphological patterns the proposed model is tested on the 17 000 km2 tiber river basin in italy ipdm allows stakeholders to identify optimal fcs plans in river basins for a comprehensive evaluation of flood effects under future ecosystem trajectories graphical abstract image 1 keywords river basin management floods systemic risk maxent portfolio decision model mcda 1 introduction 1 1 flood protection and distribution of floods flooding is a major natural hazard affecting some 520 million people every year claiming the lives of about 25 000 worldwide and causing global economic losses between usd 50 and 60 billion annually as a result it is essential that we seek to manage the risk of flooding in an effective and appropriate way also in consideration of climate change the assessment of changing dynamics of flood impacts is a public concern beyond many other climate change effects on populations liu et al 2019 the latest assessment of the intergovernmental panel on climate change ipcc ipcc 2014 on observed changes and future projections of floods was provided in chapter 3 of the ipcc special report on extremes often called the ipcc srex report1 a summary on projected flooding in this report stated that overall there is low confidence in projections of changes in fluvial floods this confidence is low due to limited and uncertain evidence of ecosystem drivers and because of the complexity of these causes at the regional changes that is not necessarily addressed quantitatively speaking thus a model that considers these heterogeneities such as hydrological land use cover change and socio political dynamical complexity and their uncertainty is needed for effective decision making aimed to systemic flood protection as for climate change in general the major changes are expected into a varied return period rather than average magnitude of the events considered including floods in particular the return period or frequency equivalently of extreme phenomena is varying because the underlying conditions influencing climate are changing or have been changing for long time in a diverse way most people and the literature talks about changes in magnitude of events that is not necessarily affected primarily rather the accumulation of unusually close events even of small or medium magnitude can bring to an intensification of extreme events resulting in extreme floods examples are about what happened recently in europe and in rome specifically where hydrologic changes driven by human induced land development have largely enhanced flooding nardi et al 2015 extreme flood events of such kind are like the ones in paris in the spring of 2016 and the 2018 european floods paprotny et al 2018 floods and other hydrology related extremes caused economic losses of euro 453 billion between 1980 and 2017 claiming the lives of more than 115 000 people across europe see zanardo et al 2019 for recent estimates of european floods similar patterns of floods and losses are observed in the americas see quinn et al 2019 for usa asia and australia infrastructures are typically designed for withstanding extreme events with a predetermined return period such as for the peak runoff of 100 years i e the runoff expected every 100 years on average volpi et al 2015 however recurrence periods are decreasing considering climatic changes therefore many extreme floods are occurring with higher frequency than in the past due to the inability of flood control infrastructure to contain them this is also associated to poorly planned and maintained urban water management systems i e storm water and sewer drainage originally designed for a 10 30 years frequency scenario that are now inefficiently managing even frequently expected rainfall events yet in general the probability distribution of events is changing with a tendency toward a longer power law distribution of events rather than an increase in the mean or the variance of these events bak et al 1988 fonstad and marcus 2003 van de wiel and coulthard 2010 convertino et al 2012 this trend has also a tendency to increase the criticality of flood events by decreasing the power law exponent regulating the flood size this implies increased persistency of flood phenomena bak et al 1988 convertino et al 2012 for this motivation that leads to high interdependencies between geographical areas beyond administrative boundaries the development of adaptive and systemic flood management models is needed in order to avoid local fragmentation of decisions that implies large sustained flood risks at the basin scale 1 2 current flood management models various models have been used to characterize the risk of extreme events to large urban and more localized infrastructure systems merz et al 2010 but very few models have addressed multiscale and multiasset risk assessment problems little has been done for ecological systems and far less considering societal impacts on populations affected by floods current models include probabilistic risk analysis and decision analysis considered as separate models meyer et al 2009 merz et al 2010 probabilistic models including bayesian analysis have been developed for situations in which there are sparse data of historical frequencies of extreme events such as extreme floods decision analysis has been used in the performance assessment of infrastructure policies and investments for instance see meyer et al 2009 for a multi criteria decision analytical application for flood risk there is a considerable literature that addresses the costs and benefits of investment in protecting infrastructure systems from the threat of extreme events in particular bier et al 1999 discusses several approaches for assessing and managing the risk of extremes which has inspired to focus on the exceedance probabilities of catastrophic flood losses these methods can be categorized as asset management methods where the assets considered were water infrastructure and the flood risk was calculated a posteriori based on the likelihood of a flood to occur and asset conditions usace 2012 other hydraulics hydrologic methods have been used to characterized the very local flooding risk without much consideration of infrastructure criticality and feasible multiasset spatial arrangements nardi et al 2015 all these studies are somewhat risk centered in a unidimensional way in other words these models do not provide indications of optimal management solutions integration of risk and decision science models considering socio environmental and economic outcomes nor the incorporation of spatial heterogeneities in a spatially explicit way lastly these methods are considering exceedance probability values rather than their distribution in a self organized criticality perspective bak et al 1988 fonstad and marcus 2003 and therefore missing the systemic probabilistic characterization of the patterns analyzed recently efforts have explored the applicability of financial analysis models to flood protection in particular modern portfolio theory markowitz 1952 was explored for the diversification of flood infrastructure investments financial portfolio analysis has been widely used in project selection and choice management tasca et al 2017 there is significant work applying such analysis to the financial risk associated to extreme events valverde and andrews 2006 haldane and may 2011a park et al 2012 convertino and james valverde 2013 including the use of metrics like value at risk var and conditional value at risk covar results of these models describe that markowitz type diversification markowitz 1952 can generate the highest expected economic return under an acceptable systemic risk threshold for a desired flood protection systemic risk is a risk that considers non linear functional and structural interdependencies among systems components leading to cascading effects haldane and may 2011b battiston and caldarelli 2013 helbing 2013 tasca and battiston 2014 battiston et al 2016 burkholz et al 2016 this is a signature of the self organized criticality of complex systems bak et al 1988 in flood management these interdependent features are for instance stakeholders type and distribution of flood control structures river network and landscape features which collectively determine non linear flood events as an example applications to the netherlands are significative aerts et al 2008 considering their pressing flood security issues in aerts et al 2008 through a systematic combination of four different types of flood protection assets excluding the no action alternative portfolios of asset types were constructed to reduce the variance of expected losses the variance reduction was greater with more types of assets compared with portfolios that only contained one asset these past efforts leave open the question about how such diversification might influence other population endpoints beyond the traditional economic metrics of performance of flood protection additionally the use of exceedance probabilities epdfs of extreme losses definable via a multicriteria function rather than just using mean variance portfolio approaches is missing we believe that there are significant simplifications to using average metrics or mean variance optimization of flood risk that often lacks of a spatial component characterization such as in zhou et al 2012 for designing flood protection plans considering extreme events first no study has yet suggested that variance minimization is appropriate as a performance objective for multicriteria projects in particular variance may not always be an appropriate component of the objective function in engineering decision making where return period design should be prevalent for example for coastal protection and flood investment for engineering design a mean variance approach does not represent extreme events because variance of events whose probability distribution can be a power law distribution with infinite variance for the power law exponent of the flood size does not characterize enough the severity of these events decision makers are concerned with low probability catastrophic events along with more frequently occurring and less severe events rather than mean variance optimization exceedance probabilities and return periods at several levels of systemic loss should be used as metrics of the risk associated to extreme events in hydrologic and ecological engineering yet this paper will explore how diversification evaluated using a multicriteria metric of flood protection alternatives might be effective to reduce the exceedance probabilities of extreme systemic losses under several scenarios of floods here we focus on flood management scenarios but any scenario can be considered for instance related to climate change it is hypothesized that there are conditions in which diversification could reduce the risk of extreme losses and be effective to address the nonsystematic portion of risks associated with climate change in other words good adaptive management can overcome unpredictable extrinsic fluctuations convertino et al 2014b this risk is non systematic because extrinsic and potentially unpredictable versus the risk associated to management the risk model determines a probability distribution of quantified flood losses in which average covariances and any other statistical moment for several types of joint flood protection assets are the output of numerical simulations the spectrum of return periods is selected to measure the risk reduction of selected flood protection portfolios among all feasible ones including the status quo yet the epdf can be considered as a macro indicator of flood protection of a basin where neutrally balanced bottom up basin heterogeneities and top down systemic risk control are considered by the portfolio model 1 3 how is the portfolio approach different the systemic perspective of the information theoretic portfolio decision model ipdm considering spatial temporal aggregated or explicit and decisional complexities is the main conceptual difference with respect to traditional models of flood management invoking static and discrete cost benefit analyses see for example zhou et al 2012 the underlying idea of the portfolio model is to integrate all information in the least complex way and formulate a decision that is comprehensive in terms of local heterogeneities and outcomes considered for any possible management plan convertino and james valverde 2013 the model leverages any information coming from any data or model such as hydrological and ecological impact models and guides further scenarios by exploring all management plans fig 1 illustrates the computational steps and information integration of ipdm the truly novel aspect of the paper is the integration of all models into ipdm via information theoretic analytics additionally ipdm can also guide data collection and define site importance in providing data considering the analysis of information entropy complexity sensitivity and diversity more technically the main innovative and integrated models of the paper and ipdm are the following flood susceptibility prediction and flood delineation the flood susceptibility is determined by the maxent model phillips et al 2006 phillips and miroslav 2008 convertino et al 2013b based on observed flood occurrences dependent on fcs plans land cover use and hydrogeomorphological variables the integrated probability distribution function that is associated to a spatial distribution of floods constitutes the basis for the calculation of the systemic flood risk sfr that also considers criticality and efficiency of fcs the exceedance probability distribution defines the arrangement and likelihood of floods the flood delineation method establishes a threshold on the maxent flood susceptibility for calibration based on historical floods different thresholds allow stakeholders the selection of a flood size distribution with a desired maximum return period multi criteria decision analytical model and pareto optimization for flood plan design the inclusion of a multi criteria decision analytical model mcda for assessing the systemic flood risk sfr to minimize a systemic value to optimize can be formulated by considering social environmental and economic outcomes related to floods an optimization model is used to determine pareto frontiers that define optimal flood control plans for the whole basin the optimal plan is defined as the one that optimize the systemic value after exploring all potential combinations of flood infrastructure portfolios this optimal sfr is not necessarily the absolute minimum if other socio economical and environmental criteria need to be maximized thus the non linearity between susceptibility risk and value is highlighted by the model if time is considered explicitly fcs strategies should be selected as optimal combination of single time point fcs plans in an adaptive management framework convertino et al 2014b information theoretic global sensitivity and uncertainty analyses the adoption of an information theoretic global sensitivity and uncertainty analyses igsua niklas lüdtke et al 2008 to characterize all input factors model parameters and state variables probabilistically and propagate their uncertainty to outputs predicted floods igsua also allows to select the most accurate and least complex predictive model within the possible maxent models where accuracy is about the distance between predictions and data and complexity is about the number and amount of predictors used in the model in a broad occam s razor purview grimm et al 2005 and for a selected return period of floods gsua also allows a better probabilistic characterization of the epdf of floods that can be used as reliable macroecological indicator of systemic hydrologic risk 2 materials and methods 2 1 the tiber river basin the tiber basin figs 3 and 4 is the largest river basin in central italy second largest river in italy draining towards the mediterranean sea and covering a land area of 17 500 km2 approximately various studies have investigated the hydrological behavior of the basin fiseha et al 2013 these studies were conducted considering the hydraulic and hydrologic processes taking place in the basin ecosystem among others flood forecasting see for instance calenda et al 2000 calvo and savi 2009 napolitano et al 2010 flood routing franchini et al 2011 and soil moisture assessment brocca et al 2009 on selected parts of the tiber river piccolroaz et al 2015 2016 di baldassarre et al 2017 the mathematical models used in these studies were of different nature from process based hydrogeomorphological models to machine learning models such as artificial neural networks much attention has been given to the issue of flood risk as the tiber river passes through many historical places in the region s urban areas including the historical core of the city of rome calenda et al 2005 rome is subjected to a non negligible risk of inundation when extreme floods propagate along the tiber river natale and savi 2007 severe floods characterized by a return period slightly lower than 200 years although 100 years floods are also very severe volpi et al 2015 can in fact overtop both the left and right river banks and inundate the northern outskirts of rome while extreme events characterized by a return period of about 1000 years can submerge large parts of the monumental center of rome the overtopping of river banks is predicted to occur for the 200 year design flood event near ponte milvio the oldest roman bridge in rome in a short reach 200 300 m long where the levees elevation is slightly depressed causing the potential overtopping of the retaining walls muraglioni extreme events characterized by a return period of about 1000 years can overtop the levees in different segments submerging large parts of the monumental center of rome the tributary network of the tiber is also characterized by elevated flood risk for the significant impact of land use changes related to transportation networks channel road intersections impact the water flow capacity and conveyance during extreme precipitation events causing frequent inundation conditions nardi et al 2015 despite the large heterogeneities of the basin most of the flooding studies in rome are associated to single hydrologic extreme responses rather than focusing on the whole river system flood patterns yet the latter are not mapped to the underlying physiographic climate and flood control infrastructure features that are potentially responsible for their occurrence yet the tiber basin provides an ideal setting for testing the proposed portfolio management model considering the magnitude and diversity of flood related forcing factors socio ecological actors and the increasing hydrologic change and urbanization trends that characterize such large basin 2 2 stochastic portfolio decision model in this paper the portfolio model in convertino and james valverde 2013 is extended to the context of floods in river systems and used via information theoretic models such as maxent and igsua fig 1 shows the conceptual diagram of the interlinked models whose connection is related via input output relationships the maximum entropy model maxent dudik et al 2007 phillips and miroslav 2008 elith et al 2010 merow et al 2013 based on the maximum entropy principle jaynes 1957 1988 was previously used in convertino et al 2013b and convertino et al 2014a for the dynamical prediction of landslides and avian species habitat in large scale ecosystems at a variety of different resolutions recently the model has been extended to microbial ecosystems li and convertino 2019 furthermore here ipdm is enriched by a fully stochastic characterization of all state variables and model parameters in an information theoretic framework such as in niklas lüdtke et al 2008 igsua is used for this purpose such framework describes the variability of the predicted patterns in a pattern oriented modeling purview grimm et al 2005 where in this case the exceedance probability distribution of flooded areas and their geographical distribution are the patterns to predict driven by the variability of local and systemic river network features i e for instance fcs efficiency and drainage connections systemic features are always exerted by the river network that defines the spatial dependencies among different areas resulting into a interconnected flood dynamics even in absence of atmospherical dependencies among rainfall patterns see e g steinhaeuser et al 2012 for climate teleconnections these geomorphic features are well known to influence many river dependent processes such as soil saturation sediment transport hydrochory and the spread of passive and reactive agents rinaldo et al 2018 agents are particles in a modeling sense whose intrinsic and extrinsic dynamics the latter more deeply dependent on the environment than the former determines the patterns of interest in the case of flood patterns local environmental conditions and systemic networks drainage flood control structure interdependency and green infrastructure networks determine flood patterns in a more general decision theoretic sense agents are stakeholders that have local and global effects on the emergence of floods via their actions and decision making at multiple scales of river basin ecosystems the ipdm modeling approach is conceptually and theoretically equivalent to the neutral metacommunity model of muneepeerakul et al 2008 and convertino 2011 that is essentially describing patterns formation of ecosystems resulting from the collective behavior of species agents generally speaking driven by local and systemic factors in this context we adopt the same philosophy but we disregard the dynamics over time yet we focus on the local generating function in this case the drainage area that is the scaling function governing the runoff in fact under homogenous conditions the drainage area is the precise surrogate of the runoff rather than the habitat suitability as in muneepeerakul et al 2008 and convertino 2011 that is inferred via maxent and the network is considered as a static network however network cumulated features are considered i e the drainage area is dependent on the network on each site further analyses about the evolution of the network via information theoretic models for forecasting the dynamics of runoff are worth exploring and undergoing however flood management is more concerned about incorporating long term dynamics therefore this approach that neglects the evolution of runoff at short time scales is very oriented toward long term management and design of water infrastructure in river basins vs real time forecasting of floods 2 3 2d maxent flooding risk model in a ipdm context a payoff function is defined as the balance between total benefits or systemic value dependent on the reduction of systemic flooded area causing socio environmental damages and the cost of flood control structures fcs this is in analogy to other systemic management models such as the enhanced adaptive management models in convertino et al 2014b where the difference in payoff is evaluated for different scenarios altering local and systemic conditions and as a function of data quality measured as value of information i e the difference in payoff for different data quality states more technically the payoff is a multicriteria function that considers multiple asset arrangements and balances benefits and fcs costs see eq 4 because cost is considered as a constraint in a pareto optimization we prefer to talk about systemic value rather than payoff flooded areas are here identified by a layer of occurrences and other numerically determined socio ecohydrological layers dependent on their importance for flood prediction land cover use flood control structures drainage area and the topographic index which are socio ecological infrastructural and geomorphic variables which are rasterized information provided to the model fig 4 the dynamics of the systemic value whether time is explicitly included in the model can be described for example by a langevin based model when values are considered or by a fokker planck model if the probability distribution is considered li and convertino 2019 shows a temporal application of the model where pdfs are propagating macroecological values in a microbial ecosystem and different ecosystem states are detected the same can be done for flood where transitions occur during the time varying hydrological dynamics of a basin in this dynamics criticality occurs when the propagating effects are maximized that is when cascading effects such as floods or information transmission in general are maximized note that criticality is something desired in biological systems vs something not desired in the case of floods in an anthropocentric purview here the dynamics is not taken into account explicitly and the systemic value is predicted as a static average pattern dependent on the average flood pattern associated to a return period of 200 years the 200 years floods were obtained from validated floodplain zoning model grounded on geomorphic theories for large scale identification of floods using digital topographic datasets nardi et al 2006 2013 2015 floodplain maps are available for the tiber river basin as well as at the global scale see nardi et al 2019 and https github com fnardi gfplain using the global floodplain gfplain geomorphic model s manfreda et al 2014 morrison et al 2018 nardi et al 2018 annis et al 2019 nardi et al 2019 kara scheel et al 2019 the probability of a flood at time t note that here we adopt a macroecological characterization of the probability convertino et al 2013b i e an average susceptibility of a community but a temporal assessment is possible for time varying conditions p y 1 c t is calculated as 1 p y 1 c t f c t exp η c t p y 1 f c t where f c is the probability density of covariates c across the river basin and η c α ρ h c α is a normalizing constant that ensures that f 1 c integrates to one f 1 being the probability density function pdf of the flooded area occurrences and ρ is the constant lagrangian multiplier of the maxent features h c phillips et al 2006 phillips and miroslav 2008 elith et al 2010 merow et al 2013 the lagrangian multiplier that multiplies all environmental features finds the optimal trade off between model complexity defined as the number of environmental variables used as predictors and model accuracy that is the distance between predictions and data convertino et al 2013b merow et al 2013 the set of parameters is identified by minimizing the prediction error between the observed and modeled floods features are transformations of the covariates in the covariate space and this allows a faster and more precise computation rather than operating in the geographical space elith et al 2010 convertino et al 2013b further details of the model can be found in convertino et al 2013b that was the first application of the model in hydro geomorphology the susceptibility of floods in eq 1 is calculated for each pixel defined by the digital elevation model fig 4 this susceptibility can be later averaged at the subbasin scale or any scale of interest for the mcda and pareto optimization model calculations the probability distribution of a flood is recalculated for each fcs plan fcs plans are matrices that alter the probability of occurrence of a flood these matrices are populated with the fcs efficiency that is the factor determining the local influence of a fcs to decrease the likelihood of a flood to occur eq 1 this efficiency can be estimated from literature or inferred ex post the calculation of floods dependent only on one fcs typology at a time thus the covariate c and fcs plans allow one to predict the multiplex flood susceptibility where multiple networked layers are interdependent from each other the fcs matrix is a multilayer matrix in case a diverse set of fcs are used the collectivity of fcs in terms of their diversity and spatial positioning determines the fsc effectiveness and systemic risk section 2 6 describes in details how the systemic risk is calculated predictions of floods may be dependent on the number of observations maxent predicts the flood susceptibility i e the probability to have a site flooded of each pixel nearby the centers of mass of an observed flood fig 2 as a function of the relationships inferred between the environmental variables and the centers of mass of randomly selected observed floods background points at the basin scale for this reason we used a procedure similar to coarse graining widely adopted in hydrology see for example convertino et al 2007 to understand how flood predictions are dependent on the number of observed floods considered as inputs of the model fig s2 note that this is analogous to a coarse graining analysis since zooming out preserves the average features of the variables considered but reduces the computational needs related to the data in input yet it is a data reduction approach that aims to preserve the predictive power of the model fig s3 shows the probability of flood susceptibility conditional to the selected predictors also called response curves of maxent and divided by likelihood of susceptibility in the random fcs scenario in the predictive scale invariant region of the model that provides the highest information content servadio and convertino 2018 li and convertino 2019 2 4 flood delineation and fractal characterization the flood delineation method is based on a threshold assigned on the maxent flood susceptibility and calibrated on observed or predicted floods with a certain return period such method allows easily the selection of the flood size distribution with a desired maximum return period nearest neighbor pixels according to the von neumann criteria or 4 neighborhood whose flood susceptibility is higher than the calibrated flood susceptibility threshold are categorized as flooded areas fig 2 the validation of the maxent model is based on reproducing the floods with the specified return period associated to a certain susceptibility threshold additional explanation of this delineation method shown in fig 2 is contained in convertino et al 2013b in the context of landslides the fractal distribution of observed and predicted flooded areas is used as a macroecological indicator of hydrological risk this distribution is captured with a varying degree of accuracy by both the box counting method see supplementary information and the korcak s law method that define flooded areas as patches these areas define the epdf shown in fig 5 the box counting method fig s1 overestimates the fractal dimension with respect to the korcak s law method as shown in convertino et al 2013c and does not precisely characterize finite size effects along the end of the tail of the distribution thus we consider the korcak s law method to determine the more likely shape and power law slope of the exceedance distribution of flooded areas that is related to the flood pattern fractal dimension the probability of exceedance of the flood areas that can be considered as the equivalent of the patch size in an ecological context known in literature as korcak s law korcak 1940 nikora et al 1999 convertino et al 2013c is 2 p s s c s ε f s s c where s is the flood area c is a constant f is a homogeneity function that depends on a characteristic flood size s c dependent on the basin size and hydrological dynamics and ε d 2 is the scaling exponent of the pareto epdf considering self affinity of floods korcak 1940 mandelbrot 1982 convertino et al 2013c d is the fractal dimension of floods that is derivable from both the epdf based on the 1d vector of flood sizes and the spatial pattern of floods via different methods note that the relationship ε d 2 is only valid within the critical regime when system s aggregates are considered as patches as according the korcak s law convertino et al 2013c otherwise for general self affine systems ε 2 d f 3 d f for which the exponent defining the criticality of the system is larger the higher the fractal dimension the true fractal dimension is related to the hurst exponent h via d f 2 h that manifests the persistency of stochastic processes convertino et al 2013c the more fractal a process is the higher the fluctuations that occur and the lower h the probability of exceedance exhibits a power law behavior although exponential behavior is observed for small flooded areas a transition in pdf epdf is observed for different fcs plans either different heterogeneous fcs plans and homogeneous fcs plans for one fcs typology at a time the transition is related to both the spatial distribution of fcs which determines the plan effectiveness and the fcs local efficiency see section 2 6 the probability distribution of the flood size is used to validate the maxent model and the box counting estimates on the real occurrences the fit of the predicted distribution of patches is performed using a maximum likelihood estimation technique mle convertino et al 2013c in our case study for the current flood distribution status quo ε d 2 0 84 the absolute value d is 1 83 from the box counting method 2 from the space filling counts and 1 67 from the stability analysis and the korcak s law the last value is considered for calculating ε because it is calculated directly on the mapped floods and it is verified to be the most reliable estimate convertino et al 2013c 2 5 stochastic mcda and pareto optimization model ipdm adopts several foundational concepts drawn from modern portfolio theory mpt including the concept of pareto optimality pareto 1971 whereas mpt focuses primarily on the task of developing computationally tractable means by which to allocate resources among various expenditures and investments over time our primary focus is on developing a holistic integrated bottom up top down model to flood management with the use of machine learning models informed by macrophysical processes one may think that these models such as maxent can be replaced by detailed flood mapping hydraulics models at fine resolution however it is much less obvious about how to use the information coming from these models solely for guiding optimal management plans here the purpose is to expose ipdm for the first time and to demonstrate how this model can be used for regional planning and detailed flood mapping with selected information that is also potentially provided by other models whose predictions e g hydraulic mapping of observed floods with a certain return period can be used to calibrated and validate ipdm and in particular maxent one of the submodels of ipdm is the mcda model that defines the necessary information for characterizing ecosystem values specifically the mcda model used for evaluating each flood control structure fcs plan for all assets is a linear multi attribute value theory mavt model convertino et al 2013a in this paper assets are the areas or communities impacted by floods thus characterized by a flood size a more detailed characterization of assets can be done if additional information is available the size of this information is unlimited potentially but only certain information is relevant for the predicted patterns servadio and convertino 2018 here a fcs plan is constituted by the current or all other possible sets of fcs attributed to each community of the river network considering their efficiency for flood protection as we discuss later in more detail our community scale is the subbasin scale the mcda model ranks fcs by scoring them considering a value determined as a linear combination of criteria values and weights the mcda model calculates the local value of fcs plans v i j f c s with respect to asset j in area community i tables s1 s3 shows all input factors of the mcda model in a broader mcda framework it is better to assess a systemic value rather than a systemic risk to emphasize the resilience approach that should be considered in landscape management this approach identifies the most valuable portfolios of alternatives that optimize the systemic value the systemic value does not focus only on the maximization of the hazard dependent inverse risk but on the maximization of all ecosystem services given all feasible hazard controlled scenarios and their social and economic outcomes we assume implicitly the dependence on time of the predicted values via maxent that characterizes the average pattern of flood areas within a management period defined by the return time this approach is a macroecological approach that is widely used in ecology and population sciences for characterizing average or persistent patterns the mcda value or systemic value generally speaking the inverse of the systemic risk sfr if social and economical values are not considered that is ultimately the risk of assets at the basin scale given a fcs plan is calculated as 3 v i j f c s 1 ν j f c s f m j f c s m j i v i j f c s where v i j f c s k w j k x j k i f c s x j k i are the values of criteria k in the mcda model for asset j and area i in our case study all criteria are minimized so the optimization problem is actually a minimization problem however the mcda model allows for multiple criteria to minimize and maximize yet the value vs risk problem formulation is more general than the one presented the weights w j k are stakeholder preferences for the asset j and criteria k that are typically independent of the area i note that here we use the portfolio considering areas as assets but more explicitly the portfolio problem is a fcs problem because a fcs can be assigned to each area and we are only characterizing the effects of fcs in each area but more precisely multiple assets j can exist in the same or different areas i these assets protected by a set of fcs m can be human populations infrastructure and other human resources or ecosystem services broadly defined such as agricultural fields with a certain crop yield and vulnerable species in ecosystems see for example convertino and james valverde 2013 for an ecological application in this paper we just consider the flood area as a criteria for assessing flood risk however the systemic risk is not perfectly equivalent to the flood area both locally i e at the subbasin scale and at the basin scale because the former considers criticality and efficiency of fcs related to their life cycle status and ability to withstand floods in isolation as a function of their structure respectively see tables s2 and s3 as well as the non linear predicted flood area determined by the combination of fcs here criticality refers to the fcs vulnerability but a more complex multicriteria function can consider a more comprehensive vulnerability for all communities considered the inverse of the systemic risk is used to calculate the systemic value that ideally considers other ecosystem services the inverse of the flood area is the local fcs effectiveness that incorporates local fcs criticality and efficiency in a purely mathematical definition the convolution of systemic risk and value is equal to one if no other ecosystem services are considered in the formulation of value the monetary quantification of the risk or value is certainly interesting and necessary for practical management however here we focus only on proposing the model able to perform such comprehensive assessment of ecosystem services table s4 shows for instance economic and social services as an example also because of the unavailability of other data thus we leave the economic evaluation of floods to subsequent applications of the model the local value v i j f c s of a fcs plan is adjusted by the probability of success of flood protection given by the vulnerability of each asset under a fcs plan ν j f c s as later described this is more precisely the criticality of a fcs in this case study and the effectiveness of the fcs plan f m j f c s m j i considering all fcs arrangements that is dependent on the intrinsic efficiency of all fcs locally f m j potentially asset specific for example a flood prone area protects well urban areas but may not protect agricultural areas and the location with respect to all other fcs on the basin i e f c s m j i at the basin scale the effect of all efficiencies estimated from fcs flood protection potential gathered from data and the spatial location of fcs constitutes the overall effectiveness of the portfolio set at the basin scale here the unknown vulnerability is considered related to the structural criticality of each fcs evaluated by experts tables s1 and s2 the more critical a fcs is the higher ν j f c s and the lower its potential to protect against floods this structural criticality can be related to infrastructure age maintenance and other factors the systemic value at the basin scale of the whole fcs plan v t f c s is calculated as a euclidian distance where the components of the distance are the local value of areas dependent on a fcs plan weighted by stakeholder preferences w i j for areas and assets the systemic value that is ultimately the value of a population given a fcs plan is 4 v t f c s i 1 l j 1 j v i j f c s w i j 2 table s4 shows a snapshot of an ideal mcda model where ecosystem services based on the maxent inferred floods can be assessed for any area and asset in this paper we calculate v as the inverse of the flood size to maximize the size of floods everywhere in the basin is dependent on the fcs plan that determines the pdf of floods the pdf is the pattern on which the model is calibrated and validated the total number of portfolio plans or fcs plans is m l that is the number of possible fcs for each area raised to a power exponent equal to the number of areas each area can be a pixel in the basin dictated by the digital elevation model but any different delineation of communities can be decided the baseline model that reproduces the 200 years return period pdf of floods the 200 years floods are 10 larger than the 100 years ones on average is considered as the status quo for the current flood management approach after exploring all fcs plans the pareto optimal set is identified as the one that maximizes v t f c s constrained or unconstrained to the available resources convertino and james valverde 2013 here we explore all potential resources that are identified by the number of potential fcs one for each community of the drainage network the optimization model of ipdm is a linear mixed integer optimization algorithm that explores all possible combinations of fcs alternatives for each asset community one alternative at a time with their expected value and cost at the local scale the maximization of the systemic value or alternatively the minimization of sfr if economic and social endpoints are not considered is performed with and without the constraint of the available resources b these resources are the one available for flood management and here they are taken proportional to the number of fcs considering the unavailability of resource information note that in this context optimization coincides with maximization since all criteria are maximized in principle the number of portfolio combinations is dictated by the number of channelized cells at the lowest scale i e 11 321 for the tiber basin and the number of fcs i e 5 including the no action alternative specifically the number of portfolio plans is the product of the number of possible fcs for each community raised to a power exponent equal to the number of communities where each fcs is present due to the large number of streams where a fcs can be ideally placed we decided more reasonably to consider the number of subbasins i e 2308 for the extracted network with a threshold of 5 km2 as communities thus the total number of portfolio plans is 52308 small scales of subbasins allows stakeholders to have a more granular control of floods but large scales may be more effective this raises the question about which is the optimal scale for flood control this question is however postponed to further studies table s1 reports the fcs considered in this case study and table s3 reports other potential fcs such as expansion areas or flood prone areas e g floodplains riparian areas vegetative buffer strips and gauging stations that can be evaluated in basin flood management and flood related services these fcs are green infrastructure that also support other ecosystem services such as promotion of species habitat and dispersal filtering of chemicals and sediments and pathogen spreading abatement in the portfolio constrained case the cost of the fcs plan c f c s m 1 m i 1 l j 1 j c m f c s m j i cannot exceed the resources b in the case of the pareto optimization unconstrained by budget if v t f c s 1 v t f c s 2 and c f c s 1 c f c s 2 then the portfolio solution f c s 1 dominates f c s 2 thus all fcs envisioned by f c s 1 are selected those are the ones that shift the tail of the pdf of the flooded area to the left the most in the budget constrained pareto optimization if v t f c s 1 v t f c s 2 then the portfolio combination f c s 1 dominates f c s 2 it can be easily proven that a payoff function i v t f c s c f c s to maximize can be used to obtain the same pareto frontiers table s6 reports the inputs of the pareto optimization model and a ideal output 2 6 metamodeling for maxent model selection and testing metamodeling is the art to detect the model with the highest information power for a stated objective saltelli et al 2004 gu et al 2012 servadio and convertino 2018 this is in line with the information theoretic principle of optimal model deign servadio and convertino 2018 the optimal set of input factors is defined as the one that simultaneously minimizes the error e ˆ in reproducing the observed floods and maximizes model accuracy that can for instance measured by the area under the curve auc zweig and campbell 1993 the auc is solely dependent on the reported flood occurrences while the error is dependent on the delineation of predicted landslides based on the predicted flood susceptibility therefore both must be considered when assessing the model and this is one of our strength with respect to classical applications of maxent the auc is evaluated considering the receiver operator characteristic curve roc phillips et al 2006 phillips and miroslav 2008 the roc is a graphical plot of the sensitivity or true positives i e the percentage of predicted flood occurrences that match the observed ones versus the complement function that is the specificity or false positives i e the percentage of predicted flood occurrences which do not match any observation note that this comparison is done at the pixel scale before any flood delineation the auc compares the likelihood that a random flood occurrence site has a higher predicted value in the model than a random site where no flood occurs thus the higher the auc the better the prediction in the jackknife test that is used as the metamodeling design test each variable is excluded in turn from the maxent model run on instantiation and a model is created with the remaining variables then a model is created using each variable in isolation in addition a run is created using all available variables when only one variable is used in the prediction the auc measures the absolute importance of the variable in predicting flood patterns yet this is the first order sensitivity index see section 2 7 the difference between the auc with all variables and the auc for the single variable prediction is a proxy of the sum of interactions or interdependencies between the variable considered in isolation and all others yet this is the second order sensitivity index see section 2 7 thus the jackknife can be used also as a global sensitivity method for the model saltelli et al 2004 beyond being a model deign test to define which variable to incorporate after the calibration on the historical flood pattern for which the error e ˆ is minimized it is possible to retain the variables for which the jackknife test shows an auc greater than 0 5 that is a standard threshold in establishing the importance of the environmental variables considered auc greater than 0 5 is meaningful that any prediction is beyond a random pattern prediction where flood are randomly predicted over space the error e ˆ is observed to be proportional to 1 n p n h where n p is the predicted number of flooded pixels over the total number of historical flooded pixels n h the calculation of 1 n p n h is conditional to the flood occurrences thus this calculation is conditional to the delineation of floods fig 2 2 7 global sensitivity and uncertainty analyses in a variance decomposition framework saltelli et al 2004 servadio and convertino 2018 the variance of the systemic value the payoff i e the systemic value minus the fcs plan cost or of any other selected function e g the flood area is attributed to the intrinsic local variability of socio environmental determinants and systemic network determinants this formulation is considering space explicitly but the variance of the systemic value can lump together spatially defined variables and the variability can be attributed to the intrinsic variability of single variables and their interactions servadio and convertino 2018 li and convertino 2019 this variance based approach has been adopted in convertino et al 2013b for the analysis of fundamental determinants of landslide patterns in an information theoretic approach we use a broader application of gsua via the consideration of the entropy of the payoff that is defined as the necessary and sufficient information contained in its probability distribution and derived from underlying variables a fully probabilistic information theoretic gsua that considers all probability distribution functions pdfs of variables and their entropy as in niklas lüdtke et al 2008 and servadio and convertino 2018 li and convertino 2019 more recently is adopted in this context we aim to determine the relative importance of variables used in predicting flood susceptibility considering the payoff its entropy can be seen as the sum of the intrinsic variability conditional to all other areas or variables m i y i y j i e the mutual information and the entropy h y c that is how much the variability of the underlying determinants contribute to the total local variability thus when finding the total entropy of the payoff the information balance equation that defines the total entropy is given by sum of the shannon entropies of all input variables lumped or not over space as considered alone for the variability of the payoff as well as the sum of their transfer entropies that is assessing the directional variable interdependence see li and convertino 2019 for a biological application of igsua the two quantities assess the variability of the payoff considering local and systemic features thus the total entropy of the flood susceptibility y can be written as 5 h y i h x i i j i t e i x i x j σ y where x i denote the i s variables covariate c in eq 1 that contribute to the payoff y in eq 5 i denotes a variable rather than an area as in eq 4 in this equation h denotes shannon entropy and t e denotes transfer entropy from the first variable to the second variable li and convertino 2019 eq 5 represents a fundamental concept of shannon s information theory and forms the general basis of sensitivity analyses servadio and convertino 2018 li and convertino 2019 the sum of the absolute values of transfer entropies is a proxy of the mutual information m i i p x j x i log 2 p x j x i p x j p x i servadio and convertino 2018 thus it considers the whole set of variable interdependencies here we do not consider the sign of variable interaction as in servadio and convertino 2018 because we focus on absolute flood predictability and because time dynamics of floods is not available explicitly σ y is a noise term that captures the unexplained variability of y related to variables not considered and discretization factors of the model e g due to numerical solvers and pdf inference algorithms eq 5 can also be extended in space variable first order importance and interaction for reproducing the pdf of floods are then calculated as mutual information indices mii niklas lüdtke et al 2008 that are defined by the mutual information normalized by the entropy of the output variable considering one variable or pairs of variables servadio and convertino 2018 these indices are respectively s i m i x i y h y and s i j m i x i x j y h y where x i is any covariate and y is the flooded area or payoff in this study we use igsua for the flood susceptibility that is the fundamental predicted pattern influencing the payoff the use of the transfer entropy whether temporal data are available can give further information about the directionality of the causality between variables in a predictive sense of the model in space and time and the time lag of their causality 3 results and discussion the main impact of fcs is assessed considering the flood size tables s2 s5 although the model can handle any information about losses on assets via the mcda component of the model table s4 the exceedance probability distribution function epdf of the predicted flood size is shown in fig 5 the epdf is computed for the predicted flood size after thresholding the maxent susceptibility to delineate floods this epdf is computed considering the size of flooded areas defined as the ensemble of pixels with probability higher than 0 85 this flood susceptibility threshold on the flood susceptibility is suitable to reproduce the 200 years return time floods figs 2 and 3 the calibration of the threshold constitutes the validation of the model on the reported floods fig 3 fig 2 shows the criteria and procedure used for delineating floods in analogy to the landslide delineation method proposed in convertino et al 2013b data about the 200 years return time floods were constructed from scenario based efforts using standard hydrological models and mapping procedures nardi et al 2006 2013 2015 in these hydrologic models major rivers are integrated with hydrogeomorphic floodplain mapping for extending the flood prone area information at the basin scale other methods for flood delineation have been used by the hydrology community and proposed the use of predictors that are also used in this study such as the topographic index salvatore manfreda et al 2014 2014 samela et al 2015 2017 in flood risk assessment calibration is only performed on low frequency hydrologic data since 30 50 years of observations are available for river flows longer data series are available for precipitations only water infrastructure need to be designed for extreme events which may have never occurred thus hydraulics and hydrologic scenario modeling serve the purpose to determine those extreme events which are the inputs of ipdm interestingly we find that the scaling exponent of the flood size distribution ε d 2 0 84 is about double than the exponent of the runoff or drainage area i e 0 43 rodríguez iturbe and rinaldo 2001 this may shed some light on the relationship between these two random variables drainage area and flood size that are physically dependent on the river network structure the latter forces the propagation of runoff and floods along the scale free drainage pattern and cross river elongation features of the two phenomena floods and runoff may just be responsible for the different power law exponents a power law exponent that is more than double than the one for floods has been observed for landslides i e 1 92 see convertino et al 2013c and convertino et al 2013b hydrologically triggered landslides are of course related to the drainage network and dependent on the same hydroclimatological extremes triggering floods however landslides occurs on hillslopes of basins and yet their scaling exponent is higher than floods which dictates their more limited size the perimeter area relationship for floods shows an exponent of about 0 75 that leaving aside numerical corrections related to the calculation methods is in accordance to the estimate of the box counting the slope of the epdf of the flooded area provides an exponent equal to 0 78 these differences in the slope of the epdf of the flood areas are only related to the different methods used to calculate such distribution see methods section however all these methods are linked to each other as shown in convertino et al 2013c the higher scaling exponent of floods shows the lower criticality in a self organized criticality perspective bak et al 1988 and persistency of floods vs the runoff as expected this implies lower probability of floods to occur considering the variability of flood patterns on fcs plans the epdf is proposed as a macroindicator of phase transitions in flood dynamics the no fcs scenario may correspond to the unaltered ecosystem scenario this scenario shows an exponent ε that is lower than the basin regulated scenario which implies a higher probability for the same flood size on average moreover the no fcs flood size distribution has a much more exponential behavior for small floods that represent the random uncorrelated dynamics of frequent inundation events it is interesting to see that the regulated basin e g the random and current fcs plans has a larger maximum flood than the unregulated basin this is actually confirming the empirical evidence of many river basins where too much suboptimal flood control causes bigger floods the randomly flood regulated basin scenario is quite worrisome because of the extent of the power law regime that is much wider and with very large floods in this case the exponential cutoff for finite size effects in the tail of the distribution highlights that the maximum flood is just limited by the size of the basin but larger floods may occur in very extreme hydrological conditions in principle the maximum flood size is weakly constrained by the basin size and more sensitive to extreme hydrometeorological forcings all other epdfs different than the random epdf are truncated pareto distribution without finite size effects therefore the maximum flood size is much smaller than the one dictated by the size of the landscape if we define α 1 ε where ε is the slope of the epdf of the flood size then α 1 83 for the current fcs plan it is known that when 1 α 2 the first moment of the distribution the mean or average is infinite along with all the higher moments when 2 α 3 such as for the pareto optimal fcs plan the first moment is finite yet more controllable but the second the variance and higher moments are infinite thus some care needs to be placed in monitoring the optimal fcs plan despite the maximum predicted flood is much smaller than the current regulated state for the pareto optimal plan α is 2 2 for the drainage area α 1 43 since ε 0 43 thus also the drainage area has infinite mean theoretically which highlight the fact that extremely large runoff phenomena occur but they are not necessarily determining floods in fig 6 we show the pareto optimal plans of fcs along the pareto frontier in green after considering all potential combinations of fcs tested on the river basin all potential fcs are placed at the subbasin scale fig s4 table s7 shows an example of a pareto optimal solution each optimal plan is located along the green pareto optimal frontier generated by considering the number of fcs as the constraint function this maximum number is in principle equal to the number of all subbasins if they are all regulated however this is never the case of course the maximum number of fcs considered in fig 6 is an example based on some more fcs beyond the current number of fcs see fig 3 and inset in fig 6 the current fcs plan 16 fcs is shown against the optimal fcs plan 15 fcs in the pareto chart the inset in fig 6 is showing over space the newly suggested fcs according to the pareto solution the current fcs that are not envisioned in the pareto plan removed and the ones to keep maintain this result shows the importance of ipdm in detecting the best spatial arrangement of fcs also in consideration of current and future needs of flood protection which can be also related to socio ecological services dependent on fcs such as the promotion of species dispersal and the efficient production of hydropower roy et al 2018 kareiva 2012 and ziv et al 2012 are all examples of potential multi ecosystem service problems that ipdm can accommodate all these needs can be included into the mcda model and be traded off with the flood protection needs the flood susceptibility and epdf over space of the pareto optimal solution as well as of all other scenarios considered is shown in fig 7 the total number of possible communities where to place a fcs are 2308 that corresponds to the subbasins for the 5 km2 thresholded stream network fig s4 the corresponding epdf of flooded areas in the pareto scenario is reported in fig 5 note that here the efficiency is identifying the intrinsic structural potential to retain floods of a fcs table s3 and the effectiveness is assessing the true field capacity of a fcs plan to retain extreme events causing floods this capacity is learnt based on data of floods and environmental data note that floods are also the outcomes of the fcs spatial arrangement beyond the socio environmental factors characterizing a basin yet pareto solutions are those that maximize the systemic effectiveness of a portfolio or set of fcs we believe that this concept of interconnected effects is often overlooked in water resource management versus traditional approaches to contain floods in specified locations without thinking in details about the effects in downstream areas the difference in systemic risk between the no fcs and the current scenario first and second plots is actually really large despite the epdf of these two scenarios is somewhat similar fig 5 such difference is due to the mcda model that considers fcs efficiency and criticality tables s2 s5 which have a profound impact in the determination of the systemic risk if social and economical criteria are also included in the calculation of the systemic risk we would have a large difference with respect to the risk determined solely by the flood size distribution the formulation of the systemic value in eq 3 that is the inverse of the systemic risk if only floods are considered in the mcda model somehow reflects the traditional framework of risk assessment where hazard vulnerability and exposures are calculated and combined together in our case the hazard is constituted by the occurrence of a flood or not exposure is determined by the predicted flood size dependent on all fcs effectiveness i e the systemic driver and vulnerability is related to both the specific local criticality and the efficiency of fcs to protect against floods the it should be noted that the systemicity of the risk is highly dependent on the joint effect of all fcs arrangements in the basin which alters the susceptibility calculated by eq 1 this in turns determines the extent and connectivity of floods fig 7 top plots the predicted floods extracted from the flood probability higher than 0 85 the flood susceptibility and the cumulative output of maxent are shown in fig 7 in maxent the log output is simply a logistic transformation of the raw output which indicates a relative probability of occurrence of a flood this is shown in the bottom plots of fig 7 in the cumulative output middle plots the value of a site is the sum of all the raw outputs from the sites with equal or lower values times 100 this represents the percentage of the potential distribution that is contained within all the sites that are not more suitable than the one considered in other words the logistic output or flood susceptibility output in the bottom plots provides the local probability value of observing a flood in a site or better a potential part of a flood at the pixel scale while the cumulative output of the model is equivalent to the exceedance probability distribution that considers the ability to get that value of flood probability or higher with respect to all other pixels in an information theoretic framework the local probability can be thought as the local response or action local generation of each site to local environmental features in generating floods while the epdf can be thought as the systemic response or reaction systemic response of each site considering surrounding sites and their propensity in generating a flood see gutiérrez roig et al 2016 for a broader view of such information theoretical purview recently gao et al 2016 formulated a general model for resilience where local and systemic network variability are decoupled in assessing the systemic response here we do somewhat the same by decoupling those local flood propensity features dictated by heterogeneous environmental features and network features dictated by the river network and fcs arrangement in the middle plots of fig 7 the more red a site is the higher the contribution of many other sites in determining the flood susceptibility in that point yet the more that considered point is connected with all others thus from the epdf in space it is possible to determine clusters of interdependent points that are likely forming floods the random fcs flood scenario is the one with most of the areas in red which are largely interdependent and giving rise to the power law distribution of flood size it is interesting to note that the pareto optimal management solution decrease the pareto criticality of floods the far right distribution of floods in fig 7 shows that floods are limited in size and disconnected which avoids cascading effects down the drainage network in the pareto optimal solution both flood susceptibility and epdf are much smaller than in any other scenario middle and bottom plots of fig 7 this gives much higher flood protection to the ecosystem as a whole and to critical cities such as rome and perugia located in the south west and north regions of the basin respectively see fig 3 the distribution of fcs is shown by the red dots in the top plots of fig 7 the pareto fcs are distributed much more homogeneously than in other scenarios yet this underlines the fact that structural distribution of assets is very different than their functional distribution manifested by floods in terms of flood size and susceptibility the worst scenario is the random fcs plan the random placement of fsc causes more harm than benefits by likely destroying the natural formation of floods along the drainage network however the suboptimal no fcs scenario is also relatively worrisome considering the more widespread distribution of medium value flood susceptibility than the random scenario the random scenario is more extreme in terms of clustered areas at high flood susceptibility including around the city of rome while the no fcs scenario is worse for distributed medium flood susceptibility this is emphasized by the epdf at the basin scale in fig 5 the way in which the normalized systemic value of fcs for flood protection increases with fcs typological diversity is shown in fig 8 in our case the diversity of fcs protecting assets is not very high since the maximum number of diverse assets is five table s1 lists all fcs plus the no action alternative here we show four groups of assets since two of them i e earth dam with mantle and nucleus and gravity brick dam and filled with soil spurs see table s1 can be grouped in the same structural category earth dam the cost is here taken as a percentage of each infrastructure type selected by ipdm at the basin scale with respect to the feasible total number of fcs in the basin in real settings the cost function of fcs is determined considering construction and maintenance costs but it can also integrate other monetary functions that are forming the basin payoff function together with the reduced flood risk they contributed to generate see fig 8 the reader should note how for high fcs typological diversity the cost of the most expensive assets concrete dams is decreasing with respect to low diversity scenarios in favor of other assets this is likely because this massive high cost fcs are disregarded in favor of other soft fcs optimally organized throughout the basin this high resource optimal diversification pattern is typically seen in other human and natural system portfolio contexts such as in stock markets and ecosystems haldane and may 2011a where optimality is reached via collective organization scale free and diversification for instance zhou et al 2012 has evidenced this in the context of flood protection it is known that optimal solutions are more stable and more diverse as well as less complex considering the balanced mutual interdependencies of assets may 2013 and their scale free distribution of values that make systems more predictable li and convertino 2019 li and convertino 2019 analyzed the diversity complexity stability landscape of microbial ecosystems and derived universal organization rules for complex systems anchored to the pareto optimality principle stability is here considered as ecosystem occurrence in an optimal state as well as the ability to move to better states or to bounce back if perturbed yet it is a broader view of resilience convertino and valverde 2019 li and convertino 2019 in ecological literature many papers have speculated the positive diversity health relationship that here we see in the context of flood protection health is here an ecosystem indicator measured as a continuous function by the systemic value that is the inverse of the number of flooded areas if other socio economical outcomes are not considered the higher the likelihood of floods the higher the health risk considering both morbidity and mortality outcomes of populations this is particularly true in developing countries where floods spread infectious and toxic agents but also in extreme settings such as in europe and usa where anthropogenic pressure and climate change are exacerbating flood frequency and their effects which can lead to infrastructure and human losses in a broader sense the diversity resilience or diversity health hypothesis formulated for socio ecological systems is reflected by the findings of fig 8 in a more engineering sense with respect to our aforementioned perspective resilience is a broad concept underlying positive feedback mechanisms leading to optimal and stable or increasing system s performance after a sequence of perturbations valverde and convertino 2019 that can be achieved via activated or automated heterogenous management such as a re arrangement of system s controls e g fcs in this case the difference in systemic value is defined as the value of information voi convertino et al 2014b that is the amount of information a decision maker can be willing to pay before making a decision in our case study this information can be economically quantified and measures in more depth as the δ between the optimal solution and the status quo or any other state being evaluated for the basin in terms of flood protection it is interesting to note that for a very diversified portfolio the normalized cost of optimal fcs plans is lower this is likely related to the better distribution of resources allowed by diversely priced fcs to underline the importance of diverse portfolio approaches we re emphasize how the susceptibility of maxent fig 7 is different than the systemic risk fig 6 even when social and economical outcomes are not considered in fact the risk is based on the flood size that is calculated a posteriori the definition of a threshold on the flood susceptibility to represent the 200 year return time floods additionally the systemic risk is not the inverse function of the systemic value because the latter is based on a more complex multicriteria function containing social and economical variables if included eq 4 this underlines the non linear relationships between susceptibility risk and value and emphasizes the need of resilience focused approaches based on long term trajectories of ecosystem values valverde and convertino 2019 convertino and james valverde 2019 the systemic uncertainty evaluation of the model is shown in fig 9 in particular the roc to measure prediction accuracy is show as well as the igsua of all variables introduced in the prediction in the form of jackknife test phillips et al 2006 phillips and miroslav 2008 convertino et al 2013b and functional network of interdependency among predictors the igsua results are shown for the optimal version of the model with the highest information theoretic power i e the highest entropy in eq 5 this high entropy state corresponds to the lowest complexity and highest predictive accuracy see also li and convertino 2019 for an application to microbial ecosystems 89 of the observed floods are reproduced which translates into a auc 0 89 all variables whose regularized training gain that is the variable specific prediction importance is lower than 0 5 are disregarded from the model the roc curve shows that the smallest floods are not reproduced well considering the small values of the sensitivity for small floods this is a general result of maxent that needs to calibrate on some event sizes that are larger than small event sizes in our case study these are the larger floods that encompass a wide range of environmental variability however it is majorly important to represent extreme events better than small ones because they contribute more to forming the extreme risk the roc curve shows an exponential increase in sensitivity after small size events while it is certainly true that small high frequency events can carry substantial economic losses big events can be catastrophic events that destroy completely the areas being flooded with cascading outcomes such as diseases and generalized transport of pollutants the jackknife test bottom bar plot in fig 9 provides an estimate of the first and second order sensitivity indices s i and s i j respectively for predicting the observed landslides see section 2 7 the functional interaction network shows s i and s i j proportional to the diameter of nodes and the width of links respectively elevation is the most important predictor of flood susceptibility and it majorly affects the topographic index as expected since ti is derived from the elevation information ti also known as the wetness index wi is a steady state wetness index that is commonly used to quantify topographic or geomorphological control on hydrological processes it is calculated based on the upslope area and the local slope convertino et al 2013b ideally in presence of dynamical information this index can also be dependent on time and contribute to almost real time forecast of floods the elevation is also affecting in a minor way all other predictors and the directionality of the functional dependence is expected to be from the elevation variable to all others whether transfer entropy is used to assess that functional variable dependence note that in this case study all these variables are static variables thus it is not possible to assess the direction of their functional dependency see section 2 7 the low importance of drainage directions was expected because floods can occur for any value of drainage directions floodplains are low laying flat areas with undefined local flow directions in fluvial corridors flood waves move by cutting meanders and local flow patterns are progressing downstream along a main direction as a result flood prone areas are characterized by almost all values of drainage directions independently of the methods used to calculate them rodríguez iturbe and rinaldo 2001 nardi et al 2008 2019 the proposed ipdm is also suited to evaluate the impact of green infrastructure solutions for flood risk management vs standard grey infrastructure temmerman et al 2013 keesstra et al 2018 standard fcs are usually adopted for the lack of a quantitative measure of the multi sectorial social economical and environmental beneficial effect of non structural and green infrastructure solutions additionally there is currently no model that tells where to locate these green infrastructure in river basins optimally nevertheless the socio economic benefit of nature based solutions can be quantitatively evaluated by means of the proposed ipdm which supports innovative decision making towards large scale and long term green infrastructure strategies a quantitative data driven decision model could support the much needed cultural change for moving decision making towards river basin hydrology driven risk management based on diverse fcs and systemic basin scale actions this would pave the way to a novel flood management paradigm based on the combination of green and grey solutions or any other diverse solution recognizing the complexity of intertwined natural and human systems that optimize human and natural resources including life safety and ecosystem productivity lastly it is crucial to mention that ipdm is open to mental modeling that is the mapping and quantitative incorporation of the whole decision space of stakeholders involved in the problem kolkman et al 2005 wood et al 2012a b convertino et al 2016 in this case mental modeling would be focused on flood protection and all its dependent ecosystem services of biological social and economic nature rinaldo et al 2018 this kind of modeling grimm et al 2005 embraces the socio hydrology paradigm embracing sustainability that is currently relatively widespread in the hydrological community blöschl and montanari 2010 viglione et al 2014 blöschl et al 2017 risk management decisions that are informed by and address decision makers and stakeholder risk perceptions and behavior are in fact essential for effective risk management policy and ultimately environmental planning therefore mental modeling is completely fitting in the systemic risk purview of ipdm via the inclusion of the socio ecological aspects of the problem e g weights in the value of eqs 3 and 4 handled beyond the traditional hydraulics and hydrologic models 4 conclusions an information theoretic portfolio decision model is proposed as an optimal model and operational process to design optimal fcs plans at the basin scale this paper proposes a case study for the tiber basin and traditional fcs types but any basin and fcs can be considered by the portfolio model as for fcs types for instance flood prone riparian areas that embrace a more engineering with nature management approach can be considered overall the portfolio approach supports a participatory transparent and rigorous decision making proposition about environmental planning that focuses on the optimization of the systemic value of fcs services for the whole basin ecosystem the model can operationalize detailed flood mapping models based on hydraulic processes and can consider any scenario such as related to climate and land use land cover change coupled to all possible management plans a flood control strategy should consider different fcs plans over time considering the changing basin conditions for simplicity in this paper we show the average static application of ipdm anchored to floods with an extreme return period the flood susceptibility is proportional to the risk although in a non linear way considering the complexity of river basins dictated by land features geomorphological features infrastructure heterogeneities and diverse stakeholder preferences such as for the tiber basin considered the truly innovative part of the study is the portfolio framework and model that brings together several submodels aiming to predict characterize predict and control flood patterns beyond the novel introduction of ipdm the following results are worth mentioning the pareto management optimal solution is shown to remove critical cascading effects related to propagation of big floods downstream this can be seen by the broken spatial distribution of floods along the network that appeared more isolated one from another additionally the pareto optimal solution decreases consistently the maximum size of the largest flood yet it reduces the pareto power law regime or critical regime of the flood size distribution in other words the optimal paretian management plan reduces the risk related to the naturally self organized floods this pareto management corresponds to a neutral state where local conflicting small scale heterogeneities are well balanced into a maximim systemic value see li and convertino 2019 for a biological application where the neutral state corresponds to the critical state we propose the shape and slope in case of a power law distribution of the exceedance distribution of flooded areas that is related to the fractal dimension of floods in 2d as a macrohydrological indicator of phase transitions in flood dynamics the random fcs plan is worse than no fcs in terms of flood size distribution because fcs produce cascading effects in terms of connected floods that can be highly dangerous if the spatial arrangement maximizes flood propagation the no fcs solution determines the natural scale free distribution of floods if no action is taken this is the critical state that is undesired because persistent and with maximum flood size when considering criticality and effectiveness of fcs via the mcda model the systemic risk of the random fcs plan is lower than the no fcs plan this risk is different because it considers socio ecological endpoints beyond the flood size after the calculation of flood susceptibility the flood delineation method based on the predicted maxent flood susceptibility and the von neumann criteria on the pixel flood susceptibility is a novel information theoretic model that can be used more easily than more complex hydraulic process oriented models such as hydrogeomorphic floodplain delineation models or hydrodynamics models in calculating the flood area and yet the flood risk based on the flood size epdf the flood size epdf is a geomorphic function indicating phase transitions in flood patterns maxent is introduced from ecological sciences as a suitable model to predict flood susceptibility in this context it is found that elevation is the most important predictor of susceptibility the second most important predictor is the topographic index that is also known as the wetness index and this can be used as a dynamical factor when predicting flood susceptibility over time second order predictors are land cover use drainage area and drainage directions ipdm allows decision makers to create a baseline value of systemic risk or vice versa of systemic ecosystem value of the basin in terms of flood safety and socio economical outcomes yet any future investment can be compared to that baseline value via the use of the pareto frontier that shows the departure of any basin state from the optimal solution the pareto frontier also shows the necessary and sufficient investment for flood protection before reaching the plateau where additional investments in resources do not increase the systemic value significantly or does not decrease the systemic flood risk equivalently if only losses in terms of flood area are considered we show that the normalized systemic value of assets increases when the diversity of fcs increases but the spatial arrangement of fcs must be pareto optimal distribution this results shows the non linearity between structural features of the basin such as river network structure and fcs arrangement and functional features such as flood size and systemic value distributions the difference in systemic value can be considered as the value of information before reaching a decision about a fcs plan information that has high socio economical and environmental value that decision makers should be willing to pay for maintaining or increasing ecosystem value authors contribution m c formulated the original idea of ipdm for flood management designed the model performed the calculations and wrote the paper a a helped in structuring the data for the model and provided all the gis support f n provided the hydrological data and expertise and contributed to the writing of the paper acknowledgments m c acknowledges the funding from the gi core station for big data cybersecurity at hokkaido university sapporo jp the microsoft ai for earth program grant bio hydro geo dynamics mapping systemic earth risk https www microsoft com en us ai ai for earth grantsg and the nsf srn project srn integrated urban infrastructure solutions for environmentally sustainable healthy and livable cities http www sustainablehealthycities org f n and a a acknowledge the support of the water resources research and documentation center warredoc of università per stranieri di perugia and the financial support of regione lazio grant number a11598 research grant media valle del fiume tevere and the autorità di bacino del fiume tevere working group flood map ps5 updating for the city of rome by means of gis and 2d hydraulic modeling dr james valverde jr department of energy is highly acknowledged for comments to the paper and the original formulation of the portfolio model with m c all data have been made available https github com matteoconvertino floodteveremaxent glossary sfr systemic flood risk ipdm information theoretic portfolio decision model fcs flood control structure igsua information theoretic global sensitivity and uncertainty analyses maxent maximum entropy mcda multi criteria decision analysis ti topographic index dem digital elevation model lc lu land cover land use auc area under the curve mi mutual information mii mutual information indices appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 06 013 
26181,the most common approach to mitigation of lake eutrophication is reduction of phosphorus emissions in particular by changing farm management this reduction can be combined with landscaping retention structures upstream of the lake the analyses of which the paper is based on the management of these structures currently focuses on maximising the quantity of phosphorus trapped regardless of lake dynamics this paper adapts a dynamical model of lake phosphorus to examine the effects of these phosphorus retention structures we highlight two effects first a structure that traps some of the phosphorus load before it reaches the lake reduces the amount of phosphorus in lake water second some retention structures slow down lake phosphorus dynamics in a way that may perversely slow lake restoration we propose a cleaning strategy that maximises the chances of restoring a lake to an oligotrophic condition we demonstrate our model with a real world case study keywords lake eutrophication dynamical model ecosystem resilience ecosystem management 1 introduction since industrialisation in the early 20th century a deterioration of the water quality in lakes has been observed mainly caused by human activities such as agriculture urbanisation and tourism ec 2006 bouwman et al 2009 bennett et al 1999 dokulil 2014 these activities are associated with chemical compounds such as phosphorus smith et al 1999 being released into the natural environment which disturbs the balance in lake ecosystems and increases phytoplankton biomass this process is called eutrophication and it is now a global issue for example some 40 of european lakes were eutrophic in 2013 european commission ip 13 947 cyanobacteria and algal proliferation is a common consequence of eutrophication and this has negative effects on the aquatic ecosystem the three main issues arising from eutrophication involve ecology health and the economy firstly cyanobacteria proliferation affects biodiversity because this proliferation occurs at the cost of other summer algae species secondly the decomposition of algae and cyanobacteria through the water column leads to anoxia or hypoxia in eutrophic lakes chislock et al 2013 both of which can have major adverse effects on lake biodiversity weinke et al 2018 and thirdly some cyanobacterial species also produce toxins which threaten human and animal health vasconcelos 2001 svirev et al 2014 therefore their proliferation presents a risk both for ecosystem and for the local population deterioration of a lake ecosystem can also have major economic impacts because it can reduce activities around the lake such as fisheries lee and jones 1991 and water sports for instance the estimated economic damage caused by the eutrophication in the usa is around 2 2 billion dollars annually dodds et al 2009 here we focus exclusively on lake eutrophication and in particular on the concentration of phosphorus in the lake water body since this which is the main limiting nutrient for lakes correll 1999 carpenter 2008 schindler et al 2008 there are two main approaches to mitigation of lake eutrophication the first one consists of limiting the phosphorus sources in the watershed and the second one of directly removing phosphorus from the lake or its closed environment the greater the input phosphorus rate the greater the phosphorus quantity in the lake water in addition the variation of the input phosphorus rate corresponds to the watershed phosphorus mass balance thus control of the watershed phosphorus mass balance in other words of the phosphorus sources is the main way to mitigate downstream lake eutrophication conley et al 2009 martin 2004 schindler et al 2008 however the economic interests linked to the phosphorus emissions such as agriculture and urbanisation may limit the reduction in phosphorus input eutrophication has also been related to meteorological events which are not controllable and to climate brias et al 2018 trolle et al 2011 in this context the other means to mitigate lake eutrophication phosphorus removal in the lake or its immediate neighbourhood may be used as a complement to the reduction in the phosphorus sources however the effectiveness of phosphorus removal is conditioned by the input phosphorus rate coming from the watershed removal consists of a one off removals and limited reduction to the amount of phosphorus in the lake water therefore if the input phosphorus rate is not reduced the lake may well return to its initial eutrophic state after each reduction thus phosphorus removal alone is not sufficient to accomplish long term goals of water quality improvement to remove phosphorus in the lake it is possible for instance to neutralise the available phosphorus in eutrophic lakes by adding chemicals smolders et al 2001 or to reduce the amount of phosphorus in the lake water body by dredging some of the phosphorus contained in sediments can be released from them into the lake water hoyer et al 2008 however we have chosen not to analyse these approaches here because they raise certain problems it is difficult sometimes impossible to implement them and they are not permanent solutions because they need to be regularly repeated here we focus on natural or artificial retention structures located between the watershed and lake such that the input flow into the lake passes through these structures some of them are designed to retain specific nutrients like phosphorus or nitrogen wetlands and semi natural areas use plants to trap these nutrients the nutrients trapped therefore depend on the plant species present kao et al 2003 other structures trap a certain quantity of nutrients such as sedimentation ponds where the water flow is low in this way some of the input nutrients are trapped in the pond sediments bjorneberg and lentz 2005 up to a maximum saturation these sedimentation ponds need to be cleaned regularly otherwise the pond s capacity for trapped input nutrients declines and ultimately vanishes retention structures are permanent but need appropriate management to maintain the desired trapping effect on phosphorus most of these retention structures are currently designed and managed in order to trap a maximum quantity of input phosphorus design and management are based on a static approach the smaller the incoming phosphorus quantity into the lake the smaller the final phosphorus quantity in the lake water but this static approach is not sufficient and the aim of this paper is to study the dynamical effects of retention structures on a downstream lake for example the static approach does not take two common dynamical effects into account the hysteretic behaviour of quantity of phosphorus and the time that this quantity of phosphorus converges to its equilibrium state the hysteretic behaviour of phosphorus means that two equilibrium lake states can correspond to the same input phosphorus rate one may be eutrophic and the other not the equilibrium lake state depends on the input phosphorus trajectory into the lake in other words the output phosphorus trajectory from the retention structure therefore appropriate retention structure management must focus on this trajectory which is the dynamical approach instead of the amount of trapped phosphorus which is the static approach in addition the convergence time for the amount of phosphorus in the water of a retention structure and lake adjacent can be length several decades for some lakes and structures the dynamical approach allows to model these convergence times and to study their impact on the lake s evolution in section 2 we present the dynamical carpenter model carpenter et al 1999 for amount of phosphorus in a lake water body which is applied in section 3 to the three kinds of retention effects in section 4 we study the dynamical effects of each type of retention on a downstream lake in section 5 we apply our model on a real life case study finally we discuss our results and look at potential directions for research 2 dynamical model of the quantity of phosphorus in a lake to study the dynamical effects of a retention structure on the quantity of phosphorus in the water of a downstream lake it is necessary to model dynamically the link between phosphorus input rate into a lake and the phosphorus quantity in this lake water body many models of phosphorus in a lake water body are based on vollenweider 1975 at the steady state ostrofsky 1978 larsen and mercier 1976 nürnberg 1984 they are not useable here because of their static aspect another approach seeks to model the lake as a whole these models often have many strongly inter connected state variables mooij et al 2010 makler pick et al 2011 robson 2014 of which the quantity of phosphorus is one however calibrating them with many variables and parameters needs large amounts of data which are not always readily available carpenter et al 1999 provides a dynamical model of the quantity of phosphorus in a lake water body this model implicitly incorporates all the phenomena involved in the phosphorus cycle including seasonal variations it is used in several cases ludwig et al 2003 cottingham et al 2015 carpenter 2005 wang et al 2012 according to carpenter et al 1999 for a phosphorus input rate l t the quantity of phosphorus in a lake water body p t is described by the following differential equation 1 d p t d t l t s h p t r p t q m q p t q the phosphorus input is composed of the input flow from upstream l t and the proportion of phosphorus that comes from the lake sediments r p t q m q p t q called the recycling rate the phosphorus output is composed of the outflow h p t and the proportion of phosphorus that enters the lake sediments s p t called the sedimentation rate this model is summarised in fig 1 in this study we use the parameters in based on a hysteretic lake for our numerical simulations in sections 3 and 4 these parameters do not correspond to a specific existing lake whose dimensions and hydraulic characteristics are known instead they are values classically used in the numerical study of hysteretic lakes wang et al 2012 janssen and carpenter 1999 martin 2004 the parameters are given table 1 there are three zones of equilibrium for a fixed phosphorus input rate the upper part of the curve corresponds to stable equilibrium points the middle part of the curve corresponds to unstable equilibrium points often called tipping points and the lower part of the curve corresponds to stable equilibrium points fig 2 a shows the evolution of the quantity of phosphorus according to some initial conditions an oligotrophic state corresponds to a low level of phosphorus in a lake water body limiting the primary production a eutrophic state to an excess of phosphorus quantity inducing massive phytoplankton biomass and a mesotrophic state at an intermediate state between the two these states are marked on the equilibrium curve of the model the lower part of the equilibrium curve corresponds to an oligotrophic state the upper part to a eutrophic state and the middle part to a mesotrophic state this distribution is a valid working hypothesis firstly because it is compatible with the qualitative definitions of the state of lake and secondly because some lakes correspond to this working hypothesis e g the bourget lake brias 2016 the points corresponding to eutrophic mesotrophic and oligotrophic states are shown on fig 2 b fig 2 a shows the starting points whose final corresponding lake states will be eutrophic or oligotrophic in this study we focus on hysteretic lakes carpenter et al 1999 janse et al 2008 rather than reversible for example lake washington edmondson 1991 or irreversible lakes for example the shagawa lake larsen et al 1979 1981 this is because the dynamical effects of retention structures on other types of lakes are easily describable whereas an irreversible lake cannot be restored by reducing incoming phosphorus carpenter et al 1999 thus no structure can restore an irreversible eutrophic lake in the case of a reversible lake one input phosphorus quantity corresponds to only one equilibrium phosphorus quantity in the lake water body therefore a static approach is sufficient to study a reversible lake the final lake state depends only on the input phosphorus quantity value 3 retention structures 3 1 introduction a number of models and studies of wetlands kadlec 2016 rahman et al 2016 lee et al 2017 giraldi et al 2010 and sedimentation ponds waajen et al 2016 palmer felgate et al 2011 pulatsü et al 2003 exist they study the correlations between internal processes based on the physical variables of these structures such as depth turbidity etc in addition some of them offer a static link between these physical variables and the rate at which phosphorus is trapped in these studies the efficiency of these structures is estimated according to the quantity of trapped phosphorus which corresponds to a static approach to study the dynamical effects of a retention structure on a downstream lake we need to dynamically model the phosphorus rate coming out of this structure here we study the effects of a generic structure on a downstream lake the device is shown in fig 3 this structure is only characterized by its effect on the phosphorus passing through it we focus on three types of effects which are summarised table 2 the delayed effect this is a delayed effect between the changes in the flow of phosphorus entering the structure and its flow at the outlet this effect corresponds to that observed by kadlec and wallace 2008 walker 1998 the fixed trapping effect in the short to medium term some structures have a trapping effect that can be considered as being fixed over time for example the houghton lake usa wetland trapping effect was stable over a 30 year measurement period kadlec 2009 the fixed trapping effect thus corresponds to a sufficiently short term for saturation effects not to be taken into account furthermore an adapted management of the retention structure may maintain its trapping capacity over a long term period for example sediment curing of a sedimentation pond or the mowing of a wetland if this maintenance if frequent enough the trapping capacity can be modelled as fixed for example yousef et al 1994 recommends a sediment cleaning frequency of 25 years to guarantee a variation in the trapping capacity of around 10 15 for detention ponds in central to southern florida otherwise it has to be modelled as a varying trapping effect the varying trapping effect the third kind of effect corresponds to sediment filling such as sedimentation ponds and some wetlands with a gradually declining trapping effect these structures require maintenance to remove phosphorus rich sediments brown et al 1981 in addition to the retention effect a structure may also have a recycling rate that corresponds to a phosphorus rate from the structure sediment the recycling event is of two types a punctual recycling and a structural recycling the punctual recycling depends on the properties of the soil gu et al 2017 kadlec 2016 and on specific events such as periods of hydration or drought dierberg et al 2012 the structural recycling is linked to the structure properties and only depends on its current state like in the case of lakes in appendix appendix c we show how the recycling may have a negative impact of the downstream lake but we also show how a regular maintenance of the structure can prevent these negative effects the trapping effect of each structure is modelled by part of the carpenter model the quantity of phosphorus in the water of a retention structure p r therefore follows this model in the general case 2 d p r t d t l t s r h r p r t r r t the main terms of this general model are given in table 3 because of the characteristics of a structure its model terms must be adapted the s r and h r parameters may or may not be dependent on the time according to the type of retention effect in addition the recycling term r r t has been separately addressed in appendix appendix c therefore in the next sections we consider a retention structure without recycling effect id est r r t 0 in order to study only the retention effect on a downstream lake and because the recycling effect may be negligible with an appropriate structure management in order to study each retention effect we first consider a simple generic structure using only one of the retention effects with a delayed effect in section 3 2 with a fixed trapping effect in section 3 3 and with a variable trapping effect in section 3 4 this allows us to distinguish the main effects of a structure each successive model introduces a new effect real structures may combine several retention effects in particular delayed and trapping fixed or not effects with recycling effect 3 2 the delayed effect here we assume that the trapping effect is nil s r p r t 0 which allows us to focus solely on the delayed effect this hypothesis corresponds to certain real structures some wetlands are specifically designed to retain nutrients other than phosphorus and thus have a negligible trapping effect on this nutrient for example the columbia system kadlec et al 2010 and the apopka c2 system dunne et al 2015 thus the dynamical model of the quantity of phosphorus in the water of the retention structure becomes d p r t d t l t h r p r t the corresponding diagram is shown in fig 4 at steady state and with a constant phosphorus input rate l the phosphorus quantity in the structure is p r e q l h r thus the phosphorus output rate from the structure with only a delayed effect is h r p r e q l with a constant phosphorus input rate l the dynamical solution of the model is p r t l h r 1 e h r t e h r t p r 0 thus the phosphorus quantity in the structure s water body converges to the value p r e q at the speed e h r t the greater the value of h r the faster the convergence figure a 17 shows the time evolution of the phosphorus output rate for certain structures based on their h r values 3 3 the fixed trapping effect with only a fixed trapping effect on the phosphorus the dynamical model of the phosphorus quantity in a retention structure is the same as equation 2 d p r t d t l t s r h r p r t where s r and h r are constant the corresponding diagram is shown in fig 5 this model is very close to the preceding one if we associate s r h r to the h r parameter above these models are mathematically the same the difference between the two models is the phosphorus output rate at steady state and with a fixed phosphorus input rate l t l the quantity of phosphorus in the water body of the retention structure is p r e q l s r h r the phosphorus output rate from this structure is therefore h r p r e q h r l s r h r l which is lower than the phosphorus input rate this is consistent with the trapping effect of such a structure on the phosphorus rate the trapping capacity does not depend on the value of s r but on the ratio h r s r h r if h r s r h r 1 which means s r 0 then the trapping effect is nil and if h r s r h r 0 which means s r h r then the trapping effect is maximal once again the speed of convergence towards steady state depends on the value of s r h r the greater this value the faster the convergence also the smaller its delayed effect 3 4 the varying trapping effect the dynamical model of the quantity of phosphorus in the water of a retention structure with a varying trapping effect is d p r t d t l t s r t h r t p r t the corresponding diagram is shown in fig 6 after a long enough time the trapping capacity becomes nil the h r and s r values therefore converge to h m a x and 0 to make the dynamical evolution of h r and s r more explicit the evolution of the trapping effect is considered as a sedimentary filling in the case of a sedimentation pond the trapped nutrients are stored in its sediments thus after a long enough time these sediments fill the structure and the structure cannot trap any more nutrients lake managers therefore have to clean sedimentation ponds regularly to reset their trapping power this filling means a decreasing filter power and so the convergence of the h r and the s r values towards h m a x and 0 the higher the sedimentation rate the faster the structure fill and the fuller the structure the lower the sedimentation rate mathematically it corresponds to the following differential system d s r t d t α r s r t the filling of the structure depends on the input nutrient rate thus in a more realistic model α r depends on l t but l t is considered as being constant here so α r too in addition when the sedimentation rate is nil the structure has the same behaviour as a structure with not trapping effect therefore when the sedimentation rate is nil h r t is constant after each reset operation corresponding to a cleaning operation for the sedimentation pond s r and h r are reset to their initial values s i and h i table 4 gives the parameters of this model finally the dynamical models of s r t and h r t are d s r t d t α r s r t d h r t d t h m a x α r α r h r t therefore s r t and h r t are such that s r t e α r t s i h r t h m a x 1 e α r t e α r t h i in fact the exact evolution of s r t and h r t does not matter here the simulations and the studied qualitative behaviours are based on the monotone convergence of s r t and h r t the speed of convergence of p r depends on the value of s r t h r t to isolate solely the varying trapping effect the s r t h r t values are assumed to be high this means that the delayed effect is negligible the convergence of the quantity of phosphorus in the water of a structure with a varying trapping effect is much faster than the decline in the trapping effect therefore the convergence of s r and h r takes longer than the convergence of the quantity of phosphorus in the water body of the structure mathematically that means s r h r α r the dynamics of the quantity of phosphorus in the water body of the structure is fast and the dynamics of its trapping effect is slow according to the timescale considered the dynamical model can thus be approximated as follows for a short timescale the phosphorus quantity in the structure is such that d p r t d t l t s r h r p r t where s r and h r are considered to be constant thus for a short timescale the varying trapping effect can be assumed to be the same as the fixed trapping effect for a long timescale the quantity of phosphorus in the structure is such that p r t l t s r t h r t where s r and h r respectively converge to 0 and h m a x the trapping effect of the structure depends on the s r and h r values if s r 0 the phosphorus output rate from the structure is lower than the phosphorus input rate 3 5 conclusion the qualitative behaviour of the three types of retention effect can be translated into three dynamical models a retention structure has a delayed effect between its phosphorus input and output rates a structure with a fixed trapping effect has a phosphorus output rate lower than its phosphorus input rate and the difference is proportional to this input rate a structure with a varying filter effect has the same properties as the second scenario initially and its trapping capacity becomes nil after a long enough time has elapsec the effects of a structure depend on the value of its parameters s r and h r fig 7 shows a qualitative distribution of these effects according to the values of s r and h r it shows that there is no effect failure depending on the values of the structure parameters thus a small variation in the parameters leads to only a small variation in the effects based on this we can consider these effects to be robust based on these parameter values the coordinates of point a correspond to parameter values where s r 0 and 0 h r 1 representing a retention structure with a delayed effect and no trapping effect the coordinates of the point b correspond to parameter values where s r 0 and h r 0 representing a retention structure with a trapping effect and no delayed effect the coordinates of the points c and d correspond to the initial and final parameter values of a structure with a varying trapping effect and without delayed effect at t 0 the parameters of this structure are such that s r t 0 0 and h r t 0 0 point c so the structure has a positive trapping effect after a long enough time the parameter values converge to s r t 0 0 and h r t 0 0 point d so the trapping effect becomes nil sediment filling it is possible to restore this trapping capacity structure which would result in instantly resetting its parameter values reset 4 influence of a retention structure on a downstream lake 4 1 introduction in this section we consider one structure one lake systems the lake state depends on the phosphorus input rate as shown in section 2 a reduction in this input rate should thus have an influence on its state but the final lake state does not depend only on the trapping capacity of the upstream structure but also on the trajectory of the quantity of phosphorus from the water body of the structure quantity of phosphorus in the water body of the lake system the order of the studied systems in this section is the same as that of the effects studied in section 3 that is the delayed effect the fixed trapping effect then the varying trapping effect 4 2 the delayed effect 4 2 1 introduction let us consider the one structure with only a delayed effect one lake system its diagram is shown in fig 8 the corresponding dynamical model is d p r t d t l t h r p r t d p t d t h r p r t s h p t r p t q m q p t q 4 2 2 the delayed effect with a fixed phosphorus input rate we consider a fixed phosphorus input rate l t l to understand the effect of the structure on a downstream lake two cases are considered the convergence time of the quantity of phosphorus in the water body of the structure is very short compared to the convergence time of the quantity of phosphorus in a downstream lake water body therefore h r s h 1 during this short convergence time the phosphorus output rate from the structure becomes equal to h r p r e q l in this case the lake s trajectory is thus unaffected by the upstream structure the convergence time of the phosphorus quantity in the structure water body is very long compared to the convergence time of the phosphorus quantity in a downstream lake water body therefore h r s h 1 for a short time the phosphorus output rate from the structure can be considered as being constant and the phosphorus quantity in the lake water body converges to the corresponding equilibrium value in this case the lake s trajectory is thus strongly influenced by the structure in summary the impact of the delayed effect on a downstream lake depends on the ratio h r s h and on the initial phosphorus quantity in the water body of the structure for different h r s h values figure a 18 shows the initial p d 0 p 0 values from which the final downstream lake state is either eutrophic or oligotrophic with some associated trajectories of h r p r t p t the initial quantity of phosphorus in the water body of the structure is a given of the system thus a delayed effect cannot restore a downstream lake it can only protect an oligotrophic lake from an increase in phosphorus inflow as we see in the following section 4 2 3 the delayed effect with a varying phosphorus input rate without an upstream structure the lake is directly impacted by changes in l t with an upstream structure these changes are delayed therefore a change in l t that modifies the state of a lake without upstream structure does not necessarily modify the state of a lake with an upstream structure the delayed effect thus increases the lake s inertia for a lake with an upstream structure the change needed in the input phosphorus rate to change the final lake state is greater than the change needed for a lake with no upstream structure figure c 21 illustrates this phenomena 4 2 4 conclusion despite the non trapping effect the delayed effect affects the evolution of the quantity of phosphorus in a downstream lake water body and thus the state of this lake the stronger the structure s delayed effect the greater the lake s inertia this delayed effect can be used to protect an oligotrophic lake with an increase of the phosphorus input rate it maintains the lake in this acceptable state however the delayed effect also maintains a eutrophic lake in its initial state see fig 9 4 3 the fixed trapping effect let us consider the one structure with only a fixed trapping effect one lake system shown in fig 10 the corresponding dynamical model is d p r t d t l t s r h r p r t d p t d t h r p r t s h p t r p t q m q p t q the ratio h r s r s h 1 is very high yielding no delayed effect part of the phosphorus input is trapped by the structure therefore if l t is constant the phosphorus input rate into the lake is h r l s r h r instead of l in the absence of a structure the set of equilibrium states of the lake with one structure is merely right shifted relative to the situation of the same lake with no structure this right shift is proportional to l the greater the value of l the greater the shift therefore the input value set for which the corresponding equilibrium state of the lake is oligotrophic is increased hence given a fixed quantity of phosphorus in the lake water body the corresponding equilibrium phosphorus input rate is greater with a trapping effect than without figure b 19 shows this right shift effect the trapping effect contributes to mitigate eutrophication for a lake with an upstream structure the phosphorus input rate needed to reach a eutrophic state is higher than with no structure and even if a lake with a structure is eutrophic the quantity of phosphorus in its water body is less than with no upstream structure for the same phosphorus input rate 4 4 the varying trapping effect 4 4 1 introduction let us consider the one structure with a varying trapping effect one lake system which is shown in fig 11 the corresponding dynamical model is d p r t d t l t s r t h r t p r t d p t d t h r t p r t s h p t r p t q m q p t q with s r 0 s i and s r t 0 when t and h r 0 h i and h r t h m a x when t the s r t h r t values are assumed to be high yielding no delayed effect in addition the phosphorus input rate l is assumed to be constant therefore the quantity of phosphorus in the water body of the structure is immediately equal to the l s r t h r t value an initial situation of the system structure lake with no delayed effect is a pair of values p 0 l which are respectively the initial quantity of phosphorus in the lake water body and the phosphorus input rate in the structure lake system 4 4 2 critical times there are three sets of p 0 l the initial quantity of phosphorus in the lake water body and the input quantity of phosphorus into the system respectively o set the set of initial conditions for which no reset is required to obtain a final oligotrophic state this set includes a set of points from which the final corresponding lake state is oligotrophic with s r h r 0 h m a x e set the set of initial conditions for which the final state is eutrophic independent of the reset date this set includes a set of points from which the final corresponding lake state is eutrophic with s r h r s i h i i set the set of initial conditions for which without reset the final lake state is eutrophic but for which if a reset dates were introduced could result in the final lake state being oligotrophic these sets of initial situations are shown in fig 12 a let us consider an initial situation in the i set without reset fig 13 a the final lake state is eutrophic with an appropriate reset date fig 13 b the final lake state is oligotrophic however not all of the reset dates result in a final oligotrophic state where there is too long delay before reset the final lake state remains eutrophic fig 13 c for each reset date the final quantity of phosphorus in the lake water body is computed and thus the corresponding final lake state is known as shown in fig 12 b there is a time interval between two critical times within which a reset date is efficient and beyond which the reset date is ineffective 4 4 3 the structure management to select the best reset date a criterion is needed we compare here two strategies to select this reset date classic management many studies on structures focus on the parameters or management for which the total trapped phosphorus is maximal the first possible strategy is thus the one such that 0 t r a p p e d p h o s p h o r u s is maximal which means that 0 h r t p r t is minimal optimal management here the goal is to mitigate eutrophication therefore the longer a lake is oligotrophic the longer it is not eutrophic thus another possible strategy is one in which the time during which the lake is oligotrophic is maximised with a finite time the reset date that maximises the quality criterion corresponds to the cleaning date for which the lake is mostly oligotrophic in other words the reset date that maximises the quality criterion is the reset date for which the lake is most quickly returned to an oligotrophic state let us consider the initial situation corresponding to the example in fig 13 fig 14 b shows the total input quantity of phosphorus that enters into the lake depending on the reset date during the simulation time 25 y e a r s the minimal total phosphorus input quantity 4 3 tons corresponds to a reset between t 12 15 y e a r s and t 12 85 y e a r s because of the limiting time of the simulation the total input quantity of phosphorus increases after this date with an infinite run time this quantity remains constant after t 12 15 y e a r s t 12 15 y e a r s is not in the critical interval so the corresponding final lake state is eutrophic fig 14 a shows the time during which the lake is oligotrophic for each reset date between t 0 and t 10 y e a r s the maximum corresponds to a reset date equal to 2 05 y e a r s this time it is within the critical interval so the corresponding final lake state is oligotrophic however the corresponding total input quantity of phosphorus 4 6 tons is higher than the minimal total input quantity of phosphorus it should be noted that these strategies are uncorrelated maximising the total trapped phosphorus does not correspond to an oligotrophic final lake state the structure management must be based only on the lake in addition it is possible to determine the optimal reset date without knowing the lake parameters or the structure parameters indeed the reset date for which the lake is most quickly returned to an oligotrophic state is such that the quantity of phosphorus in the lake water body is at its lowest level in our example this date corresponds to the time the trajectory of the quantity of phosphorus in the lake crosses the lake equilibrium curve we now present some important points for managing structures the next guideline corresponds to optimal management it does not require knowledge of the retention structure or lake parameters just the amount of phosphorus in the lake water it ensures the lake will be restord as soon as possible the input phosphorus rate is assumed to be constant 1 if after a reset the quantity of phosphorus in the lake water increases it means that the phosphorus input rate is too high to mitigate lake eutrophication the system is currently in the e set 2 otherwise i as long as the phosphorus quantity in the lake water decreases do nothing ii if the quantity of phosphorus increases reset the structure finally a structure with a varying effect lets us mitigate downstream lake eutrophication with appropriate management in addition only the knowledge of the quantity of phosphorus in the lake water is needed to efficiently manage the structure 5 a case study here we apply our approach to clear lake minesota its characteristics are given in barten 1987 since 1981 a wetland has been installed upstream of the lake in order to retain some of the incoming phosphorus we deduce the annual amount of phosphorus at different points in the system through the article barten 1987 the locations of the reading points are shown in fig 15 a and b which schematise this system the dynamical model corresponding to the system following the establishment of the wetland is d p l a k e t d t l 1 t w t s h p l a k e t r p l a k e t q m q p l a k e t q d h w w t d t h w l 2 t s w h w h w w t p l a k e is the quantity of phosphorus in the lake water body and w t is the quantity of phosphorus in the wetland water body l 1 t is the input flow of phosphorus into the lake and l 2 t the input flow of phosphorus into the wetland s h r q and m are the carpenter parameters and s w and h w are the parameters of the fixed trapping wetland because only the amount of phosphorus at the output of the structure is measured we cannot directly calibrate the model of the structure but only the model corresponding to its output w o u t h w w t d w o u t t d t d h w w t d t h w l 2 t s w h w h w w t the carpenter model is calibrated over the period 1977 1985 the corresponding parameters are available in table 5 although the measurement period is a little short to efficiently calibrate a structure model we nevertheless calibrated a model for a structure with fixed trapping effect over the period 1982 1985 whose parameters are given table 5 the calibration consisted of finding the model parameters that minimised the gap between the annual averages of the continuous models and the data the estimated trajectory of p l a k e and w o u t are shown fig 16 a and b this case study confirms and quantifies the fixed trapping effect of the wetland on the downstream lake which results in a decrease in the amount of phosphorus entering the lake the trapping capacity is 1 h w s w h w 0 8 see fig 7 thus we can see the trajectory that the lake would have taken without the installation of the wetland comparison of these two trajectories indicates that the wetland brings about a reduction of about 55 in the quantity of phosphorus in the lake water in addition the order of magnitude of the wetland s parameters indicates that its delayed effect is fairly significant h w s w s h 0 38 see fig 7 thus this system corresponds to c of figure a 18 that means that this wetland protects the lake from a possible increase in incoming phosphorus see section 4 2 3 6 discussion in our study we developed a dynamical model of the quantity of phosphorus in a retention structure water it enables us to focus on the dynamical effects of the structure and to understand how such a structure may mitigate the eutrophication of a downstream lake the trapping capacity allows the quantity of phosphorus in a downstream lake water body to be reduced and the delayed effect increases the downstream lake inertia in addition this study provides information about structures with the varying trapping effect like sedimentation ponds in particular the study of the model makes it possible to identify a clean up strategy to restore a lake downstream our study is theoretical since it focuses on the mathematical analysis of the models introduced however the models studied seem to be confirmed by the literature the identified effects correspond at least qualitatively to several wetlands and basins the delayed effect is similar to the one identified by walker 1998 the fixed trapping effect corresponds to the studied wetlands of barten 1987 kadlec 2009 and the varying trapping effect corresponds to the case study of brown et al 1981 the impact of a fixed effect on a downstream lake is confirmed by the example of the clear lake system nevertheless other case studies with longer data coverage would be useful to confirm the impact of structures on a downstream lake at the experimental level in order to overcome the lack of data and imprecision of measurements it would also be interesting to study this model in a stochastic context and then to evaluate the robustness of the results in addition a stochastic version of the model would allow for extreme external events weather pollution peaks etc to be taken into account a next steep should be to study more complex structures with several effects like delayed trapping recycling etc an alternative future study could be the analysis of the structure networks the watersheds often have several interconnected structures for example the great lakes network chapra and sonzogni 1979 or the cellular flow through certain marsh networks dunne et al 2015 a watershed can therefore have other unexpected dynamical properties depending on the network of the structure as for example the dynamical effect of a network of chemostats reported by rapaport et al 2015 rapaport 2018 if the network of structures were to be studied all the links between the structures would also have to be studied for example a channel might retain some of the phosphorus passing through it depending on its flow rate and cross section in addition according to its length a channel can also have a delayed effect in this sense a channel is also a separate structure we focused in this study on quantity of phosphorus in a lake water body our models can also be coupled to other state variables such as indicators of economic activity bacterial populations other nutrients etc in this way we could define other strategies for structure management and incorporate other policy priorities phosphorus is particularly studied because of its role in phytoplankton proliferation notably in toxic cyanobacterial development conley et al 2009 norton et al 2012 we showed that resetting a structure with a varying trapping effect like the cleaning of a sedimentation pond can act as a control on the phosphorus input rate into a lake water body therefore appropriate structure management could indirectly control cyanobacterial populations in a lake 7 conclusion in summary the design and management of a structure which must be tailored to the desired goal here the goal was to mitigate downstream lake eutrophication and not just to the quantity of trapped nutrient a fixed trapping effect reduces the quantity of phosphorus in any downstream lake in this respect the bigger the trapping effect the more efficient the structure however a retention structure can also have a delayed effect which increases the lake s inertia such a structure is beneficial in the protection of an oligotrophic lake but cannot be used to restore a eutrophic lake therefore the trapping capacity of a structure should not be the only design criteria an efficient design must also take into account the dynamical effects induced on a lake by the planned structures if this design is coupled with a reduction in nutrient emissions which is a common way to mitigate eutrophication schindler et al 2016 with a significant delayed effect the required reduction is longer or more substantial with the upstream structure than without it but with a low delayed effect the required reduction is lower with the upstream structure than without it this reduction is often a consequence of a change in human activities upstream of the lake with a structure in place the effect of this reduction can thus be increased by a trapping effect and appropriate management thus in terms of environmental impacts structures can improve the effect of a decrease in the phosphorus input rate while the economic cost involved in the eutrophication mitigation ludwig et al 2003 cools et al 2011 revolves around changes to human activity and the building and management of structures in addition management of a structure must be adapted to the goal managers in particular the strategy to mitigate a lake eutrophication is uncorrelated to that which retains the biggest quantity of input phosphorus an efficient strategy focuses only on the lake state this correlation between the management of the structure and the policy priority has been established by janssen et al 2005 for wetland management declarations of interest none acknowledgements we thank the région auvergne france for its financial support this work was supported by a contrat plan etat région from the région auvergne france cper 2015 2020 connecsens appendix a the delayed effect the delayed effect of a structure depends on the value of its h r parameter see section 3 2 figure a 17 shows the time evolution of the output rate from different structures according to their h r values the impact of such an effect on a downstream lake is presented in figure a 18 the e zone ans the o zone correspond to the initial states of a structure lake system for which the final state of the lake is oligotrophic and eutrophic respectively fig a 17 phosphorus output rates from three structures with delayed effect against time and with the same phosphorus input rate l 50 t o n s y e a r 1 and the same initial quantity of phosphorus 0 t o n the higher the h r of such a structure the faster the convergence of its quantity of phosphorus and thus of its phosphorus output rate fig a 17 fig a 18 evolution of the e zone all initial points from the e zone correspond to a final eutrophic state and the o zone all initial points from the o zone correspond to a the final oligotrophic state for a h r s h 100 b h r s h 1 c h r s h 0 1 and d h r s h 0 01 in each case we see the trajectory from the same initial point with l 0 3 t o n s y e a r 1 h r p r 0 0 05 t o n s y e a r s 1 and p 0 1 25 t o n s for a and b the initial and final states are eutrophic hence in these cases the final lake state is not significantly influenced by the upstream structure in contrast for c and d the initial lake state is eutrophic and its final state is oligotrophic so in these cases the final lake state is strongly influenced by the retention structure the trajectories are calculated for a total time of 100 years fig a 18 appendix b the fixed trapping effect figure b 19 shows the right shift induced by a fixed trapping effect on a lake downstream fig b 19 equilibrium states for a lake with and without an upstream structure with a fixed trapping effect the trapping effect here fixed is for s r h r 30 100 fig b 19 appendix c the recycling effect appendix c 1 introduction we focus here on the recycling effect of a structure upstream from a lake this recycling is characterized by an phosphorus output rate from the structure sediment there are two kinds of recycling 1 a punctual recycling caused by some events that does not depend on the studied system for example a dry period dierberg et al 2012 and 2 a structural recycling that depends on the inherent properties of the structure like the recycling rate of a lake which depends on the phosphorus quantity in its water here we focus on the impact of the structure recycling on the state change in a downstream lake in what follows we show that the recycling effect may change the lake state but an appropriate maintenance of the structure may avoid this possible change in both cases id est punctual and structural recycling for this purpose we consider a system composed by a structure with only a recycling effect upstream from a lake its diagram is shown figure c 20 fig c 20 diagram of the system one structure with a recycling effect one lake fig c 20 appendix c 2 influence of the recycling structure on a downstream lake in this short study we focus on both kinds of recycling with the same model indeed without delayed effects which corresponds to a limit case see section 3 2 the output phosphorus rate p o u t t from the structure is p o u t t l t r r t where l t is the input phosphorus rate into the structure and r r t the recycling phosphorus rate from the structure sediment r r t is either a ponctual event caracterized by an intensity i r and a time interval t r t 0 t f c 1 r r t i r t if t t r 0 if not or linked to the properties of the structure like the carpenter model in this case the recycling rate depends on the phosphorus quantity in the structure water thus directly on the input phosphorus rate l t in our case r r t i r if l t l r 0 if not where l r is a cut off phosphorus input rate this convenient is a limit case of recycling dynamics the recycling term well depends only on the current structure state and this model is not hysteretic which simplifies our study one phosphorus input rate corresponds to only one phosphorus output rate from the structure therefore in this case the recycling model can be rewritten as equation c 1 r r t i r t l t l r i r if t t r t l t l r 0 if not therefore we address both kinds of recycling through the same model by simplification we consider a recycling rate characterized only by a constant intensity i r t c s t during a given duration and with a constant input phosphorus rate l t c s t we consider an initial lake state caracterized by p t 0 p o u t t 0 l at the oligotrophic equilibrium state therefore this initial state is only characterized by the input phosphorus rate l we note there exists an interval of input phosphorus rate values l m i n l m a x such that if l l m i n the final equilibrium lake state will be oligotrophic independently of the recycling and if l l m a x the initial and final equilibrium lake states are necessarly eutrophic the recycling rate of the structure depends only on its intensity i r and its duration d r t f t 0 so considering the initial lake state characterized by the input phosphorus rate l and the recycling intensity we can compute the corresponding minimal duration d r such that if the recycling duration is longer than d r the final lake state is eutrophic and if the recycling duration is shorter than d r the final lake state is oligotrophic see fig 21 a and b fig c 21 a minimal recycling duration according to the phosphorus input rate and the recycling intensity we note that the value of l i r must be grater than l m a x to change the lake state b as an example considering the phosphorus input rate l e x 0 4 t o n s y e a r 1 and the recycling intensity i r 1 t o n s y e a r 1 the corresponding minimal recycling duration is equal to 5 m o n t h s therefore if the recycling duration is equal to 4 m o n t h s the final lake state is oligotrophic and if the recycling duration is equal to 6 m o n t h s the final lake state is eutrophic fig c 21 according to the recycling duration we can compute the total quantity of phosphorus that is recycled from the structure sediment and that is required to change the lake state it assumes that before the recycling event the structure has a trapping effect therefore each recycling duration and recycling intensity corresponds to a minimal trapping duration of the structure as an example if the phosphorus input rate l is equal to l e x 0 4 t o n s y e a r 1 and if before the recycling event the trapping capacity of the structure was modelled by s r 30 and h r 100 see section 3 3 figure c 22 shows the required trapping duration according to each recycling duration in particular this result shows that the trapping duration must be longer than 4 4 y e a r s independently of the recycling duration so finally a cleaning of the structure during this period of 4 4 y e a r s prevents any change in the lake state by a structure recycling event in addition in our example we observe that for a short recycling event the needed recycling intensity to change the lake state is huge in comparison with the other value scales of the model hence if the initial phosphorus input rate l is not very close to l m a x only extreme recycling events can change the lake state which corresponds to exceptional events on the other hand if l is very close to l m a x the recycling effect has the same impact one the downstream lake as a non extreme variation in the input phosphorus rate therefore in this case to prevent the lake state change it is better to focus on its initial condition than on possible recycling event fig c 22 according the recycling duration this figure shows the trapping duration required to accumulate enough phosphorus in order to change the lake state during the recycling event fig c 22 appendix c 3 conclusion according to the structure recycling properties the downstream lake can become eutrophic however it is possible to prevent this effect with a regular cleaning of the structure sediment in addition we have shown that a minimal recycling duration is required to change the lake state according to the recycling intensity and the initial lake state thus another way to prevent the recycling effect is cleaning the structure as soon as the structure recycling starts 
26181,the most common approach to mitigation of lake eutrophication is reduction of phosphorus emissions in particular by changing farm management this reduction can be combined with landscaping retention structures upstream of the lake the analyses of which the paper is based on the management of these structures currently focuses on maximising the quantity of phosphorus trapped regardless of lake dynamics this paper adapts a dynamical model of lake phosphorus to examine the effects of these phosphorus retention structures we highlight two effects first a structure that traps some of the phosphorus load before it reaches the lake reduces the amount of phosphorus in lake water second some retention structures slow down lake phosphorus dynamics in a way that may perversely slow lake restoration we propose a cleaning strategy that maximises the chances of restoring a lake to an oligotrophic condition we demonstrate our model with a real world case study keywords lake eutrophication dynamical model ecosystem resilience ecosystem management 1 introduction since industrialisation in the early 20th century a deterioration of the water quality in lakes has been observed mainly caused by human activities such as agriculture urbanisation and tourism ec 2006 bouwman et al 2009 bennett et al 1999 dokulil 2014 these activities are associated with chemical compounds such as phosphorus smith et al 1999 being released into the natural environment which disturbs the balance in lake ecosystems and increases phytoplankton biomass this process is called eutrophication and it is now a global issue for example some 40 of european lakes were eutrophic in 2013 european commission ip 13 947 cyanobacteria and algal proliferation is a common consequence of eutrophication and this has negative effects on the aquatic ecosystem the three main issues arising from eutrophication involve ecology health and the economy firstly cyanobacteria proliferation affects biodiversity because this proliferation occurs at the cost of other summer algae species secondly the decomposition of algae and cyanobacteria through the water column leads to anoxia or hypoxia in eutrophic lakes chislock et al 2013 both of which can have major adverse effects on lake biodiversity weinke et al 2018 and thirdly some cyanobacterial species also produce toxins which threaten human and animal health vasconcelos 2001 svirev et al 2014 therefore their proliferation presents a risk both for ecosystem and for the local population deterioration of a lake ecosystem can also have major economic impacts because it can reduce activities around the lake such as fisheries lee and jones 1991 and water sports for instance the estimated economic damage caused by the eutrophication in the usa is around 2 2 billion dollars annually dodds et al 2009 here we focus exclusively on lake eutrophication and in particular on the concentration of phosphorus in the lake water body since this which is the main limiting nutrient for lakes correll 1999 carpenter 2008 schindler et al 2008 there are two main approaches to mitigation of lake eutrophication the first one consists of limiting the phosphorus sources in the watershed and the second one of directly removing phosphorus from the lake or its closed environment the greater the input phosphorus rate the greater the phosphorus quantity in the lake water in addition the variation of the input phosphorus rate corresponds to the watershed phosphorus mass balance thus control of the watershed phosphorus mass balance in other words of the phosphorus sources is the main way to mitigate downstream lake eutrophication conley et al 2009 martin 2004 schindler et al 2008 however the economic interests linked to the phosphorus emissions such as agriculture and urbanisation may limit the reduction in phosphorus input eutrophication has also been related to meteorological events which are not controllable and to climate brias et al 2018 trolle et al 2011 in this context the other means to mitigate lake eutrophication phosphorus removal in the lake or its immediate neighbourhood may be used as a complement to the reduction in the phosphorus sources however the effectiveness of phosphorus removal is conditioned by the input phosphorus rate coming from the watershed removal consists of a one off removals and limited reduction to the amount of phosphorus in the lake water therefore if the input phosphorus rate is not reduced the lake may well return to its initial eutrophic state after each reduction thus phosphorus removal alone is not sufficient to accomplish long term goals of water quality improvement to remove phosphorus in the lake it is possible for instance to neutralise the available phosphorus in eutrophic lakes by adding chemicals smolders et al 2001 or to reduce the amount of phosphorus in the lake water body by dredging some of the phosphorus contained in sediments can be released from them into the lake water hoyer et al 2008 however we have chosen not to analyse these approaches here because they raise certain problems it is difficult sometimes impossible to implement them and they are not permanent solutions because they need to be regularly repeated here we focus on natural or artificial retention structures located between the watershed and lake such that the input flow into the lake passes through these structures some of them are designed to retain specific nutrients like phosphorus or nitrogen wetlands and semi natural areas use plants to trap these nutrients the nutrients trapped therefore depend on the plant species present kao et al 2003 other structures trap a certain quantity of nutrients such as sedimentation ponds where the water flow is low in this way some of the input nutrients are trapped in the pond sediments bjorneberg and lentz 2005 up to a maximum saturation these sedimentation ponds need to be cleaned regularly otherwise the pond s capacity for trapped input nutrients declines and ultimately vanishes retention structures are permanent but need appropriate management to maintain the desired trapping effect on phosphorus most of these retention structures are currently designed and managed in order to trap a maximum quantity of input phosphorus design and management are based on a static approach the smaller the incoming phosphorus quantity into the lake the smaller the final phosphorus quantity in the lake water but this static approach is not sufficient and the aim of this paper is to study the dynamical effects of retention structures on a downstream lake for example the static approach does not take two common dynamical effects into account the hysteretic behaviour of quantity of phosphorus and the time that this quantity of phosphorus converges to its equilibrium state the hysteretic behaviour of phosphorus means that two equilibrium lake states can correspond to the same input phosphorus rate one may be eutrophic and the other not the equilibrium lake state depends on the input phosphorus trajectory into the lake in other words the output phosphorus trajectory from the retention structure therefore appropriate retention structure management must focus on this trajectory which is the dynamical approach instead of the amount of trapped phosphorus which is the static approach in addition the convergence time for the amount of phosphorus in the water of a retention structure and lake adjacent can be length several decades for some lakes and structures the dynamical approach allows to model these convergence times and to study their impact on the lake s evolution in section 2 we present the dynamical carpenter model carpenter et al 1999 for amount of phosphorus in a lake water body which is applied in section 3 to the three kinds of retention effects in section 4 we study the dynamical effects of each type of retention on a downstream lake in section 5 we apply our model on a real life case study finally we discuss our results and look at potential directions for research 2 dynamical model of the quantity of phosphorus in a lake to study the dynamical effects of a retention structure on the quantity of phosphorus in the water of a downstream lake it is necessary to model dynamically the link between phosphorus input rate into a lake and the phosphorus quantity in this lake water body many models of phosphorus in a lake water body are based on vollenweider 1975 at the steady state ostrofsky 1978 larsen and mercier 1976 nürnberg 1984 they are not useable here because of their static aspect another approach seeks to model the lake as a whole these models often have many strongly inter connected state variables mooij et al 2010 makler pick et al 2011 robson 2014 of which the quantity of phosphorus is one however calibrating them with many variables and parameters needs large amounts of data which are not always readily available carpenter et al 1999 provides a dynamical model of the quantity of phosphorus in a lake water body this model implicitly incorporates all the phenomena involved in the phosphorus cycle including seasonal variations it is used in several cases ludwig et al 2003 cottingham et al 2015 carpenter 2005 wang et al 2012 according to carpenter et al 1999 for a phosphorus input rate l t the quantity of phosphorus in a lake water body p t is described by the following differential equation 1 d p t d t l t s h p t r p t q m q p t q the phosphorus input is composed of the input flow from upstream l t and the proportion of phosphorus that comes from the lake sediments r p t q m q p t q called the recycling rate the phosphorus output is composed of the outflow h p t and the proportion of phosphorus that enters the lake sediments s p t called the sedimentation rate this model is summarised in fig 1 in this study we use the parameters in based on a hysteretic lake for our numerical simulations in sections 3 and 4 these parameters do not correspond to a specific existing lake whose dimensions and hydraulic characteristics are known instead they are values classically used in the numerical study of hysteretic lakes wang et al 2012 janssen and carpenter 1999 martin 2004 the parameters are given table 1 there are three zones of equilibrium for a fixed phosphorus input rate the upper part of the curve corresponds to stable equilibrium points the middle part of the curve corresponds to unstable equilibrium points often called tipping points and the lower part of the curve corresponds to stable equilibrium points fig 2 a shows the evolution of the quantity of phosphorus according to some initial conditions an oligotrophic state corresponds to a low level of phosphorus in a lake water body limiting the primary production a eutrophic state to an excess of phosphorus quantity inducing massive phytoplankton biomass and a mesotrophic state at an intermediate state between the two these states are marked on the equilibrium curve of the model the lower part of the equilibrium curve corresponds to an oligotrophic state the upper part to a eutrophic state and the middle part to a mesotrophic state this distribution is a valid working hypothesis firstly because it is compatible with the qualitative definitions of the state of lake and secondly because some lakes correspond to this working hypothesis e g the bourget lake brias 2016 the points corresponding to eutrophic mesotrophic and oligotrophic states are shown on fig 2 b fig 2 a shows the starting points whose final corresponding lake states will be eutrophic or oligotrophic in this study we focus on hysteretic lakes carpenter et al 1999 janse et al 2008 rather than reversible for example lake washington edmondson 1991 or irreversible lakes for example the shagawa lake larsen et al 1979 1981 this is because the dynamical effects of retention structures on other types of lakes are easily describable whereas an irreversible lake cannot be restored by reducing incoming phosphorus carpenter et al 1999 thus no structure can restore an irreversible eutrophic lake in the case of a reversible lake one input phosphorus quantity corresponds to only one equilibrium phosphorus quantity in the lake water body therefore a static approach is sufficient to study a reversible lake the final lake state depends only on the input phosphorus quantity value 3 retention structures 3 1 introduction a number of models and studies of wetlands kadlec 2016 rahman et al 2016 lee et al 2017 giraldi et al 2010 and sedimentation ponds waajen et al 2016 palmer felgate et al 2011 pulatsü et al 2003 exist they study the correlations between internal processes based on the physical variables of these structures such as depth turbidity etc in addition some of them offer a static link between these physical variables and the rate at which phosphorus is trapped in these studies the efficiency of these structures is estimated according to the quantity of trapped phosphorus which corresponds to a static approach to study the dynamical effects of a retention structure on a downstream lake we need to dynamically model the phosphorus rate coming out of this structure here we study the effects of a generic structure on a downstream lake the device is shown in fig 3 this structure is only characterized by its effect on the phosphorus passing through it we focus on three types of effects which are summarised table 2 the delayed effect this is a delayed effect between the changes in the flow of phosphorus entering the structure and its flow at the outlet this effect corresponds to that observed by kadlec and wallace 2008 walker 1998 the fixed trapping effect in the short to medium term some structures have a trapping effect that can be considered as being fixed over time for example the houghton lake usa wetland trapping effect was stable over a 30 year measurement period kadlec 2009 the fixed trapping effect thus corresponds to a sufficiently short term for saturation effects not to be taken into account furthermore an adapted management of the retention structure may maintain its trapping capacity over a long term period for example sediment curing of a sedimentation pond or the mowing of a wetland if this maintenance if frequent enough the trapping capacity can be modelled as fixed for example yousef et al 1994 recommends a sediment cleaning frequency of 25 years to guarantee a variation in the trapping capacity of around 10 15 for detention ponds in central to southern florida otherwise it has to be modelled as a varying trapping effect the varying trapping effect the third kind of effect corresponds to sediment filling such as sedimentation ponds and some wetlands with a gradually declining trapping effect these structures require maintenance to remove phosphorus rich sediments brown et al 1981 in addition to the retention effect a structure may also have a recycling rate that corresponds to a phosphorus rate from the structure sediment the recycling event is of two types a punctual recycling and a structural recycling the punctual recycling depends on the properties of the soil gu et al 2017 kadlec 2016 and on specific events such as periods of hydration or drought dierberg et al 2012 the structural recycling is linked to the structure properties and only depends on its current state like in the case of lakes in appendix appendix c we show how the recycling may have a negative impact of the downstream lake but we also show how a regular maintenance of the structure can prevent these negative effects the trapping effect of each structure is modelled by part of the carpenter model the quantity of phosphorus in the water of a retention structure p r therefore follows this model in the general case 2 d p r t d t l t s r h r p r t r r t the main terms of this general model are given in table 3 because of the characteristics of a structure its model terms must be adapted the s r and h r parameters may or may not be dependent on the time according to the type of retention effect in addition the recycling term r r t has been separately addressed in appendix appendix c therefore in the next sections we consider a retention structure without recycling effect id est r r t 0 in order to study only the retention effect on a downstream lake and because the recycling effect may be negligible with an appropriate structure management in order to study each retention effect we first consider a simple generic structure using only one of the retention effects with a delayed effect in section 3 2 with a fixed trapping effect in section 3 3 and with a variable trapping effect in section 3 4 this allows us to distinguish the main effects of a structure each successive model introduces a new effect real structures may combine several retention effects in particular delayed and trapping fixed or not effects with recycling effect 3 2 the delayed effect here we assume that the trapping effect is nil s r p r t 0 which allows us to focus solely on the delayed effect this hypothesis corresponds to certain real structures some wetlands are specifically designed to retain nutrients other than phosphorus and thus have a negligible trapping effect on this nutrient for example the columbia system kadlec et al 2010 and the apopka c2 system dunne et al 2015 thus the dynamical model of the quantity of phosphorus in the water of the retention structure becomes d p r t d t l t h r p r t the corresponding diagram is shown in fig 4 at steady state and with a constant phosphorus input rate l the phosphorus quantity in the structure is p r e q l h r thus the phosphorus output rate from the structure with only a delayed effect is h r p r e q l with a constant phosphorus input rate l the dynamical solution of the model is p r t l h r 1 e h r t e h r t p r 0 thus the phosphorus quantity in the structure s water body converges to the value p r e q at the speed e h r t the greater the value of h r the faster the convergence figure a 17 shows the time evolution of the phosphorus output rate for certain structures based on their h r values 3 3 the fixed trapping effect with only a fixed trapping effect on the phosphorus the dynamical model of the phosphorus quantity in a retention structure is the same as equation 2 d p r t d t l t s r h r p r t where s r and h r are constant the corresponding diagram is shown in fig 5 this model is very close to the preceding one if we associate s r h r to the h r parameter above these models are mathematically the same the difference between the two models is the phosphorus output rate at steady state and with a fixed phosphorus input rate l t l the quantity of phosphorus in the water body of the retention structure is p r e q l s r h r the phosphorus output rate from this structure is therefore h r p r e q h r l s r h r l which is lower than the phosphorus input rate this is consistent with the trapping effect of such a structure on the phosphorus rate the trapping capacity does not depend on the value of s r but on the ratio h r s r h r if h r s r h r 1 which means s r 0 then the trapping effect is nil and if h r s r h r 0 which means s r h r then the trapping effect is maximal once again the speed of convergence towards steady state depends on the value of s r h r the greater this value the faster the convergence also the smaller its delayed effect 3 4 the varying trapping effect the dynamical model of the quantity of phosphorus in the water of a retention structure with a varying trapping effect is d p r t d t l t s r t h r t p r t the corresponding diagram is shown in fig 6 after a long enough time the trapping capacity becomes nil the h r and s r values therefore converge to h m a x and 0 to make the dynamical evolution of h r and s r more explicit the evolution of the trapping effect is considered as a sedimentary filling in the case of a sedimentation pond the trapped nutrients are stored in its sediments thus after a long enough time these sediments fill the structure and the structure cannot trap any more nutrients lake managers therefore have to clean sedimentation ponds regularly to reset their trapping power this filling means a decreasing filter power and so the convergence of the h r and the s r values towards h m a x and 0 the higher the sedimentation rate the faster the structure fill and the fuller the structure the lower the sedimentation rate mathematically it corresponds to the following differential system d s r t d t α r s r t the filling of the structure depends on the input nutrient rate thus in a more realistic model α r depends on l t but l t is considered as being constant here so α r too in addition when the sedimentation rate is nil the structure has the same behaviour as a structure with not trapping effect therefore when the sedimentation rate is nil h r t is constant after each reset operation corresponding to a cleaning operation for the sedimentation pond s r and h r are reset to their initial values s i and h i table 4 gives the parameters of this model finally the dynamical models of s r t and h r t are d s r t d t α r s r t d h r t d t h m a x α r α r h r t therefore s r t and h r t are such that s r t e α r t s i h r t h m a x 1 e α r t e α r t h i in fact the exact evolution of s r t and h r t does not matter here the simulations and the studied qualitative behaviours are based on the monotone convergence of s r t and h r t the speed of convergence of p r depends on the value of s r t h r t to isolate solely the varying trapping effect the s r t h r t values are assumed to be high this means that the delayed effect is negligible the convergence of the quantity of phosphorus in the water of a structure with a varying trapping effect is much faster than the decline in the trapping effect therefore the convergence of s r and h r takes longer than the convergence of the quantity of phosphorus in the water body of the structure mathematically that means s r h r α r the dynamics of the quantity of phosphorus in the water body of the structure is fast and the dynamics of its trapping effect is slow according to the timescale considered the dynamical model can thus be approximated as follows for a short timescale the phosphorus quantity in the structure is such that d p r t d t l t s r h r p r t where s r and h r are considered to be constant thus for a short timescale the varying trapping effect can be assumed to be the same as the fixed trapping effect for a long timescale the quantity of phosphorus in the structure is such that p r t l t s r t h r t where s r and h r respectively converge to 0 and h m a x the trapping effect of the structure depends on the s r and h r values if s r 0 the phosphorus output rate from the structure is lower than the phosphorus input rate 3 5 conclusion the qualitative behaviour of the three types of retention effect can be translated into three dynamical models a retention structure has a delayed effect between its phosphorus input and output rates a structure with a fixed trapping effect has a phosphorus output rate lower than its phosphorus input rate and the difference is proportional to this input rate a structure with a varying filter effect has the same properties as the second scenario initially and its trapping capacity becomes nil after a long enough time has elapsec the effects of a structure depend on the value of its parameters s r and h r fig 7 shows a qualitative distribution of these effects according to the values of s r and h r it shows that there is no effect failure depending on the values of the structure parameters thus a small variation in the parameters leads to only a small variation in the effects based on this we can consider these effects to be robust based on these parameter values the coordinates of point a correspond to parameter values where s r 0 and 0 h r 1 representing a retention structure with a delayed effect and no trapping effect the coordinates of the point b correspond to parameter values where s r 0 and h r 0 representing a retention structure with a trapping effect and no delayed effect the coordinates of the points c and d correspond to the initial and final parameter values of a structure with a varying trapping effect and without delayed effect at t 0 the parameters of this structure are such that s r t 0 0 and h r t 0 0 point c so the structure has a positive trapping effect after a long enough time the parameter values converge to s r t 0 0 and h r t 0 0 point d so the trapping effect becomes nil sediment filling it is possible to restore this trapping capacity structure which would result in instantly resetting its parameter values reset 4 influence of a retention structure on a downstream lake 4 1 introduction in this section we consider one structure one lake systems the lake state depends on the phosphorus input rate as shown in section 2 a reduction in this input rate should thus have an influence on its state but the final lake state does not depend only on the trapping capacity of the upstream structure but also on the trajectory of the quantity of phosphorus from the water body of the structure quantity of phosphorus in the water body of the lake system the order of the studied systems in this section is the same as that of the effects studied in section 3 that is the delayed effect the fixed trapping effect then the varying trapping effect 4 2 the delayed effect 4 2 1 introduction let us consider the one structure with only a delayed effect one lake system its diagram is shown in fig 8 the corresponding dynamical model is d p r t d t l t h r p r t d p t d t h r p r t s h p t r p t q m q p t q 4 2 2 the delayed effect with a fixed phosphorus input rate we consider a fixed phosphorus input rate l t l to understand the effect of the structure on a downstream lake two cases are considered the convergence time of the quantity of phosphorus in the water body of the structure is very short compared to the convergence time of the quantity of phosphorus in a downstream lake water body therefore h r s h 1 during this short convergence time the phosphorus output rate from the structure becomes equal to h r p r e q l in this case the lake s trajectory is thus unaffected by the upstream structure the convergence time of the phosphorus quantity in the structure water body is very long compared to the convergence time of the phosphorus quantity in a downstream lake water body therefore h r s h 1 for a short time the phosphorus output rate from the structure can be considered as being constant and the phosphorus quantity in the lake water body converges to the corresponding equilibrium value in this case the lake s trajectory is thus strongly influenced by the structure in summary the impact of the delayed effect on a downstream lake depends on the ratio h r s h and on the initial phosphorus quantity in the water body of the structure for different h r s h values figure a 18 shows the initial p d 0 p 0 values from which the final downstream lake state is either eutrophic or oligotrophic with some associated trajectories of h r p r t p t the initial quantity of phosphorus in the water body of the structure is a given of the system thus a delayed effect cannot restore a downstream lake it can only protect an oligotrophic lake from an increase in phosphorus inflow as we see in the following section 4 2 3 the delayed effect with a varying phosphorus input rate without an upstream structure the lake is directly impacted by changes in l t with an upstream structure these changes are delayed therefore a change in l t that modifies the state of a lake without upstream structure does not necessarily modify the state of a lake with an upstream structure the delayed effect thus increases the lake s inertia for a lake with an upstream structure the change needed in the input phosphorus rate to change the final lake state is greater than the change needed for a lake with no upstream structure figure c 21 illustrates this phenomena 4 2 4 conclusion despite the non trapping effect the delayed effect affects the evolution of the quantity of phosphorus in a downstream lake water body and thus the state of this lake the stronger the structure s delayed effect the greater the lake s inertia this delayed effect can be used to protect an oligotrophic lake with an increase of the phosphorus input rate it maintains the lake in this acceptable state however the delayed effect also maintains a eutrophic lake in its initial state see fig 9 4 3 the fixed trapping effect let us consider the one structure with only a fixed trapping effect one lake system shown in fig 10 the corresponding dynamical model is d p r t d t l t s r h r p r t d p t d t h r p r t s h p t r p t q m q p t q the ratio h r s r s h 1 is very high yielding no delayed effect part of the phosphorus input is trapped by the structure therefore if l t is constant the phosphorus input rate into the lake is h r l s r h r instead of l in the absence of a structure the set of equilibrium states of the lake with one structure is merely right shifted relative to the situation of the same lake with no structure this right shift is proportional to l the greater the value of l the greater the shift therefore the input value set for which the corresponding equilibrium state of the lake is oligotrophic is increased hence given a fixed quantity of phosphorus in the lake water body the corresponding equilibrium phosphorus input rate is greater with a trapping effect than without figure b 19 shows this right shift effect the trapping effect contributes to mitigate eutrophication for a lake with an upstream structure the phosphorus input rate needed to reach a eutrophic state is higher than with no structure and even if a lake with a structure is eutrophic the quantity of phosphorus in its water body is less than with no upstream structure for the same phosphorus input rate 4 4 the varying trapping effect 4 4 1 introduction let us consider the one structure with a varying trapping effect one lake system which is shown in fig 11 the corresponding dynamical model is d p r t d t l t s r t h r t p r t d p t d t h r t p r t s h p t r p t q m q p t q with s r 0 s i and s r t 0 when t and h r 0 h i and h r t h m a x when t the s r t h r t values are assumed to be high yielding no delayed effect in addition the phosphorus input rate l is assumed to be constant therefore the quantity of phosphorus in the water body of the structure is immediately equal to the l s r t h r t value an initial situation of the system structure lake with no delayed effect is a pair of values p 0 l which are respectively the initial quantity of phosphorus in the lake water body and the phosphorus input rate in the structure lake system 4 4 2 critical times there are three sets of p 0 l the initial quantity of phosphorus in the lake water body and the input quantity of phosphorus into the system respectively o set the set of initial conditions for which no reset is required to obtain a final oligotrophic state this set includes a set of points from which the final corresponding lake state is oligotrophic with s r h r 0 h m a x e set the set of initial conditions for which the final state is eutrophic independent of the reset date this set includes a set of points from which the final corresponding lake state is eutrophic with s r h r s i h i i set the set of initial conditions for which without reset the final lake state is eutrophic but for which if a reset dates were introduced could result in the final lake state being oligotrophic these sets of initial situations are shown in fig 12 a let us consider an initial situation in the i set without reset fig 13 a the final lake state is eutrophic with an appropriate reset date fig 13 b the final lake state is oligotrophic however not all of the reset dates result in a final oligotrophic state where there is too long delay before reset the final lake state remains eutrophic fig 13 c for each reset date the final quantity of phosphorus in the lake water body is computed and thus the corresponding final lake state is known as shown in fig 12 b there is a time interval between two critical times within which a reset date is efficient and beyond which the reset date is ineffective 4 4 3 the structure management to select the best reset date a criterion is needed we compare here two strategies to select this reset date classic management many studies on structures focus on the parameters or management for which the total trapped phosphorus is maximal the first possible strategy is thus the one such that 0 t r a p p e d p h o s p h o r u s is maximal which means that 0 h r t p r t is minimal optimal management here the goal is to mitigate eutrophication therefore the longer a lake is oligotrophic the longer it is not eutrophic thus another possible strategy is one in which the time during which the lake is oligotrophic is maximised with a finite time the reset date that maximises the quality criterion corresponds to the cleaning date for which the lake is mostly oligotrophic in other words the reset date that maximises the quality criterion is the reset date for which the lake is most quickly returned to an oligotrophic state let us consider the initial situation corresponding to the example in fig 13 fig 14 b shows the total input quantity of phosphorus that enters into the lake depending on the reset date during the simulation time 25 y e a r s the minimal total phosphorus input quantity 4 3 tons corresponds to a reset between t 12 15 y e a r s and t 12 85 y e a r s because of the limiting time of the simulation the total input quantity of phosphorus increases after this date with an infinite run time this quantity remains constant after t 12 15 y e a r s t 12 15 y e a r s is not in the critical interval so the corresponding final lake state is eutrophic fig 14 a shows the time during which the lake is oligotrophic for each reset date between t 0 and t 10 y e a r s the maximum corresponds to a reset date equal to 2 05 y e a r s this time it is within the critical interval so the corresponding final lake state is oligotrophic however the corresponding total input quantity of phosphorus 4 6 tons is higher than the minimal total input quantity of phosphorus it should be noted that these strategies are uncorrelated maximising the total trapped phosphorus does not correspond to an oligotrophic final lake state the structure management must be based only on the lake in addition it is possible to determine the optimal reset date without knowing the lake parameters or the structure parameters indeed the reset date for which the lake is most quickly returned to an oligotrophic state is such that the quantity of phosphorus in the lake water body is at its lowest level in our example this date corresponds to the time the trajectory of the quantity of phosphorus in the lake crosses the lake equilibrium curve we now present some important points for managing structures the next guideline corresponds to optimal management it does not require knowledge of the retention structure or lake parameters just the amount of phosphorus in the lake water it ensures the lake will be restord as soon as possible the input phosphorus rate is assumed to be constant 1 if after a reset the quantity of phosphorus in the lake water increases it means that the phosphorus input rate is too high to mitigate lake eutrophication the system is currently in the e set 2 otherwise i as long as the phosphorus quantity in the lake water decreases do nothing ii if the quantity of phosphorus increases reset the structure finally a structure with a varying effect lets us mitigate downstream lake eutrophication with appropriate management in addition only the knowledge of the quantity of phosphorus in the lake water is needed to efficiently manage the structure 5 a case study here we apply our approach to clear lake minesota its characteristics are given in barten 1987 since 1981 a wetland has been installed upstream of the lake in order to retain some of the incoming phosphorus we deduce the annual amount of phosphorus at different points in the system through the article barten 1987 the locations of the reading points are shown in fig 15 a and b which schematise this system the dynamical model corresponding to the system following the establishment of the wetland is d p l a k e t d t l 1 t w t s h p l a k e t r p l a k e t q m q p l a k e t q d h w w t d t h w l 2 t s w h w h w w t p l a k e is the quantity of phosphorus in the lake water body and w t is the quantity of phosphorus in the wetland water body l 1 t is the input flow of phosphorus into the lake and l 2 t the input flow of phosphorus into the wetland s h r q and m are the carpenter parameters and s w and h w are the parameters of the fixed trapping wetland because only the amount of phosphorus at the output of the structure is measured we cannot directly calibrate the model of the structure but only the model corresponding to its output w o u t h w w t d w o u t t d t d h w w t d t h w l 2 t s w h w h w w t the carpenter model is calibrated over the period 1977 1985 the corresponding parameters are available in table 5 although the measurement period is a little short to efficiently calibrate a structure model we nevertheless calibrated a model for a structure with fixed trapping effect over the period 1982 1985 whose parameters are given table 5 the calibration consisted of finding the model parameters that minimised the gap between the annual averages of the continuous models and the data the estimated trajectory of p l a k e and w o u t are shown fig 16 a and b this case study confirms and quantifies the fixed trapping effect of the wetland on the downstream lake which results in a decrease in the amount of phosphorus entering the lake the trapping capacity is 1 h w s w h w 0 8 see fig 7 thus we can see the trajectory that the lake would have taken without the installation of the wetland comparison of these two trajectories indicates that the wetland brings about a reduction of about 55 in the quantity of phosphorus in the lake water in addition the order of magnitude of the wetland s parameters indicates that its delayed effect is fairly significant h w s w s h 0 38 see fig 7 thus this system corresponds to c of figure a 18 that means that this wetland protects the lake from a possible increase in incoming phosphorus see section 4 2 3 6 discussion in our study we developed a dynamical model of the quantity of phosphorus in a retention structure water it enables us to focus on the dynamical effects of the structure and to understand how such a structure may mitigate the eutrophication of a downstream lake the trapping capacity allows the quantity of phosphorus in a downstream lake water body to be reduced and the delayed effect increases the downstream lake inertia in addition this study provides information about structures with the varying trapping effect like sedimentation ponds in particular the study of the model makes it possible to identify a clean up strategy to restore a lake downstream our study is theoretical since it focuses on the mathematical analysis of the models introduced however the models studied seem to be confirmed by the literature the identified effects correspond at least qualitatively to several wetlands and basins the delayed effect is similar to the one identified by walker 1998 the fixed trapping effect corresponds to the studied wetlands of barten 1987 kadlec 2009 and the varying trapping effect corresponds to the case study of brown et al 1981 the impact of a fixed effect on a downstream lake is confirmed by the example of the clear lake system nevertheless other case studies with longer data coverage would be useful to confirm the impact of structures on a downstream lake at the experimental level in order to overcome the lack of data and imprecision of measurements it would also be interesting to study this model in a stochastic context and then to evaluate the robustness of the results in addition a stochastic version of the model would allow for extreme external events weather pollution peaks etc to be taken into account a next steep should be to study more complex structures with several effects like delayed trapping recycling etc an alternative future study could be the analysis of the structure networks the watersheds often have several interconnected structures for example the great lakes network chapra and sonzogni 1979 or the cellular flow through certain marsh networks dunne et al 2015 a watershed can therefore have other unexpected dynamical properties depending on the network of the structure as for example the dynamical effect of a network of chemostats reported by rapaport et al 2015 rapaport 2018 if the network of structures were to be studied all the links between the structures would also have to be studied for example a channel might retain some of the phosphorus passing through it depending on its flow rate and cross section in addition according to its length a channel can also have a delayed effect in this sense a channel is also a separate structure we focused in this study on quantity of phosphorus in a lake water body our models can also be coupled to other state variables such as indicators of economic activity bacterial populations other nutrients etc in this way we could define other strategies for structure management and incorporate other policy priorities phosphorus is particularly studied because of its role in phytoplankton proliferation notably in toxic cyanobacterial development conley et al 2009 norton et al 2012 we showed that resetting a structure with a varying trapping effect like the cleaning of a sedimentation pond can act as a control on the phosphorus input rate into a lake water body therefore appropriate structure management could indirectly control cyanobacterial populations in a lake 7 conclusion in summary the design and management of a structure which must be tailored to the desired goal here the goal was to mitigate downstream lake eutrophication and not just to the quantity of trapped nutrient a fixed trapping effect reduces the quantity of phosphorus in any downstream lake in this respect the bigger the trapping effect the more efficient the structure however a retention structure can also have a delayed effect which increases the lake s inertia such a structure is beneficial in the protection of an oligotrophic lake but cannot be used to restore a eutrophic lake therefore the trapping capacity of a structure should not be the only design criteria an efficient design must also take into account the dynamical effects induced on a lake by the planned structures if this design is coupled with a reduction in nutrient emissions which is a common way to mitigate eutrophication schindler et al 2016 with a significant delayed effect the required reduction is longer or more substantial with the upstream structure than without it but with a low delayed effect the required reduction is lower with the upstream structure than without it this reduction is often a consequence of a change in human activities upstream of the lake with a structure in place the effect of this reduction can thus be increased by a trapping effect and appropriate management thus in terms of environmental impacts structures can improve the effect of a decrease in the phosphorus input rate while the economic cost involved in the eutrophication mitigation ludwig et al 2003 cools et al 2011 revolves around changes to human activity and the building and management of structures in addition management of a structure must be adapted to the goal managers in particular the strategy to mitigate a lake eutrophication is uncorrelated to that which retains the biggest quantity of input phosphorus an efficient strategy focuses only on the lake state this correlation between the management of the structure and the policy priority has been established by janssen et al 2005 for wetland management declarations of interest none acknowledgements we thank the région auvergne france for its financial support this work was supported by a contrat plan etat région from the région auvergne france cper 2015 2020 connecsens appendix a the delayed effect the delayed effect of a structure depends on the value of its h r parameter see section 3 2 figure a 17 shows the time evolution of the output rate from different structures according to their h r values the impact of such an effect on a downstream lake is presented in figure a 18 the e zone ans the o zone correspond to the initial states of a structure lake system for which the final state of the lake is oligotrophic and eutrophic respectively fig a 17 phosphorus output rates from three structures with delayed effect against time and with the same phosphorus input rate l 50 t o n s y e a r 1 and the same initial quantity of phosphorus 0 t o n the higher the h r of such a structure the faster the convergence of its quantity of phosphorus and thus of its phosphorus output rate fig a 17 fig a 18 evolution of the e zone all initial points from the e zone correspond to a final eutrophic state and the o zone all initial points from the o zone correspond to a the final oligotrophic state for a h r s h 100 b h r s h 1 c h r s h 0 1 and d h r s h 0 01 in each case we see the trajectory from the same initial point with l 0 3 t o n s y e a r 1 h r p r 0 0 05 t o n s y e a r s 1 and p 0 1 25 t o n s for a and b the initial and final states are eutrophic hence in these cases the final lake state is not significantly influenced by the upstream structure in contrast for c and d the initial lake state is eutrophic and its final state is oligotrophic so in these cases the final lake state is strongly influenced by the retention structure the trajectories are calculated for a total time of 100 years fig a 18 appendix b the fixed trapping effect figure b 19 shows the right shift induced by a fixed trapping effect on a lake downstream fig b 19 equilibrium states for a lake with and without an upstream structure with a fixed trapping effect the trapping effect here fixed is for s r h r 30 100 fig b 19 appendix c the recycling effect appendix c 1 introduction we focus here on the recycling effect of a structure upstream from a lake this recycling is characterized by an phosphorus output rate from the structure sediment there are two kinds of recycling 1 a punctual recycling caused by some events that does not depend on the studied system for example a dry period dierberg et al 2012 and 2 a structural recycling that depends on the inherent properties of the structure like the recycling rate of a lake which depends on the phosphorus quantity in its water here we focus on the impact of the structure recycling on the state change in a downstream lake in what follows we show that the recycling effect may change the lake state but an appropriate maintenance of the structure may avoid this possible change in both cases id est punctual and structural recycling for this purpose we consider a system composed by a structure with only a recycling effect upstream from a lake its diagram is shown figure c 20 fig c 20 diagram of the system one structure with a recycling effect one lake fig c 20 appendix c 2 influence of the recycling structure on a downstream lake in this short study we focus on both kinds of recycling with the same model indeed without delayed effects which corresponds to a limit case see section 3 2 the output phosphorus rate p o u t t from the structure is p o u t t l t r r t where l t is the input phosphorus rate into the structure and r r t the recycling phosphorus rate from the structure sediment r r t is either a ponctual event caracterized by an intensity i r and a time interval t r t 0 t f c 1 r r t i r t if t t r 0 if not or linked to the properties of the structure like the carpenter model in this case the recycling rate depends on the phosphorus quantity in the structure water thus directly on the input phosphorus rate l t in our case r r t i r if l t l r 0 if not where l r is a cut off phosphorus input rate this convenient is a limit case of recycling dynamics the recycling term well depends only on the current structure state and this model is not hysteretic which simplifies our study one phosphorus input rate corresponds to only one phosphorus output rate from the structure therefore in this case the recycling model can be rewritten as equation c 1 r r t i r t l t l r i r if t t r t l t l r 0 if not therefore we address both kinds of recycling through the same model by simplification we consider a recycling rate characterized only by a constant intensity i r t c s t during a given duration and with a constant input phosphorus rate l t c s t we consider an initial lake state caracterized by p t 0 p o u t t 0 l at the oligotrophic equilibrium state therefore this initial state is only characterized by the input phosphorus rate l we note there exists an interval of input phosphorus rate values l m i n l m a x such that if l l m i n the final equilibrium lake state will be oligotrophic independently of the recycling and if l l m a x the initial and final equilibrium lake states are necessarly eutrophic the recycling rate of the structure depends only on its intensity i r and its duration d r t f t 0 so considering the initial lake state characterized by the input phosphorus rate l and the recycling intensity we can compute the corresponding minimal duration d r such that if the recycling duration is longer than d r the final lake state is eutrophic and if the recycling duration is shorter than d r the final lake state is oligotrophic see fig 21 a and b fig c 21 a minimal recycling duration according to the phosphorus input rate and the recycling intensity we note that the value of l i r must be grater than l m a x to change the lake state b as an example considering the phosphorus input rate l e x 0 4 t o n s y e a r 1 and the recycling intensity i r 1 t o n s y e a r 1 the corresponding minimal recycling duration is equal to 5 m o n t h s therefore if the recycling duration is equal to 4 m o n t h s the final lake state is oligotrophic and if the recycling duration is equal to 6 m o n t h s the final lake state is eutrophic fig c 21 according to the recycling duration we can compute the total quantity of phosphorus that is recycled from the structure sediment and that is required to change the lake state it assumes that before the recycling event the structure has a trapping effect therefore each recycling duration and recycling intensity corresponds to a minimal trapping duration of the structure as an example if the phosphorus input rate l is equal to l e x 0 4 t o n s y e a r 1 and if before the recycling event the trapping capacity of the structure was modelled by s r 30 and h r 100 see section 3 3 figure c 22 shows the required trapping duration according to each recycling duration in particular this result shows that the trapping duration must be longer than 4 4 y e a r s independently of the recycling duration so finally a cleaning of the structure during this period of 4 4 y e a r s prevents any change in the lake state by a structure recycling event in addition in our example we observe that for a short recycling event the needed recycling intensity to change the lake state is huge in comparison with the other value scales of the model hence if the initial phosphorus input rate l is not very close to l m a x only extreme recycling events can change the lake state which corresponds to exceptional events on the other hand if l is very close to l m a x the recycling effect has the same impact one the downstream lake as a non extreme variation in the input phosphorus rate therefore in this case to prevent the lake state change it is better to focus on its initial condition than on possible recycling event fig c 22 according the recycling duration this figure shows the trapping duration required to accumulate enough phosphorus in order to change the lake state during the recycling event fig c 22 appendix c 3 conclusion according to the structure recycling properties the downstream lake can become eutrophic however it is possible to prevent this effect with a regular cleaning of the structure sediment in addition we have shown that a minimal recycling duration is required to change the lake state according to the recycling intensity and the initial lake state thus another way to prevent the recycling effect is cleaning the structure as soon as the structure recycling starts 
26182,reef protected beaches are vulnerable to the effects of sea level rise and degradation of their associated fringing reefs the swan hydrodynamic wave model is combined with classical theory describing the planform of beaches in equilibrium with the wave forcing to estimate the reef top hydrodynamics and the shoreline configuration in the lee of the reefs open access bayesian belief networks with high accuracy and simple user interfaces have been built to communicate the results the bbn enable end users to access all the model results and to compare different scenario to determine how changes in the wave climate or reef elevation change the shoreline configuration the results show that recession of the shoreline in the lee of fringing reefs due to sea level rise may be much greater than that expected on open coast beaches loss of reef flat elevation can also lead to severe shoreline erosion keywords bayesian belief networks reefs coastal morphology climate change communication tools 1 introduction fringing reefs protect communities and beaches from wave action under extreme and normal wave conditions providing an ecosystem service ferrario et al 2014 one result of the presence of such reefs is the formation of salients bulges at the shoreline in the lee of the reef the salient provides a buffer of sand to the beach or generates larger scale morphological features which may become vegetated over time fig 1 in general reefs act as controls on the shoreline shape gonzalez and medina 2001 and changes in reef structure can lead to changes in shoreline position de alegria arzaburu et al 2013 reguero et al 2018 for finite lengths of fringing reefs the geometry of the salient is dependent on the geometry of the reef the water depth over the reef flat and the offshore wave conditions the shoreline shape and the width of the salient in the cross shore direction is the result of complex processes of wave breaking and refraction over the reef therefore changes in offshore wave climate either wave height or direction and changes in reef elevation or roughness lead to changes in the salient size and shape leading to beach erosion and potentially the loss of the salient and the protection afforded to the shoreline behind changes in reef flat elevation relative to sea level are particularly important since the depth over the reef flat is the dominant factor controlling the wave conditions at the landward edge of the reef hardy and young 1996 gourlay 1994 sheppard et al 2005 lowe et al 2005 as the salient erodes sediment will be redistributed along the adjacent beach or transported downdrift fig 1 illustrates two examples of salients formed behind coral reefs at el nido philippines and ningaloo reef wa australia similar salients can be observed in the lee of rocky reefs wave transformation over fringing reefs have been extensively studied and the physical processes are well known being dominated by refraction and breaking processes with friction generally playing a minor role once waves are breaking except on very rough reefs monismith 2007 pearson et al 2017 wave refraction and the resulting wave direction at the shoreline is also very important in controlling the shoreline shape and beach planform kench and brander 2006 gourlay 1988 in addition to sediment retention on reef platforms mandlier and kench 2012 the details of the wave transformation processes for the present parametric study are discussed in depth in baldock et al 2019 in review and show a complex behaviour dependent on the relative importance of refraction and wave breaking fig 3 the shoreline shape behind finite lengths of reef and the impact of sea level rise or reef degradation has been less well documented and is more complex but due to the increased wave energy and changes in reef flat hydrodynamics significant changes in sediment transport and shoreline position are possible storlazzi et al 2011 baldock et al 2015 reguero et al 2018 grady et al 2013 while process modelling of coastal morphology is a complex task and therefore subject to uncertainty over time scales of evolution process based models almost universally adopt an evolution to equilibrium type transport approach where for constant forcing the plan form evolves so that longshore transport gradients and changes in planform tend to zero over time de vriend et al 1993 the uncertainty over timescales can be avoided by using the widely adopted classical equilibrium planform modelling approach where the shoreline orientation is based on the direction of the mean wave energy flux acting on the coastline elshinnawy et al 2017 jackson and cooper 2010 then long term morphological dynamics result from changes that equilibrium condition that can be described by a relationship between hydrodynamics and morphology van maanen et al 2016 and this is the approach adopted here the complexity of the wave transformation processes and resulting shape of the shoreline behind the reef means that the results are not easily communicated to end users and coastal managers particularly in developing countries and remote communities open access bayesian belief networks with high accuracy and simple user interfaces have been built to address this issue the model enables users to access all the model results and compare different scenario to determine how changes in the wave climate or reef elevation influences shoreline erosion behind the reef bayesian belief networks henceforth bbn have been widely utilised to study complex physical and biological processes recent applications to coastal processes include predicting coastal vulnerability to sea level rise barrier island geomorphology and flood hazards on reef protected beaches gutierrez et al 2011 2015 pearson et al 2017 in the present context callaghan et al 2017 2018 show how a simple bbn framework provides near perfect accuracy in synthesising the results of a multi parameter model database baldock et al 2014 for one dimensional i e shore normal wave transformation across a barrier reef and lagoon system in that case both the number of parameters describing the reef geomorphology and the wave climate were considerably smaller than those required for modelling two dimensional wave propagation across a fringing reef and the resulting shoreline shape however the same simple bbn framework can be adopted which is more transparent to users since no expert knowledge of the physical processes is required to understand the network structure further since detailed knowledge of the bathymetry and physical properties of most reefs to be studied is not always well known the bbn intrinsically enables users to account for uncertainty or missing information the present paper considers these issues and presents a number of simple bbn to communicate the results of a parametric study of wave transformation over fringing reefs and the estimated shoreline shape that required hundreds of hours of high performance computing to model the hydrodynamics and shoreline evolution to do this the approach of baldock et al 2014 2015 is followed who investigated one dimensional wave transformation over a very large range of idealised barrier reef lagoon geometry using a parametric wave transformation model the model results were then used to assess the impacts of changes in water level on wave conditions flow velocity and wave forces on different coral species in that work the results were presented graphically which limits resolution and limits access by non expert end users pearson et al 2017 adopted a similar approach and constructed a similar bbn to that of callaghan et al 2017 using a wave resolving wave model to synthesise the propagation and run up of waves over a suite of idealised fringing reefs in that study computational requirements again limited the study to one dimensional wave propagation pearson et al 2017 also note that a bbn of this form provides a useful 1st order approach which is appropriate when only relatively low resolution remotely sensed data is available here a two dimensional 2 d wave model is combined with a classical equilibrium shape planform model to determine the hydrodynamics and expected shoreline configuration for a very large range of idealised geometry and wave conditions representing wave propagation across finite length fringing reefs the bbn models were designed during workshops held in the philippines indonesia and australia during the capturing coral reef ecosystem services project https ccres net with the bbn user manuals and user guides to the coastal processes refined with stakeholder and end user input feedback we considered a number of different bbn for two reasons firstly transparency and simplicity for end users assessed through iterative design in ccres workshops secondly during this process we found that accuracy of the bbn depends on the network structure and here we show that networks constructed on the basis of expert knowledge were not in fact optimal in terms of accuracy we note that the shoreline planform model depends on the swan model and thus any errors in the wave modelling propagate into the bbn results while this is a limitation the results of many environmental models require forcing derived from other models most notably models that provide or simulate environmental parameters however swan has been demonstrated to provide an accurate representation of the hydrodynamics over coral reefs storlazzi et al 2011 buckley et al 2014 and it is the most widely verified nearshore wave hydrodynamics model available and therefore we have confidence the results are sufficiently accurate to apply for the present purposes we show that the final bbn constructed and tested provide perfect reproduction of the underlying swan model results for the hydrodynamics and are generally within 5 of the underlying shoreline results for the planform configuration hence there is no significant model of model effect i e use of the bbn does not degrade the model accuracy the bbn then allow simple and fast comparison of key parameters that characterise the hydrodynamics and shoreline configuration for different reef geometry and wave conditions enabling changes in parameters due to slr loss of reef flat elevation or changes in wave climate to be easily assessed the purpose of the bbn is to provide a simple and quick generic tool to assess how different reefs and particularly how changes to wave and water level conditions on those reefs influence the hydrodynamics over the reef and the shoreline planform behind the reef the effectiveness of different reefs in stabilising the shoreline under future scenario can then be assessed to first order alternatively the reefs for which the shoreline configuration is most sensitive to changes in wave climate water level or reef degradation can be identified for more detailed further study 2 hydrodynamic modelling the hydrodynamic modelling uses the swan booij et al 1999 model which is a parametric wave averaged model widely used for wind and swell wave propagation in coastal regions and which has been extensively verified for propagation over reefs for very similar scenario to those considered here storlazzi et al 2011 grady et al 2013 the use of the swan model in this context is discussed at length in baldock et al 2014 2015 and other than the refraction processes occurring for these 2 d scenario there is no significant difference in the modelling approach buckley et al 2014 found swan to be as accurate as wave resolving or time domain models in predicting sea and sell wave heights across fringing reeds using default model parameters swan does not include long waves which are important in enhancing wave run up in extreme conditions however the shoreline configuration model is not dependent on long waves and sediment transport and planform evolution modelling by conventional coastal engineering modelling approaches does not include long waves in part this is due to the additional time dependence required in the modelling significantly increasing computation time but largely because the influence of long waves on sediment transport processes and beach evolution is not sufficiently well known to warrant inclusion of long waves even for cross shore 1 d processes only baldock et al 2011 likewise circulation patterns currents are not included in the swan model which are strongly controlled by lagoon and channel geometry rather than the morphology of the reef lowe et al 2010 inclusion of such circulation would increase computation time very significantly and again is not warranted since nearshore currents are not used in classical equilibrium beach planform morphodynamic modelling dissipation of the majority of the incident wave energy over reefs as estimated by the swan model is dominated by breaking processes with bottom friction relatively less important except for lower energy conditions very rough reefs and deeper water depths baldock et al 2014 similar results are found with wave resolving models harris et al 2018 and in the field costa et al 2016 in addition if the depth over the reef crest reduces toward the shore breaking and dissipation will eventually occur reducing the importance of roughness in controlling the nearshore wave conditions while baldock et al 2015 found that for horizontal reef flats reef roughness influenced likely cross shore sediment transport patterns behind barrier reefs i e loss of rugosity increased vulnerability the influence of rugosity on wave direction is more limited since wave refraction is controlled by the depth we also use an equilibrium model for the shoreline planform shape which is independent of wave height hence changes in rugosity have little influence on nearshore wave directions and the shoreline evolution further the assumptions in both the shoreline modelling and the swan modelling make any confidence in comparing the shoreline configuration for otherwise identical rough and smooth reefs subject to question consequently we use smooth reefs for this modelling since degrading reefs are more likely to have lost coral structure 2 1 reef geometry the idealised 3 d fringing reef geometry is illustrated in fig 2 the most important parameters controlling the reef flat and nearshore hydrodynamics are the wave height h wave period t reef width w and water depth d pearson et al 2017 baldock et al 2014 the important parameters controlling the shoreline evolution are the water depth reef length l and the wave direction dir plus the resulting refraction and wave breaking processes on the reef while natural reef geometries are very variable a rectangular reef is simpler for an idealised parametric study and a natural extension of previous 1 d modelling sharp edges were gently rounded to avoid potential numerical instabilities and improve computational time the beach slope 1 20 fore reef slope 1 5 and side slopes 1 5 are held constant with adopted values typical to many fringing reef systems and the offshore water depth is arbitrarily set to 20 m a wide range of values table 1 are adopted for the width w 100 1000 m length l 200 1000 m and depth d 0 5 3 0 m of the reef flat and combined to generate a suite of 616 bathymetry configurations for the shoreline modelling the bay width is an additional parameter yielding 6776 configurations overall the adopted reef width values of up to 1000 m are quite common in nature quataert et al 2015 reef length is limited to 1000 m in the present study since the effect of reef length reduces once reef length exceeds the reef width and longer lengths significantly increase computation time a base case scenario with reef geometry w 400m l 400m and d 0 5m is adopted to illustrate the effects of different wave conditions or when showing results in more detail 2 2 model parameter space a summary of the wave conditions is given in table 2 which includes a large range of wave conditions from swell to storm five different wave directions and five different magnitudes of sea level rise which yields 1000 combinations or 616 000 possible combinations of reef geometry and wave conditions for the swan modelling note that the effects of sea level rise are determined by comparing model results for different depths but otherwise identical parameters the wave transformation was modelled on the hpc at the university of queensland two examples of the modelled hydrodynamics are presented in fig 3 which contrasts the wave height over the reef and the refraction patterns for two different water depths over the base case reef the wave height at the beach toe is much greater for the larger water depth but the wave angles are significantly smaller for the classical equilibrium shoreline configuration model chosen here the final shoreline planform is controlled by the wave angle at the beach toe when waves break on the reef or at the breakpoint on the beach face which occurs with deep reefs and very small offshore waves 3 shoreline planform configuration and relative position 3 1 model concept wave breaking and refraction over fringing reefs change the wave angles reaching the shoreline for finite length reefs waves approach the beach from both ends of the reef leading to a convergence zone of sediment transport and the formation of a salient salients also form with oblique waves as a result of smaller rates of longshore transport in the lee of the reef in both cases the planform shoreline shape is expected to evolve to a quasi steady condition which occurs when longshore gradients in sediment transport become zero gonzalez and medina 2001 this occurs when the wave arrives normal to the shoreline or so that the equilibrium shoreline at any point is perpendicular to the local wave direction at the shore which can be observed in fig 1a elshinnawy et al 2017 using the cerc formula for longshore sediment transport wang et al 1998 the net result is zero longshore sediment transport or a beach in static equilibrium with the incident waves this process results in the classical shoreline alignment and crenulated bay shapes observed due to refraction around headlands and within pocket beaches hsu et al 1989 perturbations to the wave climate lead to changes in the shoreline shape or a dynamic equilibrium planform elshinnawy et al 2018 lee and hsu 2017 with the same approach adopted to study the rotation of pocket beaches turki et al 2013 reguero et al 2018 adopted the same classical equilibrium beach shape concept to show that nearshore reefs often control the planform shape of the adjacent beaches the same approach is adopted to determine the planform shoreline configuration behind the idealised fringing reefs modelled in this study i e the shoreline evolves to a shape perpendicular to the calculated wave angles at the beach toe this condition is combined with the requirement for continuity of sediment volume within the coastal embayment to determine the shoreline position outside of the zone influenced by the reef where waves remain at the original offshore wave angle the bay width therefore influences the salient width to some extent leading to a total of 6 776 000 different combinations of waves bathymetry and bay width the physical processes leading to salient formation behind a fringing reef are somewhat analogous to those for salient formed behind submerged offshore breakwaters ranasinghe and turner 2006 with the difference that offshore breakwaters are much narrower in width and generally transmit less wave energy so that diffraction rather than refraction is the dominant process 3 2 salient shape and width examples from the model of the resulting shoreline planform configuration for different depths over the reef flat are illustrated in fig 4 which matches closely with the shapes of the natural salients shown in fig 1 for given wave conditions the salient width reduces as the reef flat depth increases since less refraction and less breaking occurs with deeper water over the reef this means the salient will erode if the reef flat elevation reduces due to reef degradation or the water depth increases due to sea level rise as illustrated in fig 1 the salient may build out across part of the fringing reef flat which would lead to interaction between the waves on the reef flat and the evolving morphology this is not considered in the present modelling since a coupled hydrodynamic geomorphology model would be required with iteration also needed until a stable configuration was obtained not only does such a verified model not exist but the computation times would be impractical for the current parametric study hence the wave angles used in the shoreline model are independent of the salient geometry and set solely by the breaking and refraction process over the reef it is noted that the static equilibrium beach concept assumes that the mean overall wave climate is constant for sufficient time for the beach to evolve to the stable shape while the long term wave climate is relatively stable at most locations seasonal changes in wave height and wave direction are expected which means that any beach is not in static equilibrium but is in a state of dynamic equilibrium silvester and hsu 1997 always evolving over time however the presence of permanent salients over many years suggests that these transient perturbations in the short term wave climate do not change the overall salient characteristics 3 3 steric sea level rise in addition to a reduction in salient width with greater water depth over the reef flat reef degradation or sea level rise the shoreline may additionally recede due to sea level rise itself i e a change in the static water level this assumes no additional sediment supply from the reef no berm growth or berm rollover and no other changes in beach morphology in conjunction with the rising sea level atkinson et al 2018 cowell and kench 2001 woodroffe 2008 and indeed reef islands may well accrete due to sea level rise webb and kench 2010 nevertheless in the absence of counterbalancing geomorphological processes the magnitude of the recession is the sea level rise divided by the average gradient of the beach profile between the current shoreline and a beach contour at the new sea level leatherman and beller simms 1997 an average beach gradient of 1 20 is adopted here representative of typical reef fringed sandy beaches where salients are likely to form 3 4 shoreline position and recession for different geometry and relative sea level the recession at the centreline of the salient as a function of the drop in reef flat elevation relative to mean sea level is illustrated in fig 5 for the base case scenario in this particular case the contribution to the recession of the shoreline due to a higher water depth over the reef is very similar to that from the static component of sea level rise i e the total recession doubles compared to that expected on an open coast beach of the same average slope the extra recession occurs from erosion of the salient however in general the relative contribution from the erosion of the salient varies with the wave conditions and the reef width and reef length fig 6 for a given change in water depth over the reef pseudo sea level rise the largest reduction in salient width occurs with long wave periods and small wave heights fig 6 a since the refraction and breaking patterns for these wave conditions change significantly with changes in depth over the reef changes in breaking and refraction patterns are smaller for larger waves and shorter wave periods leading to reduced changes in salient width and less erosion for those wave conditions the degree of erosion of the salient also depends strongly on the reef geometry and increases with both reef width and reef length fig 6b however for the particular wave conditions shown the influence of reef length and reef width cease once the reef length and width exceed approximately 700m in fact once the reef length exceeds the reef width then the magnitude of the shoreline change due to a water depth change remains relatively constant with further increases in reef length likewise the magnitude of the shoreline change tends to remain constant once the reef width exceeds the reef length particularly for shorter reefs each combination of wave condition reef geometry and water depth 616 000 combinations generates a salient of a specific maximum width at the reef centreline e g fig 5 taking a subset of such combinations the influence of a change in one parameter can be determined graphically from fig 7 for example for h l w d dir 2m 600m 600m 1m 0 the maximum salient width is approximately 130m a change in water depth to 1 5m reduces this width to about 110m and a change in depth to 2m results in a further decrease in width to about 90m the same approach can be used to determine the changes in salient width if wave heights change similar panel plots could be produced for wave period and wave direction however such an approach rapidly becomes cumbersome given the large number of possible combinations and is not intuitive for non expert end users to address this issue the ccres project investigated the use of bbn as to effectively communicate the results of parametric modelling of these complex processes for use in marine spatial planning activities callaghan et al 2017 2018 with the bbn models designed and refined in conjunction with workshops held in the philippines indonesia and australia including other bbn for ecosystem services and fisheries this approach is illustrated in the following section for the current data set the influence of wave direction is illustrated in fig 8 for shallow reefs an approximately symmetrical salient remains centred around the reef centreline since there is sufficient refraction to make the wave direction approximately shore normal at the centreline of the reef at the beach toe however with a pseudo sea level rise of 1 5m e g sea level rise of 0 5m and a loss of reef flat elevation of 1m refraction is very significantly reduced and the whole salient rotates to align with the new nearshore wave direction this may result in very large changes in shoreline position which varies alongshore behind the reef leading to regions of significant erosion and accretion as the sediment is redistributed from the re aligning salient 4 bayesian belief network 4 1 model approach callaghan et al 2017 2018 compared two forms of network for a bbn firstly a network based on a physical understanding of the process with output from some nodes leading to input to other output nodes depending on understood expert knowledge relationships secondly a simple all input nodes to one output node network which made no attempt to link variables and processes surprisingly the second model was more accurate and indeed statistically indistinguishable from values obtained directly from the dataset despite being many orders of magnitude smaller in terms of data storage consequently simple all input nodes to one output node networks are adopted for the different bbn built using this dataset corresponding to 1 680 000 records which has order 16 times more parameters than that of callaghan et al 2017 2018 to keep the network structure simple for non expert end users to ensure no interaction between output nodes pearl 1988 and to reduce the network sizes so that they can be used in limited mode in netica no licence required one bbn network is built for each required output e g wave height wave induced velocity salient width force on coral etc we show that this approach is more accurate than bbn with multiple output nodes since feedback can occur within the bbn when the number of training cases is limited the models are built in netica and are sufficiently compact so that they work with the free open access version of netica the bbn use the approach described in detail by callaghan et al 2018 which was also the same approach adopted by pearson et al 2017 summarised briefly here for the present bbn input and output parameters are defined in table 3 the bbn input nodes match those of the wave model which are uniform distributions as default since in many instances detailed knowledge of the wave climate may not be available but users can select a discrete input range within the bbn interface if that is preferred if output for a different known input distribution is required users can enter that distribution in netica no model re training is required and the model will propagate that distribution through the directed acyclic graph dag to all other nodes 4 2 variables with a case for every entry in the conditional probability table the learning algorithms tested by callaghan et al 2018 where there was a training case for every entry in the conditional probability table output node of the bbn indicated that the counting learning algorithm applied with a degree of 106 resulted in an output node distribution that was identical to that obtained from the training cases the same accuracy was achieved here as the training cases for hav h05 uav u05 fav and f05 have a case for every entry in the conditional probability table and consequently the bbn for these variables are identical to the probabilities estimated from the wave prediction database tables 4 6 with the results identical to 3 significant figures additional quantitative comparisons of probabilities where each node was instantiated for each discrete range in turn and compared to probabilities estimated directly from the wave prediction database i e repeating callaghan et al 2018 fig 7 confirmed exact predictions by each of these bbn fig 9 for these hydrodynamic variables it is possible to construct a single bbn with all six of these variables in a single network and still obtain exact predictions this is because there is a training case for every entry in the conditional probability table of the bbn and the two way information flow possible through the network from one output to another output has no influence on the predicted probabilities however we found that such a network was too congested for ease of use by non expert stakeholders with multiple and crossing arrows confusing the link between the input variables and the output hence we constructed different networks for each variable of interest while this requires input of the same data more than once to obtain h05 u05 and f05 for example end users indicated that simpler networks were better during in country training workshops associated with the ccres project 4 3 variables without a case for every entry in the conditional probability table for the variables describing the salient width at the reef centreline and quarter reef lengths either side xs5 xs3 and xs7 the all input nodes to one output node bbn see fig 14 for each of these variables have training cases for only 91 of entries in the conditional probability table output node this is because certain wave angles do not yield valid estimates for longshore transport that control the planform evolution of the shoreline and these conditions are discarded from the modelling this leads to minor differences for the trained output node distributions tables 7 9 and minor difference in the estimated bbn probabilities compared to those estimated directly from the wave prediction database fig 10 however the bbn probabilities are generally well within 5 of the actual wave prediction database probabilities which we regard as acceptable given the assumptions and accuracy of the wave and shoreline modelling for these shoreline variables there is feedback within a single network if that network contains all the output nodes and therefore the small differences in the predicted and actual probabilities become compounded this means a single network that contains simultaneous output for xs3 xs5 and xs7 is not desirable since the bbn probabilities are not the same as for each variable in a single network to illustrate the differences that arise with linked output nodes we built an expert knowledge based bbn to link the shoreline variables xs3 xs5 and xs7 within one network since these all describe different widths of the salient alongshore with the data also tabulated in figs 7 9 one such example network is shown in fig 11 a with the accuracy of the bbn probabilities compared with the true values in fig 11b using the same methodology as that for figs 9 and 10 clearly the expert knowledge bbn performs poorly compared to the all input to one output node bbn for these same variables fig 10 as expected from callaghan et al 2018 this is because a bbn that links the different predictions along the shore results in incorrect predictions due to feedback back through the network between the different shoreline positions thus a model that links these three variables together is not as accurate as models where there is only one output as discussed above for the hydrodynamic bbn there is a learning case for every combination of the conditional probability table consequently no more expert knowledge or assumptions about the network structure is required in the case of the shoreline position bbn 91 of the conditional probability tables are covered by the learning cases and output accuracy is within 5 hence the scope for including further expert knowledge is marginal at best callaghan et al 2017 also found that once the majority of the conditional probability table is defined by learning cases the expert knowledge in the form of a network linking physical processes together actually reduced model accuracy since it introduces approximate relationships not necessarily included in the models therefore we do not combine or update the data inferred bbn with additional expert knowledge consequently to maintain simplicity for end users to avoid end users needing to understand how more complex bbn might include unknown feedback processes and to maximise model accuracy we adopt single output models this also keeps the network structure simple and transparent for non expert end users input data does have to be repeated to obtain predictions for the variation in salient width with distance along the shore spatial variation in width but for the purposes and primary end users for the ccres models this is preferable to a network with multiple output nodes it is trivial for end users to combine and interpret results for the different variables if needed 4 4 example bbn for wave height and salient width two examples for wave height and salient width are illustrated in figs 12 and 14 respectively the file sizes for these two networks are 95 kb and 1 3 mb respectively which are small and quick and the bbn do not require any significant computer science skills to operate or use other bbn for wave induced velocity wave induced forces on different coral species table 3 together with a link to the free netica software a user guide and a training presentation giving an overview of the key coastal processes involved are available on the capturing coral reef ecosystem services ccres project website https ccres net the bbn network inputs are the bay width bayw reef width w reef length l water depth over the reef flat d wave height h wave period t and wave direction dir coral diameter is an additional input parameter for a further bbn that estimates wave forces on different species of coral following the approach of baldock et al 2014 2014b the two output nodes presented here are hav m which is the average wave height over the whole of the reef top and xs5 m which is the shoreline position at the centreline of the reef and also the maximum salient width it is straightforward to use the bbn networks see callaghan et al 2017 2018 or pearson et al 2017 for further examples of the application of similar models input values for a particular reef are chosen based on known information or left as unknowns using a uniform or chosen distribution with the mean value and standard deviation of the output parameter immediately displayed in the centre output node means and the range are displayed since the bbn has subsumed the complete dataset into discrete ranges of the input variables given that idealised reef geometry is used such that reef width and length are subjective to user interpretation and mean water depth is not usually known to a precision better than 0 2 m at best calculation of single value for the output is not justified nor needed as an example fig 12 gives the average significant wave height hav over the reef top as 0 64m with a range of 0 54 0 74m for incident waves with height 2m a period of 7s and a mean direction of 10 this compares to the exact value from the dataset of 0 54m so the prediction is within the 100 confidence band varying the input parameters allows users to investigate the influence of reef width reef length etc or to determine the change in reef flat wave conditions if the water depth or incident wave conditions change 4 5 application to a real case application to a real case is demonstrated in figs 13 and 14 the geometry of a salient formed by a fringing reef at el nido philippines fig 13 is estimated from google earth being w 400m l 800m bayw 3500m a reasonable estimate of the mean water depth is 0 75m based on visual observations of minor wave breaking with incident waves with height of order 0 5m during a site visit with the assumption that the salient is predominantly a result of long period swell approaching from a small angle in this case dir 10 deg and a typical wave height h and wave period t of 1 5m and 9s respectively the bbn estimates the expected salient width at the centreline to be xs5 195m with a standard deviation of 49m fig 14 left panel the width of the actual salient is estimated to be approximately 200m in good agreement with the model prediction keeping all conditions constant except the water depth over the reef which is raised to 1 5m representing a combined loss of reef elevation and or sea level rise of 0 75m the expected width of the salient is reduced to 97m fig 14 right panel i e a landward recession of the shoreline of approximately 100m or 115m if the steric recession is included for a typical beach gradient of 1 20 this degree of recession due to the erosion of the salient is equivalent to a 5m static sea level rise consequently the loss of reef elevation or small amounts of sea level rise can lead to very severe beach erosion in comparison to that expected on open coast beaches under normal sea level rise conditions similarly a change in the mean wave climate direction of order 10 leads to a rotation of the salient fig 8 reducing the salient width at certain locations clearly if community or tourism infrastructure were located close to the shoreline then impacts would be severe finally the bbn also allow estimates of input conditions that lead to given output scenario i e the bbn can run in reverse for what if scenario planning for example for a given reef geometry and wave conditions the salient width can be chosen to determine what minimum water depth is required to maintain a given salient width 5 conclusions the response of the shoreline in the lee of fringing reefs subject to climate changes and reef degradation has been investigated using numerical modelling to assess the impact of sea level rise changes in wave climate and loss of reef elevation the swan hydrodynamic wave model and classical theory describing the planform of beaches in equilibrium with the wave forcing have been combined to estimate the shape and size of salients formed in the lee of the reefs while the shoreline model relies on the accuracy of the wave model swan has been extensively verified for a wide range of different conditions and coastal bathymetry the model results are consistent with natural salients in el nido philippines and ningaloo reef western australia simulations are performed for over 6 000 000 different combinations of reef bathymetry wave conditions and bay width bayesian belief networks bbn were then developed to enable easy user access to the large database of model results using the bbn changes in salient width or shape can then be estimated for different or new conditions e g deeper water depth over the reef or a shift in wave direction using the suite of model results the modelling shows that salients may either accrete or erode under changing scenario with potential loss of the salient and high rates of shoreline erosion a number of different open access bbn with high accuracy and simple user interfaces have been built to assist communication of the complex and extensive dataset to end users simple networks with only one output node are used for ease of use by non expert stakeholders and to avoid inaccuracies induced by interactions between multiple output nodes we show that bbn models of this form reproduce exactly the actual probabilities in the training set when there is a case for every entry in the conditional probability table the bbn enables access to all the model results and easy comparison of different scenario allowing end users to quickly assess how changes in the wave climate or reef elevation are likely to influence the shoreline behind a reef the modelling shows that recession of the shoreline in the lee of fringing reefs due to sea level rise may be much greater than that expected on open coast beaches thus the presence of the reef potentially makes the coastline more sensitive to changes in wave climate or sea level similarly loss of reef flat elevation from coral loss can lead to significant erosion of the shoreline behind the reef the modelling suggests that many reef protected beaches are vulnerable to the effects of sea level rise and degradation of the associated fringing reefs because of the potential for significant changes in shoreline position as the relative elevation of the reef flat changes software and or data availability section physics based wave model predictions using swan 40 85 www swan tudelft nl bbn modelling using netica 4 16 www norsys com acknowledgements this work was funded by the world bank gef the university of queensland australia project entitled capturing coral reef ecosystems services ccres and australian research council grant dp dp14010130 the high performance computing was supported by queensland cyber infrastructure foundation australia and the university of queensland b s acknowledges the university of queensland grant rm 2014001465 co funded by global change institute the university of queensland australia 
26182,reef protected beaches are vulnerable to the effects of sea level rise and degradation of their associated fringing reefs the swan hydrodynamic wave model is combined with classical theory describing the planform of beaches in equilibrium with the wave forcing to estimate the reef top hydrodynamics and the shoreline configuration in the lee of the reefs open access bayesian belief networks with high accuracy and simple user interfaces have been built to communicate the results the bbn enable end users to access all the model results and to compare different scenario to determine how changes in the wave climate or reef elevation change the shoreline configuration the results show that recession of the shoreline in the lee of fringing reefs due to sea level rise may be much greater than that expected on open coast beaches loss of reef flat elevation can also lead to severe shoreline erosion keywords bayesian belief networks reefs coastal morphology climate change communication tools 1 introduction fringing reefs protect communities and beaches from wave action under extreme and normal wave conditions providing an ecosystem service ferrario et al 2014 one result of the presence of such reefs is the formation of salients bulges at the shoreline in the lee of the reef the salient provides a buffer of sand to the beach or generates larger scale morphological features which may become vegetated over time fig 1 in general reefs act as controls on the shoreline shape gonzalez and medina 2001 and changes in reef structure can lead to changes in shoreline position de alegria arzaburu et al 2013 reguero et al 2018 for finite lengths of fringing reefs the geometry of the salient is dependent on the geometry of the reef the water depth over the reef flat and the offshore wave conditions the shoreline shape and the width of the salient in the cross shore direction is the result of complex processes of wave breaking and refraction over the reef therefore changes in offshore wave climate either wave height or direction and changes in reef elevation or roughness lead to changes in the salient size and shape leading to beach erosion and potentially the loss of the salient and the protection afforded to the shoreline behind changes in reef flat elevation relative to sea level are particularly important since the depth over the reef flat is the dominant factor controlling the wave conditions at the landward edge of the reef hardy and young 1996 gourlay 1994 sheppard et al 2005 lowe et al 2005 as the salient erodes sediment will be redistributed along the adjacent beach or transported downdrift fig 1 illustrates two examples of salients formed behind coral reefs at el nido philippines and ningaloo reef wa australia similar salients can be observed in the lee of rocky reefs wave transformation over fringing reefs have been extensively studied and the physical processes are well known being dominated by refraction and breaking processes with friction generally playing a minor role once waves are breaking except on very rough reefs monismith 2007 pearson et al 2017 wave refraction and the resulting wave direction at the shoreline is also very important in controlling the shoreline shape and beach planform kench and brander 2006 gourlay 1988 in addition to sediment retention on reef platforms mandlier and kench 2012 the details of the wave transformation processes for the present parametric study are discussed in depth in baldock et al 2019 in review and show a complex behaviour dependent on the relative importance of refraction and wave breaking fig 3 the shoreline shape behind finite lengths of reef and the impact of sea level rise or reef degradation has been less well documented and is more complex but due to the increased wave energy and changes in reef flat hydrodynamics significant changes in sediment transport and shoreline position are possible storlazzi et al 2011 baldock et al 2015 reguero et al 2018 grady et al 2013 while process modelling of coastal morphology is a complex task and therefore subject to uncertainty over time scales of evolution process based models almost universally adopt an evolution to equilibrium type transport approach where for constant forcing the plan form evolves so that longshore transport gradients and changes in planform tend to zero over time de vriend et al 1993 the uncertainty over timescales can be avoided by using the widely adopted classical equilibrium planform modelling approach where the shoreline orientation is based on the direction of the mean wave energy flux acting on the coastline elshinnawy et al 2017 jackson and cooper 2010 then long term morphological dynamics result from changes that equilibrium condition that can be described by a relationship between hydrodynamics and morphology van maanen et al 2016 and this is the approach adopted here the complexity of the wave transformation processes and resulting shape of the shoreline behind the reef means that the results are not easily communicated to end users and coastal managers particularly in developing countries and remote communities open access bayesian belief networks with high accuracy and simple user interfaces have been built to address this issue the model enables users to access all the model results and compare different scenario to determine how changes in the wave climate or reef elevation influences shoreline erosion behind the reef bayesian belief networks henceforth bbn have been widely utilised to study complex physical and biological processes recent applications to coastal processes include predicting coastal vulnerability to sea level rise barrier island geomorphology and flood hazards on reef protected beaches gutierrez et al 2011 2015 pearson et al 2017 in the present context callaghan et al 2017 2018 show how a simple bbn framework provides near perfect accuracy in synthesising the results of a multi parameter model database baldock et al 2014 for one dimensional i e shore normal wave transformation across a barrier reef and lagoon system in that case both the number of parameters describing the reef geomorphology and the wave climate were considerably smaller than those required for modelling two dimensional wave propagation across a fringing reef and the resulting shoreline shape however the same simple bbn framework can be adopted which is more transparent to users since no expert knowledge of the physical processes is required to understand the network structure further since detailed knowledge of the bathymetry and physical properties of most reefs to be studied is not always well known the bbn intrinsically enables users to account for uncertainty or missing information the present paper considers these issues and presents a number of simple bbn to communicate the results of a parametric study of wave transformation over fringing reefs and the estimated shoreline shape that required hundreds of hours of high performance computing to model the hydrodynamics and shoreline evolution to do this the approach of baldock et al 2014 2015 is followed who investigated one dimensional wave transformation over a very large range of idealised barrier reef lagoon geometry using a parametric wave transformation model the model results were then used to assess the impacts of changes in water level on wave conditions flow velocity and wave forces on different coral species in that work the results were presented graphically which limits resolution and limits access by non expert end users pearson et al 2017 adopted a similar approach and constructed a similar bbn to that of callaghan et al 2017 using a wave resolving wave model to synthesise the propagation and run up of waves over a suite of idealised fringing reefs in that study computational requirements again limited the study to one dimensional wave propagation pearson et al 2017 also note that a bbn of this form provides a useful 1st order approach which is appropriate when only relatively low resolution remotely sensed data is available here a two dimensional 2 d wave model is combined with a classical equilibrium shape planform model to determine the hydrodynamics and expected shoreline configuration for a very large range of idealised geometry and wave conditions representing wave propagation across finite length fringing reefs the bbn models were designed during workshops held in the philippines indonesia and australia during the capturing coral reef ecosystem services project https ccres net with the bbn user manuals and user guides to the coastal processes refined with stakeholder and end user input feedback we considered a number of different bbn for two reasons firstly transparency and simplicity for end users assessed through iterative design in ccres workshops secondly during this process we found that accuracy of the bbn depends on the network structure and here we show that networks constructed on the basis of expert knowledge were not in fact optimal in terms of accuracy we note that the shoreline planform model depends on the swan model and thus any errors in the wave modelling propagate into the bbn results while this is a limitation the results of many environmental models require forcing derived from other models most notably models that provide or simulate environmental parameters however swan has been demonstrated to provide an accurate representation of the hydrodynamics over coral reefs storlazzi et al 2011 buckley et al 2014 and it is the most widely verified nearshore wave hydrodynamics model available and therefore we have confidence the results are sufficiently accurate to apply for the present purposes we show that the final bbn constructed and tested provide perfect reproduction of the underlying swan model results for the hydrodynamics and are generally within 5 of the underlying shoreline results for the planform configuration hence there is no significant model of model effect i e use of the bbn does not degrade the model accuracy the bbn then allow simple and fast comparison of key parameters that characterise the hydrodynamics and shoreline configuration for different reef geometry and wave conditions enabling changes in parameters due to slr loss of reef flat elevation or changes in wave climate to be easily assessed the purpose of the bbn is to provide a simple and quick generic tool to assess how different reefs and particularly how changes to wave and water level conditions on those reefs influence the hydrodynamics over the reef and the shoreline planform behind the reef the effectiveness of different reefs in stabilising the shoreline under future scenario can then be assessed to first order alternatively the reefs for which the shoreline configuration is most sensitive to changes in wave climate water level or reef degradation can be identified for more detailed further study 2 hydrodynamic modelling the hydrodynamic modelling uses the swan booij et al 1999 model which is a parametric wave averaged model widely used for wind and swell wave propagation in coastal regions and which has been extensively verified for propagation over reefs for very similar scenario to those considered here storlazzi et al 2011 grady et al 2013 the use of the swan model in this context is discussed at length in baldock et al 2014 2015 and other than the refraction processes occurring for these 2 d scenario there is no significant difference in the modelling approach buckley et al 2014 found swan to be as accurate as wave resolving or time domain models in predicting sea and sell wave heights across fringing reeds using default model parameters swan does not include long waves which are important in enhancing wave run up in extreme conditions however the shoreline configuration model is not dependent on long waves and sediment transport and planform evolution modelling by conventional coastal engineering modelling approaches does not include long waves in part this is due to the additional time dependence required in the modelling significantly increasing computation time but largely because the influence of long waves on sediment transport processes and beach evolution is not sufficiently well known to warrant inclusion of long waves even for cross shore 1 d processes only baldock et al 2011 likewise circulation patterns currents are not included in the swan model which are strongly controlled by lagoon and channel geometry rather than the morphology of the reef lowe et al 2010 inclusion of such circulation would increase computation time very significantly and again is not warranted since nearshore currents are not used in classical equilibrium beach planform morphodynamic modelling dissipation of the majority of the incident wave energy over reefs as estimated by the swan model is dominated by breaking processes with bottom friction relatively less important except for lower energy conditions very rough reefs and deeper water depths baldock et al 2014 similar results are found with wave resolving models harris et al 2018 and in the field costa et al 2016 in addition if the depth over the reef crest reduces toward the shore breaking and dissipation will eventually occur reducing the importance of roughness in controlling the nearshore wave conditions while baldock et al 2015 found that for horizontal reef flats reef roughness influenced likely cross shore sediment transport patterns behind barrier reefs i e loss of rugosity increased vulnerability the influence of rugosity on wave direction is more limited since wave refraction is controlled by the depth we also use an equilibrium model for the shoreline planform shape which is independent of wave height hence changes in rugosity have little influence on nearshore wave directions and the shoreline evolution further the assumptions in both the shoreline modelling and the swan modelling make any confidence in comparing the shoreline configuration for otherwise identical rough and smooth reefs subject to question consequently we use smooth reefs for this modelling since degrading reefs are more likely to have lost coral structure 2 1 reef geometry the idealised 3 d fringing reef geometry is illustrated in fig 2 the most important parameters controlling the reef flat and nearshore hydrodynamics are the wave height h wave period t reef width w and water depth d pearson et al 2017 baldock et al 2014 the important parameters controlling the shoreline evolution are the water depth reef length l and the wave direction dir plus the resulting refraction and wave breaking processes on the reef while natural reef geometries are very variable a rectangular reef is simpler for an idealised parametric study and a natural extension of previous 1 d modelling sharp edges were gently rounded to avoid potential numerical instabilities and improve computational time the beach slope 1 20 fore reef slope 1 5 and side slopes 1 5 are held constant with adopted values typical to many fringing reef systems and the offshore water depth is arbitrarily set to 20 m a wide range of values table 1 are adopted for the width w 100 1000 m length l 200 1000 m and depth d 0 5 3 0 m of the reef flat and combined to generate a suite of 616 bathymetry configurations for the shoreline modelling the bay width is an additional parameter yielding 6776 configurations overall the adopted reef width values of up to 1000 m are quite common in nature quataert et al 2015 reef length is limited to 1000 m in the present study since the effect of reef length reduces once reef length exceeds the reef width and longer lengths significantly increase computation time a base case scenario with reef geometry w 400m l 400m and d 0 5m is adopted to illustrate the effects of different wave conditions or when showing results in more detail 2 2 model parameter space a summary of the wave conditions is given in table 2 which includes a large range of wave conditions from swell to storm five different wave directions and five different magnitudes of sea level rise which yields 1000 combinations or 616 000 possible combinations of reef geometry and wave conditions for the swan modelling note that the effects of sea level rise are determined by comparing model results for different depths but otherwise identical parameters the wave transformation was modelled on the hpc at the university of queensland two examples of the modelled hydrodynamics are presented in fig 3 which contrasts the wave height over the reef and the refraction patterns for two different water depths over the base case reef the wave height at the beach toe is much greater for the larger water depth but the wave angles are significantly smaller for the classical equilibrium shoreline configuration model chosen here the final shoreline planform is controlled by the wave angle at the beach toe when waves break on the reef or at the breakpoint on the beach face which occurs with deep reefs and very small offshore waves 3 shoreline planform configuration and relative position 3 1 model concept wave breaking and refraction over fringing reefs change the wave angles reaching the shoreline for finite length reefs waves approach the beach from both ends of the reef leading to a convergence zone of sediment transport and the formation of a salient salients also form with oblique waves as a result of smaller rates of longshore transport in the lee of the reef in both cases the planform shoreline shape is expected to evolve to a quasi steady condition which occurs when longshore gradients in sediment transport become zero gonzalez and medina 2001 this occurs when the wave arrives normal to the shoreline or so that the equilibrium shoreline at any point is perpendicular to the local wave direction at the shore which can be observed in fig 1a elshinnawy et al 2017 using the cerc formula for longshore sediment transport wang et al 1998 the net result is zero longshore sediment transport or a beach in static equilibrium with the incident waves this process results in the classical shoreline alignment and crenulated bay shapes observed due to refraction around headlands and within pocket beaches hsu et al 1989 perturbations to the wave climate lead to changes in the shoreline shape or a dynamic equilibrium planform elshinnawy et al 2018 lee and hsu 2017 with the same approach adopted to study the rotation of pocket beaches turki et al 2013 reguero et al 2018 adopted the same classical equilibrium beach shape concept to show that nearshore reefs often control the planform shape of the adjacent beaches the same approach is adopted to determine the planform shoreline configuration behind the idealised fringing reefs modelled in this study i e the shoreline evolves to a shape perpendicular to the calculated wave angles at the beach toe this condition is combined with the requirement for continuity of sediment volume within the coastal embayment to determine the shoreline position outside of the zone influenced by the reef where waves remain at the original offshore wave angle the bay width therefore influences the salient width to some extent leading to a total of 6 776 000 different combinations of waves bathymetry and bay width the physical processes leading to salient formation behind a fringing reef are somewhat analogous to those for salient formed behind submerged offshore breakwaters ranasinghe and turner 2006 with the difference that offshore breakwaters are much narrower in width and generally transmit less wave energy so that diffraction rather than refraction is the dominant process 3 2 salient shape and width examples from the model of the resulting shoreline planform configuration for different depths over the reef flat are illustrated in fig 4 which matches closely with the shapes of the natural salients shown in fig 1 for given wave conditions the salient width reduces as the reef flat depth increases since less refraction and less breaking occurs with deeper water over the reef this means the salient will erode if the reef flat elevation reduces due to reef degradation or the water depth increases due to sea level rise as illustrated in fig 1 the salient may build out across part of the fringing reef flat which would lead to interaction between the waves on the reef flat and the evolving morphology this is not considered in the present modelling since a coupled hydrodynamic geomorphology model would be required with iteration also needed until a stable configuration was obtained not only does such a verified model not exist but the computation times would be impractical for the current parametric study hence the wave angles used in the shoreline model are independent of the salient geometry and set solely by the breaking and refraction process over the reef it is noted that the static equilibrium beach concept assumes that the mean overall wave climate is constant for sufficient time for the beach to evolve to the stable shape while the long term wave climate is relatively stable at most locations seasonal changes in wave height and wave direction are expected which means that any beach is not in static equilibrium but is in a state of dynamic equilibrium silvester and hsu 1997 always evolving over time however the presence of permanent salients over many years suggests that these transient perturbations in the short term wave climate do not change the overall salient characteristics 3 3 steric sea level rise in addition to a reduction in salient width with greater water depth over the reef flat reef degradation or sea level rise the shoreline may additionally recede due to sea level rise itself i e a change in the static water level this assumes no additional sediment supply from the reef no berm growth or berm rollover and no other changes in beach morphology in conjunction with the rising sea level atkinson et al 2018 cowell and kench 2001 woodroffe 2008 and indeed reef islands may well accrete due to sea level rise webb and kench 2010 nevertheless in the absence of counterbalancing geomorphological processes the magnitude of the recession is the sea level rise divided by the average gradient of the beach profile between the current shoreline and a beach contour at the new sea level leatherman and beller simms 1997 an average beach gradient of 1 20 is adopted here representative of typical reef fringed sandy beaches where salients are likely to form 3 4 shoreline position and recession for different geometry and relative sea level the recession at the centreline of the salient as a function of the drop in reef flat elevation relative to mean sea level is illustrated in fig 5 for the base case scenario in this particular case the contribution to the recession of the shoreline due to a higher water depth over the reef is very similar to that from the static component of sea level rise i e the total recession doubles compared to that expected on an open coast beach of the same average slope the extra recession occurs from erosion of the salient however in general the relative contribution from the erosion of the salient varies with the wave conditions and the reef width and reef length fig 6 for a given change in water depth over the reef pseudo sea level rise the largest reduction in salient width occurs with long wave periods and small wave heights fig 6 a since the refraction and breaking patterns for these wave conditions change significantly with changes in depth over the reef changes in breaking and refraction patterns are smaller for larger waves and shorter wave periods leading to reduced changes in salient width and less erosion for those wave conditions the degree of erosion of the salient also depends strongly on the reef geometry and increases with both reef width and reef length fig 6b however for the particular wave conditions shown the influence of reef length and reef width cease once the reef length and width exceed approximately 700m in fact once the reef length exceeds the reef width then the magnitude of the shoreline change due to a water depth change remains relatively constant with further increases in reef length likewise the magnitude of the shoreline change tends to remain constant once the reef width exceeds the reef length particularly for shorter reefs each combination of wave condition reef geometry and water depth 616 000 combinations generates a salient of a specific maximum width at the reef centreline e g fig 5 taking a subset of such combinations the influence of a change in one parameter can be determined graphically from fig 7 for example for h l w d dir 2m 600m 600m 1m 0 the maximum salient width is approximately 130m a change in water depth to 1 5m reduces this width to about 110m and a change in depth to 2m results in a further decrease in width to about 90m the same approach can be used to determine the changes in salient width if wave heights change similar panel plots could be produced for wave period and wave direction however such an approach rapidly becomes cumbersome given the large number of possible combinations and is not intuitive for non expert end users to address this issue the ccres project investigated the use of bbn as to effectively communicate the results of parametric modelling of these complex processes for use in marine spatial planning activities callaghan et al 2017 2018 with the bbn models designed and refined in conjunction with workshops held in the philippines indonesia and australia including other bbn for ecosystem services and fisheries this approach is illustrated in the following section for the current data set the influence of wave direction is illustrated in fig 8 for shallow reefs an approximately symmetrical salient remains centred around the reef centreline since there is sufficient refraction to make the wave direction approximately shore normal at the centreline of the reef at the beach toe however with a pseudo sea level rise of 1 5m e g sea level rise of 0 5m and a loss of reef flat elevation of 1m refraction is very significantly reduced and the whole salient rotates to align with the new nearshore wave direction this may result in very large changes in shoreline position which varies alongshore behind the reef leading to regions of significant erosion and accretion as the sediment is redistributed from the re aligning salient 4 bayesian belief network 4 1 model approach callaghan et al 2017 2018 compared two forms of network for a bbn firstly a network based on a physical understanding of the process with output from some nodes leading to input to other output nodes depending on understood expert knowledge relationships secondly a simple all input nodes to one output node network which made no attempt to link variables and processes surprisingly the second model was more accurate and indeed statistically indistinguishable from values obtained directly from the dataset despite being many orders of magnitude smaller in terms of data storage consequently simple all input nodes to one output node networks are adopted for the different bbn built using this dataset corresponding to 1 680 000 records which has order 16 times more parameters than that of callaghan et al 2017 2018 to keep the network structure simple for non expert end users to ensure no interaction between output nodes pearl 1988 and to reduce the network sizes so that they can be used in limited mode in netica no licence required one bbn network is built for each required output e g wave height wave induced velocity salient width force on coral etc we show that this approach is more accurate than bbn with multiple output nodes since feedback can occur within the bbn when the number of training cases is limited the models are built in netica and are sufficiently compact so that they work with the free open access version of netica the bbn use the approach described in detail by callaghan et al 2018 which was also the same approach adopted by pearson et al 2017 summarised briefly here for the present bbn input and output parameters are defined in table 3 the bbn input nodes match those of the wave model which are uniform distributions as default since in many instances detailed knowledge of the wave climate may not be available but users can select a discrete input range within the bbn interface if that is preferred if output for a different known input distribution is required users can enter that distribution in netica no model re training is required and the model will propagate that distribution through the directed acyclic graph dag to all other nodes 4 2 variables with a case for every entry in the conditional probability table the learning algorithms tested by callaghan et al 2018 where there was a training case for every entry in the conditional probability table output node of the bbn indicated that the counting learning algorithm applied with a degree of 106 resulted in an output node distribution that was identical to that obtained from the training cases the same accuracy was achieved here as the training cases for hav h05 uav u05 fav and f05 have a case for every entry in the conditional probability table and consequently the bbn for these variables are identical to the probabilities estimated from the wave prediction database tables 4 6 with the results identical to 3 significant figures additional quantitative comparisons of probabilities where each node was instantiated for each discrete range in turn and compared to probabilities estimated directly from the wave prediction database i e repeating callaghan et al 2018 fig 7 confirmed exact predictions by each of these bbn fig 9 for these hydrodynamic variables it is possible to construct a single bbn with all six of these variables in a single network and still obtain exact predictions this is because there is a training case for every entry in the conditional probability table of the bbn and the two way information flow possible through the network from one output to another output has no influence on the predicted probabilities however we found that such a network was too congested for ease of use by non expert stakeholders with multiple and crossing arrows confusing the link between the input variables and the output hence we constructed different networks for each variable of interest while this requires input of the same data more than once to obtain h05 u05 and f05 for example end users indicated that simpler networks were better during in country training workshops associated with the ccres project 4 3 variables without a case for every entry in the conditional probability table for the variables describing the salient width at the reef centreline and quarter reef lengths either side xs5 xs3 and xs7 the all input nodes to one output node bbn see fig 14 for each of these variables have training cases for only 91 of entries in the conditional probability table output node this is because certain wave angles do not yield valid estimates for longshore transport that control the planform evolution of the shoreline and these conditions are discarded from the modelling this leads to minor differences for the trained output node distributions tables 7 9 and minor difference in the estimated bbn probabilities compared to those estimated directly from the wave prediction database fig 10 however the bbn probabilities are generally well within 5 of the actual wave prediction database probabilities which we regard as acceptable given the assumptions and accuracy of the wave and shoreline modelling for these shoreline variables there is feedback within a single network if that network contains all the output nodes and therefore the small differences in the predicted and actual probabilities become compounded this means a single network that contains simultaneous output for xs3 xs5 and xs7 is not desirable since the bbn probabilities are not the same as for each variable in a single network to illustrate the differences that arise with linked output nodes we built an expert knowledge based bbn to link the shoreline variables xs3 xs5 and xs7 within one network since these all describe different widths of the salient alongshore with the data also tabulated in figs 7 9 one such example network is shown in fig 11 a with the accuracy of the bbn probabilities compared with the true values in fig 11b using the same methodology as that for figs 9 and 10 clearly the expert knowledge bbn performs poorly compared to the all input to one output node bbn for these same variables fig 10 as expected from callaghan et al 2018 this is because a bbn that links the different predictions along the shore results in incorrect predictions due to feedback back through the network between the different shoreline positions thus a model that links these three variables together is not as accurate as models where there is only one output as discussed above for the hydrodynamic bbn there is a learning case for every combination of the conditional probability table consequently no more expert knowledge or assumptions about the network structure is required in the case of the shoreline position bbn 91 of the conditional probability tables are covered by the learning cases and output accuracy is within 5 hence the scope for including further expert knowledge is marginal at best callaghan et al 2017 also found that once the majority of the conditional probability table is defined by learning cases the expert knowledge in the form of a network linking physical processes together actually reduced model accuracy since it introduces approximate relationships not necessarily included in the models therefore we do not combine or update the data inferred bbn with additional expert knowledge consequently to maintain simplicity for end users to avoid end users needing to understand how more complex bbn might include unknown feedback processes and to maximise model accuracy we adopt single output models this also keeps the network structure simple and transparent for non expert end users input data does have to be repeated to obtain predictions for the variation in salient width with distance along the shore spatial variation in width but for the purposes and primary end users for the ccres models this is preferable to a network with multiple output nodes it is trivial for end users to combine and interpret results for the different variables if needed 4 4 example bbn for wave height and salient width two examples for wave height and salient width are illustrated in figs 12 and 14 respectively the file sizes for these two networks are 95 kb and 1 3 mb respectively which are small and quick and the bbn do not require any significant computer science skills to operate or use other bbn for wave induced velocity wave induced forces on different coral species table 3 together with a link to the free netica software a user guide and a training presentation giving an overview of the key coastal processes involved are available on the capturing coral reef ecosystem services ccres project website https ccres net the bbn network inputs are the bay width bayw reef width w reef length l water depth over the reef flat d wave height h wave period t and wave direction dir coral diameter is an additional input parameter for a further bbn that estimates wave forces on different species of coral following the approach of baldock et al 2014 2014b the two output nodes presented here are hav m which is the average wave height over the whole of the reef top and xs5 m which is the shoreline position at the centreline of the reef and also the maximum salient width it is straightforward to use the bbn networks see callaghan et al 2017 2018 or pearson et al 2017 for further examples of the application of similar models input values for a particular reef are chosen based on known information or left as unknowns using a uniform or chosen distribution with the mean value and standard deviation of the output parameter immediately displayed in the centre output node means and the range are displayed since the bbn has subsumed the complete dataset into discrete ranges of the input variables given that idealised reef geometry is used such that reef width and length are subjective to user interpretation and mean water depth is not usually known to a precision better than 0 2 m at best calculation of single value for the output is not justified nor needed as an example fig 12 gives the average significant wave height hav over the reef top as 0 64m with a range of 0 54 0 74m for incident waves with height 2m a period of 7s and a mean direction of 10 this compares to the exact value from the dataset of 0 54m so the prediction is within the 100 confidence band varying the input parameters allows users to investigate the influence of reef width reef length etc or to determine the change in reef flat wave conditions if the water depth or incident wave conditions change 4 5 application to a real case application to a real case is demonstrated in figs 13 and 14 the geometry of a salient formed by a fringing reef at el nido philippines fig 13 is estimated from google earth being w 400m l 800m bayw 3500m a reasonable estimate of the mean water depth is 0 75m based on visual observations of minor wave breaking with incident waves with height of order 0 5m during a site visit with the assumption that the salient is predominantly a result of long period swell approaching from a small angle in this case dir 10 deg and a typical wave height h and wave period t of 1 5m and 9s respectively the bbn estimates the expected salient width at the centreline to be xs5 195m with a standard deviation of 49m fig 14 left panel the width of the actual salient is estimated to be approximately 200m in good agreement with the model prediction keeping all conditions constant except the water depth over the reef which is raised to 1 5m representing a combined loss of reef elevation and or sea level rise of 0 75m the expected width of the salient is reduced to 97m fig 14 right panel i e a landward recession of the shoreline of approximately 100m or 115m if the steric recession is included for a typical beach gradient of 1 20 this degree of recession due to the erosion of the salient is equivalent to a 5m static sea level rise consequently the loss of reef elevation or small amounts of sea level rise can lead to very severe beach erosion in comparison to that expected on open coast beaches under normal sea level rise conditions similarly a change in the mean wave climate direction of order 10 leads to a rotation of the salient fig 8 reducing the salient width at certain locations clearly if community or tourism infrastructure were located close to the shoreline then impacts would be severe finally the bbn also allow estimates of input conditions that lead to given output scenario i e the bbn can run in reverse for what if scenario planning for example for a given reef geometry and wave conditions the salient width can be chosen to determine what minimum water depth is required to maintain a given salient width 5 conclusions the response of the shoreline in the lee of fringing reefs subject to climate changes and reef degradation has been investigated using numerical modelling to assess the impact of sea level rise changes in wave climate and loss of reef elevation the swan hydrodynamic wave model and classical theory describing the planform of beaches in equilibrium with the wave forcing have been combined to estimate the shape and size of salients formed in the lee of the reefs while the shoreline model relies on the accuracy of the wave model swan has been extensively verified for a wide range of different conditions and coastal bathymetry the model results are consistent with natural salients in el nido philippines and ningaloo reef western australia simulations are performed for over 6 000 000 different combinations of reef bathymetry wave conditions and bay width bayesian belief networks bbn were then developed to enable easy user access to the large database of model results using the bbn changes in salient width or shape can then be estimated for different or new conditions e g deeper water depth over the reef or a shift in wave direction using the suite of model results the modelling shows that salients may either accrete or erode under changing scenario with potential loss of the salient and high rates of shoreline erosion a number of different open access bbn with high accuracy and simple user interfaces have been built to assist communication of the complex and extensive dataset to end users simple networks with only one output node are used for ease of use by non expert stakeholders and to avoid inaccuracies induced by interactions between multiple output nodes we show that bbn models of this form reproduce exactly the actual probabilities in the training set when there is a case for every entry in the conditional probability table the bbn enables access to all the model results and easy comparison of different scenario allowing end users to quickly assess how changes in the wave climate or reef elevation are likely to influence the shoreline behind a reef the modelling shows that recession of the shoreline in the lee of fringing reefs due to sea level rise may be much greater than that expected on open coast beaches thus the presence of the reef potentially makes the coastline more sensitive to changes in wave climate or sea level similarly loss of reef flat elevation from coral loss can lead to significant erosion of the shoreline behind the reef the modelling suggests that many reef protected beaches are vulnerable to the effects of sea level rise and degradation of the associated fringing reefs because of the potential for significant changes in shoreline position as the relative elevation of the reef flat changes software and or data availability section physics based wave model predictions using swan 40 85 www swan tudelft nl bbn modelling using netica 4 16 www norsys com acknowledgements this work was funded by the world bank gef the university of queensland australia project entitled capturing coral reef ecosystems services ccres and australian research council grant dp dp14010130 the high performance computing was supported by queensland cyber infrastructure foundation australia and the university of queensland b s acknowledges the university of queensland grant rm 2014001465 co funded by global change institute the university of queensland australia 
26183,managing coastal flood risks involves selecting a portfolio of different strategies analyzing this choice typically requires a model state of the art coastal risk models provide detailed regional information but they can be difficult to implement computationally challenging and potentially inaccessible simple economic damage models are more accessible but may not incorporate important features and thus fail to model risks and trade offs with enough fidelity to support decision making here we develop a new framework to analyze coastal flood risk management the framework is computationally inexpensive yet incorporates common features of many coastal cities we apply this framework to an idealized coastal city and assess and optimize two objectives using combinations of risk mitigation strategies against a wide range of future states of the world we find that optimization using combinations of strategies allows for identification of pareto optimal strategy combinations that outperform individual strategy options keywords storm surge flood risk coastal damage resilience multi objective robust decision making risk management new york hurricane climate 1 introduction communities have used dikes or levees to protect coastal areas from floods for centuries in the netherlands for example dikes have been used to protect small regions since the 13th century gerritsen 2005 the presence of dike rings surrounding low areas was so prevalent that the dutch language has a word for it polder which has come into english usage as well stevenson 2010 numerous other defensive strategies are available to reduce the risk of storm surge these risk mitigation strategies have advantages and disadvantages impacted for example by where the strategies are considered or how they appeal to different stakeholders these strategies can include insurance preservation or enhancement of natural barriers construction of physical barriers across waterways installation of active measures such as pumps adoption of zoning restrictions withdrawal or relocation of development physical alteration of buildings and resiliency improvements fema 2011 de blasio and bruno 2014 regardless of which strategy or combination of strategies is considered policymakers require a means of assessing the needed level of protection and evaluating the performance of candidate strategies in terms of often divergent stakeholder objectives groves et al 2016 prior to 1953 in the netherlands the predominant practice had been to establish dike height based on the highest previously observed flood levels plus a three foot safety margin van dantzig 1956 battjes and gerritsen 2002 today a common approach to establishing flood protection levels is based on a return level the estimated height that is expected to be exceeded for a specified probability of occurrence in the united states the 100 year return level is used for this purpose and has led to the widespread use of base flood in reference to the expected water level to be reached or exceeded with a 1 probability in a given year the us government uses base flood to set policy us cfr 725 executive orders 11988 1988 2015 fema 2015 bellomot et al 1999 recently researchers and policymakers have expressed concerns about the adequacy of this standard and the efficacy of current implementation of the standard highfield et al 2013 galloway et al 2006 wing et al 2018 other approaches to estimating flood risks and establishing flood safety measures are possible in january of 1953 in the netherlands a winter storm generated a storm surge subsequently named the big flood that caused extensive damage and killed 1836 people gerritsen 2005 in response the government formed the delta commission which turned to the dutch mathematician david van dantzig van dantzig 1956 gerritsen 2005 zevenbergen et al 2013 he approached the problem from both a statistical and economic perspective on the statistical side he examined historical tide gauge data to estimate the probability that a given flood height had been exceeded on the economic side he considered the current cost of constructing a dike and the corresponding net present cost of future damages that should be expected the sum of these two quantities is the net present cost of the strategy minimizing this net present cost results in an economically optimal risk reduction strategy that sets an economically optimal dike height van dantzig 1956 researchers have subsequently improved van dantzig style models to account for parameter uncertainty and improve flood probability models slijkhuis et al 1997 speijker et al 2000 huisman et al 2010 oddo et al 2017 wong et al 2017 and to investigate methods of learning for optimization of future height adjustments kok and hoekstra 2008 garner and keller 2018 however several assumptions limit the utility of van dantzig style models for example van dantzig style models are restricted to evaluating a single protection strategy such as construction of a levee or dike in contrast policymakers today are interested in considering other approaches to risk mitigation and combinations of different strategies ligtvoet et al 2012 new york city special initiative for resilient rebuilding 2013 fischbach et al 2017 a second example of van dantzig style model limitations is their simplifying assumptions that dike cost is proportional to height and damage due to flooding is zero until levees are breached or over topped at which point 100 damage occurs van dantzig 1956 these may be reasonable approximations for traditional dikes protecting flat terrain in the low polders found in the netherlands but are a poor representation of many major cities located on a rising coast or hilly terrain another potential weakness of a van dantzig style model is its focus on a single objective the discounted expected total cost the stakeholder community impacted by flood events and affected by flood risk mitigation efforts is large and diverse with correspondingly diverse values harman et al 2015 bessette et al 2017 associated with multiple and often conflicting sets of objectives harman et al 2015 porthin et al 2013 oddo et al 2017 unfortunately estimated base flood levels or dike heights do not allow city planners to evaluate changes to damage associated with surge heights above the selected return level additionally return periods for these estimated levels are typically long compared to the record of surge observations available grinsted et al 2012 2013 menéndez and woodworth 2010 ceres et al 2017 lee et al 2017 this relative sparsity of data leads to large uncertainties surrounding estimates of long period return levels coles 2001 and the potential for bias in estimating extreme event risks using common extreme value analysis methods coles et al 2003 ceres et al 2017 researchers have developed more complex methods and models for assessing various aspects of storm surge risk superstorms katrina and sandy have motivated substantial investments in storm surge protection for both the new orleans and new york city metropolitan areas respectively fischbach and rand gulf states policy institute 2012 aerts et al 2013 groves et al 2016 aerts et al 2014 the magnitude of proposed investments justified extensive site specific research on both future storm surge risk for these broad geographical regions and for evaluation of several proposals to mitigate that risk to support these efforts researchers have developed state of the art storm surge modeling frameworks these frameworks incorporate hurricane track and intensity prediction models driving hydrological storm surge models incorporate site specific bathymetry and geography that in turn interact with local topography and potential defensive measures to generate simulated inundation levels over a wide area these inundation levels are then matched to detailed demographic data and damage models to produce area wide and location specific estimates of economic impact e g kerr et al 2013 taflanidis et al 2013 flowerdew et al 2010 orton et al 2012 fischbach and rand gulf states policy institute 2012 aerts et al 2014 these models assist decisionmakers and stakeholders exploring the specific impacts of a storm surge risk mitigation strategy because the models realistically quantify storm surge damages that can be expected for a given set of storms in new orleans for example the coastal louisiana risk assessment clara model was developed to estimate flood extent associated flood depths and resulting damages for the entire louisiana coastal region fischbach and rand gulf states policy institute 2012 johnson et al 2013 policymakers can use this approach to evaluate the effectiveness of several risk mitigation strategies in response to specific storms over the modeled region for instance given five storm surge barrier options for lake pontchartrain the clara model was recently used to evaluate the reduction in storm surge damage across 15 counties against 77 storms fischbach et al 2017 in many cases clara style frameworks couple several computationally expensive models for instance clara uses the advanced circulation adcirc and the simulating waves nearshore swan models to develop water and wave height levels throughout the study region computationally these models have long run times that limit the analysis to relatively few storms and tracks fischbach et al 2017 hence using this approach to evaluate the effectiveness of multiple risk mitigation strategies against the full range of possible future storms can be computationally unfeasible this then influences the selection of optimal mitigation strategies additionally the expense associated with creating similar models for other regions will exceed the resources available to many communities storm surge models of intermediate complexity have been developed and can serve an important role in evaluating storm surge risk as one example a simplified hydrodynamic model was developed to calculate inundation levels for the bay of bengal lewis et al 2013 another study used a simple statistical model of storm surges coupled to a digital terrain model and gridded demographic information to assess the impact of sea level rise on copenhagen s risk of catastrophic storm surge hallegatte et al 2011 a third example utilizes a statistical model of storm surges coupled to a model that simulates the evolution of barrier islands which in turn is linked to an agent based economic model that predicts the performance of resort areas dependent upon those barrier islands mcnamara and werner 2008 mcnamara and keeler 2013 however these models are location specific and are limited in their ability to analyze multiple strategies as such the use of intermediate complexity models is infrequent the gaps in storm surge modeling complexity and usage suggests a need for a new framework with different modeling capabilities here we develop the island city on a wedge icow a model framework that bridges the gap between van dantzig and clara style modeling approaches using an idealized geography of a city situated on a rising coastline such as manhattan show in fig 1 the model simulates the increasing damage that occurs with larger storm surges and the distribution of those damages across the city the icow framework can be used to evaluate combinations of multiple risk reduction strategies such as insurance preservation or enhancement of natural barriers construction of physical barriers and sea gates across waterways installation of active measures such as pumps adoption of zoning restrictions withdrawal or relocation of development physical alteration of buildings and resiliency improvements we demonstrate the utility of this model by evaluating combinations of withdrawal building resistance and dikes by evaluating several objectives over differing time scales these objectives can include investment cost investment timing median or maximum annual storm surge damage or the distribution of damage within the city the framework is flexible enough to incorporate other effects such as economic loss associated with withdrawal strategies or potential value shifts associated with construction of levees or dikes icow requires much less run time than the clara model as a result icow is capable of evaluating the efficacy of multiple combinations of storm surge risk mitigation strategies over a wide range of potential futures while computationally inexpensive icow improves upon van dantzig style models in several important respects the framework incorporates typical characteristic features of coastal cities features intended to improve its overall fidelity as a result the icow framework can extend insights gained from clara style models by evaluating many more combinations of defensive strategies against many potentially divergent objectives and over a wider range of future risk scenarios this additional potential insight however is limited to a much narrower spatial range limited to a single community of relatively uniform characteristics and may provide a less realistic representation of specific future outcomes such as the precise amount of damages expected from a specific storm the icow s lower complexity and more limited spatial extent allows for coupling with multiple objective evolutionary algorithms moea to optimize complex combinations of risk mitigation strategies evaluated against multiple and potentially divergent objectives of interest to stakeholder communities because the icow framework is largely self contained and does not demand high resolution geospatial information such as bathymetry data topography ground coverage or utilization it is considerably easier and less expensive to implement for a particular coastal community compared to clara style models the remainder of this article explains the general characteristics and features of the icow framework in terms of an xlrm framework lempert et al 2006 describes the icow computational environment and experimental design presents example applications and discusses results and conclusions 2 icow xlrm framework we describe the icow framework using the xlrm framework for robust decision making lempert et al 2006 fig 2 shows the logical relationships between exogenous factors x model strategy levers l modeling relationships r and performance metrics m exogenous factors x are characteristics of the simulated city environment that are fixed from the perspective of decisionmakers and stakeholders levers l simulate the actions that decisionmakers can implement to affect the city s response to storm surge metrics m define outcomes that are of potential interest to different stakeholders relationships r represent the model logic implemented to simulate the city response in terms of metrics that result from a particular combination of exogenous factors and lever settings for this article and to demonstrate the capabilities of the icow framework in general we use many simplifying assumptions regarding the modeled city s characteristics the icow design and associated parameterizations are intended to represent the general qualities and behaviors of the xlrm components described below peer reviewed literature and quantified data for these categories are often not available and require justification as discussed below additionally location specific information relevant to xlrm categories e g dike construction costs often varies substantially from location to location similarly the simplified topography and demographics of the modeled city e g the regular wedge shape geometry of the modeled community are intended to be representative of a generic coastal community rather than any particular site icow framework results are sensitive to these simplifying assumptions and representative and generic parameter settings 2 1 exogenous factors within the xlrm framework lempert et al 2006 exogenous factors represent any condition that has an impact on the output metrics that cannot be changed by decisionmakers or stakeholders for this article the primary exogenous factors are the initial city parameters and the sequence of storms that will impact the city we incorporate other exogenous factors as model parameters e g the per volume cost to construct a dike or the height of the city seawall see supplemental table appendix c for full details 2 1 1 surge simulation storm surges are simulated as annual highest surge heights generated from a nonstationary generalized extreme value gev model such that the 100 year storm surge is increasing at a rate of 1 m per century due to an increase in both the location and scale parameters ceres et al 2017 some storm surges generated in this manner may be larger than physically plausible storm surges used for this study are clipped such that surge heights exceeding a threshold are capped at the threshold we use a thresholds of 12 m for the first year and increase by 0 01 m per year thereafter this threshold is chosen as a compromise between the goal of accounting for the full range of risk exposure based on the statistical model used to generate the surges and the desire to provide fidelity to physical reality we use 5000 realizations of 50 year sequences of storm surges for all examples discussed in this article this number represents a compromise between computational efficiency and stable results see the supplemental materials section appendix b for additional figures and discussion on the number of realizations used 2 1 2 initial city characteristics many cities e g boston nyc and san diego are situated at the water s edge and consist of a gradually rising terrain in these cases higher surge levels will result in larger areas and greater depths of inundation the waterfront areas in these cities such as manhattan new york city ny fig 1 are often densely packed with tall buildings icow simulates an island city situated on a rectangular wedge the most prominent characteristic of the icow city is the gradually rising elevation of the city with distance from the city s lowest waterfront fig 3 the city terrain is elevated from the normal water level by a seawall icow buildings are uniformly tall and higher than the highest potential surges damage to buildings accrues based on the volume of the building flooded buildings have a basement volume that floods completely when water reaches the level of the building based on volume real estate value is initially assumed to be constant regardless of building height or location relative to the waterfront in the simplest configuration city value and density are uniformly and continuously distributed from water s edge to the highest city elevation as defensive strategies are added this value density may change the addition of defensive strategies divides the city into zones of different damage vulnerabilities as described below 2 2 strategies and levers as discussed in the introduction numerous defensive strategies are available to mitigate storm surge risk for this study we consider one fixed and three adjustable defensive strategies that can be implemented to varying degrees and in combination the fixed defensive strategy is the presence of a seawall around the city the three adjustable defensive strategies are i withdrawal from at risk areas ii improving resistance to damage and iii construction of a dike all strategies are assumed to be implemented uniformly across the width of the city parallel to the coastline see fig 3 a within icow levers are the mechanisms by which defensive strategies are implemented 2 2 1 seawall the icow design assumes an existent seawall of fixed height surrounding the city no damage occurs to the city as long as surge heights are below the level of the seawall when surge height exceeds the seawall the height of water that affects the city is equal to the amount of excess and an additional height caused by wave run up for this study there is no option to increase the seawall height thus icow does not include seawall height as a lever 2 2 2 withdrawal the first icow adjustable strategy is to specify withdrawal from regions of the city below some withdrawal height and relocation to the city s higher levels this height is the single withdrawal strategy lever an actual withdrawal strategy implemented in a city such as new york might involve relocating individual buildings at very low elevations that are the least resistant to flood damage in the model all unmodified buildings have the same resistance to flooding damage thus the most vulnerable buildings are uniformly located at the lowest elevations in the modeled withdrawal strategy it is this lowest fractions of building volume without regard to individual buildings that is withdrawn as withdrawal height increases this modeled representation seems reasonable so long as the area of the city relocated the withdrawal height remains low as the withdrawal height increases the fidelity of the modeled withdrawal strategy to an actually implemented strategy would be expected to decrease in icow we model this relocation of the lowest elevation buildings by creating a zero value zone zone 0 and redistributing the value that had been contained in that zone over the remaining area of the city subsequently and uniformly increasing the value density in the remaining city area 2 2 3 resistance the second adjustable strategy is implementation of resistance to damage for the lowest portions of buildings which are most exposed to flooding there are two levers associated with this strategy the first lever resistance height r is a specified height above the withdrawal height below this level buildings are modified to increase their resistance to flood damage implementing resistance creates a resistant zone zone 1 the upper extent of zone 1 in terms of the city elevation is constrained by the lower of the resistance height or by the location of a dike base building volume flooded below the building s resistance height is subjected to less damage than nonresistant buildings the percentage p of resistance to damage compared to the non resistant damage that results in resistant sections is the second lever resistance strategy lever flooded volumes above the resistance height are subjected to damage at the nonresistant rate 2 2 4 dikes construction of a dike is the third strategy considered for this study as with implementation of the other strategies we assume dikes are constructed parallel to the shoreline because icow is modeled as a wedge shaped island the dike must have sections on either end of the dike to keep surges from flowing around the sides of the dike icow dikes greatly reduce flooding damage to buildings located behind them unless they fail or are overtopped discussed further in section 2 4 3 two levers implement the dike strategies the first lever is the elevation of the dike base b above the withdrawal height the second lever is the height of the dike d the area behind above the dike base but below the dike top are protected by the dike and define zone 3 2 2 5 strategy combinations icow can simulate simultaneous implementation of these three strategies through the simultaneous operation of the five model levers the levers are summarized in table 1 the relationship between strategy levers and resultant zones is illustrated in fig 3a and b 2 3 metrics implementing storm surge mitigation strategies has many consequences and the importance of these consequences may vary among different stakeholders to the extent that these consequences can be measured there are many possible metrics associated with risk mitigation performance as examples these could include implementation costs maintenance costs the cost to repair buildings damaged by flood the direct or indirect economic damage associated with storm driven changes to the economy damage to important city infrastructure probabilities of nuisance flooding the probability of catastrophic events or even deaths some consequences may be more difficult to model such as a strategy s impact on a community s city character and culture icow is able to calculate many metrics corresponding to the objectives of many different stakeholders to simplify the illustration of the impact of allowing for multiple strategies and levers we examine three metrics implementation cost expected damage costs and the total cost which is the sum of investment cost and expected damage cost 2 4 relationships within an xlrm modeling framework relationships convert exogenous factors and lever settings into metrics in this section we discuss the relationships between exogenous factors lever settings and the cost and damage metrics for each strategy in general icow model metrics including cost and damage are based on volume of the city effected in the most basic case where no strategies are implemented cost is zero and damage will be proportional to the value density of the area flooded and the building volume flooded actual building damage from storm surges in general increases with exposure to higher water levels but is also affected by other factors such as wave action flow velocity and duration of inundation fema 2011 2015 for simplicity we model damage based on storm surge height the actual damage function for particular buildings however varies by building usage construction type and more broadly by regional location prahl et al 2018 the icow framework adopts a generic damage model where in an unprotected state damage to a structure starts occurring when the water level exceeds the building base elevation at that level a set damage occurs associated for instance with basement flooding above this level additional damage is proportional to the volume of the building flooded when calculated in this way the damage function which emerges for aggregated areas of the icow city will align with other research into damage functions for urban areas prahl et al 2018 this basic relationship is modified by the employment of strategy levers as discussed below 2 4 1 withdrawal costs and damage relationships conceptually for a given building icow s withdrawal cost can be thought of as the total cost to acquire a new building at a higher location the cost to relocate and the cost to remove the old structure the cost to implement a withdrawal strategy is based on the area to be relocated the value density of that area and the total remaining area in the city available for relocation a fraction of the displaced infrastructure will relocate outside the city the cost to implement withdrawal c w is 1 c w v i w f w c i t y h e i g h t w where v i is the initial city value and f w is a factor intended to adjust for any local conditions such as the presence of historic buildings or heavy industry facilities as well as relocation and demolition costs that might make it more or less expensive to relocate the value of the city after withdrawal v w is 2 v w v i 1 f l w c i t y h e i g h t where f l is the faction of infrastructure that will leave rather than relocate within the city once withdrawal is implemented no damage will occur to city areas below the withdrawal height w zone 0 above this height city density is higher and damage will therefore be proportionally higher per volume when surge heights reach the levels above the withdrawal area 2 4 2 resistance cost and damage relationships modifying buildings to be completely invulnerable to storm surge damage is generally infeasible due to increasing costs and costs vary based on building type and materials intuitively the cost of implementing resistance should rise as the percentage of resistance p incorporated increases and this cost increases sharply as resistance approaches 100 we model this characteristic cost structure using a linearly increasing resistance cost c r per unit value until a threshold percentage is reached as resistance percentage increases above this value we add an exponentially increasing term that increases sharply as resistance percentage approaches 100 so that the cost fraction of resistance with respect to p f c r increases according to 3 f c r f l i n p f e x p m a x 0 p t h r e s h o l d 1 p where f l i n is a factor that controls the linear rate of increase in cost at low percentage increases and f e x p is a factor that controls the exponential rate of increase at low percentages increasing the percentage resistance to damage results in a linear increase in cost as resistance percentage increases above a threshold and approaches one where building volume would be completely invulnerable to surge damage cost per volume increases sharply such that increasing resistance fraction to one would be infinitely costly we assume that the total cost of implementing resistance increases at this rate in proportion to the volume being made resistant but because the width and slope of the city is constant the relationship can be greatly simplified and expressed solely in terms of elevation changes in cases where resistance is not constrained by the presence of a dike resistance cost is 4 c r v w f c r r r 2 b h c i t y e l e v a t i o n w where b is the representative basement depth and h is the city building height in this case there will be an unprotected zone 2 in front of the dike when the resistance zone 1 is constrained by a dike i e r is higher than b then there is no zone 2 and the areas behind the dike are in the area protected by the dike zone 3 no resistance is incorporated behind the dike regardless of the resistance height r when zone 1 is constrained by the presence of a dike the resistance cost is given by 5 c r v w f c r b r b 2 b h c i t y e l e v a t i o n w in the resistant area zone 1 damage is reduced by the resistance fraction for resistant volumes flooded when flooding exceeds the resistance height additional damage accrues at the normal nonresistant rate for volume flooded above the resistant height 2 4 3 dike cost and damage relationships empirical data and peer reviewed literature on dike costs is sparse and indicates a wide range of dike construction costs jonkman et al 2013 thus we make several simplifying assumptions with the aim of modeling the generic case that could be modified as needed to match the particular circumstances of an actual coastal community real dikes have sloped sides or when they have a constant width profile with respect to height d require greater strength at the base compared to the top of the dike therefore with all other factors being equal taller dikes are more expensive than shorter ones zhu and lund 2009 jonkman et al 2013 icow currently models all surge barriers as having sloped sides icow dikes have sloped sides and a flat top thus the volume of a dike increases approximately geometrically with height dikes are modeled as perpendicular to the waterfront because the city is modeled as a wedge shaped island and to prevent surges from flowing around the edges of the dikes icow dikes are u shaped therefore the overall length of a dike will increase based on the additional length of dike required to be constructed on the sloping sides of the city these portions of the dike are irregular tetrahedrons because the city slope s is low the length of the wings is long compared to the dike height which allows for the simplifying assumption that dike wing lengths at the top and bottom of the dikes are equal icow modeled dike cost is proportional to volume additionally large scale projects such as dikes typically incur a fixed startup cost which reflects the costs necessary to plan design and approve projects cost to acquire and prepare dike sites and to make the initial and wrap up costs for large scale projects that are dependent on site location and dike height or volume zhu and lund 2009 jonkman et al 2013 for simplicity we emulate start up cost as a fixed additional height dike volume v d used to calculate cost is based on the a height h which is the sum of the design height and a fixed initial startup height city width w c i t y dike top width w d slope of the dike sides s and slope of the city s according to 6 v d w c i t y h w d h s 2 1 6 h 4 h 1 s 2 s 2 2 h 5 h 1 s s 4 4 h 6 s 2 s 4 4 h 4 2 h h 1 s 4 h 2 s 2 h 2 s 2 s 2 s 2 2 h 3 h 1 s s 2 1 2 w d h 2 s 2 as a result startup costs will be larger but account for a lower percentage of total costs for taller dikes the icow model specifies dikes based on the location of the dike base relative to the withdrawal height and the height of the dike from the dike base cost of the dike c d is calculated by multiplying dike volume v d from equation 6 with the per cubic meter cost of the dike c d p v to yield 7 c d v d c d p v the resultant dike height to cost profile is shown in supplemental fig a 6 when dikes function properly and when they are not overtopped they greatly reduce the damage in the protected zones dikes however will fail when they are overtopped and they may fail for a variety of reasons prior to water levels exceeding their design protection heights tobin 1995 apel et al 2004 sills et al 2008 example reasons for failure include improper design incorrect operation inadequate maintenance or foundation erosion icow represents this nonzero probability of failure with a low fixed probability of failure at all surge heights h s u r g e below a high percentage of the dike height above this threshold t d f probability of failure increases linearly with height until it reaches one at the dike s design height when surge height is less than the threshold 8 p d f h s u r g e t d f d t d f when the dike holds damage in zone 3 is reduced by a fraction of what would otherwise accrue in the dike protected zone if the dike fails or is overtopped damage will accrue in the dike protected zone at a rate greater than the unprotected rate 2 4 4 aggregate damages damage accumulating by zone for a range of surge heights for one combination of mitigation strategies is illustrated in fig 3b damage to each zone d z is calculated based on the value of the zone v a l z the volume of the zone v o l z the volume flooded v o l f and the per volume rate of damage incurred by flooding based on the strategy implemented in the zone f d a m a g e 9 d z v a l z v o l f v o l z f d a m a g e where v o l f depends on both levers and surge height icow features many permutations of this damage function depending upon the lever settings the resulting city zone configuration and the height of the surge relative to the heights of the city zones and implemented protective strategies the icow model parameters used in this paper are inspired by the situation in manhattan in that the building heights are tall relative to potential storm surges dikes are inexpensive relative to the assets protected behind them there is an existing seawall and the breadth of the coastline is long relative to the volume of city protected the model parameters are easily customized such that they can represent a particular city with greater fidelity differences between icow and van dantzig style models are summarized in table 2 at this point we have presented the icow setup and definitions the implementation is described in section 3 1 results are described in section 5 and conclusions are summarized in section 7 3 methods 3 1 icow framework computational description the icow framework consists of two modules the icow module and a set of multi objective evolutionary algorithms moea together these work in tandem to evaluate and optimize multiple risk mitigation strategies 3 1 1 icow module we developed the icow module in c for computational efficiency it consists of one program with two major components executed in sequence the first component takes the exogenous factors associated with the initial baseline city and characterizes the city in terms of the value distribution zones and damage functions described in section 2 4 in response to the chosen strategy lever settings described in 2 2 the costs to implement the strategy lever inputs and city value changes are icow module output metrics the second component takes the city s value distribution zones and damage functions as inputs and evaluates the city s response to the exogenous set of storm surge sequences described in section 2 1 1 to generate the remaining module output metrics for this study the defensive strategies are established at time zero and hence the first component to characterize the city is evaluated only once and the characteristics for the city are set for all storm surges evaluated the impacts of storm surge output metrics can include average cost in dollars over a time span as discussed above flood frequency dike breach frequency or frequency of events over an unacceptable damage threshold for this study we use both the cost of implementing strategies and the total cost of damages over 50 years as the input metrics for the moea discussed below in sections 3 1 2 and 5 3 1 2 moea module as more strategies are considered to provide varying degrees of protection and as the number of objectives increases selecting optimal solutions becomes much more challenging hadka and reed 2013 even with a greatly simplified model compared to clara selecting the best mix of surge risk mitigation strategies for a given set of objectives is nontrivial we use the borg moea to solve the optimization problem because of its high computational efficiency and easy scalability to parallel computing environments hadka and reed 2013 2015 the borg moea is not a single moea algorithm rather it is an auto adaptive class of high performance moea algorithms based on the evolutionary progress of a population of candidate solutions hadka and reed 2013 we use initial borg population sizes of 200 members this initial population evolves over one million functional evaluations to a final larger populations of more than 1000 members for all figures in this article we compared results generated using the borg moea with solutions generated by the nsgaii algorithm deb et al 2000 and find consistent solutions 4 example applications we explore pareto optimal strategies for protecting cities against future storm surges we identify multiple cases with increasing complexity for optimization we start with the relatively simple van dantzig style single objective single lever solution section 4 1 to demonstrate the capabilities of the icow module we then increase the number of objectives to two and increase complexity by examining the one two four and five lever cases 4 1 single objective optimization optimizing van dantzig style models with one lever dike height and one objective to minimize net present cost is simple and in many cases can be solved analytically e g van dantzig 1956 the icow model can be configured to emulate van dantzig style behavior by varying implementation of one strategy with the other strategies held constant i e vary dike height but fix withdrawal height dike setback and resistance height to zero and measuring the net cost that results from every dike height the optimal solution that emerges is a single level defined by an optimal dike height and a resultant net cost see supplemental section appendix a for additional discussion and examples note that a point solution based on lowest net cost may not be realistic decision makers funds available for storm surge risk mitigation investments might for instance compete with funding for mitigation of other risks or funding for other economically valuable investments e g education or sports facilities thus fully funding a van dantzig style economically optimal level may not be possible alternatively other stakeholders may prefer the lower variability and uncertainty in future risk over the more tangible current investment costs in these cases being able to examine higher levels of storm surge risk mitigation investment in terms of reducing the variability of future risk is appropriate 4 2 multiple objective optimization with multiple levers to provide decisionmakers with more options we increase the complexity of the problem by incrementally increasing the number of levers to provide many combinations of strategies and determine pareto optimal two objectives solutions we identify three cases to help illustrate the additional complexities when optimizing across multiple strategies simultaneously for the first case we compare the cost and risk reduction associated with a dike only single lever d strategy with a dike height d and resistance height r two lever strategy in the single lever example a dike can be constructed at the seawall to any height the resultant city consists of zones 3 and 4 in the two lever example policy makers can either construct a dike or incorporate resistance with a fixed percentage p given the structure of the icow model the two levers are mutually exclusive the resultant city then consists of either zones 3 and 4 or zones 2 and 4 for the second case we add two additional levers dike base location b and resistance percentage p and compare results to the first case in this four level example dike height and resistance height are no longer mutually exclusive dikes can be constructed to any height at any location and resistance can be incorporated in front of the dike to any height at any percentage for the third illustrative case we add the additional lever of complete withdrawal from the lowest city levels a five lever example and compare results with the other cases 5 results for the first case we consider when the only solution available is construction of a dike in examining this case with one and two levers see fig 4 a the icow framework identifies an economic barrier to entry situation when no optimal solutions emerge as investment level increases until the investment cost exceeds the dike fixed start up cost in this situtation when the dike is so low the increased damages associated with a failed or overtopped dike exceeds the damages avoided when the dike is not present once this dike investment threshold is reached the framework continues to identify solutions with taller more expensive dikes until additional dike height no longer reduces damages incorporating a resistance option allows policy makers to identify sensible low cost solutions that substantially decrease storm surge damage however once the dike investment threshold is reached the optimal strategy shifts abruptly to increasing dike height this can be seen in fig 4 panel b before the transition point 100 of investment cost is applied to the resistance strategy while after the transition 100 of investment cost is applied to a dike strategy for the second case with four levers the optimal strategy combinations that emerge are superior to the one and two lever examples in that equal or lower damages can be obtained for any given investment level see fig 4c fig 4d shows the shift in the percentage of optimal investment for any given level of investment allocated to each strategy that occurs as the level of investment increases at low levels of investment the optimal strategy is based solely on implementation of resistance at higher investment levels better performance is achieved using a dike only solution at the highest considered levels of investment the optimal solution consists of combining strategies by setting the dike back from the seawall and implementing resistance in the area between the waterfront and dike base in the third case with five levers the additional withdrawal lever allows for the identification of further pareto improvements at the highest investment levels as shown in fig 4 e and f in both the four and five lever cases jagged fractional contribution are evident between 30 60 billion in fig 4 panels d and f these variations occur when the borg moea algorithm identifies diverse solutions with approximately equivalent trade offs between the two objectives damage and investment cost resulting over multiple combinations of lever settings to help visualize the solutions for a given level of investment we can also identify the city elevations for the pareto optimal strategies fig 5 for the five lever example at the lowest levels of investment the lowest cost damage reduction strategy is to implement resistance into an increasing area of the city once a sharp cost threshold is reached the pareto optimal strategy shifts abruptly to implementation of an increasingly tall dike at the city seawall as the amount of investment continues to increase corresponding to the objective to further decrease damage the optimal solution sets the dike back from the seawall at gradually increasing elevations and with slightly increasing dike heights the area between the seawall and the dike is fortified through implementation of resistance first and then as investment continues to increase through a combination of withdrawal and resistance the jagged fractional contributions visible in fig 4 panel f manifests itself in fig 5 as variations in the city elevations for adjacent in terms of investment cost strategies the character of these optimal solutions are highly dependent and sensitive to icow model parameters for instance very small parameter changes to the cost of resistance relative to withdrawal and dike costs can result in surprising and nonlinear changes to the character of the resulting solution sets 6 discussion and limitations simple conceptual storm surge damage models may fail to provide sufficient fidelity in simulating storm surge risk mitigation more complicated regional storm surge damage models improve upon these simple models in terms of realism fidelity and the area covered but they can be difficult to design and implement additionally they are computationally expensive to evaluate multiple combinations of risk mitigation strategies over a wide range of future storm surge risks while considering the differing objectives of various stakeholder communities the combination of these drawbacks may make this approach impracticable for some communities we use the icow framework to analyze an idealized city designed to resemble major cities such as new york every coastal community however is unique thus many icow assumptions such as the assumptions regarding the regular wedge shape the uniform density or the high value density of the island city and parameters such as the costs to implement resistance or the damage functions associated with different zones will not always be appropriate moreover local policymakers are likely to have better local information available as to the proper values for many icow parameters for instance local experience in improving buildings to make them more damage resistant will provide cost information that better reflects local circumstances to make icow useful to city planners at a specific site we must adjust icow to reflect city specific characteristics at a minimum such site specific adjustments need to account for the following factors site specific projections of storm surges local economic demographics and their relationship to the site topography typical building heights and adjustments to damage functions that account for site specific building usage and construction type additionally some simplifying assumptions described in this paper would probably need to be refined examples of these could include the assumption of uniform city density and value with respect to elevation from the waterfront the cost basis for constructing dikes or implementing resistance or the simplifying assumption that a single surge level is the best factor relating projected storm characteristics to damage two objective optimal solutions are sensitive to icow parameters as discussed above but fully evaluating this sensitivity has not yet been accomplished and may be computationally infeasible conducting sensitivity analysis with more than two objectives will likely be even more challenging in the course of adapting icow to a particular community a sensitivity analysis would be centered on the particular combination of assumptions and parameters used in simulating the community under study to provide insight for decisionmakers into critical uncertainties and to focus limited resources towards understanding and mitigating those uncertainties 7 conclusions we develop the icow as a storm surge risk modeling framework of intermediate complexity for decisionmakers and stakeholders the framework is intended to fill the gap between simple conceptual storm surge damage models and realistic but complex and expensive state of the art regional storm surge risk modeling frameworks we demonstrate the capabilities of the icow framework to evaluate and optimize surge risk mitigation strategies ranging from a simple single objective single lever problem to a more complicated and computationally challenging two objective five lever problem decisionmakers can use icow to explore a more comprehensive set of strategies over a wider range of future risk scenarios than previous done with more complicated regional state of the art models insights gained by this approach however will be more limited in geospatial extent to a single area of fairly uniform characteristics and in terms of less realistic representation of specific future outcomes stakeholders can use icow to supplement insights gained from those higher complexity models and illustrate trade offs between conflicting objectives icow provides one degree of spatial resolution in terms of distance from the waterfront and thus represents a compromise between the point single polder solutions provided by van dantzig style approaches and regional gridded results produced by state of the art frameworks icow is relatively simple to modify and implement and so it may be useful to decisionmakers with limited resources but who nevertheless need methods to identify evaluate and illustrate the multiple trade offs implicit in any storm surge risk mitigation strategy the icow framework can be easily modified or extended and can be integrated with other modeling systems such as the building blocks for relevant ice and climate knowledge brick modeling framework wong et al 2017 to further explore storm surge risk in a regional or global context code availability all icow software code used to create the figures in this article are provided as open source under a gnu non commercial licence https github com rceres icow and at http datacommons psu edu supplemental material supplemental material in the appendices provides additional discussion on emulation of van dantzig style model emulation explanations and implications of using gev based storm surges and information on convergence and computational expense associated with changing the number of futures evaluated conflicts of interest the authors are not aware of financial or personal relationships that would pose a conflict of interest acknowledgements we thank d hadka for outstanding technical support with the rhodium multi objective tool kit and the borg moea we thank b lee m haran d titley r lempert and j lawrence for helpful discussions this research was partially supported by the national science foundation through the network for sustainable climate risk management scrim under nsf cooperative agreement geo 1240507 and the penn state center for climate risk management any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nsf errors and opinions are of course those of the authors all authors contributed to the icow conceptual design rc developed detailed model designs wrote all software code and integrated icow software with the borg moea algorithm all authors developed the experimental plan rc designed and executed experiments and conducted analysis of results cf and kk provided guidance for the project rc wrote the article and all authors contributed to editing appendix a van dantzig style model emulation the icow framework is capable of emulating van dantzig style single objective optimization for example the van dantzig style optimal dike height for the illustrative city used in this journal is shown in fig a 6 while the emulation produces a single optimal dike height and a visually similar result there are some differences and the resultant optimal dike heights will not be equal to those obtained directly from van dantzig 1956 the icow van dantzig emulation shown in fig a 6 shows optimization of dike height with respect to the net cost over a fixed 50 year timespan the optimization performed by van dantzig did not consider a fixed time interval van dantzig 1956 his solution comprised an optimal height based on then current analysis of tidal records and considering the net present value of both current investments and damages incurred for the entire span of the future van dantzig 1956 the dike cost model used in van dantzig 1956 is a simple linear relationship to dike height that incorporates no start up costs icow dike cost is based on the wedge shaped geography of the city model the resultant dike volume and a startup cost as described in section 2 4 3 and equations 6 and 7 the resultant relationship between dike height and cost is nonlinear fig a 6 icow emulation of van dantzig style optimal dike height the optimal dike height occurs when the net cost is minimized equating to a dike height of approximately 5 5m fig a 6 fig a 6 shows a gap between a zero dike height and approximately 0 7 m where no solutions exist moreover fig a 6 shows an increase in net cost between the no investment maximum damage point and next optimal point above this gap this gap and subsequent increase in net cost is due to the fixed start up cost of dikes assumed in the icow module that does not occur in van dantzig s model van dantzig 1956 for instance as implemented by oddo et al 2017 additionally because the icow framework increases damage behind a dike if the dike fails or is breached at very low dike heights total damage increases with increased dike height the borg moea algorithm therefore does not identify any optimal solutions in these dike height ranges see discussion in 2 4 3 appendix b relationship between states of the world evaluated and wall time each icow framework state of the world consists of an exogenous 50 year sequence of storm surges the representation of risk associated with the most extreme storm surges improves with more states of the world evaluated evaluating more states of the world however adds computational cost to understand this relationship we measure the computational wall time required to conduct five lever icow optimizations using 1 to 20 000 states of the world for 100 000 500 000 and 1 million functional evaluations fig b 7 we select 5000 states of the world as a compromise between computational efficiency and stable results for use in all experiments described in this article fig b 7 relationship between computational cost and states of the world evaluated the wall time in seconds y axis required to evaluate states of the world where one state of the world is a 50 year sequence of annual highest storm surges x axis fig b 7 appendix c icow model parameters table 3 icow parameters table 3 name value units description v i 1 5 t initial city value b 30 m building height h city 17 m height of the city depth city 2 km distance from seawall to highest point length city 43 km length of the lowest seawall coast h seawall 1 75 m seawall height dike start 3 m equivalent height added to actual height so larger dikes have larger startup costs dike top width 4 m controls the width of the dike top to adjust dike volume s 0 5 m slope of the dike sides dike cost 10 m3 dike cost per volume dike value ratio 1 1 n a controls increase in value for areas protected by a dik unprotected ratio 1 0 n a controls increase in value for areas protected f w 1 0 none withdrawal factor adjusting cost to relocate withdrawal fraction 0 01 n a fraction of population and infrastructure that will leave if withdrawn f lin 0 35 n a linear factor relating percent resistant to resistance cost f exp 0 9 n a factor for exponential cost increase when percent resistant t exp t exp 0 6 n a percent resistant threshold above which resistance cost increases exponentially basement depth 3 0 m representative depth associated with damage occurring when surge reaches building futures evaluated 5000 n a the number of 50 year storm surge sequences used for each borg functional evaluation borg 5000 k borg epsilon for 1 and 2 levers epsilon 500 k borg epsilon for 4 and 5 levers pop initial 200 n a initial borg number of icow cities nfe 100 k number of borg functional evaluations table 4 icow damage algorithm parameters table 4 parameter value units description f damage 0 39 none fraction of inundated buildings damaged protected damage factor 1 3 none increased damage that occurs in protected areas if dike fails t df 0 95 none dike height fraction above which failure probability increases with surge height threshold level 1 375 none fraction of city value above which damage increases more rapidly wave runup 1 1 none surge factor when surge exceeds seawall height 
26183,managing coastal flood risks involves selecting a portfolio of different strategies analyzing this choice typically requires a model state of the art coastal risk models provide detailed regional information but they can be difficult to implement computationally challenging and potentially inaccessible simple economic damage models are more accessible but may not incorporate important features and thus fail to model risks and trade offs with enough fidelity to support decision making here we develop a new framework to analyze coastal flood risk management the framework is computationally inexpensive yet incorporates common features of many coastal cities we apply this framework to an idealized coastal city and assess and optimize two objectives using combinations of risk mitigation strategies against a wide range of future states of the world we find that optimization using combinations of strategies allows for identification of pareto optimal strategy combinations that outperform individual strategy options keywords storm surge flood risk coastal damage resilience multi objective robust decision making risk management new york hurricane climate 1 introduction communities have used dikes or levees to protect coastal areas from floods for centuries in the netherlands for example dikes have been used to protect small regions since the 13th century gerritsen 2005 the presence of dike rings surrounding low areas was so prevalent that the dutch language has a word for it polder which has come into english usage as well stevenson 2010 numerous other defensive strategies are available to reduce the risk of storm surge these risk mitigation strategies have advantages and disadvantages impacted for example by where the strategies are considered or how they appeal to different stakeholders these strategies can include insurance preservation or enhancement of natural barriers construction of physical barriers across waterways installation of active measures such as pumps adoption of zoning restrictions withdrawal or relocation of development physical alteration of buildings and resiliency improvements fema 2011 de blasio and bruno 2014 regardless of which strategy or combination of strategies is considered policymakers require a means of assessing the needed level of protection and evaluating the performance of candidate strategies in terms of often divergent stakeholder objectives groves et al 2016 prior to 1953 in the netherlands the predominant practice had been to establish dike height based on the highest previously observed flood levels plus a three foot safety margin van dantzig 1956 battjes and gerritsen 2002 today a common approach to establishing flood protection levels is based on a return level the estimated height that is expected to be exceeded for a specified probability of occurrence in the united states the 100 year return level is used for this purpose and has led to the widespread use of base flood in reference to the expected water level to be reached or exceeded with a 1 probability in a given year the us government uses base flood to set policy us cfr 725 executive orders 11988 1988 2015 fema 2015 bellomot et al 1999 recently researchers and policymakers have expressed concerns about the adequacy of this standard and the efficacy of current implementation of the standard highfield et al 2013 galloway et al 2006 wing et al 2018 other approaches to estimating flood risks and establishing flood safety measures are possible in january of 1953 in the netherlands a winter storm generated a storm surge subsequently named the big flood that caused extensive damage and killed 1836 people gerritsen 2005 in response the government formed the delta commission which turned to the dutch mathematician david van dantzig van dantzig 1956 gerritsen 2005 zevenbergen et al 2013 he approached the problem from both a statistical and economic perspective on the statistical side he examined historical tide gauge data to estimate the probability that a given flood height had been exceeded on the economic side he considered the current cost of constructing a dike and the corresponding net present cost of future damages that should be expected the sum of these two quantities is the net present cost of the strategy minimizing this net present cost results in an economically optimal risk reduction strategy that sets an economically optimal dike height van dantzig 1956 researchers have subsequently improved van dantzig style models to account for parameter uncertainty and improve flood probability models slijkhuis et al 1997 speijker et al 2000 huisman et al 2010 oddo et al 2017 wong et al 2017 and to investigate methods of learning for optimization of future height adjustments kok and hoekstra 2008 garner and keller 2018 however several assumptions limit the utility of van dantzig style models for example van dantzig style models are restricted to evaluating a single protection strategy such as construction of a levee or dike in contrast policymakers today are interested in considering other approaches to risk mitigation and combinations of different strategies ligtvoet et al 2012 new york city special initiative for resilient rebuilding 2013 fischbach et al 2017 a second example of van dantzig style model limitations is their simplifying assumptions that dike cost is proportional to height and damage due to flooding is zero until levees are breached or over topped at which point 100 damage occurs van dantzig 1956 these may be reasonable approximations for traditional dikes protecting flat terrain in the low polders found in the netherlands but are a poor representation of many major cities located on a rising coast or hilly terrain another potential weakness of a van dantzig style model is its focus on a single objective the discounted expected total cost the stakeholder community impacted by flood events and affected by flood risk mitigation efforts is large and diverse with correspondingly diverse values harman et al 2015 bessette et al 2017 associated with multiple and often conflicting sets of objectives harman et al 2015 porthin et al 2013 oddo et al 2017 unfortunately estimated base flood levels or dike heights do not allow city planners to evaluate changes to damage associated with surge heights above the selected return level additionally return periods for these estimated levels are typically long compared to the record of surge observations available grinsted et al 2012 2013 menéndez and woodworth 2010 ceres et al 2017 lee et al 2017 this relative sparsity of data leads to large uncertainties surrounding estimates of long period return levels coles 2001 and the potential for bias in estimating extreme event risks using common extreme value analysis methods coles et al 2003 ceres et al 2017 researchers have developed more complex methods and models for assessing various aspects of storm surge risk superstorms katrina and sandy have motivated substantial investments in storm surge protection for both the new orleans and new york city metropolitan areas respectively fischbach and rand gulf states policy institute 2012 aerts et al 2013 groves et al 2016 aerts et al 2014 the magnitude of proposed investments justified extensive site specific research on both future storm surge risk for these broad geographical regions and for evaluation of several proposals to mitigate that risk to support these efforts researchers have developed state of the art storm surge modeling frameworks these frameworks incorporate hurricane track and intensity prediction models driving hydrological storm surge models incorporate site specific bathymetry and geography that in turn interact with local topography and potential defensive measures to generate simulated inundation levels over a wide area these inundation levels are then matched to detailed demographic data and damage models to produce area wide and location specific estimates of economic impact e g kerr et al 2013 taflanidis et al 2013 flowerdew et al 2010 orton et al 2012 fischbach and rand gulf states policy institute 2012 aerts et al 2014 these models assist decisionmakers and stakeholders exploring the specific impacts of a storm surge risk mitigation strategy because the models realistically quantify storm surge damages that can be expected for a given set of storms in new orleans for example the coastal louisiana risk assessment clara model was developed to estimate flood extent associated flood depths and resulting damages for the entire louisiana coastal region fischbach and rand gulf states policy institute 2012 johnson et al 2013 policymakers can use this approach to evaluate the effectiveness of several risk mitigation strategies in response to specific storms over the modeled region for instance given five storm surge barrier options for lake pontchartrain the clara model was recently used to evaluate the reduction in storm surge damage across 15 counties against 77 storms fischbach et al 2017 in many cases clara style frameworks couple several computationally expensive models for instance clara uses the advanced circulation adcirc and the simulating waves nearshore swan models to develop water and wave height levels throughout the study region computationally these models have long run times that limit the analysis to relatively few storms and tracks fischbach et al 2017 hence using this approach to evaluate the effectiveness of multiple risk mitigation strategies against the full range of possible future storms can be computationally unfeasible this then influences the selection of optimal mitigation strategies additionally the expense associated with creating similar models for other regions will exceed the resources available to many communities storm surge models of intermediate complexity have been developed and can serve an important role in evaluating storm surge risk as one example a simplified hydrodynamic model was developed to calculate inundation levels for the bay of bengal lewis et al 2013 another study used a simple statistical model of storm surges coupled to a digital terrain model and gridded demographic information to assess the impact of sea level rise on copenhagen s risk of catastrophic storm surge hallegatte et al 2011 a third example utilizes a statistical model of storm surges coupled to a model that simulates the evolution of barrier islands which in turn is linked to an agent based economic model that predicts the performance of resort areas dependent upon those barrier islands mcnamara and werner 2008 mcnamara and keeler 2013 however these models are location specific and are limited in their ability to analyze multiple strategies as such the use of intermediate complexity models is infrequent the gaps in storm surge modeling complexity and usage suggests a need for a new framework with different modeling capabilities here we develop the island city on a wedge icow a model framework that bridges the gap between van dantzig and clara style modeling approaches using an idealized geography of a city situated on a rising coastline such as manhattan show in fig 1 the model simulates the increasing damage that occurs with larger storm surges and the distribution of those damages across the city the icow framework can be used to evaluate combinations of multiple risk reduction strategies such as insurance preservation or enhancement of natural barriers construction of physical barriers and sea gates across waterways installation of active measures such as pumps adoption of zoning restrictions withdrawal or relocation of development physical alteration of buildings and resiliency improvements we demonstrate the utility of this model by evaluating combinations of withdrawal building resistance and dikes by evaluating several objectives over differing time scales these objectives can include investment cost investment timing median or maximum annual storm surge damage or the distribution of damage within the city the framework is flexible enough to incorporate other effects such as economic loss associated with withdrawal strategies or potential value shifts associated with construction of levees or dikes icow requires much less run time than the clara model as a result icow is capable of evaluating the efficacy of multiple combinations of storm surge risk mitigation strategies over a wide range of potential futures while computationally inexpensive icow improves upon van dantzig style models in several important respects the framework incorporates typical characteristic features of coastal cities features intended to improve its overall fidelity as a result the icow framework can extend insights gained from clara style models by evaluating many more combinations of defensive strategies against many potentially divergent objectives and over a wider range of future risk scenarios this additional potential insight however is limited to a much narrower spatial range limited to a single community of relatively uniform characteristics and may provide a less realistic representation of specific future outcomes such as the precise amount of damages expected from a specific storm the icow s lower complexity and more limited spatial extent allows for coupling with multiple objective evolutionary algorithms moea to optimize complex combinations of risk mitigation strategies evaluated against multiple and potentially divergent objectives of interest to stakeholder communities because the icow framework is largely self contained and does not demand high resolution geospatial information such as bathymetry data topography ground coverage or utilization it is considerably easier and less expensive to implement for a particular coastal community compared to clara style models the remainder of this article explains the general characteristics and features of the icow framework in terms of an xlrm framework lempert et al 2006 describes the icow computational environment and experimental design presents example applications and discusses results and conclusions 2 icow xlrm framework we describe the icow framework using the xlrm framework for robust decision making lempert et al 2006 fig 2 shows the logical relationships between exogenous factors x model strategy levers l modeling relationships r and performance metrics m exogenous factors x are characteristics of the simulated city environment that are fixed from the perspective of decisionmakers and stakeholders levers l simulate the actions that decisionmakers can implement to affect the city s response to storm surge metrics m define outcomes that are of potential interest to different stakeholders relationships r represent the model logic implemented to simulate the city response in terms of metrics that result from a particular combination of exogenous factors and lever settings for this article and to demonstrate the capabilities of the icow framework in general we use many simplifying assumptions regarding the modeled city s characteristics the icow design and associated parameterizations are intended to represent the general qualities and behaviors of the xlrm components described below peer reviewed literature and quantified data for these categories are often not available and require justification as discussed below additionally location specific information relevant to xlrm categories e g dike construction costs often varies substantially from location to location similarly the simplified topography and demographics of the modeled city e g the regular wedge shape geometry of the modeled community are intended to be representative of a generic coastal community rather than any particular site icow framework results are sensitive to these simplifying assumptions and representative and generic parameter settings 2 1 exogenous factors within the xlrm framework lempert et al 2006 exogenous factors represent any condition that has an impact on the output metrics that cannot be changed by decisionmakers or stakeholders for this article the primary exogenous factors are the initial city parameters and the sequence of storms that will impact the city we incorporate other exogenous factors as model parameters e g the per volume cost to construct a dike or the height of the city seawall see supplemental table appendix c for full details 2 1 1 surge simulation storm surges are simulated as annual highest surge heights generated from a nonstationary generalized extreme value gev model such that the 100 year storm surge is increasing at a rate of 1 m per century due to an increase in both the location and scale parameters ceres et al 2017 some storm surges generated in this manner may be larger than physically plausible storm surges used for this study are clipped such that surge heights exceeding a threshold are capped at the threshold we use a thresholds of 12 m for the first year and increase by 0 01 m per year thereafter this threshold is chosen as a compromise between the goal of accounting for the full range of risk exposure based on the statistical model used to generate the surges and the desire to provide fidelity to physical reality we use 5000 realizations of 50 year sequences of storm surges for all examples discussed in this article this number represents a compromise between computational efficiency and stable results see the supplemental materials section appendix b for additional figures and discussion on the number of realizations used 2 1 2 initial city characteristics many cities e g boston nyc and san diego are situated at the water s edge and consist of a gradually rising terrain in these cases higher surge levels will result in larger areas and greater depths of inundation the waterfront areas in these cities such as manhattan new york city ny fig 1 are often densely packed with tall buildings icow simulates an island city situated on a rectangular wedge the most prominent characteristic of the icow city is the gradually rising elevation of the city with distance from the city s lowest waterfront fig 3 the city terrain is elevated from the normal water level by a seawall icow buildings are uniformly tall and higher than the highest potential surges damage to buildings accrues based on the volume of the building flooded buildings have a basement volume that floods completely when water reaches the level of the building based on volume real estate value is initially assumed to be constant regardless of building height or location relative to the waterfront in the simplest configuration city value and density are uniformly and continuously distributed from water s edge to the highest city elevation as defensive strategies are added this value density may change the addition of defensive strategies divides the city into zones of different damage vulnerabilities as described below 2 2 strategies and levers as discussed in the introduction numerous defensive strategies are available to mitigate storm surge risk for this study we consider one fixed and three adjustable defensive strategies that can be implemented to varying degrees and in combination the fixed defensive strategy is the presence of a seawall around the city the three adjustable defensive strategies are i withdrawal from at risk areas ii improving resistance to damage and iii construction of a dike all strategies are assumed to be implemented uniformly across the width of the city parallel to the coastline see fig 3 a within icow levers are the mechanisms by which defensive strategies are implemented 2 2 1 seawall the icow design assumes an existent seawall of fixed height surrounding the city no damage occurs to the city as long as surge heights are below the level of the seawall when surge height exceeds the seawall the height of water that affects the city is equal to the amount of excess and an additional height caused by wave run up for this study there is no option to increase the seawall height thus icow does not include seawall height as a lever 2 2 2 withdrawal the first icow adjustable strategy is to specify withdrawal from regions of the city below some withdrawal height and relocation to the city s higher levels this height is the single withdrawal strategy lever an actual withdrawal strategy implemented in a city such as new york might involve relocating individual buildings at very low elevations that are the least resistant to flood damage in the model all unmodified buildings have the same resistance to flooding damage thus the most vulnerable buildings are uniformly located at the lowest elevations in the modeled withdrawal strategy it is this lowest fractions of building volume without regard to individual buildings that is withdrawn as withdrawal height increases this modeled representation seems reasonable so long as the area of the city relocated the withdrawal height remains low as the withdrawal height increases the fidelity of the modeled withdrawal strategy to an actually implemented strategy would be expected to decrease in icow we model this relocation of the lowest elevation buildings by creating a zero value zone zone 0 and redistributing the value that had been contained in that zone over the remaining area of the city subsequently and uniformly increasing the value density in the remaining city area 2 2 3 resistance the second adjustable strategy is implementation of resistance to damage for the lowest portions of buildings which are most exposed to flooding there are two levers associated with this strategy the first lever resistance height r is a specified height above the withdrawal height below this level buildings are modified to increase their resistance to flood damage implementing resistance creates a resistant zone zone 1 the upper extent of zone 1 in terms of the city elevation is constrained by the lower of the resistance height or by the location of a dike base building volume flooded below the building s resistance height is subjected to less damage than nonresistant buildings the percentage p of resistance to damage compared to the non resistant damage that results in resistant sections is the second lever resistance strategy lever flooded volumes above the resistance height are subjected to damage at the nonresistant rate 2 2 4 dikes construction of a dike is the third strategy considered for this study as with implementation of the other strategies we assume dikes are constructed parallel to the shoreline because icow is modeled as a wedge shaped island the dike must have sections on either end of the dike to keep surges from flowing around the sides of the dike icow dikes greatly reduce flooding damage to buildings located behind them unless they fail or are overtopped discussed further in section 2 4 3 two levers implement the dike strategies the first lever is the elevation of the dike base b above the withdrawal height the second lever is the height of the dike d the area behind above the dike base but below the dike top are protected by the dike and define zone 3 2 2 5 strategy combinations icow can simulate simultaneous implementation of these three strategies through the simultaneous operation of the five model levers the levers are summarized in table 1 the relationship between strategy levers and resultant zones is illustrated in fig 3a and b 2 3 metrics implementing storm surge mitigation strategies has many consequences and the importance of these consequences may vary among different stakeholders to the extent that these consequences can be measured there are many possible metrics associated with risk mitigation performance as examples these could include implementation costs maintenance costs the cost to repair buildings damaged by flood the direct or indirect economic damage associated with storm driven changes to the economy damage to important city infrastructure probabilities of nuisance flooding the probability of catastrophic events or even deaths some consequences may be more difficult to model such as a strategy s impact on a community s city character and culture icow is able to calculate many metrics corresponding to the objectives of many different stakeholders to simplify the illustration of the impact of allowing for multiple strategies and levers we examine three metrics implementation cost expected damage costs and the total cost which is the sum of investment cost and expected damage cost 2 4 relationships within an xlrm modeling framework relationships convert exogenous factors and lever settings into metrics in this section we discuss the relationships between exogenous factors lever settings and the cost and damage metrics for each strategy in general icow model metrics including cost and damage are based on volume of the city effected in the most basic case where no strategies are implemented cost is zero and damage will be proportional to the value density of the area flooded and the building volume flooded actual building damage from storm surges in general increases with exposure to higher water levels but is also affected by other factors such as wave action flow velocity and duration of inundation fema 2011 2015 for simplicity we model damage based on storm surge height the actual damage function for particular buildings however varies by building usage construction type and more broadly by regional location prahl et al 2018 the icow framework adopts a generic damage model where in an unprotected state damage to a structure starts occurring when the water level exceeds the building base elevation at that level a set damage occurs associated for instance with basement flooding above this level additional damage is proportional to the volume of the building flooded when calculated in this way the damage function which emerges for aggregated areas of the icow city will align with other research into damage functions for urban areas prahl et al 2018 this basic relationship is modified by the employment of strategy levers as discussed below 2 4 1 withdrawal costs and damage relationships conceptually for a given building icow s withdrawal cost can be thought of as the total cost to acquire a new building at a higher location the cost to relocate and the cost to remove the old structure the cost to implement a withdrawal strategy is based on the area to be relocated the value density of that area and the total remaining area in the city available for relocation a fraction of the displaced infrastructure will relocate outside the city the cost to implement withdrawal c w is 1 c w v i w f w c i t y h e i g h t w where v i is the initial city value and f w is a factor intended to adjust for any local conditions such as the presence of historic buildings or heavy industry facilities as well as relocation and demolition costs that might make it more or less expensive to relocate the value of the city after withdrawal v w is 2 v w v i 1 f l w c i t y h e i g h t where f l is the faction of infrastructure that will leave rather than relocate within the city once withdrawal is implemented no damage will occur to city areas below the withdrawal height w zone 0 above this height city density is higher and damage will therefore be proportionally higher per volume when surge heights reach the levels above the withdrawal area 2 4 2 resistance cost and damage relationships modifying buildings to be completely invulnerable to storm surge damage is generally infeasible due to increasing costs and costs vary based on building type and materials intuitively the cost of implementing resistance should rise as the percentage of resistance p incorporated increases and this cost increases sharply as resistance approaches 100 we model this characteristic cost structure using a linearly increasing resistance cost c r per unit value until a threshold percentage is reached as resistance percentage increases above this value we add an exponentially increasing term that increases sharply as resistance percentage approaches 100 so that the cost fraction of resistance with respect to p f c r increases according to 3 f c r f l i n p f e x p m a x 0 p t h r e s h o l d 1 p where f l i n is a factor that controls the linear rate of increase in cost at low percentage increases and f e x p is a factor that controls the exponential rate of increase at low percentages increasing the percentage resistance to damage results in a linear increase in cost as resistance percentage increases above a threshold and approaches one where building volume would be completely invulnerable to surge damage cost per volume increases sharply such that increasing resistance fraction to one would be infinitely costly we assume that the total cost of implementing resistance increases at this rate in proportion to the volume being made resistant but because the width and slope of the city is constant the relationship can be greatly simplified and expressed solely in terms of elevation changes in cases where resistance is not constrained by the presence of a dike resistance cost is 4 c r v w f c r r r 2 b h c i t y e l e v a t i o n w where b is the representative basement depth and h is the city building height in this case there will be an unprotected zone 2 in front of the dike when the resistance zone 1 is constrained by a dike i e r is higher than b then there is no zone 2 and the areas behind the dike are in the area protected by the dike zone 3 no resistance is incorporated behind the dike regardless of the resistance height r when zone 1 is constrained by the presence of a dike the resistance cost is given by 5 c r v w f c r b r b 2 b h c i t y e l e v a t i o n w in the resistant area zone 1 damage is reduced by the resistance fraction for resistant volumes flooded when flooding exceeds the resistance height additional damage accrues at the normal nonresistant rate for volume flooded above the resistant height 2 4 3 dike cost and damage relationships empirical data and peer reviewed literature on dike costs is sparse and indicates a wide range of dike construction costs jonkman et al 2013 thus we make several simplifying assumptions with the aim of modeling the generic case that could be modified as needed to match the particular circumstances of an actual coastal community real dikes have sloped sides or when they have a constant width profile with respect to height d require greater strength at the base compared to the top of the dike therefore with all other factors being equal taller dikes are more expensive than shorter ones zhu and lund 2009 jonkman et al 2013 icow currently models all surge barriers as having sloped sides icow dikes have sloped sides and a flat top thus the volume of a dike increases approximately geometrically with height dikes are modeled as perpendicular to the waterfront because the city is modeled as a wedge shaped island and to prevent surges from flowing around the edges of the dikes icow dikes are u shaped therefore the overall length of a dike will increase based on the additional length of dike required to be constructed on the sloping sides of the city these portions of the dike are irregular tetrahedrons because the city slope s is low the length of the wings is long compared to the dike height which allows for the simplifying assumption that dike wing lengths at the top and bottom of the dikes are equal icow modeled dike cost is proportional to volume additionally large scale projects such as dikes typically incur a fixed startup cost which reflects the costs necessary to plan design and approve projects cost to acquire and prepare dike sites and to make the initial and wrap up costs for large scale projects that are dependent on site location and dike height or volume zhu and lund 2009 jonkman et al 2013 for simplicity we emulate start up cost as a fixed additional height dike volume v d used to calculate cost is based on the a height h which is the sum of the design height and a fixed initial startup height city width w c i t y dike top width w d slope of the dike sides s and slope of the city s according to 6 v d w c i t y h w d h s 2 1 6 h 4 h 1 s 2 s 2 2 h 5 h 1 s s 4 4 h 6 s 2 s 4 4 h 4 2 h h 1 s 4 h 2 s 2 h 2 s 2 s 2 s 2 2 h 3 h 1 s s 2 1 2 w d h 2 s 2 as a result startup costs will be larger but account for a lower percentage of total costs for taller dikes the icow model specifies dikes based on the location of the dike base relative to the withdrawal height and the height of the dike from the dike base cost of the dike c d is calculated by multiplying dike volume v d from equation 6 with the per cubic meter cost of the dike c d p v to yield 7 c d v d c d p v the resultant dike height to cost profile is shown in supplemental fig a 6 when dikes function properly and when they are not overtopped they greatly reduce the damage in the protected zones dikes however will fail when they are overtopped and they may fail for a variety of reasons prior to water levels exceeding their design protection heights tobin 1995 apel et al 2004 sills et al 2008 example reasons for failure include improper design incorrect operation inadequate maintenance or foundation erosion icow represents this nonzero probability of failure with a low fixed probability of failure at all surge heights h s u r g e below a high percentage of the dike height above this threshold t d f probability of failure increases linearly with height until it reaches one at the dike s design height when surge height is less than the threshold 8 p d f h s u r g e t d f d t d f when the dike holds damage in zone 3 is reduced by a fraction of what would otherwise accrue in the dike protected zone if the dike fails or is overtopped damage will accrue in the dike protected zone at a rate greater than the unprotected rate 2 4 4 aggregate damages damage accumulating by zone for a range of surge heights for one combination of mitigation strategies is illustrated in fig 3b damage to each zone d z is calculated based on the value of the zone v a l z the volume of the zone v o l z the volume flooded v o l f and the per volume rate of damage incurred by flooding based on the strategy implemented in the zone f d a m a g e 9 d z v a l z v o l f v o l z f d a m a g e where v o l f depends on both levers and surge height icow features many permutations of this damage function depending upon the lever settings the resulting city zone configuration and the height of the surge relative to the heights of the city zones and implemented protective strategies the icow model parameters used in this paper are inspired by the situation in manhattan in that the building heights are tall relative to potential storm surges dikes are inexpensive relative to the assets protected behind them there is an existing seawall and the breadth of the coastline is long relative to the volume of city protected the model parameters are easily customized such that they can represent a particular city with greater fidelity differences between icow and van dantzig style models are summarized in table 2 at this point we have presented the icow setup and definitions the implementation is described in section 3 1 results are described in section 5 and conclusions are summarized in section 7 3 methods 3 1 icow framework computational description the icow framework consists of two modules the icow module and a set of multi objective evolutionary algorithms moea together these work in tandem to evaluate and optimize multiple risk mitigation strategies 3 1 1 icow module we developed the icow module in c for computational efficiency it consists of one program with two major components executed in sequence the first component takes the exogenous factors associated with the initial baseline city and characterizes the city in terms of the value distribution zones and damage functions described in section 2 4 in response to the chosen strategy lever settings described in 2 2 the costs to implement the strategy lever inputs and city value changes are icow module output metrics the second component takes the city s value distribution zones and damage functions as inputs and evaluates the city s response to the exogenous set of storm surge sequences described in section 2 1 1 to generate the remaining module output metrics for this study the defensive strategies are established at time zero and hence the first component to characterize the city is evaluated only once and the characteristics for the city are set for all storm surges evaluated the impacts of storm surge output metrics can include average cost in dollars over a time span as discussed above flood frequency dike breach frequency or frequency of events over an unacceptable damage threshold for this study we use both the cost of implementing strategies and the total cost of damages over 50 years as the input metrics for the moea discussed below in sections 3 1 2 and 5 3 1 2 moea module as more strategies are considered to provide varying degrees of protection and as the number of objectives increases selecting optimal solutions becomes much more challenging hadka and reed 2013 even with a greatly simplified model compared to clara selecting the best mix of surge risk mitigation strategies for a given set of objectives is nontrivial we use the borg moea to solve the optimization problem because of its high computational efficiency and easy scalability to parallel computing environments hadka and reed 2013 2015 the borg moea is not a single moea algorithm rather it is an auto adaptive class of high performance moea algorithms based on the evolutionary progress of a population of candidate solutions hadka and reed 2013 we use initial borg population sizes of 200 members this initial population evolves over one million functional evaluations to a final larger populations of more than 1000 members for all figures in this article we compared results generated using the borg moea with solutions generated by the nsgaii algorithm deb et al 2000 and find consistent solutions 4 example applications we explore pareto optimal strategies for protecting cities against future storm surges we identify multiple cases with increasing complexity for optimization we start with the relatively simple van dantzig style single objective single lever solution section 4 1 to demonstrate the capabilities of the icow module we then increase the number of objectives to two and increase complexity by examining the one two four and five lever cases 4 1 single objective optimization optimizing van dantzig style models with one lever dike height and one objective to minimize net present cost is simple and in many cases can be solved analytically e g van dantzig 1956 the icow model can be configured to emulate van dantzig style behavior by varying implementation of one strategy with the other strategies held constant i e vary dike height but fix withdrawal height dike setback and resistance height to zero and measuring the net cost that results from every dike height the optimal solution that emerges is a single level defined by an optimal dike height and a resultant net cost see supplemental section appendix a for additional discussion and examples note that a point solution based on lowest net cost may not be realistic decision makers funds available for storm surge risk mitigation investments might for instance compete with funding for mitigation of other risks or funding for other economically valuable investments e g education or sports facilities thus fully funding a van dantzig style economically optimal level may not be possible alternatively other stakeholders may prefer the lower variability and uncertainty in future risk over the more tangible current investment costs in these cases being able to examine higher levels of storm surge risk mitigation investment in terms of reducing the variability of future risk is appropriate 4 2 multiple objective optimization with multiple levers to provide decisionmakers with more options we increase the complexity of the problem by incrementally increasing the number of levers to provide many combinations of strategies and determine pareto optimal two objectives solutions we identify three cases to help illustrate the additional complexities when optimizing across multiple strategies simultaneously for the first case we compare the cost and risk reduction associated with a dike only single lever d strategy with a dike height d and resistance height r two lever strategy in the single lever example a dike can be constructed at the seawall to any height the resultant city consists of zones 3 and 4 in the two lever example policy makers can either construct a dike or incorporate resistance with a fixed percentage p given the structure of the icow model the two levers are mutually exclusive the resultant city then consists of either zones 3 and 4 or zones 2 and 4 for the second case we add two additional levers dike base location b and resistance percentage p and compare results to the first case in this four level example dike height and resistance height are no longer mutually exclusive dikes can be constructed to any height at any location and resistance can be incorporated in front of the dike to any height at any percentage for the third illustrative case we add the additional lever of complete withdrawal from the lowest city levels a five lever example and compare results with the other cases 5 results for the first case we consider when the only solution available is construction of a dike in examining this case with one and two levers see fig 4 a the icow framework identifies an economic barrier to entry situation when no optimal solutions emerge as investment level increases until the investment cost exceeds the dike fixed start up cost in this situtation when the dike is so low the increased damages associated with a failed or overtopped dike exceeds the damages avoided when the dike is not present once this dike investment threshold is reached the framework continues to identify solutions with taller more expensive dikes until additional dike height no longer reduces damages incorporating a resistance option allows policy makers to identify sensible low cost solutions that substantially decrease storm surge damage however once the dike investment threshold is reached the optimal strategy shifts abruptly to increasing dike height this can be seen in fig 4 panel b before the transition point 100 of investment cost is applied to the resistance strategy while after the transition 100 of investment cost is applied to a dike strategy for the second case with four levers the optimal strategy combinations that emerge are superior to the one and two lever examples in that equal or lower damages can be obtained for any given investment level see fig 4c fig 4d shows the shift in the percentage of optimal investment for any given level of investment allocated to each strategy that occurs as the level of investment increases at low levels of investment the optimal strategy is based solely on implementation of resistance at higher investment levels better performance is achieved using a dike only solution at the highest considered levels of investment the optimal solution consists of combining strategies by setting the dike back from the seawall and implementing resistance in the area between the waterfront and dike base in the third case with five levers the additional withdrawal lever allows for the identification of further pareto improvements at the highest investment levels as shown in fig 4 e and f in both the four and five lever cases jagged fractional contribution are evident between 30 60 billion in fig 4 panels d and f these variations occur when the borg moea algorithm identifies diverse solutions with approximately equivalent trade offs between the two objectives damage and investment cost resulting over multiple combinations of lever settings to help visualize the solutions for a given level of investment we can also identify the city elevations for the pareto optimal strategies fig 5 for the five lever example at the lowest levels of investment the lowest cost damage reduction strategy is to implement resistance into an increasing area of the city once a sharp cost threshold is reached the pareto optimal strategy shifts abruptly to implementation of an increasingly tall dike at the city seawall as the amount of investment continues to increase corresponding to the objective to further decrease damage the optimal solution sets the dike back from the seawall at gradually increasing elevations and with slightly increasing dike heights the area between the seawall and the dike is fortified through implementation of resistance first and then as investment continues to increase through a combination of withdrawal and resistance the jagged fractional contributions visible in fig 4 panel f manifests itself in fig 5 as variations in the city elevations for adjacent in terms of investment cost strategies the character of these optimal solutions are highly dependent and sensitive to icow model parameters for instance very small parameter changes to the cost of resistance relative to withdrawal and dike costs can result in surprising and nonlinear changes to the character of the resulting solution sets 6 discussion and limitations simple conceptual storm surge damage models may fail to provide sufficient fidelity in simulating storm surge risk mitigation more complicated regional storm surge damage models improve upon these simple models in terms of realism fidelity and the area covered but they can be difficult to design and implement additionally they are computationally expensive to evaluate multiple combinations of risk mitigation strategies over a wide range of future storm surge risks while considering the differing objectives of various stakeholder communities the combination of these drawbacks may make this approach impracticable for some communities we use the icow framework to analyze an idealized city designed to resemble major cities such as new york every coastal community however is unique thus many icow assumptions such as the assumptions regarding the regular wedge shape the uniform density or the high value density of the island city and parameters such as the costs to implement resistance or the damage functions associated with different zones will not always be appropriate moreover local policymakers are likely to have better local information available as to the proper values for many icow parameters for instance local experience in improving buildings to make them more damage resistant will provide cost information that better reflects local circumstances to make icow useful to city planners at a specific site we must adjust icow to reflect city specific characteristics at a minimum such site specific adjustments need to account for the following factors site specific projections of storm surges local economic demographics and their relationship to the site topography typical building heights and adjustments to damage functions that account for site specific building usage and construction type additionally some simplifying assumptions described in this paper would probably need to be refined examples of these could include the assumption of uniform city density and value with respect to elevation from the waterfront the cost basis for constructing dikes or implementing resistance or the simplifying assumption that a single surge level is the best factor relating projected storm characteristics to damage two objective optimal solutions are sensitive to icow parameters as discussed above but fully evaluating this sensitivity has not yet been accomplished and may be computationally infeasible conducting sensitivity analysis with more than two objectives will likely be even more challenging in the course of adapting icow to a particular community a sensitivity analysis would be centered on the particular combination of assumptions and parameters used in simulating the community under study to provide insight for decisionmakers into critical uncertainties and to focus limited resources towards understanding and mitigating those uncertainties 7 conclusions we develop the icow as a storm surge risk modeling framework of intermediate complexity for decisionmakers and stakeholders the framework is intended to fill the gap between simple conceptual storm surge damage models and realistic but complex and expensive state of the art regional storm surge risk modeling frameworks we demonstrate the capabilities of the icow framework to evaluate and optimize surge risk mitigation strategies ranging from a simple single objective single lever problem to a more complicated and computationally challenging two objective five lever problem decisionmakers can use icow to explore a more comprehensive set of strategies over a wider range of future risk scenarios than previous done with more complicated regional state of the art models insights gained by this approach however will be more limited in geospatial extent to a single area of fairly uniform characteristics and in terms of less realistic representation of specific future outcomes stakeholders can use icow to supplement insights gained from those higher complexity models and illustrate trade offs between conflicting objectives icow provides one degree of spatial resolution in terms of distance from the waterfront and thus represents a compromise between the point single polder solutions provided by van dantzig style approaches and regional gridded results produced by state of the art frameworks icow is relatively simple to modify and implement and so it may be useful to decisionmakers with limited resources but who nevertheless need methods to identify evaluate and illustrate the multiple trade offs implicit in any storm surge risk mitigation strategy the icow framework can be easily modified or extended and can be integrated with other modeling systems such as the building blocks for relevant ice and climate knowledge brick modeling framework wong et al 2017 to further explore storm surge risk in a regional or global context code availability all icow software code used to create the figures in this article are provided as open source under a gnu non commercial licence https github com rceres icow and at http datacommons psu edu supplemental material supplemental material in the appendices provides additional discussion on emulation of van dantzig style model emulation explanations and implications of using gev based storm surges and information on convergence and computational expense associated with changing the number of futures evaluated conflicts of interest the authors are not aware of financial or personal relationships that would pose a conflict of interest acknowledgements we thank d hadka for outstanding technical support with the rhodium multi objective tool kit and the borg moea we thank b lee m haran d titley r lempert and j lawrence for helpful discussions this research was partially supported by the national science foundation through the network for sustainable climate risk management scrim under nsf cooperative agreement geo 1240507 and the penn state center for climate risk management any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nsf errors and opinions are of course those of the authors all authors contributed to the icow conceptual design rc developed detailed model designs wrote all software code and integrated icow software with the borg moea algorithm all authors developed the experimental plan rc designed and executed experiments and conducted analysis of results cf and kk provided guidance for the project rc wrote the article and all authors contributed to editing appendix a van dantzig style model emulation the icow framework is capable of emulating van dantzig style single objective optimization for example the van dantzig style optimal dike height for the illustrative city used in this journal is shown in fig a 6 while the emulation produces a single optimal dike height and a visually similar result there are some differences and the resultant optimal dike heights will not be equal to those obtained directly from van dantzig 1956 the icow van dantzig emulation shown in fig a 6 shows optimization of dike height with respect to the net cost over a fixed 50 year timespan the optimization performed by van dantzig did not consider a fixed time interval van dantzig 1956 his solution comprised an optimal height based on then current analysis of tidal records and considering the net present value of both current investments and damages incurred for the entire span of the future van dantzig 1956 the dike cost model used in van dantzig 1956 is a simple linear relationship to dike height that incorporates no start up costs icow dike cost is based on the wedge shaped geography of the city model the resultant dike volume and a startup cost as described in section 2 4 3 and equations 6 and 7 the resultant relationship between dike height and cost is nonlinear fig a 6 icow emulation of van dantzig style optimal dike height the optimal dike height occurs when the net cost is minimized equating to a dike height of approximately 5 5m fig a 6 fig a 6 shows a gap between a zero dike height and approximately 0 7 m where no solutions exist moreover fig a 6 shows an increase in net cost between the no investment maximum damage point and next optimal point above this gap this gap and subsequent increase in net cost is due to the fixed start up cost of dikes assumed in the icow module that does not occur in van dantzig s model van dantzig 1956 for instance as implemented by oddo et al 2017 additionally because the icow framework increases damage behind a dike if the dike fails or is breached at very low dike heights total damage increases with increased dike height the borg moea algorithm therefore does not identify any optimal solutions in these dike height ranges see discussion in 2 4 3 appendix b relationship between states of the world evaluated and wall time each icow framework state of the world consists of an exogenous 50 year sequence of storm surges the representation of risk associated with the most extreme storm surges improves with more states of the world evaluated evaluating more states of the world however adds computational cost to understand this relationship we measure the computational wall time required to conduct five lever icow optimizations using 1 to 20 000 states of the world for 100 000 500 000 and 1 million functional evaluations fig b 7 we select 5000 states of the world as a compromise between computational efficiency and stable results for use in all experiments described in this article fig b 7 relationship between computational cost and states of the world evaluated the wall time in seconds y axis required to evaluate states of the world where one state of the world is a 50 year sequence of annual highest storm surges x axis fig b 7 appendix c icow model parameters table 3 icow parameters table 3 name value units description v i 1 5 t initial city value b 30 m building height h city 17 m height of the city depth city 2 km distance from seawall to highest point length city 43 km length of the lowest seawall coast h seawall 1 75 m seawall height dike start 3 m equivalent height added to actual height so larger dikes have larger startup costs dike top width 4 m controls the width of the dike top to adjust dike volume s 0 5 m slope of the dike sides dike cost 10 m3 dike cost per volume dike value ratio 1 1 n a controls increase in value for areas protected by a dik unprotected ratio 1 0 n a controls increase in value for areas protected f w 1 0 none withdrawal factor adjusting cost to relocate withdrawal fraction 0 01 n a fraction of population and infrastructure that will leave if withdrawn f lin 0 35 n a linear factor relating percent resistant to resistance cost f exp 0 9 n a factor for exponential cost increase when percent resistant t exp t exp 0 6 n a percent resistant threshold above which resistance cost increases exponentially basement depth 3 0 m representative depth associated with damage occurring when surge reaches building futures evaluated 5000 n a the number of 50 year storm surge sequences used for each borg functional evaluation borg 5000 k borg epsilon for 1 and 2 levers epsilon 500 k borg epsilon for 4 and 5 levers pop initial 200 n a initial borg number of icow cities nfe 100 k number of borg functional evaluations table 4 icow damage algorithm parameters table 4 parameter value units description f damage 0 39 none fraction of inundated buildings damaged protected damage factor 1 3 none increased damage that occurs in protected areas if dike fails t df 0 95 none dike height fraction above which failure probability increases with surge height threshold level 1 375 none fraction of city value above which damage increases more rapidly wave runup 1 1 none surge factor when surge exceeds seawall height 
26184,in this paper we focus on potato late blight control in the netherlands to analyse the social ecological interactions between farmer behaviour and disease dynamics an agent based model was developed to analyse the use of crop resistance for sustainable disease control the framework on farmers decision making was based on a behavioural theory and supported by data from literature and interviews with dutch potato farmers this framework was integrated with a previously developed spatially explicit model on potato late blight dynamics we assumed a scenario where a new resistant potato variety was introduced to the market the model reproduced a boom and bust cycle the percentage of farmers growing the resistant variety increased until resistance breakdown occurred by emergence and spread of a virulent strain and in response farmers switched to other potato varieties and management strategies several factors and processes were identified that could contribute to the development of sustainable disease management strategies keywords phytophthora infestans social ecological systems resistance management consumat 1 introduction one of the main challenges in global food production is to control upcoming pests and diseases mack et al 2000 examples of emerging infectious diseases of crops include banana xanthomonas wilt and wheat rust vurro et al 2010 these diseases can have huge impacts on human well being economy and biodiversity control of the disease is the result of an interplay between pathogens hosts and actors e g farmers governments and researchers the actors in the system can have divergent approaches in disease control as a result of different perspectives and objectives in this paper we focus on potato late blight control to analyse interactions between farmer behaviour and disease dynamics phytophthora infestans the causal agent of late blight first arrived in europe in 1845 where it was responsible for the irish potato famine in which one million people died and another one million people emigrated p infestans has a high evolutionary potential and as new strains evolve also new outbreaks of the disease emerge causing devastating epidemics globally anderson et al 2004 because the disease is dispersed by wind control strategies should be analysed at the landscape level in this project we focus on the netherlands which is a large producer of seed ware and starch potatoes haverkort et al 2008 the high potato density and favourable weather conditions for the disease moderate temperatures and high humidity result in frequent outbreaks of the disease currently the use of fungicides is the most important control method but these are harmful for the environment the use of resistant varieties could improve sustainability of late blight management however as a result of pathogen adaptation new virulent strains can emerge resulting in resistance breakdown previously we described late blight management in potato production as a social ecological system which is driven by interrelated social and biophysical processes that interact across multiple temporal and spatial scales pacilly et al 2016 since the disease incidence in the landscape is influenced by biophysical processes as well as crop management strategies it is important to focus not only on epidemiological processes but also on decision making concerning disease management in this system farmers play a key role since they decide on crop management their management strategies affect the disease pressure in a landscape and the sustainability in terms of environmental pollution and breakdown of disease resistance which can be considered as a common good secondly farmers also respond to conditions influences and changes in the socio institutional setting e g policies markets extension and peer to peer communication and the biophysical environment e g soil and weather and they adapt their management strategies based on past experiences therefore to identify effective and sustainable late blight management strategies it is important to consider the social ecological interactions previously we described the social ecological system of potato late blight and used modelling for analysing the effect of management strategies on disease control pacilly et al 2016 2018 model scenarios were discussed in workshops with farmers pacilly et al 2019 section 1 1 describes the findings from previous work in this paper we focus on the social ecological interactions by adding the dimension of decision making by multiple interacting farmers in a landscape in the model exploring the interactions and feedback mechanisms related to farmers decision making and late blight dynamics can increase understanding of the system behaviour and contribute to the development of effective management strategies and policies agent based modelling was used to simulate the social ecological interactions agent based models have been recognized as a useful tool to analyse human decision making in a spatial environment in which biophysical processes occur an 2012 agent based models consist of heterogeneous entities which interact with each other and the environment and is therefore very suitable for simulating individual decision making agents this is very relevant in the context of agricultural systems where individual households are the main decision makers but can strongly differ agent based modelling can therefore contribute to improved understanding of farmers behaviour in response to changing environmental economic or institutional conditions huber et al 2018 furthermore farm management includes both short and long term decisions for example decisions on crop selection are made yearly as a result of annual production cycles while the use of inputs such as fungicides or fertilizers are made on a daily basis during the growing season also with respect to disease dynamics processes occur at different temporal scales disease is spread during the growing season but breakdown of resistance can take multiple years both short and long term processes are important characteristics of the system agent based modelling is a useful tool for simulating and analysing these complex interactions at different scales during the last decades the number of studies using agent based modelling to couple social and natural systems has rapidly increased reviews on the use of models in social ecological systems research are provided by an 2012 filatova et al 2013 matthews et al 2007 parker et al 2003 and schlüter et al 2012 one of the challenges that was identified includes the integrating of social and ecological systems parker et al 2008 the number of models that is able to simulate two way feedbacks between human and environmental subsystems are scarce while this is essential for studying non linear interactions between human and natural systems filatova et al 2016 in this study we aim to contribute to this field of research by developing a model framework which integrates farmer behaviour and disease dynamics to simulate human behaviour several methods have been used when data on decision making is scarce or missing theories can be used and the implications can be confronted with empirical data groeneveld et al 2017 the most common theory to simulate human behaviour is to assume rational decision making also referred to as the homo economicus groeneveld et al 2017 schlüter et al 2017 according to this theory agents have perfect knowledge and make calculations to identify the optimal decision that maximizes their utility or profit however studies on farmer behaviour have shown that farmers are also influenced by many other factors such as peer networks individual preferences and culture austin et al 1998 edwards jones 2006 willock et al 1999 in recent years agent based models have become more popular for modelling agricultural systems and the impact of policies groeneveld et al 2017 huber et al 2018 however there is need for improved representation of farmers heterogeneous decision making for models to be relevant tools for policy assessment better understanding of farmers decision making is needed reidsma et al 2018 current european agricultural agent based models lack consideration of values social interactions norm consideration and learning in farmers decision making huber et al 2018 furthermore the representation should consider the socioeconomic and natural environment as well as farm household characteristics in our study we used the consumat approach to simulate farmers decision making on late blight management jager and janssen 2012 jager et al 2000 janssen and jager 2001 this framework was selected to develop more behaviourally rich agents the consumat approach includes elements which have been recognized as important factors influencing farmers decision making such as social interactions through a network it also allows the implementation of a heterogeneous farmer population the framework was first developed to explore consumer behaviour but is now widely used in many fields of research including farmers decision making malawska and topping 2016 speelman 2014 van duinen et al 2016 the consumat approach incorporates aspects from a range of behavioural theories such as theories on human needs motivational processes social comparison theory social learning theory and reasoned action theory according to the consumat approach agents engage in different behavioural strategies dependent on their level of satisfaction and uncertainty these behavioural strategies are repetition imitation optimisation and social comparison fig 1 the advantage of the consumat approach is that it is a highly formalized theory which allows easy implementation in an agent based model and only few assumptions have to be made the framework was combined with the previously developed ecological model to simulate the use of crop resistance in disease control by analysing the adoption of the resistant variety by farmers and the durability of resistance over time therefore we assumed a scenario in which a new resistant variety is introduced to the market previously we described that acceptance of resistant varieties is low because these varieties do not yet meet all the requirements of farmers and retailers pacilly et al 2016 in the model this was reflected in a lower yield level of the resistant variety the purpose of the model is to explore the social ecological interactions to identify factors that could be important in the development of sustainable disease management strategies we explored several scenarios that could affect the selection of management strategies by farmers and consequently resistance durability in the following sections we present a more detailed description of the model the scenario analysis and the results in the discussion we evaluate the model findings the implications for disease management and steps for further research 1 1 previous work in a previous study we developed a model to analyse the interactions between late blight management strategies disease dynamics and the abiotic environment at landscape level pacilly et al 2018 the model was used to show opportunities and risks related to the use of plant resistance in disease control growing a resistant variety can reduce disease incidence in the landscape however in the long run resistance breakdown was observed by emergence of a new virulent strain due to pathogen adaptation the durability of resistance was affected by the fraction of resistant fields in the landscape and it was found that low 0 2 as well as high 0 8 proportions of resistant fields could increase resistance durability according to the dispersal scaling hypothesis disease dispersal is affected by habitat size and dispersal distance where an increase in habitat size is described as a positive dispersal force and an increase in dispersal distance as a negative dispersal force skelsey and garrett 2013 a larger fraction of resistant fields means a lower fraction of susceptible fields which results in a lower disease incidence in the landscape and reduces the risk that virulent spores emerge virulent spores are the result of mutations during spore production in infections in susceptible fields however with higher fractions of resistant fields virulent spores also have an increased chance of arriving in these fields because of a lower dispersal distance between susceptible and resistant fields as a result of the interactions between dispersal distance and habitat size the risk on infections in resistant fields is highest with about equal proportions of susceptible and resistant fields in the landscape the previously developed model was adapted and used in workshops with conventional and organic farmers to demonstrate and discuss the potential role of resistant varieties for effective and sustainable control of late blight pacilly et al 2019 several scenarios were presented that showed the use of crop resistance and fungicide application in disease control at the landscape level farmers that attended the workshops in total 51 farmers were able to recognize the processes and patterns emerging from the model the use of model based scenarios in workshops was very useful to increase farmers knowledge of the system and served as a good starting point for discussions among participants 2 material and methods an agent based model was developed to simulate processes on crop growth disease dynamics and farmer interactions and decision making on disease management in an agricultural landscape over time the model can be described as a so called midrange model the aim is neither to exactly model the situation in a certain region nor to make a purely theoretical point gilbert 2008 the purpose of the model is to increase understanding of the system behaviour rather than making predictions for the future this study is a first step in the development of a social ecological model on late blight control in this model version we focus on the integration of model components but the individual processes were kept as simple as possible in future research the model processes could be further extended steps for further research are described in the discussion section 4 6 the model was implemented in netlogo version 5 2 0 wilensky 1999 a version of the model is available on the openabm website http www openabm org below we present an overview of the model with a description of the main model processes a detailed description of the model following the odd protocol overview design details can be found in appendix a grimm et al 2006 2010 for detailed information on the epidemiological framework we refer to pacilly et al 2018 the framework on farmers decision making was based on the consumat approach jager and janssen 2012 jager et al 2000 janssen and jager 2001 and supported by data from literature and interviews with dutch potato farmers in total 25 farmers were interviewed including 18 conventional and 7 organic farmers pacilly et al 2016 semi structured interviews were carried out on topics such as general farm characteristics the social network late blight management strategies and the use of late blight resistant varieties as a result of the semi structured interviews only qualitative data was available the main results are described in pacilly et al 2016 in the model description section 2 1 we refer to the interviews when the data supports the model framework furthermore data from the interviews was used to validate the model results which is described in the discussion section 4 2 1 model description 2 1 1 model overview the model represents a square agricultural landscape of 10 km 10 km fig 2 the potato density 24 and the mean field size 7 ha were derived from landscape data of an agricultural region in the netherlands the noordoostpolder and these parameters were used as input for the model the grid cells represent a square area of 200 m 200 m 4 ha and are clustered in agricultural fields crop rotation was not included in the model crop rotation can affect the number of initial infections however in the model the fraction of initial infected potato fields was kept constant between years the spatial location of infections were randomly selected which reflects randomizing the location of potato fields as a result of crop rotation the model is populated by 350 farmers each of whom manages one potato field a network was initialised in which farmers are connected to the closest farmers around them shortest spatial distance farmers use one of the following late blight management strategies they can grow a susceptible or resistant late blight potato variety with or without the use of fungicides these strategies have different effects on field and landscape performance field performance is analysed for criteria infection level yield and income landscape performance also relates to disease dispersal and resistance durability a conceptual overview of the model is shown in fig 1 we simulate the growing season from may 1 to september 30 for 50 years crop growth and disease dynamics are updated at a daily time step at the end of each growing season farmers analyse their field performance and select a management strategy for the following year the decision making framework to select a management strategy is based on the consumat approach jager and janssen 2012 janssen and jager 2001 based on their field performance farmers determine their satisfaction and uncertainty level which results in one of the following behavioural strategies repetition imitation optimisation or social comparison see section 2 1 5 the decision making process is influenced by personal characteristics including their need satisfaction and uncertainty tolerance level four farmer types are distinguished which differ in the weights assigned to the criteria table 1 the weights represent the importance of the different criteria to the farmer interaction between farms is related to social interaction with respect to farmers decision making and spatial interactions related to disease dispersal crop growth and late blight dynamics are simulated at grid cell level we consider only one susceptible variety and one resistant potato variety with a single resistance gene in our model we assume the resistant variety has a lower potential yield compared to the susceptible variety which is reflected in the crop growth parameters late blight resistant potato varieties currently available on the market have a moderate yield level however the yield potential may increase in the future with the introduction of new resistant varieties bionext 2017 at the start of each growing season the infection is initialised in a fraction of the potato grid cells randomly selected when a grid cell is infected spores are produced that are dispersed to nearby cells up to a distance of 1000 m where they can cause infections two types of late blight are distinguished the wild type and the virulent strain the wild type can only infect the susceptible variety while a virulent strain can also infect the resistant variety at the start of the simulation only the wild type is present the virulent strain may emerge during the growing season as the result of mutations during spore production the ratio between the wild type and virulent strain at the end of the growing season was used to calculate the number of initial infections of the wild type and virulent strain in the following year since late blight development and crop growth are weather dependent we used measured weather data from may 1 to september 30 152 days for 36 years 1981 2016 as model input these years represent variable weather conditions for crop growth and late blight dynamics to simulate crop growth mean daily temperature and total radiation was calculated and used as input for the model secondly based on calculation rules using hourly temperature and relative humidity during a 24 h period we determined whether a day was suitable for sporangia to cause infection skelsey et al 2009 expansion of existing lesions occurs every time step but new infections as a result of spore germination can only occur on so called blight days for more details we refer to pacilly et al 2018 at the start of each year a dataset is randomly selected out of these 36 years of weather data 2 1 2 late blight management strategies in the model in the netherlands the use of fungicides is currently the most widely used method in the control of late blight while crop resistance has been identified as an important strategy for more sustainable control haverkort et al 2008 lammerts van bueren et al 2008 combining these two types of disease control results in the following four strategies growing a susceptible potato variety with sus or without fungicide application sus or growing a resistant potato variety with res or without fungicide application res in the model farmers select one of these strategies for their potato field fig 1 the management strategies can be related to current farm practices of conventional and organic farmers in conventional agriculture mainly susceptible varieties are grown combined with fungicide application sus since chemical control is not allowed in organic potato production and not enough resistant potatoes are available to supply the whole market bionext 2017 organic farmers grow a combination of susceptible and resistant varieties and do not apply fungicides sus and res in years with early outbreaks of the disease organic farmers can therefore suffer high yield losses in susceptible fields but in years with a low infection pressure farmers can make a profit combining a resistant variety with reduced use of fungicides res has been proposed to prevent resistance breakdown and to increase resistance durability haverkort et al 2016 this strategy has also been described as part of integrated disease management to reduce the use of chemical fungicides kirk et al 2005 mundt et al 2002 nærstad et al 2007 although we do not distinguish organic and conventional potato production in the model the late blight management strategies represent the different approaches in late blight control in case fungicides are used on susceptible and resistant fields sus and res weekly application is assumed starting at the day of crop emergence weekly application is standard practice for many potato farmers also because it is combined with the application of other chemicals for other diseases decision support systems dss are available that can help farmers to improve efficiency of spraying by optimising the use and timing of fungicide application however many farmers do not strictly follow their advice over the season farmers use different type of chemicals in the model we distinguish preventive and curative fungicides we assume preventive fungicides are applied at the start of the growing season to reduce the infection efficiency of the spores when the disease severity in potato grid cells reaches 1 curative fungicides are applied which have a similar effect on the infection efficiency but also reduce the expansion of existing lesions to prevent spread of the disease during the growing season the government has implemented a policy that regulates maximum late blight disease thresholds nvwa 2008 at an estimated disease severity of 5 in the field the potato haulm has to be destroyed an inspection system was set up including an anonymous hotline that could fine farmers in case these regulations were not followed following these regulations we assume that all farmers in the model destroy the potato haulm when the disease severity in potato grid cells reaches 5 this means that leaf and tuber growth stops directly and the disease can no longer disperse to other fields in case of early outbreaks of the disease this can cause severe yield losses besides these strategies also other strategies exist that could be used in late blight control such as pre sprouting to reduce the growing period soil management and chemicals against infections in tubers pacilly et al 2016 however some of these strategies only have a small effect while others are too complex to implement in the model to allow integration with processes on decision making we only focus on fungicide application and crop resistance in late blight control at this stage as a result of advancements in potato breeding it is expected that new late blight resistant varieties will enter the market the coming years therefore it is very relevant to analyse the use of crop resistance in disease control 2 1 3 analysing field performance in the model we implemented three criteria to evaluate the field performance of farmers late blight infection level potato yield and income infection level is an important criterion to evaluate the effectiveness of the selected management strategy and infection in a field can also reduce potato yield and farmer income furthermore late blight management strategies include trade offs between infection level yield and income the application of fungicides can reduce the risk of infection and prevent yield losses but causes additional costs growing a resistant variety can reduce the risk of infection but has a negative effect on potato yield and income since we assume that the resistant variety has a lower potential yield compared to the susceptible variety in addition growing a resistant variety creates a risk on resistance breakdown by emergence of a new virulent strain to evaluate the field performance in relation to late blight management strategies it is therefore important to consider all three criteria secondly the importance of these criteria also differs between farmers table 1 2 1 3 1 infection level in the model late blight severity percentage of infected leaf tissue within a field is affected by the weather conditions disease dispersal and late blight management strategies fungicide use and crop resistance the simulated disease severity ranged from no or very low disease severity to a very high disease severity 10 4 to 5 development of the disease at field level is limited since we assume that the potato haulm is destroyed when disease severity reaches 5 in practice in the field disease severity is generally estimated visually because the human eye can estimate low and high disease severity more precisely than mid range levels it is proposed to correct this by using a logarithmic rather than a linear scale cooke 2006 taken these factors into account a scale from 1 to 4 was developed to analyse the infection level using data on disease severity 1 0 1 2 0 1 1 3 1 5 4 5 2 1 3 2 potato yield at the end of the growing season the mean potato yield tonnes ha of farmers is calculated potato yield is affected by the potato variety weather conditions including temperature and radiation and infection with late blight in the model we assume that the resistant potato variety has a 20 lower potential yield compared to the susceptible variety 2 1 3 3 income we use a standard approach to calculate farmer income farmers gross margin ha 1 is based on the actual potato yield times the price for potatoes minus production costs with respect to production costs we only consider the costs related to fungicide application since we focus on comparing late blight management strategies costs for fungicides are related to the number of applications the type of fungicides used machinery labour and fuel the mean number of applications per farmer was calculated over the year assuming weekly application and the costs per application fc were estimated at 50 ha 1 haverkort et al 2008 the price for potatoes was set on 13 per 100 kg which was derived from a dataset on potato prices for conventional ware potatoes in the netherlands between 2000 and 2017 wur 2018 the same price was used for the susceptible and resistant variety correlations between potato price and overall potato yield were not included because input and output markets are beyond the scope of this regional study crop prices were assumed to be constant over time in reality the crop price is affected by many other factors such as market fluctuations and the market farmers produce for e g organic conventional frozen and fry table and fresh haverkort et al 2008 pavlista and feuz 2005 furthermore to prevent price risks most farmers have contracts with trading companies on crop prices since we focus on comparing differences in income related to late blight management strategies these factors were not included in the current model version as a result of this simplification values on income do not represent actual numbers 2 1 4 farmer population many studies have shown the importance of social interactions within networks in decision making processes also with respect to dutch farmers oerlemans and assouline 2004 van duinen et al 2016 according to the consumat approach agents are influenced through interactions within networks when they engage in social comparison or imitation unfortunately no empirical data was available on social networks among dutch potato farmers however previous results from interviews showed that potato farmers influence each other and copy each other s behaviour pacilly et al 2016 since farmers have social interactions e g as neighbours friends and in study groups and they spend much time on their land they are well aware how surrounding farmers manage their crops potato late blight disperses by wind so infections in neighbouring fields can increase the risk of infection with respect to social interaction on late blight control we therefore assume farmers are in a network with the closest farmers around them shortest distance between fields in each model run a network is initialised in which farmers are connected to the closest farmers around them with a mean number of 5 links per farmer representing a social network of neighbours we explored the effect of alternative network structures appendix b increasing the mean number of links per farmer from 5 to 10 links did not affect the model results as well as a different network setup in which farmers are connected to the closest farmers around them of the same type assuming that you interact more with people who are more similar to you to create a heterogeneous population characteristics of farmers are varied within a certain range farmers uncertainty tolerance level and need satisfaction are randomly selected between 0 and 1 the weights represent the importance of the criteria to the farmer in their decision making based on these preferences farmers have different objectives resulting in four farmer types yield maximizer profit maximizer risk averse farmer and neutral farmer at the start of the simulation the farmer type of each farmer is randomly selected profit and yield maximizing farmers are types previously described in the literature malawska and topping 2016 furthermore studies have shown that farmers can have different risk attitudes which influences decision making on disease control mcroberts et al 2011 willock et al 1999 gardebroek 2006 showed that dutch farmers have different risk attitudes which indicates that farmers manage risk differently organic farmers are less risk averse than conventional farmers but also within these groups risk attitudes were heterogeneous risk perception in late blight control can for example be related to the size of potential negative impact of infection on yield and income pacilly et al 2016 risk averse farmers would adopt strategies that reduce the risk of infection such as resistant varieties or fungicides even when it comes at the expense of profit finger et al 2017 risk aversion can also result in the overuse of fungicides as was observed in the netherlands skevas et al 2012 2013 in the model the risk averse farmers therefore aim to minimize the infection level in their field last we distinguish a neutral farmer which does not have a preference for one of the criteria and the weights for the criteria are set equal table 1 for the other farmer types the weight of one of the criteria was set at 0 8 depending on their objective and for the other two criteria at 0 1 the weights therefore represent clear differences in objectives between farmer types 2 1 5 behavioural strategies according to the consumat approach behaviour of agents is affected by the levels of satisfaction and uncertainty to determine farmers satisfaction and uncertainty the actual ai potential pi and predicted ei performance is calculated for the three criteria infection level yield and income the potential field performance is the maximum result which could be achieved in a specific year without any losses as a result of yield limiting and yield reducing factors van ittersum and rabbinge 1997 in our model we only consider losses as a result of infection with the disease the potential yield of both potato varieties is output of the model and based on the temperature and radiation in a specific year the potential income is calculated in the same way as the actual income but using the potential yield the potential infection level was set at 1 for all four strategies which represents the lowest level of disease severity that could be achieved farmers also estimate the field performance for the coming year for each performance criterion they calculate the mean value using historical values of their own field for the last five years to create a list of reference values the model is run for five years before the actual simulation starts see appendix a in the model satisfaction is defined as the ratio between the actual and the potential performance and uncertainty as the ratio between the actual and the estimated predicted performance for each performance criterion the satisfaction and uncertainty is calculated the overall satisfaction st and uncertainty ut is based on the result for each criterion i influenced by the weights wi equations 1 and 2 1 s t w i a i p i 2 u t w i a i e i based on the consumat approach farmers compare their total satisfaction and uncertainty level with their personal need satisfaction and uncertainly tolerance level if the results are below their thresholds farmers are uncertain and or unsatisfied based on these results farmers engage in one of the following behavioural strategies indicated below in italics the consumat approach is a highly formalized theory which describes very detailed how agents make decisions according to their behavioural strategy the consumat approach could be easily applied to farmers decision making on late blight management strategies if a farmer is satisfied and certain he will repeat its current behaviour and continue using the same management strategy repetition if a farmer is uncertain he will interact with other farmers in his network to make an informed decision agents who are uncertain are more likely to engage in strategies that involve interactions while agents who are certain are more likely to rely on their own experiences if a farmer is uncertain but satisfied he will engage in imitation in this case he will adopt the management strategy that is used by the majority of farmers in his network when farmers are unsatisfied they engage in strategic decision making optimisation or social comparison this strategy relates to rational decision making in which farmers are aiming to optimize their field performance in relation to their preferences first they select the criterion which they want to optimize infection level yield or income see section 2 1 3 and 2 1 4 this is based on the satisfaction level for each criterion and on the criteria weights table 1 in case farmers are unsatisfied and uncertain they engage in social comparison in this strategy farmers analyse the field performance of the farmers in their network and adopt the management strategy of the farmer that has the highest score for the specific criteria when farmers are unsatisfied but certain they engage in optimising behaviour optimisation in this case they compare the mean field performance of all management strategies of the last year and adopt the management strategy that has the highest result for the criteria they want to improve when the resistant variety with and without fungicides have the same score for a criterion it is assumed that farmers select the resistant variety without fungicides 2 2 scenario analysis we assumed a situation where a new resistant potato variety was introduced to the market we analysed the effect on disease control by adoption of the resistant variety by farmers and the durability of resistance at the start of the simulation all farmers are growing a susceptible variety and the majority applies fungicides 90 three scenarios were explored in which we analysed the effect of higher fungicide costs and higher yield or potato price of the resistant variety these changes represent possible future scenarios as a result of actions by stakeholders first we increased the yield potential of the resistant potato variety so it is similar to the susceptible variety yield scenario breeding companies continue to develop new late blight resistant potato varieties and it is likely that in the future new resistant varieties will be introduced with higher yield levels secondly as a result of stakeholder cooperation the price for resistant varieties could increase in the future in the standard settings the crop price of resistant and susceptible varieties was the same recently the organic sector made an agreement to upscale the production of resistant varieties to completely service the organic market over the coming years bionext 2017 an increase in demand could also result in a higher crop price in the price scenario we therefore increased the price of the resistant variety by 25 in the third scenario fungicide scenario we doubled the price per fungicide application from 50 ha 1 to 100 ha 1 about half of all fungicides applied in the netherlands are used in the control of late blight the environmental costs are related to the pollution of groundwater energy costs for application and negative effects on human health haverkort et al 2008 increased environmental awareness could possibly lead to higher prices for fungicides for example when government increases taxes economic instruments such as pesticide taxes can contribute to an optimal pesticide policy finger et al 2017 skevas et al 2013 we analysed how these changes could affect the adoption of management strategies by farmers and the control of late blight to analyse the model results a number of output variables were calculated at the end of each growing season we recorded the behavioural strategies and management strategies of farmers as well as the mean performance per strategy for the criteria infection level yield and income to analyse disease dynamics we calculated the disease incidence the percentage of infected potato grid cells with a disease severity 1 skelsey et al 2010 and the infected resistant fields the percentage of resistant potato grid cells in the landscape infected with the virulent strain we also recorded the year infections in resistant fields were observed followed by establishment of the virulent strain in the population year of resistance breakdown this occurs as a result of between year survival of the virulent strain resulting in initial infections in the following year for each scenario simulation runs were repeated 100 times 3 results 3 1 dynamics over time 3 1 1 example of two model runs for the analysis of the results we observed the patterns that emerged from the model patterns are described as observations of any kind showing non random structure and therefore containing information on the underlying mechanisms grimm et al 2005 after 50 years of simulation we observed two different patterns in the first pattern at some moment during the simulation infections in resistant fields were observed and the virulent strain established in the population fig 3 e while in the second pattern this process was not observed fig 3k so in the first pattern resistance breakdown occurred while in the other pattern resistance remained effective during the simulation time to analyse the dynamics over time an example of one model run of both patterns is shown in fig 3 from the start of the simulation in both patterns the number of susceptible fields decreased and the number of resistant fields increased fig 3a and g susceptible fields without fungicide application had a high risk of infection and yield and income are fluctuating strongly as a result of the weather conditions that affect spread of the disease fungicide application on susceptible fields could not prevent infection completely but no large losses in yield and income were observed due to infection with late blight farmers were unsatisfied with their field performance which led to optimising behaviour fig 3f and l farmers that optimized on infection level adopted the resistant variety since this strategy scored better on infection level as a result of crop resistance fig 3b and h in the model it was assumed that farmers won t apply additional fungicides on the resistant variety when the resistance is effective so farmers adopted the resistant variety without fungicides in both simulation runs after a couple of years a small percentage of resistant fields was infected by emergence of a virulent strain fig 3e and k however in pattern 2 the virulent strain was not able to spread and establish in the population a small number of farmers responded to this event and switched to the resistant variety with fungicide application fig 3g in pattern 1 after 8 years the virulent strain was able to establish in the population and the percentage of infected resistant fields rapidly increased over time figs 3e and 4 as a result of the relative high percentage of resistant fields 20 the virulent strain could spread fast through the landscape which gave farmers a very short time to adapt when the percentage of infected resistant fields reached 55 the percentage of farmers growing a resistant variety without fungicides started to decrease the spread of the virulent strain led to simulated losses in yield and income of resistant fields which resulted in reduced farmer satisfaction and increased uncertainty and farmers switching to other management strategies mean yield and income of resistant varieties without fungicide application decreased by 25 table 5 social comparison and imitation mainly led to the adoption of the susceptible variety with fungicide application since this management strategy is used by the majority of the farmers and resulted in a lower infection level and higher yield and income optimising behaviour on infection level led to the adoption of the resistant variety with fungicide application however because the virulent strain was already present in the population additional fungicide application only slowed down infection within the field but could not eradicate the virulent strain from the landscape as a result the infection level in fields with the initially resistant variety increased and yield and income of resistant fields without fungicides decreased since the resistant variety had a lower potential yield compared to the susceptible variety at some point the yield of resistant fields without fungicides dropped below the yield of susceptible fields without fungicide application within five years after the first resistant fields were infected almost no farmers were growing the resistant variety without fungicides anymore the majority of farmers adopted the susceptible variety with fungicides and a small number of farmers the resistant variety with fungicides fig 3a the resistant variety with fungicides had a lower level for yield and income but in some years had a lower infection level after resistance breakdown the disease incidence in the landscape was highly variable per year a small fraction of farmers remained unsatisfied and or uncertain resulting in social comparing optimising and imitating behaviour however no alternative strategies were available that led to a significant improvement in the simulation runs in the baseline scenario where resistance was still effective after 50 years the percentage of farmers growing a resistant variety stabilized at 19 5 3 1 sd table 2 this was also observed in the example shown in fig 3b the majority of farmers was growing the susceptible variety with fungicide application 79 0 2 4 most farmers were satisfied and certain about their field performance and engaged in repeating behaviour which resulted in a stable situation with respect to late blight management strategies after 50 years of simulation no strong spatial pattern in management strategies was observed clusters of farmers growing the resistant were observed as well as farmers growing a resistant variety surrounded by susceptible fields fig 2 this shows that besides spatial processes including disease spread and the social network also individual characteristics of farmers have a strong effect on decision making when resistance breakdown occurred the spatial distribution of management strategies seemed more random since the majority of farmers 92 is using the same strategy susceptible variety with fungicides table 2 3 1 2 time until resistance breakdown the year of emergence of infections in resistant fields followed by establishment of the virulent strain in the population was analysed table 3 resistance breakdown was observed in 73 of the model runs in 42 of the model runs establishment of the virulent strain occurred in the first 10 years after the introduction of the resistant variety followed by an additional 17 during the ten following years once this period had passed the risk of establishment decreased in 27 of the model runs the resistance was still effective after 50 years the first years after introduction of the resistant variety a larger number of farmers was growing a susceptible variety without fungicide application these fields could act as sources of infection and there was an increased risk that the virulent strain emerged in the year the virulent strain established in the population the mean percentage of farmers that used a susceptible variety without fungicides was 2 0 1 5 after this first period the risk of establishment decreased as a result of a lower fraction of susceptible fields without fungicides however as long as farmers are present that grow the susceptible fields without fungicides resistance breakdown can occur which was also observed in the model after 30 or 40 years in a sensitivity analysis we varied the initial fraction of farmers growing a susceptible variety without fungicides appendix c the results showed that increasing the initial fraction of farmers growing a susceptible variety without fungicides increased the risk on resistance breakdown and establishment of the virulent strain mainly during the first ten years of the simulation which shows that these fields act as an infection source of the virulent strain however no strong affect was observed on the management of farmers at the end of the simulation appendix c 3 1 3 farmer characteristics we analysed the personal characteristics of farmers per management strategy at the end of the simulation table 4 the results show how farmer characteristics affect decision making and the adoption of management strategies during the simulation farmers growing a susceptible variety without fungicides mainly had a low need satisfaction 0 35 and uncertainty tolerance level 0 31 as a result they mainly engaged in repeating behaviour and the farmers growing the susceptible variety without fungicides from the start of the simulation were less likely to change their strategy since this strategy resulted in most years in a higher infection level lower yield level and lower income compared to the other strategies table 5 almost no farmers adopted this strategy during the simulation farmers growing a resistant variety with or without fungicides had a relative high need satisfaction 0 72 0 75 and also a high value for the weight infection level 0 62 which shows that these were mainly risk averse farmers table 1 unsatisfied farmers engaging in optimising behaviour related to infection level would select the resistant variety without fungicide application and after infections in resistant fields the resistant variety with fungicides however when both of these strategies were not effective anymore to prevent infection they switched back to the susceptible variety with fungicides farmers growing the susceptible variety with fungicides were a large group of farmers the weights for the criteria infection level yield and income were almost equal and standard deviations were high which indicates that these farmers were a mix of yield optimizers income maximizers and neutral farmers in the baseline scenario the susceptible variety with fungicide application resulted in the highest yield and income table 5 growing the susceptible variety with fungicides did not result in losses in yield and income as a result of infection and therefore many farmers continued using this strategy 3 2 scenario analysis to compare the model scenarios we analysed the management strategies of farmers after 50 years of simulation for the two patterns with and without resistance breakdown table 2 also the year resistance breakdown occurred was analysed as well as the mean field performance per management strategy tables 3 and 5 overall we observed small effects of higher parameter values related to costs for fungicide application yield level of the resistant variety and crop price of the resistant variety with respect to management strategies of farmers in the situation where resistance breakdown occurred the percentage of farmers growing a resistant variety without fungicides remained low in all scenarios but the percentage of farmers growing a resistant variety with fungicides slightly increased in case of a higher crop price and higher yield level of the resistant variety table 2 when resistance was overcome by the pathogen the mean field performance of the resistant variety was lower or equal compared to the susceptible variety with fungicides in all scenarios table 5 when resistance remained effective the percentage of farmers growing a resistant variety without fungicides increased from 19 5 to maximum 27 0 increasing the yield level of the resistant variety had the largest effect since this resulted in a higher yield and income in all three scenarios the resistant variety without fungicides resulted in a higher income compared to the susceptible variety with fungicides table 5 however many farmers who started with the susceptible variety with fungicide application continued using this strategy because fungicides effectively suppress the disease so stable levels in yield and income are achieved as a result farmers were satisfied and certain and mainly engaged in repeating behaviour differences in management strategies as a result of higher parameter values related to costs for fungicide application yield level of the resistant variety and crop price of the resistant variety did not have a strong effect on resistance breakdown table 3 some variation was observed mainly during the first 20 years of the simulation but this was probably the result of random processes such as the weather conditions and the allocation of potato fields since no large differences between management strategies were observed 4 discussion 4 1 boom and bust cycles simulating the interactions between farmers decision making and late blight dynamics increased understanding on the effects of adoption of a resistant potato variety by farmers on disease dynamics and resistance durability assuming a scenario where a new resistant variety with a single resistance gene became available the model showed a gradual increase of farmers growing the resistant variety in the majority of model runs resistance breakdown occurred within the first 20 years of the simulation by emergence of a new virulent strain the virulent strain spread over the landscape and became dominant in the late blight population decreasing yield and income of resistant fields in the model farmers responded to this event by switching to other management strategies mainly to growing the susceptible variety with fungicide application this pattern has been described previously as a boom and bust cycle because of the often rapid rise and fall in the effectiveness of host resistance against pathogen populations in agriculture brown and tellier 2011 pink and puddephat 1999 one cycle includes several stages a introduction of a resistant variety with a novel resistance gene source b increase in use of the resistant variety c emergence of a new virulent strain d a rapid increase of the virulent strain in the population e the complete loss of resistance in the crop f decrease in use of the variety with the specific resistance gene followed by g decline of the virulent strain in the population assuming fitness costs are associated to virulence the cycle can be repeated multiple times when varieties are introduced with new resistance traits for potato late blight boom and bust cycles have been observed after the introduction of resistant varieties from earlier breeding programs fry 2008 when varieties were introduced containing resistance genes from the closely related species solanum demissum new virulent strains emerged that overcame resistance malcolmson 1969 boom and bust cycles are a general phenomenon in monocultures with gene for gene interactions and have also been described in other crops including oilseed rape wheat and barley de vallavieille pope et al 2012 rouxel et al 2003 wolfe and mcdermott 1994 in our model we assumed that no costs are associated with virulence hence the virulent strain did not decline in the population when farmers stopped growing the variety with the matching resistance gene according to experimental data no or only few relations between fitness costs and virulence have been found montarry et al 2010 schöber and turkensteen 1992 in other crops it has been observed that virulent strains rarely revert to their initial frequencies after removal of the variety with the corresponding resistance gene mundt 2014 this is relevant information with respect to deployment strategies such as gene rotation or stacked resistance with previously defeated resistance genes if virulent strains remain present in the pathogen population these resistance management strategies will be less effective because the virulent strains can rapidly reproduce after reintroduction of resistance genes or more easily adapt to varieties with multiple resistance genes when one of these genes has already been overcome a number of theoretical models exist that reproduced boom and bust cycles by simulating host pathogen interactions at the landscape scale brown and tellier 2011 but as far as we know none of these included the interactions with respect to farmers decision making by exploring the interactions between farmer behaviour and the spatially explicit evolutionary dynamics of the pathogen we identified potential factors and processes that could affect the adoption of a resistant potato variety and resistance durability these factors and the implications for disease management are described in the following sections 4 2 scenario analysis the results from this study showed that in the current situation the use of susceptible varieties with fungicide application resulted in the highest yield and income which is in line with current management strategies of conventional farmers in all model scenarios almost no farmers were growing the susceptible variety without fungicides since this strategy resulted in most years in a high infection level and losses in yield and income the organic sector currently represents about 1 of the total potato production area in the netherlands due to severe late blight outbreaks between 2000 and 2007 and a lack of resistant varieties its acreage decreased by 20 showing that growing susceptible varieties without any effective control is not profitable in years with high disease pressure even with a premium price for organic potatoes lammerts van bueren et al 2008 the scenarios including higher fungicide costs and higher yield or potato price of the resistant variety affected the field performance of management strategies and consequently the selection of management strategies by farmers tables 2 and 5 when the resistance remained effective all three scenarios resulted in a higher yield and income of resistant fields without fungicides compared to susceptible fields with fungicides as a result more farmers adopted the resistant variety in the model and therefore these strategies could contribute to sustainable disease control however the risk on resistance breakdown was high and when the resistance was overcome farmers switched back to the use of fungicides farmers in the model were simulated as social agents who interacted with each other and the environment simulating the social ecological interactions can increase insight in the potential effects of certain policies or changes in the socio economic environment and be used to identify strategies that foster a transition towards more sustainable disease management 4 3 regime shifts the model showed that resistance breakdown did not occur in all simulation runs emergence of virulent spores as a result of mutation has a low probability in addition spread of the virulent strain is affected by processes such as the weather conditions and allocation of potato varieties which varies between years and model runs resistance breakdown as a result of emergence and spread of the virulent strain in the late blight population could be described as a regime shift filatova et al 2016 a regime shift transforms the system resulting in new properties structure feedbacks and underlying behaviour of components or agents when resistance breakdown occurred in the model the system changed with respect to the field performance of management strategies farmer behaviour and the pathogen population figs 3 and 4 regime shifts can occur as a result of gradual changes in the system components or from interactions between processes operating at different spatial and temporal scales in the model establishment of the virulent strain occurred when the ratio between the wild type and virulent strain exceeded a threshold resulting in initial infections of the virulent strain in the following year this threshold can be reached when the virulent strain is able to emerge and spread during the growing season which is affected by the management strategies of farmers as well as a number of random processes such as the weather conditions the allocation of farmers and potato fields and the location of infection sources at the start of the growing season predicting critical transitions is often very difficult because the state of the system may show little change before the tipping point is reached scheffer et al 2009 with respect to late blight control it has been suggested to set up monitoring programmes to yield direct insight in the p infestans adaptation process at population level haverkort et al 2016 kessel et al 2018 when a virulent strain is detected and the resistance is at risk of being overcome additional management strategies are needed for example by additional application of fungicides on resistant fields in the model farmers growing a resistant variety started applying additional fungicides but after infections in resistant fields were observed at this point the application of fungicides could only slow down spread of the virulent strain but the resistance was already overcome monitoring programmes could therefore be very useful to inform farmers about the risk on infections in resistant fields so additional measures are taken before the virulent strain will spread in the population 4 4 implications for late blight control the model showed that the risk on emergence of new virulent strains and resulting infections in resistant fields was mainly high during the first 10 years after the introduction of resistant variety during this period the number of farmers growing the resistant variety gradually increased and farmers growing the susceptible variety without fungicides decreased in this transition period there is a higher risk that virulent spores emerge from susceptible fields and spread to neighbouring resistant fields this is relevant information since stakeholders in the dutch organic potato sector recently agreed to upscale the use of late blight resistant varieties bionext 2017 there is currently insufficient supply of resistant seed potatoes for the entire organic market so the coming years a situation will occur were organic farmers will grow partly susceptible and partly resistant varieties during this transition phase organic farmers must be aware of the risk of resistance breakdown and take immediate countermeasures when they observe infections in resistant fields the model showed that in some years a small fraction of resistant fields was infected but the virulent strain was not able to establish in the population therefore a strategy that could be used by farmers to increase resistance durability includes immediate haulm destruction to prevent spread and establishment of the virulent strain in the late blight population the results showed that when resistance remained effective only part of the farmer agents in the model adopted the resistant variety even when this resulted in a higher yield and income compared to the susceptible variety we started with a situation in which the majority of farmers was growing the susceptible variety with fungicides the model showed that the effect of habitual behaviour is very strong which means that when farmer agents are satisfied and certain they would not change their management strategy although fungicides could not prevent infection completely they suppressed the disease so stable levels in yield and income are reached which resulted in a high satisfaction and low uncertainty of farmers as a result only risk averse farmers with a high need satisfaction adopted the resistant variety in the model these results suggest that when new resistant varieties are introduced to the market investments are probably needed to promote these to farmers and to increase their adoption interviews with conventional farmers showed that they do not consider late blight as a big problem because the application of fungicides leads to effective and cheap control pacilly et al 2016 these results support this finding secondly to prevent emergence and spread of virulent strains additional management strategies are needed to increase durability of resistance the development of sustainable crop protection systems therefore requires cooperation between actors in the whole sector to achieve structural transformations in disease control 4 5 further research to simulate farmers decision making we used the consumat approach a well founded theory on human behaviour and previously used to simulate farmers decision making the implementation of the framework was supported by data from the literature on farmer behaviour and results from interviews with dutch potato farmers the model was able to reproduce patterns and trends observed in reality e g boom and bust cycles which supports the validity of the model framework grimm et al 2005 however different model structures at the micro scale can lead to the same emergent patterns at the macro scale schulze et al 2017 methods to validate processes on human behaviour include expert validation and role playing games ligtenberg et al 2010 secondly alternative models of decision making could be implemented to analyse the sensitivity of the results to different assumption of human decision making schlüter et al 2017 these methods are important steps for further research besides implementing alternative theories on human behaviour we identified some other relevant processes that could be implemented for further research we made simplifications on model components such as management strategies the farmer population the network structure and market effects although the model framework proved to be sufficient considering the purpose of this study these processes could be extended in future research additional data collection could contribute to the implementation of these processes for example collecting survey data of the farmer population secondly in the current model the landscape consisted of farmers that each manage one potato field while in reality farmers can have multiple fields spread over the farm these potato fields can be managed in different ways and farmers usually grow a number of different potato varieties also as a way of risk management the current model structure represents decision making at field level however this result in a relatively high number of farmers in the model landscape thirdly in the model we included only one susceptible and one resistant variety currently a number of different resistant varieties is available with resistance genes from different sources more diversity in crop resistance can potentially reduce the risk on resistance breakdown and spread of virulent strains lof and van der werf 2017 mundt 2014 lastly since potatoes are reproduced vegetatively by the use of seed potatoes it takes some time to increase the production of newly introduced potato varieties the availability of seed potatoes can therefore constrain a rapid adoption of new resistant varieties it would be interesting to implement these factors in the model to analyse the effect on the adoption of resistant varieties the allocation of susceptible and resistant fields in the landscape and resistance durability in the model stakeholders such as breeding companies the government and the market were represented as drivers of the system which influenced farmers decision making however each of these stakeholders have their own objectives and interests which leads to various types of interactions such as competition cooperation and trading pacilly et al 2016 agent based models are very suitable to include multiple types of agents and their interactions as a next step it would be interesting to explore the interactions between farmers other stakeholders and late blight dynamics with respect to the use of crop resistance in late blight control it would be mainly interesting to focus on the role of breeding companies and the effect of breeding and marketing strategies on late blight control 5 conclusion in this paper we combined a framework on farmer behaviour to an epidemiological framework on potato late blight to explore the use of crop resistance in disease control the framework on farmers decision making was based on the consumat approach and supported by data from literature on farmer behaviour and interviews with dutch potato farmers after introduction of a new resistant variety the model reproduced a so called boom and bust cycle the percentage of farmers growing the resistant variety increased boom until resistance breakdown occurred by emergence and spread of a virulent strain and in response farmers switched to other potato varieties and management strategies bust by exploring the interactions between farmer behaviour and late blight dynamics the model increased insights in the factors and processes that could affect the adoption of a resistant potato variety and resistance durability for example a higher crop price and yield of the resistant variety increased the adoption by farmers however also a large number of farmers continued growing the susceptible variety with fungicides which suggests that cooperation in the whole potato sector is needed to achieve structural transformations in disease control in addition the high risk on resistance breakdown stresses the importance of resistance management strategies to increase resistance durability it was found that emergence and spread of the virulent strain is the result of interactions between management strategies of farmers the weather conditions and the allocation of potato varieties by exploring the social ecological interactions related to disease control the model contributed to the field of social ecological system research and agent based modelling the number of models that tackles two way feedbacks between social and ecological systems is scarce also due to the inherent complexity of such systems filatova et al 2013 parker et al 2008 schulze et al 2017 this study provides a framework for linking decision making processes of farmers to disease dynamics in an agent based model implementing these two way linkages allowed us to explore non linear dynamics and feedback mechanisms within the social ecological system this approach could be useful for a whole range of systems focussing on management of emerging infectious diseases of crops acknowledgements we would like to thank the strategic research programme complex adaptive systems ip op cas of wageningen university research for financing this research the contribution of jg was partly funded by the cgiar research program on roots tubers and bananas rtb and supported by cgiar fund donors appendix a model description the model description follows the odd overview design concepts details protocol for describing agent based models grimm et al 2006 2010 in the model description we focus on the processes related to farmers decision making for more details on the epidemiological framework we refer to pacilly et al 2018 a 1 model purpose the aim of the model is to simulate the interactions between farmers decision making and late blight dynamics in an agricultural landscape with potato fields the model is used to simulate the use of crop resistance in disease control by analysing the adoption of the resistant variety by farmers and the durability of resistance over time table a 1 overview of late blight management strategies implemented in the model table a 1 potato variety fungicide application no yes susceptible sus sus resistant res res fig a 1 overview of behavioural strategies in relation to farmers satisfaction and uncertainty according to the consumat approach fig a 1 a 2 entities state variables and scales the model includes three types of entities farmers grid cells and agricultural fields the model represents an agricultural landscape of 10 10 km2 and the grid cells are 200 200 m2 4 ha the model is populated by farmers each of whom manages one potato field which consists of one or more grid cells a network was initialised in which farmers are connected to the closest farmers around them shortest distance which represents a network of neighbours an overview of farmer variables is shown in table a 2 for the state variables and model parameters related grid cells we refer to pacilly et al 2018 in the model farmers select one of four late blight management strategies for their field table a 1 farmers can choose between a susceptible or late blight resistant potato variety with or without the use of fungicides these strategies have different effects on field and landscape performance field performance was analysed for criteria including yield income and infection level to calculate farmers income the crop price pm was set at 13 per 100 kg 1 and the fungicide costs fc at 50 per application for the decision making processes we used behavioural strategies according to the consumat approach jager and janssen 2012 jager et al 2000 janssen and jager 2001 to evaluate their field performance farmers calculate the actual estimated predicted and potential performance per criterion these results are used to determine farmers satisfaction and uncertainty levels which leads to one of the following behavioural strategies repeating imitating optimising and social comparison figure a 1 the decision making process is influenced by personal characteristics including farmers need satisfaction and uncertainty tolerance level we distinguish four farmer types in the model which differ in the weights assigned to the criteria table 1 weights represent farmer preferences related to the criteria infection level yield and income processes on crop growth and disease dynamics are simulated at grid cell level the grid cells are characterised by location field number potato variety susceptible or resistant fungicide use and variables and parameters for crop growth and late blight infection pacilly et al 2018 we consider only one type of susceptible and resistant variety with one resistance gene we assume the resistant variety has a 20 lower potential yield compared to the susceptible variety which is reflected in the crop growth parameters two types of late blight are distinguished in the model the wild type and the virulent strain the wild type can only infect the susceptible variety while a virulent strain can also infect the resistant variety at the start of the simulation only the wild type is present the virulent strain can emerge during the growing season as the result of mutation to simulate disease dispersal we used an aged structured population model skelsey et al 2010 when spores germinate lesions first enter a latent phase of five days after which they become infectious and produce spores after the infectious phase lesions are added to the pool of no longer infectious tissue a fraction of the produced spores is dispersed by wind to nearby cells where they can cause infections since late blight development and crop growth is weather dependant we used measured weather data as input for the model table a 2 overview of farmer variables and parameters for each farmer the parameters were randomly selected based on a mean value μ or within a range see also table 1 table a 2 symbol description unit value farmer parameters fs field size ha μ 7 l network links no μ 5 ft farmer type 1 4 uf uncertainty tolerance level 0 1 sf need satisfaction 0 1 wd weight of infection level criterion 0 1 wp weight of income criterion 0 1 wy weight of yield criterion 0 1 farmer variables bs behavioural strategy ms management strategy fn fungicide applications mean no year 1 ed estimated infection level ep estimated income ha 1 ey estimated yield tonnes ha 1 ad actual infection level ap actual income ha 1 ay actual yield tonnes ha 1 pd potential infection level pp potential income ha 1 py potential yield tonnes ha 1 sd infection level satisfaction sp income satisfaction sy yield satisfaction st total satisfaction ud infection level uncertainty up income uncertainty uy yield uncertainty ut total uncertainty a 3 process overview and scheduling the time step in the model is one day and we simulate the potato growing season from may 1 to september 30 for 50 years processes in the model include figure a 2 1 crop growth and disease dynamics grid cells 2 update field performance farmers 3 calculate relative satisfaction and uncertainty farmers 4 select behavioural strategy farmers 5 select management strategy farmers and 6 predict field performance farmers processes on crop growth and late blight dispersal are updated on a daily step decision making processes of farmers 2 6 are executed at the end of the growing season within each submodel grid cells and agents are processed in a random order a detailed description of model processes can be found in section a 7 fig a 2 flow chart of the model processes focussing on farmers decision making fig a 2 a 4 design concepts basic principles the model is a spatial representation of the social ecological system of potato late blight management we focus on the interactions and feedbacks mechanisms between farmers decision making and disease dynamics in an agricultural landscape for the epidemiological processes we used the previously developed model pacilly et al 2018 and we added a social dimension of decision making on late blight control the framework on farmers decision making was based on the consumat approach jager and janssen 2012 janssen and jager 2001 and results from interviews with dutch potato farmers pacilly et al 2016 emergence both social and ecological dynamics are emerging from the model as a result of interactions between farmers decision making and disease dynamics including farmers behavioural strategies farmers late blight management strategies field performance for the criteria infection level yield and income and disease dispersal at landscape level adaptation during the simulation farmers can change their late blight management strategy four late blight management strategies are implemented in the model table a 1 at the end of each year farmers analyse their field performance and based on the results for the criteria on infection level yield and income their satisfaction and uncertainty is calculated according to the consumat approach one of four behavioural strategies is selected figure a 1 repetition imitation social comparison and optimisation based on their behavioural strategy farmers continue using the same late blight management strategy or select one of the other three strategies see section a 7 for more details objectives when farmers are unsatisfied and or uncertain one of the following behavioural strategies is selected according to the consumat approach imitation social comparison and optimisation in the case of social comparison and optimisation farmers aim to select a management strategy that results in a higher satisfaction by improving their field performance related to the criteria infection level yield and income prediction based on observed results farmers predict their field performance expected values for the three criteria infection level yield and income the expected values are calculated by taking the mean value using historical values of their own field of the last five years sensing farmers can sense the field performance of the farmers in their network as well as their late blight management strategy in the case of optimising behaviour farmers have information about the mean field performance per management strategy of the last growing season interaction agents interact by sensing the state variables of other agents in their network secondly the field performance of farmers is affected by the management strategies of other farmers in the landscape as a result of spatial interactions related to disease dispersal stochasticity at the start of the simulation the landscape is initialised in which farmers and potato fields are randomly allocated in the landscape management strategies are randomly divided over the farmers secondly for a number of farmer characteristics the values are randomly selected to create a heterogeneous population including the number of contacts links uncertainty tolerance level 0 1 need satisfaction 0 1 and farmer type farmer types differ in the weights which represent farmer preferences for the different criteria table 1 with respect to disease processes at the start of each year the infection is initialised in a fraction of the potato grid cells randomly selected weather data is used as input for the model and each year data of one year is selected from a dataset of 36 years during the growing season spores are dispersed by wind and every time step the wind direction is randomly selected northeast southeast southwest and northwest collectives each field is a collective of one or more grid cells which is managed by a farmer farmers select a management strategy for their field and they evaluate their field performance by using the mean value of the grid cells belonging to the field observation at the end of each year data on landscape level was recorded including behavioural strategies and management strategies of farmers as well as variables related to disease dispersal in the landscape secondly the mean performance of each management strategy was calculated for the criteria infection level yield and income in the model interface several graphs are presented to observe the output over time a 5 initialisation the model represents an agricultural landscape of 10 km 10 km with a potato density of 24 the model consists of 50 50 grid cells which represent a squared area of 200 m 200 m at the start of the simulation the landscape is initialised with 350 farmers that each manage one potato field with a mean size of 7 ha these parameters were derived from landscape data of a dutch agricultural region a network is initialised in which farmers are connected to the closest farmers around them with a mean number of 5 links per farmers at the start of the simulation we assume that all farmers grow a susceptible variety and the majority applies fungicides 90 of the farmers before the actual simulation started the model was first run for five years without decision making processes of farmers to create a list of reference values related to farmers field performance an overview of initial values of crop growth and late blight can be found in pacilly et al 2018 to create a heterogeneous population farmer characteristics were selected randomly see table 1 and section a 4 stochasticity a 6 input data meteorological data was used as input for the model to simulate crop growth and late blight dispersal during the growing season may 1 to september 30 data from two dutch weather stations was used eelde 1981 1993 and marknesse 1994 2016 in this way a dataset of 36 years of weather data was created mean daily temperature and total radiation was calculated and used to simulate crop growth secondly based on calculation rules using hourly temperature and humidity during a 24 h period we determined if a day was suitable for sporangia to cause infection skelsey et al 2009 on a so called blight day newly produced spores can cause infections as a result of spore germination see pacilly et al 2018 for more details a 7 submodels below the model procedures as shown in figure a 2 are described in more detail 1 crop and disease dynamics grid cells at the start of each year late blight infections are initialised in a fraction of the potato fields randomly selected during the growing season may till september processes related to crop growth and disease dynamics are simulated with a daily time step according to governmental regulations the potato haulm is destroyed when the disease severity in a field reaches 5 as a result crop growth stops directly and the disease can no longer disperse to other fields for a detailed description of these model processes we refer to pacilly et al 2018 2 update field performance farmers at the end of each year the actual and potential field performance is determined for farmers for each performance criterion i infection level yield and income a actual field performance i yield ay the mean potato yield is calculated tonnes per ha which is affected by the potato variety weather conditions and infection with late blight ii income ap income ha 1 is based on the actual yield and the price for potatoes minus costs for fungicide application equation a 1 the crop price pt is set at 13 per 100 kg 1 this value was derived from a dataset on potato prices in the netherlands from 2000 to 2017 wur 2018 for the susceptible and resistant varieties the same value is used costs for fungicides are related to the mean number of applications fn and the costs per application fc set at 50 a 1 a p a y p t 10 f n f c iii infection level ad to analyse the infection level a scale from 1 to 4 was developed using results on disease severity the percentage of infected leaf tissue where high values represent a high disease severity 1 0 1 2 0 1 1 3 1 5 4 5 b the potential performance is the maximum result which could be achieved in a specific year without any losses caused by the disease i potential yield py is determined by calculating the maximum yield that could be achieved for susceptible and resistant fields based on the weather conditions in that year temperature and radiation ii the potential income pp is calculated in the same way as the actual income but using the potential yield equation a 2 a 2 p p p y p t 10 f n f c iii the potential infection level was set at 1 for all management strategies which represents no or a very low infection level 3 calculate relative satisfaction and uncertainty farmers farmers calculate the relative uncertainty and satisfaction for each performance criterion i the overall uncertainty and satisfaction is influenced by the weights weights represent farmer preferences for the different criteria a satisfaction is defined as the ratio between the actual field performance ai and the potential field performance pi for each performance criterion i the total satisfaction is based on the satisfaction level for each criterion and their weights wi equation a 3 a 3 s t w i a i p i b uncertainty is defined as the ratio between the actual field performance and the estimated value ei the total uncertainty is based on the uncertainty for each criterion i influenced by the weights equation a 4 a 4 u t w i a i e i 4 select behavioural strategy farmers farmers compare their relative satisfaction and uncertainty level st and ut to their personal need satisfaction sf and uncertainty tolerance level uf based on the consumat framework farmers select one of four behavioural strategies figure a 1 a if unsatisfied and uncertain st sf and ut uf social comparison b if unsatisfied and certain st sf and ut uf optimisation c if satisfied and uncertain st sf and ut uf imitation d if satisfied and certain st sf and ut uf repetition 5 select management strategy farmers according to their behavioural strategy farmers select a management strategy a social comparison farmers select the criterion they want to improve infection level yield or income therefore farmers compare the results of their field for each performance criterion i by calculating the weighted satisfaction oi which is based on the satisfaction level and the weights equation a 5 a 5 o i s i 1 w i b the criterion with the lowest score is selected by farmers which represents the criterion they want to optimize for this criterion farmers compare the performance of the other farmers in their network and take over the management strategy of the farmer with the best result c optimisation farmers select the criterion they want to optimize similar to social comparison farmers compare the mean performance of all four management strategies based on the results of the previous year and adopt the management strategy that has the best result for the criterion the farmer wants to optimize if management strategies were not used by farmers the results of the previous year are used since the management strategies including the resistant variety with and without fungicides are not used by farmers at the start of the simulation the potential values are used which represents the mean field performance of these two strategies when the resistant variety with and without fungicides have the same highest score it is assumed that farmers select the resistant variety without fungicides d imitation farmers adopt the management strategy which is used by the majority of farmers in their network if this includes two or more strategies one of these strategies is randomly selected e repetition farmers don t change their management strategy 6 predict field performance farmers farmers estimate the value ei for each performance criterion i for the coming year therefore they calculate the mean value using historical values of their own field of the last five years appendix b results alternative network structure table b 1 year of resistance breakdown as a result of establishment of the virulent strain the effect of farmer network structure was analysed standard settings mean number of links per farmer 5 and farmers are connected to the closest farmers around them shortest distance higher number of links mean number of links per farmer 10 and farmers are connected to the closest farmers around them connecting farmer types mean of links per farmer 5 and farmers are connected to the closest farmers around them of the same farmer type for each scenario the model was run for 100 times table b 1 network structure year resistance breakdown percentage of runs 0 10 10 20 20 30 30 40 40 50 50 standard settings 41 17 5 4 6 27 higher number of links 41 14 5 2 10 28 connecting farmer types 44 14 10 4 6 22 table b 2 management strategies of farmers at the end of the simulation year 50 in case resistance breakdown occurs and in case the resistance remains effective the effect of farmer network structure was analysed standard settings mean number of links per farmer 5 and farmers are connected to the closest farmers around them shortest distance higher number of links mean number of links per farmer 10 and farmers are connected to the closest farmers around them connecting farmer types mean of links per farmer 5 and farmers are connected to the closest farmers around them of the same farmer type mean values are shown sd based on 100 runs table b 2 network structure management strategies of farmers sus sus res res resistance breakdown occurs standard settings 0 9 0 7 92 0 3 7 0 2 1 4 6 9 3 5 higher number of links 0 9 0 6 91 8 3 6 0 2 1 2 7 0 3 6 connecting farmer types 1 0 0 9 89 3 5 0 0 6 3 2 9 1 4 6 resistance remains effective standard settings 0 6 0 6 79 0 2 4 19 5 3 1 1 0 1 9 increasing the number of links 0 7 0 7 79 3 3 4 19 7 3 0 0 3 0 6 connecting farmer types 0 4 0 4 74 2 3 2 24 9 3 2 0 5 1 2 appendix c model initialisation fraction of farmers growing a susceptible variety without fungicides table c 1 year of resistance breakdown as a result of establishment of the virulent strain the fraction of farmers growing a susceptible variety without fungicides at the start of the simulation was varied for each parameter value the model was run 100 times table c 1 initial fraction of farmers growing a susceptible variety without fungicides sus year resistance breakdown percentage of runs 0 10 10 20 20 30 30 40 40 50 50 0 00 0 0 0 0 0 100 0 05 32 9 7 4 3 45 0 10 41 17 5 4 6 27 0 15 56 14 3 2 1 24 0 20 67 13 7 2 3 8 0 25 76 8 6 1 1 8 0 30 82 11 4 0 3 0 table c 2 management strategies of farmers at the end of the simulation year 50 in case resistance breakdown occurs and in case the resistance remains effective the fraction of farmers growing a susceptible variety without fungicides at the start of the simulation was varied the other farmers are growing a susceptible variety with fungicide application mean values are shown sd based on 100 runs table c 2 initial fraction of farmers growing a susceptible variety without fungicides sus management strategies of farmers sus sus res res resistance breakdown occurs 0 00 0 05 0 5 0 5 92 6 3 4 0 1 0 5 6 8 3 3 0 10 0 9 0 7 92 0 3 7 0 2 1 4 6 9 3 5 0 15 1 1 0 6 90 9 4 0 0 0 0 1 8 0 3 9 0 20 1 5 0 7 90 2 3 8 0 0 0 1 8 3 3 7 0 25 1 9 0 7 89 6 4 2 0 0 0 2 8 5 4 1 0 30 2 1 0 9 89 1 4 0 0 0 0 1 8 8 4 0 resistance remains effective 0 00 0 0 0 0 81 1 2 6 18 9 2 6 0 0 0 0 0 05 0 4 0 4 79 9 2 2 19 5 2 3 0 3 0 6 0 10 0 6 0 6 79 0 2 4 19 5 3 1 1 0 1 9 0 15 1 0 0 5 77 0 3 5 20 7 4 1 1 3 2 1 0 20 1 3 0 8 75 7 3 6 22 9 4 0 0 2 0 4 0 25 1 7 1 0 74 6 2 1 23 4 2 2 0 4 0 5 0 30 
26184,in this paper we focus on potato late blight control in the netherlands to analyse the social ecological interactions between farmer behaviour and disease dynamics an agent based model was developed to analyse the use of crop resistance for sustainable disease control the framework on farmers decision making was based on a behavioural theory and supported by data from literature and interviews with dutch potato farmers this framework was integrated with a previously developed spatially explicit model on potato late blight dynamics we assumed a scenario where a new resistant potato variety was introduced to the market the model reproduced a boom and bust cycle the percentage of farmers growing the resistant variety increased until resistance breakdown occurred by emergence and spread of a virulent strain and in response farmers switched to other potato varieties and management strategies several factors and processes were identified that could contribute to the development of sustainable disease management strategies keywords phytophthora infestans social ecological systems resistance management consumat 1 introduction one of the main challenges in global food production is to control upcoming pests and diseases mack et al 2000 examples of emerging infectious diseases of crops include banana xanthomonas wilt and wheat rust vurro et al 2010 these diseases can have huge impacts on human well being economy and biodiversity control of the disease is the result of an interplay between pathogens hosts and actors e g farmers governments and researchers the actors in the system can have divergent approaches in disease control as a result of different perspectives and objectives in this paper we focus on potato late blight control to analyse interactions between farmer behaviour and disease dynamics phytophthora infestans the causal agent of late blight first arrived in europe in 1845 where it was responsible for the irish potato famine in which one million people died and another one million people emigrated p infestans has a high evolutionary potential and as new strains evolve also new outbreaks of the disease emerge causing devastating epidemics globally anderson et al 2004 because the disease is dispersed by wind control strategies should be analysed at the landscape level in this project we focus on the netherlands which is a large producer of seed ware and starch potatoes haverkort et al 2008 the high potato density and favourable weather conditions for the disease moderate temperatures and high humidity result in frequent outbreaks of the disease currently the use of fungicides is the most important control method but these are harmful for the environment the use of resistant varieties could improve sustainability of late blight management however as a result of pathogen adaptation new virulent strains can emerge resulting in resistance breakdown previously we described late blight management in potato production as a social ecological system which is driven by interrelated social and biophysical processes that interact across multiple temporal and spatial scales pacilly et al 2016 since the disease incidence in the landscape is influenced by biophysical processes as well as crop management strategies it is important to focus not only on epidemiological processes but also on decision making concerning disease management in this system farmers play a key role since they decide on crop management their management strategies affect the disease pressure in a landscape and the sustainability in terms of environmental pollution and breakdown of disease resistance which can be considered as a common good secondly farmers also respond to conditions influences and changes in the socio institutional setting e g policies markets extension and peer to peer communication and the biophysical environment e g soil and weather and they adapt their management strategies based on past experiences therefore to identify effective and sustainable late blight management strategies it is important to consider the social ecological interactions previously we described the social ecological system of potato late blight and used modelling for analysing the effect of management strategies on disease control pacilly et al 2016 2018 model scenarios were discussed in workshops with farmers pacilly et al 2019 section 1 1 describes the findings from previous work in this paper we focus on the social ecological interactions by adding the dimension of decision making by multiple interacting farmers in a landscape in the model exploring the interactions and feedback mechanisms related to farmers decision making and late blight dynamics can increase understanding of the system behaviour and contribute to the development of effective management strategies and policies agent based modelling was used to simulate the social ecological interactions agent based models have been recognized as a useful tool to analyse human decision making in a spatial environment in which biophysical processes occur an 2012 agent based models consist of heterogeneous entities which interact with each other and the environment and is therefore very suitable for simulating individual decision making agents this is very relevant in the context of agricultural systems where individual households are the main decision makers but can strongly differ agent based modelling can therefore contribute to improved understanding of farmers behaviour in response to changing environmental economic or institutional conditions huber et al 2018 furthermore farm management includes both short and long term decisions for example decisions on crop selection are made yearly as a result of annual production cycles while the use of inputs such as fungicides or fertilizers are made on a daily basis during the growing season also with respect to disease dynamics processes occur at different temporal scales disease is spread during the growing season but breakdown of resistance can take multiple years both short and long term processes are important characteristics of the system agent based modelling is a useful tool for simulating and analysing these complex interactions at different scales during the last decades the number of studies using agent based modelling to couple social and natural systems has rapidly increased reviews on the use of models in social ecological systems research are provided by an 2012 filatova et al 2013 matthews et al 2007 parker et al 2003 and schlüter et al 2012 one of the challenges that was identified includes the integrating of social and ecological systems parker et al 2008 the number of models that is able to simulate two way feedbacks between human and environmental subsystems are scarce while this is essential for studying non linear interactions between human and natural systems filatova et al 2016 in this study we aim to contribute to this field of research by developing a model framework which integrates farmer behaviour and disease dynamics to simulate human behaviour several methods have been used when data on decision making is scarce or missing theories can be used and the implications can be confronted with empirical data groeneveld et al 2017 the most common theory to simulate human behaviour is to assume rational decision making also referred to as the homo economicus groeneveld et al 2017 schlüter et al 2017 according to this theory agents have perfect knowledge and make calculations to identify the optimal decision that maximizes their utility or profit however studies on farmer behaviour have shown that farmers are also influenced by many other factors such as peer networks individual preferences and culture austin et al 1998 edwards jones 2006 willock et al 1999 in recent years agent based models have become more popular for modelling agricultural systems and the impact of policies groeneveld et al 2017 huber et al 2018 however there is need for improved representation of farmers heterogeneous decision making for models to be relevant tools for policy assessment better understanding of farmers decision making is needed reidsma et al 2018 current european agricultural agent based models lack consideration of values social interactions norm consideration and learning in farmers decision making huber et al 2018 furthermore the representation should consider the socioeconomic and natural environment as well as farm household characteristics in our study we used the consumat approach to simulate farmers decision making on late blight management jager and janssen 2012 jager et al 2000 janssen and jager 2001 this framework was selected to develop more behaviourally rich agents the consumat approach includes elements which have been recognized as important factors influencing farmers decision making such as social interactions through a network it also allows the implementation of a heterogeneous farmer population the framework was first developed to explore consumer behaviour but is now widely used in many fields of research including farmers decision making malawska and topping 2016 speelman 2014 van duinen et al 2016 the consumat approach incorporates aspects from a range of behavioural theories such as theories on human needs motivational processes social comparison theory social learning theory and reasoned action theory according to the consumat approach agents engage in different behavioural strategies dependent on their level of satisfaction and uncertainty these behavioural strategies are repetition imitation optimisation and social comparison fig 1 the advantage of the consumat approach is that it is a highly formalized theory which allows easy implementation in an agent based model and only few assumptions have to be made the framework was combined with the previously developed ecological model to simulate the use of crop resistance in disease control by analysing the adoption of the resistant variety by farmers and the durability of resistance over time therefore we assumed a scenario in which a new resistant variety is introduced to the market previously we described that acceptance of resistant varieties is low because these varieties do not yet meet all the requirements of farmers and retailers pacilly et al 2016 in the model this was reflected in a lower yield level of the resistant variety the purpose of the model is to explore the social ecological interactions to identify factors that could be important in the development of sustainable disease management strategies we explored several scenarios that could affect the selection of management strategies by farmers and consequently resistance durability in the following sections we present a more detailed description of the model the scenario analysis and the results in the discussion we evaluate the model findings the implications for disease management and steps for further research 1 1 previous work in a previous study we developed a model to analyse the interactions between late blight management strategies disease dynamics and the abiotic environment at landscape level pacilly et al 2018 the model was used to show opportunities and risks related to the use of plant resistance in disease control growing a resistant variety can reduce disease incidence in the landscape however in the long run resistance breakdown was observed by emergence of a new virulent strain due to pathogen adaptation the durability of resistance was affected by the fraction of resistant fields in the landscape and it was found that low 0 2 as well as high 0 8 proportions of resistant fields could increase resistance durability according to the dispersal scaling hypothesis disease dispersal is affected by habitat size and dispersal distance where an increase in habitat size is described as a positive dispersal force and an increase in dispersal distance as a negative dispersal force skelsey and garrett 2013 a larger fraction of resistant fields means a lower fraction of susceptible fields which results in a lower disease incidence in the landscape and reduces the risk that virulent spores emerge virulent spores are the result of mutations during spore production in infections in susceptible fields however with higher fractions of resistant fields virulent spores also have an increased chance of arriving in these fields because of a lower dispersal distance between susceptible and resistant fields as a result of the interactions between dispersal distance and habitat size the risk on infections in resistant fields is highest with about equal proportions of susceptible and resistant fields in the landscape the previously developed model was adapted and used in workshops with conventional and organic farmers to demonstrate and discuss the potential role of resistant varieties for effective and sustainable control of late blight pacilly et al 2019 several scenarios were presented that showed the use of crop resistance and fungicide application in disease control at the landscape level farmers that attended the workshops in total 51 farmers were able to recognize the processes and patterns emerging from the model the use of model based scenarios in workshops was very useful to increase farmers knowledge of the system and served as a good starting point for discussions among participants 2 material and methods an agent based model was developed to simulate processes on crop growth disease dynamics and farmer interactions and decision making on disease management in an agricultural landscape over time the model can be described as a so called midrange model the aim is neither to exactly model the situation in a certain region nor to make a purely theoretical point gilbert 2008 the purpose of the model is to increase understanding of the system behaviour rather than making predictions for the future this study is a first step in the development of a social ecological model on late blight control in this model version we focus on the integration of model components but the individual processes were kept as simple as possible in future research the model processes could be further extended steps for further research are described in the discussion section 4 6 the model was implemented in netlogo version 5 2 0 wilensky 1999 a version of the model is available on the openabm website http www openabm org below we present an overview of the model with a description of the main model processes a detailed description of the model following the odd protocol overview design details can be found in appendix a grimm et al 2006 2010 for detailed information on the epidemiological framework we refer to pacilly et al 2018 the framework on farmers decision making was based on the consumat approach jager and janssen 2012 jager et al 2000 janssen and jager 2001 and supported by data from literature and interviews with dutch potato farmers in total 25 farmers were interviewed including 18 conventional and 7 organic farmers pacilly et al 2016 semi structured interviews were carried out on topics such as general farm characteristics the social network late blight management strategies and the use of late blight resistant varieties as a result of the semi structured interviews only qualitative data was available the main results are described in pacilly et al 2016 in the model description section 2 1 we refer to the interviews when the data supports the model framework furthermore data from the interviews was used to validate the model results which is described in the discussion section 4 2 1 model description 2 1 1 model overview the model represents a square agricultural landscape of 10 km 10 km fig 2 the potato density 24 and the mean field size 7 ha were derived from landscape data of an agricultural region in the netherlands the noordoostpolder and these parameters were used as input for the model the grid cells represent a square area of 200 m 200 m 4 ha and are clustered in agricultural fields crop rotation was not included in the model crop rotation can affect the number of initial infections however in the model the fraction of initial infected potato fields was kept constant between years the spatial location of infections were randomly selected which reflects randomizing the location of potato fields as a result of crop rotation the model is populated by 350 farmers each of whom manages one potato field a network was initialised in which farmers are connected to the closest farmers around them shortest spatial distance farmers use one of the following late blight management strategies they can grow a susceptible or resistant late blight potato variety with or without the use of fungicides these strategies have different effects on field and landscape performance field performance is analysed for criteria infection level yield and income landscape performance also relates to disease dispersal and resistance durability a conceptual overview of the model is shown in fig 1 we simulate the growing season from may 1 to september 30 for 50 years crop growth and disease dynamics are updated at a daily time step at the end of each growing season farmers analyse their field performance and select a management strategy for the following year the decision making framework to select a management strategy is based on the consumat approach jager and janssen 2012 janssen and jager 2001 based on their field performance farmers determine their satisfaction and uncertainty level which results in one of the following behavioural strategies repetition imitation optimisation or social comparison see section 2 1 5 the decision making process is influenced by personal characteristics including their need satisfaction and uncertainty tolerance level four farmer types are distinguished which differ in the weights assigned to the criteria table 1 the weights represent the importance of the different criteria to the farmer interaction between farms is related to social interaction with respect to farmers decision making and spatial interactions related to disease dispersal crop growth and late blight dynamics are simulated at grid cell level we consider only one susceptible variety and one resistant potato variety with a single resistance gene in our model we assume the resistant variety has a lower potential yield compared to the susceptible variety which is reflected in the crop growth parameters late blight resistant potato varieties currently available on the market have a moderate yield level however the yield potential may increase in the future with the introduction of new resistant varieties bionext 2017 at the start of each growing season the infection is initialised in a fraction of the potato grid cells randomly selected when a grid cell is infected spores are produced that are dispersed to nearby cells up to a distance of 1000 m where they can cause infections two types of late blight are distinguished the wild type and the virulent strain the wild type can only infect the susceptible variety while a virulent strain can also infect the resistant variety at the start of the simulation only the wild type is present the virulent strain may emerge during the growing season as the result of mutations during spore production the ratio between the wild type and virulent strain at the end of the growing season was used to calculate the number of initial infections of the wild type and virulent strain in the following year since late blight development and crop growth are weather dependent we used measured weather data from may 1 to september 30 152 days for 36 years 1981 2016 as model input these years represent variable weather conditions for crop growth and late blight dynamics to simulate crop growth mean daily temperature and total radiation was calculated and used as input for the model secondly based on calculation rules using hourly temperature and relative humidity during a 24 h period we determined whether a day was suitable for sporangia to cause infection skelsey et al 2009 expansion of existing lesions occurs every time step but new infections as a result of spore germination can only occur on so called blight days for more details we refer to pacilly et al 2018 at the start of each year a dataset is randomly selected out of these 36 years of weather data 2 1 2 late blight management strategies in the model in the netherlands the use of fungicides is currently the most widely used method in the control of late blight while crop resistance has been identified as an important strategy for more sustainable control haverkort et al 2008 lammerts van bueren et al 2008 combining these two types of disease control results in the following four strategies growing a susceptible potato variety with sus or without fungicide application sus or growing a resistant potato variety with res or without fungicide application res in the model farmers select one of these strategies for their potato field fig 1 the management strategies can be related to current farm practices of conventional and organic farmers in conventional agriculture mainly susceptible varieties are grown combined with fungicide application sus since chemical control is not allowed in organic potato production and not enough resistant potatoes are available to supply the whole market bionext 2017 organic farmers grow a combination of susceptible and resistant varieties and do not apply fungicides sus and res in years with early outbreaks of the disease organic farmers can therefore suffer high yield losses in susceptible fields but in years with a low infection pressure farmers can make a profit combining a resistant variety with reduced use of fungicides res has been proposed to prevent resistance breakdown and to increase resistance durability haverkort et al 2016 this strategy has also been described as part of integrated disease management to reduce the use of chemical fungicides kirk et al 2005 mundt et al 2002 nærstad et al 2007 although we do not distinguish organic and conventional potato production in the model the late blight management strategies represent the different approaches in late blight control in case fungicides are used on susceptible and resistant fields sus and res weekly application is assumed starting at the day of crop emergence weekly application is standard practice for many potato farmers also because it is combined with the application of other chemicals for other diseases decision support systems dss are available that can help farmers to improve efficiency of spraying by optimising the use and timing of fungicide application however many farmers do not strictly follow their advice over the season farmers use different type of chemicals in the model we distinguish preventive and curative fungicides we assume preventive fungicides are applied at the start of the growing season to reduce the infection efficiency of the spores when the disease severity in potato grid cells reaches 1 curative fungicides are applied which have a similar effect on the infection efficiency but also reduce the expansion of existing lesions to prevent spread of the disease during the growing season the government has implemented a policy that regulates maximum late blight disease thresholds nvwa 2008 at an estimated disease severity of 5 in the field the potato haulm has to be destroyed an inspection system was set up including an anonymous hotline that could fine farmers in case these regulations were not followed following these regulations we assume that all farmers in the model destroy the potato haulm when the disease severity in potato grid cells reaches 5 this means that leaf and tuber growth stops directly and the disease can no longer disperse to other fields in case of early outbreaks of the disease this can cause severe yield losses besides these strategies also other strategies exist that could be used in late blight control such as pre sprouting to reduce the growing period soil management and chemicals against infections in tubers pacilly et al 2016 however some of these strategies only have a small effect while others are too complex to implement in the model to allow integration with processes on decision making we only focus on fungicide application and crop resistance in late blight control at this stage as a result of advancements in potato breeding it is expected that new late blight resistant varieties will enter the market the coming years therefore it is very relevant to analyse the use of crop resistance in disease control 2 1 3 analysing field performance in the model we implemented three criteria to evaluate the field performance of farmers late blight infection level potato yield and income infection level is an important criterion to evaluate the effectiveness of the selected management strategy and infection in a field can also reduce potato yield and farmer income furthermore late blight management strategies include trade offs between infection level yield and income the application of fungicides can reduce the risk of infection and prevent yield losses but causes additional costs growing a resistant variety can reduce the risk of infection but has a negative effect on potato yield and income since we assume that the resistant variety has a lower potential yield compared to the susceptible variety in addition growing a resistant variety creates a risk on resistance breakdown by emergence of a new virulent strain to evaluate the field performance in relation to late blight management strategies it is therefore important to consider all three criteria secondly the importance of these criteria also differs between farmers table 1 2 1 3 1 infection level in the model late blight severity percentage of infected leaf tissue within a field is affected by the weather conditions disease dispersal and late blight management strategies fungicide use and crop resistance the simulated disease severity ranged from no or very low disease severity to a very high disease severity 10 4 to 5 development of the disease at field level is limited since we assume that the potato haulm is destroyed when disease severity reaches 5 in practice in the field disease severity is generally estimated visually because the human eye can estimate low and high disease severity more precisely than mid range levels it is proposed to correct this by using a logarithmic rather than a linear scale cooke 2006 taken these factors into account a scale from 1 to 4 was developed to analyse the infection level using data on disease severity 1 0 1 2 0 1 1 3 1 5 4 5 2 1 3 2 potato yield at the end of the growing season the mean potato yield tonnes ha of farmers is calculated potato yield is affected by the potato variety weather conditions including temperature and radiation and infection with late blight in the model we assume that the resistant potato variety has a 20 lower potential yield compared to the susceptible variety 2 1 3 3 income we use a standard approach to calculate farmer income farmers gross margin ha 1 is based on the actual potato yield times the price for potatoes minus production costs with respect to production costs we only consider the costs related to fungicide application since we focus on comparing late blight management strategies costs for fungicides are related to the number of applications the type of fungicides used machinery labour and fuel the mean number of applications per farmer was calculated over the year assuming weekly application and the costs per application fc were estimated at 50 ha 1 haverkort et al 2008 the price for potatoes was set on 13 per 100 kg which was derived from a dataset on potato prices for conventional ware potatoes in the netherlands between 2000 and 2017 wur 2018 the same price was used for the susceptible and resistant variety correlations between potato price and overall potato yield were not included because input and output markets are beyond the scope of this regional study crop prices were assumed to be constant over time in reality the crop price is affected by many other factors such as market fluctuations and the market farmers produce for e g organic conventional frozen and fry table and fresh haverkort et al 2008 pavlista and feuz 2005 furthermore to prevent price risks most farmers have contracts with trading companies on crop prices since we focus on comparing differences in income related to late blight management strategies these factors were not included in the current model version as a result of this simplification values on income do not represent actual numbers 2 1 4 farmer population many studies have shown the importance of social interactions within networks in decision making processes also with respect to dutch farmers oerlemans and assouline 2004 van duinen et al 2016 according to the consumat approach agents are influenced through interactions within networks when they engage in social comparison or imitation unfortunately no empirical data was available on social networks among dutch potato farmers however previous results from interviews showed that potato farmers influence each other and copy each other s behaviour pacilly et al 2016 since farmers have social interactions e g as neighbours friends and in study groups and they spend much time on their land they are well aware how surrounding farmers manage their crops potato late blight disperses by wind so infections in neighbouring fields can increase the risk of infection with respect to social interaction on late blight control we therefore assume farmers are in a network with the closest farmers around them shortest distance between fields in each model run a network is initialised in which farmers are connected to the closest farmers around them with a mean number of 5 links per farmer representing a social network of neighbours we explored the effect of alternative network structures appendix b increasing the mean number of links per farmer from 5 to 10 links did not affect the model results as well as a different network setup in which farmers are connected to the closest farmers around them of the same type assuming that you interact more with people who are more similar to you to create a heterogeneous population characteristics of farmers are varied within a certain range farmers uncertainty tolerance level and need satisfaction are randomly selected between 0 and 1 the weights represent the importance of the criteria to the farmer in their decision making based on these preferences farmers have different objectives resulting in four farmer types yield maximizer profit maximizer risk averse farmer and neutral farmer at the start of the simulation the farmer type of each farmer is randomly selected profit and yield maximizing farmers are types previously described in the literature malawska and topping 2016 furthermore studies have shown that farmers can have different risk attitudes which influences decision making on disease control mcroberts et al 2011 willock et al 1999 gardebroek 2006 showed that dutch farmers have different risk attitudes which indicates that farmers manage risk differently organic farmers are less risk averse than conventional farmers but also within these groups risk attitudes were heterogeneous risk perception in late blight control can for example be related to the size of potential negative impact of infection on yield and income pacilly et al 2016 risk averse farmers would adopt strategies that reduce the risk of infection such as resistant varieties or fungicides even when it comes at the expense of profit finger et al 2017 risk aversion can also result in the overuse of fungicides as was observed in the netherlands skevas et al 2012 2013 in the model the risk averse farmers therefore aim to minimize the infection level in their field last we distinguish a neutral farmer which does not have a preference for one of the criteria and the weights for the criteria are set equal table 1 for the other farmer types the weight of one of the criteria was set at 0 8 depending on their objective and for the other two criteria at 0 1 the weights therefore represent clear differences in objectives between farmer types 2 1 5 behavioural strategies according to the consumat approach behaviour of agents is affected by the levels of satisfaction and uncertainty to determine farmers satisfaction and uncertainty the actual ai potential pi and predicted ei performance is calculated for the three criteria infection level yield and income the potential field performance is the maximum result which could be achieved in a specific year without any losses as a result of yield limiting and yield reducing factors van ittersum and rabbinge 1997 in our model we only consider losses as a result of infection with the disease the potential yield of both potato varieties is output of the model and based on the temperature and radiation in a specific year the potential income is calculated in the same way as the actual income but using the potential yield the potential infection level was set at 1 for all four strategies which represents the lowest level of disease severity that could be achieved farmers also estimate the field performance for the coming year for each performance criterion they calculate the mean value using historical values of their own field for the last five years to create a list of reference values the model is run for five years before the actual simulation starts see appendix a in the model satisfaction is defined as the ratio between the actual and the potential performance and uncertainty as the ratio between the actual and the estimated predicted performance for each performance criterion the satisfaction and uncertainty is calculated the overall satisfaction st and uncertainty ut is based on the result for each criterion i influenced by the weights wi equations 1 and 2 1 s t w i a i p i 2 u t w i a i e i based on the consumat approach farmers compare their total satisfaction and uncertainty level with their personal need satisfaction and uncertainly tolerance level if the results are below their thresholds farmers are uncertain and or unsatisfied based on these results farmers engage in one of the following behavioural strategies indicated below in italics the consumat approach is a highly formalized theory which describes very detailed how agents make decisions according to their behavioural strategy the consumat approach could be easily applied to farmers decision making on late blight management strategies if a farmer is satisfied and certain he will repeat its current behaviour and continue using the same management strategy repetition if a farmer is uncertain he will interact with other farmers in his network to make an informed decision agents who are uncertain are more likely to engage in strategies that involve interactions while agents who are certain are more likely to rely on their own experiences if a farmer is uncertain but satisfied he will engage in imitation in this case he will adopt the management strategy that is used by the majority of farmers in his network when farmers are unsatisfied they engage in strategic decision making optimisation or social comparison this strategy relates to rational decision making in which farmers are aiming to optimize their field performance in relation to their preferences first they select the criterion which they want to optimize infection level yield or income see section 2 1 3 and 2 1 4 this is based on the satisfaction level for each criterion and on the criteria weights table 1 in case farmers are unsatisfied and uncertain they engage in social comparison in this strategy farmers analyse the field performance of the farmers in their network and adopt the management strategy of the farmer that has the highest score for the specific criteria when farmers are unsatisfied but certain they engage in optimising behaviour optimisation in this case they compare the mean field performance of all management strategies of the last year and adopt the management strategy that has the highest result for the criteria they want to improve when the resistant variety with and without fungicides have the same score for a criterion it is assumed that farmers select the resistant variety without fungicides 2 2 scenario analysis we assumed a situation where a new resistant potato variety was introduced to the market we analysed the effect on disease control by adoption of the resistant variety by farmers and the durability of resistance at the start of the simulation all farmers are growing a susceptible variety and the majority applies fungicides 90 three scenarios were explored in which we analysed the effect of higher fungicide costs and higher yield or potato price of the resistant variety these changes represent possible future scenarios as a result of actions by stakeholders first we increased the yield potential of the resistant potato variety so it is similar to the susceptible variety yield scenario breeding companies continue to develop new late blight resistant potato varieties and it is likely that in the future new resistant varieties will be introduced with higher yield levels secondly as a result of stakeholder cooperation the price for resistant varieties could increase in the future in the standard settings the crop price of resistant and susceptible varieties was the same recently the organic sector made an agreement to upscale the production of resistant varieties to completely service the organic market over the coming years bionext 2017 an increase in demand could also result in a higher crop price in the price scenario we therefore increased the price of the resistant variety by 25 in the third scenario fungicide scenario we doubled the price per fungicide application from 50 ha 1 to 100 ha 1 about half of all fungicides applied in the netherlands are used in the control of late blight the environmental costs are related to the pollution of groundwater energy costs for application and negative effects on human health haverkort et al 2008 increased environmental awareness could possibly lead to higher prices for fungicides for example when government increases taxes economic instruments such as pesticide taxes can contribute to an optimal pesticide policy finger et al 2017 skevas et al 2013 we analysed how these changes could affect the adoption of management strategies by farmers and the control of late blight to analyse the model results a number of output variables were calculated at the end of each growing season we recorded the behavioural strategies and management strategies of farmers as well as the mean performance per strategy for the criteria infection level yield and income to analyse disease dynamics we calculated the disease incidence the percentage of infected potato grid cells with a disease severity 1 skelsey et al 2010 and the infected resistant fields the percentage of resistant potato grid cells in the landscape infected with the virulent strain we also recorded the year infections in resistant fields were observed followed by establishment of the virulent strain in the population year of resistance breakdown this occurs as a result of between year survival of the virulent strain resulting in initial infections in the following year for each scenario simulation runs were repeated 100 times 3 results 3 1 dynamics over time 3 1 1 example of two model runs for the analysis of the results we observed the patterns that emerged from the model patterns are described as observations of any kind showing non random structure and therefore containing information on the underlying mechanisms grimm et al 2005 after 50 years of simulation we observed two different patterns in the first pattern at some moment during the simulation infections in resistant fields were observed and the virulent strain established in the population fig 3 e while in the second pattern this process was not observed fig 3k so in the first pattern resistance breakdown occurred while in the other pattern resistance remained effective during the simulation time to analyse the dynamics over time an example of one model run of both patterns is shown in fig 3 from the start of the simulation in both patterns the number of susceptible fields decreased and the number of resistant fields increased fig 3a and g susceptible fields without fungicide application had a high risk of infection and yield and income are fluctuating strongly as a result of the weather conditions that affect spread of the disease fungicide application on susceptible fields could not prevent infection completely but no large losses in yield and income were observed due to infection with late blight farmers were unsatisfied with their field performance which led to optimising behaviour fig 3f and l farmers that optimized on infection level adopted the resistant variety since this strategy scored better on infection level as a result of crop resistance fig 3b and h in the model it was assumed that farmers won t apply additional fungicides on the resistant variety when the resistance is effective so farmers adopted the resistant variety without fungicides in both simulation runs after a couple of years a small percentage of resistant fields was infected by emergence of a virulent strain fig 3e and k however in pattern 2 the virulent strain was not able to spread and establish in the population a small number of farmers responded to this event and switched to the resistant variety with fungicide application fig 3g in pattern 1 after 8 years the virulent strain was able to establish in the population and the percentage of infected resistant fields rapidly increased over time figs 3e and 4 as a result of the relative high percentage of resistant fields 20 the virulent strain could spread fast through the landscape which gave farmers a very short time to adapt when the percentage of infected resistant fields reached 55 the percentage of farmers growing a resistant variety without fungicides started to decrease the spread of the virulent strain led to simulated losses in yield and income of resistant fields which resulted in reduced farmer satisfaction and increased uncertainty and farmers switching to other management strategies mean yield and income of resistant varieties without fungicide application decreased by 25 table 5 social comparison and imitation mainly led to the adoption of the susceptible variety with fungicide application since this management strategy is used by the majority of the farmers and resulted in a lower infection level and higher yield and income optimising behaviour on infection level led to the adoption of the resistant variety with fungicide application however because the virulent strain was already present in the population additional fungicide application only slowed down infection within the field but could not eradicate the virulent strain from the landscape as a result the infection level in fields with the initially resistant variety increased and yield and income of resistant fields without fungicides decreased since the resistant variety had a lower potential yield compared to the susceptible variety at some point the yield of resistant fields without fungicides dropped below the yield of susceptible fields without fungicide application within five years after the first resistant fields were infected almost no farmers were growing the resistant variety without fungicides anymore the majority of farmers adopted the susceptible variety with fungicides and a small number of farmers the resistant variety with fungicides fig 3a the resistant variety with fungicides had a lower level for yield and income but in some years had a lower infection level after resistance breakdown the disease incidence in the landscape was highly variable per year a small fraction of farmers remained unsatisfied and or uncertain resulting in social comparing optimising and imitating behaviour however no alternative strategies were available that led to a significant improvement in the simulation runs in the baseline scenario where resistance was still effective after 50 years the percentage of farmers growing a resistant variety stabilized at 19 5 3 1 sd table 2 this was also observed in the example shown in fig 3b the majority of farmers was growing the susceptible variety with fungicide application 79 0 2 4 most farmers were satisfied and certain about their field performance and engaged in repeating behaviour which resulted in a stable situation with respect to late blight management strategies after 50 years of simulation no strong spatial pattern in management strategies was observed clusters of farmers growing the resistant were observed as well as farmers growing a resistant variety surrounded by susceptible fields fig 2 this shows that besides spatial processes including disease spread and the social network also individual characteristics of farmers have a strong effect on decision making when resistance breakdown occurred the spatial distribution of management strategies seemed more random since the majority of farmers 92 is using the same strategy susceptible variety with fungicides table 2 3 1 2 time until resistance breakdown the year of emergence of infections in resistant fields followed by establishment of the virulent strain in the population was analysed table 3 resistance breakdown was observed in 73 of the model runs in 42 of the model runs establishment of the virulent strain occurred in the first 10 years after the introduction of the resistant variety followed by an additional 17 during the ten following years once this period had passed the risk of establishment decreased in 27 of the model runs the resistance was still effective after 50 years the first years after introduction of the resistant variety a larger number of farmers was growing a susceptible variety without fungicide application these fields could act as sources of infection and there was an increased risk that the virulent strain emerged in the year the virulent strain established in the population the mean percentage of farmers that used a susceptible variety without fungicides was 2 0 1 5 after this first period the risk of establishment decreased as a result of a lower fraction of susceptible fields without fungicides however as long as farmers are present that grow the susceptible fields without fungicides resistance breakdown can occur which was also observed in the model after 30 or 40 years in a sensitivity analysis we varied the initial fraction of farmers growing a susceptible variety without fungicides appendix c the results showed that increasing the initial fraction of farmers growing a susceptible variety without fungicides increased the risk on resistance breakdown and establishment of the virulent strain mainly during the first ten years of the simulation which shows that these fields act as an infection source of the virulent strain however no strong affect was observed on the management of farmers at the end of the simulation appendix c 3 1 3 farmer characteristics we analysed the personal characteristics of farmers per management strategy at the end of the simulation table 4 the results show how farmer characteristics affect decision making and the adoption of management strategies during the simulation farmers growing a susceptible variety without fungicides mainly had a low need satisfaction 0 35 and uncertainty tolerance level 0 31 as a result they mainly engaged in repeating behaviour and the farmers growing the susceptible variety without fungicides from the start of the simulation were less likely to change their strategy since this strategy resulted in most years in a higher infection level lower yield level and lower income compared to the other strategies table 5 almost no farmers adopted this strategy during the simulation farmers growing a resistant variety with or without fungicides had a relative high need satisfaction 0 72 0 75 and also a high value for the weight infection level 0 62 which shows that these were mainly risk averse farmers table 1 unsatisfied farmers engaging in optimising behaviour related to infection level would select the resistant variety without fungicide application and after infections in resistant fields the resistant variety with fungicides however when both of these strategies were not effective anymore to prevent infection they switched back to the susceptible variety with fungicides farmers growing the susceptible variety with fungicides were a large group of farmers the weights for the criteria infection level yield and income were almost equal and standard deviations were high which indicates that these farmers were a mix of yield optimizers income maximizers and neutral farmers in the baseline scenario the susceptible variety with fungicide application resulted in the highest yield and income table 5 growing the susceptible variety with fungicides did not result in losses in yield and income as a result of infection and therefore many farmers continued using this strategy 3 2 scenario analysis to compare the model scenarios we analysed the management strategies of farmers after 50 years of simulation for the two patterns with and without resistance breakdown table 2 also the year resistance breakdown occurred was analysed as well as the mean field performance per management strategy tables 3 and 5 overall we observed small effects of higher parameter values related to costs for fungicide application yield level of the resistant variety and crop price of the resistant variety with respect to management strategies of farmers in the situation where resistance breakdown occurred the percentage of farmers growing a resistant variety without fungicides remained low in all scenarios but the percentage of farmers growing a resistant variety with fungicides slightly increased in case of a higher crop price and higher yield level of the resistant variety table 2 when resistance was overcome by the pathogen the mean field performance of the resistant variety was lower or equal compared to the susceptible variety with fungicides in all scenarios table 5 when resistance remained effective the percentage of farmers growing a resistant variety without fungicides increased from 19 5 to maximum 27 0 increasing the yield level of the resistant variety had the largest effect since this resulted in a higher yield and income in all three scenarios the resistant variety without fungicides resulted in a higher income compared to the susceptible variety with fungicides table 5 however many farmers who started with the susceptible variety with fungicide application continued using this strategy because fungicides effectively suppress the disease so stable levels in yield and income are achieved as a result farmers were satisfied and certain and mainly engaged in repeating behaviour differences in management strategies as a result of higher parameter values related to costs for fungicide application yield level of the resistant variety and crop price of the resistant variety did not have a strong effect on resistance breakdown table 3 some variation was observed mainly during the first 20 years of the simulation but this was probably the result of random processes such as the weather conditions and the allocation of potato fields since no large differences between management strategies were observed 4 discussion 4 1 boom and bust cycles simulating the interactions between farmers decision making and late blight dynamics increased understanding on the effects of adoption of a resistant potato variety by farmers on disease dynamics and resistance durability assuming a scenario where a new resistant variety with a single resistance gene became available the model showed a gradual increase of farmers growing the resistant variety in the majority of model runs resistance breakdown occurred within the first 20 years of the simulation by emergence of a new virulent strain the virulent strain spread over the landscape and became dominant in the late blight population decreasing yield and income of resistant fields in the model farmers responded to this event by switching to other management strategies mainly to growing the susceptible variety with fungicide application this pattern has been described previously as a boom and bust cycle because of the often rapid rise and fall in the effectiveness of host resistance against pathogen populations in agriculture brown and tellier 2011 pink and puddephat 1999 one cycle includes several stages a introduction of a resistant variety with a novel resistance gene source b increase in use of the resistant variety c emergence of a new virulent strain d a rapid increase of the virulent strain in the population e the complete loss of resistance in the crop f decrease in use of the variety with the specific resistance gene followed by g decline of the virulent strain in the population assuming fitness costs are associated to virulence the cycle can be repeated multiple times when varieties are introduced with new resistance traits for potato late blight boom and bust cycles have been observed after the introduction of resistant varieties from earlier breeding programs fry 2008 when varieties were introduced containing resistance genes from the closely related species solanum demissum new virulent strains emerged that overcame resistance malcolmson 1969 boom and bust cycles are a general phenomenon in monocultures with gene for gene interactions and have also been described in other crops including oilseed rape wheat and barley de vallavieille pope et al 2012 rouxel et al 2003 wolfe and mcdermott 1994 in our model we assumed that no costs are associated with virulence hence the virulent strain did not decline in the population when farmers stopped growing the variety with the matching resistance gene according to experimental data no or only few relations between fitness costs and virulence have been found montarry et al 2010 schöber and turkensteen 1992 in other crops it has been observed that virulent strains rarely revert to their initial frequencies after removal of the variety with the corresponding resistance gene mundt 2014 this is relevant information with respect to deployment strategies such as gene rotation or stacked resistance with previously defeated resistance genes if virulent strains remain present in the pathogen population these resistance management strategies will be less effective because the virulent strains can rapidly reproduce after reintroduction of resistance genes or more easily adapt to varieties with multiple resistance genes when one of these genes has already been overcome a number of theoretical models exist that reproduced boom and bust cycles by simulating host pathogen interactions at the landscape scale brown and tellier 2011 but as far as we know none of these included the interactions with respect to farmers decision making by exploring the interactions between farmer behaviour and the spatially explicit evolutionary dynamics of the pathogen we identified potential factors and processes that could affect the adoption of a resistant potato variety and resistance durability these factors and the implications for disease management are described in the following sections 4 2 scenario analysis the results from this study showed that in the current situation the use of susceptible varieties with fungicide application resulted in the highest yield and income which is in line with current management strategies of conventional farmers in all model scenarios almost no farmers were growing the susceptible variety without fungicides since this strategy resulted in most years in a high infection level and losses in yield and income the organic sector currently represents about 1 of the total potato production area in the netherlands due to severe late blight outbreaks between 2000 and 2007 and a lack of resistant varieties its acreage decreased by 20 showing that growing susceptible varieties without any effective control is not profitable in years with high disease pressure even with a premium price for organic potatoes lammerts van bueren et al 2008 the scenarios including higher fungicide costs and higher yield or potato price of the resistant variety affected the field performance of management strategies and consequently the selection of management strategies by farmers tables 2 and 5 when the resistance remained effective all three scenarios resulted in a higher yield and income of resistant fields without fungicides compared to susceptible fields with fungicides as a result more farmers adopted the resistant variety in the model and therefore these strategies could contribute to sustainable disease control however the risk on resistance breakdown was high and when the resistance was overcome farmers switched back to the use of fungicides farmers in the model were simulated as social agents who interacted with each other and the environment simulating the social ecological interactions can increase insight in the potential effects of certain policies or changes in the socio economic environment and be used to identify strategies that foster a transition towards more sustainable disease management 4 3 regime shifts the model showed that resistance breakdown did not occur in all simulation runs emergence of virulent spores as a result of mutation has a low probability in addition spread of the virulent strain is affected by processes such as the weather conditions and allocation of potato varieties which varies between years and model runs resistance breakdown as a result of emergence and spread of the virulent strain in the late blight population could be described as a regime shift filatova et al 2016 a regime shift transforms the system resulting in new properties structure feedbacks and underlying behaviour of components or agents when resistance breakdown occurred in the model the system changed with respect to the field performance of management strategies farmer behaviour and the pathogen population figs 3 and 4 regime shifts can occur as a result of gradual changes in the system components or from interactions between processes operating at different spatial and temporal scales in the model establishment of the virulent strain occurred when the ratio between the wild type and virulent strain exceeded a threshold resulting in initial infections of the virulent strain in the following year this threshold can be reached when the virulent strain is able to emerge and spread during the growing season which is affected by the management strategies of farmers as well as a number of random processes such as the weather conditions the allocation of farmers and potato fields and the location of infection sources at the start of the growing season predicting critical transitions is often very difficult because the state of the system may show little change before the tipping point is reached scheffer et al 2009 with respect to late blight control it has been suggested to set up monitoring programmes to yield direct insight in the p infestans adaptation process at population level haverkort et al 2016 kessel et al 2018 when a virulent strain is detected and the resistance is at risk of being overcome additional management strategies are needed for example by additional application of fungicides on resistant fields in the model farmers growing a resistant variety started applying additional fungicides but after infections in resistant fields were observed at this point the application of fungicides could only slow down spread of the virulent strain but the resistance was already overcome monitoring programmes could therefore be very useful to inform farmers about the risk on infections in resistant fields so additional measures are taken before the virulent strain will spread in the population 4 4 implications for late blight control the model showed that the risk on emergence of new virulent strains and resulting infections in resistant fields was mainly high during the first 10 years after the introduction of resistant variety during this period the number of farmers growing the resistant variety gradually increased and farmers growing the susceptible variety without fungicides decreased in this transition period there is a higher risk that virulent spores emerge from susceptible fields and spread to neighbouring resistant fields this is relevant information since stakeholders in the dutch organic potato sector recently agreed to upscale the use of late blight resistant varieties bionext 2017 there is currently insufficient supply of resistant seed potatoes for the entire organic market so the coming years a situation will occur were organic farmers will grow partly susceptible and partly resistant varieties during this transition phase organic farmers must be aware of the risk of resistance breakdown and take immediate countermeasures when they observe infections in resistant fields the model showed that in some years a small fraction of resistant fields was infected but the virulent strain was not able to establish in the population therefore a strategy that could be used by farmers to increase resistance durability includes immediate haulm destruction to prevent spread and establishment of the virulent strain in the late blight population the results showed that when resistance remained effective only part of the farmer agents in the model adopted the resistant variety even when this resulted in a higher yield and income compared to the susceptible variety we started with a situation in which the majority of farmers was growing the susceptible variety with fungicides the model showed that the effect of habitual behaviour is very strong which means that when farmer agents are satisfied and certain they would not change their management strategy although fungicides could not prevent infection completely they suppressed the disease so stable levels in yield and income are reached which resulted in a high satisfaction and low uncertainty of farmers as a result only risk averse farmers with a high need satisfaction adopted the resistant variety in the model these results suggest that when new resistant varieties are introduced to the market investments are probably needed to promote these to farmers and to increase their adoption interviews with conventional farmers showed that they do not consider late blight as a big problem because the application of fungicides leads to effective and cheap control pacilly et al 2016 these results support this finding secondly to prevent emergence and spread of virulent strains additional management strategies are needed to increase durability of resistance the development of sustainable crop protection systems therefore requires cooperation between actors in the whole sector to achieve structural transformations in disease control 4 5 further research to simulate farmers decision making we used the consumat approach a well founded theory on human behaviour and previously used to simulate farmers decision making the implementation of the framework was supported by data from the literature on farmer behaviour and results from interviews with dutch potato farmers the model was able to reproduce patterns and trends observed in reality e g boom and bust cycles which supports the validity of the model framework grimm et al 2005 however different model structures at the micro scale can lead to the same emergent patterns at the macro scale schulze et al 2017 methods to validate processes on human behaviour include expert validation and role playing games ligtenberg et al 2010 secondly alternative models of decision making could be implemented to analyse the sensitivity of the results to different assumption of human decision making schlüter et al 2017 these methods are important steps for further research besides implementing alternative theories on human behaviour we identified some other relevant processes that could be implemented for further research we made simplifications on model components such as management strategies the farmer population the network structure and market effects although the model framework proved to be sufficient considering the purpose of this study these processes could be extended in future research additional data collection could contribute to the implementation of these processes for example collecting survey data of the farmer population secondly in the current model the landscape consisted of farmers that each manage one potato field while in reality farmers can have multiple fields spread over the farm these potato fields can be managed in different ways and farmers usually grow a number of different potato varieties also as a way of risk management the current model structure represents decision making at field level however this result in a relatively high number of farmers in the model landscape thirdly in the model we included only one susceptible and one resistant variety currently a number of different resistant varieties is available with resistance genes from different sources more diversity in crop resistance can potentially reduce the risk on resistance breakdown and spread of virulent strains lof and van der werf 2017 mundt 2014 lastly since potatoes are reproduced vegetatively by the use of seed potatoes it takes some time to increase the production of newly introduced potato varieties the availability of seed potatoes can therefore constrain a rapid adoption of new resistant varieties it would be interesting to implement these factors in the model to analyse the effect on the adoption of resistant varieties the allocation of susceptible and resistant fields in the landscape and resistance durability in the model stakeholders such as breeding companies the government and the market were represented as drivers of the system which influenced farmers decision making however each of these stakeholders have their own objectives and interests which leads to various types of interactions such as competition cooperation and trading pacilly et al 2016 agent based models are very suitable to include multiple types of agents and their interactions as a next step it would be interesting to explore the interactions between farmers other stakeholders and late blight dynamics with respect to the use of crop resistance in late blight control it would be mainly interesting to focus on the role of breeding companies and the effect of breeding and marketing strategies on late blight control 5 conclusion in this paper we combined a framework on farmer behaviour to an epidemiological framework on potato late blight to explore the use of crop resistance in disease control the framework on farmers decision making was based on the consumat approach and supported by data from literature on farmer behaviour and interviews with dutch potato farmers after introduction of a new resistant variety the model reproduced a so called boom and bust cycle the percentage of farmers growing the resistant variety increased boom until resistance breakdown occurred by emergence and spread of a virulent strain and in response farmers switched to other potato varieties and management strategies bust by exploring the interactions between farmer behaviour and late blight dynamics the model increased insights in the factors and processes that could affect the adoption of a resistant potato variety and resistance durability for example a higher crop price and yield of the resistant variety increased the adoption by farmers however also a large number of farmers continued growing the susceptible variety with fungicides which suggests that cooperation in the whole potato sector is needed to achieve structural transformations in disease control in addition the high risk on resistance breakdown stresses the importance of resistance management strategies to increase resistance durability it was found that emergence and spread of the virulent strain is the result of interactions between management strategies of farmers the weather conditions and the allocation of potato varieties by exploring the social ecological interactions related to disease control the model contributed to the field of social ecological system research and agent based modelling the number of models that tackles two way feedbacks between social and ecological systems is scarce also due to the inherent complexity of such systems filatova et al 2013 parker et al 2008 schulze et al 2017 this study provides a framework for linking decision making processes of farmers to disease dynamics in an agent based model implementing these two way linkages allowed us to explore non linear dynamics and feedback mechanisms within the social ecological system this approach could be useful for a whole range of systems focussing on management of emerging infectious diseases of crops acknowledgements we would like to thank the strategic research programme complex adaptive systems ip op cas of wageningen university research for financing this research the contribution of jg was partly funded by the cgiar research program on roots tubers and bananas rtb and supported by cgiar fund donors appendix a model description the model description follows the odd overview design concepts details protocol for describing agent based models grimm et al 2006 2010 in the model description we focus on the processes related to farmers decision making for more details on the epidemiological framework we refer to pacilly et al 2018 a 1 model purpose the aim of the model is to simulate the interactions between farmers decision making and late blight dynamics in an agricultural landscape with potato fields the model is used to simulate the use of crop resistance in disease control by analysing the adoption of the resistant variety by farmers and the durability of resistance over time table a 1 overview of late blight management strategies implemented in the model table a 1 potato variety fungicide application no yes susceptible sus sus resistant res res fig a 1 overview of behavioural strategies in relation to farmers satisfaction and uncertainty according to the consumat approach fig a 1 a 2 entities state variables and scales the model includes three types of entities farmers grid cells and agricultural fields the model represents an agricultural landscape of 10 10 km2 and the grid cells are 200 200 m2 4 ha the model is populated by farmers each of whom manages one potato field which consists of one or more grid cells a network was initialised in which farmers are connected to the closest farmers around them shortest distance which represents a network of neighbours an overview of farmer variables is shown in table a 2 for the state variables and model parameters related grid cells we refer to pacilly et al 2018 in the model farmers select one of four late blight management strategies for their field table a 1 farmers can choose between a susceptible or late blight resistant potato variety with or without the use of fungicides these strategies have different effects on field and landscape performance field performance was analysed for criteria including yield income and infection level to calculate farmers income the crop price pm was set at 13 per 100 kg 1 and the fungicide costs fc at 50 per application for the decision making processes we used behavioural strategies according to the consumat approach jager and janssen 2012 jager et al 2000 janssen and jager 2001 to evaluate their field performance farmers calculate the actual estimated predicted and potential performance per criterion these results are used to determine farmers satisfaction and uncertainty levels which leads to one of the following behavioural strategies repeating imitating optimising and social comparison figure a 1 the decision making process is influenced by personal characteristics including farmers need satisfaction and uncertainty tolerance level we distinguish four farmer types in the model which differ in the weights assigned to the criteria table 1 weights represent farmer preferences related to the criteria infection level yield and income processes on crop growth and disease dynamics are simulated at grid cell level the grid cells are characterised by location field number potato variety susceptible or resistant fungicide use and variables and parameters for crop growth and late blight infection pacilly et al 2018 we consider only one type of susceptible and resistant variety with one resistance gene we assume the resistant variety has a 20 lower potential yield compared to the susceptible variety which is reflected in the crop growth parameters two types of late blight are distinguished in the model the wild type and the virulent strain the wild type can only infect the susceptible variety while a virulent strain can also infect the resistant variety at the start of the simulation only the wild type is present the virulent strain can emerge during the growing season as the result of mutation to simulate disease dispersal we used an aged structured population model skelsey et al 2010 when spores germinate lesions first enter a latent phase of five days after which they become infectious and produce spores after the infectious phase lesions are added to the pool of no longer infectious tissue a fraction of the produced spores is dispersed by wind to nearby cells where they can cause infections since late blight development and crop growth is weather dependant we used measured weather data as input for the model table a 2 overview of farmer variables and parameters for each farmer the parameters were randomly selected based on a mean value μ or within a range see also table 1 table a 2 symbol description unit value farmer parameters fs field size ha μ 7 l network links no μ 5 ft farmer type 1 4 uf uncertainty tolerance level 0 1 sf need satisfaction 0 1 wd weight of infection level criterion 0 1 wp weight of income criterion 0 1 wy weight of yield criterion 0 1 farmer variables bs behavioural strategy ms management strategy fn fungicide applications mean no year 1 ed estimated infection level ep estimated income ha 1 ey estimated yield tonnes ha 1 ad actual infection level ap actual income ha 1 ay actual yield tonnes ha 1 pd potential infection level pp potential income ha 1 py potential yield tonnes ha 1 sd infection level satisfaction sp income satisfaction sy yield satisfaction st total satisfaction ud infection level uncertainty up income uncertainty uy yield uncertainty ut total uncertainty a 3 process overview and scheduling the time step in the model is one day and we simulate the potato growing season from may 1 to september 30 for 50 years processes in the model include figure a 2 1 crop growth and disease dynamics grid cells 2 update field performance farmers 3 calculate relative satisfaction and uncertainty farmers 4 select behavioural strategy farmers 5 select management strategy farmers and 6 predict field performance farmers processes on crop growth and late blight dispersal are updated on a daily step decision making processes of farmers 2 6 are executed at the end of the growing season within each submodel grid cells and agents are processed in a random order a detailed description of model processes can be found in section a 7 fig a 2 flow chart of the model processes focussing on farmers decision making fig a 2 a 4 design concepts basic principles the model is a spatial representation of the social ecological system of potato late blight management we focus on the interactions and feedbacks mechanisms between farmers decision making and disease dynamics in an agricultural landscape for the epidemiological processes we used the previously developed model pacilly et al 2018 and we added a social dimension of decision making on late blight control the framework on farmers decision making was based on the consumat approach jager and janssen 2012 janssen and jager 2001 and results from interviews with dutch potato farmers pacilly et al 2016 emergence both social and ecological dynamics are emerging from the model as a result of interactions between farmers decision making and disease dynamics including farmers behavioural strategies farmers late blight management strategies field performance for the criteria infection level yield and income and disease dispersal at landscape level adaptation during the simulation farmers can change their late blight management strategy four late blight management strategies are implemented in the model table a 1 at the end of each year farmers analyse their field performance and based on the results for the criteria on infection level yield and income their satisfaction and uncertainty is calculated according to the consumat approach one of four behavioural strategies is selected figure a 1 repetition imitation social comparison and optimisation based on their behavioural strategy farmers continue using the same late blight management strategy or select one of the other three strategies see section a 7 for more details objectives when farmers are unsatisfied and or uncertain one of the following behavioural strategies is selected according to the consumat approach imitation social comparison and optimisation in the case of social comparison and optimisation farmers aim to select a management strategy that results in a higher satisfaction by improving their field performance related to the criteria infection level yield and income prediction based on observed results farmers predict their field performance expected values for the three criteria infection level yield and income the expected values are calculated by taking the mean value using historical values of their own field of the last five years sensing farmers can sense the field performance of the farmers in their network as well as their late blight management strategy in the case of optimising behaviour farmers have information about the mean field performance per management strategy of the last growing season interaction agents interact by sensing the state variables of other agents in their network secondly the field performance of farmers is affected by the management strategies of other farmers in the landscape as a result of spatial interactions related to disease dispersal stochasticity at the start of the simulation the landscape is initialised in which farmers and potato fields are randomly allocated in the landscape management strategies are randomly divided over the farmers secondly for a number of farmer characteristics the values are randomly selected to create a heterogeneous population including the number of contacts links uncertainty tolerance level 0 1 need satisfaction 0 1 and farmer type farmer types differ in the weights which represent farmer preferences for the different criteria table 1 with respect to disease processes at the start of each year the infection is initialised in a fraction of the potato grid cells randomly selected weather data is used as input for the model and each year data of one year is selected from a dataset of 36 years during the growing season spores are dispersed by wind and every time step the wind direction is randomly selected northeast southeast southwest and northwest collectives each field is a collective of one or more grid cells which is managed by a farmer farmers select a management strategy for their field and they evaluate their field performance by using the mean value of the grid cells belonging to the field observation at the end of each year data on landscape level was recorded including behavioural strategies and management strategies of farmers as well as variables related to disease dispersal in the landscape secondly the mean performance of each management strategy was calculated for the criteria infection level yield and income in the model interface several graphs are presented to observe the output over time a 5 initialisation the model represents an agricultural landscape of 10 km 10 km with a potato density of 24 the model consists of 50 50 grid cells which represent a squared area of 200 m 200 m at the start of the simulation the landscape is initialised with 350 farmers that each manage one potato field with a mean size of 7 ha these parameters were derived from landscape data of a dutch agricultural region a network is initialised in which farmers are connected to the closest farmers around them with a mean number of 5 links per farmers at the start of the simulation we assume that all farmers grow a susceptible variety and the majority applies fungicides 90 of the farmers before the actual simulation started the model was first run for five years without decision making processes of farmers to create a list of reference values related to farmers field performance an overview of initial values of crop growth and late blight can be found in pacilly et al 2018 to create a heterogeneous population farmer characteristics were selected randomly see table 1 and section a 4 stochasticity a 6 input data meteorological data was used as input for the model to simulate crop growth and late blight dispersal during the growing season may 1 to september 30 data from two dutch weather stations was used eelde 1981 1993 and marknesse 1994 2016 in this way a dataset of 36 years of weather data was created mean daily temperature and total radiation was calculated and used to simulate crop growth secondly based on calculation rules using hourly temperature and humidity during a 24 h period we determined if a day was suitable for sporangia to cause infection skelsey et al 2009 on a so called blight day newly produced spores can cause infections as a result of spore germination see pacilly et al 2018 for more details a 7 submodels below the model procedures as shown in figure a 2 are described in more detail 1 crop and disease dynamics grid cells at the start of each year late blight infections are initialised in a fraction of the potato fields randomly selected during the growing season may till september processes related to crop growth and disease dynamics are simulated with a daily time step according to governmental regulations the potato haulm is destroyed when the disease severity in a field reaches 5 as a result crop growth stops directly and the disease can no longer disperse to other fields for a detailed description of these model processes we refer to pacilly et al 2018 2 update field performance farmers at the end of each year the actual and potential field performance is determined for farmers for each performance criterion i infection level yield and income a actual field performance i yield ay the mean potato yield is calculated tonnes per ha which is affected by the potato variety weather conditions and infection with late blight ii income ap income ha 1 is based on the actual yield and the price for potatoes minus costs for fungicide application equation a 1 the crop price pt is set at 13 per 100 kg 1 this value was derived from a dataset on potato prices in the netherlands from 2000 to 2017 wur 2018 for the susceptible and resistant varieties the same value is used costs for fungicides are related to the mean number of applications fn and the costs per application fc set at 50 a 1 a p a y p t 10 f n f c iii infection level ad to analyse the infection level a scale from 1 to 4 was developed using results on disease severity the percentage of infected leaf tissue where high values represent a high disease severity 1 0 1 2 0 1 1 3 1 5 4 5 b the potential performance is the maximum result which could be achieved in a specific year without any losses caused by the disease i potential yield py is determined by calculating the maximum yield that could be achieved for susceptible and resistant fields based on the weather conditions in that year temperature and radiation ii the potential income pp is calculated in the same way as the actual income but using the potential yield equation a 2 a 2 p p p y p t 10 f n f c iii the potential infection level was set at 1 for all management strategies which represents no or a very low infection level 3 calculate relative satisfaction and uncertainty farmers farmers calculate the relative uncertainty and satisfaction for each performance criterion i the overall uncertainty and satisfaction is influenced by the weights weights represent farmer preferences for the different criteria a satisfaction is defined as the ratio between the actual field performance ai and the potential field performance pi for each performance criterion i the total satisfaction is based on the satisfaction level for each criterion and their weights wi equation a 3 a 3 s t w i a i p i b uncertainty is defined as the ratio between the actual field performance and the estimated value ei the total uncertainty is based on the uncertainty for each criterion i influenced by the weights equation a 4 a 4 u t w i a i e i 4 select behavioural strategy farmers farmers compare their relative satisfaction and uncertainty level st and ut to their personal need satisfaction sf and uncertainty tolerance level uf based on the consumat framework farmers select one of four behavioural strategies figure a 1 a if unsatisfied and uncertain st sf and ut uf social comparison b if unsatisfied and certain st sf and ut uf optimisation c if satisfied and uncertain st sf and ut uf imitation d if satisfied and certain st sf and ut uf repetition 5 select management strategy farmers according to their behavioural strategy farmers select a management strategy a social comparison farmers select the criterion they want to improve infection level yield or income therefore farmers compare the results of their field for each performance criterion i by calculating the weighted satisfaction oi which is based on the satisfaction level and the weights equation a 5 a 5 o i s i 1 w i b the criterion with the lowest score is selected by farmers which represents the criterion they want to optimize for this criterion farmers compare the performance of the other farmers in their network and take over the management strategy of the farmer with the best result c optimisation farmers select the criterion they want to optimize similar to social comparison farmers compare the mean performance of all four management strategies based on the results of the previous year and adopt the management strategy that has the best result for the criterion the farmer wants to optimize if management strategies were not used by farmers the results of the previous year are used since the management strategies including the resistant variety with and without fungicides are not used by farmers at the start of the simulation the potential values are used which represents the mean field performance of these two strategies when the resistant variety with and without fungicides have the same highest score it is assumed that farmers select the resistant variety without fungicides d imitation farmers adopt the management strategy which is used by the majority of farmers in their network if this includes two or more strategies one of these strategies is randomly selected e repetition farmers don t change their management strategy 6 predict field performance farmers farmers estimate the value ei for each performance criterion i for the coming year therefore they calculate the mean value using historical values of their own field of the last five years appendix b results alternative network structure table b 1 year of resistance breakdown as a result of establishment of the virulent strain the effect of farmer network structure was analysed standard settings mean number of links per farmer 5 and farmers are connected to the closest farmers around them shortest distance higher number of links mean number of links per farmer 10 and farmers are connected to the closest farmers around them connecting farmer types mean of links per farmer 5 and farmers are connected to the closest farmers around them of the same farmer type for each scenario the model was run for 100 times table b 1 network structure year resistance breakdown percentage of runs 0 10 10 20 20 30 30 40 40 50 50 standard settings 41 17 5 4 6 27 higher number of links 41 14 5 2 10 28 connecting farmer types 44 14 10 4 6 22 table b 2 management strategies of farmers at the end of the simulation year 50 in case resistance breakdown occurs and in case the resistance remains effective the effect of farmer network structure was analysed standard settings mean number of links per farmer 5 and farmers are connected to the closest farmers around them shortest distance higher number of links mean number of links per farmer 10 and farmers are connected to the closest farmers around them connecting farmer types mean of links per farmer 5 and farmers are connected to the closest farmers around them of the same farmer type mean values are shown sd based on 100 runs table b 2 network structure management strategies of farmers sus sus res res resistance breakdown occurs standard settings 0 9 0 7 92 0 3 7 0 2 1 4 6 9 3 5 higher number of links 0 9 0 6 91 8 3 6 0 2 1 2 7 0 3 6 connecting farmer types 1 0 0 9 89 3 5 0 0 6 3 2 9 1 4 6 resistance remains effective standard settings 0 6 0 6 79 0 2 4 19 5 3 1 1 0 1 9 increasing the number of links 0 7 0 7 79 3 3 4 19 7 3 0 0 3 0 6 connecting farmer types 0 4 0 4 74 2 3 2 24 9 3 2 0 5 1 2 appendix c model initialisation fraction of farmers growing a susceptible variety without fungicides table c 1 year of resistance breakdown as a result of establishment of the virulent strain the fraction of farmers growing a susceptible variety without fungicides at the start of the simulation was varied for each parameter value the model was run 100 times table c 1 initial fraction of farmers growing a susceptible variety without fungicides sus year resistance breakdown percentage of runs 0 10 10 20 20 30 30 40 40 50 50 0 00 0 0 0 0 0 100 0 05 32 9 7 4 3 45 0 10 41 17 5 4 6 27 0 15 56 14 3 2 1 24 0 20 67 13 7 2 3 8 0 25 76 8 6 1 1 8 0 30 82 11 4 0 3 0 table c 2 management strategies of farmers at the end of the simulation year 50 in case resistance breakdown occurs and in case the resistance remains effective the fraction of farmers growing a susceptible variety without fungicides at the start of the simulation was varied the other farmers are growing a susceptible variety with fungicide application mean values are shown sd based on 100 runs table c 2 initial fraction of farmers growing a susceptible variety without fungicides sus management strategies of farmers sus sus res res resistance breakdown occurs 0 00 0 05 0 5 0 5 92 6 3 4 0 1 0 5 6 8 3 3 0 10 0 9 0 7 92 0 3 7 0 2 1 4 6 9 3 5 0 15 1 1 0 6 90 9 4 0 0 0 0 1 8 0 3 9 0 20 1 5 0 7 90 2 3 8 0 0 0 1 8 3 3 7 0 25 1 9 0 7 89 6 4 2 0 0 0 2 8 5 4 1 0 30 2 1 0 9 89 1 4 0 0 0 0 1 8 8 4 0 resistance remains effective 0 00 0 0 0 0 81 1 2 6 18 9 2 6 0 0 0 0 0 05 0 4 0 4 79 9 2 2 19 5 2 3 0 3 0 6 0 10 0 6 0 6 79 0 2 4 19 5 3 1 1 0 1 9 0 15 1 0 0 5 77 0 3 5 20 7 4 1 1 3 2 1 0 20 1 3 0 8 75 7 3 6 22 9 4 0 0 2 0 4 0 25 1 7 1 0 74 6 2 1 23 4 2 2 0 4 0 5 0 30 
