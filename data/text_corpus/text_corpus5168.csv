index,text
25840,to mitigate the unavailability of accurate river channel terrain data for flood modeling this work develops a new approach to construct channel terrain based on available global coarse digital elevation models dems compared to other established methods the approach constructs the river surface using the sparse cross section data from available coarse dems and not measurements for a 134 km river reach of the yangtze river the absolute percentage error of simulated peak discharge with the coarse dems is 76 77 and the delay time of peak discharge is 13 h however with the reconstructed terrain data are 12 13 and 0 h they are 9 70 and 1 h on a 382 km river reach the simulation results show that the approach can be used to reliably predict the flood propagation process without fine terrain data the proposed approach can be used to improve flood management in areas without fine river terrain data keywords river topography flood routing spatial interpolation linear interpolation numerical simulation 1 introduction with economic development and global population growth the population density in urban areas near rivers or lakes is increasing which has led to the loss of life and property when flooding and other natural hazards occur luo et al 2020 flooding poses the most common threat to humans in almost every part of the world and has caused more than one third of the total economic loss due to natural hazards globally additionally two thirds of all people impacted by natural hazards are affected by floods duan et al 2019 xia et al 2019 predicting the path area and occurrence time of floods can help reduce losses through establishing effective protection and rescue plans numerical simulations based on the solution of the shallow water equations swes are the most suitable approach for generic flood prediction considering the complexity of the problem and the number of potential situations bates et al 2010 ziliani et al 2019 hydrodynamic models based on the swes are effective tools for simulating water flows and are widely used to study flood forecasting bermudez et al 2017 luo et al 2018 and sediment transport teresa contreras and escauriaza 2020 huo et al 2021 a high performance hydrodynamic model is necessary for the accurate simulation of dam break flood evolution because the level and discharge of dam break floods can change drastically in a short period aureli et al 2015 the performance of hydrodynamic models is mainly affected by the model structure e g governing equations and numerical computation method and the precision of model parameters e g manning s roughness coefficient and model inputs e g initial and boundary conditions such as those based on the inflow hydrograph and riverbed geometry tiny changes in the model structure model parameters and model inputs can produce completely different results cao et al 2019 therefore a reasonable model structure appropriate model parameters and accurate model inputs are needed in high performance hydrodynamic models input data for hydrodynamic models often include terrain data and the accuracy of these data can seriously affect the accuracy of simulation results terrain data which determine slope gradients the flow direction channel networks and other factors related to spatial water movement are indispensable input data in hydrodynamic models digital elevation models dems are an important source of terrain data used in analyzing and visualizing surface landforms nevertheless two issues with dems affect flood simulation results on the one hand coarse dems cannot meet the requirements of most hydrodynamic models because the precision of river channel representation is unsatisfactory fig 1 due to the inability of river bed elevations to be captured by remote sensing which cannot penetrate the water surface coarse dems have not only a lower resolution but also poor quality ali et al 2015 coarse dems deteriorate the performance of hydrodynamic models due to the poor representation of the real topography shen and tan 2020 in some cases the vertical error in coarse dems is larger than the amplitude of most flood waves archer et al 2018 on the other hand complete fine scale dems are difficult to obtain in large catchments for instance lidar light detection and ranging and sonar sound navigation and ranging techniques are ideal ways to obtain topographical data but riverbeds which are covered with water and imperative in flood forecasting can be difficult to survey with lidar systems bailly et al 2010 and very shallow water is difficult to detect with sonar systems engquist et al 2017 due to the limitations of measurement technology and the flood forecasting time numerical methods have become practical and effective for constructing riverbed terrain currently a simple approach is used in modeling systems such as hec ras hydrologic engineering center river analysis system hec davis ca usa and mike 11 dhi h√∏rsholm denmark the results of which depend on the arbitrary determination of the river centerline additionally some classical approaches to spatial interpolation have been applied in river channel reconstruction for example seven spatial interpolation techniques have been used to study the effect of spatial trends on the interpolation of river bathymetry merwade 2009 based on the research of sun et al 1996 parametric cubic splines pcss were used to extract a set of equally spaced nodes in a simplified 2d model for river meander migration motta et al 2012 comparatively sophisticated approaches have also been presented bend oriented bathymetry interpolation has been proposed to improve river stage forecasting and this approach is feasible for bathymetric interpolation in rivers with apparent bends lin et al 2018 the quartic hermite spline with parameters qhsp method for constructing 3d river channel terrain based on limited cross section data has also been presented hu et al 2019 a software toolbox called riverbox has been developed for the reconstruction of riverbeds using the scripting language python 2 7 implemented in arcgis 10 5 1 dysarz 2018 with the unique opportunity provided by advances in spatial interpolation technology this work aims to answer the following research questions 1 is a single swe based hydrodynamic model able to reliably simulate the fluvial flooding process in low resolution and low precision dems areas 2 what are the dominant factors that influence the prediction accuracy of fluvial flood modeling in low resolution and low precision dems areas based on hydrodynamic models to answer these two research questions a new river channel reconstruction method using low resolution and low precision dems is developed in this work the new reconstruction method enables the simulation of the fluvial flooding process at a dems resolution of 30 m from baige to benzilan in the jinsha river china the remainder of this paper is organized as follows in section 2 river channel reconstruction methods are introduced section 3 provides information about the 2018 baige floods and the datasets required for the simulations section 4 presents and discusses the results of river channel reconstruction and flood simulation and finally brief conclusions are drawn in section 5 2 methodology 2 1 dems reconstruction a river channel is a long and narrow zone that is different from the landscape on both sides the topography of the river channel is a water conservancy channel and the water flowing in a watershed converges and connects with water from other blocks thus a channel network has strong connectivity and the flow rate increases gradually with decreasing network elevation based on the characteristics of river landforms we propose the hypothesis that riverbed elevations in downstream cells are not larger than those in upstream cells and develop a trend smoothing method tsm to process coarse river channel terrain data the tsm process is as follows fig 2 boundary definition for the extent of river channels is the first step in the tsm high resolution multiyear remote sensing images such as those provided by google earth are used to extract river boundaries the second step is to select a channel cross section and extract the minimum elevation value in the cross section we established cross sections every 5 km from upstream to downstream in this study then smoothing the minimum elevation value in the cross section sequence is necessary to satisfy the basic hypothesis because of data limitations fig 1 and we do so with a moving average approach fig 3 next we estimate the surface of the riverbed from the points after smoothing many methods of spatial interpolation can be selected such as kriging and spline interpolation in this study the shape of the river cross section exceeding the main aim of this work is not considered and a rectangle is the default shape six interpolation methods are selected to estimate the surface of the riverbed and the depression filling algorithm dfa commonly used in hydrological analysis is used for comparison with the tsm the description of these methods is as follows 1 dfa sinks are often errors due to the resolution of the data or the rounding of elevations to the nearest integer value sinks should be filled to ensure that basins and streams are properly delineated the dfa uses the flow direction and sinks to locate and fill surface depressions in dems the method can be used to create hydrologic elevation models and fill depressions to create a dem without depressions a flow path grid and a grid with watershed basins 2 global polynomial interpolation global polynomial gp interpolation fits a smooth surface defined by a mathematical function a polynomial to the input sample points the trend surface changes gradually and captures coarse scale patterns in the data 3 inverse distance weighted interpolation inverse distance weighted idw interpolation determines cell values using a linearly weighted combination of a set of sample points li et al 2020 the weight is a function of the inverse distance the surface being interpolated should be that of a locationally dependent variable 1 z p i 1 n w i z i where z p is the predicted value for unknown points z i is the value at a given point with a measured elevation and w i is the weight assigned to z i the numbers of sample points and the corresponding weights affect the idw interpolation results in this study 12 neighboring sample points are used and the power of distance is set to 2 4 universal kriging kriging is an advanced geostatistical procedure that generates an estimated surface from a scattered set of points with z values 2 Œ≥ h 1 2 n i 1 n z x i z x i h 2 where x is a point in the data set z x is the corresponding elevation and n is the number of pairs of sample points separated by a distance h all kriging estimators are variants of the following basic equation 3 z ÀÜ x 0 Œº i 1 n Œª i z x i Œº x 0 where Œº is a known stationary mean of the entire domain Œº x 0 is the mean elevation of points in the search window i is the kriging weight linked to h sampson et al 2013 and n is the number of sample points included in the estimation in the universal kriging uk algorithm three simplifications are made based on eq 3 1 Œº is replaced with Œº x 0 2 i n is set equal to 1 and 3 Œº x 0 is the trend component of each point in the neighborhood search window this study selects universal kriging with linear drift and 12 neighboring sample points 5 thin plate spline tps interpolation a raster surface can be interpolated from points using the two dimensional minimum curvature spline technique the resulting smooth surface passes exactly through the input points tps interpolation is achieved by minimizing a criterion function that balances the tradeoff between the representation of the data and the smoothness of the interpolated surface chen and li 2019 specifically the objective function of tps interpolation is expressed as 4 min f z f 2 Œª t f where is the euclidean norm Œª is a smoothing parameter with the optimal value determined by generalized cross validation or the trial and error method and t f represents the penalty term of the smoothness 6 topo to raster the topo to raster topor interpolation method is specifically designed for the creation of hydrologically correct dems topor interpolation uses a finite differential interpolation technique and it is optimized to streamline the local interpolation computational method such as by employing idw interpolation without loss of continuity for the surfaces obtained by the kriging or spline radial basis function interpolation method the difference is defined based on the first and second partial derivatives of f in the interpolation method as described by the following equation arseni et al 2019 5 j h 1 2 h 2 f x 2 f y 2 d x d y f x x 2 f x y 2 f y y 2 d x d y essentially topor is a combined interpolation method that uses a discrete technique based on spline polynomial functions of degree m and smoothness k where the roughness can be modified for the generation of dems with steep field changes such as those in areas with notable currents ridges or cliffs these landforms are also called local maximums or local minimums in the present research point elevation features are used 7 tsm based on the decreasing trend in channel elevation from upstream to downstream we recommend piecewise linear fitting for the riverbed elevation and the establishment of a variable characterizing distance changes in river flow direction combined with the spatial shape of the jinsha river study area the latitude gradient represents the general direction of the river channel in this study 2 2 numerical simulation 1 mathematical formulation in many engineering hydrodynamic problems the full 3d flow effects can be simplified when the horizontal dimensions in physical domains are much larger than the water depth such as in floodplains large river reaches shallow lakes etc the 2d swes can be derived by integrating the three dimensional 3d reynolds averaged navier stokes equations using the hydrostatic assumption in matrix form the conservation law of the 2d swes can be written as follows liang and marche 2009 liang 2010 6 q t f x g y s where t represents time x and y are the cartesian coordinates q is the vector of flow variables f and g are the flux vectors in the x and y directions respectively and s is the source vector composed of slope source terms s b and friction source terms s f the vector terms are usually given by 7 q h q x q y f u h u q x g h 2 2 u q y g v h v q x v q y g h 2 2 8 s s b s f 0 g h z b x g h z b y 0 c f u u 2 v 2 c f v u 2 v 2 where h is the water depth u and v are the depth averaged velocities in the x and y directions respectively q x uh and q y vh denote the unit width discharges in the x and y directions respectively g is gravitational acceleration z b represents the bed elevation and c f is the bed roughness coefficient which is generally computed by gn 2 h 1 3 in this case n is the manning coefficient 2 numerical method the numerical model for solving the swes involves an explicit godunov type cell centered structured finite volume scheme the swes are discretized into algebraic equations by the finite volume method the fluxes of mass and momentum are computed by the hllc harten lax van leer contact approximate riemann solver i e harten lax and van leer approximate riemann solver with the contact wave restored the slope source terms are evaluated by the slope flux method proposed by hou et al 2013 the friction source terms are calculated by the improved implicit method when computing the fluxes and the slope source terms the values at the midpoints of the cell edges are required the two stage explicit runge kutta approach is applied to update the flow variables to a new time level these values are evaluated by a novel 2d edge based muscl monotonic upwind scheme for conservation laws scheme the code is programmed using c and cuda compute unified device architecture which can run on gpus graphics processing units to substantially accelerate the computations 3 case study the 2018 baige landslide dam breach flood event is simulated in this study to demonstrate the performance of the new river channel reconstruction method this section describes the key hydrological and geographical information associated with the event and the key data used to support the simulations 3 1 description of the 2018 baige event in the jinsha river the jinsha river drainage is located in the upper reaches of the yangtze river basin 90 105 e 24 36 n with an area of approximately 470 000 km2 thus accounting for 26 3 of the yangtze river basin fig 4 the river length is approximately 768 km the area is influenced by tropical monsoons subtropical monsoons plateau monsoons and complex terrain the climates in the basin are complex and diverse and they include a cold semiarid plateau climate and a subtropical humid monsoon climate the average annual temperature in the basin is 3 0 c the temperature increases generally from the northwest to the southeast the average annual temperature in the upper area is below 0 0 c and that in the middle and lower reaches is above 19 0 c the average annual precipitation is 753 mm and this total increases from the northwest to the southeast with downstream precipitation being 3 83 times that of the source area the flood season of the jinsha river is from may to october the runoff and sediment in the flood season account for 68 2 80 2 and 85 5 98 2 of the annual totals respectively lu et al 2019 the total elevation change in the jinsha river basin is 3300 m and hydroelectric production reaches more than 100 million kw each year accounting for more than 40 0 of the hydraulic energy resources of the yangtze river basin an important hydropower base in china the jinsha river basin is prone to large scale land sliding and many historical landslide damming events have been recorded along the mainstream and tributaries of the jinsha river wang et al 2014 in late 2018 oct 11 and nov 3 2018 two large scale landslides occurred successively in the same location in baige village boro town tibet china the entire slope rock mass slid rapidly and blocked the river causing upstream inundation and downstream flooding although the first landslide dam breached naturally the second formed a barrier lake at the same place with a volume of approximately 524 million m3 thus seriously threatening the lives of the people living upstream and downstream despite immediate and prolonged disaster mitigation measures natural and manual breaching of the dam caused floods in the lower reaches of the jinsha river inundating 3400 houses and destroying a 3 5 103 km2 area of cropland more than 102 000 people were evacuated from towns at risk in sichuan tibet and yunnan provinces ouyang et al 2019 zhang et al 2019a 2019b the flood inundated many houses in the towns of judian and shigu additionally many roads were blocked bridges were destroyed and farmland areas were flooded fan et al 2020 here we focus on the dam breach flooding from baige to benzilan a reach of 382 km downstream on november 3 2018 although several dams have been planned to be constructed in this reach none have been built or were in operation during the baige event 3 2 data the data used to establish the model include a dem of the entire basin flood inflow locations and the flood flow data collected at several downstream hydrological stations the dem which is produced from advanced spaceborne thermal emission and reflection radiometer global digital elevation model version 2 data at a 30 m spatial resolution is provided by the geospatial data cloud site computer network information center chinese academy of sciences http www gscloud cn the flood inflow locations and flood flow data observed at several downstream hydrological stations fig 5 were published by the bureau of hydrology changjiang water resources commission of china 4 results and discussion several simulations are performed for the flood events involving the jinsha river the manning coefficient for the river channel is chosen to be 0 02 s m 1 3 which is a typical value for the jinsha river as suggested in the existing literature hou et al 2020 the hourly step discharges fig 5 are entered along the northern border in the boundary conditions file while the southern border is set using the free boundary condition and other borders are set as wall boundaries the observed discharge before the dam break is used to pre run the model over a dry bed to produce the initial flow conditions for all subsequent simulations considering the numbers of computations and computational efficiency the yebatan batang reach of the jinsha river length of 134 km is selected for the preliminary study in section 4 1 and the bage benzilan reach length of 382 km is selected for the improved study in section 4 2 the performance of the simulated results is evaluated in terms of the absolute percentage error ape delay time dt nash sutcliffe efficiency nse and coefficient of determination r 2 the ape and dt refer to the relative errors in the peak discharge and absolute errors in the peak time between simulated and observed values respectively the nse and r 2 are comprehensive criteria for evaluating the consistency between simulated and observed values the ape dt nse and r 2 are expressed as follows 9 a p e m p s p m p 100 10 d t t s p t m p 11 n s e 1 i 1 n m i s i 2 i 1 n m i m 2 12 r 2 i 1 n m i m s i s 2 i 1 n m i m 2 i 1 n s i s 2 where m p and s p are the observed and simulated peak discharge and whose times are t mp and t sp respectively n is the length of data series m i and s i are the observed and simulated discharge at the ith hour and whose averages are m and s respectively 4 1 channel construction for the yebatan batang reach of the jinsha river 4 1 1 interpolation method the smoothed minimum elevation value in each cross section is the only input required for the analysis of channel reconstruction with respect to the seven approaches dfa gp idw uk tps topor and tsm in the spatial interpolation of river channels proper boundary identification is extremely important in the second type of analysis elevation differences as final targets for channel reconstruction are compared in the study area the main channels are identified by remote sensing recognition and measured the elevation difference after interpolation fig 6 shows the relationship between the elevation and distance produced with several methods both maps clearly illustrate the different patterns of channel reconstruction obtained with the analysis methods dfa only fills on the basis of the original dem od data to form gently sloping terrain data although this approach is greatly affected by the input terrain data and the flow of water does not decrease with channel elevation table 1 summarizes the main elevation statistics 5 km downstream of yebatan topor overcurves the middle portion of the plot at the channel cross section point and thus cannot represent the actual river terrain the results of idw uk tps and tsm are ideal fig 6b shows the river topography generated with the idw uk tps and tpm methods in 8 km segments of the river in the study area idw and uk yield large sags in the channel which do not represent the actual channel elevation however the channel terrain generated by tps and tsm is relatively smooth the three dimensional dem obtained by tsm at 99 e and 30 n is shown in fig 7 and the channel terrain is continuous compared with the plot in fig 1 the interpolation methods and data quality severely affect the accuracy of the final interpolated surface the reasons are 1 most isotropic methods used for the spatial interpolation are not designed to handle the continuous bending river channel terrain and 2 the interpolation of sparse data e g cross sections lose this trend merwade 2009 therefore in sections 4 1 2 and 4 1 3 tps and tsm are used to generate high precision topographic data for the region 4 1 2 interpolation distance to determine the influence of interpolation section spacing on the flood evolution simulation results tps is used to interpolate sections of the river with 5 km 10 km 15 km and 20 km spacing in the study area which are named s05 s10 s15 and s20 respectively the tsm method is used as follows taking the inlet elevation of the study area as the starting point and the outlet elevation as the end point the channel elevation is linearly interpolated at intervals of 5 km 10 km 15 km 20 km 30 km 40 km and 60 km according to the latitude gradient of the channel corresponding to x05 x10 x15 x20 x30 x40 and x60 respectively only the inlet and outlet cross sections are used to linearly interpolate the latitude gradient and construct a channel topography with a uniform channel gradient named xd to compare the influence of the river terrain representation method on the flood evolution simulation results od was used as a reference for the simulations the number of cells in the calculation domain was 579 3603 and the simulation time was 60 h fig 8 and table 2 show the results of the simulated discharge process for the interpolation of the river channels with different cross section spacings between yebatan and batang there is a certain difference between the simulated and measured outflow process data at batang station and the od simulation accuracy is the worst nse 15 23 r 2 0 20 because the channel terrain data deviate from the actual elevation and there are false depressions resulting present thus resulting in poor simulation accuracy the flood discharge process in river terrain simulations using tps is delayed by 1 h overall and the simulated peak discharge is smaller than the measured value ape 40 the effects of different section spacings on the flood evolution simulations are not very significant notably the overall nse is less than 0 15 and the differences do not exceed 0 08 the generated river terrain using the tsm method is greatly affected by the interpolated cross section spacing specifically the effectiveness of flood evolution simulation improves as the interpolated cross section spacing increases the flood peak discharge also displays an increasing trend with increasing interpolation section spacing with the ape displaying a decreasing trend the xd simulation is the best out of all the calculation examples nse 0 92 r 2 0 96 and the simulated results are similar to the actual measurements for the same interpolation section spacing the channel simulations generated by tsm are better than those generated by tps mainly because the channel terrain generated by tps is stepped and not as smooth as the tsm channel terrain as glenn et al 2016 pointed out dems accuracy is influenced by the interpolation distance and the coordinate system rather than the specific interpolation method and the choice of cross sections location method which consequently affect model results that rely on terrain input 4 1 3 recession depth to improve the accuracy of model calculations which is decreased by the large number of false floodplains in simulated river channels it is necessary to recess the river channel terrain to a certain extent so that the flow path is concentrated in the main channel fig 9 specifically the s05 river channel is recessed 5 m 10 m 15 m 20 m 30 m 50 m denoted as s05d05 s05d10 s05d15 s05d20 s05d30 and s05d50 respectively the same process is used for xd to obtain xdd05 xdd10 xdd15 xdd20 xdd30 and xdd50 the simulated flow process results fig 10 for different recession depths in the yebatan batang reach suggest that the influence of river channel depth on the simulation of flood evolution is not as obvious as that for the interpolation interval the ape and dt of peak discharge on the terrain with recession is similar to that without recession the simulation effect of the flood evolution based on s05 displays a threshold phenomenon with the digging of the river channel that is when the recession depth is less than 20 m the simulation accuracy increases with increasing digging depth and when the recession depth is greater than 20 m the simulation accuracy decreases with increasing recession depth when the recession depth is 20 m the simulation accuracy is the highest in s05 nse 0 11 r 2 0 87 table 3 the accuracy of flood evolution simulation based on xd increases with increasing recession depth when the recession depth of xd is 50 m the nse and r 2 are 0 94 and 0 98 respectively 4 2 channel construction for the baige benzilan reach of the jinsha river according to section 4 1 the tsm method produces the best river terrain reconstruction and flood simulation results the interpolation interval is the second key factor that influences the interpolation results but the effect of adjusting the recession depth is not obvious to verify the main factors that affect the accuracy of river terrain reconstruction and flood simulation the tsm method is used to interpolate the reach of the jinsha river from baige to benzilan the number of cells in the calculation domain was 1902 10443 and the simulation time was 60 h 4 2 1 interpolation distance similar to the approach in section 4 1 the interpolation interval is the main research variable and the tsm method is used for interpolation the interpolation intervals are 10 km 20 km 50 km 100 km and 190 km and the simulations are named ls010 ls020 ls050 ls100 and ls190 respectively additionally the entire interval is still selected for interpolation and named lsd fig 11 displays the simulation results for a dam break flood at different interpolation intervals in the baige benzilan section of the jinsha river compared with the values measured at the yebatan batang and benzilan stations the simulation results display a longer delay time and a smaller peak discharge as the distance downstream increases the peak flood time is delayed by approximately 1 h at yebatan station approximately 2 4 h at batang and 9 10 h at benzilan station table 4 the peak discharge ape is 3 98 18 17 at yebatan station 10 84 38 16 at batang station and 28 07 39 99 at benzilan station for different interpolation intervals the peak time and discharge associated with ls190 and lsd are better than those of the other simulations ls010 and ls020 have good results at yebatan station in terms of the flood simulation accuracy with a nse of 0 74 and 0 73 respectively and the results for ls190 and lsd are also satisfactory the terrain construction results at batang and benzilan are unsatisfactory at different interpolation intervals and the corresponding nse values are less than 0 50 notably the simulation results for ls190 and lsd at batang station are better than those at other stations and this finding is consistent with the conclusion obtained in section 4 1 that the larger the interpolation interval is the better the flood simulation effect the nse of the simulation results at benzilan station is less than 0 and the r 2 value is less than 0 10 4 2 2 channel width the flood simulation results in sections 4 1 2 and 4 2 1 indicate that the impact of the interpolation interval on the flood simulation results in long distance river sections is minimal although adopting an interval based approach can solve the time delay problems in flood simulations for short distance river sections because the width of the water surface increases as the level rises during the flooding process this study widens the reconstructed area of the river channel by expanding the channel area to the buffer area fig 12 which is identified by remote sensing images in this section the river channel terrain reconstruction process is the same as that used for lsd the buffer values are set to 50 m 100 m 150 m 200 m 250 m 300 m 350 m and 400 m and the corresponding result sets are named lsdbu050 lsdbu100 lsdbu150 lsdbu200 lsdbu250 lsdbu300 lsdbu350 and lsdbu400 respectively fig 13 displays the simulation results for a dam break flood at different channel widths in the baige benzilan section of the jinsha river overall the channel width has a notable influence on the timing of the flood peak and peak discharge when the river channel topography is reconstructed over a long distance and the simulations effectively reflect the site measured flood process the partially enlarged view indicates that the simulated flood peak time and peak discharge are advanced and increase as the buffer value increases when the value is less than 150 m additionally the simulated flood peak time and peak discharge are delayed and decrease as the buffer value increases when the value is greater than 150 m the simulated flood peak times at yebatan station for different buffer values are close to the measured values and the peak discharges are similar ape 6 92 at batang station the simulated flood peak times for lsdbu100 lsdbu150 and lsdbu200 are 2 h earlier than the observed values and the flood peak flows are approximately 5000 m3 s higher than the actual measurements ape 22 69 different channel widths yield large differences in the flood peak time 5 h dt 9 h and peak flow 2 76 ape 28 07 at benzilan station the simulated flood peak times for lsdbu100 and lsdbu150 are 5 h prior to the measured times and the peak discharges are approximately 3500 m3 s higher than the observation values table 5 shows the flood simulation accuracy for different buffer values and the simulation results are satisfactory for yebatan station with an nse of 0 85 and r 2 of 0 90 except for lsd at batang station the nse values of the calculation examples except lsd exceed 0 60 and r 2 exceeds 0 70 thus the simulation results are good the different buffer values at benzilan station yield large differences in the accuracy of the simulation results some simulations such as lsdbu50 lsdbu250 lsdbu300 lsdbu350 and lsdbu400 have good accuracy with nse and r 2 values of 0 70 but the results of other simulations are unsatisfactory the nse values of lsdbu50 lsdbu300 and lsdbu350 at each site reach 0 80 with r 2 of 0 85 and the simulation results reflect the actual flood process the range of dt over the different channel widths is from 5 h to 9 h at benzilan which demonstrates the influence of channel width on the results of hydrodynamic models moreover the same interpretation could be found in the research of benjankar et al 2015 except the cross sectional spacing channel width as one of river channel features chiefly determines the different results of hydrodynamic modeling 4 2 3 grid resolution when simulating large scale flood evolution events it is particularly important to understand the effect of grid resolution on the hydrodynamic simulation results therefore the dam break flood evolution at 60 m and 120 m resolutions is simulated and the results are compared with those for a 30 m resolution lsdbu050 for which the best results were obtained in section 4 2 2 is selected as the 30 m resolution control and then resampled to resolutions of 60 m and 120 m named lsdbu050dx60 and lsdbu050dx120 respectively fig 14 compares the flood process simulation results for the baige benzilan section of the jinsha river at different resolutions notably the peak time at each site is delayed and the peak flow decreases as the grid resolution decreases the peak times at yebatan station batang station and benzilan station for lsdbu050dx60 are delayed by 1 h 2 h and 5 h respectively additionally the simulated peak discharge is lower than the value of 4000 m3 s measured at yebatan station and the values at the other two stations are relatively close for lsdbu050dx120 peak time delays of 3 h 7 h and 19 h are observed at yebatan station batang station and benzilan station respectively and the simulated peak flows are lower than the measured values of 9500 m3 s 6700 m3 s and 5700 m3 s respectively the flood simulation accuracy at different grid resolutions is shown in table 6 the flood simulation results for lsdbu050dx60 at the yebatan and batang stations meet the relevant accuracy requirements with nse 0 5 although the flood simulation results for lsdbu050dx120 do not nse 0 5 the results show that a reliable hydrodynamic simulation of the channel flood process can be achieved using a grid that truly reflects the actual terrain features and channel shape 5 conclusions to improve the simulation accuracy of the river channel terrain in two dimensional flood evolution simulations a simple and rapid method for river channel terrain reconstruction is proposed in this paper the new river terrain reconstruction method is applied to predict the extreme flood events in the lower reaches of the jinsha river caused by the collapse of baige barrier lake at a spatial resolution of 30 m the simulation successfully reproduced the event and the results were compared with the existing observations in terms of the flood time and flood process several simulations of the jinsha river were performed for short distance and long distance reaches to effectively demonstrate the potential of the new river reconstruction method in efficient flood evolution simulation and flood forecasting progressive simulations were performed to check the reconstruction parameters of different river terrains a quantitative analysis of the simulation results showed that the flood prediction method was more sensitive to the interpolation interval in river reconstruction than the recession depth for short distance river reaches additionally the method was more sensitive to the channel width than the interpolation interval for long distance river reaches in the simulation of highly dynamic flood events in areas with low precision terrain data considered in this paper reliable predictions can be obtained through the smooth reconstruction of river terrain thus reducing the large amount of work required to measure high precision terrain data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is partly supported by the national key research and development program of china no 2016yfc0402704 national natural science foundation of china nos 51609199 52009104 shaanxi international science technology foundation of china grant no 2017 kw 014 water conservancy science and technology project of shaanxi province grant no 2017slkj 14 basic research on the sponge city of fengxi new town in xixian new area no 2018610002000097 
25840,to mitigate the unavailability of accurate river channel terrain data for flood modeling this work develops a new approach to construct channel terrain based on available global coarse digital elevation models dems compared to other established methods the approach constructs the river surface using the sparse cross section data from available coarse dems and not measurements for a 134 km river reach of the yangtze river the absolute percentage error of simulated peak discharge with the coarse dems is 76 77 and the delay time of peak discharge is 13 h however with the reconstructed terrain data are 12 13 and 0 h they are 9 70 and 1 h on a 382 km river reach the simulation results show that the approach can be used to reliably predict the flood propagation process without fine terrain data the proposed approach can be used to improve flood management in areas without fine river terrain data keywords river topography flood routing spatial interpolation linear interpolation numerical simulation 1 introduction with economic development and global population growth the population density in urban areas near rivers or lakes is increasing which has led to the loss of life and property when flooding and other natural hazards occur luo et al 2020 flooding poses the most common threat to humans in almost every part of the world and has caused more than one third of the total economic loss due to natural hazards globally additionally two thirds of all people impacted by natural hazards are affected by floods duan et al 2019 xia et al 2019 predicting the path area and occurrence time of floods can help reduce losses through establishing effective protection and rescue plans numerical simulations based on the solution of the shallow water equations swes are the most suitable approach for generic flood prediction considering the complexity of the problem and the number of potential situations bates et al 2010 ziliani et al 2019 hydrodynamic models based on the swes are effective tools for simulating water flows and are widely used to study flood forecasting bermudez et al 2017 luo et al 2018 and sediment transport teresa contreras and escauriaza 2020 huo et al 2021 a high performance hydrodynamic model is necessary for the accurate simulation of dam break flood evolution because the level and discharge of dam break floods can change drastically in a short period aureli et al 2015 the performance of hydrodynamic models is mainly affected by the model structure e g governing equations and numerical computation method and the precision of model parameters e g manning s roughness coefficient and model inputs e g initial and boundary conditions such as those based on the inflow hydrograph and riverbed geometry tiny changes in the model structure model parameters and model inputs can produce completely different results cao et al 2019 therefore a reasonable model structure appropriate model parameters and accurate model inputs are needed in high performance hydrodynamic models input data for hydrodynamic models often include terrain data and the accuracy of these data can seriously affect the accuracy of simulation results terrain data which determine slope gradients the flow direction channel networks and other factors related to spatial water movement are indispensable input data in hydrodynamic models digital elevation models dems are an important source of terrain data used in analyzing and visualizing surface landforms nevertheless two issues with dems affect flood simulation results on the one hand coarse dems cannot meet the requirements of most hydrodynamic models because the precision of river channel representation is unsatisfactory fig 1 due to the inability of river bed elevations to be captured by remote sensing which cannot penetrate the water surface coarse dems have not only a lower resolution but also poor quality ali et al 2015 coarse dems deteriorate the performance of hydrodynamic models due to the poor representation of the real topography shen and tan 2020 in some cases the vertical error in coarse dems is larger than the amplitude of most flood waves archer et al 2018 on the other hand complete fine scale dems are difficult to obtain in large catchments for instance lidar light detection and ranging and sonar sound navigation and ranging techniques are ideal ways to obtain topographical data but riverbeds which are covered with water and imperative in flood forecasting can be difficult to survey with lidar systems bailly et al 2010 and very shallow water is difficult to detect with sonar systems engquist et al 2017 due to the limitations of measurement technology and the flood forecasting time numerical methods have become practical and effective for constructing riverbed terrain currently a simple approach is used in modeling systems such as hec ras hydrologic engineering center river analysis system hec davis ca usa and mike 11 dhi h√∏rsholm denmark the results of which depend on the arbitrary determination of the river centerline additionally some classical approaches to spatial interpolation have been applied in river channel reconstruction for example seven spatial interpolation techniques have been used to study the effect of spatial trends on the interpolation of river bathymetry merwade 2009 based on the research of sun et al 1996 parametric cubic splines pcss were used to extract a set of equally spaced nodes in a simplified 2d model for river meander migration motta et al 2012 comparatively sophisticated approaches have also been presented bend oriented bathymetry interpolation has been proposed to improve river stage forecasting and this approach is feasible for bathymetric interpolation in rivers with apparent bends lin et al 2018 the quartic hermite spline with parameters qhsp method for constructing 3d river channel terrain based on limited cross section data has also been presented hu et al 2019 a software toolbox called riverbox has been developed for the reconstruction of riverbeds using the scripting language python 2 7 implemented in arcgis 10 5 1 dysarz 2018 with the unique opportunity provided by advances in spatial interpolation technology this work aims to answer the following research questions 1 is a single swe based hydrodynamic model able to reliably simulate the fluvial flooding process in low resolution and low precision dems areas 2 what are the dominant factors that influence the prediction accuracy of fluvial flood modeling in low resolution and low precision dems areas based on hydrodynamic models to answer these two research questions a new river channel reconstruction method using low resolution and low precision dems is developed in this work the new reconstruction method enables the simulation of the fluvial flooding process at a dems resolution of 30 m from baige to benzilan in the jinsha river china the remainder of this paper is organized as follows in section 2 river channel reconstruction methods are introduced section 3 provides information about the 2018 baige floods and the datasets required for the simulations section 4 presents and discusses the results of river channel reconstruction and flood simulation and finally brief conclusions are drawn in section 5 2 methodology 2 1 dems reconstruction a river channel is a long and narrow zone that is different from the landscape on both sides the topography of the river channel is a water conservancy channel and the water flowing in a watershed converges and connects with water from other blocks thus a channel network has strong connectivity and the flow rate increases gradually with decreasing network elevation based on the characteristics of river landforms we propose the hypothesis that riverbed elevations in downstream cells are not larger than those in upstream cells and develop a trend smoothing method tsm to process coarse river channel terrain data the tsm process is as follows fig 2 boundary definition for the extent of river channels is the first step in the tsm high resolution multiyear remote sensing images such as those provided by google earth are used to extract river boundaries the second step is to select a channel cross section and extract the minimum elevation value in the cross section we established cross sections every 5 km from upstream to downstream in this study then smoothing the minimum elevation value in the cross section sequence is necessary to satisfy the basic hypothesis because of data limitations fig 1 and we do so with a moving average approach fig 3 next we estimate the surface of the riverbed from the points after smoothing many methods of spatial interpolation can be selected such as kriging and spline interpolation in this study the shape of the river cross section exceeding the main aim of this work is not considered and a rectangle is the default shape six interpolation methods are selected to estimate the surface of the riverbed and the depression filling algorithm dfa commonly used in hydrological analysis is used for comparison with the tsm the description of these methods is as follows 1 dfa sinks are often errors due to the resolution of the data or the rounding of elevations to the nearest integer value sinks should be filled to ensure that basins and streams are properly delineated the dfa uses the flow direction and sinks to locate and fill surface depressions in dems the method can be used to create hydrologic elevation models and fill depressions to create a dem without depressions a flow path grid and a grid with watershed basins 2 global polynomial interpolation global polynomial gp interpolation fits a smooth surface defined by a mathematical function a polynomial to the input sample points the trend surface changes gradually and captures coarse scale patterns in the data 3 inverse distance weighted interpolation inverse distance weighted idw interpolation determines cell values using a linearly weighted combination of a set of sample points li et al 2020 the weight is a function of the inverse distance the surface being interpolated should be that of a locationally dependent variable 1 z p i 1 n w i z i where z p is the predicted value for unknown points z i is the value at a given point with a measured elevation and w i is the weight assigned to z i the numbers of sample points and the corresponding weights affect the idw interpolation results in this study 12 neighboring sample points are used and the power of distance is set to 2 4 universal kriging kriging is an advanced geostatistical procedure that generates an estimated surface from a scattered set of points with z values 2 Œ≥ h 1 2 n i 1 n z x i z x i h 2 where x is a point in the data set z x is the corresponding elevation and n is the number of pairs of sample points separated by a distance h all kriging estimators are variants of the following basic equation 3 z ÀÜ x 0 Œº i 1 n Œª i z x i Œº x 0 where Œº is a known stationary mean of the entire domain Œº x 0 is the mean elevation of points in the search window i is the kriging weight linked to h sampson et al 2013 and n is the number of sample points included in the estimation in the universal kriging uk algorithm three simplifications are made based on eq 3 1 Œº is replaced with Œº x 0 2 i n is set equal to 1 and 3 Œº x 0 is the trend component of each point in the neighborhood search window this study selects universal kriging with linear drift and 12 neighboring sample points 5 thin plate spline tps interpolation a raster surface can be interpolated from points using the two dimensional minimum curvature spline technique the resulting smooth surface passes exactly through the input points tps interpolation is achieved by minimizing a criterion function that balances the tradeoff between the representation of the data and the smoothness of the interpolated surface chen and li 2019 specifically the objective function of tps interpolation is expressed as 4 min f z f 2 Œª t f where is the euclidean norm Œª is a smoothing parameter with the optimal value determined by generalized cross validation or the trial and error method and t f represents the penalty term of the smoothness 6 topo to raster the topo to raster topor interpolation method is specifically designed for the creation of hydrologically correct dems topor interpolation uses a finite differential interpolation technique and it is optimized to streamline the local interpolation computational method such as by employing idw interpolation without loss of continuity for the surfaces obtained by the kriging or spline radial basis function interpolation method the difference is defined based on the first and second partial derivatives of f in the interpolation method as described by the following equation arseni et al 2019 5 j h 1 2 h 2 f x 2 f y 2 d x d y f x x 2 f x y 2 f y y 2 d x d y essentially topor is a combined interpolation method that uses a discrete technique based on spline polynomial functions of degree m and smoothness k where the roughness can be modified for the generation of dems with steep field changes such as those in areas with notable currents ridges or cliffs these landforms are also called local maximums or local minimums in the present research point elevation features are used 7 tsm based on the decreasing trend in channel elevation from upstream to downstream we recommend piecewise linear fitting for the riverbed elevation and the establishment of a variable characterizing distance changes in river flow direction combined with the spatial shape of the jinsha river study area the latitude gradient represents the general direction of the river channel in this study 2 2 numerical simulation 1 mathematical formulation in many engineering hydrodynamic problems the full 3d flow effects can be simplified when the horizontal dimensions in physical domains are much larger than the water depth such as in floodplains large river reaches shallow lakes etc the 2d swes can be derived by integrating the three dimensional 3d reynolds averaged navier stokes equations using the hydrostatic assumption in matrix form the conservation law of the 2d swes can be written as follows liang and marche 2009 liang 2010 6 q t f x g y s where t represents time x and y are the cartesian coordinates q is the vector of flow variables f and g are the flux vectors in the x and y directions respectively and s is the source vector composed of slope source terms s b and friction source terms s f the vector terms are usually given by 7 q h q x q y f u h u q x g h 2 2 u q y g v h v q x v q y g h 2 2 8 s s b s f 0 g h z b x g h z b y 0 c f u u 2 v 2 c f v u 2 v 2 where h is the water depth u and v are the depth averaged velocities in the x and y directions respectively q x uh and q y vh denote the unit width discharges in the x and y directions respectively g is gravitational acceleration z b represents the bed elevation and c f is the bed roughness coefficient which is generally computed by gn 2 h 1 3 in this case n is the manning coefficient 2 numerical method the numerical model for solving the swes involves an explicit godunov type cell centered structured finite volume scheme the swes are discretized into algebraic equations by the finite volume method the fluxes of mass and momentum are computed by the hllc harten lax van leer contact approximate riemann solver i e harten lax and van leer approximate riemann solver with the contact wave restored the slope source terms are evaluated by the slope flux method proposed by hou et al 2013 the friction source terms are calculated by the improved implicit method when computing the fluxes and the slope source terms the values at the midpoints of the cell edges are required the two stage explicit runge kutta approach is applied to update the flow variables to a new time level these values are evaluated by a novel 2d edge based muscl monotonic upwind scheme for conservation laws scheme the code is programmed using c and cuda compute unified device architecture which can run on gpus graphics processing units to substantially accelerate the computations 3 case study the 2018 baige landslide dam breach flood event is simulated in this study to demonstrate the performance of the new river channel reconstruction method this section describes the key hydrological and geographical information associated with the event and the key data used to support the simulations 3 1 description of the 2018 baige event in the jinsha river the jinsha river drainage is located in the upper reaches of the yangtze river basin 90 105 e 24 36 n with an area of approximately 470 000 km2 thus accounting for 26 3 of the yangtze river basin fig 4 the river length is approximately 768 km the area is influenced by tropical monsoons subtropical monsoons plateau monsoons and complex terrain the climates in the basin are complex and diverse and they include a cold semiarid plateau climate and a subtropical humid monsoon climate the average annual temperature in the basin is 3 0 c the temperature increases generally from the northwest to the southeast the average annual temperature in the upper area is below 0 0 c and that in the middle and lower reaches is above 19 0 c the average annual precipitation is 753 mm and this total increases from the northwest to the southeast with downstream precipitation being 3 83 times that of the source area the flood season of the jinsha river is from may to october the runoff and sediment in the flood season account for 68 2 80 2 and 85 5 98 2 of the annual totals respectively lu et al 2019 the total elevation change in the jinsha river basin is 3300 m and hydroelectric production reaches more than 100 million kw each year accounting for more than 40 0 of the hydraulic energy resources of the yangtze river basin an important hydropower base in china the jinsha river basin is prone to large scale land sliding and many historical landslide damming events have been recorded along the mainstream and tributaries of the jinsha river wang et al 2014 in late 2018 oct 11 and nov 3 2018 two large scale landslides occurred successively in the same location in baige village boro town tibet china the entire slope rock mass slid rapidly and blocked the river causing upstream inundation and downstream flooding although the first landslide dam breached naturally the second formed a barrier lake at the same place with a volume of approximately 524 million m3 thus seriously threatening the lives of the people living upstream and downstream despite immediate and prolonged disaster mitigation measures natural and manual breaching of the dam caused floods in the lower reaches of the jinsha river inundating 3400 houses and destroying a 3 5 103 km2 area of cropland more than 102 000 people were evacuated from towns at risk in sichuan tibet and yunnan provinces ouyang et al 2019 zhang et al 2019a 2019b the flood inundated many houses in the towns of judian and shigu additionally many roads were blocked bridges were destroyed and farmland areas were flooded fan et al 2020 here we focus on the dam breach flooding from baige to benzilan a reach of 382 km downstream on november 3 2018 although several dams have been planned to be constructed in this reach none have been built or were in operation during the baige event 3 2 data the data used to establish the model include a dem of the entire basin flood inflow locations and the flood flow data collected at several downstream hydrological stations the dem which is produced from advanced spaceborne thermal emission and reflection radiometer global digital elevation model version 2 data at a 30 m spatial resolution is provided by the geospatial data cloud site computer network information center chinese academy of sciences http www gscloud cn the flood inflow locations and flood flow data observed at several downstream hydrological stations fig 5 were published by the bureau of hydrology changjiang water resources commission of china 4 results and discussion several simulations are performed for the flood events involving the jinsha river the manning coefficient for the river channel is chosen to be 0 02 s m 1 3 which is a typical value for the jinsha river as suggested in the existing literature hou et al 2020 the hourly step discharges fig 5 are entered along the northern border in the boundary conditions file while the southern border is set using the free boundary condition and other borders are set as wall boundaries the observed discharge before the dam break is used to pre run the model over a dry bed to produce the initial flow conditions for all subsequent simulations considering the numbers of computations and computational efficiency the yebatan batang reach of the jinsha river length of 134 km is selected for the preliminary study in section 4 1 and the bage benzilan reach length of 382 km is selected for the improved study in section 4 2 the performance of the simulated results is evaluated in terms of the absolute percentage error ape delay time dt nash sutcliffe efficiency nse and coefficient of determination r 2 the ape and dt refer to the relative errors in the peak discharge and absolute errors in the peak time between simulated and observed values respectively the nse and r 2 are comprehensive criteria for evaluating the consistency between simulated and observed values the ape dt nse and r 2 are expressed as follows 9 a p e m p s p m p 100 10 d t t s p t m p 11 n s e 1 i 1 n m i s i 2 i 1 n m i m 2 12 r 2 i 1 n m i m s i s 2 i 1 n m i m 2 i 1 n s i s 2 where m p and s p are the observed and simulated peak discharge and whose times are t mp and t sp respectively n is the length of data series m i and s i are the observed and simulated discharge at the ith hour and whose averages are m and s respectively 4 1 channel construction for the yebatan batang reach of the jinsha river 4 1 1 interpolation method the smoothed minimum elevation value in each cross section is the only input required for the analysis of channel reconstruction with respect to the seven approaches dfa gp idw uk tps topor and tsm in the spatial interpolation of river channels proper boundary identification is extremely important in the second type of analysis elevation differences as final targets for channel reconstruction are compared in the study area the main channels are identified by remote sensing recognition and measured the elevation difference after interpolation fig 6 shows the relationship between the elevation and distance produced with several methods both maps clearly illustrate the different patterns of channel reconstruction obtained with the analysis methods dfa only fills on the basis of the original dem od data to form gently sloping terrain data although this approach is greatly affected by the input terrain data and the flow of water does not decrease with channel elevation table 1 summarizes the main elevation statistics 5 km downstream of yebatan topor overcurves the middle portion of the plot at the channel cross section point and thus cannot represent the actual river terrain the results of idw uk tps and tsm are ideal fig 6b shows the river topography generated with the idw uk tps and tpm methods in 8 km segments of the river in the study area idw and uk yield large sags in the channel which do not represent the actual channel elevation however the channel terrain generated by tps and tsm is relatively smooth the three dimensional dem obtained by tsm at 99 e and 30 n is shown in fig 7 and the channel terrain is continuous compared with the plot in fig 1 the interpolation methods and data quality severely affect the accuracy of the final interpolated surface the reasons are 1 most isotropic methods used for the spatial interpolation are not designed to handle the continuous bending river channel terrain and 2 the interpolation of sparse data e g cross sections lose this trend merwade 2009 therefore in sections 4 1 2 and 4 1 3 tps and tsm are used to generate high precision topographic data for the region 4 1 2 interpolation distance to determine the influence of interpolation section spacing on the flood evolution simulation results tps is used to interpolate sections of the river with 5 km 10 km 15 km and 20 km spacing in the study area which are named s05 s10 s15 and s20 respectively the tsm method is used as follows taking the inlet elevation of the study area as the starting point and the outlet elevation as the end point the channel elevation is linearly interpolated at intervals of 5 km 10 km 15 km 20 km 30 km 40 km and 60 km according to the latitude gradient of the channel corresponding to x05 x10 x15 x20 x30 x40 and x60 respectively only the inlet and outlet cross sections are used to linearly interpolate the latitude gradient and construct a channel topography with a uniform channel gradient named xd to compare the influence of the river terrain representation method on the flood evolution simulation results od was used as a reference for the simulations the number of cells in the calculation domain was 579 3603 and the simulation time was 60 h fig 8 and table 2 show the results of the simulated discharge process for the interpolation of the river channels with different cross section spacings between yebatan and batang there is a certain difference between the simulated and measured outflow process data at batang station and the od simulation accuracy is the worst nse 15 23 r 2 0 20 because the channel terrain data deviate from the actual elevation and there are false depressions resulting present thus resulting in poor simulation accuracy the flood discharge process in river terrain simulations using tps is delayed by 1 h overall and the simulated peak discharge is smaller than the measured value ape 40 the effects of different section spacings on the flood evolution simulations are not very significant notably the overall nse is less than 0 15 and the differences do not exceed 0 08 the generated river terrain using the tsm method is greatly affected by the interpolated cross section spacing specifically the effectiveness of flood evolution simulation improves as the interpolated cross section spacing increases the flood peak discharge also displays an increasing trend with increasing interpolation section spacing with the ape displaying a decreasing trend the xd simulation is the best out of all the calculation examples nse 0 92 r 2 0 96 and the simulated results are similar to the actual measurements for the same interpolation section spacing the channel simulations generated by tsm are better than those generated by tps mainly because the channel terrain generated by tps is stepped and not as smooth as the tsm channel terrain as glenn et al 2016 pointed out dems accuracy is influenced by the interpolation distance and the coordinate system rather than the specific interpolation method and the choice of cross sections location method which consequently affect model results that rely on terrain input 4 1 3 recession depth to improve the accuracy of model calculations which is decreased by the large number of false floodplains in simulated river channels it is necessary to recess the river channel terrain to a certain extent so that the flow path is concentrated in the main channel fig 9 specifically the s05 river channel is recessed 5 m 10 m 15 m 20 m 30 m 50 m denoted as s05d05 s05d10 s05d15 s05d20 s05d30 and s05d50 respectively the same process is used for xd to obtain xdd05 xdd10 xdd15 xdd20 xdd30 and xdd50 the simulated flow process results fig 10 for different recession depths in the yebatan batang reach suggest that the influence of river channel depth on the simulation of flood evolution is not as obvious as that for the interpolation interval the ape and dt of peak discharge on the terrain with recession is similar to that without recession the simulation effect of the flood evolution based on s05 displays a threshold phenomenon with the digging of the river channel that is when the recession depth is less than 20 m the simulation accuracy increases with increasing digging depth and when the recession depth is greater than 20 m the simulation accuracy decreases with increasing recession depth when the recession depth is 20 m the simulation accuracy is the highest in s05 nse 0 11 r 2 0 87 table 3 the accuracy of flood evolution simulation based on xd increases with increasing recession depth when the recession depth of xd is 50 m the nse and r 2 are 0 94 and 0 98 respectively 4 2 channel construction for the baige benzilan reach of the jinsha river according to section 4 1 the tsm method produces the best river terrain reconstruction and flood simulation results the interpolation interval is the second key factor that influences the interpolation results but the effect of adjusting the recession depth is not obvious to verify the main factors that affect the accuracy of river terrain reconstruction and flood simulation the tsm method is used to interpolate the reach of the jinsha river from baige to benzilan the number of cells in the calculation domain was 1902 10443 and the simulation time was 60 h 4 2 1 interpolation distance similar to the approach in section 4 1 the interpolation interval is the main research variable and the tsm method is used for interpolation the interpolation intervals are 10 km 20 km 50 km 100 km and 190 km and the simulations are named ls010 ls020 ls050 ls100 and ls190 respectively additionally the entire interval is still selected for interpolation and named lsd fig 11 displays the simulation results for a dam break flood at different interpolation intervals in the baige benzilan section of the jinsha river compared with the values measured at the yebatan batang and benzilan stations the simulation results display a longer delay time and a smaller peak discharge as the distance downstream increases the peak flood time is delayed by approximately 1 h at yebatan station approximately 2 4 h at batang and 9 10 h at benzilan station table 4 the peak discharge ape is 3 98 18 17 at yebatan station 10 84 38 16 at batang station and 28 07 39 99 at benzilan station for different interpolation intervals the peak time and discharge associated with ls190 and lsd are better than those of the other simulations ls010 and ls020 have good results at yebatan station in terms of the flood simulation accuracy with a nse of 0 74 and 0 73 respectively and the results for ls190 and lsd are also satisfactory the terrain construction results at batang and benzilan are unsatisfactory at different interpolation intervals and the corresponding nse values are less than 0 50 notably the simulation results for ls190 and lsd at batang station are better than those at other stations and this finding is consistent with the conclusion obtained in section 4 1 that the larger the interpolation interval is the better the flood simulation effect the nse of the simulation results at benzilan station is less than 0 and the r 2 value is less than 0 10 4 2 2 channel width the flood simulation results in sections 4 1 2 and 4 2 1 indicate that the impact of the interpolation interval on the flood simulation results in long distance river sections is minimal although adopting an interval based approach can solve the time delay problems in flood simulations for short distance river sections because the width of the water surface increases as the level rises during the flooding process this study widens the reconstructed area of the river channel by expanding the channel area to the buffer area fig 12 which is identified by remote sensing images in this section the river channel terrain reconstruction process is the same as that used for lsd the buffer values are set to 50 m 100 m 150 m 200 m 250 m 300 m 350 m and 400 m and the corresponding result sets are named lsdbu050 lsdbu100 lsdbu150 lsdbu200 lsdbu250 lsdbu300 lsdbu350 and lsdbu400 respectively fig 13 displays the simulation results for a dam break flood at different channel widths in the baige benzilan section of the jinsha river overall the channel width has a notable influence on the timing of the flood peak and peak discharge when the river channel topography is reconstructed over a long distance and the simulations effectively reflect the site measured flood process the partially enlarged view indicates that the simulated flood peak time and peak discharge are advanced and increase as the buffer value increases when the value is less than 150 m additionally the simulated flood peak time and peak discharge are delayed and decrease as the buffer value increases when the value is greater than 150 m the simulated flood peak times at yebatan station for different buffer values are close to the measured values and the peak discharges are similar ape 6 92 at batang station the simulated flood peak times for lsdbu100 lsdbu150 and lsdbu200 are 2 h earlier than the observed values and the flood peak flows are approximately 5000 m3 s higher than the actual measurements ape 22 69 different channel widths yield large differences in the flood peak time 5 h dt 9 h and peak flow 2 76 ape 28 07 at benzilan station the simulated flood peak times for lsdbu100 and lsdbu150 are 5 h prior to the measured times and the peak discharges are approximately 3500 m3 s higher than the observation values table 5 shows the flood simulation accuracy for different buffer values and the simulation results are satisfactory for yebatan station with an nse of 0 85 and r 2 of 0 90 except for lsd at batang station the nse values of the calculation examples except lsd exceed 0 60 and r 2 exceeds 0 70 thus the simulation results are good the different buffer values at benzilan station yield large differences in the accuracy of the simulation results some simulations such as lsdbu50 lsdbu250 lsdbu300 lsdbu350 and lsdbu400 have good accuracy with nse and r 2 values of 0 70 but the results of other simulations are unsatisfactory the nse values of lsdbu50 lsdbu300 and lsdbu350 at each site reach 0 80 with r 2 of 0 85 and the simulation results reflect the actual flood process the range of dt over the different channel widths is from 5 h to 9 h at benzilan which demonstrates the influence of channel width on the results of hydrodynamic models moreover the same interpretation could be found in the research of benjankar et al 2015 except the cross sectional spacing channel width as one of river channel features chiefly determines the different results of hydrodynamic modeling 4 2 3 grid resolution when simulating large scale flood evolution events it is particularly important to understand the effect of grid resolution on the hydrodynamic simulation results therefore the dam break flood evolution at 60 m and 120 m resolutions is simulated and the results are compared with those for a 30 m resolution lsdbu050 for which the best results were obtained in section 4 2 2 is selected as the 30 m resolution control and then resampled to resolutions of 60 m and 120 m named lsdbu050dx60 and lsdbu050dx120 respectively fig 14 compares the flood process simulation results for the baige benzilan section of the jinsha river at different resolutions notably the peak time at each site is delayed and the peak flow decreases as the grid resolution decreases the peak times at yebatan station batang station and benzilan station for lsdbu050dx60 are delayed by 1 h 2 h and 5 h respectively additionally the simulated peak discharge is lower than the value of 4000 m3 s measured at yebatan station and the values at the other two stations are relatively close for lsdbu050dx120 peak time delays of 3 h 7 h and 19 h are observed at yebatan station batang station and benzilan station respectively and the simulated peak flows are lower than the measured values of 9500 m3 s 6700 m3 s and 5700 m3 s respectively the flood simulation accuracy at different grid resolutions is shown in table 6 the flood simulation results for lsdbu050dx60 at the yebatan and batang stations meet the relevant accuracy requirements with nse 0 5 although the flood simulation results for lsdbu050dx120 do not nse 0 5 the results show that a reliable hydrodynamic simulation of the channel flood process can be achieved using a grid that truly reflects the actual terrain features and channel shape 5 conclusions to improve the simulation accuracy of the river channel terrain in two dimensional flood evolution simulations a simple and rapid method for river channel terrain reconstruction is proposed in this paper the new river terrain reconstruction method is applied to predict the extreme flood events in the lower reaches of the jinsha river caused by the collapse of baige barrier lake at a spatial resolution of 30 m the simulation successfully reproduced the event and the results were compared with the existing observations in terms of the flood time and flood process several simulations of the jinsha river were performed for short distance and long distance reaches to effectively demonstrate the potential of the new river reconstruction method in efficient flood evolution simulation and flood forecasting progressive simulations were performed to check the reconstruction parameters of different river terrains a quantitative analysis of the simulation results showed that the flood prediction method was more sensitive to the interpolation interval in river reconstruction than the recession depth for short distance river reaches additionally the method was more sensitive to the channel width than the interpolation interval for long distance river reaches in the simulation of highly dynamic flood events in areas with low precision terrain data considered in this paper reliable predictions can be obtained through the smooth reconstruction of river terrain thus reducing the large amount of work required to measure high precision terrain data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is partly supported by the national key research and development program of china no 2016yfc0402704 national natural science foundation of china nos 51609199 52009104 shaanxi international science technology foundation of china grant no 2017 kw 014 water conservancy science and technology project of shaanxi province grant no 2017slkj 14 basic research on the sponge city of fengxi new town in xixian new area no 2018610002000097 
25841,dam construction in developing nations is on the rise monitoring these dams is essential to understanding downstream hydrologic impacts and for better planning and management of water resources satellite observations and advancements in information technology now present a unique opportunity to overcome the traditional limitations of reservoir monitoring in this study a global reservoir monitoring framework was developed as an online tool for near real time monitoring and impact analysis of existing and planned reservoirs based on publicly available and global satellite observations the framework used a mass balance approach to monitor 1598 reservoirs in south america africa and southeast asia simulated streamflow of the developed tool was validated in 25 river basins against a multidecadal record of in situ discharge the simulated storage change was validated against in situ data from 77 reservoirs the framework was able to capture reservoir state realistically for more than 75 of these reservoirs at most in situ gaging locations the reservoir tool was able to capture streamflow with a correlation of more than 0 9 and a normalized root mean square error of 50 or less the tool can now be used to study existing or planned reservoirs for short and long term decision making and policy analysis keywords reservoir monitoring satellite remote sensing landsat global hydrology 1 introduction by the end of the 20th century approximately 58 000 large dams higher than 15 m with a total reservoir surface area of about 300 000 km2 lehner et al 2011 had been built for hydroelectricity irrigation and water supply needs with rapid growth in human population and energy demand dam construction in the developing world is currently rising zarfl et al 2014 the majority of dams either planned or already under construction are concentrated in south america asia and africa mainly in developing countries with human development index values ranging from low to medium as defined by the united nations development program fig 1 many of these new or upcoming dams are designed to harness the hydropower potential of rivers and to supply water for drinking and irrigation dams and reservoirs have long been treated as cheap and clean energy sources with low carbon emissions and benefits related to flood control food security irrigation and socio economic development hereafter dams and reservoirs will be used interchangeably in this paper for example dams built for irrigation purposes provide water to 30 40 of the world s agricultural lands and produce about 40 of the world s food world bank 2020 dams also have long term downstream consequences due to disruptions they pose to the natural flow regime flow regulation and river fragmentation bunn and arthington 2002 narrowing of river beds and downstream erosion khan et al 2014 seawater intrusion sikder 2013 and thermal stratification lugg and copeland 2014 are among the many ways dams negatively impact river systems grill et al 2015 studied existing and future large dams showed a 48 alteration of rivers and predicted that the number could rise to 93 if all planned dams are built unplanned and uncoordinated dam management also affects downstream flooding mishra and shah 2018 disrupts pre existing agriculture production strobl and strobl 2011 and weakens the ecosystem poff and zimmerman 2010 due to significant impacts of dams on the environment it is necessary for all basin inhabitants particularly those living downstream to understand the operating pattern and state of upstream reservoirs downstream nations as well as all riparian nations can make both immediate and long term decisions for water management and maximize stakeholder benefits through the unhindered exchange of hydrologic data and reservoir operating information understanding the role dams play in flow regimes is also necessary to manage water related hazards such as floods and droughts public safety and infrastructure resilience woldemichael et al 2012 effects of human alterations to the land hydrologic cycle gao et al 2012 and impacts of multi reservoir systems on downstream river discharge d√∂ll and zhang 2009 it is also critical to know how reservoir operating patterns will need to be revised to address climate change especially in climate vulnerable regions such as the amazon pokhrel et al 2014 and the mekong basin lauri et al 2012 despite clear need and urgent calls for the unhindered exchange of hydrologic information khattar and ames 2020 global and freely accessible reservoir monitoring information is currently unavailable for inhabitants of regulated basins around the world this is due to insufficient ground observations particularly in developing regions limited data sharing protocols alsdorf et al 2007 hossain et al 2007 and lack of financial resources solander et al 2016a b such hurdles make it challenging to study and routinely monitor the impact of reservoirs around the world gao et al 2012 due to the absence of direct measurements models have been used to monitor reservoirs at the continental and global scale as well as to assess future climate projections d√∂ll et al 2003 2009 hanasaki et al 2006 meigh et al 1999 however large scale modeling approaches do not accurately resolve individual dams making them less relevant for local decision makers when global reservoir modeling systems were still in the early stages of development it was often assumed that a rectangular or inverse pyramid shaped bathymetry would represent the storage capacity of reservoirs d√∂ll et al 2003 meigh et al 1999 the first grid based explicit representation of a reservoir used in global hydrological models was reported by hanasaki et al 2006 and subsequently improved by several other studies pokhrel et al 2014 voisin et al 2013 solander et al 2016a b proposed a very idealized reservoir model by using temperature as the primary factor to simulate seasonal changes in reservoir management despite the above mentioned modeling studies used to simulate the effect of dams on river discharge several issues remain unsolved relating to spatiotemporal dynamics of the individual reservoir bierkens et al 2015 for example most schemes were developed for macroscale hydrologic models with typical grid sizes of more than 50 km fatichi et al 2016 which is unsuitable for representing small to medium sized reservoirs also none of the studies considered reservoir surface area dynamics and bathymetry which is critical to capturing reservoir evaporation and impacts on the weather degu et al 2011 the calibration and validation of most global models were completed using in situ observations from developed regions thus the skill of such modeling systems remained untested for operational decision making and policy analysis in developing regions gao et al 2012 most recently yigzaw et al 2018 proposed a method to define a characteristic shape of the reservoir for an area elevation relationship that facilitates easier software representation of reservoir bathymetry in earth system models that also works well for reservoir related studies however every reservoir is unique in its bathymetry and defining an idealized shape may not be suitable for deriving skillful storage change or detecting reservoir operating patterns for the individual dam thus to reduce uncertainties related to the actual representation of reservoir topography along with surface area dynamics in global monitoring satellite remote sensing is needed that can measure or estimate a wide range of variables and provide data to determine a reservoir s state over the last few decades satellite remote sensing has played an important role in providing spatio temporal observations of surface water and hydrologic processes at global scale coverage with near real time availability khaki et al 2020 biswas and hossain 2018 avisse et al 2017 kansakar and hossain 2016 gebregiorgis and hossain 2011 the application potential of remote sensing observations for deriving reservoir state and operating patterns has been well established in previous studies bonnema et al 2016 gao et al 2012 the same approach used in bonnema et al 2016 was later applied to over 20 reservoirs of the mekong basin to infer the operating pattern of reservoirs and residence time bonnema and hossain 2017 hossain et al 2019 however such approaches have so far been limited to individual regions or reservoirs with the availability of remotely sensed observations and computationally robust and versatile online software tools such as cloud and distributed computing it is now possible to have a global scale and freely accessible monitoring framework for existing and planned reservoirs wood et al 2011 in order to build an online global scale reservoir framework for information monitoring several priority issues must be addressed these issues include the following 1 automatic delineation of spatial extent around the reservoir shoreline to derive the dynamic surface area commonly known as the region of interest roi 2 construction of the area elevation relationship to define reservoir bathymetry 3 selection of a universally applicable method to estimate the time series of the reservoir s water surface area 4 setup and calibration of the upstream hydrological model to derive reservoir inflow 5 advanced understanding of remote sensing models and data for the users and 6 availability of computational resources with the availability of the grand dam database lehner et al 2011 a georeferenced reservoir database is now available for global scale studies this database however does not provide all necessary information on reservoirs such as the maximum areal extent of reservoir or roi in previous studies gao et al 2012 bonnema et al 2016 the roi had been manually identified using the visible infrared imageries some studies i e zhao and gao 2018 used a fixed buffering distance around a reservoir polygon for defining roi for global scale studies which may not yield accurate results for reservoirs that are highly irregular in shape the extraction of reservoir surface area poses the most difficult challenge in obtaining a globally scalable method there are numerous studies bonnema and hossain 2017 gao 2015 pekel et al 2016 zhao and gao 2018 related to water area extraction of lakes and reservoirs using different passive and active satellite sensors i e landsat 5 7 8 sentinel 1 and 2 the calculation of reservoir inflow on a global scale is another critical issue there is no global parameterization for any of the hydrological models that can be applied universally nijssen et al 2001a b optimized the variable infiltration capacity vic model parameters at the global scale to estimate global river discharge and sensitivity to climate change however the spatial resolution is too coarse for most reservoirs and the approach lacks a streamflow routing model there are a number of other streamflow datasets and global hydrological modeling frameworks lin et al 2019 barbarossa et al 2018 that exist based on global hydrological model setups two major limitations with any dataset are the lack of publicly accessible operational modeling at a daily or weekly timescale and datasets with spatial resolution that is too coarse to capture the dynamics of reservoirs of all sizes thus routine monitoring of reservoirs in near real time that is also publicly accessible cannot be achieved with existing hydrological models and frameworks even if all the above constraints were addressed an advanced understanding of remote sensing and modeling would still be required for stakeholders from developing countries wishing to manage their river basins under increasing transboundary regulation fortunately there have been recent technological advancements including the publicly available cloud which can now eliminate many of the limitations faced by inhabitants of developing nations another advancement is distributed computing that reduces the requirement of high internet bandwidth for downloading and processing of large scale datasets for example google earth engine gee gorelick et al 2017 combines a multi petabyte catalog of satellite imagery and geospatial datasets with the planetary scale analysis that has been applied in several studies of reservoirs biswas et al 2019 bonnema and hossain 2017 zhao and gao 2018 this study describes an operational framework for developing the software needed for a global freely available monitoring system of reservoirs in developing regions this modeling framework known as reservoir assessment tool rat is motivated by the need to democratize access to information on reservoir operations and study reservoir impacts on hydrology so that inhabitants and water managers can devise 21st century solutions those end users who lack advanced knowledge of remote sensing access to in situ or transboundary data or the capacity to operate complex hydrological models will find such a software tool useful in their efforts to monitor and manage their reservoirs the study describing the development of rat is organized as follows first the reservoir mass balance approach used in rat is shown in section 2 1 with datasets described in section 2 2 using these datasets the specific methods used to estimate different parameters of reservoir states are described in section 2 3 the user interface and overview of the proposed operational framework are briefly described in section 3 validation results for the rat framework are presented in section 4 where accuracy of the simulated storage change surface area estimation and reservoir inflow is assessed against reference data the conclusion and scope for further development of the rat framework are described in section 5 2 framework description 2 1 overview and reservoir mass balance approach in this study satellite based remote sensing data were used to estimate reservoir outflow by employing a reservoir mass balance equation 1 for monitoring reservoir dynamics fill release and storage change this mass balance is the core component of the rat framework a schematic diagram of the mass balance concept is shown in fig 2 1 o i e Œ¥ s here the terms represent the following o outflow i inflow e evaporative loss and Œ¥s storage change in this study the term outflow was used as a proxy for release which also included parallel diversions and other consumptive uses the reservoir surface water extent areas at 1 and at in fig 2 were extracted from visible nir imageries corresponding heights ht 1 and ht were extracted using area elevation curve aec and finally Œ¥s was calculated using equation 2 details about the aec development are discussed in section 2 3 2 2 Œ¥ s a t 1 a t 2 h t h t 1 first the roi previously explained in the introduction section of any reservoir is defined by following a reservoir size dependent buffer distance shown in table 1 the roi is used to clip satellite observations for preparing the area elevation relationship and to extract the time series of surface water area storage change of any reservoir can be computed using the reservoir water surface area elevation time series and area elevation relationship shown in fig 2 this is a widely used technique that has been reported to yield acceptable skill bonnema et al 2016 gao 2015 gao et al 2012 meteorological observations and land surface parameters are forced into a hydrological model to derive reservoir inflow the inflow evaporation and storage change can then be used to infer the reservoir outflow using mass balance details about the datasets and specific methods are discussed in sections 2 2 2 3 and 2 4 2 2 datasets the land elevation dataset used in this study was the shuttle radar topography mission srtm 30 m resolution digital elevation model dem hennig et al 2001 three sensors were used to derive water extent area time series including 1 usgs landsat 8 collection 1 tier 1 and real time data raw scenes 2 sentinel 1 sar grid c band synthetic aperture radar ground range detected and 3 sentinel 2 msi multispectral instrument level 1c in the gridded hydrological model fao harmonized world soil database nachtergaele et al 2008 usgs global land cover characteristics glcc and land cover database geological survey 1997 were used for land surface parameters chirps precipitation funk et al 2015 maximum and minimum temperature and average wind speed at 10 m height from noaa ncep climate prediction center provided meteorological forcing data for this study for routing the hydrological model outputs global flow direction at 1 16 spatial resolution wu et al 2011 was used 2 3 storage change calculation the method followed in this study to calculate change in reservoir storage is shown in fig 2 major components mentioned in the mass balance equation are a roi generation b aec extraction c time series processing of water extent area d storage change calculation e simulation of reservoir inflow from the hydrological model and evaporation from the reservoir and f reservoir outflow calculation aside from hydrologic modeling items e and f all components are executed in the cloud using google earth engine gee to minimize internet bandwidth needed for downloading large datasets gee has been extensively used in different large scale hydrological analyses biswas et al 2019 pekel et al 2016 zhao and gao 2018 which offers highly advanced and previously unachievable computational possibilities 2 3 1 roi delineation after many trials over several reservoirs using multiple approaches we classified the reservoirs according to the polygon defined in the grand database the polygon area was used to classify reservoirs into seven distinct classes to identify the appropriate buffering distance to create the roi before deciding the buffering distance for each class the frequency of occurrence map of the global surface water dataset gswd prepared by pekel et al 2016 was used for visual comparison over 70 80 reservoirs by following the maximum water extent of the gswd dataset a suitable buffer distance was decided table 1 2 3 2 area elevation curve extraction using the delineated roi mentioned in section 2 3 1 and srtm dem data the area elevation curve aec was derived in two steps first the roi of the selected reservoir was used to clip the srtm dem the srtm dem elevation was used to generate the area elevation relationship which was valid for elevation above the water surface at the time of the srtm overpass which was in february 2000 the histogram of srtm dem was populated to count the number of cells corresponding to each of the elevation data the area of individual elevation was then calculated and incremented to get incremental area the steepest slope of the aec was used to identify elevations corresponding to reservoir surface areas areas less than the area of water surface elevation were considered to be satellite noise and discarded from further analysis next the relationship developed in the first step was extrapolated to the near zero surface area in order to complete a virtual area elevation relationship for elevations lower than the srtm observed water surface during extrapolation univariate spline was found to be the best estimator and was therefore used as the operational area elevation relationship generator finally these two area elevation relationships were merged to create the complete area elevation curve the whole methodology of aec development is summarized graphically in fig 3 for more information on the area elevation curve generation approach readers are referred to the works of bonnema et al 2016 and bonnema and hossain 2017 2 3 3 surface water area extraction the surface area time series and the aec are prerequisites to calculating the storage change of any reservoir first the roi polygon and aec are prepared by following the approach mentioned in sections 2 2 1 and 2 2 2 all imagery scenes are first filtered using a predefined date window and the roi polygon and then clipped using the roi in the case of sentinel 2 and landsat cloudy pixels were removed from the roi region of the scene the area of scenes was calculated and filtered out for areas less than 80 of the roi after the removal of cloudy and partially covered scene in the case of sentinel 1 scenes were filtered based on polarization look angle and date window and pixels with less than 16 db backscatter value ahmad et al 2019 were treated as water for landsat and sentinel 2 different index based methods were assessed such as normalized difference water index ndwi mcfeeters 1996 modified normalized difference water index mndwi xu 2006 water index fisher et al 2016 advanced water extraction index feyisa et al 2014 and dynamic surface water extent dswe jones 2019 the extracted time series were then used to calculate the storage change time series fig 4 2 3 4 calculating storage change the storage change time series was calculated from the surface time series of the water extent area and the aec fig 4 for any pair of consecutive surface water area data corresponding elevations were computed from the aec storage change was calculated from two consecutive heights and elevations using equation 2 2 4 simulation of reservoir inflow the reservoir inflow was simulated using a hydrological model with streamflow routing capability variable infiltration capacity vic model liang et al 1994 lohmann et al 1998 was chosen for simulating the gridded surface runoff evaporation and baseflow in the upstream catchment area of the reservoir meteorological observations forced the model along with land surface parameters for soil and land surface parameters fao land cover and world harmonized soil dataset were used meteorological parameters used in this study were precipitation maximum and minimum temperature and average wind speed all input forcings to the vic model were prepared at 0 0625 by 0 0625 spatial resolution to match the dominant river tracing drt flow direction at 0 0625 resolution wu et al 2011 we chose the finest resolution of the hydrological model and drt flow direction to cover the maximum number of reservoirs possible from the grand database lehner et al 2011 there are several calibration parameters for the vic model which can be used to improve simulated streamflow some of these parameters i e saturated hydraulic conductivity and the exponent of the unsaturated hydraulic conductivity curve were estimated from soil properties by following the approach mentioned in nijssen et al 2001a b initially the two calibration parameters variable infiltration parameter and depth of the soil layer were taken from nijssen et al 2001a b we found that those critical parameters identified in nijssen et al 2001a b were thoroughly investigated based on climatic zone and geographical region and presented the best available baseline study these calibration parameters were resampled to a spatial resolution of 0 0625 by 0 0625 by using the cubic spline interpolation technique the calibrated parameters were further updated wherever it was available to ensure better estimation of reservoir inflow by following more recent studies in several basins for example we used parameters for ganges brahmaputra meghna basins from siddique e akbor et al 2014 for indus basin from iqbal et al 2017 for mekong basin from hossain et al 2017 and for nile basin from eldardiry and hossain 2019 upon completion of hydrological model simulation outputs were forced into the routing model lohmann et al 1998 along with the drt flow direction wu et al 2011 to simulate reservoir inflow drt flow direction derived flow accumulation was matched with satellite imagery and river networks manually in most of the reservoir locations it was done by comparing the flow accumulation from the drt flow direction to satellite imagery at different locations with the assistance of google earth https www google com earth 2 4 1 calculation of reservoir evaporation and outflow the total net evaporation computed by the vic hydrological model was used to compute the evaporation from the reservoirs users are referred to https vic readthedocs io for a detailed description of total net evaporation calculation of vic model here the vic model grid closest to the dam location was identified and the simulated total net evaporation at that grid cell was assumed to represent reservoir surface evaporation over a unit area this amount of evaporation from the grid cell was multiplied by the reservoir surface area to calculate the evaporation from the reservoir equation 1 was used to calculate outflow from the reservoir the inflow volume storage change volume and evaporation amount were used to calculate outflow volume between two consecutive storage changes we have assumed the role of seepage and groundwater loss as minor based on a previous study bonnema et al 2016 and thus discarded them from the mass balance approach 2 4 2 consideration of upstream reservoirs where there is a series of reservoirs along a river and its tributaries and the inflow volume of the upstream reservoir is greater than 10 of the natural inflow to the downstream reservoir the influence of the upstream reservoir on downstream inflow was considered this was done by deriving the difference between the inflow and outflow of the upstream reservoir and adjusting for that for downstream reservoir inflow 3 the interface of reservoir assessment tool rat and operational reservoir monitoring 3 1 graphical user interface gui the main window of the frontend is shown in fig 5 which can be accessed through http depts washington edu saswe rat beta the detailed design of the frontend and salient features of the tool are discussed in the user manual of the tool and also available in the github link https github com nbiswasuw rat reservoir assessment tool currently 1598 dams from the grand database version 1 3 located in south america africa and southeast asia are modeled operationally and visualized on the rat frontend interface all reservoir parameters i e aec surface water extent inflow storage change and outflow were added to the frontend 3 2 monitoring of reservoirs 3 2 1 monitoring of existing reservoirs as more recent and frequent satellite observations on reservoir areas become available via gee the rat framework automatically processes the data runs the hydrological model and post processed model outputs to create updated estimates of outflow inflow storage change of the existing reservoirs the water extent time series is extracted with the latest available scenes per the methodology used for water extent area extraction mentioned in section 2 3 3 the available aec data and water extent time series are processed to get the storage change time series vic model is simulated weekly at a daily time step to get the most recent inflow into the reservoirs finally the outflow is calculated from the inflow and storage change all of these time series data are made available in the frontend for user access the data and information interchange between the backend server and the frontend gui of the tool is explained in fig 6 if the inflow into any reservoir is not calculated the user can make a request through the frontend which is explained next in section 3 3 3 3 user request for adding new reservoirs to rat when data is not available over a reservoir location shown in the rat framework jquery allows a user to push a request button shown in figure 16 of the rat framework user manual the user needs to specify the grand id of the reservoir when sending the request and other information as mentioned in the user manual see figure 16 of the user manual the form can also be accessed through this link https forms gle muebn4bheie1b91j7 the request will push notifications to the administrator of the rat framework to take further action after being notified the administrator can review the request to add the missing dam to the available list for calculation of reservoir state during regular monitoring of reservoirs the newly added reservoir will be considered for deriving all the parameters including aec extraction and the user notified of the availability of data 4 results and discussion the developed rat software framework was applied in estimating reservoir storage change inflow and outflow reservoir inflow and storage change were compared against in situ measured data the reservoir outflow was derived from the inflow and storage change using the mass balance approach discussed in section 2 1 thus it is assumed that accuracy of reservoir outflow is dependent on inflow and storage change accuracy 4 1 accuracy of reservoir storage change in situ measurements of daily reservoir storage were web scraped from the central water commission cwc http cwc gov in of india this web scraping is very similar in nature to a hydrologic platform development work described by biswas and hossain 2018 a map showing the reservoirs locations along with their surface area in different colors and area perimeter ratio termed as irregularity index shown in different colors is shown in fig 7 to quantify the proposed framework s performance for different sizes of reservoirs 77 reservoirs were classified according to their surface areas from grand database in five different classes very small small medium large and very large as seen in table 2 to compare the simulated storage change based on reservoir shape the reservoirs were classified according to the ratio of surface area to the perimeter from grand database which is termed here as irregularity index the categories based on area perimeter ratio are highly irregular very irregular irregular regular very regular and highly regular details about the classification based on the area perimeter ratio are shown in table 3 three reservoirs each with a distinctive irregularity index are shown in fig 8 to illustrate the differences in reservoir shapes based on the irregularity index the simulated storage change was compared against in situ storage change of the individual reservoir on a monthly basis different sensors landsat 8 sentinel 1 and 2 and different index based methods i e ndwi mndwi wi awei for landsat 8 and sentinel 2 backscatter coefficient for sentinel 1 were tested to compare their accuracy the correlation coefficient and the normalized root mean square error nrmse were used to quantify the accuracy of individual methods and sensors for different reservoir classes the sensors and methods are described in table 4 the mean correlation coefficient and the normalized rmse comparison of different reservoir sizes are shown in fig 9 all of the sensors and methods yield a correlation coefficient of more than 0 7 for all types of reservoirs the correlation coefficient is highest for reservoirs with more than 200 km2 of surface area very large reservoirs for landsat marked as l8 in fig 9 all methods yielded a correlation coefficient of more than 0 8 which means more than 80 of the in situ storage can be represented by the rat framework except for very large reservoirs the normalized rmse comparison revealed that sentinel 2 generated storage changes are less accurate than those of landsat or sentinel 1 possibly because all of the indices were extensively tested for waterbody detection using landsat data sentinel 1 provided better representation than sentinel 2 however the accuracy was less than that of landsat although the data from sentinel 1 was more accurate than that of all other sensors except landsat a few unrealistic estimations and a smaller number of samples resulted in a low score vegetated inundation poor quality atmospheric composition during imagery acquisition and incorrect identification of sand pixels as water are likely some of the underlying issues resulting in the low performance of sentinel 1 martinis et al 2015 in the case of very small small and medium reservoirs landsat performed better for all indices than the other two sensors among the different methods of landsat 8 the dswe method generated time series with continuous underestimation of water area and fewer records compared to the others this was due to multiple filtering conditions additionally gee processing time was almost five times higher for dswe than the calculations of other indices making gee a less practical method of global reservoir monitoring mndwi method was found to have limited skill for reservoirs located in steep terrains compared to all other methods ndwi produced consistently better results with a simpler processing approach the storage change time series also was classified according to reservoir the irregularity index which was used for very irregularly shaped reservoirs with extensive shorelines the satellite imageries have limitations in detecting water pixels at the edges thus it was helpful to quantify the relative performance of the sensors and methods the mean correlation coefficient and mean of the normalized rmse for each of the classes were compared and shown in fig 10 the accuracy of every method and sensor decreased with the irregularity of the reservoirs landsat based methods worked best for reservoirs in the irregular category for regular shaped reservoirs almost all methods yielded similar results highly irregular shaped reservoirs returned the lowest correlation coefficient mostly due to water detection along the reservoirs shorelines considering all the advantages and disadvantages of each of the sensors and methods the landsat 8 based ndwi method s performance was found to be most robust and consistent and was therefore selected for the operational rat software framework 4 2 validation of reservoir surface area estimation the rat framework s simulated reservoir surface area was compared with the latest published reservoir surface area dataset prepared by zhao and gao 2018 zhao and gao 2018 dataset provides the surface water extent area of the grand database from 1984 to 2015 on a monthly scale we compared the total reservoir surface area from the ndwi method of landsat 8 sensor and the dswe method for landsat 5 of all the rat domain reservoirs to the total surface area of the same reservoirs estimated by zhao and gao 2018 the rat framework was extensively validated for the landsat 8 satellite imagery and it was found that the ndwi method worked best in the case of the landsat 8 due to differences in sensor characteristics and differences in spectral band ranges we found that the dswe method worked best for the landsat 5 for operational purposes water areas generated from landsat 5 using the dswe method and landsat 8 using the ndwi method were combined to produce monthly timeseries and then compared with zhao and gao 2018 data fig 11 it was also found that reservoir surface area records were discontinuous for many reservoirs in the zhao and gao 2018 data during the years before 2000 consequently comparison began with the year 2000 the proposed framework yielded a correlation coefficient of 0 92 fig 11 shows that the reservoir surface area s seasonal variation is more clearly visible when the proposed framework generated dataset was used we should note that zhao and gao 2018 dataset is not available in near real time scale for monitoring reservoir dynamics from the latest available satellite imagery 4 3 validation of vic hydrological model using the land surface parameters and meteorological forcing mentioned in section 2 3 4 the vic and route model was simulated for the rat domain south america africa and southeast asia the simulated daily streamflow was first compared against ground based daily discharge data collected through different sources information about validation stations is mentioned in table 5 the stations along with the respective basins are shown in the upper panel of fig 12 time series comparisons of two stations are shown in the middle panel of fig 12 the left panel is tabatinga station located in the amazon basin where the correlation coefficient was less than 0 7 and the vic model was not very accurate in representing low flow and high flow peaks a similar case was observed at other stations timeseries comparison of kampong cham station located on the mekong river shown in the middle right corner found accuracy was highest for the correlation coefficient this basin performed exceptionally well due to subbasin scale calibration performed by hossain et al 2017 nevertheless the hydrological model showed dry season flow to be lower than the actual flow and overestimated the peaks summary statistics of all validation stations are shown in the lower panel of fig 12 in southeast asia s stations the correlation coefficients were more than 0 8 whereas the south american stations showed more than 0 6 also in some stations nrmse was higher with a good correlation coefficient due to the model s underperformance in capturing seasonality we found that flow direction modification improved the results significantly 4 4 comparison of streamflow with grades streamflow we compared streamflow at different inflow locations with the global reach level a priori discharge estimates for surface water and ocean topography modeled streamflow grades lin et al 2019 we compared our model s estimated streamflow with the grades model s simulated streamflow at 44 randomly chosen locations along the river reaches within the rat domain the summary statistics are shown in fig 13 the average correlation coefficient of all the stations was 0 62 and the mean of normalized root mean square error was 0 49 again stations located in southeast asia performed better compared to the stations in the south america region this is a clear indication that better calibration at regional and basin scales can improve simulated streamflow accuracy at the locations where our model underperformed 5 conclusion and future scope to our knowledge the online software framework for reservoir monitoring called rat is the first of its kind given that the rat tool is now publicly available for the world to use and benefit from we believe the following are some examples of potential applications of this framework by using this tool long term records denoting real time behavior and operating rules at reservoirs can become publicly available rat can help users and the scientific community derive a global picture of reservoir monitoring how they are being operated and how they are likely impacting natural river flow and its variability as a function of climate hydrologic regime and socio economic indicators with further improvements in hydrological modeling using locally available ground observations the rat framework can be used with higher accuracy in local regional and global scale operational water resources management considering its near real time data availability the rat framework can facilitate feasibility study of proposed planned dams it can be used to estimate the future reservoir capacity and inflow availability at any location which is useful in optimizing reservoir benefits the rat framework presents future possibilities to study the impact of harnessing hydropower on river temperature greenhouse gas emissions aquatic habitats land use and landcover change and agriculture practices the rat tool can be used to minimize conflict between riparian countries i e egypt and ethiopia over nile basin china india and bangladesh over ganges brahmaputra meghna basin china laos thailand cambodia vietnam over mekong basin as it can be considered an unbiased tool to all parties and provide data needed to drive fair and transparent water sharing agreements there are some future improvements that can be considered to make the framework more applicable in solving real world water challenges while the hydrological model validation showed promise known uncertainties mandate that current results be carefully interpreted additionally the vic model used here will still need time to be improved and validated further at a more granular level this need for improvement at many regions is evident when a superior goodness of fit such as nash sutcliffe efficiency fig 14 is compared with in situ flow accumulated over 16 days to match landsat revisit time and for reservoir management improved reservoir inflow estimation can be made by using regional and basin scale calibration and validation of the hydrological model is one of them the more complex method of water pixel identification from satellite imagery with artificial intelligence and the application of machine learning algorithms may yield better estimation of reservoir surface area reservoir bathymetry data from ground surveying or lidar applications may be used to define the area elevation curve more accurately lastly reservoir outflow calculated in this study includes all types of diversions and consumptive water uses for various purposes which may not be very accurate nor comparable to the actual reservoir releases thus the simulated outflow should be compared with measured flow at in situ locations immediately downstream of a reservoir the focus of our study here was on reservoirs and on methods of enabling the monitoring prediction of reservoir states surface area storage change inflow and outflow at weekly to monthly scales of reservoir management it should be noted that our study is not about developing a global hydrological modeling framework or even promoting an existing one for that matter our rat framework is agnostic enough that the current hydrological model vic can be replaced with other competing hydrological models our work to develop such an open and publicly available tool is driven by our mission to democratize water information on regulated river basins for all stakeholders and to facilitate more equitable water management we believe that such a tool can level the playing field for stakeholder agencies and riparian nations that suffer from limited access to information on water availability due to hydro politics lack of in situ infrastructure or low adaptive capacity software availability https github com nbiswasuw rat reservoir assessment tool key findings 1 a web based framework was developed for near realtime monitoring and impact analysis of reservoirs around the world 2 the framework is freely available and able to monitor the dynamic state for more than 1500 reservoirs 3 the storage changes of more than seventy five percent of reservoirs were accurately captured with skillful inflow simulation at bi weekly timescale declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by a nasa applied science program grant in water resources nnx15ac63g and nnx16aq54g additional support from nasa applied sciences servir grant 80 nssc20k0152 is also acknowledged appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105043 
25841,dam construction in developing nations is on the rise monitoring these dams is essential to understanding downstream hydrologic impacts and for better planning and management of water resources satellite observations and advancements in information technology now present a unique opportunity to overcome the traditional limitations of reservoir monitoring in this study a global reservoir monitoring framework was developed as an online tool for near real time monitoring and impact analysis of existing and planned reservoirs based on publicly available and global satellite observations the framework used a mass balance approach to monitor 1598 reservoirs in south america africa and southeast asia simulated streamflow of the developed tool was validated in 25 river basins against a multidecadal record of in situ discharge the simulated storage change was validated against in situ data from 77 reservoirs the framework was able to capture reservoir state realistically for more than 75 of these reservoirs at most in situ gaging locations the reservoir tool was able to capture streamflow with a correlation of more than 0 9 and a normalized root mean square error of 50 or less the tool can now be used to study existing or planned reservoirs for short and long term decision making and policy analysis keywords reservoir monitoring satellite remote sensing landsat global hydrology 1 introduction by the end of the 20th century approximately 58 000 large dams higher than 15 m with a total reservoir surface area of about 300 000 km2 lehner et al 2011 had been built for hydroelectricity irrigation and water supply needs with rapid growth in human population and energy demand dam construction in the developing world is currently rising zarfl et al 2014 the majority of dams either planned or already under construction are concentrated in south america asia and africa mainly in developing countries with human development index values ranging from low to medium as defined by the united nations development program fig 1 many of these new or upcoming dams are designed to harness the hydropower potential of rivers and to supply water for drinking and irrigation dams and reservoirs have long been treated as cheap and clean energy sources with low carbon emissions and benefits related to flood control food security irrigation and socio economic development hereafter dams and reservoirs will be used interchangeably in this paper for example dams built for irrigation purposes provide water to 30 40 of the world s agricultural lands and produce about 40 of the world s food world bank 2020 dams also have long term downstream consequences due to disruptions they pose to the natural flow regime flow regulation and river fragmentation bunn and arthington 2002 narrowing of river beds and downstream erosion khan et al 2014 seawater intrusion sikder 2013 and thermal stratification lugg and copeland 2014 are among the many ways dams negatively impact river systems grill et al 2015 studied existing and future large dams showed a 48 alteration of rivers and predicted that the number could rise to 93 if all planned dams are built unplanned and uncoordinated dam management also affects downstream flooding mishra and shah 2018 disrupts pre existing agriculture production strobl and strobl 2011 and weakens the ecosystem poff and zimmerman 2010 due to significant impacts of dams on the environment it is necessary for all basin inhabitants particularly those living downstream to understand the operating pattern and state of upstream reservoirs downstream nations as well as all riparian nations can make both immediate and long term decisions for water management and maximize stakeholder benefits through the unhindered exchange of hydrologic data and reservoir operating information understanding the role dams play in flow regimes is also necessary to manage water related hazards such as floods and droughts public safety and infrastructure resilience woldemichael et al 2012 effects of human alterations to the land hydrologic cycle gao et al 2012 and impacts of multi reservoir systems on downstream river discharge d√∂ll and zhang 2009 it is also critical to know how reservoir operating patterns will need to be revised to address climate change especially in climate vulnerable regions such as the amazon pokhrel et al 2014 and the mekong basin lauri et al 2012 despite clear need and urgent calls for the unhindered exchange of hydrologic information khattar and ames 2020 global and freely accessible reservoir monitoring information is currently unavailable for inhabitants of regulated basins around the world this is due to insufficient ground observations particularly in developing regions limited data sharing protocols alsdorf et al 2007 hossain et al 2007 and lack of financial resources solander et al 2016a b such hurdles make it challenging to study and routinely monitor the impact of reservoirs around the world gao et al 2012 due to the absence of direct measurements models have been used to monitor reservoirs at the continental and global scale as well as to assess future climate projections d√∂ll et al 2003 2009 hanasaki et al 2006 meigh et al 1999 however large scale modeling approaches do not accurately resolve individual dams making them less relevant for local decision makers when global reservoir modeling systems were still in the early stages of development it was often assumed that a rectangular or inverse pyramid shaped bathymetry would represent the storage capacity of reservoirs d√∂ll et al 2003 meigh et al 1999 the first grid based explicit representation of a reservoir used in global hydrological models was reported by hanasaki et al 2006 and subsequently improved by several other studies pokhrel et al 2014 voisin et al 2013 solander et al 2016a b proposed a very idealized reservoir model by using temperature as the primary factor to simulate seasonal changes in reservoir management despite the above mentioned modeling studies used to simulate the effect of dams on river discharge several issues remain unsolved relating to spatiotemporal dynamics of the individual reservoir bierkens et al 2015 for example most schemes were developed for macroscale hydrologic models with typical grid sizes of more than 50 km fatichi et al 2016 which is unsuitable for representing small to medium sized reservoirs also none of the studies considered reservoir surface area dynamics and bathymetry which is critical to capturing reservoir evaporation and impacts on the weather degu et al 2011 the calibration and validation of most global models were completed using in situ observations from developed regions thus the skill of such modeling systems remained untested for operational decision making and policy analysis in developing regions gao et al 2012 most recently yigzaw et al 2018 proposed a method to define a characteristic shape of the reservoir for an area elevation relationship that facilitates easier software representation of reservoir bathymetry in earth system models that also works well for reservoir related studies however every reservoir is unique in its bathymetry and defining an idealized shape may not be suitable for deriving skillful storage change or detecting reservoir operating patterns for the individual dam thus to reduce uncertainties related to the actual representation of reservoir topography along with surface area dynamics in global monitoring satellite remote sensing is needed that can measure or estimate a wide range of variables and provide data to determine a reservoir s state over the last few decades satellite remote sensing has played an important role in providing spatio temporal observations of surface water and hydrologic processes at global scale coverage with near real time availability khaki et al 2020 biswas and hossain 2018 avisse et al 2017 kansakar and hossain 2016 gebregiorgis and hossain 2011 the application potential of remote sensing observations for deriving reservoir state and operating patterns has been well established in previous studies bonnema et al 2016 gao et al 2012 the same approach used in bonnema et al 2016 was later applied to over 20 reservoirs of the mekong basin to infer the operating pattern of reservoirs and residence time bonnema and hossain 2017 hossain et al 2019 however such approaches have so far been limited to individual regions or reservoirs with the availability of remotely sensed observations and computationally robust and versatile online software tools such as cloud and distributed computing it is now possible to have a global scale and freely accessible monitoring framework for existing and planned reservoirs wood et al 2011 in order to build an online global scale reservoir framework for information monitoring several priority issues must be addressed these issues include the following 1 automatic delineation of spatial extent around the reservoir shoreline to derive the dynamic surface area commonly known as the region of interest roi 2 construction of the area elevation relationship to define reservoir bathymetry 3 selection of a universally applicable method to estimate the time series of the reservoir s water surface area 4 setup and calibration of the upstream hydrological model to derive reservoir inflow 5 advanced understanding of remote sensing models and data for the users and 6 availability of computational resources with the availability of the grand dam database lehner et al 2011 a georeferenced reservoir database is now available for global scale studies this database however does not provide all necessary information on reservoirs such as the maximum areal extent of reservoir or roi in previous studies gao et al 2012 bonnema et al 2016 the roi had been manually identified using the visible infrared imageries some studies i e zhao and gao 2018 used a fixed buffering distance around a reservoir polygon for defining roi for global scale studies which may not yield accurate results for reservoirs that are highly irregular in shape the extraction of reservoir surface area poses the most difficult challenge in obtaining a globally scalable method there are numerous studies bonnema and hossain 2017 gao 2015 pekel et al 2016 zhao and gao 2018 related to water area extraction of lakes and reservoirs using different passive and active satellite sensors i e landsat 5 7 8 sentinel 1 and 2 the calculation of reservoir inflow on a global scale is another critical issue there is no global parameterization for any of the hydrological models that can be applied universally nijssen et al 2001a b optimized the variable infiltration capacity vic model parameters at the global scale to estimate global river discharge and sensitivity to climate change however the spatial resolution is too coarse for most reservoirs and the approach lacks a streamflow routing model there are a number of other streamflow datasets and global hydrological modeling frameworks lin et al 2019 barbarossa et al 2018 that exist based on global hydrological model setups two major limitations with any dataset are the lack of publicly accessible operational modeling at a daily or weekly timescale and datasets with spatial resolution that is too coarse to capture the dynamics of reservoirs of all sizes thus routine monitoring of reservoirs in near real time that is also publicly accessible cannot be achieved with existing hydrological models and frameworks even if all the above constraints were addressed an advanced understanding of remote sensing and modeling would still be required for stakeholders from developing countries wishing to manage their river basins under increasing transboundary regulation fortunately there have been recent technological advancements including the publicly available cloud which can now eliminate many of the limitations faced by inhabitants of developing nations another advancement is distributed computing that reduces the requirement of high internet bandwidth for downloading and processing of large scale datasets for example google earth engine gee gorelick et al 2017 combines a multi petabyte catalog of satellite imagery and geospatial datasets with the planetary scale analysis that has been applied in several studies of reservoirs biswas et al 2019 bonnema and hossain 2017 zhao and gao 2018 this study describes an operational framework for developing the software needed for a global freely available monitoring system of reservoirs in developing regions this modeling framework known as reservoir assessment tool rat is motivated by the need to democratize access to information on reservoir operations and study reservoir impacts on hydrology so that inhabitants and water managers can devise 21st century solutions those end users who lack advanced knowledge of remote sensing access to in situ or transboundary data or the capacity to operate complex hydrological models will find such a software tool useful in their efforts to monitor and manage their reservoirs the study describing the development of rat is organized as follows first the reservoir mass balance approach used in rat is shown in section 2 1 with datasets described in section 2 2 using these datasets the specific methods used to estimate different parameters of reservoir states are described in section 2 3 the user interface and overview of the proposed operational framework are briefly described in section 3 validation results for the rat framework are presented in section 4 where accuracy of the simulated storage change surface area estimation and reservoir inflow is assessed against reference data the conclusion and scope for further development of the rat framework are described in section 5 2 framework description 2 1 overview and reservoir mass balance approach in this study satellite based remote sensing data were used to estimate reservoir outflow by employing a reservoir mass balance equation 1 for monitoring reservoir dynamics fill release and storage change this mass balance is the core component of the rat framework a schematic diagram of the mass balance concept is shown in fig 2 1 o i e Œ¥ s here the terms represent the following o outflow i inflow e evaporative loss and Œ¥s storage change in this study the term outflow was used as a proxy for release which also included parallel diversions and other consumptive uses the reservoir surface water extent areas at 1 and at in fig 2 were extracted from visible nir imageries corresponding heights ht 1 and ht were extracted using area elevation curve aec and finally Œ¥s was calculated using equation 2 details about the aec development are discussed in section 2 3 2 2 Œ¥ s a t 1 a t 2 h t h t 1 first the roi previously explained in the introduction section of any reservoir is defined by following a reservoir size dependent buffer distance shown in table 1 the roi is used to clip satellite observations for preparing the area elevation relationship and to extract the time series of surface water area storage change of any reservoir can be computed using the reservoir water surface area elevation time series and area elevation relationship shown in fig 2 this is a widely used technique that has been reported to yield acceptable skill bonnema et al 2016 gao 2015 gao et al 2012 meteorological observations and land surface parameters are forced into a hydrological model to derive reservoir inflow the inflow evaporation and storage change can then be used to infer the reservoir outflow using mass balance details about the datasets and specific methods are discussed in sections 2 2 2 3 and 2 4 2 2 datasets the land elevation dataset used in this study was the shuttle radar topography mission srtm 30 m resolution digital elevation model dem hennig et al 2001 three sensors were used to derive water extent area time series including 1 usgs landsat 8 collection 1 tier 1 and real time data raw scenes 2 sentinel 1 sar grid c band synthetic aperture radar ground range detected and 3 sentinel 2 msi multispectral instrument level 1c in the gridded hydrological model fao harmonized world soil database nachtergaele et al 2008 usgs global land cover characteristics glcc and land cover database geological survey 1997 were used for land surface parameters chirps precipitation funk et al 2015 maximum and minimum temperature and average wind speed at 10 m height from noaa ncep climate prediction center provided meteorological forcing data for this study for routing the hydrological model outputs global flow direction at 1 16 spatial resolution wu et al 2011 was used 2 3 storage change calculation the method followed in this study to calculate change in reservoir storage is shown in fig 2 major components mentioned in the mass balance equation are a roi generation b aec extraction c time series processing of water extent area d storage change calculation e simulation of reservoir inflow from the hydrological model and evaporation from the reservoir and f reservoir outflow calculation aside from hydrologic modeling items e and f all components are executed in the cloud using google earth engine gee to minimize internet bandwidth needed for downloading large datasets gee has been extensively used in different large scale hydrological analyses biswas et al 2019 pekel et al 2016 zhao and gao 2018 which offers highly advanced and previously unachievable computational possibilities 2 3 1 roi delineation after many trials over several reservoirs using multiple approaches we classified the reservoirs according to the polygon defined in the grand database the polygon area was used to classify reservoirs into seven distinct classes to identify the appropriate buffering distance to create the roi before deciding the buffering distance for each class the frequency of occurrence map of the global surface water dataset gswd prepared by pekel et al 2016 was used for visual comparison over 70 80 reservoirs by following the maximum water extent of the gswd dataset a suitable buffer distance was decided table 1 2 3 2 area elevation curve extraction using the delineated roi mentioned in section 2 3 1 and srtm dem data the area elevation curve aec was derived in two steps first the roi of the selected reservoir was used to clip the srtm dem the srtm dem elevation was used to generate the area elevation relationship which was valid for elevation above the water surface at the time of the srtm overpass which was in february 2000 the histogram of srtm dem was populated to count the number of cells corresponding to each of the elevation data the area of individual elevation was then calculated and incremented to get incremental area the steepest slope of the aec was used to identify elevations corresponding to reservoir surface areas areas less than the area of water surface elevation were considered to be satellite noise and discarded from further analysis next the relationship developed in the first step was extrapolated to the near zero surface area in order to complete a virtual area elevation relationship for elevations lower than the srtm observed water surface during extrapolation univariate spline was found to be the best estimator and was therefore used as the operational area elevation relationship generator finally these two area elevation relationships were merged to create the complete area elevation curve the whole methodology of aec development is summarized graphically in fig 3 for more information on the area elevation curve generation approach readers are referred to the works of bonnema et al 2016 and bonnema and hossain 2017 2 3 3 surface water area extraction the surface area time series and the aec are prerequisites to calculating the storage change of any reservoir first the roi polygon and aec are prepared by following the approach mentioned in sections 2 2 1 and 2 2 2 all imagery scenes are first filtered using a predefined date window and the roi polygon and then clipped using the roi in the case of sentinel 2 and landsat cloudy pixels were removed from the roi region of the scene the area of scenes was calculated and filtered out for areas less than 80 of the roi after the removal of cloudy and partially covered scene in the case of sentinel 1 scenes were filtered based on polarization look angle and date window and pixels with less than 16 db backscatter value ahmad et al 2019 were treated as water for landsat and sentinel 2 different index based methods were assessed such as normalized difference water index ndwi mcfeeters 1996 modified normalized difference water index mndwi xu 2006 water index fisher et al 2016 advanced water extraction index feyisa et al 2014 and dynamic surface water extent dswe jones 2019 the extracted time series were then used to calculate the storage change time series fig 4 2 3 4 calculating storage change the storage change time series was calculated from the surface time series of the water extent area and the aec fig 4 for any pair of consecutive surface water area data corresponding elevations were computed from the aec storage change was calculated from two consecutive heights and elevations using equation 2 2 4 simulation of reservoir inflow the reservoir inflow was simulated using a hydrological model with streamflow routing capability variable infiltration capacity vic model liang et al 1994 lohmann et al 1998 was chosen for simulating the gridded surface runoff evaporation and baseflow in the upstream catchment area of the reservoir meteorological observations forced the model along with land surface parameters for soil and land surface parameters fao land cover and world harmonized soil dataset were used meteorological parameters used in this study were precipitation maximum and minimum temperature and average wind speed all input forcings to the vic model were prepared at 0 0625 by 0 0625 spatial resolution to match the dominant river tracing drt flow direction at 0 0625 resolution wu et al 2011 we chose the finest resolution of the hydrological model and drt flow direction to cover the maximum number of reservoirs possible from the grand database lehner et al 2011 there are several calibration parameters for the vic model which can be used to improve simulated streamflow some of these parameters i e saturated hydraulic conductivity and the exponent of the unsaturated hydraulic conductivity curve were estimated from soil properties by following the approach mentioned in nijssen et al 2001a b initially the two calibration parameters variable infiltration parameter and depth of the soil layer were taken from nijssen et al 2001a b we found that those critical parameters identified in nijssen et al 2001a b were thoroughly investigated based on climatic zone and geographical region and presented the best available baseline study these calibration parameters were resampled to a spatial resolution of 0 0625 by 0 0625 by using the cubic spline interpolation technique the calibrated parameters were further updated wherever it was available to ensure better estimation of reservoir inflow by following more recent studies in several basins for example we used parameters for ganges brahmaputra meghna basins from siddique e akbor et al 2014 for indus basin from iqbal et al 2017 for mekong basin from hossain et al 2017 and for nile basin from eldardiry and hossain 2019 upon completion of hydrological model simulation outputs were forced into the routing model lohmann et al 1998 along with the drt flow direction wu et al 2011 to simulate reservoir inflow drt flow direction derived flow accumulation was matched with satellite imagery and river networks manually in most of the reservoir locations it was done by comparing the flow accumulation from the drt flow direction to satellite imagery at different locations with the assistance of google earth https www google com earth 2 4 1 calculation of reservoir evaporation and outflow the total net evaporation computed by the vic hydrological model was used to compute the evaporation from the reservoirs users are referred to https vic readthedocs io for a detailed description of total net evaporation calculation of vic model here the vic model grid closest to the dam location was identified and the simulated total net evaporation at that grid cell was assumed to represent reservoir surface evaporation over a unit area this amount of evaporation from the grid cell was multiplied by the reservoir surface area to calculate the evaporation from the reservoir equation 1 was used to calculate outflow from the reservoir the inflow volume storage change volume and evaporation amount were used to calculate outflow volume between two consecutive storage changes we have assumed the role of seepage and groundwater loss as minor based on a previous study bonnema et al 2016 and thus discarded them from the mass balance approach 2 4 2 consideration of upstream reservoirs where there is a series of reservoirs along a river and its tributaries and the inflow volume of the upstream reservoir is greater than 10 of the natural inflow to the downstream reservoir the influence of the upstream reservoir on downstream inflow was considered this was done by deriving the difference between the inflow and outflow of the upstream reservoir and adjusting for that for downstream reservoir inflow 3 the interface of reservoir assessment tool rat and operational reservoir monitoring 3 1 graphical user interface gui the main window of the frontend is shown in fig 5 which can be accessed through http depts washington edu saswe rat beta the detailed design of the frontend and salient features of the tool are discussed in the user manual of the tool and also available in the github link https github com nbiswasuw rat reservoir assessment tool currently 1598 dams from the grand database version 1 3 located in south america africa and southeast asia are modeled operationally and visualized on the rat frontend interface all reservoir parameters i e aec surface water extent inflow storage change and outflow were added to the frontend 3 2 monitoring of reservoirs 3 2 1 monitoring of existing reservoirs as more recent and frequent satellite observations on reservoir areas become available via gee the rat framework automatically processes the data runs the hydrological model and post processed model outputs to create updated estimates of outflow inflow storage change of the existing reservoirs the water extent time series is extracted with the latest available scenes per the methodology used for water extent area extraction mentioned in section 2 3 3 the available aec data and water extent time series are processed to get the storage change time series vic model is simulated weekly at a daily time step to get the most recent inflow into the reservoirs finally the outflow is calculated from the inflow and storage change all of these time series data are made available in the frontend for user access the data and information interchange between the backend server and the frontend gui of the tool is explained in fig 6 if the inflow into any reservoir is not calculated the user can make a request through the frontend which is explained next in section 3 3 3 3 user request for adding new reservoirs to rat when data is not available over a reservoir location shown in the rat framework jquery allows a user to push a request button shown in figure 16 of the rat framework user manual the user needs to specify the grand id of the reservoir when sending the request and other information as mentioned in the user manual see figure 16 of the user manual the form can also be accessed through this link https forms gle muebn4bheie1b91j7 the request will push notifications to the administrator of the rat framework to take further action after being notified the administrator can review the request to add the missing dam to the available list for calculation of reservoir state during regular monitoring of reservoirs the newly added reservoir will be considered for deriving all the parameters including aec extraction and the user notified of the availability of data 4 results and discussion the developed rat software framework was applied in estimating reservoir storage change inflow and outflow reservoir inflow and storage change were compared against in situ measured data the reservoir outflow was derived from the inflow and storage change using the mass balance approach discussed in section 2 1 thus it is assumed that accuracy of reservoir outflow is dependent on inflow and storage change accuracy 4 1 accuracy of reservoir storage change in situ measurements of daily reservoir storage were web scraped from the central water commission cwc http cwc gov in of india this web scraping is very similar in nature to a hydrologic platform development work described by biswas and hossain 2018 a map showing the reservoirs locations along with their surface area in different colors and area perimeter ratio termed as irregularity index shown in different colors is shown in fig 7 to quantify the proposed framework s performance for different sizes of reservoirs 77 reservoirs were classified according to their surface areas from grand database in five different classes very small small medium large and very large as seen in table 2 to compare the simulated storage change based on reservoir shape the reservoirs were classified according to the ratio of surface area to the perimeter from grand database which is termed here as irregularity index the categories based on area perimeter ratio are highly irregular very irregular irregular regular very regular and highly regular details about the classification based on the area perimeter ratio are shown in table 3 three reservoirs each with a distinctive irregularity index are shown in fig 8 to illustrate the differences in reservoir shapes based on the irregularity index the simulated storage change was compared against in situ storage change of the individual reservoir on a monthly basis different sensors landsat 8 sentinel 1 and 2 and different index based methods i e ndwi mndwi wi awei for landsat 8 and sentinel 2 backscatter coefficient for sentinel 1 were tested to compare their accuracy the correlation coefficient and the normalized root mean square error nrmse were used to quantify the accuracy of individual methods and sensors for different reservoir classes the sensors and methods are described in table 4 the mean correlation coefficient and the normalized rmse comparison of different reservoir sizes are shown in fig 9 all of the sensors and methods yield a correlation coefficient of more than 0 7 for all types of reservoirs the correlation coefficient is highest for reservoirs with more than 200 km2 of surface area very large reservoirs for landsat marked as l8 in fig 9 all methods yielded a correlation coefficient of more than 0 8 which means more than 80 of the in situ storage can be represented by the rat framework except for very large reservoirs the normalized rmse comparison revealed that sentinel 2 generated storage changes are less accurate than those of landsat or sentinel 1 possibly because all of the indices were extensively tested for waterbody detection using landsat data sentinel 1 provided better representation than sentinel 2 however the accuracy was less than that of landsat although the data from sentinel 1 was more accurate than that of all other sensors except landsat a few unrealistic estimations and a smaller number of samples resulted in a low score vegetated inundation poor quality atmospheric composition during imagery acquisition and incorrect identification of sand pixels as water are likely some of the underlying issues resulting in the low performance of sentinel 1 martinis et al 2015 in the case of very small small and medium reservoirs landsat performed better for all indices than the other two sensors among the different methods of landsat 8 the dswe method generated time series with continuous underestimation of water area and fewer records compared to the others this was due to multiple filtering conditions additionally gee processing time was almost five times higher for dswe than the calculations of other indices making gee a less practical method of global reservoir monitoring mndwi method was found to have limited skill for reservoirs located in steep terrains compared to all other methods ndwi produced consistently better results with a simpler processing approach the storage change time series also was classified according to reservoir the irregularity index which was used for very irregularly shaped reservoirs with extensive shorelines the satellite imageries have limitations in detecting water pixels at the edges thus it was helpful to quantify the relative performance of the sensors and methods the mean correlation coefficient and mean of the normalized rmse for each of the classes were compared and shown in fig 10 the accuracy of every method and sensor decreased with the irregularity of the reservoirs landsat based methods worked best for reservoirs in the irregular category for regular shaped reservoirs almost all methods yielded similar results highly irregular shaped reservoirs returned the lowest correlation coefficient mostly due to water detection along the reservoirs shorelines considering all the advantages and disadvantages of each of the sensors and methods the landsat 8 based ndwi method s performance was found to be most robust and consistent and was therefore selected for the operational rat software framework 4 2 validation of reservoir surface area estimation the rat framework s simulated reservoir surface area was compared with the latest published reservoir surface area dataset prepared by zhao and gao 2018 zhao and gao 2018 dataset provides the surface water extent area of the grand database from 1984 to 2015 on a monthly scale we compared the total reservoir surface area from the ndwi method of landsat 8 sensor and the dswe method for landsat 5 of all the rat domain reservoirs to the total surface area of the same reservoirs estimated by zhao and gao 2018 the rat framework was extensively validated for the landsat 8 satellite imagery and it was found that the ndwi method worked best in the case of the landsat 8 due to differences in sensor characteristics and differences in spectral band ranges we found that the dswe method worked best for the landsat 5 for operational purposes water areas generated from landsat 5 using the dswe method and landsat 8 using the ndwi method were combined to produce monthly timeseries and then compared with zhao and gao 2018 data fig 11 it was also found that reservoir surface area records were discontinuous for many reservoirs in the zhao and gao 2018 data during the years before 2000 consequently comparison began with the year 2000 the proposed framework yielded a correlation coefficient of 0 92 fig 11 shows that the reservoir surface area s seasonal variation is more clearly visible when the proposed framework generated dataset was used we should note that zhao and gao 2018 dataset is not available in near real time scale for monitoring reservoir dynamics from the latest available satellite imagery 4 3 validation of vic hydrological model using the land surface parameters and meteorological forcing mentioned in section 2 3 4 the vic and route model was simulated for the rat domain south america africa and southeast asia the simulated daily streamflow was first compared against ground based daily discharge data collected through different sources information about validation stations is mentioned in table 5 the stations along with the respective basins are shown in the upper panel of fig 12 time series comparisons of two stations are shown in the middle panel of fig 12 the left panel is tabatinga station located in the amazon basin where the correlation coefficient was less than 0 7 and the vic model was not very accurate in representing low flow and high flow peaks a similar case was observed at other stations timeseries comparison of kampong cham station located on the mekong river shown in the middle right corner found accuracy was highest for the correlation coefficient this basin performed exceptionally well due to subbasin scale calibration performed by hossain et al 2017 nevertheless the hydrological model showed dry season flow to be lower than the actual flow and overestimated the peaks summary statistics of all validation stations are shown in the lower panel of fig 12 in southeast asia s stations the correlation coefficients were more than 0 8 whereas the south american stations showed more than 0 6 also in some stations nrmse was higher with a good correlation coefficient due to the model s underperformance in capturing seasonality we found that flow direction modification improved the results significantly 4 4 comparison of streamflow with grades streamflow we compared streamflow at different inflow locations with the global reach level a priori discharge estimates for surface water and ocean topography modeled streamflow grades lin et al 2019 we compared our model s estimated streamflow with the grades model s simulated streamflow at 44 randomly chosen locations along the river reaches within the rat domain the summary statistics are shown in fig 13 the average correlation coefficient of all the stations was 0 62 and the mean of normalized root mean square error was 0 49 again stations located in southeast asia performed better compared to the stations in the south america region this is a clear indication that better calibration at regional and basin scales can improve simulated streamflow accuracy at the locations where our model underperformed 5 conclusion and future scope to our knowledge the online software framework for reservoir monitoring called rat is the first of its kind given that the rat tool is now publicly available for the world to use and benefit from we believe the following are some examples of potential applications of this framework by using this tool long term records denoting real time behavior and operating rules at reservoirs can become publicly available rat can help users and the scientific community derive a global picture of reservoir monitoring how they are being operated and how they are likely impacting natural river flow and its variability as a function of climate hydrologic regime and socio economic indicators with further improvements in hydrological modeling using locally available ground observations the rat framework can be used with higher accuracy in local regional and global scale operational water resources management considering its near real time data availability the rat framework can facilitate feasibility study of proposed planned dams it can be used to estimate the future reservoir capacity and inflow availability at any location which is useful in optimizing reservoir benefits the rat framework presents future possibilities to study the impact of harnessing hydropower on river temperature greenhouse gas emissions aquatic habitats land use and landcover change and agriculture practices the rat tool can be used to minimize conflict between riparian countries i e egypt and ethiopia over nile basin china india and bangladesh over ganges brahmaputra meghna basin china laos thailand cambodia vietnam over mekong basin as it can be considered an unbiased tool to all parties and provide data needed to drive fair and transparent water sharing agreements there are some future improvements that can be considered to make the framework more applicable in solving real world water challenges while the hydrological model validation showed promise known uncertainties mandate that current results be carefully interpreted additionally the vic model used here will still need time to be improved and validated further at a more granular level this need for improvement at many regions is evident when a superior goodness of fit such as nash sutcliffe efficiency fig 14 is compared with in situ flow accumulated over 16 days to match landsat revisit time and for reservoir management improved reservoir inflow estimation can be made by using regional and basin scale calibration and validation of the hydrological model is one of them the more complex method of water pixel identification from satellite imagery with artificial intelligence and the application of machine learning algorithms may yield better estimation of reservoir surface area reservoir bathymetry data from ground surveying or lidar applications may be used to define the area elevation curve more accurately lastly reservoir outflow calculated in this study includes all types of diversions and consumptive water uses for various purposes which may not be very accurate nor comparable to the actual reservoir releases thus the simulated outflow should be compared with measured flow at in situ locations immediately downstream of a reservoir the focus of our study here was on reservoirs and on methods of enabling the monitoring prediction of reservoir states surface area storage change inflow and outflow at weekly to monthly scales of reservoir management it should be noted that our study is not about developing a global hydrological modeling framework or even promoting an existing one for that matter our rat framework is agnostic enough that the current hydrological model vic can be replaced with other competing hydrological models our work to develop such an open and publicly available tool is driven by our mission to democratize water information on regulated river basins for all stakeholders and to facilitate more equitable water management we believe that such a tool can level the playing field for stakeholder agencies and riparian nations that suffer from limited access to information on water availability due to hydro politics lack of in situ infrastructure or low adaptive capacity software availability https github com nbiswasuw rat reservoir assessment tool key findings 1 a web based framework was developed for near realtime monitoring and impact analysis of reservoirs around the world 2 the framework is freely available and able to monitor the dynamic state for more than 1500 reservoirs 3 the storage changes of more than seventy five percent of reservoirs were accurately captured with skillful inflow simulation at bi weekly timescale declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by a nasa applied science program grant in water resources nnx15ac63g and nnx16aq54g additional support from nasa applied sciences servir grant 80 nssc20k0152 is also acknowledged appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105043 
25842,in participatory modeling pm a conceptual model emerges from an exchange of information and opinions among stakeholders this usually happens in a series of in person workshops restricted to a certain number of attendees during designated time intervals our goal is to open up the pm workshop process to engage an unlimited number of participants at various locations while supporting them with the functionality that the modeling context can offer we develop a real time moderated steering environment named discussoo to facilitate online pm users express their opinions about a topic by providing their comments in online discussions as the discussion evolves an ensemble of artificial intelligence algorithms in the background automatically produces a dynamic conceptual model to visualize the on going exchange of opinions moderators can use this model to provide feedback to users and guide the discussion policymakers and managers can use discussoo to support more transparent and meaningful engagement of stakeholders graphical abstract image 1 keywords mental model public engagement online discussion nlp ai 1 introduction decision making is a highly cognitive process based on perception belief knowledge and memory to choose an action amongst various possible alternatives when dealing with complex problems decision makers need to choose one option based on multiple and sometimes contradicting criteria in addition most often decision makers themselves hold contradicting opinions and perspectives on problems making it difficult to agree on the best option for example in natural resource management a group of stakeholders has to decide on where to invest available resources how much to allocate to each stakeholder and how to protect resources and ecosystem services optimal decisions assume a compromise among all of these factors in such cases stakeholder involvement can help to increase transparency trust and public acceptability of future decisions in this regard participatory modeling pm is one of the most practical methods to facilitate the engagement of stakeholders in the process of decision making and enable collaboration to achieve a mutually acceptable solution sterling et al 2019 it supports the decision making process by ensuring that the stakeholders have a shared forum in which they can openly and freely express their opinions and suggestions it is also a purposeful learning process for action that engages the implicit and explicit knowledge of stakeholders to create formalized and shared representations of reality using a range of modeling tools voinov et al 2018 as a school of thought this method emphasizes transparency and the necessity of engaging those impacted by the decisions barnaud and van paassen 2013 it also helps to overcome power imbalance especially when there are prevalent imbricated layers of power amid citizens organizations and government pm organizes stakeholder engagement around the process of building models of the systems of interest models are used as abstractions of real life systems and the mechanisms involved pm connects modeling science with stakeholder knowledge and experiences to produce an understanding of the situation shared by the group models offer a unique opportunity to study human reasoning and elucidate the way in which one thinks and makes decisions regarding the system at stake voinov et al 2016 shared models can greatly improve the power dynamics by offering several complimentary entry areas for logical and cognitive contributions building explicit shared models using the pm approach can greatly increase the applicability and robustness of the results pm practitioners usually organize a series of workshops for the stakeholders to share their ideas and preferences and solicit knowledge from them about the system and about the decisions that are to be chosen these workshops are normally restricted in time and can accommodate only a limited number of participants it would be hard to conduct a constructive discussion with too many stakeholders invited and to get all of them join a workshop at a particular time in one place currently according to well consolidated trends in communications many discussions take place over the internet as part of social and multimedia environments social web people go online to submit their ideas and opinions about various issues this can happens on social platforms such as twitter or facebook lengthy exchanges of comments take place in blogs where posts attract many people sharing their feelings and information this leads us to the idea of moving a part or perhaps even the whole workshop process in pm to the internet offering stakeholders an easy and asynchronous way to learn from other stakeholders and express their opinions about the problem at stake moreover if the discussion is all digitally recorded it becomes available for machine processing when additional information can be extracted from the comments available and fed back into the discussion to enrich it one could refer to the existing online platforms to mimic some parts of the discussions that normally occur during workshops however they offer only limited access to the information collected in the comments making it hardly possible to process the comments and derive any knowledge from them moreover existing platforms do not assume any place or role for facilitators or moderators of the discussion who could guide the discussion or provide users with instant feedback while models are powerful communication and learning tools none of the available platforms can translate discussions into more formal models qualitative or quantitative leaving the users alone in the process of deep understanding these considerations convinced us that instead of relying on existing platforms we should develop a specialized online tool that can be deployed in any pm process this tool discussoo provides a platform for stakeholders to share their opinions by submitting comments while the recorded texts are immediately processed and information is extracted from available comments in an attempt to generate on the fly conceptual models of the systems discussed which can be immediately fed back into the discussion for stakeholder checking and confirmation having direct access to all the texts submitted as comments we can use artificial intelligence ai techniques to derive patterns from the data available even when the sheer volume and heterogeneity of information available are huge information extraction ie is a technique to identify and analyze relevant and useful structured data from a large collection of unstructured data that is being generated by people and machines the extracted structured data is ready to be used for deriving information and conducting further analysis adnan and akbar 2019 natural language processing nlp is one of the main ai methods that can be used for ie and has been widely applied in different contexts to examine conventional texts we rely on this method to analyze the text in comments when running a discussion in discussoo it helps us to extract concepts keywords relationships and interactions which can then further lead to some more formal models of the topic in the discussion however as with all ai methods they require training and learning to excel in performance it might take some time for us to make sure that the methods we use always produce meaningful and useful conceptual models automatically at this stage we also allow the moderator to play a role in checking and improving the conceptual model in terms of research value discussoo aims to generate and properly visualize mental models that underlie a given discussion in the specific context of this work we are not referring to a formal definition of mental model we rather assume a mental model to be a structured machine processable representation of a given conversation such a representation is expected to summarize the key knowledge that is collaboratively produced by discussion participants discussion mining has been already applied in different contexts and most common techniques are relatively consolidated nagao et al 2004 for instance in massive open online courses moocs data in certain online spaces including also discussion forums is often mined pillutla et al 2020 to achieve such different goals as answer prioritization almatrafi et al 2018 identification of at risk learners he et al 2015 success prediction kennedy et al 2015 klusener and fortenbacher 2015 fostering smart learning liu et al 2016 interaction and knowledge building analysis lucas et al 2014 and discussion classification rossi and gnawali 2014 wise et al 2017 focusing more specifically on mental models their understanding reconstruction and representation vary very much from case to case however although a generic solution is probably unrealistic a number of consolidated mining techniques can ideally support the generic conceptualization of mental models in many applications and domains for instance mental models may be associated with concept maps eventually enriched by additional semantics indeed concept map mining has been formally proposed in 2011 villalon and calvo 2008 assuming an automatic process of unstructured data such a contextualization has been applied to generate concept maps from news tseng et al 2010 and more generic documents zubrinic et al 2012 more sophisticated approaches may be defined by assuming probabilistic solutions e g using fuzzy logic pillutla and giababnelli 2019 as far as the authors know there is not much technological support specifically designed for pm which normally relies on generic tools for collaboration in person while still relatively generic technology mediated solutions start to appear e g reddy et al 2019 we believe that the tool proposed in this paper may specifically address pm s needs yet preserving a fundamental genericity that should also allow other applications discussoo is a generic solution that aims to enable effective scalable discussions among stakeholders indeed the moderation functionalities and the ai powered features both with smart management of different discussion threads create a digital environment that encourages focused discussions and eventually decision making the tool has not been designed to address specific topics or to work exclusively in a given application domain it is rather understood as a generic asset however from the experience matured so far we expect the adoption of the tool in fields characterized by a variety of stakeholders such as sustainability and policy making last but not least digital platforms become even more relevant in situations of crisis emergency or disruption the covid 19 pandemic we are currently experiencing is a perfect example in more general terms we believe that accessing a digital platform is more affordable than any in person activity which may have other advantages though discussoo is currently available for free as a web platform from a user perspective there are no significant technological restrictions or barriers to access the platform except access to a last generation web browser limitations or barriers determined by extreme conditions such as limited internet connectivity or lack of knowledge to use online tools are unfortunately not under our control we note that online participation is in principle more affordable and open than attendance at face to face meetings so we believe our tool promotes fosters and facilitates the inclusion of minorities and local communities and not their exclusion the remainder of this paper is organized as follows section 2 introduces discussoo concept its components and modules we explain the need for such a web based platform and show how it can be used to systematically manage the pm process we then briefly describe some of the advanced artificial intelligence ai algorithms and technologies that can automatically build conceptual models with multiple stakeholders independent of their numbers geographical locations section 3 elaborates the details of using discusso in a real case study section 4 highlights the associations between discussoo and pm and demonstrates how the evolving group mental model generated in real time from the ongoing discussion and presented to the participants can facilitate online workshops the last section discusses the implications of the proposed tool and future directions 2 discussoo an online intelligent tool for pm while involving multiple stakeholders in a discussion provides an opportunity for groups and organizations to improve the quality of their decisions they inevitably face a new set of challenges with regards to engaging people and managing conversations there are many online and offline forums platforms tools available for involving the community in serious discussion however they do not offer much assistance to users in extracting insights from ongoing discussions or learning from them from the experience of the pm we know that systems modeling can be a useful tool to organize the discussion process and use it for improving decision making we develop discussoo to use online discussions for conducting the first step of the modeling process the generation of a conceptual model of the system at stake in the following sections we introduce discussoo and explain how it can empower the pm process 2 1 interface discussoo is an online web based platform for serious discussions largely inspired by the need to provide online support for the pm it deploys advanced ai methods and nlp techniques to automatically derive group mental models from the ongoing discussions and interactively visualize them as cognitive maps to leverage group communication and decision making on the landing page we have a list of ongoing discussions around different topics and a user can join any topics of interest or start a new discussion if needed there are also hidden discussions which have restricted access and only those invited users can join by clicking on the invitation link by clicking on the join discussion button the user opens the discussion page which contains the background information and the goals of a particular discussion as shown in fig 1 on the right hand side we present references to other relevant on going discussions include links to facebook linkedin twitter if the same posts are also shared there thread posts and discussion are the terms used interchangeably to describe the details of the topic chosen users can get involved in the discussions by pressing the join button each discussion post opens a new page where users can contribute to the discussion by leaving comments in order to make the discussions more effective and meaningful users can be assigned different roles as listed below 1 admin admin has control over the whole platform and can access the administration panel to approve new discussions create new users and new groups or assign users to groups admin can also assign remove group moderators while updating editing the group information 2 moderator all discussions can be facilitated by a moderator controlling the flow of comments exchange and can play a role in avoiding escalation of conflict between participants moderators can create new users form groups and manage individual discussions by editing or deleting the posts and comments using this content control mechanism moderators can protect the integrity of the topics and discussions in doing so they directly deal with unlawful or illegal comments such as defamatory statements verbal bullying and copyright infringement beside facilitation of the engagement moderators can encourage lively debates and stimulate the process by making it more enriching for all the participants they answer the questions that may arise in the course of the discussion in the form of comments 3 group moderator in discussoo multiple discussions can be conducted at the same time with different groups of participants for managing these discussions we have introduced the feature of groups and each group will have one or more moderators to facilitate discussions within that group group moderator will have all the controls such as editing deleting users posts and comments within a group the key difference between this role and the previously described one is the extent scope as the moderation activity of a group moderator offers access to discussions only within a group of participants it primarily responds to practical needs in presence of multiple parallel discussions which normally require a de centralized moderation model additionally it allows dynamic delegation of responsibilities as the function of discussion directions and participants involved 4 normal user anyone will be able to access the home page but in order to perform any activity they have to register and sign in if users do not register they can still participate in the discussion anonymously as a guest if the discussion is not a closed one for this purpose a system generated user name will be assigned to them and later on they can register anytime with their own email address users can both start a new thread for discussion and contribute to the ongoing discussions either by adding new comments or by responding to the already existing comments 5 registered users can start new discussions by posting a question and giving some background and context information there are multiple options for selecting the audience for each discussion discussions can be made public or private public discussions will be visible to all the users while private discussions will not be listed on the website and will be shared only with those who have the link to the discussion like in google docs alternatively participants can be invited to join the discussion by providing their email addresses in a csv file in this case anyone who is invited and has the link to the discussion should be able to access it discussions can be shared in different social media platforms facebook twitter and linkedin or can be sent to an email address in order to get a wider audience engaged in a discussion there is a feature of sharing posts discussions in discussoo to different social media platforms so that new comments can be collected from there to enrich the analysis the analyzed results can be fed back into the discussion and reference can be provided within the tool to allow users to understand the context each post has a section for likes comments share save hide report users can rate other comments to express their agreement or disagreement instead of a binary like dislike grading we use a rating from 0 to 9 to allow a more nuanced expression of feelings at any time the average rating of a comment is calculated and presented here overall discussoo interface offers a real time moderated web environment to engage participants in a virtual discussion of a problem users are presented with questions about different topics and problems to discuss on which they can express their opinions thoughts in the form of comments users can then respond to or expand on a comment based on other user s opinions and or initiate a new line of discussion for pm we see this as an addition or replacement for in person workshops which are essential for that process 2 2 engine in addition to tracking the exchange of comments discussoo has a back end which uses ai techniques to extract new knowledge from the on going discussions and improve understanding of problems by generating mental models from the information provided by users fig 2 while the discussion continues the recorded texts are mined in real time using an ensemble of algorithms including but not limited to concept mining topic modelling and sentiment analysis from the comments we extract a set of keywords concepts and the relationships among them the information extracted is presented in the form of cognitive maps causal loop diagrams or networks diagrams which we intend to further translate into semi quantitative models such as fuzzy cognitive maps through visualizing a dynamic collective mental model the tool provides instantaneous feedback to its users tells them how the discussion is evolving which may potentially influence their contextual information to refine and update their individual mental models feedback can also be introduced by targeted moderator comments to steer or nudge the discussion towards a more meaningful collective mental model potentially leading to improved outcome consensus or agreement we believe that this functionality can enrich the pm process and in many cases could replace the rather limited workshop schedule by a much more open and inclusive online format the moderator can play a role in moving around or grouping comments to focus on particular issues in the discussion additionally the moderator can also directly impact the conceptual model generated by removing concepts and interactions indeed as the conceptualization is automatically generated by applying generic npl techniques the outcome may include inaccuracies that potentially introduce noise to the model produced other users do not have direct access to the diagram but can ask the moderator to make changes to generate conceptual models from unstructured data e g comments sentences we use an ensemble of different nlp techniques including named entity recognition ner lample et al 2016 sentiment analysis rajput 2020 text summarization nenkova and mckeown 2012 aspect based mining zhang and liu 2014 and topic modelling yau et al 2014 one of the most fundamental tasks in nlp is extracting the entities from the content using ie we use this to find important concepts and references in the comments provided ner is a sub task of ie to distinguish named entities that appear in the unstructured text and to put them into predefined categories for example names of individuals locations companies dates etc adnan and akbar 2019 this algorithm uses the grammar rules of a language and supervised learning models there are also built in and trained ner models available such as open nlp shelar et al 2020 in discussoo we use this to identify and segment the named entities and classify categorize them under various predefined classes sentiment analysis is another broadly utilized method in nlp sentiment analysis is often applied to analyze customer surveys and reviews on social media where people post their feedback about products or services the simplest form of output in sentiment analysis is a 3 point scale such as positive negative neutral in complex cases the outcome can be a fuzzy numeric scale a range from 0 to 10 where 0 is defined either as positive or negative in the spectrum based on which a desired number of sentiment groups is defined this method has applications in both supervised where labels are required with training dataset and unsupervised where labels are not required with training dataset learning on the one hand na√Øve bayes along with random forest smo and j48 are examples of sentiment analysis in supervised learning settings ahmad et al 2017 on the other hand unsupervised methods determine the polarities of the words and calculate the sentiment scores in a sentence in discussoo sentiment analysis is used to first extract opinions from discussions and then scale each of them as positive negative or neutral this helps us in tracking any changes in users opinions and understanding whether learning occurs throughout the discussion text summarization is a vital nlp task with many potential applications in extraction and abstraction extraction is mainly related to i extracting key aspects from textual content and creating an intermediate representation ii assigning scores to sentences in the text in that representation iii and forming a summary by choosing multiple sentences nenkova and mckeown 2012 extractive methods select a subset from a corpus of words sentences or documents in the source content to shape a summary the main tasks of the extraction based summarization are the identification of the key phrases and using them to select sentences in the text document for incorporation in the summary nenkova and mckeown 2012 abstractive methods however first form an internal semantic representation and afterwards utilize natural language generation methods to deliver a summary such a summary may contain words that are not expressive in the original text in contrast to the extraction abstraction based techniques paraphrase segments of the original document gudivada and rao 2018 the fundamental task of aspect based or feature based mining is to extract and summarize individual opinions about entities and their aspects hu and liu 2004 it performs three main sub tasks i classifying and mining entities in analyzed texts ii classifying and extracting aspects of the entities iii discovering sentiment polarities on entities and aspects of entities for instance in the following sentence i have read another paper about climate change today and am really worried about the temperature anomaly in greenland and the fast decline of the ice sheet there the aspect based opinion mining method should recognize that the author has a negative opinion about climate change the temperature anomaly in greenland is an aspect and climate change is the entity in discussoo if the user comments are very long paragraphs then text summarization algorithm is used for condensing the comments and reducing the size of the initial text while preserving key informational elements and the meaning of content moreover when we utilize it in combination with sentiment analysis we can mine emotions from the text sentiments can only tell us the polarity of the sentence whether it is positive negative or neutral whereas emotions are about the feelings and senses in a sentence such as happiness love fear anger or hate hence emotions are the drivers of sentiments and determine their polarities topic modeling is a category of machine learning methods for extracting the latent topics that appear in documents yau et al 2014 one of the main advantages of this technique for our case is its independence from the labelled dataset enabling us to find topics in an unstructured dataset correlated topic model ctm latent dirichlet allocation lda wei and croft 2006 probabilistic latent semantic analysis plsa and latent semantic analysis lsa are four well known topic modeling algorithms discussoo deployed lda for the text analysis the reason for this selection is that every textual content involves a several topics and each topic comprises many words the information required by lda is just the textual content and the anticipated number of topics making it suitable for comment analysis in our tool in extracting relationships discussoo encompasses two different techniques namely textrunner and formal concept analysis fca valverde albacete et al 2016 textrunner algorithm enables the machine to learn the relationships entities and classes from a text it has three key components including i self supervised learner to classify the input data into trustworthy or not ii single pass extractor to extract all the possible relations among the identified entities and iii redundancy based assessor to determine the optimal number of relations besides fca explains the relationships between a specific set of objects and their properties it has is widely used as a knowledge representation structure particularly in knowledge engineering and ontology generation problems in technology as a result discussoo is expected to produce a group mental model using a combination of all the described algorithms 3 case study as mentioned above the testing of algorithms implemented in discussoo for analyzing discussions is still quite limited however a number of experiments have been conducted within different application domains and additional ones are expected to be conducted in the near future as with many machine learning approaches the more data is collected the better we can train the algorithms and improve their performance the feedback obtained and the experience matured in real world scenarios are providing key insights and directions to holistically improve the quality of experience of discussoo users this agile methodology allows a smooth harmonization between research and development and enables a natural evolution of the tool in response to concrete users needs for example in september 2020 an educational research based organization named institute of public policy and governance adopted the tool in their engagement activities involving teachers and lecturers their discussions were to analyze recent changes in teaching and learning strategies during the covid period they invited teachers from different institutes to share their opinions and thoughts about various topics related to online classes student performance strategies for virtual management and so on 81 teachers have shared 139 comments to express their point of view and experiences they have created 11 groups related to challenges strategies needs leadership innovations etc within these groups they opened 24 discussions as shown in fig 3 we select a discussion titled what challenges have been new or heightened because of this period for further analysis because of two reasons firstly this discussion has the highest number of users and comments and secondly as the topic is not organization specific and generic it is more understandable for participants analyzing these comments discussoo managed to extract the key concepts calculated the frequency of their appearance in the discussion and determined the sentiments of the opinions table 1 these key phrases and sentiments are extracted using amazon comprehend as a preliminary result next they are further refined using a combination of algorithms to extract concepts for creating conceptual models and sentiments to identify any changes in the opinions there are numerous ways that concept maps can be visualized giabanelli et al 2016 for example in fig 4 we present a conceptual model that was extracted by combining word2vect algorithm and word embedding technique for creating a training model this model helped us to get a relation score between concepts it also performed ner described in section 2 2 to extract top concepts related to location person organization event area etc for the extracted concepts it identified all closest child concepts with a score and visualized them using the network graph algorithm these are the results of our initial attempts to analyze the comments 4 discussion addressing pm challenges with discussoo the development of a model with stakeholders is a multi stage process that conceptually captures the following processes 1 scoping and abstraction 2 envisioning and goal setting 3 data collection and preliminary analysis 4 model development and 5 presentation of the modeling results to the audience in most of these stages we can find a role for discussoo to play fig 5 scoping and abstraction the initial phase of pm focuses on framing problems and defining relevant questions to achieve integration cooperation and solutions it is important to gain a joint understanding of a problem models that come out of stakeholder engagement workshops help to represent the discussed topic in a formalized and structured way it is important to set the conversational boundaries appropriately to avoid model complication and allow for comprehension and reflection falconi and palmer 2017 narrowing down the scope of the problem is highly critical especially in the first engagement with stakeholders when comprehensive instructions of the pm workshop is given to the participant to guide them through the process in many cases the scoping process is organized in the form of facilitated open discussion it can also be supported by analyzing the development of the problem boundaries during certain periods of time e g looking at the graphs with data over time for several parameters that are directly connected to the discussed problem discussoo platform accommodates the scoping process in two different ways firstly users can initiate a preliminary discussion on the platform to clarify the boundaries of a problem secondly they have the option of using this tool to discuss a pre agreed problem statement as they may already be defined in separate consultations with stakeholders there is certainly a role for the moderator to properly frame the questions and lead the discussion in the desired direction to a certain extent the self selection of the participants and their perspectives on the matter can also automatically determine the boundaries of the problem in contrast to the offline pm which is limited by the time and space factors to run a workshop the online discussion platform provides an opportunity for the pm organizers to engage an unlimited number of stakeholders regardless of their geographic location time constraints and role in the decision making process the discussions can be widely open to all users or they can be restricted to a certain nominated group of participants envisioning and goal setting during the process of envisioning and goal setting the group of participants decides on the structure of engagement clarifies the questions for steering the process and ensures their major interests and ideas are represented and discussed belt 2004 emphasizes that an envisioning exercise is crucial for creating positive attitudes in participants at the initial stages of pm the goal is to design a model to provide a bigger picture of the desired state of the situation as opposed to the current state that is perceived as problematic in discussoo this is the next step of the ongoing discussion where participants will share their points of view about the problem by leaving comments they can also see the views of others on the same issue react to their opinions by replying to their comments or scoring them as such the interactions communications and learning by others can occur throughout the whole process the posts and comments and the scores they get as well as their sequence and logical connectivity provide the key concepts and relationships data facts logic data availability is a crucial component for both qualitative and quantitative model development pm as a method follows post normal approach and implies the use of data not only from secondary sources such as historical data articles reports and white papers but also primary sources such as information extracted from direct interaction with stakeholders in forms of interviews surveys discussion groups funtowicz and ravetz 1994 voinov and bousquet 2010 the stakeholders or problem owners are treated as one of the main contributors to the knowledge about the problem because they have direct exposure to the discussed problem videira et al 2016 in discussoo platform we have the advantage of directly including links urls to relevant data and information already available elsewhere in future implementations this information can be automatically supplemented by web crawlers that will be mining the web and further enriching the discussion by relevant references as well as fact checking the posted discussions themselves as well as comments responses to other comments scores for other comments are an additional source of data the text and numbers retrieved from the user s interface are cleaned structured and stored in a master database in real time to feed the algorithms again as the discussion evolves there are important roles for the moderators to take on firstly they can take precautionary measures and ensure that the exchange does not touch upon any sensitive unpleasant context for any of the participants secondly with the help of facilitation techniques they can guide the discussions among the participants towards more constructive modes of exchange e g asking questions improving the logic of the conversation etc thirdly we intend to introduce some gamified interventions in an attempt to increase cooperation among participants in the group bakhanova et al 2020 having said that many of these approaches and issues are yet to be tested modeling group model building is the process of representing the perceptions values and opinions of stakeholders about the discussed problem in a structured and logically consistent way this stage is the core part of pm because through the model development practices the participants may learn about the problem and modify their perceptions by knowledge integration voinov and bousquet 2010 so far we mainly focus on the ideas of vennix 1996 to use nominal group technique when developing a qualitative model with a group of participants this technique refers to the gradual elicitation of the knowledge from the participants that use a combination of brainstorming and facilitation principles to initially identify the elements of a problem and the connections between them before developing the overall framework of the model currently discussoo can help in designing only conceptual models of systems considered in some cases we can also elicit some semi quantitative metrics for weights of influences when analyzing how frequently various terms and concepts are mentioned and how they are scored in the discussion instead of involving a modeler in the process the centralized ai component of discussoo can automatically develop a model by capturing the opinions of users in the form of concepts topics and sentiments attached to those opinions including the relationships between them these become the building blocks of dynamic conceptual models such as word clouds cognitive maps causal loop diagrams etc the automation of the model development stage brings some benefits in terms of debiasing the process ideally modelers are expected to be completely neutral and objective when putting models together but in reality their interest and association with the topic make them also biased and subjective normally it is expected that this subjectivity is identified and managed in particular by communicating to the stakeholders in a pm process that scientists also hold values and beliefs which can lead to better understanding and trust voinov et al 2014 discussoo could provide additional support as the generated conceptual models are not the result of some deliberate choices made by selected participant s modelers but are instead obtained by directly applying standard analytical techniques to the conversation all critical assets composing the target model are derived from the analysis of the conversation in terms of text and structure the former aims to infer key concepts while the latter targets the definition of the relationships among them the conceptual models become the results of a quantitative analysis of the occurrences of the different keywords and the interactions among them it is yet to be seen whether the algorithms used will become a source of additional bias on their own but the human intervention in the actual model building process becomes somewhat restricted presentation and evaluation one of the main objectives of conducting pm is to foster shared learning about the problem by providing feedback to the participants jordan et al 2018 in doing so the final stage of the process provides the stakeholders with visual presentations that could help them to disseminate the knowledge conclusions learnt during the modeling process to a wider audience belt 2004 hence diverse forms of model visualizations and interfaces are used to facilitate model communication the management flight simulator is one of the well known examples maier and gr√∂√üler 2000 they give non modelers an opportunity to interact with the model and see the response of the system to the changes in the model parameters the interactive modeling approaches also proved to be effective for learning about complex problems bakhanova et al 2020 elsawah et al 2017 in discussoo we are not developing any quantitative models but the qualitative mental models are visualized on the fly and immediately fed back into the discussion informing the participants and helping them to produce new and creative solutions to the complex and controversial societal issues the users can suggest new elements or change the connections between the existing ones by expressing their opinions in the comments in offline traditional pm the participants can modify the model in direct interactions with the modeler providing their feedback and asking for further explanation and discussions around the concepts and relations presented in the model in discussoo users do not modify the model directly instead they indirectly change the model by providing additional comments and evaluating existing ones all interactions are considered and all of them are counted in the model however due to the quantitative measurements e g number of occurrences of a given keyword not all contributions lead to a significant change in the conceptual model it is worth mentioning that the application of discussoo goes beyond pm as it can help in any engagement where serious discussions are required this tool can be deployed by governmental agencies e g councils states and federal agencies to seek their stakeholders feedback in particular from citizens discussing policy proposals and planning or management options organizations and businesses in the private sector e g customer and employee engagement customer satisfaction or sustainability reporting who intend to find optimal ways for managing shared geographically dispersed resources such as agricultural lands mines or freshwater supplies professionals in various levels e g research centers universities consultancies in their engagement with clients to devise or advise on public policies teachers for on line teaching to supplement or replace class discussions for example recently discussoo has been used to analyze the comments posted in the chat during the national bushfire climate summit 1 1 https www youtube com watch v tkxhwcqju5w feature youtu be whereas part of the emergency leaders for climate action summit 2020 a panel of four experts were presenting their opinions about the recent 2019 bushfire outbreak in australia at the same time some several dozen viewers were actively exchanging opinions in the chat box the text generated from their comments was stored in the master database and used to feed a set of text mining algorithms in the engine we argue that this analysis can help organizers to better understand the reaction of the public to the challenge of bushfires in australia in fig 6 the main keywords expressed by users have been visualized using discussoo keywords such as climate change indigenous fire burn animal wildlife are highly relevant to the context of a discussion regarding bushfires however irrelevant keywords such as thank and people appear in the results here we can argue that firstly these comments are not extracted from a moderated discussion and secondly as we are dealing with unstructured data a lot of extra filtration or data cleaning is required in order to get good results we also observe a growing interest in utilizing the tool in the post covid outbreak life when there is more reliance on distant communications and fewer opportunities for face to face meetings overall we see discussoo as a unified platform for engaging researchers stakeholders and decision makers in a participatory process for better decision making with no restrictions neither for the size of the group nor for the timing and location of their meetings 5 conclusions our research aimed at designing an online tool to enhance or replace the in person workshop process that is a key element of any pm effort discussoo is an intelligent web tool that provides an online pm platform to enable large groups of people to engage in a discussion and deliver a real time conceptual model by automatically eliciting analyzing and presenting group mental models we have designed and developed a prototype that can run serious discussions and analyze the collected information we explored how it can help to solve some challenges associated with pm in particular the challenges related to knowledge integration and synthesis of participants ideas public engagement scaling overcoming biases during the pm process and improving learning among stakeholders governments academics independent media and citizens can use the tool for serious discussions while there are quite a few online platforms table 2 with somewhat similar purposes none of them offers an opportunity to learn from the ongoing discussion and improve the mental models that it can potentially reveal the promise of discussoo is two fold instead of requiring stakeholders to be at a certain location for a certain time to attend a workshop and exchange knowledge about the system they are offered an asynchronous remote mode of participation when they can attend at any time that is convenient for them and from any location office home beach anywhere from where they can get on line and join the discussion instead of conducting the discussion and soliciting ideas opinions and data first and then moving to assemble this information in a form of some conceptual model here the model is generated on the fly by ai algorithms running in the background and processing the comments as they are posted the conceptual model is immediately fed back into the discussion enriching it and helping stakeholders to focus on the most important and perhaps controversial parts of the system that is analyzed discussoo allows anonymity to users to write on the web behind the curtain enabling them to communicate their views and feelings unreservedly in fact the collaborative process of sharing one another s mental models can 1 enhance the awareness of stakeholders users of their own internal assumptions and values and how these relate to others 2 help stakeholders users to identify their commonalities 3 facilitate the emergence of a common vision for action based on the co construction of a shared mental model that enables an empowered and joint commitment to achieve conservation outcomes and 4 increase social and community participation in decision making especially underrepresented and isolated groups there are other benefits that come with the system such as compensating for bias and credibility of the pm results by minimizing the moderated involvement in the process and generating machine driven outputs we are only starting to experiment with discussoo and there is yet much to learn from how the tool works and certainly many further improvements will be needed in fig 7 we present some of the challenges and opportunities for discussoo in the future they largely fall into two categories technical and ethical on the technical side there is still much improvement needed for the ai algorithms used to elicit and present the mental models involved we expect them to improve as more usage data will be accumulated and better machine learning algorithms will be developed and applied the analysis of large volumes of data can bring new challenges with regards to the needs for more computing power massive ai data pipelines and memory and higher energy usage eventually we can learn much about how we think individually and in a group and how group dynamics can affect our personal mental models lavin et al 2018 this immediately raises multiple ethical issues that will need to be sorted out a number of effective security mechanisms should be devised in order to protect the mental models of users from malicious use understanding of individual group mental models in a discussion forum can improve knowledge and help to accommodate a plurality of values perceptions and beliefs among stakeholders thereby increasing the chance of implementation success however it is yet to be researched how exactly this can be done in the most productive way we invite interested readers to utilize discussoo when planning for their online pm workshops to reflect both on the positive and negative aspects of this tool suggestions for inclusion of new features and capabilities are also welcome our tool is user friendly free of charge does not require technical knowledge and is ready to be used by anyone in any stakeholder engagement and pm project these applications and discussions can greatly help us to identify ways for future improvement and provide new sets of training data for algorithms to present more useful and accurate results discussoo is expected to evolve in the future as core functionalities will be extended and further refined to increase effectiveness and flexibility in a variety of contexts and applications we also aim to address practical needs such as among others the support of languages other than english declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is a part of ph d research supported by funding from the faculty of engineering and information technology university of technology sydney authors wish to thank the editor and five anonymous reviewers for their valuable comments and suggestions on this manuscript 
25842,in participatory modeling pm a conceptual model emerges from an exchange of information and opinions among stakeholders this usually happens in a series of in person workshops restricted to a certain number of attendees during designated time intervals our goal is to open up the pm workshop process to engage an unlimited number of participants at various locations while supporting them with the functionality that the modeling context can offer we develop a real time moderated steering environment named discussoo to facilitate online pm users express their opinions about a topic by providing their comments in online discussions as the discussion evolves an ensemble of artificial intelligence algorithms in the background automatically produces a dynamic conceptual model to visualize the on going exchange of opinions moderators can use this model to provide feedback to users and guide the discussion policymakers and managers can use discussoo to support more transparent and meaningful engagement of stakeholders graphical abstract image 1 keywords mental model public engagement online discussion nlp ai 1 introduction decision making is a highly cognitive process based on perception belief knowledge and memory to choose an action amongst various possible alternatives when dealing with complex problems decision makers need to choose one option based on multiple and sometimes contradicting criteria in addition most often decision makers themselves hold contradicting opinions and perspectives on problems making it difficult to agree on the best option for example in natural resource management a group of stakeholders has to decide on where to invest available resources how much to allocate to each stakeholder and how to protect resources and ecosystem services optimal decisions assume a compromise among all of these factors in such cases stakeholder involvement can help to increase transparency trust and public acceptability of future decisions in this regard participatory modeling pm is one of the most practical methods to facilitate the engagement of stakeholders in the process of decision making and enable collaboration to achieve a mutually acceptable solution sterling et al 2019 it supports the decision making process by ensuring that the stakeholders have a shared forum in which they can openly and freely express their opinions and suggestions it is also a purposeful learning process for action that engages the implicit and explicit knowledge of stakeholders to create formalized and shared representations of reality using a range of modeling tools voinov et al 2018 as a school of thought this method emphasizes transparency and the necessity of engaging those impacted by the decisions barnaud and van paassen 2013 it also helps to overcome power imbalance especially when there are prevalent imbricated layers of power amid citizens organizations and government pm organizes stakeholder engagement around the process of building models of the systems of interest models are used as abstractions of real life systems and the mechanisms involved pm connects modeling science with stakeholder knowledge and experiences to produce an understanding of the situation shared by the group models offer a unique opportunity to study human reasoning and elucidate the way in which one thinks and makes decisions regarding the system at stake voinov et al 2016 shared models can greatly improve the power dynamics by offering several complimentary entry areas for logical and cognitive contributions building explicit shared models using the pm approach can greatly increase the applicability and robustness of the results pm practitioners usually organize a series of workshops for the stakeholders to share their ideas and preferences and solicit knowledge from them about the system and about the decisions that are to be chosen these workshops are normally restricted in time and can accommodate only a limited number of participants it would be hard to conduct a constructive discussion with too many stakeholders invited and to get all of them join a workshop at a particular time in one place currently according to well consolidated trends in communications many discussions take place over the internet as part of social and multimedia environments social web people go online to submit their ideas and opinions about various issues this can happens on social platforms such as twitter or facebook lengthy exchanges of comments take place in blogs where posts attract many people sharing their feelings and information this leads us to the idea of moving a part or perhaps even the whole workshop process in pm to the internet offering stakeholders an easy and asynchronous way to learn from other stakeholders and express their opinions about the problem at stake moreover if the discussion is all digitally recorded it becomes available for machine processing when additional information can be extracted from the comments available and fed back into the discussion to enrich it one could refer to the existing online platforms to mimic some parts of the discussions that normally occur during workshops however they offer only limited access to the information collected in the comments making it hardly possible to process the comments and derive any knowledge from them moreover existing platforms do not assume any place or role for facilitators or moderators of the discussion who could guide the discussion or provide users with instant feedback while models are powerful communication and learning tools none of the available platforms can translate discussions into more formal models qualitative or quantitative leaving the users alone in the process of deep understanding these considerations convinced us that instead of relying on existing platforms we should develop a specialized online tool that can be deployed in any pm process this tool discussoo provides a platform for stakeholders to share their opinions by submitting comments while the recorded texts are immediately processed and information is extracted from available comments in an attempt to generate on the fly conceptual models of the systems discussed which can be immediately fed back into the discussion for stakeholder checking and confirmation having direct access to all the texts submitted as comments we can use artificial intelligence ai techniques to derive patterns from the data available even when the sheer volume and heterogeneity of information available are huge information extraction ie is a technique to identify and analyze relevant and useful structured data from a large collection of unstructured data that is being generated by people and machines the extracted structured data is ready to be used for deriving information and conducting further analysis adnan and akbar 2019 natural language processing nlp is one of the main ai methods that can be used for ie and has been widely applied in different contexts to examine conventional texts we rely on this method to analyze the text in comments when running a discussion in discussoo it helps us to extract concepts keywords relationships and interactions which can then further lead to some more formal models of the topic in the discussion however as with all ai methods they require training and learning to excel in performance it might take some time for us to make sure that the methods we use always produce meaningful and useful conceptual models automatically at this stage we also allow the moderator to play a role in checking and improving the conceptual model in terms of research value discussoo aims to generate and properly visualize mental models that underlie a given discussion in the specific context of this work we are not referring to a formal definition of mental model we rather assume a mental model to be a structured machine processable representation of a given conversation such a representation is expected to summarize the key knowledge that is collaboratively produced by discussion participants discussion mining has been already applied in different contexts and most common techniques are relatively consolidated nagao et al 2004 for instance in massive open online courses moocs data in certain online spaces including also discussion forums is often mined pillutla et al 2020 to achieve such different goals as answer prioritization almatrafi et al 2018 identification of at risk learners he et al 2015 success prediction kennedy et al 2015 klusener and fortenbacher 2015 fostering smart learning liu et al 2016 interaction and knowledge building analysis lucas et al 2014 and discussion classification rossi and gnawali 2014 wise et al 2017 focusing more specifically on mental models their understanding reconstruction and representation vary very much from case to case however although a generic solution is probably unrealistic a number of consolidated mining techniques can ideally support the generic conceptualization of mental models in many applications and domains for instance mental models may be associated with concept maps eventually enriched by additional semantics indeed concept map mining has been formally proposed in 2011 villalon and calvo 2008 assuming an automatic process of unstructured data such a contextualization has been applied to generate concept maps from news tseng et al 2010 and more generic documents zubrinic et al 2012 more sophisticated approaches may be defined by assuming probabilistic solutions e g using fuzzy logic pillutla and giababnelli 2019 as far as the authors know there is not much technological support specifically designed for pm which normally relies on generic tools for collaboration in person while still relatively generic technology mediated solutions start to appear e g reddy et al 2019 we believe that the tool proposed in this paper may specifically address pm s needs yet preserving a fundamental genericity that should also allow other applications discussoo is a generic solution that aims to enable effective scalable discussions among stakeholders indeed the moderation functionalities and the ai powered features both with smart management of different discussion threads create a digital environment that encourages focused discussions and eventually decision making the tool has not been designed to address specific topics or to work exclusively in a given application domain it is rather understood as a generic asset however from the experience matured so far we expect the adoption of the tool in fields characterized by a variety of stakeholders such as sustainability and policy making last but not least digital platforms become even more relevant in situations of crisis emergency or disruption the covid 19 pandemic we are currently experiencing is a perfect example in more general terms we believe that accessing a digital platform is more affordable than any in person activity which may have other advantages though discussoo is currently available for free as a web platform from a user perspective there are no significant technological restrictions or barriers to access the platform except access to a last generation web browser limitations or barriers determined by extreme conditions such as limited internet connectivity or lack of knowledge to use online tools are unfortunately not under our control we note that online participation is in principle more affordable and open than attendance at face to face meetings so we believe our tool promotes fosters and facilitates the inclusion of minorities and local communities and not their exclusion the remainder of this paper is organized as follows section 2 introduces discussoo concept its components and modules we explain the need for such a web based platform and show how it can be used to systematically manage the pm process we then briefly describe some of the advanced artificial intelligence ai algorithms and technologies that can automatically build conceptual models with multiple stakeholders independent of their numbers geographical locations section 3 elaborates the details of using discusso in a real case study section 4 highlights the associations between discussoo and pm and demonstrates how the evolving group mental model generated in real time from the ongoing discussion and presented to the participants can facilitate online workshops the last section discusses the implications of the proposed tool and future directions 2 discussoo an online intelligent tool for pm while involving multiple stakeholders in a discussion provides an opportunity for groups and organizations to improve the quality of their decisions they inevitably face a new set of challenges with regards to engaging people and managing conversations there are many online and offline forums platforms tools available for involving the community in serious discussion however they do not offer much assistance to users in extracting insights from ongoing discussions or learning from them from the experience of the pm we know that systems modeling can be a useful tool to organize the discussion process and use it for improving decision making we develop discussoo to use online discussions for conducting the first step of the modeling process the generation of a conceptual model of the system at stake in the following sections we introduce discussoo and explain how it can empower the pm process 2 1 interface discussoo is an online web based platform for serious discussions largely inspired by the need to provide online support for the pm it deploys advanced ai methods and nlp techniques to automatically derive group mental models from the ongoing discussions and interactively visualize them as cognitive maps to leverage group communication and decision making on the landing page we have a list of ongoing discussions around different topics and a user can join any topics of interest or start a new discussion if needed there are also hidden discussions which have restricted access and only those invited users can join by clicking on the invitation link by clicking on the join discussion button the user opens the discussion page which contains the background information and the goals of a particular discussion as shown in fig 1 on the right hand side we present references to other relevant on going discussions include links to facebook linkedin twitter if the same posts are also shared there thread posts and discussion are the terms used interchangeably to describe the details of the topic chosen users can get involved in the discussions by pressing the join button each discussion post opens a new page where users can contribute to the discussion by leaving comments in order to make the discussions more effective and meaningful users can be assigned different roles as listed below 1 admin admin has control over the whole platform and can access the administration panel to approve new discussions create new users and new groups or assign users to groups admin can also assign remove group moderators while updating editing the group information 2 moderator all discussions can be facilitated by a moderator controlling the flow of comments exchange and can play a role in avoiding escalation of conflict between participants moderators can create new users form groups and manage individual discussions by editing or deleting the posts and comments using this content control mechanism moderators can protect the integrity of the topics and discussions in doing so they directly deal with unlawful or illegal comments such as defamatory statements verbal bullying and copyright infringement beside facilitation of the engagement moderators can encourage lively debates and stimulate the process by making it more enriching for all the participants they answer the questions that may arise in the course of the discussion in the form of comments 3 group moderator in discussoo multiple discussions can be conducted at the same time with different groups of participants for managing these discussions we have introduced the feature of groups and each group will have one or more moderators to facilitate discussions within that group group moderator will have all the controls such as editing deleting users posts and comments within a group the key difference between this role and the previously described one is the extent scope as the moderation activity of a group moderator offers access to discussions only within a group of participants it primarily responds to practical needs in presence of multiple parallel discussions which normally require a de centralized moderation model additionally it allows dynamic delegation of responsibilities as the function of discussion directions and participants involved 4 normal user anyone will be able to access the home page but in order to perform any activity they have to register and sign in if users do not register they can still participate in the discussion anonymously as a guest if the discussion is not a closed one for this purpose a system generated user name will be assigned to them and later on they can register anytime with their own email address users can both start a new thread for discussion and contribute to the ongoing discussions either by adding new comments or by responding to the already existing comments 5 registered users can start new discussions by posting a question and giving some background and context information there are multiple options for selecting the audience for each discussion discussions can be made public or private public discussions will be visible to all the users while private discussions will not be listed on the website and will be shared only with those who have the link to the discussion like in google docs alternatively participants can be invited to join the discussion by providing their email addresses in a csv file in this case anyone who is invited and has the link to the discussion should be able to access it discussions can be shared in different social media platforms facebook twitter and linkedin or can be sent to an email address in order to get a wider audience engaged in a discussion there is a feature of sharing posts discussions in discussoo to different social media platforms so that new comments can be collected from there to enrich the analysis the analyzed results can be fed back into the discussion and reference can be provided within the tool to allow users to understand the context each post has a section for likes comments share save hide report users can rate other comments to express their agreement or disagreement instead of a binary like dislike grading we use a rating from 0 to 9 to allow a more nuanced expression of feelings at any time the average rating of a comment is calculated and presented here overall discussoo interface offers a real time moderated web environment to engage participants in a virtual discussion of a problem users are presented with questions about different topics and problems to discuss on which they can express their opinions thoughts in the form of comments users can then respond to or expand on a comment based on other user s opinions and or initiate a new line of discussion for pm we see this as an addition or replacement for in person workshops which are essential for that process 2 2 engine in addition to tracking the exchange of comments discussoo has a back end which uses ai techniques to extract new knowledge from the on going discussions and improve understanding of problems by generating mental models from the information provided by users fig 2 while the discussion continues the recorded texts are mined in real time using an ensemble of algorithms including but not limited to concept mining topic modelling and sentiment analysis from the comments we extract a set of keywords concepts and the relationships among them the information extracted is presented in the form of cognitive maps causal loop diagrams or networks diagrams which we intend to further translate into semi quantitative models such as fuzzy cognitive maps through visualizing a dynamic collective mental model the tool provides instantaneous feedback to its users tells them how the discussion is evolving which may potentially influence their contextual information to refine and update their individual mental models feedback can also be introduced by targeted moderator comments to steer or nudge the discussion towards a more meaningful collective mental model potentially leading to improved outcome consensus or agreement we believe that this functionality can enrich the pm process and in many cases could replace the rather limited workshop schedule by a much more open and inclusive online format the moderator can play a role in moving around or grouping comments to focus on particular issues in the discussion additionally the moderator can also directly impact the conceptual model generated by removing concepts and interactions indeed as the conceptualization is automatically generated by applying generic npl techniques the outcome may include inaccuracies that potentially introduce noise to the model produced other users do not have direct access to the diagram but can ask the moderator to make changes to generate conceptual models from unstructured data e g comments sentences we use an ensemble of different nlp techniques including named entity recognition ner lample et al 2016 sentiment analysis rajput 2020 text summarization nenkova and mckeown 2012 aspect based mining zhang and liu 2014 and topic modelling yau et al 2014 one of the most fundamental tasks in nlp is extracting the entities from the content using ie we use this to find important concepts and references in the comments provided ner is a sub task of ie to distinguish named entities that appear in the unstructured text and to put them into predefined categories for example names of individuals locations companies dates etc adnan and akbar 2019 this algorithm uses the grammar rules of a language and supervised learning models there are also built in and trained ner models available such as open nlp shelar et al 2020 in discussoo we use this to identify and segment the named entities and classify categorize them under various predefined classes sentiment analysis is another broadly utilized method in nlp sentiment analysis is often applied to analyze customer surveys and reviews on social media where people post their feedback about products or services the simplest form of output in sentiment analysis is a 3 point scale such as positive negative neutral in complex cases the outcome can be a fuzzy numeric scale a range from 0 to 10 where 0 is defined either as positive or negative in the spectrum based on which a desired number of sentiment groups is defined this method has applications in both supervised where labels are required with training dataset and unsupervised where labels are not required with training dataset learning on the one hand na√Øve bayes along with random forest smo and j48 are examples of sentiment analysis in supervised learning settings ahmad et al 2017 on the other hand unsupervised methods determine the polarities of the words and calculate the sentiment scores in a sentence in discussoo sentiment analysis is used to first extract opinions from discussions and then scale each of them as positive negative or neutral this helps us in tracking any changes in users opinions and understanding whether learning occurs throughout the discussion text summarization is a vital nlp task with many potential applications in extraction and abstraction extraction is mainly related to i extracting key aspects from textual content and creating an intermediate representation ii assigning scores to sentences in the text in that representation iii and forming a summary by choosing multiple sentences nenkova and mckeown 2012 extractive methods select a subset from a corpus of words sentences or documents in the source content to shape a summary the main tasks of the extraction based summarization are the identification of the key phrases and using them to select sentences in the text document for incorporation in the summary nenkova and mckeown 2012 abstractive methods however first form an internal semantic representation and afterwards utilize natural language generation methods to deliver a summary such a summary may contain words that are not expressive in the original text in contrast to the extraction abstraction based techniques paraphrase segments of the original document gudivada and rao 2018 the fundamental task of aspect based or feature based mining is to extract and summarize individual opinions about entities and their aspects hu and liu 2004 it performs three main sub tasks i classifying and mining entities in analyzed texts ii classifying and extracting aspects of the entities iii discovering sentiment polarities on entities and aspects of entities for instance in the following sentence i have read another paper about climate change today and am really worried about the temperature anomaly in greenland and the fast decline of the ice sheet there the aspect based opinion mining method should recognize that the author has a negative opinion about climate change the temperature anomaly in greenland is an aspect and climate change is the entity in discussoo if the user comments are very long paragraphs then text summarization algorithm is used for condensing the comments and reducing the size of the initial text while preserving key informational elements and the meaning of content moreover when we utilize it in combination with sentiment analysis we can mine emotions from the text sentiments can only tell us the polarity of the sentence whether it is positive negative or neutral whereas emotions are about the feelings and senses in a sentence such as happiness love fear anger or hate hence emotions are the drivers of sentiments and determine their polarities topic modeling is a category of machine learning methods for extracting the latent topics that appear in documents yau et al 2014 one of the main advantages of this technique for our case is its independence from the labelled dataset enabling us to find topics in an unstructured dataset correlated topic model ctm latent dirichlet allocation lda wei and croft 2006 probabilistic latent semantic analysis plsa and latent semantic analysis lsa are four well known topic modeling algorithms discussoo deployed lda for the text analysis the reason for this selection is that every textual content involves a several topics and each topic comprises many words the information required by lda is just the textual content and the anticipated number of topics making it suitable for comment analysis in our tool in extracting relationships discussoo encompasses two different techniques namely textrunner and formal concept analysis fca valverde albacete et al 2016 textrunner algorithm enables the machine to learn the relationships entities and classes from a text it has three key components including i self supervised learner to classify the input data into trustworthy or not ii single pass extractor to extract all the possible relations among the identified entities and iii redundancy based assessor to determine the optimal number of relations besides fca explains the relationships between a specific set of objects and their properties it has is widely used as a knowledge representation structure particularly in knowledge engineering and ontology generation problems in technology as a result discussoo is expected to produce a group mental model using a combination of all the described algorithms 3 case study as mentioned above the testing of algorithms implemented in discussoo for analyzing discussions is still quite limited however a number of experiments have been conducted within different application domains and additional ones are expected to be conducted in the near future as with many machine learning approaches the more data is collected the better we can train the algorithms and improve their performance the feedback obtained and the experience matured in real world scenarios are providing key insights and directions to holistically improve the quality of experience of discussoo users this agile methodology allows a smooth harmonization between research and development and enables a natural evolution of the tool in response to concrete users needs for example in september 2020 an educational research based organization named institute of public policy and governance adopted the tool in their engagement activities involving teachers and lecturers their discussions were to analyze recent changes in teaching and learning strategies during the covid period they invited teachers from different institutes to share their opinions and thoughts about various topics related to online classes student performance strategies for virtual management and so on 81 teachers have shared 139 comments to express their point of view and experiences they have created 11 groups related to challenges strategies needs leadership innovations etc within these groups they opened 24 discussions as shown in fig 3 we select a discussion titled what challenges have been new or heightened because of this period for further analysis because of two reasons firstly this discussion has the highest number of users and comments and secondly as the topic is not organization specific and generic it is more understandable for participants analyzing these comments discussoo managed to extract the key concepts calculated the frequency of their appearance in the discussion and determined the sentiments of the opinions table 1 these key phrases and sentiments are extracted using amazon comprehend as a preliminary result next they are further refined using a combination of algorithms to extract concepts for creating conceptual models and sentiments to identify any changes in the opinions there are numerous ways that concept maps can be visualized giabanelli et al 2016 for example in fig 4 we present a conceptual model that was extracted by combining word2vect algorithm and word embedding technique for creating a training model this model helped us to get a relation score between concepts it also performed ner described in section 2 2 to extract top concepts related to location person organization event area etc for the extracted concepts it identified all closest child concepts with a score and visualized them using the network graph algorithm these are the results of our initial attempts to analyze the comments 4 discussion addressing pm challenges with discussoo the development of a model with stakeholders is a multi stage process that conceptually captures the following processes 1 scoping and abstraction 2 envisioning and goal setting 3 data collection and preliminary analysis 4 model development and 5 presentation of the modeling results to the audience in most of these stages we can find a role for discussoo to play fig 5 scoping and abstraction the initial phase of pm focuses on framing problems and defining relevant questions to achieve integration cooperation and solutions it is important to gain a joint understanding of a problem models that come out of stakeholder engagement workshops help to represent the discussed topic in a formalized and structured way it is important to set the conversational boundaries appropriately to avoid model complication and allow for comprehension and reflection falconi and palmer 2017 narrowing down the scope of the problem is highly critical especially in the first engagement with stakeholders when comprehensive instructions of the pm workshop is given to the participant to guide them through the process in many cases the scoping process is organized in the form of facilitated open discussion it can also be supported by analyzing the development of the problem boundaries during certain periods of time e g looking at the graphs with data over time for several parameters that are directly connected to the discussed problem discussoo platform accommodates the scoping process in two different ways firstly users can initiate a preliminary discussion on the platform to clarify the boundaries of a problem secondly they have the option of using this tool to discuss a pre agreed problem statement as they may already be defined in separate consultations with stakeholders there is certainly a role for the moderator to properly frame the questions and lead the discussion in the desired direction to a certain extent the self selection of the participants and their perspectives on the matter can also automatically determine the boundaries of the problem in contrast to the offline pm which is limited by the time and space factors to run a workshop the online discussion platform provides an opportunity for the pm organizers to engage an unlimited number of stakeholders regardless of their geographic location time constraints and role in the decision making process the discussions can be widely open to all users or they can be restricted to a certain nominated group of participants envisioning and goal setting during the process of envisioning and goal setting the group of participants decides on the structure of engagement clarifies the questions for steering the process and ensures their major interests and ideas are represented and discussed belt 2004 emphasizes that an envisioning exercise is crucial for creating positive attitudes in participants at the initial stages of pm the goal is to design a model to provide a bigger picture of the desired state of the situation as opposed to the current state that is perceived as problematic in discussoo this is the next step of the ongoing discussion where participants will share their points of view about the problem by leaving comments they can also see the views of others on the same issue react to their opinions by replying to their comments or scoring them as such the interactions communications and learning by others can occur throughout the whole process the posts and comments and the scores they get as well as their sequence and logical connectivity provide the key concepts and relationships data facts logic data availability is a crucial component for both qualitative and quantitative model development pm as a method follows post normal approach and implies the use of data not only from secondary sources such as historical data articles reports and white papers but also primary sources such as information extracted from direct interaction with stakeholders in forms of interviews surveys discussion groups funtowicz and ravetz 1994 voinov and bousquet 2010 the stakeholders or problem owners are treated as one of the main contributors to the knowledge about the problem because they have direct exposure to the discussed problem videira et al 2016 in discussoo platform we have the advantage of directly including links urls to relevant data and information already available elsewhere in future implementations this information can be automatically supplemented by web crawlers that will be mining the web and further enriching the discussion by relevant references as well as fact checking the posted discussions themselves as well as comments responses to other comments scores for other comments are an additional source of data the text and numbers retrieved from the user s interface are cleaned structured and stored in a master database in real time to feed the algorithms again as the discussion evolves there are important roles for the moderators to take on firstly they can take precautionary measures and ensure that the exchange does not touch upon any sensitive unpleasant context for any of the participants secondly with the help of facilitation techniques they can guide the discussions among the participants towards more constructive modes of exchange e g asking questions improving the logic of the conversation etc thirdly we intend to introduce some gamified interventions in an attempt to increase cooperation among participants in the group bakhanova et al 2020 having said that many of these approaches and issues are yet to be tested modeling group model building is the process of representing the perceptions values and opinions of stakeholders about the discussed problem in a structured and logically consistent way this stage is the core part of pm because through the model development practices the participants may learn about the problem and modify their perceptions by knowledge integration voinov and bousquet 2010 so far we mainly focus on the ideas of vennix 1996 to use nominal group technique when developing a qualitative model with a group of participants this technique refers to the gradual elicitation of the knowledge from the participants that use a combination of brainstorming and facilitation principles to initially identify the elements of a problem and the connections between them before developing the overall framework of the model currently discussoo can help in designing only conceptual models of systems considered in some cases we can also elicit some semi quantitative metrics for weights of influences when analyzing how frequently various terms and concepts are mentioned and how they are scored in the discussion instead of involving a modeler in the process the centralized ai component of discussoo can automatically develop a model by capturing the opinions of users in the form of concepts topics and sentiments attached to those opinions including the relationships between them these become the building blocks of dynamic conceptual models such as word clouds cognitive maps causal loop diagrams etc the automation of the model development stage brings some benefits in terms of debiasing the process ideally modelers are expected to be completely neutral and objective when putting models together but in reality their interest and association with the topic make them also biased and subjective normally it is expected that this subjectivity is identified and managed in particular by communicating to the stakeholders in a pm process that scientists also hold values and beliefs which can lead to better understanding and trust voinov et al 2014 discussoo could provide additional support as the generated conceptual models are not the result of some deliberate choices made by selected participant s modelers but are instead obtained by directly applying standard analytical techniques to the conversation all critical assets composing the target model are derived from the analysis of the conversation in terms of text and structure the former aims to infer key concepts while the latter targets the definition of the relationships among them the conceptual models become the results of a quantitative analysis of the occurrences of the different keywords and the interactions among them it is yet to be seen whether the algorithms used will become a source of additional bias on their own but the human intervention in the actual model building process becomes somewhat restricted presentation and evaluation one of the main objectives of conducting pm is to foster shared learning about the problem by providing feedback to the participants jordan et al 2018 in doing so the final stage of the process provides the stakeholders with visual presentations that could help them to disseminate the knowledge conclusions learnt during the modeling process to a wider audience belt 2004 hence diverse forms of model visualizations and interfaces are used to facilitate model communication the management flight simulator is one of the well known examples maier and gr√∂√üler 2000 they give non modelers an opportunity to interact with the model and see the response of the system to the changes in the model parameters the interactive modeling approaches also proved to be effective for learning about complex problems bakhanova et al 2020 elsawah et al 2017 in discussoo we are not developing any quantitative models but the qualitative mental models are visualized on the fly and immediately fed back into the discussion informing the participants and helping them to produce new and creative solutions to the complex and controversial societal issues the users can suggest new elements or change the connections between the existing ones by expressing their opinions in the comments in offline traditional pm the participants can modify the model in direct interactions with the modeler providing their feedback and asking for further explanation and discussions around the concepts and relations presented in the model in discussoo users do not modify the model directly instead they indirectly change the model by providing additional comments and evaluating existing ones all interactions are considered and all of them are counted in the model however due to the quantitative measurements e g number of occurrences of a given keyword not all contributions lead to a significant change in the conceptual model it is worth mentioning that the application of discussoo goes beyond pm as it can help in any engagement where serious discussions are required this tool can be deployed by governmental agencies e g councils states and federal agencies to seek their stakeholders feedback in particular from citizens discussing policy proposals and planning or management options organizations and businesses in the private sector e g customer and employee engagement customer satisfaction or sustainability reporting who intend to find optimal ways for managing shared geographically dispersed resources such as agricultural lands mines or freshwater supplies professionals in various levels e g research centers universities consultancies in their engagement with clients to devise or advise on public policies teachers for on line teaching to supplement or replace class discussions for example recently discussoo has been used to analyze the comments posted in the chat during the national bushfire climate summit 1 1 https www youtube com watch v tkxhwcqju5w feature youtu be whereas part of the emergency leaders for climate action summit 2020 a panel of four experts were presenting their opinions about the recent 2019 bushfire outbreak in australia at the same time some several dozen viewers were actively exchanging opinions in the chat box the text generated from their comments was stored in the master database and used to feed a set of text mining algorithms in the engine we argue that this analysis can help organizers to better understand the reaction of the public to the challenge of bushfires in australia in fig 6 the main keywords expressed by users have been visualized using discussoo keywords such as climate change indigenous fire burn animal wildlife are highly relevant to the context of a discussion regarding bushfires however irrelevant keywords such as thank and people appear in the results here we can argue that firstly these comments are not extracted from a moderated discussion and secondly as we are dealing with unstructured data a lot of extra filtration or data cleaning is required in order to get good results we also observe a growing interest in utilizing the tool in the post covid outbreak life when there is more reliance on distant communications and fewer opportunities for face to face meetings overall we see discussoo as a unified platform for engaging researchers stakeholders and decision makers in a participatory process for better decision making with no restrictions neither for the size of the group nor for the timing and location of their meetings 5 conclusions our research aimed at designing an online tool to enhance or replace the in person workshop process that is a key element of any pm effort discussoo is an intelligent web tool that provides an online pm platform to enable large groups of people to engage in a discussion and deliver a real time conceptual model by automatically eliciting analyzing and presenting group mental models we have designed and developed a prototype that can run serious discussions and analyze the collected information we explored how it can help to solve some challenges associated with pm in particular the challenges related to knowledge integration and synthesis of participants ideas public engagement scaling overcoming biases during the pm process and improving learning among stakeholders governments academics independent media and citizens can use the tool for serious discussions while there are quite a few online platforms table 2 with somewhat similar purposes none of them offers an opportunity to learn from the ongoing discussion and improve the mental models that it can potentially reveal the promise of discussoo is two fold instead of requiring stakeholders to be at a certain location for a certain time to attend a workshop and exchange knowledge about the system they are offered an asynchronous remote mode of participation when they can attend at any time that is convenient for them and from any location office home beach anywhere from where they can get on line and join the discussion instead of conducting the discussion and soliciting ideas opinions and data first and then moving to assemble this information in a form of some conceptual model here the model is generated on the fly by ai algorithms running in the background and processing the comments as they are posted the conceptual model is immediately fed back into the discussion enriching it and helping stakeholders to focus on the most important and perhaps controversial parts of the system that is analyzed discussoo allows anonymity to users to write on the web behind the curtain enabling them to communicate their views and feelings unreservedly in fact the collaborative process of sharing one another s mental models can 1 enhance the awareness of stakeholders users of their own internal assumptions and values and how these relate to others 2 help stakeholders users to identify their commonalities 3 facilitate the emergence of a common vision for action based on the co construction of a shared mental model that enables an empowered and joint commitment to achieve conservation outcomes and 4 increase social and community participation in decision making especially underrepresented and isolated groups there are other benefits that come with the system such as compensating for bias and credibility of the pm results by minimizing the moderated involvement in the process and generating machine driven outputs we are only starting to experiment with discussoo and there is yet much to learn from how the tool works and certainly many further improvements will be needed in fig 7 we present some of the challenges and opportunities for discussoo in the future they largely fall into two categories technical and ethical on the technical side there is still much improvement needed for the ai algorithms used to elicit and present the mental models involved we expect them to improve as more usage data will be accumulated and better machine learning algorithms will be developed and applied the analysis of large volumes of data can bring new challenges with regards to the needs for more computing power massive ai data pipelines and memory and higher energy usage eventually we can learn much about how we think individually and in a group and how group dynamics can affect our personal mental models lavin et al 2018 this immediately raises multiple ethical issues that will need to be sorted out a number of effective security mechanisms should be devised in order to protect the mental models of users from malicious use understanding of individual group mental models in a discussion forum can improve knowledge and help to accommodate a plurality of values perceptions and beliefs among stakeholders thereby increasing the chance of implementation success however it is yet to be researched how exactly this can be done in the most productive way we invite interested readers to utilize discussoo when planning for their online pm workshops to reflect both on the positive and negative aspects of this tool suggestions for inclusion of new features and capabilities are also welcome our tool is user friendly free of charge does not require technical knowledge and is ready to be used by anyone in any stakeholder engagement and pm project these applications and discussions can greatly help us to identify ways for future improvement and provide new sets of training data for algorithms to present more useful and accurate results discussoo is expected to evolve in the future as core functionalities will be extended and further refined to increase effectiveness and flexibility in a variety of contexts and applications we also aim to address practical needs such as among others the support of languages other than english declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is a part of ph d research supported by funding from the faculty of engineering and information technology university of technology sydney authors wish to thank the editor and five anonymous reviewers for their valuable comments and suggestions on this manuscript 
25843,relying on solar energy alone to power water monitoring stations limits the ability to monitor water resources in low light locations such as for streams with dense tree canopy cover to address this limitation water monitoring stations could have a supplementary micro hydro turbine to harvest kinetic energy from streamflow to explore this possibility we simulated the energy harvesting potential at 42 locations with long term stream velocity records and determined the canopy cover for streams across the contiguous united states conus results show that a site with typical streamflow velocity and dense evergreen tree canopy would have 1 6 times less sample loss from 68 to 43 after adding hydro energy harvesters many streams across the conus with dense canopy cover could benefit from this especially headwater streams and streams in the southeastern us where many watersheds have more than 50 of their stream length under dense tree canopy cover keywords water monitoring self powered water sensing ubiquitous water sensing smart water systems 1 introduction water resources have been monitored with networks of in situ sensing systems for studying many hydrological and environmental problems such as flooding runoff pollution and aquatic ecosystem degradation for decades bartos et al 2018 rode et al 2016 ruhala and zarnetske 2017 wymore et al 2018 in the united states by the time of writing of this paper the united states geological survey usgs has collected and made publicly available surface water historical instantaneous data water quantity and or quality for 14 481 different active and inactive or discontinued stream locations across the nation while this may seem like a large number of monitoring sites considering the nationwide rivers inventory nri estimated a total of 5 200 000 km of streams in the contiguous united states conus benke 1990 it means that there is only one usgs water sensing station for approximately every 360 km of stream length this density of in stream measurement is insufficient for understanding many properties and functions occurring within waterbodies horsburgh et al 2010 kirchner et al 2004 while the limited spatial density of current water sensing networks is certainly a function of the high installation and maintenance cost of each individual stream gauging station another limiting factor is the availability of energy to power the sensing equipment given that many sensing locations are off grid energy harvesting the process of deriving energy from the surrounding environment and converting it to a usable form e g electrical energy must be used to power the sensing system and recharge batteries at the sensing station a variety of energy harvesting approaches from different renewable energy sources e g vibration wind and solar energy harvesting for wireless sensor networks have been studied in past research studies see olatinwo and joubert 2019 for a review among such approaches the most common energy harvesting approach for water monitoring is solar power chen et al 2017 due to its relatively low cost robustness and high power output ratings shaikh and zeadally 2016 prior studies have shown how solar energy harvesting can be exploited for sensing environmental variables and water related characteristics bartos et al 2018 jones et al 2017 kapetanovic et al 2017 quinn et al 2010 additionally tools such as pvlib python an open source community supported tool holmgren et al 2018 enable estimating solar power generation prior to actual deployment of a given solar panel however solar energy harvesting is not always a viable option due to low light conditions e g forested areas with dense tree canopies or high wall canyons consistent cloudy weather azevedo and lopes 2016 jeong and culler 2012 taneja et al 2008 or within urban environments e g within subsurface drainage infrastructure this motivates the need to investigate other energy harvesting sources for such locations for water monitoring one obvious energy harvesting source is the kinetic energy of water flow as a potential supplemental energy source to solar energy for low light conditions while prior research has widely explored the potential for larger scale hydropower harvesting applications such as run of river hydropower plants e g yildiz and vrugt 2019 fewer studies have investigated the smaller scale micro hydropower harvesting potential for monitoring applications in natural streams or stormwater infrastructure systems there has been significant work however exploring micro hydropower harvesting for drinking water pipe networks that show this technology s potential for other water resource and environmental applications hoffmann et al 2013 r√∂del et al 2016 ye and soga 2012 there has also been recent laboratory work exploring micro hydro turbine power harvesting that can inform field application studies two such studies evaluated the effect of different energy harvesting mechanisms e g hydro turbine harvesting power from flow velocity and different design choices e g hydro turbine geometries on the efficiency of the systems azevedo and lopes 2016 kamenar et al 2016 these systems were able to generate hydro power up to hundreds of milliwatts based on both the energy harvester design choices and the characteristics of energy harvesting environment e g flow velocity for hydro turbine which could be enough to power a water sensing system in the natural environment with real time data communication any energy harvesting sources e g solar or hydro can be temporary and spatially limited or even periodically unavailable leading to intermittent power outages and sensing gaps without sufficient energy storage infrastructure e g batteries therefore a combined use of energy harvesting sources when possible is expected to prevent or reduce the time periods when the sensing system is out of power reducing the need for sophisticated energy storage infrastructure for example morais et al 2008 showed how using a small solar panel combined with a small hydro generator placed in a nearby irrigation or hydroponic water pipe and a wind generator was able to supply a generic wireless data acquisition platform energy store in agricultural and livestock environments even with low solar radiation this prior research provides important understanding and software needed to enable energy harvesting from different energy sources i e solar hydro and wind for sensing the natural environments and highlights the importance of a combined energy harvesting from various energy sources when individual sources are limited however reviewing prior research reveals that hydro energy harvesting to power in stream water monitoring stations for locations with limited access to solar energy i e riparian forest zones where sunlight penetrating the tree canopy and reaching the solar panel underneath it can be highly limited is not explored yet this comparison is necessary to better understand if hydro energy harvesting is a viable option for achieving high coverage and dense spatial sensing of waterways in particular for those locations with limited availability of solar power furthermore knowing what fraction of streams in the conus are candidates for hydro energy harvesting due to dense tree canopy limiting solar energy harvesting will aid in understanding the potential impact of advancing hydro energy harvesting to monitor more locations in the stream network toward a long term vision of ubiquitous monitoring of the nation s waterbodies given these knowledge gaps the first goal of this study is to compare and simulate energy harvesting in multiple river locations using a solar panel versus a hydro turbine in order to investigate their complementary relationship in self powering water monitoring stations under different light conditions due to the presence of tree canopy covering waterways the second goal is to determine the stream length in the conus with dense tree canopy to estimate the potential impact of hydro energy harvesting section 2 discusses the materials and methods used to achieve both of these goals including the datasets used to simulate the solar and hydro energy harvesting sensor stations specifications and energy harvesting simulations section 3 presents the results of the analysis including a discussion of the findings and their limitations finally we summarize and offer conclusions from the study in section 4 2 materials and methods in this section we first describe the datasets used to estimate harvestable energy from both hydro and power sources second we present the self powered sensor station specifications and its key components to define a typical energy draw and battery specifications for such stations third we explain the energy generation store and consumption model adopted for this small scale hydro solar sensing station simulation fourth we describe the different experimental setups used to address our primary research goal stated earlier fifth we explain the performed geospatial analysis of tree canopy cover for streams in the conus to address our secondary research goal formerly mentioned finally we discuss the implications of modelling limitations and estimation errors for our study fig 1 shows the diagrammatic representations of the components used to simulate the energy harvesting from hydro and solar energy sources to power a water monitoring station all the components present in this figure are described in sections 2 1 2 2 2 3 and 2 4 2 1 datasets 2 1 1 hydro energy harvesting dataset the usgs has recorded and shared streamflow velocity for about 400 locations out of the previously mentioned 14 481 active and inactive or discontinued stream locations across the united states these 400 locations are located on natural streams or man made channels u s geological survey 2016 velocity measurements are recorded using acoustic doppler velocity meters this data is often used to estimate the flow discharge in waterbodies levesque and oberg 2012 the available velocity time series for each usgs site is either a reading from the sensor directly identifiable using the usgs parameter code 72254 mean streamflow velocity over the cross sectional area of the waterbody identifiable using the usgs parameter code 72255 or both because the streamflow velocity time series is either measured at a point or represents the mean streamflow velocity at a cross section it does not capture the variation of streamflow velocity in depth width and along the river additionally there were a few negative values reported at many of the usgs sites which can happen due to backwater flow conditions that can occur at stream confluences streams flowing into lakes or reservoirs tide affected streams regulated stream flows dams or control structures strong prevailing winds or where structures e g bridges and culverts restrict flow the available streamflow velocity record duration varies across the 400 sites from about four months to about 12 years most often streamflow velocity measurements are recorded on a 15 min interval we chose to use a five year study period of 2010 2014 because it had the maximum number of sites with recordings during this period we identified 42 sites with a low missing data fraction during this period in total 2 1 of the dataset was missing due to data gaps the largest gap within the data was 23 47 days and the median gap across all sites was 1 1 h across all sites 80 of the data gaps were less than 4 2 h because the missing data made up a small fraction of the overall dataset and because the gaps tended to be small relative to the overall period of analysis we used a simple linear interpolation as a standard gap filling technique applied consistently across all data gaps the resulting gap filled streamflow velocity time series for the 42 sites were used to estimate the harvestable energy using a flow velocity to power transfer curve from a small hydro turbine which was provided by the manufacturer this flow velocity to power transfer curve is explained in section 2 2 1 2 1 2 solar energy harvesting dataset to estimate the harvestable solar energy historical satellite derived estimated weather data provided by clean power research s solaranywhere for the conus was used clean power research service database solaranywhere www document 2019 the dataset includes solar irradiance global horizontal irradiance direct normal irradiance and diffuse horizontal irradiance wind speed at a height of 10 m and ambient dry bulb temperature the spatial resolution of the data is 10 km and the data is available on an hourly time step the weather dataset was linearly interpolated for every minute to be consistent with the streamflow velocity dataset the interpolated dataset was used as input for a photovoltaic pv model to estimate the harvestable solar energy as explained in more detail later in section 2 2 1 2 2 sensor stations specifications the water monitoring sensor station simulation model has four main factors 1 energy from the external environment 2 one or more harvesters 3 an energy storage medium and 4 an electrical load from sensors and other electronic components the energy from the external environment e g water kinetic energy or solar radiation is collected and converted to a usable energy form i e electrical energy by the harvester to meet the power demand of the load the energy storage medium is necessary to store energy during periods when the harvested energy is greater than the consumed energy and then provide the stored energy during periods when the energy consumption by the unit exceeds the harvested energy the external environment was explored in the previous section here we provide specifications of the harvesters energy storage medium and the power consuming unit load that were used in the study 2 2 1 energy harvesters to estimate the harvestable hydro power a small commercial portable hydro turbine called waterlily was considered waterlily turbine www document 2020 one of the very few commercially available options of its kind depending on the stream hydrology and channel morphology customized hydro turbines to adopt with the conditions can be considered fig 2 depicts the waterlily hydro turbine streamflow velocity to power transfer curve provided by the manufacturer the minimum streamflow velocity value required to generate energy is 0 5 m s and the peak power of about 14 3 w is achieved at approximately 3 2 m s we assumed the hydro turbine could harvest energy from both positive and negative flows thus the negative velocities described earlier were treated the same as positive velocities throughout all the hydro simulations we used a parallel combination of two waterlily hydro turbines therefore at a given flow velocity the harvested power from fig 2 is doubled using two hydro turbines was necessary to provide enough energy for the water monitoring station to estimate the harvestable solar power we used pvlib python an open source community supported tool holmgren et al 2018 in this study we used pvlib python version 0 6 4 holmgren et al 2019 for all the harvesting locations the simulations assumed a 20 watt 12 v solar panel kyocera ks20 www document 2019 operating at the maximum power point the surface azimuth of the solar panel was set to 180 for all energy harvesting sites during the entire simulation period while the solar panel surface tilt was considered to be equal to the latitude of the energy harvesting location the weather dataset described in section 2 1 2 was used as input to the solar energy harvester system simulations 2 2 2 power consumption model the sensor station power consumption model assumed in this work includes three main modules controller sensor and communication the whole station was assumed to have two operating modes active where all modules are consuming energy and idle or sleep where all modules are inactive the sensor station alternates between active and idle modes acquiring sensor measurements and periodically transferring data over the cellular network in active mode while saving energy in between measurements by alternating to idle mode the station s average energy consumption is then used as input to the simulator for the purposes of the analysis performed in this paper the sensor station hardware platform from the open storm project bartos et al 2018 is used as a reference of a possible implementation of such system the following paragraphs provide additional details for each of the modules used in the model the controller module is a programmable device capable of interfacing and turning sensor and communication modules on and off for example the open storm project adopts a custom printed circuit board with a cypress cy8c5888lti lp097 programmable system on chip soc the controller module together with the lowest power consuming sensing modality was reported by the open storm project team to consume about 10 ma at 3 7v voltage source on active mode for the purposes of the analysis performed in this paper the controller power consumption is assumed to be 37 mw and it should be viewed as a target power consumption budget that the controller module must adhere to the idle or sleep mode was reported by the open storm project team to consume up to 60 Œºa at 3 7v and this worst case is considered on the sensor station model the sensor modules selected for this analysis and supported by the open storm project comprises of a collection of typical water sensors used in a sensor station table 1 the active power consumption for each module together with the recommended inline voltage isolator part number be ivi is provided by the manufacturer atlas scientific www document 2020 the open storm project team reported that it typically performs one measurement every 10 min and it takes 5 s to perform a measurement one additional parameter typically measured on water quality sensor stations is turbidity since atlas scientific currently does not commercialize turbidity sensors we considered the turbidity sensor manufactured by global water global water www document 2019 in this work the global water turbidity sensor wq730 consumes up to 60 ma at 12 v power supply resulting in a power consumption of 720 mw the wq730 sensor requires a recommended warm up time of 8 s before start measuring and for the purposes of this paper analysis we considered a 1 s window to actually acquire the measurement resulting on a total of 9 s of active time and consuming a total of 6 48 j per measurement the communication module is an electronic device capable of transmitting and receiving information through a cellphone network the open storm project bartos et al 2018 sensor node uses the module telit cc864 dual to perform the communication task and an average of 25 ma current consumption at 3 7 v voltage source was reported by the open storm project team each communication event was reported to happen every hour and take up to 60 s in this work the worst case of 60 s for a communication event total energy consumption of 5 55j per event is assumed while no energy is consumed while in idle or sleep mode it is also assumed that the controller remains active for the whole transmission event of 60 s consuming a total of 2 22j per transmission event each communication event is assumed to happen every 24 sampling events what would result in one communication event every 2 h for a 5 min sampling interval the average energy consumption per measurement spent on the communication task is therefore estimated to be 0 324j per measurement event to calculate the total energy consumption per measurement of the complete station using all sensors all five atlas scientific sensors and the global water turbidity sensor we add the energy consumption per measurement of all sensors 13 722j the energy consumption of the controller device during sensor measurements 0 333j for 9 s operation due to the turbidity sensor and the average energy consumption per measurement of the communications module 0 324j resulting in a total of approximately 14 38j the average energy consumption per simulation step e l o a d is calculated by equation 1 1 e l o a d s i m s t e p e a c t i v e s i s i m s t e p p i d l e 1 9 s i 60 t i where simstep is the simulation step in seconds si is the sampling interval in seconds e a c t i v e is the total energy consumption per measurement in j p i d l e is the sensor station power consumption during idle mode in watts assumed to be 222 Œºw and ti is the transmission interval in seconds the time between successive communication events for a simulation step of 1 min a sampling interval of 5 min and transition interval of 2 h e l o a d is calculated to be approximately 2 89 j per minute since it is usually more than enough to measure the water quality parameters at waterways at sub hourly intervals which successfully captures the dynamics of the change in parameters the default sampling interval for all the water quality sensors was fixed to 5 min unless another sampling interval is noted 2 2 3 energy storage unit and charge controller power storage is necessary to manage the energy harvesting fluctuations over the deployment period of the energy harvesting system to store energy during high energy harvesting periods and use the stored energy during energy harvesting droughts also on many occasions the instantaneous harvested power is not enough to directly power the electronics which again emphasizes the need for an energy storage medium the battery model used in this paper considers charging and discharging efficiencies as well as a constant energy leakage parameter as used in buchli et al 2014 the following parameters were considered for this analysis maximum capacity of 0 4 kwh constant leakage of 0 66 j per minute 2 self discharge in one month charging and discharging efficiencies of 0 9 and 0 7 respectively are assumed the battery is considered to be at full charge in the beginning of each simulation one example of a commercial battery with 0 4 kwh capacity is the victron energy 12v 34 ah battery victron energy www document 2020 a charge controller is considered to control battery s charging and discharging operation as assumed in the work done by buchli et al 2014 the charge controller protects the battery by disconnecting the load when stored energy reaches a minimum level complete depletion in our case and only reconnecting it after the battery is recharged to threshold percentage of the nominal capacity assumed as 30 in this work the charge controller efficiency is assumed to be 90 as used in buchli et al 2014 2 3 energy generation store and consumption model the simulation model used in this paper follows the work presented in buchli et al 2014 by considering a harvest store use energy model and calculating the battery state of charge after every simulation step equations 2 4 summarize the adopted model 2 e b a t o u t k e l o a d n b a t o u t e l e a k 3 e b a t i n k min n b a t i n n c c e h k max 0 n b a t o u t b n o m b k 1 e b a t o u t k 4 b k max 0 min n b a t o u t b n o m b k 1 e b a t i n k e b a t o u t k where e b a t o u t k is the resulting energy decrease on the battery s charge b k at the simulation step k by supplying the load e l o a d with discharging efficiency n b a t o u t and leakage e l e a k e b a t i n k is the resulting energy increase on the battery s charge due to the harvested energy e h k with charging efficiency n b a t i n and charge controller efficiency n c c these equations also limit the battery s charge between zero and the battery s maximum capacity n b a t o u t b n o m therefore resulting in energy lost as overflow when the battery capacity is not large enough to accommodate the potential incoming charging energy more details about the adopted model and its parameters can be found in buchli et al 2014 2 4 energy harvesting simulations to perform the energy harvesting simulations we used the pvlib python library for simulating solar energy harvesting from historical hourly satellite derived estimated weather data described earlier and waterlily power transfer curve to harvest energy from gap filled streamflow velocity explained earlier using two hydro turbines then a self powered discrete time water monitoring station was simulated adopting the energy generation store and consumption model from buchli et al 2014 described in section 2 3 integrating all of these we were able to determine the state of the self powered sensing system components e g instantaneous harvestable power based on the input energy or the battery charge level for any given simulation time period for a certain monitoring system with known parameters e g battery capacity or initial charge additionally our simulations provide two main outputs sample loss and energy loss sample loss is the fraction of samples that could not be measured due to insufficient energy this occurs when the sensor station load exceeds the harvestable energy and available energy from the storage unit energy loss is the amount of energy that could be potentially harvested but cannot be used to either take a measurement or increase the storage unit because it is already full these two parameters help to understand the two limits of energy for the system too little energy preventing sampling on the one end and too much energy exceeding storage limits on the other end there are several simplifying assumptions and limitations that are important to understand when translating these simulation results to real world practice first when deploying a hydro turbine in a river to harvest power for a real world application since the particular harvester considered has rotating parts it is prone to stop functioning when debris becomes lodged in its blade in this study we assumed that the hydro turbine is not affected by this practical reality because we assume there is a way to protect the blade from debris using a protective filter or screen that would not significantly alter the streamflow velocity also the waterlily hydro turbine was used throughout all the hydro harvesting simulations as it was one of the very few commercially available small scale hydro turbines depending on the stream hydrology and channel morphology customized hydro turbines other than waterlily hydro turbine can be considered to better adopt with local stream and channel conditions at a harvesting site for the solar energy harvesting simulations the reduction of solar radiation caused by topography i e hills and valleys is not considered in this study only solar radiation reductions due to tree canopy is considered ignoring the topography should not introduce significant errors into the solar energy harvesting estimation because the 42 river locations considered in the study are located in regions with low relief topography lastly for both the hydro and solar energy harvesting we assumed that a cellphone network coverage is always available at all the locations to transmit and receive information which in reality might be unavailable in some locations or some periods of time in order to explore the effect of different factors on the hydro and solar energy harvesting opportunities four different experimental simulations were performed experiment 1 used a single harvesting modality hydro turbines or solar panel experiment 2 altered sampling intervals on a system by hydro alone given that it is more power limited compared to a system powered by solar alone experiment 3 explored the impact of tree canopy on a solar powered system finally experiment 4 simulated a system that uses both hydro and solar energy harvesting to understand the complementarity of these two energy harvesting sources the following sections describe these four experiments in more detail 2 4 1 experiment 1 energy harvesting potential using only one harvesting modality in this experiment we estimated the sample and energy losses for all 42 usgs stations assuming only two hydro harvesters or a solar energy harvester were was available at each station the instantaneous harvested power at each usgs station was averaged over a 5 min window the assumed sampling interval in this experiment for both energy harvesting scenarios this harvestable energy was compared to the load of the sensing stations and the state of the energy storage unit to estimate sample loss due to insufficient power and energy loss due to battery saturation this experiment assumed no loss of solar power due to tree canopy an assumption relaxed in experiment 3 2 4 2 experiment 2 effect of sampling interval on the hydro powered sensing system in this experiment we varied the sampling interval that was assumed to be a fixed 5 min interval in experiment 1 we found that only hydro power and not solar power generated a sample loss in experiment 1 when no tree canopy was assumed therefore we performed this experiment for only hydro power for each of the 42 usgs locations we varied the sampling interval from one to several minutes to explore how changing the sampling interval affects the sample loss and energy loss this information established the sampling frequency a station powered from streamflow velocity alone could maintain and how periods of low flow and high flow can be balanced to reduce sample loss for stations where solar energy harvesting is not an option for example it is not possible to mount and deliver solar power or the site is sufficiently low light that solar power will be limited this experiment gives a measure of the potential of hydro power for sustaining a water sensing station 2 4 3 experiment 3 effect of tree canopy on the solar energy harvesting in this experiment we explored how tree canopy can reduce solar energy harvesting a single layer of leaf can absorb reflect and transmit 80 10 and 10 of the incoming visible radiation respectively brown and gillespie 1995 when considering a complete tree canopy the amount of solar radiation reaching the ground where a solar panel would be installed i e the subcanopy can vary significantly based on the type density and arrangement of the trees olpenda et al 2018 and these factors can be highly localized jeong and culler 2012 taneja et al 2008 one common approach for quantifying tree canopy density is using a hemispherical photograph of the sky directly above the observer s location and then using software e g gap light analyzer gla frazer et al 1999 to estimate light transmission in the photo bode et al 2014 in this study we adapted the results from chikita 2018 and garner et al garner et al 2014 2017 which utilized the hemispherical photograph technique to estimate subcanopy solar radiation in rivers in riparian forests for a variety of vegetation covers and across different seasons in these studies the reduction factor represents the fraction of net solar radiation reached the below canopy surface based on these studies we created a function to reduction factors in harvestable subcanopy solar radiation based on the month of the year for dense deciduous tree canopy cover fig 3 we found that these reduction factors are in line with the actual harvested solar measurements in the deep forest with dense tree canopy reported in other previous studies jeong and culler 2012 taneja et al 2008 the ideal way to simulate harvestable solar power under tree canopy is to directly consider the effects of tree canopy on the weather input data including solar net radiation components i e ghi dhi dni wind speed and air temperature and then do the simulations based on the modified input data however simulating the precise effects of tree canopy on each individual weather input data is impractical and requires rigorous observational data extremely varying from site to site thus to simulate solar power under tree canopy in our study we applied this reduction factor function on the simulated power output from the pvlib python when no tree canopy was considered moreover for dense evergreen tree canopy cover a constant reduction factor of 0 96 across the year for subcanopy solar radiation was assumed 2 4 4 experiment 4 complementary relationship between hydro and solar energy harvesting in experiment 4 we combined the prior knowledge gained in experiments 1 3 to address a primary research goal of this study understanding the complementary nature of hydro and solar power for more ubiquitous water sensing we used three usgs stations as examples for this analysis one with low hydro power potential one with typical hydro power potential and one with high hydro power potential we then ran simulations where these stations are assumed to be under varying tree canopy densities to study how hydro and solar power interact to reduce sampling loss during periods of either low solar power generation or low hydro power generation we see hydro power complementing solar power in situations where sensing is needed under tree canopy making solar energy harvesting alone potentially insufficient for such situations using hydro energy harvesting to supplement solar energy harvesting could make sampling in these regions a more viable option 2 5 geospatial analysis of tree canopy cover for streams in the conus we performed a geospatial analysis to estimate the fraction of stream length in the conus covered by dense tree canopy to estimate the potential impact of hydro energy harvesting as the second goal of this study to do this we used the percent tree canopy raster file available through the us forest service derived from landsat imagery giving a 0 100 tree canopy fraction during growing season at a 30 m spatial resolution coulston et al 2012 we also used the national hydrography dataset plus nhdplus v2 0 stream network that includes about 2 7 million reaches covering the conus usepa and usgs 2012 to calculate the percent tree canopy over the nhdplus network we used arcmap with the following steps first the percent tree canopy raster file was intersected with the nhdplus v2 0 stream network to extract the percent tree canopy pixels covering the centerline of the stream network then the percent tree canopy of the extracted pixel were averaged over the length of each stream reach therefore the average percent tree canopy across each stream reach was estimated for the entire nhdplus stream network these data were then summarized by 4 digit hydrologic unit code huc4 watersheds to provide insights into regional patterns in tree canopy cover for streams in the conus one limitation of this geospatial analysis is that for the stream reaches with widths smaller than 30 m the spatial resolution of the percent tree canopy raster file some errors are potentially introduced into the estimations these estimation errors are due to the percent tree canopy pixels intersected with the centerline of the narrow streams exceeding the width of the narrow stream reach this would lead to consider tree canopy cover over not only the stream reach width but also the non stream land cover next to the reach these errors are expected to grow as the stream width gets significantly narrower than 30 m 2 6 implications of modelling limitations and estimation errors for the study as it was described earlier in this section the water quality sensor station model is primarily built upon validated results of a wide set of models ranging from environmental parameters to electronics and sensor operation behavior although each model component has its own limitations and estimation errors and the complete system was not prototyped the overall goal of this modelling effort was to make reasonable assumptions that enables a practical and coarse assessment of energy harvesting sensor stations at a scale that would be otherwise prohibitive in terms of time and financial resources furthermore the current model is expected to be adapted for particular cases of interest with prototype and measurements of both energy generation under relevant environments and energy consumption of specific electronics and sensors as an example of such an adaptation for locations with significant topographic relief e g for rivers in steep canyons the negative effect of topography on solar energy harvesting cannot be ignored however the locations studied in this research were all located in low relief areas which justifies ignoring the topography when estimating solar energy harvesting 3 results and discussion 3 1 estimated harvestable energy using only one harvesting modality fig 4 shows the average harvestable hydro and solar energy in 5 min intervals over the 2010 2014 period at each of the 42 usgs sites resulting from experiment 1 fig 5 presents the same information in map form to aid better understanding of the spatial patterns of energy harvesting for the energy harvesting sites these results provide insight into how much hydro and solar power can be harvested at each site and how the sites compare to each other in terms of the potential for hydro and solar energy harvesting the average harvestable hydro energy in 5 min ranged from 0 to 778 j with a median over the 42 sites of 19 1 and an average of 87 5 j on the other hand the average harvestable solar energy in 5 min ranged from 994 3 to 1460 8 j with a median of 1285 1 and an average of 1242 6 j over the 42 sites this assumes no reductions in solar energy harvesting due to tree canopy an assumption that is relaxed in experiment 3 under this assumption and not surprisingly solar energy harvesting almost always significantly exceeded the potential for hydro energy harvesting by an average factor of 14 2x across all stations a number of stations had hydro energy harvesting potential comparable with solar energy harvesting potential a few of these stations with consistently high streamflow velocities are at high latitudes and in regions with significant cloud cover less cloud cover is largely responsible for stations in the west recording higher average harvestable solar energy between 1279 and 1460 j per 5 min compared to those in the central and eastern us between 994 and 1193 j per 5 min as shown in figs 4 and 5 one site located in michigan 04159130 an area with significant cloud cover had the greatest average harvestable hydro energy 778 j per 5 min intervals and the greatest average harvestable hydro energy over the average harvestable solar ratio i e 78 among all the 42 usgs sites this site is also at a high latitude receiving less solar radiation compared to sites in lower latitudes the location also has consistently high streamflow velocities throughout the simulation period always exceeding the minimum streamflow velocity value required to generate energy i e 0 5 m s another site 04165710 also located at higher latitudes with rather high streamflow velocities had comparable hydro power potential to that of the solar power potential with an average harvestable hydro energy over average harvestable solar energy ratio of near 30 however two other sites 07374525 and 09527597 which had significant streamflow velocities leading to a hydro energy harvesting potential about 50 of the solar energy harvesting potential were not located at high latitudes fig 6 a shows the percentage of sample loss at each station due to power outages when using only the two hydro energy harvesters and sampling every 5 min when using only the two hydro energy harvesters the average sample loss for the 42 sites was 45 3 while 10 sites had no sample loss for a fixed sampling interval of 5 min fig 6b shows the energy loss due to battery saturation over total harvestable energy for the same sites and over the same time period using the two hydro energy harvesters to leverage this extra energy and reduce the energy loss sampling can be done more frequently which is explored in experiment 2 the median and average energy loss over total harvestable energy help show the variability of such extra energy across the stations which were 12 4 and 32 1 respectively across all stations table 2 shows classification of the sites based on the scenarios of sample loss and or energy loss when using only the two hydro energy harvesters for the 10 of the sites where no sample loss was expected while energy loss was experienced these sites can either have fixed sampling rates smaller than 5 min with no sample loss or can have dynamic sampling intervals over the sensors deployment period for these sites sampling intervals can be decreased some but not much based on the availability of harvestable energy for 14 other sites both sample loss and energy loss were experienced indicating that there are long periods where little or no power is harvested despite short periods of very high streamflow velocity that exceeded battery capacity for 18 of the sites sample loss ranging from 42 9 to 88 6 occurred with no energy loss these sites harvest less power compared to other sites and depend more heavily on the initial battery charge the more the initial charge is the less sample loss they experience when using only the solar energy harvester the results showed no sample loss while the energy loss over total harvestable solar energy had values ranging from 97 0 to 98 0 for all the 42 locations therefore for the given energy consumption budget from the water monitoring system the sampling interval can be significantly lowered to more finely measure water parameters using solar energy harvesting alone assuming cloud cover is the only factor limiting solar energy harvesting at the site this again emphasizes the high potential of the solar compared to hydro energy harvesting under this open sky assumption which is relaxed later in experiment 3 3 2 effect of sampling interval on hydro powered sensing system to better understand the hydro power harvesting potential for the 42 sites we first explored the smallest sampling interval leading to no sample loss for each site fig 7 12 sites were able to sample at every 5 min or smaller sampling intervals while they experienced no sample loss these sites have higher potential for hydro power harvesting compared to rest of the sites and had average harvestable energy ranging from 58 5 to 778 4 j per 5 min most interestingly three of such sites 04159130 04165710 and 09527597 were able to sample as frequent as every 1 min without experiencing any sample loss another important observation among such 12 sites with high hydro power potential is that there are five sites with average harvestable energy from 58 5 to 90 j per 5 min supporting sampling intervals as low as two or 3 min with no sample loss while for site 07374525 with an average harvestable energy of 567 8 j per 5 min the lowest supported sampling interval with no sample loss is higher 5 min due to presence of extreme low and high streamflow velocity periods at this site seven sites had more typical hydro power potential supporting minimum sampling interval with no sample loss in the range of six to 15 min the average harvestable energy for such sites ranged from 15 9 to 117 3 j per 5 min eight sites with lower hydro power potential could support sampling intervals between 16 and 66 min with average harvestable energy from 3 3 to 65 3 j per 5 min while two other sites were able to support much less frequent samplings with no sample loss i e sampling intervals of 152 and 235 min finally for 13 of the sites with extremely low or no hydro power potential regardless of how large the sampling interval was selected sampling loss was experienced no value for minimum sampling interval with no sample loss this was expected as we assumed 2 self discharge in one month meaning that after at most 50 months a battery with full initial charge would be completely depleted for a case where no sensing nor communication is performed while the entire simulation period for the simulations was 5 years 60 month this equates to no samples being taken at least for the 10 last months of the simulations for such sites we further explored the effect of sampling interval on hydro powered sensing system for three chosen sites with different hydro power potentials fig 8 shows the effect of sampling interval on sample loss and harvestable energy loss ratio for three different harvesting sites resulting from experiment 2 these three sites were selected based on their potential harvestable hydro energy representing a a low potential hydro harvesting site usgs 04092750 b a typical potential hydro harvesting site usgs 05537980 and c a high potential hydro harvesting site usgs 04165710 the low potential hydro harvesting site is in class 4 of table 2 meaning no energy loss occurred but sample loss did occur when the sampling interval was 5 min the average harvestable energy in 5 min for this site was 3 3 j for a 5 min sampling interval the sample loss was 86 8 while no energy loss was expected for sampling intervals greater than or equal to 52 min no sample loss was expected which made this site as a site with low hydro power potential for any sampling interval from one to 60 min no energy loss was experienced suggesting the battery may be oversized for this scenario the typical potential harvesting site 05537980 is in class 3 of table 2 meaning both energy loss and sample loss occurred when the sampling interval is 5 min the average harvestable energy in 5 min for this site was 37 4 j for a 5 min sampling interval the sample loss was 31 4 while the energy loss ratio was 47 4 however for sampling intervals greater than or equal to 15 min no sample loss was experienced which made this site as a site with typical hydro power potential but the energy loss ratio became even higher the high harvesting site is in class 1 of table 2 meaning energy loss occurred but no sample loss when the sampling interval was 5 min the average harvestable energy in 5 min for this site was 283 8 j for a 5 min water quality measurement sampling interval the sample loss was zero while the energy loss was 89 6 even by decreasing the sampling interval to 1 min no sample loss was observed which made this site as a site with high hydro power potential while the energy loss ratio was still considerable this suggests that for sites that have high potential for hydro energy harvesting it is possible to set a fixed sampling interval as low as 1 min to have a better temporal picture of the water quality dynamics within a system 3 3 effect of tree canopy on solar energy harvesting in experiment 3 we simulated the solar powered sensing systems under three different tree canopy scenarios i no tree canopy ii a dense deciduous tree canopy using the reduction factors from fig 2 and iii a dense evergreen tree canopy that leads to a constant reduction of 96 in the harvestable solar power this simulation assumed measurements are recorded every 1 min five times smaller than that of default configuration to consider a more power consuming scenario for applications where a very high temporal resolution of water quality dynamics is needed with a sensor station load of 14 38 j per measurement in fig 9 the average harvestable solar energy over the 1 min intervals percent of sample loss using only solar energy harvesting and percent of energy loss for the three different tree canopy scenarios are reported these data are reported for the three stations with low typical and high hydro energy harvesting potential introduced in the prior experiment under no tree canopy and dense deciduous tree canopy scenarios all three of the usgs sites experienced no sample loss while the deciduous tree canopy would result in about 75 reduction in average harvestable solar energy no sample loss in the dense deciduous tree canopy scenarios was experienced this is due mainly to the average harvestable solar energy per minute for the three locations still being about 3 4 greater than the energy consumption per minute of the sensing station and the reliable nature of solar power the periods that solar power is fully unavailable for a prolonged period of time are extremely rare however for the same three locations a dense evergreen tree canopy led to sample losses of 67 6 68 0 69 9 for the low typical and high potential hydro harvesting sites respectively this indicates an opportunity for using hydro power where available under the dense tree canopy conditions especially as shown for dense evergreen tree canopy to complement the solar power in order to have a more ubiquitous sensing of water bodies this opportunity is further explored in experiment 4 where hydro energy harvesting is used to complement solar energy harvesting and in the geospatial analysis of tree canopy cover in the conus where we seek to better understand fraction of streams where tree canopy is limiting solar energy harvesting 3 4 complementarity of hydro and solar power under low light conditions due to dense tree canopy in experiment 4 we explored how hydro power alongside solar power can lead to a denser sensing of water bodies fig 10 illustrates the average harvestable energy over the 1 min intervals percent of sample loss and percent of energy loss for the three different energy harvesting scenarios these energy harvesting scenarios only use hydro power combined use of hydro power and solar power in the case of dense deciduous tree canopy and combined use of hydro power and solar power in the case of dense evergreen tree canopy in the low potential hydro harvesting site 04092750 the harvestable hydro energy was very limited 0 67 j per 1 min so the effect of the combined use of hydro and solar in dense evergreen tree canopy was not significant sample loss reduced from 67 6 to 64 7 moving from solar with dense evergreen tree canopy therefore for low potential hydro harvesting sites like this the combined use of hydro power and solar power would not significantly contribute to a more robust sensing solution in the typical potential hydro harvesting site 05537980 the harvestable hydro energy was considerable 7 48 j per 1 min combined use of hydro and solar under dense evergreen tree canopy was beneficial at this site allowing sample loss to be reduced from 68 0 to 43 0 and energy loss to be increased from 0 to 7 7 compared to using solar power alone for the high potential hydro harvesting site 04165710 with a hydro power production of 56 8 j per 1 min use of hydro power not only reduced the sample loss from 69 9 to 0 but also increased the energy loss from 0 to 59 5 thus for such high potential harvesting sites the combined use of hydro and solar under dense evergreen tree canopy would reduce sample loss significantly potentially to zero and allow for increased sampling frequencies given the excess energy generated that is now being lost 3 5 geospatial analysis of tree canopy cover for waterbodies in the contiguous united states to better understand the potential impact of hydro energy harvesting for water bodies in the conus we performed a geospatial analysis to determine tree canopy coverage that would lead to lower light conditions and hydro energy harvesting opportunities fig 11 shows the mean percent tree canopy over the entire conus nhdplus network is 31 05 smaller streams with stream orders 1 5 show a mean percent tree canopy of 25 32 while larger streams with stream orders greater than 5 show 1 15 mean percent tree canopy cover this difference was expected as our analysis only intersected the percent tree canopy dataset with nhdplus reach centerlines still this is important because the majority of the stream network in the conus is 1 5 order streams 97 by stream length representing a significant opportunity for hydro energy harvesting approaches exploring this data spatially shows significant geographic patterns in tree canopy cover in the conus fig 12 presents the mean percent tree canopy over the nhdplus network averaged for each huc4 watershed southeastern northwestern and eastern regions show a high mean percent tree canopy over the nhdplus stream network meaning that available solar energy is limited for these regions relative to other regions as a result these areas are the best candidates for exploring the complementarity of other energy harvesting sources to supplement solar power northern regions with high percent tree canopy are particularly well suited given that experiment 1 showed high latitude stations having comparable solar and hydro power generation profiles even without considering tree canopy exploring the percentage of stream length with a given percentage of tree canopy across the huc4 watersheds in the conus reveals specific regions of the country where a high fraction of the streams have significant tree canopy coverage fig 13 shows the mean percent tree canopy over the nhdplus network the percent length of nhdplus network curve for the entire nhdplus network is shown as a blue dashed line and the percent length for each huc4 watershed is shown as a solid line this figure shows that for 11 of the stream length in the conus the mean percent tree canopy was greater than 80 among the 205 huc4 watersheds in the conus 19 all located in the southeastern us and with a mean percent tree canopy of 70 80 across the watershed had tree canopy cover greater than 80 for 48 70 of their stream length see a in fig 13 also for the same hucs 25 50 of the stream length had mean percent tree canopy greater than 90 see b in fig 13 other hucs tend to have less stream length at such high percent tree canopy levels suggesting the southeastern us is one of the highest potential regions for benefiting from hydro energy harvesting stream monitoring stations 4 conclusions in this study we compared energy harvesting using a solar panel versus hydro turbines and investigated their complementarity nature to make a self powered water monitoring station under low light conditions we simulated the hydro power using a commercially available hydro turbine and available real world streamflow velocity data collected at 42 stations by the united states geological survey usgs for streams across the conus the solar energy harvesting potential was modeled using a photovoltaic python library pvlib python an open source community supported tool holmgren et al 2018 and a specific solar panel setup to model solar power generation the required weather data obtained from solaranywhere were used as an input to pvlib python after estimating harvestable hydro and solor power a self powered discrete time water monitoring station was simulated adopting the energy generation store and consumption model from buchli et al 2014 then four different energy harvesting experiments were explored along with a geospatial analysis of tree canopy coverage for streams in the conus we found that for the selected harvesters solar power almost always outperformed hydro power potential under the assumption of no tree canopy in experiment 1 which only used either two hydro turbines or a solar panel solar energy harvesting potential ranged from 994 3 to 1460 8 j per 5 min with a mean value of 1242 6 j per 5 min however hydro power averaged harvestable energy in 5 min generated far less power ranging from 0 to 778 j per 5 min with an average of 87 5 j per 5 min for most stations solar energy harvesting significantly exceeded the potential for hydro energy harvesting by an average factor of 14 2x across all stations however a number of stations with consistently high streamflow velocities some of which are located at high latitudes and in regions with significant cloud cover experienced hydro energy harvesting potential near solar energy harvesting potential we found that using two hydro energy harvesters would result in a 45 3 sample loss on average across the 42 usgs stations used in the analysis but no sample loss for 10 stations with high velocities and more significant cloud cover assuming a fixed sampling interval of 5 min drawing 14 38 j per measurement using the same assumptions a solar powered water sensing system only would result in no sample loss for the 42 stations as expected solar power resulted in high energy loss rates of 97 6 on average whereas hydro power energy loss rates averaged 32 1 we found that solar power when unimpeded by tree canopy and other factors outside of cloud cover can often support sampling intervals much lower than 5 min the assumptions included a multi sensor water quality sampling station as described in the paper that includes energy harvester s power consuming units including controller sensor and communication units with a total drawing of 14 38 j per measurement and an energy storage unit we found that solar panels without energy reduction due to tree canopy cover could easily support sampling rates of 1 min and likely higher as well although sampling frequencies below 1 min were not investigated in this study there was greater variability across hydro powered sites in terms of harvestable energy a typical hydro powered site could support sampling rates closer to 15 min without sample loss which is still a significant benefit for understanding hydrologic dynamics a few of the 42 sampling sites analyzed could support a much higher sampling rate of up to one to 5 min without sample loss comparable to solar harvesting these sites had a high average streamflow velocity of approximately 0 6 m s on average throughout the study region and ranging from approximately 0 4 m s to 1 0 m s at each site introducing tree canopy into the analysis showed how dense tree canopy can significantly reduce solar energy harvesting potential for many of the sampling sites very dense evergreen tree canopy can lead to a 96 reduction in harvestable power making hydro power at such locations much more attractive we found that for sites with a high potential for hydro energy harvesting and dense evergreen tree cover sampling loss for a 1 min sampling interval can be reduced from 69 9 to zero by using hydro energy harvesting to complement solar energy harvesting for a similar site but with more typical hydro energy harvesting potential sampling loss can be reduced from 68 0 to 43 0 by conducting a geospatial analysis of streams and canopy cover during the leaf on season for the conus we found that 11 of streams by stream length have a mean percent tree canopy of over 80 for 19 huc4 watersheds located in southeast us where tree canopy is the highest 48 70 of the stream length in each huc4 has at least 80 canopy cover also in the same 19 huc4 watersheds 25 50 of the stream length has a tree canopy cover greater than 90 these 19 huc4 watersheds are an obvious target for further exploring the potential of hydro energy harvesting to complement solar energy harvesting based on the results of this study we conclude that hydro energy harvesting for stream locations with high flow velocities preferably above 0 6 m s on average and dense tree canopy especially evergreen tree canopy can complement or even replace solar energy harvesting approaches given dense tree canopy covers a considerable percent of us waterways this alternative source of energy for powering water monitoring stations should be further explored the complimentary nature of hydro energy harvesting to solar energy harvesting can not only increase sampling coverage in space but also be used to reduce the variability in solar energy harvesting e g cloudy days in order to increase sampling coverage in time further work exploring how dynamic power management strategies that optimize sampling interval given past and potentially forecasted energy available from solar and hydro sources could lead to the identification of opportunities to increase base sampling frequency with lower chance of incurring on sample loss for example during periods of high flow where important dynamics such as nutrient and pollutant transport are occurring an example of forecasting energy available from a hydro source is illustrated by ahmad and hossain 2019 where short term weather predictions and antecedent hydrological variables are used as inputs to an artificial neural network forecasting reservoir inflow and maximizing the hydropower generation such dynamic power management strategies that take advantage of streamflow forecasting capabilities would enable more ubiquitous monitoring of our water resources by not only covering currently unmonitored streams that have dense tree canopy or are within deep canyons but also new insights into temporal dynamics within riverine systems by increasing sampling frequencies during periods of high flow when solar power is unavailable but hydro power is plentiful software and data availability the pvlib python library a community supported tool can be found at https pvlib python readthedocs io en stable the developed python codes to perform this study including the codes utilizing the pvlib python library for this study can be found at https github com uva hydroinformatics hydrosolarenergyharvestingforwatermonitoring the codes necessary to perform the geospatial analysis estimating the tree canopy cover for streams in the conus can be found in maghami et al 2021 the data used in this study are from the sources listed in the following table while the stream velocity time series for the 42 usgs sites can be automatically downloaded using the scripts provided a zipped folder containing this dataset is made available in the same github repository for convenience the historical satellite derived estimated weather data for the same sites requires a license to access there is a free academic license available upon registering for an account on clean power research s solaranywhere portal https data solaranywhere com that was used in this study but this license does not allow for public sharing of the data the stream network for conus and percent tree canopy for conus are large datasets so were not replicated in hydroshare but can be downloaded using the data sources provided in the table below the subsequent processed datasets used in the analysis are published through hydroshare maghami et al 2021 the huc2 and huc4 watershed polygons for the conus can be easily accessed via the data source mentioned in the table below further information on how all the datasets are processed and used for subsequent steps can be found in the hydroshare resource and the github repository name of dataset data source data format data availability stream velocity time series u s geological survey https waterdata usgs gov nwis tab delimited ascii files free historical satellite derived estimated weather data clean power research s solaranywhere https data solaranywhere com tmy3 format csv files free with academic license stream network for conus national hydrography dataset plus nhdplus v2 0 https nhdplus com nhdplus nhdplusv2 home php gdb files free percent tree canopy for conus the usda forest service remote sensing applications center rsac https data fs usda gov geodata rastergateway treecanopycover img raster files free huc2 and huc4 watershed polygons for conus u s geological survey and usda natural resources conservation service https datagateway nrcs usda gov catalog productdescription wbd html gdb files free declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding was provided by the university of virginia school of engineering and applied science through a competitive research innovation award 
25843,relying on solar energy alone to power water monitoring stations limits the ability to monitor water resources in low light locations such as for streams with dense tree canopy cover to address this limitation water monitoring stations could have a supplementary micro hydro turbine to harvest kinetic energy from streamflow to explore this possibility we simulated the energy harvesting potential at 42 locations with long term stream velocity records and determined the canopy cover for streams across the contiguous united states conus results show that a site with typical streamflow velocity and dense evergreen tree canopy would have 1 6 times less sample loss from 68 to 43 after adding hydro energy harvesters many streams across the conus with dense canopy cover could benefit from this especially headwater streams and streams in the southeastern us where many watersheds have more than 50 of their stream length under dense tree canopy cover keywords water monitoring self powered water sensing ubiquitous water sensing smart water systems 1 introduction water resources have been monitored with networks of in situ sensing systems for studying many hydrological and environmental problems such as flooding runoff pollution and aquatic ecosystem degradation for decades bartos et al 2018 rode et al 2016 ruhala and zarnetske 2017 wymore et al 2018 in the united states by the time of writing of this paper the united states geological survey usgs has collected and made publicly available surface water historical instantaneous data water quantity and or quality for 14 481 different active and inactive or discontinued stream locations across the nation while this may seem like a large number of monitoring sites considering the nationwide rivers inventory nri estimated a total of 5 200 000 km of streams in the contiguous united states conus benke 1990 it means that there is only one usgs water sensing station for approximately every 360 km of stream length this density of in stream measurement is insufficient for understanding many properties and functions occurring within waterbodies horsburgh et al 2010 kirchner et al 2004 while the limited spatial density of current water sensing networks is certainly a function of the high installation and maintenance cost of each individual stream gauging station another limiting factor is the availability of energy to power the sensing equipment given that many sensing locations are off grid energy harvesting the process of deriving energy from the surrounding environment and converting it to a usable form e g electrical energy must be used to power the sensing system and recharge batteries at the sensing station a variety of energy harvesting approaches from different renewable energy sources e g vibration wind and solar energy harvesting for wireless sensor networks have been studied in past research studies see olatinwo and joubert 2019 for a review among such approaches the most common energy harvesting approach for water monitoring is solar power chen et al 2017 due to its relatively low cost robustness and high power output ratings shaikh and zeadally 2016 prior studies have shown how solar energy harvesting can be exploited for sensing environmental variables and water related characteristics bartos et al 2018 jones et al 2017 kapetanovic et al 2017 quinn et al 2010 additionally tools such as pvlib python an open source community supported tool holmgren et al 2018 enable estimating solar power generation prior to actual deployment of a given solar panel however solar energy harvesting is not always a viable option due to low light conditions e g forested areas with dense tree canopies or high wall canyons consistent cloudy weather azevedo and lopes 2016 jeong and culler 2012 taneja et al 2008 or within urban environments e g within subsurface drainage infrastructure this motivates the need to investigate other energy harvesting sources for such locations for water monitoring one obvious energy harvesting source is the kinetic energy of water flow as a potential supplemental energy source to solar energy for low light conditions while prior research has widely explored the potential for larger scale hydropower harvesting applications such as run of river hydropower plants e g yildiz and vrugt 2019 fewer studies have investigated the smaller scale micro hydropower harvesting potential for monitoring applications in natural streams or stormwater infrastructure systems there has been significant work however exploring micro hydropower harvesting for drinking water pipe networks that show this technology s potential for other water resource and environmental applications hoffmann et al 2013 r√∂del et al 2016 ye and soga 2012 there has also been recent laboratory work exploring micro hydro turbine power harvesting that can inform field application studies two such studies evaluated the effect of different energy harvesting mechanisms e g hydro turbine harvesting power from flow velocity and different design choices e g hydro turbine geometries on the efficiency of the systems azevedo and lopes 2016 kamenar et al 2016 these systems were able to generate hydro power up to hundreds of milliwatts based on both the energy harvester design choices and the characteristics of energy harvesting environment e g flow velocity for hydro turbine which could be enough to power a water sensing system in the natural environment with real time data communication any energy harvesting sources e g solar or hydro can be temporary and spatially limited or even periodically unavailable leading to intermittent power outages and sensing gaps without sufficient energy storage infrastructure e g batteries therefore a combined use of energy harvesting sources when possible is expected to prevent or reduce the time periods when the sensing system is out of power reducing the need for sophisticated energy storage infrastructure for example morais et al 2008 showed how using a small solar panel combined with a small hydro generator placed in a nearby irrigation or hydroponic water pipe and a wind generator was able to supply a generic wireless data acquisition platform energy store in agricultural and livestock environments even with low solar radiation this prior research provides important understanding and software needed to enable energy harvesting from different energy sources i e solar hydro and wind for sensing the natural environments and highlights the importance of a combined energy harvesting from various energy sources when individual sources are limited however reviewing prior research reveals that hydro energy harvesting to power in stream water monitoring stations for locations with limited access to solar energy i e riparian forest zones where sunlight penetrating the tree canopy and reaching the solar panel underneath it can be highly limited is not explored yet this comparison is necessary to better understand if hydro energy harvesting is a viable option for achieving high coverage and dense spatial sensing of waterways in particular for those locations with limited availability of solar power furthermore knowing what fraction of streams in the conus are candidates for hydro energy harvesting due to dense tree canopy limiting solar energy harvesting will aid in understanding the potential impact of advancing hydro energy harvesting to monitor more locations in the stream network toward a long term vision of ubiquitous monitoring of the nation s waterbodies given these knowledge gaps the first goal of this study is to compare and simulate energy harvesting in multiple river locations using a solar panel versus a hydro turbine in order to investigate their complementary relationship in self powering water monitoring stations under different light conditions due to the presence of tree canopy covering waterways the second goal is to determine the stream length in the conus with dense tree canopy to estimate the potential impact of hydro energy harvesting section 2 discusses the materials and methods used to achieve both of these goals including the datasets used to simulate the solar and hydro energy harvesting sensor stations specifications and energy harvesting simulations section 3 presents the results of the analysis including a discussion of the findings and their limitations finally we summarize and offer conclusions from the study in section 4 2 materials and methods in this section we first describe the datasets used to estimate harvestable energy from both hydro and power sources second we present the self powered sensor station specifications and its key components to define a typical energy draw and battery specifications for such stations third we explain the energy generation store and consumption model adopted for this small scale hydro solar sensing station simulation fourth we describe the different experimental setups used to address our primary research goal stated earlier fifth we explain the performed geospatial analysis of tree canopy cover for streams in the conus to address our secondary research goal formerly mentioned finally we discuss the implications of modelling limitations and estimation errors for our study fig 1 shows the diagrammatic representations of the components used to simulate the energy harvesting from hydro and solar energy sources to power a water monitoring station all the components present in this figure are described in sections 2 1 2 2 2 3 and 2 4 2 1 datasets 2 1 1 hydro energy harvesting dataset the usgs has recorded and shared streamflow velocity for about 400 locations out of the previously mentioned 14 481 active and inactive or discontinued stream locations across the united states these 400 locations are located on natural streams or man made channels u s geological survey 2016 velocity measurements are recorded using acoustic doppler velocity meters this data is often used to estimate the flow discharge in waterbodies levesque and oberg 2012 the available velocity time series for each usgs site is either a reading from the sensor directly identifiable using the usgs parameter code 72254 mean streamflow velocity over the cross sectional area of the waterbody identifiable using the usgs parameter code 72255 or both because the streamflow velocity time series is either measured at a point or represents the mean streamflow velocity at a cross section it does not capture the variation of streamflow velocity in depth width and along the river additionally there were a few negative values reported at many of the usgs sites which can happen due to backwater flow conditions that can occur at stream confluences streams flowing into lakes or reservoirs tide affected streams regulated stream flows dams or control structures strong prevailing winds or where structures e g bridges and culverts restrict flow the available streamflow velocity record duration varies across the 400 sites from about four months to about 12 years most often streamflow velocity measurements are recorded on a 15 min interval we chose to use a five year study period of 2010 2014 because it had the maximum number of sites with recordings during this period we identified 42 sites with a low missing data fraction during this period in total 2 1 of the dataset was missing due to data gaps the largest gap within the data was 23 47 days and the median gap across all sites was 1 1 h across all sites 80 of the data gaps were less than 4 2 h because the missing data made up a small fraction of the overall dataset and because the gaps tended to be small relative to the overall period of analysis we used a simple linear interpolation as a standard gap filling technique applied consistently across all data gaps the resulting gap filled streamflow velocity time series for the 42 sites were used to estimate the harvestable energy using a flow velocity to power transfer curve from a small hydro turbine which was provided by the manufacturer this flow velocity to power transfer curve is explained in section 2 2 1 2 1 2 solar energy harvesting dataset to estimate the harvestable solar energy historical satellite derived estimated weather data provided by clean power research s solaranywhere for the conus was used clean power research service database solaranywhere www document 2019 the dataset includes solar irradiance global horizontal irradiance direct normal irradiance and diffuse horizontal irradiance wind speed at a height of 10 m and ambient dry bulb temperature the spatial resolution of the data is 10 km and the data is available on an hourly time step the weather dataset was linearly interpolated for every minute to be consistent with the streamflow velocity dataset the interpolated dataset was used as input for a photovoltaic pv model to estimate the harvestable solar energy as explained in more detail later in section 2 2 1 2 2 sensor stations specifications the water monitoring sensor station simulation model has four main factors 1 energy from the external environment 2 one or more harvesters 3 an energy storage medium and 4 an electrical load from sensors and other electronic components the energy from the external environment e g water kinetic energy or solar radiation is collected and converted to a usable energy form i e electrical energy by the harvester to meet the power demand of the load the energy storage medium is necessary to store energy during periods when the harvested energy is greater than the consumed energy and then provide the stored energy during periods when the energy consumption by the unit exceeds the harvested energy the external environment was explored in the previous section here we provide specifications of the harvesters energy storage medium and the power consuming unit load that were used in the study 2 2 1 energy harvesters to estimate the harvestable hydro power a small commercial portable hydro turbine called waterlily was considered waterlily turbine www document 2020 one of the very few commercially available options of its kind depending on the stream hydrology and channel morphology customized hydro turbines to adopt with the conditions can be considered fig 2 depicts the waterlily hydro turbine streamflow velocity to power transfer curve provided by the manufacturer the minimum streamflow velocity value required to generate energy is 0 5 m s and the peak power of about 14 3 w is achieved at approximately 3 2 m s we assumed the hydro turbine could harvest energy from both positive and negative flows thus the negative velocities described earlier were treated the same as positive velocities throughout all the hydro simulations we used a parallel combination of two waterlily hydro turbines therefore at a given flow velocity the harvested power from fig 2 is doubled using two hydro turbines was necessary to provide enough energy for the water monitoring station to estimate the harvestable solar power we used pvlib python an open source community supported tool holmgren et al 2018 in this study we used pvlib python version 0 6 4 holmgren et al 2019 for all the harvesting locations the simulations assumed a 20 watt 12 v solar panel kyocera ks20 www document 2019 operating at the maximum power point the surface azimuth of the solar panel was set to 180 for all energy harvesting sites during the entire simulation period while the solar panel surface tilt was considered to be equal to the latitude of the energy harvesting location the weather dataset described in section 2 1 2 was used as input to the solar energy harvester system simulations 2 2 2 power consumption model the sensor station power consumption model assumed in this work includes three main modules controller sensor and communication the whole station was assumed to have two operating modes active where all modules are consuming energy and idle or sleep where all modules are inactive the sensor station alternates between active and idle modes acquiring sensor measurements and periodically transferring data over the cellular network in active mode while saving energy in between measurements by alternating to idle mode the station s average energy consumption is then used as input to the simulator for the purposes of the analysis performed in this paper the sensor station hardware platform from the open storm project bartos et al 2018 is used as a reference of a possible implementation of such system the following paragraphs provide additional details for each of the modules used in the model the controller module is a programmable device capable of interfacing and turning sensor and communication modules on and off for example the open storm project adopts a custom printed circuit board with a cypress cy8c5888lti lp097 programmable system on chip soc the controller module together with the lowest power consuming sensing modality was reported by the open storm project team to consume about 10 ma at 3 7v voltage source on active mode for the purposes of the analysis performed in this paper the controller power consumption is assumed to be 37 mw and it should be viewed as a target power consumption budget that the controller module must adhere to the idle or sleep mode was reported by the open storm project team to consume up to 60 Œºa at 3 7v and this worst case is considered on the sensor station model the sensor modules selected for this analysis and supported by the open storm project comprises of a collection of typical water sensors used in a sensor station table 1 the active power consumption for each module together with the recommended inline voltage isolator part number be ivi is provided by the manufacturer atlas scientific www document 2020 the open storm project team reported that it typically performs one measurement every 10 min and it takes 5 s to perform a measurement one additional parameter typically measured on water quality sensor stations is turbidity since atlas scientific currently does not commercialize turbidity sensors we considered the turbidity sensor manufactured by global water global water www document 2019 in this work the global water turbidity sensor wq730 consumes up to 60 ma at 12 v power supply resulting in a power consumption of 720 mw the wq730 sensor requires a recommended warm up time of 8 s before start measuring and for the purposes of this paper analysis we considered a 1 s window to actually acquire the measurement resulting on a total of 9 s of active time and consuming a total of 6 48 j per measurement the communication module is an electronic device capable of transmitting and receiving information through a cellphone network the open storm project bartos et al 2018 sensor node uses the module telit cc864 dual to perform the communication task and an average of 25 ma current consumption at 3 7 v voltage source was reported by the open storm project team each communication event was reported to happen every hour and take up to 60 s in this work the worst case of 60 s for a communication event total energy consumption of 5 55j per event is assumed while no energy is consumed while in idle or sleep mode it is also assumed that the controller remains active for the whole transmission event of 60 s consuming a total of 2 22j per transmission event each communication event is assumed to happen every 24 sampling events what would result in one communication event every 2 h for a 5 min sampling interval the average energy consumption per measurement spent on the communication task is therefore estimated to be 0 324j per measurement event to calculate the total energy consumption per measurement of the complete station using all sensors all five atlas scientific sensors and the global water turbidity sensor we add the energy consumption per measurement of all sensors 13 722j the energy consumption of the controller device during sensor measurements 0 333j for 9 s operation due to the turbidity sensor and the average energy consumption per measurement of the communications module 0 324j resulting in a total of approximately 14 38j the average energy consumption per simulation step e l o a d is calculated by equation 1 1 e l o a d s i m s t e p e a c t i v e s i s i m s t e p p i d l e 1 9 s i 60 t i where simstep is the simulation step in seconds si is the sampling interval in seconds e a c t i v e is the total energy consumption per measurement in j p i d l e is the sensor station power consumption during idle mode in watts assumed to be 222 Œºw and ti is the transmission interval in seconds the time between successive communication events for a simulation step of 1 min a sampling interval of 5 min and transition interval of 2 h e l o a d is calculated to be approximately 2 89 j per minute since it is usually more than enough to measure the water quality parameters at waterways at sub hourly intervals which successfully captures the dynamics of the change in parameters the default sampling interval for all the water quality sensors was fixed to 5 min unless another sampling interval is noted 2 2 3 energy storage unit and charge controller power storage is necessary to manage the energy harvesting fluctuations over the deployment period of the energy harvesting system to store energy during high energy harvesting periods and use the stored energy during energy harvesting droughts also on many occasions the instantaneous harvested power is not enough to directly power the electronics which again emphasizes the need for an energy storage medium the battery model used in this paper considers charging and discharging efficiencies as well as a constant energy leakage parameter as used in buchli et al 2014 the following parameters were considered for this analysis maximum capacity of 0 4 kwh constant leakage of 0 66 j per minute 2 self discharge in one month charging and discharging efficiencies of 0 9 and 0 7 respectively are assumed the battery is considered to be at full charge in the beginning of each simulation one example of a commercial battery with 0 4 kwh capacity is the victron energy 12v 34 ah battery victron energy www document 2020 a charge controller is considered to control battery s charging and discharging operation as assumed in the work done by buchli et al 2014 the charge controller protects the battery by disconnecting the load when stored energy reaches a minimum level complete depletion in our case and only reconnecting it after the battery is recharged to threshold percentage of the nominal capacity assumed as 30 in this work the charge controller efficiency is assumed to be 90 as used in buchli et al 2014 2 3 energy generation store and consumption model the simulation model used in this paper follows the work presented in buchli et al 2014 by considering a harvest store use energy model and calculating the battery state of charge after every simulation step equations 2 4 summarize the adopted model 2 e b a t o u t k e l o a d n b a t o u t e l e a k 3 e b a t i n k min n b a t i n n c c e h k max 0 n b a t o u t b n o m b k 1 e b a t o u t k 4 b k max 0 min n b a t o u t b n o m b k 1 e b a t i n k e b a t o u t k where e b a t o u t k is the resulting energy decrease on the battery s charge b k at the simulation step k by supplying the load e l o a d with discharging efficiency n b a t o u t and leakage e l e a k e b a t i n k is the resulting energy increase on the battery s charge due to the harvested energy e h k with charging efficiency n b a t i n and charge controller efficiency n c c these equations also limit the battery s charge between zero and the battery s maximum capacity n b a t o u t b n o m therefore resulting in energy lost as overflow when the battery capacity is not large enough to accommodate the potential incoming charging energy more details about the adopted model and its parameters can be found in buchli et al 2014 2 4 energy harvesting simulations to perform the energy harvesting simulations we used the pvlib python library for simulating solar energy harvesting from historical hourly satellite derived estimated weather data described earlier and waterlily power transfer curve to harvest energy from gap filled streamflow velocity explained earlier using two hydro turbines then a self powered discrete time water monitoring station was simulated adopting the energy generation store and consumption model from buchli et al 2014 described in section 2 3 integrating all of these we were able to determine the state of the self powered sensing system components e g instantaneous harvestable power based on the input energy or the battery charge level for any given simulation time period for a certain monitoring system with known parameters e g battery capacity or initial charge additionally our simulations provide two main outputs sample loss and energy loss sample loss is the fraction of samples that could not be measured due to insufficient energy this occurs when the sensor station load exceeds the harvestable energy and available energy from the storage unit energy loss is the amount of energy that could be potentially harvested but cannot be used to either take a measurement or increase the storage unit because it is already full these two parameters help to understand the two limits of energy for the system too little energy preventing sampling on the one end and too much energy exceeding storage limits on the other end there are several simplifying assumptions and limitations that are important to understand when translating these simulation results to real world practice first when deploying a hydro turbine in a river to harvest power for a real world application since the particular harvester considered has rotating parts it is prone to stop functioning when debris becomes lodged in its blade in this study we assumed that the hydro turbine is not affected by this practical reality because we assume there is a way to protect the blade from debris using a protective filter or screen that would not significantly alter the streamflow velocity also the waterlily hydro turbine was used throughout all the hydro harvesting simulations as it was one of the very few commercially available small scale hydro turbines depending on the stream hydrology and channel morphology customized hydro turbines other than waterlily hydro turbine can be considered to better adopt with local stream and channel conditions at a harvesting site for the solar energy harvesting simulations the reduction of solar radiation caused by topography i e hills and valleys is not considered in this study only solar radiation reductions due to tree canopy is considered ignoring the topography should not introduce significant errors into the solar energy harvesting estimation because the 42 river locations considered in the study are located in regions with low relief topography lastly for both the hydro and solar energy harvesting we assumed that a cellphone network coverage is always available at all the locations to transmit and receive information which in reality might be unavailable in some locations or some periods of time in order to explore the effect of different factors on the hydro and solar energy harvesting opportunities four different experimental simulations were performed experiment 1 used a single harvesting modality hydro turbines or solar panel experiment 2 altered sampling intervals on a system by hydro alone given that it is more power limited compared to a system powered by solar alone experiment 3 explored the impact of tree canopy on a solar powered system finally experiment 4 simulated a system that uses both hydro and solar energy harvesting to understand the complementarity of these two energy harvesting sources the following sections describe these four experiments in more detail 2 4 1 experiment 1 energy harvesting potential using only one harvesting modality in this experiment we estimated the sample and energy losses for all 42 usgs stations assuming only two hydro harvesters or a solar energy harvester were was available at each station the instantaneous harvested power at each usgs station was averaged over a 5 min window the assumed sampling interval in this experiment for both energy harvesting scenarios this harvestable energy was compared to the load of the sensing stations and the state of the energy storage unit to estimate sample loss due to insufficient power and energy loss due to battery saturation this experiment assumed no loss of solar power due to tree canopy an assumption relaxed in experiment 3 2 4 2 experiment 2 effect of sampling interval on the hydro powered sensing system in this experiment we varied the sampling interval that was assumed to be a fixed 5 min interval in experiment 1 we found that only hydro power and not solar power generated a sample loss in experiment 1 when no tree canopy was assumed therefore we performed this experiment for only hydro power for each of the 42 usgs locations we varied the sampling interval from one to several minutes to explore how changing the sampling interval affects the sample loss and energy loss this information established the sampling frequency a station powered from streamflow velocity alone could maintain and how periods of low flow and high flow can be balanced to reduce sample loss for stations where solar energy harvesting is not an option for example it is not possible to mount and deliver solar power or the site is sufficiently low light that solar power will be limited this experiment gives a measure of the potential of hydro power for sustaining a water sensing station 2 4 3 experiment 3 effect of tree canopy on the solar energy harvesting in this experiment we explored how tree canopy can reduce solar energy harvesting a single layer of leaf can absorb reflect and transmit 80 10 and 10 of the incoming visible radiation respectively brown and gillespie 1995 when considering a complete tree canopy the amount of solar radiation reaching the ground where a solar panel would be installed i e the subcanopy can vary significantly based on the type density and arrangement of the trees olpenda et al 2018 and these factors can be highly localized jeong and culler 2012 taneja et al 2008 one common approach for quantifying tree canopy density is using a hemispherical photograph of the sky directly above the observer s location and then using software e g gap light analyzer gla frazer et al 1999 to estimate light transmission in the photo bode et al 2014 in this study we adapted the results from chikita 2018 and garner et al garner et al 2014 2017 which utilized the hemispherical photograph technique to estimate subcanopy solar radiation in rivers in riparian forests for a variety of vegetation covers and across different seasons in these studies the reduction factor represents the fraction of net solar radiation reached the below canopy surface based on these studies we created a function to reduction factors in harvestable subcanopy solar radiation based on the month of the year for dense deciduous tree canopy cover fig 3 we found that these reduction factors are in line with the actual harvested solar measurements in the deep forest with dense tree canopy reported in other previous studies jeong and culler 2012 taneja et al 2008 the ideal way to simulate harvestable solar power under tree canopy is to directly consider the effects of tree canopy on the weather input data including solar net radiation components i e ghi dhi dni wind speed and air temperature and then do the simulations based on the modified input data however simulating the precise effects of tree canopy on each individual weather input data is impractical and requires rigorous observational data extremely varying from site to site thus to simulate solar power under tree canopy in our study we applied this reduction factor function on the simulated power output from the pvlib python when no tree canopy was considered moreover for dense evergreen tree canopy cover a constant reduction factor of 0 96 across the year for subcanopy solar radiation was assumed 2 4 4 experiment 4 complementary relationship between hydro and solar energy harvesting in experiment 4 we combined the prior knowledge gained in experiments 1 3 to address a primary research goal of this study understanding the complementary nature of hydro and solar power for more ubiquitous water sensing we used three usgs stations as examples for this analysis one with low hydro power potential one with typical hydro power potential and one with high hydro power potential we then ran simulations where these stations are assumed to be under varying tree canopy densities to study how hydro and solar power interact to reduce sampling loss during periods of either low solar power generation or low hydro power generation we see hydro power complementing solar power in situations where sensing is needed under tree canopy making solar energy harvesting alone potentially insufficient for such situations using hydro energy harvesting to supplement solar energy harvesting could make sampling in these regions a more viable option 2 5 geospatial analysis of tree canopy cover for streams in the conus we performed a geospatial analysis to estimate the fraction of stream length in the conus covered by dense tree canopy to estimate the potential impact of hydro energy harvesting as the second goal of this study to do this we used the percent tree canopy raster file available through the us forest service derived from landsat imagery giving a 0 100 tree canopy fraction during growing season at a 30 m spatial resolution coulston et al 2012 we also used the national hydrography dataset plus nhdplus v2 0 stream network that includes about 2 7 million reaches covering the conus usepa and usgs 2012 to calculate the percent tree canopy over the nhdplus network we used arcmap with the following steps first the percent tree canopy raster file was intersected with the nhdplus v2 0 stream network to extract the percent tree canopy pixels covering the centerline of the stream network then the percent tree canopy of the extracted pixel were averaged over the length of each stream reach therefore the average percent tree canopy across each stream reach was estimated for the entire nhdplus stream network these data were then summarized by 4 digit hydrologic unit code huc4 watersheds to provide insights into regional patterns in tree canopy cover for streams in the conus one limitation of this geospatial analysis is that for the stream reaches with widths smaller than 30 m the spatial resolution of the percent tree canopy raster file some errors are potentially introduced into the estimations these estimation errors are due to the percent tree canopy pixels intersected with the centerline of the narrow streams exceeding the width of the narrow stream reach this would lead to consider tree canopy cover over not only the stream reach width but also the non stream land cover next to the reach these errors are expected to grow as the stream width gets significantly narrower than 30 m 2 6 implications of modelling limitations and estimation errors for the study as it was described earlier in this section the water quality sensor station model is primarily built upon validated results of a wide set of models ranging from environmental parameters to electronics and sensor operation behavior although each model component has its own limitations and estimation errors and the complete system was not prototyped the overall goal of this modelling effort was to make reasonable assumptions that enables a practical and coarse assessment of energy harvesting sensor stations at a scale that would be otherwise prohibitive in terms of time and financial resources furthermore the current model is expected to be adapted for particular cases of interest with prototype and measurements of both energy generation under relevant environments and energy consumption of specific electronics and sensors as an example of such an adaptation for locations with significant topographic relief e g for rivers in steep canyons the negative effect of topography on solar energy harvesting cannot be ignored however the locations studied in this research were all located in low relief areas which justifies ignoring the topography when estimating solar energy harvesting 3 results and discussion 3 1 estimated harvestable energy using only one harvesting modality fig 4 shows the average harvestable hydro and solar energy in 5 min intervals over the 2010 2014 period at each of the 42 usgs sites resulting from experiment 1 fig 5 presents the same information in map form to aid better understanding of the spatial patterns of energy harvesting for the energy harvesting sites these results provide insight into how much hydro and solar power can be harvested at each site and how the sites compare to each other in terms of the potential for hydro and solar energy harvesting the average harvestable hydro energy in 5 min ranged from 0 to 778 j with a median over the 42 sites of 19 1 and an average of 87 5 j on the other hand the average harvestable solar energy in 5 min ranged from 994 3 to 1460 8 j with a median of 1285 1 and an average of 1242 6 j over the 42 sites this assumes no reductions in solar energy harvesting due to tree canopy an assumption that is relaxed in experiment 3 under this assumption and not surprisingly solar energy harvesting almost always significantly exceeded the potential for hydro energy harvesting by an average factor of 14 2x across all stations a number of stations had hydro energy harvesting potential comparable with solar energy harvesting potential a few of these stations with consistently high streamflow velocities are at high latitudes and in regions with significant cloud cover less cloud cover is largely responsible for stations in the west recording higher average harvestable solar energy between 1279 and 1460 j per 5 min compared to those in the central and eastern us between 994 and 1193 j per 5 min as shown in figs 4 and 5 one site located in michigan 04159130 an area with significant cloud cover had the greatest average harvestable hydro energy 778 j per 5 min intervals and the greatest average harvestable hydro energy over the average harvestable solar ratio i e 78 among all the 42 usgs sites this site is also at a high latitude receiving less solar radiation compared to sites in lower latitudes the location also has consistently high streamflow velocities throughout the simulation period always exceeding the minimum streamflow velocity value required to generate energy i e 0 5 m s another site 04165710 also located at higher latitudes with rather high streamflow velocities had comparable hydro power potential to that of the solar power potential with an average harvestable hydro energy over average harvestable solar energy ratio of near 30 however two other sites 07374525 and 09527597 which had significant streamflow velocities leading to a hydro energy harvesting potential about 50 of the solar energy harvesting potential were not located at high latitudes fig 6 a shows the percentage of sample loss at each station due to power outages when using only the two hydro energy harvesters and sampling every 5 min when using only the two hydro energy harvesters the average sample loss for the 42 sites was 45 3 while 10 sites had no sample loss for a fixed sampling interval of 5 min fig 6b shows the energy loss due to battery saturation over total harvestable energy for the same sites and over the same time period using the two hydro energy harvesters to leverage this extra energy and reduce the energy loss sampling can be done more frequently which is explored in experiment 2 the median and average energy loss over total harvestable energy help show the variability of such extra energy across the stations which were 12 4 and 32 1 respectively across all stations table 2 shows classification of the sites based on the scenarios of sample loss and or energy loss when using only the two hydro energy harvesters for the 10 of the sites where no sample loss was expected while energy loss was experienced these sites can either have fixed sampling rates smaller than 5 min with no sample loss or can have dynamic sampling intervals over the sensors deployment period for these sites sampling intervals can be decreased some but not much based on the availability of harvestable energy for 14 other sites both sample loss and energy loss were experienced indicating that there are long periods where little or no power is harvested despite short periods of very high streamflow velocity that exceeded battery capacity for 18 of the sites sample loss ranging from 42 9 to 88 6 occurred with no energy loss these sites harvest less power compared to other sites and depend more heavily on the initial battery charge the more the initial charge is the less sample loss they experience when using only the solar energy harvester the results showed no sample loss while the energy loss over total harvestable solar energy had values ranging from 97 0 to 98 0 for all the 42 locations therefore for the given energy consumption budget from the water monitoring system the sampling interval can be significantly lowered to more finely measure water parameters using solar energy harvesting alone assuming cloud cover is the only factor limiting solar energy harvesting at the site this again emphasizes the high potential of the solar compared to hydro energy harvesting under this open sky assumption which is relaxed later in experiment 3 3 2 effect of sampling interval on hydro powered sensing system to better understand the hydro power harvesting potential for the 42 sites we first explored the smallest sampling interval leading to no sample loss for each site fig 7 12 sites were able to sample at every 5 min or smaller sampling intervals while they experienced no sample loss these sites have higher potential for hydro power harvesting compared to rest of the sites and had average harvestable energy ranging from 58 5 to 778 4 j per 5 min most interestingly three of such sites 04159130 04165710 and 09527597 were able to sample as frequent as every 1 min without experiencing any sample loss another important observation among such 12 sites with high hydro power potential is that there are five sites with average harvestable energy from 58 5 to 90 j per 5 min supporting sampling intervals as low as two or 3 min with no sample loss while for site 07374525 with an average harvestable energy of 567 8 j per 5 min the lowest supported sampling interval with no sample loss is higher 5 min due to presence of extreme low and high streamflow velocity periods at this site seven sites had more typical hydro power potential supporting minimum sampling interval with no sample loss in the range of six to 15 min the average harvestable energy for such sites ranged from 15 9 to 117 3 j per 5 min eight sites with lower hydro power potential could support sampling intervals between 16 and 66 min with average harvestable energy from 3 3 to 65 3 j per 5 min while two other sites were able to support much less frequent samplings with no sample loss i e sampling intervals of 152 and 235 min finally for 13 of the sites with extremely low or no hydro power potential regardless of how large the sampling interval was selected sampling loss was experienced no value for minimum sampling interval with no sample loss this was expected as we assumed 2 self discharge in one month meaning that after at most 50 months a battery with full initial charge would be completely depleted for a case where no sensing nor communication is performed while the entire simulation period for the simulations was 5 years 60 month this equates to no samples being taken at least for the 10 last months of the simulations for such sites we further explored the effect of sampling interval on hydro powered sensing system for three chosen sites with different hydro power potentials fig 8 shows the effect of sampling interval on sample loss and harvestable energy loss ratio for three different harvesting sites resulting from experiment 2 these three sites were selected based on their potential harvestable hydro energy representing a a low potential hydro harvesting site usgs 04092750 b a typical potential hydro harvesting site usgs 05537980 and c a high potential hydro harvesting site usgs 04165710 the low potential hydro harvesting site is in class 4 of table 2 meaning no energy loss occurred but sample loss did occur when the sampling interval was 5 min the average harvestable energy in 5 min for this site was 3 3 j for a 5 min sampling interval the sample loss was 86 8 while no energy loss was expected for sampling intervals greater than or equal to 52 min no sample loss was expected which made this site as a site with low hydro power potential for any sampling interval from one to 60 min no energy loss was experienced suggesting the battery may be oversized for this scenario the typical potential harvesting site 05537980 is in class 3 of table 2 meaning both energy loss and sample loss occurred when the sampling interval is 5 min the average harvestable energy in 5 min for this site was 37 4 j for a 5 min sampling interval the sample loss was 31 4 while the energy loss ratio was 47 4 however for sampling intervals greater than or equal to 15 min no sample loss was experienced which made this site as a site with typical hydro power potential but the energy loss ratio became even higher the high harvesting site is in class 1 of table 2 meaning energy loss occurred but no sample loss when the sampling interval was 5 min the average harvestable energy in 5 min for this site was 283 8 j for a 5 min water quality measurement sampling interval the sample loss was zero while the energy loss was 89 6 even by decreasing the sampling interval to 1 min no sample loss was observed which made this site as a site with high hydro power potential while the energy loss ratio was still considerable this suggests that for sites that have high potential for hydro energy harvesting it is possible to set a fixed sampling interval as low as 1 min to have a better temporal picture of the water quality dynamics within a system 3 3 effect of tree canopy on solar energy harvesting in experiment 3 we simulated the solar powered sensing systems under three different tree canopy scenarios i no tree canopy ii a dense deciduous tree canopy using the reduction factors from fig 2 and iii a dense evergreen tree canopy that leads to a constant reduction of 96 in the harvestable solar power this simulation assumed measurements are recorded every 1 min five times smaller than that of default configuration to consider a more power consuming scenario for applications where a very high temporal resolution of water quality dynamics is needed with a sensor station load of 14 38 j per measurement in fig 9 the average harvestable solar energy over the 1 min intervals percent of sample loss using only solar energy harvesting and percent of energy loss for the three different tree canopy scenarios are reported these data are reported for the three stations with low typical and high hydro energy harvesting potential introduced in the prior experiment under no tree canopy and dense deciduous tree canopy scenarios all three of the usgs sites experienced no sample loss while the deciduous tree canopy would result in about 75 reduction in average harvestable solar energy no sample loss in the dense deciduous tree canopy scenarios was experienced this is due mainly to the average harvestable solar energy per minute for the three locations still being about 3 4 greater than the energy consumption per minute of the sensing station and the reliable nature of solar power the periods that solar power is fully unavailable for a prolonged period of time are extremely rare however for the same three locations a dense evergreen tree canopy led to sample losses of 67 6 68 0 69 9 for the low typical and high potential hydro harvesting sites respectively this indicates an opportunity for using hydro power where available under the dense tree canopy conditions especially as shown for dense evergreen tree canopy to complement the solar power in order to have a more ubiquitous sensing of water bodies this opportunity is further explored in experiment 4 where hydro energy harvesting is used to complement solar energy harvesting and in the geospatial analysis of tree canopy cover in the conus where we seek to better understand fraction of streams where tree canopy is limiting solar energy harvesting 3 4 complementarity of hydro and solar power under low light conditions due to dense tree canopy in experiment 4 we explored how hydro power alongside solar power can lead to a denser sensing of water bodies fig 10 illustrates the average harvestable energy over the 1 min intervals percent of sample loss and percent of energy loss for the three different energy harvesting scenarios these energy harvesting scenarios only use hydro power combined use of hydro power and solar power in the case of dense deciduous tree canopy and combined use of hydro power and solar power in the case of dense evergreen tree canopy in the low potential hydro harvesting site 04092750 the harvestable hydro energy was very limited 0 67 j per 1 min so the effect of the combined use of hydro and solar in dense evergreen tree canopy was not significant sample loss reduced from 67 6 to 64 7 moving from solar with dense evergreen tree canopy therefore for low potential hydro harvesting sites like this the combined use of hydro power and solar power would not significantly contribute to a more robust sensing solution in the typical potential hydro harvesting site 05537980 the harvestable hydro energy was considerable 7 48 j per 1 min combined use of hydro and solar under dense evergreen tree canopy was beneficial at this site allowing sample loss to be reduced from 68 0 to 43 0 and energy loss to be increased from 0 to 7 7 compared to using solar power alone for the high potential hydro harvesting site 04165710 with a hydro power production of 56 8 j per 1 min use of hydro power not only reduced the sample loss from 69 9 to 0 but also increased the energy loss from 0 to 59 5 thus for such high potential harvesting sites the combined use of hydro and solar under dense evergreen tree canopy would reduce sample loss significantly potentially to zero and allow for increased sampling frequencies given the excess energy generated that is now being lost 3 5 geospatial analysis of tree canopy cover for waterbodies in the contiguous united states to better understand the potential impact of hydro energy harvesting for water bodies in the conus we performed a geospatial analysis to determine tree canopy coverage that would lead to lower light conditions and hydro energy harvesting opportunities fig 11 shows the mean percent tree canopy over the entire conus nhdplus network is 31 05 smaller streams with stream orders 1 5 show a mean percent tree canopy of 25 32 while larger streams with stream orders greater than 5 show 1 15 mean percent tree canopy cover this difference was expected as our analysis only intersected the percent tree canopy dataset with nhdplus reach centerlines still this is important because the majority of the stream network in the conus is 1 5 order streams 97 by stream length representing a significant opportunity for hydro energy harvesting approaches exploring this data spatially shows significant geographic patterns in tree canopy cover in the conus fig 12 presents the mean percent tree canopy over the nhdplus network averaged for each huc4 watershed southeastern northwestern and eastern regions show a high mean percent tree canopy over the nhdplus stream network meaning that available solar energy is limited for these regions relative to other regions as a result these areas are the best candidates for exploring the complementarity of other energy harvesting sources to supplement solar power northern regions with high percent tree canopy are particularly well suited given that experiment 1 showed high latitude stations having comparable solar and hydro power generation profiles even without considering tree canopy exploring the percentage of stream length with a given percentage of tree canopy across the huc4 watersheds in the conus reveals specific regions of the country where a high fraction of the streams have significant tree canopy coverage fig 13 shows the mean percent tree canopy over the nhdplus network the percent length of nhdplus network curve for the entire nhdplus network is shown as a blue dashed line and the percent length for each huc4 watershed is shown as a solid line this figure shows that for 11 of the stream length in the conus the mean percent tree canopy was greater than 80 among the 205 huc4 watersheds in the conus 19 all located in the southeastern us and with a mean percent tree canopy of 70 80 across the watershed had tree canopy cover greater than 80 for 48 70 of their stream length see a in fig 13 also for the same hucs 25 50 of the stream length had mean percent tree canopy greater than 90 see b in fig 13 other hucs tend to have less stream length at such high percent tree canopy levels suggesting the southeastern us is one of the highest potential regions for benefiting from hydro energy harvesting stream monitoring stations 4 conclusions in this study we compared energy harvesting using a solar panel versus hydro turbines and investigated their complementarity nature to make a self powered water monitoring station under low light conditions we simulated the hydro power using a commercially available hydro turbine and available real world streamflow velocity data collected at 42 stations by the united states geological survey usgs for streams across the conus the solar energy harvesting potential was modeled using a photovoltaic python library pvlib python an open source community supported tool holmgren et al 2018 and a specific solar panel setup to model solar power generation the required weather data obtained from solaranywhere were used as an input to pvlib python after estimating harvestable hydro and solor power a self powered discrete time water monitoring station was simulated adopting the energy generation store and consumption model from buchli et al 2014 then four different energy harvesting experiments were explored along with a geospatial analysis of tree canopy coverage for streams in the conus we found that for the selected harvesters solar power almost always outperformed hydro power potential under the assumption of no tree canopy in experiment 1 which only used either two hydro turbines or a solar panel solar energy harvesting potential ranged from 994 3 to 1460 8 j per 5 min with a mean value of 1242 6 j per 5 min however hydro power averaged harvestable energy in 5 min generated far less power ranging from 0 to 778 j per 5 min with an average of 87 5 j per 5 min for most stations solar energy harvesting significantly exceeded the potential for hydro energy harvesting by an average factor of 14 2x across all stations however a number of stations with consistently high streamflow velocities some of which are located at high latitudes and in regions with significant cloud cover experienced hydro energy harvesting potential near solar energy harvesting potential we found that using two hydro energy harvesters would result in a 45 3 sample loss on average across the 42 usgs stations used in the analysis but no sample loss for 10 stations with high velocities and more significant cloud cover assuming a fixed sampling interval of 5 min drawing 14 38 j per measurement using the same assumptions a solar powered water sensing system only would result in no sample loss for the 42 stations as expected solar power resulted in high energy loss rates of 97 6 on average whereas hydro power energy loss rates averaged 32 1 we found that solar power when unimpeded by tree canopy and other factors outside of cloud cover can often support sampling intervals much lower than 5 min the assumptions included a multi sensor water quality sampling station as described in the paper that includes energy harvester s power consuming units including controller sensor and communication units with a total drawing of 14 38 j per measurement and an energy storage unit we found that solar panels without energy reduction due to tree canopy cover could easily support sampling rates of 1 min and likely higher as well although sampling frequencies below 1 min were not investigated in this study there was greater variability across hydro powered sites in terms of harvestable energy a typical hydro powered site could support sampling rates closer to 15 min without sample loss which is still a significant benefit for understanding hydrologic dynamics a few of the 42 sampling sites analyzed could support a much higher sampling rate of up to one to 5 min without sample loss comparable to solar harvesting these sites had a high average streamflow velocity of approximately 0 6 m s on average throughout the study region and ranging from approximately 0 4 m s to 1 0 m s at each site introducing tree canopy into the analysis showed how dense tree canopy can significantly reduce solar energy harvesting potential for many of the sampling sites very dense evergreen tree canopy can lead to a 96 reduction in harvestable power making hydro power at such locations much more attractive we found that for sites with a high potential for hydro energy harvesting and dense evergreen tree cover sampling loss for a 1 min sampling interval can be reduced from 69 9 to zero by using hydro energy harvesting to complement solar energy harvesting for a similar site but with more typical hydro energy harvesting potential sampling loss can be reduced from 68 0 to 43 0 by conducting a geospatial analysis of streams and canopy cover during the leaf on season for the conus we found that 11 of streams by stream length have a mean percent tree canopy of over 80 for 19 huc4 watersheds located in southeast us where tree canopy is the highest 48 70 of the stream length in each huc4 has at least 80 canopy cover also in the same 19 huc4 watersheds 25 50 of the stream length has a tree canopy cover greater than 90 these 19 huc4 watersheds are an obvious target for further exploring the potential of hydro energy harvesting to complement solar energy harvesting based on the results of this study we conclude that hydro energy harvesting for stream locations with high flow velocities preferably above 0 6 m s on average and dense tree canopy especially evergreen tree canopy can complement or even replace solar energy harvesting approaches given dense tree canopy covers a considerable percent of us waterways this alternative source of energy for powering water monitoring stations should be further explored the complimentary nature of hydro energy harvesting to solar energy harvesting can not only increase sampling coverage in space but also be used to reduce the variability in solar energy harvesting e g cloudy days in order to increase sampling coverage in time further work exploring how dynamic power management strategies that optimize sampling interval given past and potentially forecasted energy available from solar and hydro sources could lead to the identification of opportunities to increase base sampling frequency with lower chance of incurring on sample loss for example during periods of high flow where important dynamics such as nutrient and pollutant transport are occurring an example of forecasting energy available from a hydro source is illustrated by ahmad and hossain 2019 where short term weather predictions and antecedent hydrological variables are used as inputs to an artificial neural network forecasting reservoir inflow and maximizing the hydropower generation such dynamic power management strategies that take advantage of streamflow forecasting capabilities would enable more ubiquitous monitoring of our water resources by not only covering currently unmonitored streams that have dense tree canopy or are within deep canyons but also new insights into temporal dynamics within riverine systems by increasing sampling frequencies during periods of high flow when solar power is unavailable but hydro power is plentiful software and data availability the pvlib python library a community supported tool can be found at https pvlib python readthedocs io en stable the developed python codes to perform this study including the codes utilizing the pvlib python library for this study can be found at https github com uva hydroinformatics hydrosolarenergyharvestingforwatermonitoring the codes necessary to perform the geospatial analysis estimating the tree canopy cover for streams in the conus can be found in maghami et al 2021 the data used in this study are from the sources listed in the following table while the stream velocity time series for the 42 usgs sites can be automatically downloaded using the scripts provided a zipped folder containing this dataset is made available in the same github repository for convenience the historical satellite derived estimated weather data for the same sites requires a license to access there is a free academic license available upon registering for an account on clean power research s solaranywhere portal https data solaranywhere com that was used in this study but this license does not allow for public sharing of the data the stream network for conus and percent tree canopy for conus are large datasets so were not replicated in hydroshare but can be downloaded using the data sources provided in the table below the subsequent processed datasets used in the analysis are published through hydroshare maghami et al 2021 the huc2 and huc4 watershed polygons for the conus can be easily accessed via the data source mentioned in the table below further information on how all the datasets are processed and used for subsequent steps can be found in the hydroshare resource and the github repository name of dataset data source data format data availability stream velocity time series u s geological survey https waterdata usgs gov nwis tab delimited ascii files free historical satellite derived estimated weather data clean power research s solaranywhere https data solaranywhere com tmy3 format csv files free with academic license stream network for conus national hydrography dataset plus nhdplus v2 0 https nhdplus com nhdplus nhdplusv2 home php gdb files free percent tree canopy for conus the usda forest service remote sensing applications center rsac https data fs usda gov geodata rastergateway treecanopycover img raster files free huc2 and huc4 watershed polygons for conus u s geological survey and usda natural resources conservation service https datagateway nrcs usda gov catalog productdescription wbd html gdb files free declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding was provided by the university of virginia school of engineering and applied science through a competitive research innovation award 
25844,as coastal circulation models have evolved to predict storm induced flooding they must include progressively more overland regions that are normally dry to where now it is possible for more than half of the domain to be needed in none or only some of the computations while this evolution has improved real time forecasting and long term mitigation of coastal flooding it poses a problem for parallelization in an hpc environment especially for static paradigms in which the workload is balanced only at the start of the simulation in this study a dynamic rebalancing of computational work is developed for a finite element based shallow water ocean circulation model of extensive overland flooding the implementation has a low overhead cost and we demonstrate a realistic hurricane forced coastal flooding simulation can achieve peak speed ups near 45 over the static case thus operating now at 80 90 efficiency keywords storm surge coastal flooding dynamic load balancing finite element modeling zoltan toolkit parmetis 1 software availability the code instructions and a test to run the software detailed in this work can be found by this private link https figshare com s 41827afadc318047e2ea the advanced hydrodynamic circulation adcirc code was originally developed in fortran77 by joannes westerink rick luettich and clint dawson since then it has been continuously updated using fortran90 and fortran77 and maintained by a community of developers adcirc is a commercial code www adcirc org that is otherwise free for research and academic purposes adcirc dlb which is a branch of adcirc requires a linux based system with distributed memory architecture that uses the message passing interface adcirc dlb must be compiled with an intel fortran 16 0 or more recent intel fortran compiler either mpich or mvapich and it requires the zoltan toolkit part of the trilinos package https cs sandia gov zoltan and parmetis http glaros dtc umn edu gkhome metis parmetis overview 2 introduction in two dimensional 2d finite element modeling of wind and tidally driven coastal circulations a portion of the computational domain is included above the local mean sea level lmsl state to simulate coastal flooding to obtain a high fidelity model while keeping computing cost relatively low an unstructured mesh composed of elements with highly variable sizes is used to represent the large horizontal scale separation i e o 10 m o 100 km of tide storm surge and coastal flooding processes gorman et al 2008 marks et al 2017 roberts et al 2019b predictions with these unstructured meshes are used to provide crucial information for coastal hazard assessment and design e g cstorm project cialone et al 2017 quetzalc√≥atl et al 2019 water level guidance e g iflood khalid and ferreira 2020 and emergency management operations staneva et al 2016 blanton et al 2012 2018 dresback et al 2013 to simulate coastal flooding in the region of focus the mesh often contains an extensive floodplain extending up to an elevation of 10 m 15 m above local mean sea level fig 1 bunya et al 2010 blanton et al 2012 teng et al 2017 on the floodplain high resolution elements of size 10 50 m enable a representation of the fine horizontal geometric length scales and complex land cover variability that control coastal inundation patterns e g dietrich et al 2010 forbes et al 2010 hope et al 2013 sebastian et al 2014 some works demonstrate that relatively coarse mesh resolution e g 50 m overland may result in inaccurate inundation area bilskie and hagen 2013 bilskie et al 2015 cobell et al 2013 and inaccurate velocities neelz and pender 2008 as coarser mesh resolution may alias vertical features such as elevated roadways and levees that can largely alter inundation patterns as a consequence of the high resolution elements that are used to represent overland features an often disproportionate number of degrees of freedom dofs in the modeling system become located overland for example in a modeling system used for operational coastal flooding predictions fleming et al 2008 technology riverside inc and aecom 2015 approximately 55 percent of the vertices are above the lmsl state at initialization the transient nature of coastal flooding further implies that although the overland regions are included due to uncertainty in the magnitude and location of potential storms the majority of overland dofs will never be flooded during any given simulation the computationally expensive aspect of modeling coastal flows over land have motivated the development of a broad range of numerical strategies chen et al 2003 casulli 2019 behrens and bader 2009 leveque et al 2011a taeb and weaver 2019 androsov et al 2019 wittmann et al 2017 ginting and mundani 2019 sanders et al 2010 traditionally coastal flooding is modeled using a pre determined highly refined mesh with a logic based wetting drying approach medeiros and hagen 2012a luettich and j westerink 1999 candy 2017 sanders et al 2010 another approach is to start the calculation with an initially coarse mesh and then adaptively refine the mesh as the event propagates leveque et al 2011b adaptive methods perform well for the simulation of transoceanic tsunamis storm surges and the associated coastal flooding in which the background state is quiescent before the event occurs behrens and bader 2009 leveque et al 2011a however for predictions of total water levels the background tide and wind driven signal continuously interact complicating mesh refinement strategies multi grid approaches e g taeb and weaver 2019 also have been shown to reduce wall clock times for detailed coastal flooding simulations by simulating with a hierarchy of meshes but require an extensive mesh development and coupling approaches to merge solutions instead of using a pre determined sufficiently fine mesh sub grid scale modeling techniques e g casulli and walters 2000 casulli 2019 candy 2017 kennedy et al 2019 aim at improving the accuracy of an under resolved mesh by incorporating these geometries through porosity functions this methodology is promising but there are limitations on the practical ability to provide closures for all subgrid physics kennedy et al 2019 the topic of dynamic load balancing has also been applied for the prediction of overland flooding to improve computational efficiency ginting and mundani 2019 sanders et al 2010 wittmann et al 2017 ginting et al 2020 dynamic load balancing in the context of flooding is important for computational efficiency as the computational costs of wet and dry cells are often nonequivalent wittmann et al 2017 ginting and mundani 2019 ginting et al 2020 and inherently time varying as a flood event progresses sanders et al 2010 one strategy presented in wittmann et al 2017 was to vectorize the flux calculation and mask out the dry cell calculations resulting from wetting and drying in a shared memory parallelism finite volume solver ginting and mundani 2019 using a shared memory parallelism finite volume approach with a space filling curve to adaptively distribute meshes with a weighted dynamic load balancing strategy ginting et al 2020 further advanced by performing dynamic load balancing with a static domain decomposition in a hybrid openmp mpi parallel computing environment a finite volume solver called parbrezo with domain decomposition mpi parallelism showed a 97 reduction in execution time in a simulation of regional hurricane driven storm surge on an unstructured grid by using a weighted domain decomposition approach and a static grid partitioning technique sanders et al 2010 by considering the challenges and recognizing the complexity involved in mesh design for coastal ocean modeling bilgili et al 2006 gorman et al 2007 2008 roberts et al 2019a a dynamic load balancing approach is developed to reduce the computational cost of modeling wind induced inundation of the floodplain using a pre determined mesh with a widely used finite element model called the advanced circulation model luettich and westerink 2004 our approach relies on a cell weighting approach similar to that of ginting and mundani 2019 and sanders et al 2010 the approach uses dynamic grid partitioning and is capable of redistributing all components of the mesh during execution between cores to reflect the time varying movement of the wet dry boundary we show how our approach more efficiently utilizes computational resources and is minimally invasive in the sense that it does not require any modifications to pre existing models to obtain speedups to facilitate dynamic load balancing in a highly scalable parallel element solver such as adcirc we rely on the zoltan toolkit boman et al 2012 and parmetis karypis and kumar 1998 to provide essential functionality to the application the rest of this article is organized as follows first we describe our strategy to reduce the computational cost associated with the floodplain by integrating the zoltan toolkit within a finite element solver and developing an algorithm to dynamically remove the floodplain from the computational problem then we describe the effect and behavior of the load balancing approach when it is applied to an idealized case study finally we test our implementation in a real world coastal flooding application in north carolina u s a the paper concludes with a discussion on the key findings 3 methods 3 1 adcirc hydrodynamic model the dynamic load balancing application is built around the advanced hydrodynamic circulation model luettich and westerink 2004 to improve the computational performance of both existing and future modeling systems that rely on adcirc adcirc http adcirc org has become one of the most widely used community modeling platforms for storm surge coastal flooding predictions across academia united states governmental agencies and the private sector this is due to its inclusion of critical physics e g sub grid scale features such as levees and floodwalls explicit inclusion of spatially varying land cover and its influence on both surface wind conditions and bottom stress coupled waves surge tides and runoff interfaces to multiple meteorological model forcings accurate numerics and optimization for high performance computing furthermore an active development community continues to advance the model s capabilities adcirc solves the shallow water equations swes using the generalized wave continuity equation gwce kellogg 1988 lynch and gray 1979 the swe are discretized by using a continuous galerkin cg finite element fem scheme in space and a finite difference scheme in time and the method is formally second order accurate luettich and westerink 2004 a parallel version of adcirc is implemented for a distributed memory parallel system with the message passing interface mpi the solver is scalable and demonstrates linear speed up at cpu cores of 10 000 and greater tanaka et al 2010 adcirc represents the wetting and drying process on an elemental basis in which elements must either be fully wet or fully dry through logic based conditions luettich and j westerink 1999 dietrich et al 2004 the wetting and drying process creates a moving boundary in the computational domain that can move at most one element per time step due to the order of the wet dry logical operations while many different and potentially more advanced wetting and drying methods exist greenberg et al 2005 bates and hervouet 1999 k√§rn√§ et al 2011 candy 2017 warner et al 2013 medeiros and hagen 2012b the adcirc model with its wetting and drying approach is used by various united states government and state entities for coastal design and risk assessment among other use cases e g cobell et al 2013 both adcirc v53 adcirc and adcirc dynamic load balancing adcirc dlb source codes are compiled identically using intel fortran 17 1 compiler with the o2 optimization strategy and the mvapich implementation of message passing double precision arithmetic is used for all calculations besides the dlb capability there are no differences in the source code implementation between the two versions of the code 3 2 dependencies the zoltan toolkit is used as a data parallel programming library that we built the load balancing algorithm upon devine et al 2002 boman et al 2012 built with mpi zoltan provides configurable mpi based unstructured grid data partitioning through an interface to several state of the art graph and hypergraph partitioners it also provides a scalable data exchange algorithm that uses distributed data directories to locate non local data pinar and hendrickson 2001 devine et al 2002 zoltan requires users to define a set of call back functions to facilitate the neighbor exchange of enqueued data in this work we choose to work with a graph based partitioner parmetis karypis and kumar 1998 due to its widespread usage and its integration with the zoltan toolkit specifically we use parmetis v4 0 3 program with the implementation of the k way multi level graph decomposition algorithm parmetis s part k way graph decomposition algorithm is called via zoltan s interface 3 3 load balancing strategy our strategy to reduce the cost of modeling the coastal floodplain involves trading a computational workload balance for a memory imbalance in this strategy the decomposition is done in such a way that the load is balanced with respect to the wet state component of the mesh as a consequence the majority of the dry state component of the mesh belong only to a few subdomains hence introducing memory imbalances as shall be explained below in section 3 5 when the majority of dry state mesh are grouped together the computing time can be reduced through a rearrangement of data in memory the data rearrangement leads to an offline portion of the mesh that is not involved in timestepping and an online portion that is fig 2 at the start of the simulation the wet dry boundary is located near the cold start water level depending on the extent of the floodplain this enables the majority of dry state mesh data to be located in the offline portion of the subdomains and a theoretical work savings to be obtained through our load balancing approach fig 2 as the movement of the wet dry boundary begins to reach the boundary of the offline state subdomains the computational workload may need to be rebalanced either to maintain a target performance level and or to satisfy constraints imposed by our approach in this way the simulation can be thought of as a sequence of epochs each composed of a rebalancing phase and a subsequent calculation phase in the case that all mesh data are wetted the load and thus the theoretical speed of the parallel application ideally should become approximately equivalent to that of static adcirc 3 4 mesh partitioning to form the decomposition the mesh is represented as an undirected graph vertices of the graph are assigned a non dimensional weight proportional to its anticipated computational expense in a similar manner to ginting and mundani 2019 ginting et al 2020 to reflect communication costs and data dependencies the edges of the graph are given a non dimensional weight as well elements and vertices are weighted depending on an offline or online status which is based on a set of user defined criteria elements that have an offline status are weighted with a value of 0 whereas online elements are weighted with a value of 1 graph edges that connect two offline elements are weighted with the value 1 whereas graph edges that connect either two online elements or one online element to an offline element are weighted with a relatively larger value of 1 000 graph edge weights with a value of 0 are not permitted in parmetis parmetis then uses these weights in partitioning the graph into a number of subdomain the disparity in graph weights between the online and offline portion of the mesh represent their true cost algorithm 1 mesh partitioning algorithm image 1 to decompose the mesh a sequence of migration and query operations are performed algorithm 1 we stress here that the entire mesh both dry and wet state components can be redistributed to new subdomain configurations each rebalancing epoch to start the application slices of the finite element mesh along with their weights are passed to a call back function that makes calls to parmetis within the zoltan toolkit elements are first migrated so that each element of the mesh is owned by only a core query operations are performed using the distributed data directories provided in zoltan note that data directories for both vertices and elements are over allocated by a factor of three times the initial number of vertices or the number of elements on each core respectively to avoid reallocation operations during dynamic load balancing this over allocation factor was found to be sufficient to avoid reallocations of data directories while avoiding hash collisions vertex ownership is formed implicitly by the element ownership since three vertices of the element belong to the element s owner on the border of the subdomain only the relatively higher numbered core is designated an owner of the vertices thereafter a single layer of vertices are imported on the relatively lower numbered core in each pair of adjacent subdomains the duplicated vertices that border all subdomains are termed halo vertices 3 5 array rearrangement after algorithm 1 the arrays representing the mesh are rearranged according to their online offline status fig 3 online elements are placed contiguously in memory at the start of the array and offline elements are then located in memory after the last online element in this memory arrangement the number of loop iterations can be reduced by the number of offline elements on each core by reducing the loop range in this way a speed up of the simulation can be achieved provided work is balanced across all cores the reduction in loop iterations by array rearrangement is referred to as loop clipping when the elemental loop range is reduced all of the online element s vertices must be accessible this is accomplished by iterating through the online state elements and placing their associated vertices at the start of the array while ensuring that vertices are not duplicated the process is repeated for the offline state elements array placing these offline vertices after the last online state vertex fig 3 loop extents are set to the memory location of the online element and online vertex with the respective adjacent memory location labeled offline 3 6 rebalancing strategies to trigger a rebalance a set of vertices called checkpoints shared between the boundary of the online and offline state portion of the mesh are checked for wetting each time step fig 2 b c if any checkpoint vertex is wetted the simulation must pause and rebalance by executing algorithm 1 note that at a minimum the buffer zone is one element wide and since the wetting drying algorithm can at most only move the wet dry boundary one element per timestep section 3 1 this avoids a scenario where the wetting drying front would not advance landward in the same manner as adcirc we ve found that it is not possible to set all dry elements and vertices offline instead a zone of elements buffering the region between the checkpoint vertices and the online component of the domain is created to control the frequency of rebalance events as is shown later the buffer zone enables this approach to be practical for coastal flooding predictions we implemented the following rebalance heuristics 1 distance if a vertex s nearest distance from the wet dry boundary exceeds a user defined threshold db in meters the vertex is set to an offline state if the three vertices of an element all exceed db then the element is set to an offline state an assumption here is that locations in close proximity to the shoreline are most likely to be flooded we stress that the distance to the shoreline front does not necessarily imply that the element will be flooded as the wetting drying algorithm advances the front based on a balance between the hydrodynamic pressure gradient force across an element and friction which is related to the forcings topography and landcover nearby the element luettich and j westerink 1999 dietrich et al 2004 2 topographical a vertex s elevation must be located above a user defined value hb in meters above the model s reference datum to be set to an offline state if the three vertices of an element all exceed hb then the element is set to an offline state in practice the hb can be selected based on the local inter tidal range the definition of the criteria to determine online and offline state vertices are globally defined among cores to prevent excessively frequent rebalance events for example every simulation timestep each time the checkpoints of the buffer are wetted hb is elevated by 10 of its existing value this is not necessary for db since the nearest distance is calculated from a wet dry boundary which is moving in space the selection of increasing 10 hb each rebalance event was based on the idea that we want to gradually lift the checkpoint vertices away from the wet dry front as flooding occurs to maximize the amount of dofs set to an offline state as more flooding occurs hb elevates a larger increment and avoids a rebalancing every timestep which would otherwise occur during a highly energetic flooding event a key assumption in the formation of the buffer is that the solution does not exhibit numerical artifacts and or instabilities it is noted that the shoreward movement of the wet dry boundary was not used as a factor in determining rebalance events however the user does have the option to schedule rebalance events which can be used to lower the buffer zone shoreward after a flooding event has passed note that improving the computational efficiencies post storm through lowering the buffer shoreward could become more important for real time predictions due to operational constraints 3 7 performance metrics a set of statistics are calculated to assess the performance of adcirc dlb the total wall clock time t of the simulation is the sum of the time spent computing tc and the time spent rebalancing trb 1 t t c t rb where tc and trb are summed over the entire simulation specifically trb represents the cumulative time spent performing the parallel partitioning algorithm while tc represents the cumulative time spent in timestepping the speed up over the static version of the code is calculated as 2 su o b s t s t a t i c t d l b in which tdlb is the wall clock time spent timestepping for a given simulation computed with adcirc dlb and tstatic is the wall clock time spent timestepping using adcirc the theoretical maximum speed up sumax is calculated by dividing the total number of vertices vtotal by the average number of wetted vertices vwet for a simulation 3 su m a x v t o t a l 1 n t t 1 t n t v w e t t where v w e t t is the number of wet vertices at timestep t nt is the number of simulation time steps following this if 50 of the domain is wet throughout the entire simulation the simulation would have a maximum speed up factor of 2 0 the sumax represents a maximum potential speed up assuming 1 the problem is perfectly load balanced 2 it has zero communication overhead 3 dry state vertices can be completely ignored from the computational problem and 4 there is zero rebalancing cost following this the speed up efficiency sue is computed as the ratio of the suobs to sumax 4 s u e s u o b s s u m a x where sue 1 100 implies a perfectly efficient calculation note that this efficiency metric is subjective to the modeling scenario which influences the amount of wetting and drying in the domain nevertheless if there are fewer dry vertices then there will be a smaller potential for speed up and thus a smaller sumax however this doesn t necessarily translate to a smaller sue that has to be earned by minimizing the costs of the dry vertices and the time spent in the dynamic load balancing e g tdlb operation as much as possible to investigate the performance of the application the load imbalance is calculated load imbalance is defined as the ratio of the maximum load l m a x k calculated over all cores divided by the average load l a v e r a g e k calculated over all cores 5 r i m b 1 k k 1 k r i m b k r i m b k l m a x k l a v e r a g e k 1 for all k epochs the quantity l is calculated as the number of online state vertices per core all cores ideally should maintain an equal load with an r i m b k 0 in order to ensure that the idle computing time is at a minimum for efficient parallelism inter processor communication volume should be minimized in adcirc messages are exchanged several times during a given timestep at the halo vertices of the subdomain the inter processor communication volume is estimated through the surface area to volume sv ratio per k th epoch which approximates the relative amount of communication cost as compared to the computing cost 6 s v 1 k k 1 k 1 n s n 1 n s s v n k number of halo vertices on subdomain n total number of vertices on subdomain n for ns subdomains over k epochs the number of halo vertices on a given subdomain in eq 6 is influenced by the shape of subdomains the number of vertices and whether the subdomains are connected in the computational domain as the number of vertices per core is reduced with the usage of more processors s v increases implying more communication over computation 4 results simulations were executed on the computational hydraulics laboratory s computer cluster called aegaeon https coast nd edu aegaeon contains 83 compute nodes of dual 12 core e5 2680 2 50 ghz haswell processors 1 992 cores in total each node contains 64 gb of random access memory that is shared among each node s 24 cores the nodes are connected via a high speed 56 gb infiniband network file input output was disabled i e no logging and no output file writing however for the hurricane irene test case meteorological forcing files winds and surface pressure data needed to be read into memory every 15 simulation minutes all timing results were repeated three times and the average is reported 4 1 simple tide on an idealized floodplain the dlb capability is evaluated first for an idealized problem with a channel and floodplain this problem is referred to as the ideal channel and is selected because it allows for a tidal signal to inundate and recede on a gradual linear sloping beach with a narrowing channel along its centerline to provide variations in horizontal directions while the problem has a predictable wet dry boundary the point of the experiment is to demonstrate the application of dynamic load balancing and some of the associated concepts 4 1 1 model and setup a channel and floodplain fig 4 are represented with an unstructured mesh with 64 415 vertices and 127 784 elements the topography bathymetry is characterized by an axially symmetric channel that has a parabolic cross sectional profile the model contains a large floodplain and it has an initial distribution of vertices with approximately 66 dry and 34 wet at a cold start state bathymetry varies linearly and gradually with a constant slope of approximately 0 002 from 8 m to 2 m above the model s lmsl onto the floodplain element sizes vary from 75 m near the open boundary to 15 m along the shoreline and then coarsens to 35 m overland a tide with an amplitude of 1 m and a period of 12 42 h i e semi diurnal m2 frequency and a ramp of 0 5 simulation days is prescribed along the open boundary fig 4 this boundary forcing generates a fluctuation of 1 m above and below lmsl with episodic inundation covering 13 240 vertices 20 55 of the vertices hatched area in fig 4 this fluctuation of water levels leads to a maximum of 54 30 of total vertices inundated in the mesh the application of the elevation specified boundary with a 0 5 simulation day ramp generates a wet dry boundary that rises in a rigid fashion and mimics a cresting tidal inundation in a small enclosed estuary the simulation uses a time step of 0 5 s with a maximum courant number less than 0 25 and the gwce is solved with an explicit time scheme with a mass lumping approach for this problem the maximum theoretical speed up c f eq 3 is 2 56 the ideal channel problem is run using five different core configurations i e 3 6 12 24 48 cores each core configuration is run with a sequence of progressively wider buffer configurations four configurations based on nearest distance to the wet dry boundary are considered db 50 m db 100 m db 200 m db 500 m fig 4 likewise four configurations are based on topography i e meters above the lmsl hb 0 10 m hb 0 20 m hb 0 50 m and hb 1 0 m an infinite size d b h b configuration that represents the static case using the decomposition algorithms was also tested each time a rebalance event is triggered in the depth based configurations hb is increased by 10 of its current value while the db remains fixed c f section 3 6 4 1 2 effect on decomposition compared to the case when all elements vertices are weighted equally the db 50 m configuration greatly increases the number of vertices in subdomains located in overland regions fig 5 c d consequently the number of total vertices in subdomains with predominately online vertices are reduced which reduces the computational problem size the number of dry state online vertices are greatly reduced per core while the distribution of online vertices remains well balanced fig 5 b as db is increased more vertices are set offline and this reduces the number of vertices in subdomains with predominately online vertices 4 1 3 timing improvements adcirc dlb demonstrates approximately linear scaling in all cases considered fig 6 a significant speed ups were measured e g up to suobs 2 44 but became less significant and more inefficient with the usage of more cores in configurations where a small value of db is considered the time spent rebalancing became similar in value to the total simulation time diminishing the speed up overall adcirc dlb performed slightly slower than adcirc in the db hb configurations it was found that because adcirc dlb decomposes the dual graph whereas adcirc decomposes the nodal graph the decomposition of the dual graph produced slightly greater load imbalances resulting in marginally slower execution times adcirc dlb produced 31 out of 32 faster simulations with speed up factors that ranged between suobs 1 10 to suobs 2 44 fig 6 c distance and depth based buffer performed similarly although changes to db produced greater changes in timing results than changes to hb we observed that configurations using small distance based buffers experienced far more rebalance events as the wet dry boundary initially advanced shoreward from its cold start state than the depth based buffer configurations this resulted in more variation in timing results the fastest simulation was hb 50 cm and produced a speed up factor that ranged from 2 44 to 1 42 when considering 3 to 48 core configurations respectively the highest speed up of suobs 2 44 was achieved as a result of minimizing trb while maximizing the sumax for instance configurations with a hb 0 50 m had greater trb which resulted in slower simulations despite that their theoretical maximum speed up sumax was greater the time spent rebalancing represented maximally 16 of the total simulation time while the majority of simulations 31 out of 32 spent 3 of the total time in rebalancing operations fig 6 c while the time spent in rebalancing operations can be considered relatively small trb does not exhibit scalability fig 6 d as rebalancing requires a number of communication and query operations c f algorithm 1 the slowest performing experiments used buffer configurations that initially set the most data offline which leads to more rebalancing events and thus far greater trb for instance the db 10 m corresponded to the narrowest buffer and resulted in a slow down of 1 42 using 48 cores due to trb fig 6 b for all simulations that were faster than adcirc their efficiency sue ranged from 53 to 96 fig 7 the 3 and 6 core configurations were consistently more efficient than the 12 and 24 core configurations the most efficient sue 94 simulation was measured using 6 cores with the hb 50 cm buffer while the db 200 m configuration produced the second most efficient simulation with a maximum sue 92 the rest of the configurations produced lower sue as the hb elevated and db increased as less of the theoretical speed up was realized subtracting trb from t demonstrates that the theoretical speed up increases as more data is set offline but at the cost of more time spent rebalancing fig 7 a and b when trb is removed from the timing statistics the most efficient simulations are those that initially set the greater component of the mesh offline i e db 50 m and hb 10 cm fig 7 a however the opposite is observed when trb is included which indicates that a configuration that minimizes both t and trb exists with a buffer configuration near to the hb 50 cm and db 200 m fig 7 b in all experiments compared to adcirc decompositions generated by adcirc dlb had larger s v by approximately 1 whereas less consistent differences for r i m b were measured fig 8 configurations that set more data to an offline state produced a consistent increase to the s v by maximally 3 5 the increase in s v is expected considering that the size of the online component of the problem is reduced through array rearrangement and shortening of loops fig 8 c the 48 core setup leads to buffers configurations falling below the scaling limit as was reported in tanaka et al 2010 similarly to s v a clear increase to r i m b is measured by 6 8 as compared to adcirc dlb and also increases as more cores are used unlike s v however the disparity in r i m b between different buffer configurations is greater there is no clear relationship between the number of vertices set to an offline state and r i m b 4 1 4 comparison with static adcirc a comparison of water levels between adcirc and adcirc dlb is shown in fig 9 for the best performing experiment i e 6 cores hb 50 cm in the ideal channel test case adcirc dlb solutions are numerically identical to that of adcirc and the timing of wetting and drying is unaltered by the dynamic load balancing fig 9 4 1 5 summary of experiment the best performance suobs 2 44 was obtained using a depth based buffer of hb 50 cm speed up factors had relatively larger variation from suobs 1 10 to suobs 2 44 depending on the configuration of the buffer trb represented less than 3 of the total simulation time with the exception of db 50 m where it created a significant slow down overall db had more variability than hb in trb to its configuration than the depth based buffer the application of dlb produced decompositions that had greater surface area to volume s v and greater imbalance factors r i m b by 1 3 but this did not appear to impact parallel performance significantly in general the application of dlb reduced the problem size fig 8 c and this can lead to higher levels sv and more rimb based on these findings for applications our recommendation is to use a depth based buffer configuration with an initial height of 0 50 cm which was the best performing experiment further when selecting the buffer it is important to ensure that the application of dlb does not reduce the problem size below the scaling limit of adcirc 4 2 flooding from hurricane irene in north carolina the performance of adcirc dlb is further assessed in a hurricane driven coastal flooding simulation of hurricane irene 2011 using an extensively validated mesh of the mid atlantic united states region this mesh was validated during hurricane isabel 2003 blanton et al 2018 and hurricane irene dresback et al 2013 and the model is used for real time predictions for the north carolina forecasting system blanton et al 2012 additionally it contains an extensive floodplain unlike the ideal channel problem this test problem features an irreguarly moving wet dry boundary forced by a combination of meteorological forcings and astronomical tides interacting with observed topography bathymetry datasets hurricane irene impacted the mid atlantic region as a category 1 hurricane with 10 min sustained winds of approximately 78 mph on august 27 28 2011 seroka et al 2016 measured water levels of approximately 3 m above lmsl were observed on the westward side of the cyclone s track near the tar pamlico sound and neuse river basins in north carolina fig 10 dresback et al 2013 elsewhere in the pamlico sound peak water levels of 1 m 2 m above lmsl were observed 4 2 1 model and setup the mesh used for this experiment is referred to as north carolina v9 nc9 and contains 608 114 vertices and 1 200 767 elements with the finest resolution located nearshore of approximately 30 m and expanding to approximately 500 m in size over the floodplain dresback et al 2013 along the south atlantic bight a patch of elements were modified to improve their quality otherwise the mesh was identical to the original at a cold start state approximately 56 of the mesh vertices are dried in the area impacted by hurricane irene the tides range between 0 40 and 0 60 m above lmsl approximately 10 12 of the floodplain vertices are flooded during an average tidal cycle fig 10 the extent of the floodplain for the nc9 mesh is substantial and was designed based on a historical review of flooding in the region by the developers of the model blanton et al 2012 in areas nearby large rivers such as the tar and neuse rivers the inland extent of the model extends up to the 8 m elevation contour above mean sea level outside of the riverine areas the model domain extends up to the 15 m elevation contour above sea level the sumax for this particular setup was 1 97 the setup used in this section resembles the operational simulation used in fleming et al 2008 a simulation of 8 days explicit numerical scheme 0 5 s time step atmospheric and tidal forcing and point based outputs of free surface elevation wind and pressure hindcast fields from ocean weather inc owi were used to force the model between the dates of august 21 2011 12 00 00 utc to august 29 2008 00 00 00 utc at a time increment of 15 min temporally consistent tidal processes are included in the simulation by specifying elevation boundary conditions on an open ocean segment using the tpxo9 1 atlas egbert and erofeeva 2019 for four major semi diurnal m2 n2 s2 k2 and four major diurnal tidal constituents k1 o1 p1 q1 forcings are ramped from a cold start state using a hyperbolic tangent function over the first two days of the simulation to avoid exciting transient modes in the study region free surface elevation are recorded every 6 simulation minutes from simulation days 2 8 at 74 rapidly deployed gauges by the united states geological survey service usgs and an additional 8 national oceanic service nos gauges depicted in fig 10 4 2 2 comparison with measured data adcirc dlb solutions are effectively identical to that of adcirc to within small differences often seen when running adcirc with different core configurations figs 11 and 12 these differences are possibly introduced through the threshold based wet dry logic dietrich et al 2004 compared to observations of measured water levels the timing and peak of simulated water level responses at the eight noaa oceanic service nos gauges in the region were accurately captured with the exception of stations 1 and station 4 which both under predicted the water level response fig 11 further good agreement between simulated results and high water mark observations at 74 usgs sensor in the neuse tar and pamlico river basins are measured 4 2 3 timing improvements the model setup is executed on five core configurations more specifically 60 120 240 360 480 cores for adcirc dlb we use the best performing buffer configuration in terms of sue and suobs from section 4 1 which was hb 50 cm significant speed ups that ranged from 1 32 to 1 84 with 480 to 60 cores were measured respectively fig 13 this implies a reduction in wall clock times between 10 and 190 min with the usage of 480 and 60 cores respectively trb were relatively negligible occupying less than 58 s of the total simulation time all simulations rebalanced for a total of 22 times as water levels flooded the floodplain trb demonstrated weak scalability and reduced from trb 58 s to approximately trb 40 s using 60 to 480 cores respectively the measured speed ups can be considered highly efficient with a sue ranging from 69 to 93 our most efficient configuration sue 93 occurred with the 120 core setup the 60 core configuration performed similarly with a sue 87 8 configurations that used more than 120 cores are less efficient the 480 cores setup is the least efficient simulation with a sue 69 similar to the timing results obtained in section 4 1 trb did not negatively affect the model efficiency and only a small reduction of efficiency 0 1 0 3 was measured that could be attributed to the rebalancing fig 14 in tanaka et al 2010 the scaling limit for the adcirc solver with a similar core configuration to ours occurred when there were on average 1 000 vertices per core our results reflect a similar pattern with a deviation in the scaling curve fig 13 the application of array rearrangement and loop clipping reduces the problem size to below the scaling limit in the 360 and 480 core configurations which explains the measured deviation from an optimal scaling rate fig 13 a in contrast to the ideal channel case c f section 4 1 the movement of the wet dry front is complex during the coastal flooding event with substantial shoreward movement of the wet dry front in the wetland environments immediately north of the neuse river and in contrast limited shoreward advancement of the wet dry front along the steeper banks and tributaries of the neuse river fig 15 in spite of the irregular movement of the wet dry front the depth based buffer configuration criteria remains robust and enables a significant speed up of the overall calculation while triggering relatively few rebalancing events 5 discussion and conclusion the aim of this work is to reduce the wall clock times spent modeling wind driven coastal flooding on unstructured triangular meshes using the adcirc solver regional coastal adcirc meshes often contain relatively large amounts of dry state vertices to represent the finely detailed nature of the coastal floodplains considering variable resolution unstructured mesh model development is both challenging and time consuming modelers cannot design a stable and robust modeling mesh system that is hand crafted for each storm that may occur in a given area as a result all dry state floodplain vertices are not actively solved for and this make the model s parallel execution susceptible to large work imbalances and inefficiencies our solution involves in memory re decomposing the unstructured mesh in parallel based on the moving domain boundary determined by the wet dry elemental state each time the problem is re decomposed local arrays of vertex and elemental data are rearranged to reduce the extent of loops involved in the calculation leading to an acceleration of the program the third party libraries zoltan boman et al 2012 and parmetis karypis and kumar 1998 were used to implement adcirc dlb the performance of adcirc dlb as compared to static adcirc was studied given a range of computational resources 3 480 cores in both an idealized problem and a hurricane scenario overall adcirc dlb exhibited similar scaling behavior to adcirc with a linear reduction in simulation time reductions between approximately 20 50 to the total wall clock time of a realistic coastal flooding simulation were achieved with approximately 70 90 efficiency compared to a theoretical speed up estimate however the reduction in the problem size associated with dlb given the same amount of computational resources can lead to relative calculation slow downs to the same calculation computed on adcirc when the minimum number of vertices per core falls below 1 000 in this part of the scaling regime the communication cost dominates the computing cost and dynamic load balancing offers little gain performance was also significantly affected by the criteria used to determine what portion of the mesh was set as offline criteria based on elevation hb was more efficient than using a minimum distance criteria db from the wet dry boundary minimum distance criteria produced more rebalance events as the tide advanced shoreward compared to the depth based criteria which leads to more time spent rebalancing and overall slower performance based on the results our recommendation is to remove as much of the inter tidal zone using a topographic criteria i e hb 0 50 m or a depth similar to the local tidal range and let the program automatically lift hb as wind driven coastal flooding occurs we demonstrated that adcirc dlb does not sacrifice the well established accuracy available in the adcirc solver parallel scalability or require new meshes models to be developed future work intends to apply adcirc dlb to operational storm tide forecasting systems including regional models e g fleming et al 2008 as well as emerging global ones e g pringle et al 2021 seroka et al 2020 while some current research is focused on more efficiently incorporating coastal floodplains into modeling systems to avoid components of the mesh that are rarely flooded through mesh decimation techniques bilskie et al 2020 our dynamic load balancing concept may provide a different approach to coastal ocean model development for instance dynamic load balancing could reduce the need to develop many regional models with carefully designed floodplains by instead enabling the user to develop one substantial modeling domain with a large overland extent while incurring minimal computational overhead for example a next generation comprehensive modeling system of the east and gulf coasts of the united states with fine resolution overland would imply that the vast majority of vertices would remain in a dry state for any given event in this case the size of the problem and the number of dry state vertices would make it a suitable candidate to take advantage of adcirc dlb to accelerate the calculation funding this material is based upon work supported by the u s department of homeland security under grant award number 2015 st 061 nd0001 01 and national science foundation grant award number nsf aci 1339738 the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies either expressed or implied of the u s department of homeland security or the national science foundation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank the two anonymous reviewers who helped improve the quality of the manuscript we thank ocean weather inc for allowing us to use their meteorological forcing inputs for the hurricane irene test problem we thank dr brian blanton at renaissance computing institute at the university of north carolina at chapel hill for providing the mesh and input files used in the hurricane irene test problem kr prepared the manuscript designed and implemented the coding upgrades into adcirc designed and performed the experiments and conducted the analysis of the results jcd dw and jjw provided feedback throughout the project supervised the research and improved the manuscript wp improved the manuscript presentation and provided critical feedback 
25844,as coastal circulation models have evolved to predict storm induced flooding they must include progressively more overland regions that are normally dry to where now it is possible for more than half of the domain to be needed in none or only some of the computations while this evolution has improved real time forecasting and long term mitigation of coastal flooding it poses a problem for parallelization in an hpc environment especially for static paradigms in which the workload is balanced only at the start of the simulation in this study a dynamic rebalancing of computational work is developed for a finite element based shallow water ocean circulation model of extensive overland flooding the implementation has a low overhead cost and we demonstrate a realistic hurricane forced coastal flooding simulation can achieve peak speed ups near 45 over the static case thus operating now at 80 90 efficiency keywords storm surge coastal flooding dynamic load balancing finite element modeling zoltan toolkit parmetis 1 software availability the code instructions and a test to run the software detailed in this work can be found by this private link https figshare com s 41827afadc318047e2ea the advanced hydrodynamic circulation adcirc code was originally developed in fortran77 by joannes westerink rick luettich and clint dawson since then it has been continuously updated using fortran90 and fortran77 and maintained by a community of developers adcirc is a commercial code www adcirc org that is otherwise free for research and academic purposes adcirc dlb which is a branch of adcirc requires a linux based system with distributed memory architecture that uses the message passing interface adcirc dlb must be compiled with an intel fortran 16 0 or more recent intel fortran compiler either mpich or mvapich and it requires the zoltan toolkit part of the trilinos package https cs sandia gov zoltan and parmetis http glaros dtc umn edu gkhome metis parmetis overview 2 introduction in two dimensional 2d finite element modeling of wind and tidally driven coastal circulations a portion of the computational domain is included above the local mean sea level lmsl state to simulate coastal flooding to obtain a high fidelity model while keeping computing cost relatively low an unstructured mesh composed of elements with highly variable sizes is used to represent the large horizontal scale separation i e o 10 m o 100 km of tide storm surge and coastal flooding processes gorman et al 2008 marks et al 2017 roberts et al 2019b predictions with these unstructured meshes are used to provide crucial information for coastal hazard assessment and design e g cstorm project cialone et al 2017 quetzalc√≥atl et al 2019 water level guidance e g iflood khalid and ferreira 2020 and emergency management operations staneva et al 2016 blanton et al 2012 2018 dresback et al 2013 to simulate coastal flooding in the region of focus the mesh often contains an extensive floodplain extending up to an elevation of 10 m 15 m above local mean sea level fig 1 bunya et al 2010 blanton et al 2012 teng et al 2017 on the floodplain high resolution elements of size 10 50 m enable a representation of the fine horizontal geometric length scales and complex land cover variability that control coastal inundation patterns e g dietrich et al 2010 forbes et al 2010 hope et al 2013 sebastian et al 2014 some works demonstrate that relatively coarse mesh resolution e g 50 m overland may result in inaccurate inundation area bilskie and hagen 2013 bilskie et al 2015 cobell et al 2013 and inaccurate velocities neelz and pender 2008 as coarser mesh resolution may alias vertical features such as elevated roadways and levees that can largely alter inundation patterns as a consequence of the high resolution elements that are used to represent overland features an often disproportionate number of degrees of freedom dofs in the modeling system become located overland for example in a modeling system used for operational coastal flooding predictions fleming et al 2008 technology riverside inc and aecom 2015 approximately 55 percent of the vertices are above the lmsl state at initialization the transient nature of coastal flooding further implies that although the overland regions are included due to uncertainty in the magnitude and location of potential storms the majority of overland dofs will never be flooded during any given simulation the computationally expensive aspect of modeling coastal flows over land have motivated the development of a broad range of numerical strategies chen et al 2003 casulli 2019 behrens and bader 2009 leveque et al 2011a taeb and weaver 2019 androsov et al 2019 wittmann et al 2017 ginting and mundani 2019 sanders et al 2010 traditionally coastal flooding is modeled using a pre determined highly refined mesh with a logic based wetting drying approach medeiros and hagen 2012a luettich and j westerink 1999 candy 2017 sanders et al 2010 another approach is to start the calculation with an initially coarse mesh and then adaptively refine the mesh as the event propagates leveque et al 2011b adaptive methods perform well for the simulation of transoceanic tsunamis storm surges and the associated coastal flooding in which the background state is quiescent before the event occurs behrens and bader 2009 leveque et al 2011a however for predictions of total water levels the background tide and wind driven signal continuously interact complicating mesh refinement strategies multi grid approaches e g taeb and weaver 2019 also have been shown to reduce wall clock times for detailed coastal flooding simulations by simulating with a hierarchy of meshes but require an extensive mesh development and coupling approaches to merge solutions instead of using a pre determined sufficiently fine mesh sub grid scale modeling techniques e g casulli and walters 2000 casulli 2019 candy 2017 kennedy et al 2019 aim at improving the accuracy of an under resolved mesh by incorporating these geometries through porosity functions this methodology is promising but there are limitations on the practical ability to provide closures for all subgrid physics kennedy et al 2019 the topic of dynamic load balancing has also been applied for the prediction of overland flooding to improve computational efficiency ginting and mundani 2019 sanders et al 2010 wittmann et al 2017 ginting et al 2020 dynamic load balancing in the context of flooding is important for computational efficiency as the computational costs of wet and dry cells are often nonequivalent wittmann et al 2017 ginting and mundani 2019 ginting et al 2020 and inherently time varying as a flood event progresses sanders et al 2010 one strategy presented in wittmann et al 2017 was to vectorize the flux calculation and mask out the dry cell calculations resulting from wetting and drying in a shared memory parallelism finite volume solver ginting and mundani 2019 using a shared memory parallelism finite volume approach with a space filling curve to adaptively distribute meshes with a weighted dynamic load balancing strategy ginting et al 2020 further advanced by performing dynamic load balancing with a static domain decomposition in a hybrid openmp mpi parallel computing environment a finite volume solver called parbrezo with domain decomposition mpi parallelism showed a 97 reduction in execution time in a simulation of regional hurricane driven storm surge on an unstructured grid by using a weighted domain decomposition approach and a static grid partitioning technique sanders et al 2010 by considering the challenges and recognizing the complexity involved in mesh design for coastal ocean modeling bilgili et al 2006 gorman et al 2007 2008 roberts et al 2019a a dynamic load balancing approach is developed to reduce the computational cost of modeling wind induced inundation of the floodplain using a pre determined mesh with a widely used finite element model called the advanced circulation model luettich and westerink 2004 our approach relies on a cell weighting approach similar to that of ginting and mundani 2019 and sanders et al 2010 the approach uses dynamic grid partitioning and is capable of redistributing all components of the mesh during execution between cores to reflect the time varying movement of the wet dry boundary we show how our approach more efficiently utilizes computational resources and is minimally invasive in the sense that it does not require any modifications to pre existing models to obtain speedups to facilitate dynamic load balancing in a highly scalable parallel element solver such as adcirc we rely on the zoltan toolkit boman et al 2012 and parmetis karypis and kumar 1998 to provide essential functionality to the application the rest of this article is organized as follows first we describe our strategy to reduce the computational cost associated with the floodplain by integrating the zoltan toolkit within a finite element solver and developing an algorithm to dynamically remove the floodplain from the computational problem then we describe the effect and behavior of the load balancing approach when it is applied to an idealized case study finally we test our implementation in a real world coastal flooding application in north carolina u s a the paper concludes with a discussion on the key findings 3 methods 3 1 adcirc hydrodynamic model the dynamic load balancing application is built around the advanced hydrodynamic circulation model luettich and westerink 2004 to improve the computational performance of both existing and future modeling systems that rely on adcirc adcirc http adcirc org has become one of the most widely used community modeling platforms for storm surge coastal flooding predictions across academia united states governmental agencies and the private sector this is due to its inclusion of critical physics e g sub grid scale features such as levees and floodwalls explicit inclusion of spatially varying land cover and its influence on both surface wind conditions and bottom stress coupled waves surge tides and runoff interfaces to multiple meteorological model forcings accurate numerics and optimization for high performance computing furthermore an active development community continues to advance the model s capabilities adcirc solves the shallow water equations swes using the generalized wave continuity equation gwce kellogg 1988 lynch and gray 1979 the swe are discretized by using a continuous galerkin cg finite element fem scheme in space and a finite difference scheme in time and the method is formally second order accurate luettich and westerink 2004 a parallel version of adcirc is implemented for a distributed memory parallel system with the message passing interface mpi the solver is scalable and demonstrates linear speed up at cpu cores of 10 000 and greater tanaka et al 2010 adcirc represents the wetting and drying process on an elemental basis in which elements must either be fully wet or fully dry through logic based conditions luettich and j westerink 1999 dietrich et al 2004 the wetting and drying process creates a moving boundary in the computational domain that can move at most one element per time step due to the order of the wet dry logical operations while many different and potentially more advanced wetting and drying methods exist greenberg et al 2005 bates and hervouet 1999 k√§rn√§ et al 2011 candy 2017 warner et al 2013 medeiros and hagen 2012b the adcirc model with its wetting and drying approach is used by various united states government and state entities for coastal design and risk assessment among other use cases e g cobell et al 2013 both adcirc v53 adcirc and adcirc dynamic load balancing adcirc dlb source codes are compiled identically using intel fortran 17 1 compiler with the o2 optimization strategy and the mvapich implementation of message passing double precision arithmetic is used for all calculations besides the dlb capability there are no differences in the source code implementation between the two versions of the code 3 2 dependencies the zoltan toolkit is used as a data parallel programming library that we built the load balancing algorithm upon devine et al 2002 boman et al 2012 built with mpi zoltan provides configurable mpi based unstructured grid data partitioning through an interface to several state of the art graph and hypergraph partitioners it also provides a scalable data exchange algorithm that uses distributed data directories to locate non local data pinar and hendrickson 2001 devine et al 2002 zoltan requires users to define a set of call back functions to facilitate the neighbor exchange of enqueued data in this work we choose to work with a graph based partitioner parmetis karypis and kumar 1998 due to its widespread usage and its integration with the zoltan toolkit specifically we use parmetis v4 0 3 program with the implementation of the k way multi level graph decomposition algorithm parmetis s part k way graph decomposition algorithm is called via zoltan s interface 3 3 load balancing strategy our strategy to reduce the cost of modeling the coastal floodplain involves trading a computational workload balance for a memory imbalance in this strategy the decomposition is done in such a way that the load is balanced with respect to the wet state component of the mesh as a consequence the majority of the dry state component of the mesh belong only to a few subdomains hence introducing memory imbalances as shall be explained below in section 3 5 when the majority of dry state mesh are grouped together the computing time can be reduced through a rearrangement of data in memory the data rearrangement leads to an offline portion of the mesh that is not involved in timestepping and an online portion that is fig 2 at the start of the simulation the wet dry boundary is located near the cold start water level depending on the extent of the floodplain this enables the majority of dry state mesh data to be located in the offline portion of the subdomains and a theoretical work savings to be obtained through our load balancing approach fig 2 as the movement of the wet dry boundary begins to reach the boundary of the offline state subdomains the computational workload may need to be rebalanced either to maintain a target performance level and or to satisfy constraints imposed by our approach in this way the simulation can be thought of as a sequence of epochs each composed of a rebalancing phase and a subsequent calculation phase in the case that all mesh data are wetted the load and thus the theoretical speed of the parallel application ideally should become approximately equivalent to that of static adcirc 3 4 mesh partitioning to form the decomposition the mesh is represented as an undirected graph vertices of the graph are assigned a non dimensional weight proportional to its anticipated computational expense in a similar manner to ginting and mundani 2019 ginting et al 2020 to reflect communication costs and data dependencies the edges of the graph are given a non dimensional weight as well elements and vertices are weighted depending on an offline or online status which is based on a set of user defined criteria elements that have an offline status are weighted with a value of 0 whereas online elements are weighted with a value of 1 graph edges that connect two offline elements are weighted with the value 1 whereas graph edges that connect either two online elements or one online element to an offline element are weighted with a relatively larger value of 1 000 graph edge weights with a value of 0 are not permitted in parmetis parmetis then uses these weights in partitioning the graph into a number of subdomain the disparity in graph weights between the online and offline portion of the mesh represent their true cost algorithm 1 mesh partitioning algorithm image 1 to decompose the mesh a sequence of migration and query operations are performed algorithm 1 we stress here that the entire mesh both dry and wet state components can be redistributed to new subdomain configurations each rebalancing epoch to start the application slices of the finite element mesh along with their weights are passed to a call back function that makes calls to parmetis within the zoltan toolkit elements are first migrated so that each element of the mesh is owned by only a core query operations are performed using the distributed data directories provided in zoltan note that data directories for both vertices and elements are over allocated by a factor of three times the initial number of vertices or the number of elements on each core respectively to avoid reallocation operations during dynamic load balancing this over allocation factor was found to be sufficient to avoid reallocations of data directories while avoiding hash collisions vertex ownership is formed implicitly by the element ownership since three vertices of the element belong to the element s owner on the border of the subdomain only the relatively higher numbered core is designated an owner of the vertices thereafter a single layer of vertices are imported on the relatively lower numbered core in each pair of adjacent subdomains the duplicated vertices that border all subdomains are termed halo vertices 3 5 array rearrangement after algorithm 1 the arrays representing the mesh are rearranged according to their online offline status fig 3 online elements are placed contiguously in memory at the start of the array and offline elements are then located in memory after the last online element in this memory arrangement the number of loop iterations can be reduced by the number of offline elements on each core by reducing the loop range in this way a speed up of the simulation can be achieved provided work is balanced across all cores the reduction in loop iterations by array rearrangement is referred to as loop clipping when the elemental loop range is reduced all of the online element s vertices must be accessible this is accomplished by iterating through the online state elements and placing their associated vertices at the start of the array while ensuring that vertices are not duplicated the process is repeated for the offline state elements array placing these offline vertices after the last online state vertex fig 3 loop extents are set to the memory location of the online element and online vertex with the respective adjacent memory location labeled offline 3 6 rebalancing strategies to trigger a rebalance a set of vertices called checkpoints shared between the boundary of the online and offline state portion of the mesh are checked for wetting each time step fig 2 b c if any checkpoint vertex is wetted the simulation must pause and rebalance by executing algorithm 1 note that at a minimum the buffer zone is one element wide and since the wetting drying algorithm can at most only move the wet dry boundary one element per timestep section 3 1 this avoids a scenario where the wetting drying front would not advance landward in the same manner as adcirc we ve found that it is not possible to set all dry elements and vertices offline instead a zone of elements buffering the region between the checkpoint vertices and the online component of the domain is created to control the frequency of rebalance events as is shown later the buffer zone enables this approach to be practical for coastal flooding predictions we implemented the following rebalance heuristics 1 distance if a vertex s nearest distance from the wet dry boundary exceeds a user defined threshold db in meters the vertex is set to an offline state if the three vertices of an element all exceed db then the element is set to an offline state an assumption here is that locations in close proximity to the shoreline are most likely to be flooded we stress that the distance to the shoreline front does not necessarily imply that the element will be flooded as the wetting drying algorithm advances the front based on a balance between the hydrodynamic pressure gradient force across an element and friction which is related to the forcings topography and landcover nearby the element luettich and j westerink 1999 dietrich et al 2004 2 topographical a vertex s elevation must be located above a user defined value hb in meters above the model s reference datum to be set to an offline state if the three vertices of an element all exceed hb then the element is set to an offline state in practice the hb can be selected based on the local inter tidal range the definition of the criteria to determine online and offline state vertices are globally defined among cores to prevent excessively frequent rebalance events for example every simulation timestep each time the checkpoints of the buffer are wetted hb is elevated by 10 of its existing value this is not necessary for db since the nearest distance is calculated from a wet dry boundary which is moving in space the selection of increasing 10 hb each rebalance event was based on the idea that we want to gradually lift the checkpoint vertices away from the wet dry front as flooding occurs to maximize the amount of dofs set to an offline state as more flooding occurs hb elevates a larger increment and avoids a rebalancing every timestep which would otherwise occur during a highly energetic flooding event a key assumption in the formation of the buffer is that the solution does not exhibit numerical artifacts and or instabilities it is noted that the shoreward movement of the wet dry boundary was not used as a factor in determining rebalance events however the user does have the option to schedule rebalance events which can be used to lower the buffer zone shoreward after a flooding event has passed note that improving the computational efficiencies post storm through lowering the buffer shoreward could become more important for real time predictions due to operational constraints 3 7 performance metrics a set of statistics are calculated to assess the performance of adcirc dlb the total wall clock time t of the simulation is the sum of the time spent computing tc and the time spent rebalancing trb 1 t t c t rb where tc and trb are summed over the entire simulation specifically trb represents the cumulative time spent performing the parallel partitioning algorithm while tc represents the cumulative time spent in timestepping the speed up over the static version of the code is calculated as 2 su o b s t s t a t i c t d l b in which tdlb is the wall clock time spent timestepping for a given simulation computed with adcirc dlb and tstatic is the wall clock time spent timestepping using adcirc the theoretical maximum speed up sumax is calculated by dividing the total number of vertices vtotal by the average number of wetted vertices vwet for a simulation 3 su m a x v t o t a l 1 n t t 1 t n t v w e t t where v w e t t is the number of wet vertices at timestep t nt is the number of simulation time steps following this if 50 of the domain is wet throughout the entire simulation the simulation would have a maximum speed up factor of 2 0 the sumax represents a maximum potential speed up assuming 1 the problem is perfectly load balanced 2 it has zero communication overhead 3 dry state vertices can be completely ignored from the computational problem and 4 there is zero rebalancing cost following this the speed up efficiency sue is computed as the ratio of the suobs to sumax 4 s u e s u o b s s u m a x where sue 1 100 implies a perfectly efficient calculation note that this efficiency metric is subjective to the modeling scenario which influences the amount of wetting and drying in the domain nevertheless if there are fewer dry vertices then there will be a smaller potential for speed up and thus a smaller sumax however this doesn t necessarily translate to a smaller sue that has to be earned by minimizing the costs of the dry vertices and the time spent in the dynamic load balancing e g tdlb operation as much as possible to investigate the performance of the application the load imbalance is calculated load imbalance is defined as the ratio of the maximum load l m a x k calculated over all cores divided by the average load l a v e r a g e k calculated over all cores 5 r i m b 1 k k 1 k r i m b k r i m b k l m a x k l a v e r a g e k 1 for all k epochs the quantity l is calculated as the number of online state vertices per core all cores ideally should maintain an equal load with an r i m b k 0 in order to ensure that the idle computing time is at a minimum for efficient parallelism inter processor communication volume should be minimized in adcirc messages are exchanged several times during a given timestep at the halo vertices of the subdomain the inter processor communication volume is estimated through the surface area to volume sv ratio per k th epoch which approximates the relative amount of communication cost as compared to the computing cost 6 s v 1 k k 1 k 1 n s n 1 n s s v n k number of halo vertices on subdomain n total number of vertices on subdomain n for ns subdomains over k epochs the number of halo vertices on a given subdomain in eq 6 is influenced by the shape of subdomains the number of vertices and whether the subdomains are connected in the computational domain as the number of vertices per core is reduced with the usage of more processors s v increases implying more communication over computation 4 results simulations were executed on the computational hydraulics laboratory s computer cluster called aegaeon https coast nd edu aegaeon contains 83 compute nodes of dual 12 core e5 2680 2 50 ghz haswell processors 1 992 cores in total each node contains 64 gb of random access memory that is shared among each node s 24 cores the nodes are connected via a high speed 56 gb infiniband network file input output was disabled i e no logging and no output file writing however for the hurricane irene test case meteorological forcing files winds and surface pressure data needed to be read into memory every 15 simulation minutes all timing results were repeated three times and the average is reported 4 1 simple tide on an idealized floodplain the dlb capability is evaluated first for an idealized problem with a channel and floodplain this problem is referred to as the ideal channel and is selected because it allows for a tidal signal to inundate and recede on a gradual linear sloping beach with a narrowing channel along its centerline to provide variations in horizontal directions while the problem has a predictable wet dry boundary the point of the experiment is to demonstrate the application of dynamic load balancing and some of the associated concepts 4 1 1 model and setup a channel and floodplain fig 4 are represented with an unstructured mesh with 64 415 vertices and 127 784 elements the topography bathymetry is characterized by an axially symmetric channel that has a parabolic cross sectional profile the model contains a large floodplain and it has an initial distribution of vertices with approximately 66 dry and 34 wet at a cold start state bathymetry varies linearly and gradually with a constant slope of approximately 0 002 from 8 m to 2 m above the model s lmsl onto the floodplain element sizes vary from 75 m near the open boundary to 15 m along the shoreline and then coarsens to 35 m overland a tide with an amplitude of 1 m and a period of 12 42 h i e semi diurnal m2 frequency and a ramp of 0 5 simulation days is prescribed along the open boundary fig 4 this boundary forcing generates a fluctuation of 1 m above and below lmsl with episodic inundation covering 13 240 vertices 20 55 of the vertices hatched area in fig 4 this fluctuation of water levels leads to a maximum of 54 30 of total vertices inundated in the mesh the application of the elevation specified boundary with a 0 5 simulation day ramp generates a wet dry boundary that rises in a rigid fashion and mimics a cresting tidal inundation in a small enclosed estuary the simulation uses a time step of 0 5 s with a maximum courant number less than 0 25 and the gwce is solved with an explicit time scheme with a mass lumping approach for this problem the maximum theoretical speed up c f eq 3 is 2 56 the ideal channel problem is run using five different core configurations i e 3 6 12 24 48 cores each core configuration is run with a sequence of progressively wider buffer configurations four configurations based on nearest distance to the wet dry boundary are considered db 50 m db 100 m db 200 m db 500 m fig 4 likewise four configurations are based on topography i e meters above the lmsl hb 0 10 m hb 0 20 m hb 0 50 m and hb 1 0 m an infinite size d b h b configuration that represents the static case using the decomposition algorithms was also tested each time a rebalance event is triggered in the depth based configurations hb is increased by 10 of its current value while the db remains fixed c f section 3 6 4 1 2 effect on decomposition compared to the case when all elements vertices are weighted equally the db 50 m configuration greatly increases the number of vertices in subdomains located in overland regions fig 5 c d consequently the number of total vertices in subdomains with predominately online vertices are reduced which reduces the computational problem size the number of dry state online vertices are greatly reduced per core while the distribution of online vertices remains well balanced fig 5 b as db is increased more vertices are set offline and this reduces the number of vertices in subdomains with predominately online vertices 4 1 3 timing improvements adcirc dlb demonstrates approximately linear scaling in all cases considered fig 6 a significant speed ups were measured e g up to suobs 2 44 but became less significant and more inefficient with the usage of more cores in configurations where a small value of db is considered the time spent rebalancing became similar in value to the total simulation time diminishing the speed up overall adcirc dlb performed slightly slower than adcirc in the db hb configurations it was found that because adcirc dlb decomposes the dual graph whereas adcirc decomposes the nodal graph the decomposition of the dual graph produced slightly greater load imbalances resulting in marginally slower execution times adcirc dlb produced 31 out of 32 faster simulations with speed up factors that ranged between suobs 1 10 to suobs 2 44 fig 6 c distance and depth based buffer performed similarly although changes to db produced greater changes in timing results than changes to hb we observed that configurations using small distance based buffers experienced far more rebalance events as the wet dry boundary initially advanced shoreward from its cold start state than the depth based buffer configurations this resulted in more variation in timing results the fastest simulation was hb 50 cm and produced a speed up factor that ranged from 2 44 to 1 42 when considering 3 to 48 core configurations respectively the highest speed up of suobs 2 44 was achieved as a result of minimizing trb while maximizing the sumax for instance configurations with a hb 0 50 m had greater trb which resulted in slower simulations despite that their theoretical maximum speed up sumax was greater the time spent rebalancing represented maximally 16 of the total simulation time while the majority of simulations 31 out of 32 spent 3 of the total time in rebalancing operations fig 6 c while the time spent in rebalancing operations can be considered relatively small trb does not exhibit scalability fig 6 d as rebalancing requires a number of communication and query operations c f algorithm 1 the slowest performing experiments used buffer configurations that initially set the most data offline which leads to more rebalancing events and thus far greater trb for instance the db 10 m corresponded to the narrowest buffer and resulted in a slow down of 1 42 using 48 cores due to trb fig 6 b for all simulations that were faster than adcirc their efficiency sue ranged from 53 to 96 fig 7 the 3 and 6 core configurations were consistently more efficient than the 12 and 24 core configurations the most efficient sue 94 simulation was measured using 6 cores with the hb 50 cm buffer while the db 200 m configuration produced the second most efficient simulation with a maximum sue 92 the rest of the configurations produced lower sue as the hb elevated and db increased as less of the theoretical speed up was realized subtracting trb from t demonstrates that the theoretical speed up increases as more data is set offline but at the cost of more time spent rebalancing fig 7 a and b when trb is removed from the timing statistics the most efficient simulations are those that initially set the greater component of the mesh offline i e db 50 m and hb 10 cm fig 7 a however the opposite is observed when trb is included which indicates that a configuration that minimizes both t and trb exists with a buffer configuration near to the hb 50 cm and db 200 m fig 7 b in all experiments compared to adcirc decompositions generated by adcirc dlb had larger s v by approximately 1 whereas less consistent differences for r i m b were measured fig 8 configurations that set more data to an offline state produced a consistent increase to the s v by maximally 3 5 the increase in s v is expected considering that the size of the online component of the problem is reduced through array rearrangement and shortening of loops fig 8 c the 48 core setup leads to buffers configurations falling below the scaling limit as was reported in tanaka et al 2010 similarly to s v a clear increase to r i m b is measured by 6 8 as compared to adcirc dlb and also increases as more cores are used unlike s v however the disparity in r i m b between different buffer configurations is greater there is no clear relationship between the number of vertices set to an offline state and r i m b 4 1 4 comparison with static adcirc a comparison of water levels between adcirc and adcirc dlb is shown in fig 9 for the best performing experiment i e 6 cores hb 50 cm in the ideal channel test case adcirc dlb solutions are numerically identical to that of adcirc and the timing of wetting and drying is unaltered by the dynamic load balancing fig 9 4 1 5 summary of experiment the best performance suobs 2 44 was obtained using a depth based buffer of hb 50 cm speed up factors had relatively larger variation from suobs 1 10 to suobs 2 44 depending on the configuration of the buffer trb represented less than 3 of the total simulation time with the exception of db 50 m where it created a significant slow down overall db had more variability than hb in trb to its configuration than the depth based buffer the application of dlb produced decompositions that had greater surface area to volume s v and greater imbalance factors r i m b by 1 3 but this did not appear to impact parallel performance significantly in general the application of dlb reduced the problem size fig 8 c and this can lead to higher levels sv and more rimb based on these findings for applications our recommendation is to use a depth based buffer configuration with an initial height of 0 50 cm which was the best performing experiment further when selecting the buffer it is important to ensure that the application of dlb does not reduce the problem size below the scaling limit of adcirc 4 2 flooding from hurricane irene in north carolina the performance of adcirc dlb is further assessed in a hurricane driven coastal flooding simulation of hurricane irene 2011 using an extensively validated mesh of the mid atlantic united states region this mesh was validated during hurricane isabel 2003 blanton et al 2018 and hurricane irene dresback et al 2013 and the model is used for real time predictions for the north carolina forecasting system blanton et al 2012 additionally it contains an extensive floodplain unlike the ideal channel problem this test problem features an irreguarly moving wet dry boundary forced by a combination of meteorological forcings and astronomical tides interacting with observed topography bathymetry datasets hurricane irene impacted the mid atlantic region as a category 1 hurricane with 10 min sustained winds of approximately 78 mph on august 27 28 2011 seroka et al 2016 measured water levels of approximately 3 m above lmsl were observed on the westward side of the cyclone s track near the tar pamlico sound and neuse river basins in north carolina fig 10 dresback et al 2013 elsewhere in the pamlico sound peak water levels of 1 m 2 m above lmsl were observed 4 2 1 model and setup the mesh used for this experiment is referred to as north carolina v9 nc9 and contains 608 114 vertices and 1 200 767 elements with the finest resolution located nearshore of approximately 30 m and expanding to approximately 500 m in size over the floodplain dresback et al 2013 along the south atlantic bight a patch of elements were modified to improve their quality otherwise the mesh was identical to the original at a cold start state approximately 56 of the mesh vertices are dried in the area impacted by hurricane irene the tides range between 0 40 and 0 60 m above lmsl approximately 10 12 of the floodplain vertices are flooded during an average tidal cycle fig 10 the extent of the floodplain for the nc9 mesh is substantial and was designed based on a historical review of flooding in the region by the developers of the model blanton et al 2012 in areas nearby large rivers such as the tar and neuse rivers the inland extent of the model extends up to the 8 m elevation contour above mean sea level outside of the riverine areas the model domain extends up to the 15 m elevation contour above sea level the sumax for this particular setup was 1 97 the setup used in this section resembles the operational simulation used in fleming et al 2008 a simulation of 8 days explicit numerical scheme 0 5 s time step atmospheric and tidal forcing and point based outputs of free surface elevation wind and pressure hindcast fields from ocean weather inc owi were used to force the model between the dates of august 21 2011 12 00 00 utc to august 29 2008 00 00 00 utc at a time increment of 15 min temporally consistent tidal processes are included in the simulation by specifying elevation boundary conditions on an open ocean segment using the tpxo9 1 atlas egbert and erofeeva 2019 for four major semi diurnal m2 n2 s2 k2 and four major diurnal tidal constituents k1 o1 p1 q1 forcings are ramped from a cold start state using a hyperbolic tangent function over the first two days of the simulation to avoid exciting transient modes in the study region free surface elevation are recorded every 6 simulation minutes from simulation days 2 8 at 74 rapidly deployed gauges by the united states geological survey service usgs and an additional 8 national oceanic service nos gauges depicted in fig 10 4 2 2 comparison with measured data adcirc dlb solutions are effectively identical to that of adcirc to within small differences often seen when running adcirc with different core configurations figs 11 and 12 these differences are possibly introduced through the threshold based wet dry logic dietrich et al 2004 compared to observations of measured water levels the timing and peak of simulated water level responses at the eight noaa oceanic service nos gauges in the region were accurately captured with the exception of stations 1 and station 4 which both under predicted the water level response fig 11 further good agreement between simulated results and high water mark observations at 74 usgs sensor in the neuse tar and pamlico river basins are measured 4 2 3 timing improvements the model setup is executed on five core configurations more specifically 60 120 240 360 480 cores for adcirc dlb we use the best performing buffer configuration in terms of sue and suobs from section 4 1 which was hb 50 cm significant speed ups that ranged from 1 32 to 1 84 with 480 to 60 cores were measured respectively fig 13 this implies a reduction in wall clock times between 10 and 190 min with the usage of 480 and 60 cores respectively trb were relatively negligible occupying less than 58 s of the total simulation time all simulations rebalanced for a total of 22 times as water levels flooded the floodplain trb demonstrated weak scalability and reduced from trb 58 s to approximately trb 40 s using 60 to 480 cores respectively the measured speed ups can be considered highly efficient with a sue ranging from 69 to 93 our most efficient configuration sue 93 occurred with the 120 core setup the 60 core configuration performed similarly with a sue 87 8 configurations that used more than 120 cores are less efficient the 480 cores setup is the least efficient simulation with a sue 69 similar to the timing results obtained in section 4 1 trb did not negatively affect the model efficiency and only a small reduction of efficiency 0 1 0 3 was measured that could be attributed to the rebalancing fig 14 in tanaka et al 2010 the scaling limit for the adcirc solver with a similar core configuration to ours occurred when there were on average 1 000 vertices per core our results reflect a similar pattern with a deviation in the scaling curve fig 13 the application of array rearrangement and loop clipping reduces the problem size to below the scaling limit in the 360 and 480 core configurations which explains the measured deviation from an optimal scaling rate fig 13 a in contrast to the ideal channel case c f section 4 1 the movement of the wet dry front is complex during the coastal flooding event with substantial shoreward movement of the wet dry front in the wetland environments immediately north of the neuse river and in contrast limited shoreward advancement of the wet dry front along the steeper banks and tributaries of the neuse river fig 15 in spite of the irregular movement of the wet dry front the depth based buffer configuration criteria remains robust and enables a significant speed up of the overall calculation while triggering relatively few rebalancing events 5 discussion and conclusion the aim of this work is to reduce the wall clock times spent modeling wind driven coastal flooding on unstructured triangular meshes using the adcirc solver regional coastal adcirc meshes often contain relatively large amounts of dry state vertices to represent the finely detailed nature of the coastal floodplains considering variable resolution unstructured mesh model development is both challenging and time consuming modelers cannot design a stable and robust modeling mesh system that is hand crafted for each storm that may occur in a given area as a result all dry state floodplain vertices are not actively solved for and this make the model s parallel execution susceptible to large work imbalances and inefficiencies our solution involves in memory re decomposing the unstructured mesh in parallel based on the moving domain boundary determined by the wet dry elemental state each time the problem is re decomposed local arrays of vertex and elemental data are rearranged to reduce the extent of loops involved in the calculation leading to an acceleration of the program the third party libraries zoltan boman et al 2012 and parmetis karypis and kumar 1998 were used to implement adcirc dlb the performance of adcirc dlb as compared to static adcirc was studied given a range of computational resources 3 480 cores in both an idealized problem and a hurricane scenario overall adcirc dlb exhibited similar scaling behavior to adcirc with a linear reduction in simulation time reductions between approximately 20 50 to the total wall clock time of a realistic coastal flooding simulation were achieved with approximately 70 90 efficiency compared to a theoretical speed up estimate however the reduction in the problem size associated with dlb given the same amount of computational resources can lead to relative calculation slow downs to the same calculation computed on adcirc when the minimum number of vertices per core falls below 1 000 in this part of the scaling regime the communication cost dominates the computing cost and dynamic load balancing offers little gain performance was also significantly affected by the criteria used to determine what portion of the mesh was set as offline criteria based on elevation hb was more efficient than using a minimum distance criteria db from the wet dry boundary minimum distance criteria produced more rebalance events as the tide advanced shoreward compared to the depth based criteria which leads to more time spent rebalancing and overall slower performance based on the results our recommendation is to remove as much of the inter tidal zone using a topographic criteria i e hb 0 50 m or a depth similar to the local tidal range and let the program automatically lift hb as wind driven coastal flooding occurs we demonstrated that adcirc dlb does not sacrifice the well established accuracy available in the adcirc solver parallel scalability or require new meshes models to be developed future work intends to apply adcirc dlb to operational storm tide forecasting systems including regional models e g fleming et al 2008 as well as emerging global ones e g pringle et al 2021 seroka et al 2020 while some current research is focused on more efficiently incorporating coastal floodplains into modeling systems to avoid components of the mesh that are rarely flooded through mesh decimation techniques bilskie et al 2020 our dynamic load balancing concept may provide a different approach to coastal ocean model development for instance dynamic load balancing could reduce the need to develop many regional models with carefully designed floodplains by instead enabling the user to develop one substantial modeling domain with a large overland extent while incurring minimal computational overhead for example a next generation comprehensive modeling system of the east and gulf coasts of the united states with fine resolution overland would imply that the vast majority of vertices would remain in a dry state for any given event in this case the size of the problem and the number of dry state vertices would make it a suitable candidate to take advantage of adcirc dlb to accelerate the calculation funding this material is based upon work supported by the u s department of homeland security under grant award number 2015 st 061 nd0001 01 and national science foundation grant award number nsf aci 1339738 the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies either expressed or implied of the u s department of homeland security or the national science foundation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank the two anonymous reviewers who helped improve the quality of the manuscript we thank ocean weather inc for allowing us to use their meteorological forcing inputs for the hurricane irene test problem we thank dr brian blanton at renaissance computing institute at the university of north carolina at chapel hill for providing the mesh and input files used in the hurricane irene test problem kr prepared the manuscript designed and implemented the coding upgrades into adcirc designed and performed the experiments and conducted the analysis of the results jcd dw and jjw provided feedback throughout the project supervised the research and improved the manuscript wp improved the manuscript presentation and provided critical feedback 
