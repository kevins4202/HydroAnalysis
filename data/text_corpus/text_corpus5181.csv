index,text
25905,system of systems approaches for integrated assessments have become prevalent in recent years such approaches integrate a variety of models from different disciplines and modeling paradigms to represent a socio environmental or social ecological system aiming to holistically inform policy and decision making processes central to the system of systems approaches is the representation of systems in a multi tier framework with nested scales current modeling paradigms however have disciplinary specific lineage leading to inconsistencies in the conceptualization and integration of socio environmental systems in this paper a multidisciplinary team of researchers from engineering natural and social sciences have come together to detail socio technical practices and challenges that arise in the consideration of scale throughout the socio environmental modeling process we identify key paths forward focused on explicit consideration of scale and uncertainty strengthening interdisciplinary communication and improvement of the documentation process we call for a grand vision and commensurate funding for holistic system of systems research that engages researchers stakeholders and policy makers in a multi tiered process for the co creation of knowledge and solutions to major socio environmental problems keywords social ecological modeling interdisciplinary modeling integrated modeling scale issues system of systems approach 1 introduction socio environmental systems ses function across a range of inter related scales that collectively represent a system of systems sos the term sos has been used since the 1950s and various definitions exist nielsen et al 2015 in this paper we distinguish between an sos as a collection of human and natural systems and sos models which are engineered representations of an sos the former is defined as an interconnected collection of multiple heterogeneous distributed systems that collectively may give rise to emergent behavior where each system represents a process or set of processes in the modeling of sos we follow little et al 2019 who define sos models as a collection of independent constituent systems in which each fulfills its own purpose while acting jointly towards a common goal p 84 in environmental modeling sos models may take the form of integrated assessment models iams or more generally integrated environmental models iems which are commonly applied to inform environmental management processes ewert et al 2011 iwanaga et al 2020 letcher kelly et al 2013 matott et al 2009 central to sos modeling is the view of system representations as a multi tier structure with different levels of abstraction where systems and indicators at lower levels can be scaled up to higher levels these representations capture processes that operate at different scales e g temporal spatial organizational in contrast to single system approaches which assume such drivers to be exogenous and crucially do not account for any feedback mechanisms between the represented systems this view also sets the focus on how to integrate knowledge from the different disciplines involved and coordinate information exchange among these in a consistent and meaningful way knowledge integration is not limited to the technical coupling of models but to integration among multi scale stakeholder and expert processes this combined socio technical focus makes scale issues and their treatment a core consideration of sos modeling 1 1 the need for a holistic treatment of scale a crucial ingredient in sos modeling is attending to the socio technical processes involved representation of scales is defined by modelers for a particular purpose and is ultimately subject to human processes meadows 2008 accordingly the representation of an sos is the end product of what the people involved implicitly or explicitly have chosen to represent and how they implemented their choices these then influence the model structure and uncertainties embedded and the consideration of its different dimensions analyses conducted and data and methods used glynn et al 2017 gorddard et al 2016 voinov et al 2018 such choices are subject to the available knowledge experiences biases beliefs heuristics and social values as well as the perceived purpose s of the modeling a key scale issue in sos modeling is the development of a consistent and defensible characterization of scale elsawah et al 2020 existing systems analysis and modeling approaches tend to come from entrenched disciplinary paradigms and so with a specific focus on their scales and facets and embedded language and terms inconsistencies then manifest in the conceptualization and treatment of scale in sos approaches which prevent researchers from 1 understanding the implications of scale choices 2 formulating implementing and validating models that are relevant to the questions of interest 3 predicting future sos responses in support of decision making elsawah et al 2020 little et al 2019 razavi et al 2020 and 4 communicating modeling results in ways that help identify trade offs and synergies within an sos and among the systems under investigation fridman and kissinger 2019 miyasaka et al 2017 addressing issues that arise from the conceptualization and representation of multiple scales are often omitted or left for future discussion ayllón et al 2018 discrepancies in the treatment of scale can be addressed firstly by developing a shared understanding of the system s being analyzed through a holistic interdisciplinary process thompson 2009 white et al 2019 there is increasing recognition that holistic approaches are necessary to enable an integrated assessment of scale issues in socio environmental social ecological systems schlüter et al 2019a 2019b hoekstra et al 2014 the rise of inter multidisciplinary fields such as socio hydrology elshafei et al 2014 sivapalan et al 2012 and eco hydrology hannah et al 2004 porporato and rodriguez iturbe 2002 gives further credence to this need for soss in particular it is necessary to additionally acknowledge the socio technical influences on their modeling explicit inclusion of the socio technical perspective pushes beyond traditional modeling approaches as it advocates assimilation of not only the data and mechanistic processes across different systems but also includes the knowledge and information held in the social institutions involved in the modeling 1 2 purpose the purpose of this paper is to advance knowledge and implementation of interdisciplinary sos modeling by identifying and articulating the practices issues and challenges involved with respect to issues of scale central to this interdisciplinary lens is making concrete the multidimensional nature of scale issues and the interplay among these here the term interdisciplinary is favored over trans or multi disciplinary as the focus is on the blending of disciplinary knowledge white et al 2019 the primary audience of the paper is modelers albeit in different domains and scientific disciplines with an interest in adopting an sos approach as a methodological framework in ses modeling in the following section 2 we first provide definitions for the key terminology used throughout this paper these definitions are not intended to be universal but are provided to contextualize and aid in communication given the range of disciplines involved in ses modeling in section 3 we explore issues of scale which need to be considered throughout the modeling we then describe in section 4 the long term challenges towards resolving such scale issues and suggest paths to be taken in the shorter term 2 concepts and definitions of scale 2 1 the process of defining scales sos models principally provide a representation of the interactions that occur between the systems involved holistic integration of knowledge from the various disciplines involved is necessary so that the implications of the different methodological choices on scale can be understood elsawah et al 2020 to this end a three day workshop was held in october 2019 in which a culturally and disciplinary diverse group of 20 participants convened to share their knowledge an additional 3 contributed in complementary ways to the drafting of this paper contributors originated from europe north america and the asia pacific and included engineers economists social scientists mathematicians physicists hydrologists computer scientists and ecologists to prevent miscommunication we developed a set of terms outlined in section 2 2 to build a shared language rubin et al 2010 spitzberg and cupach 1989 thompson 2009 although prior definitions of scale are available see for example cash et al 2006 gibson et al 2000 it was considered useful to develop a shared empathetic understanding of each other s perspectives banerjee et al 2019 thomas and mcdonagh 2013 the process additionally served to break down cognitive constraints macleod and nagatsu 2018 which may otherwise blind researchers to relevant notions of scale allowing disciplinary bias to creep in and knowledge gaps to form the range of disciplines involved in ses modeling often makes addressing cognitive constraints difficult as there are different notions of scale and related terms are used in different ways depending on context this variance has been observed in the use of common terms with conflicting definitions between and sometimes within disciplinary fields bridle et al 2013 2 2 scale terminology in sos modeling defining the terminology associated with scales was an arduous process at first owing to the diversity amongst workshop participants a brief overview of the resulting primary terms used in this paper is provided in table 1 for the discussion here scale is taken to have an expansive definition covering the scope of work to be conducted in the treatment and representation of system processes aspects of scale that had unanimous consensus included the commensurability of the choice of scale within the purpose of the modeling and the consistency of spatial and temporal scales across models it was also acknowledged that scale can mean many things beyond the spatial and temporal for example the less tangible such as treatment of ethical considerations within the modeling process e g häyhä et al 2016 regardless of definitions treatment of scales and the choices made in this treatment influences the model uncertainties and the outcomes of the modeling commensurability refers to the appropriateness of the selected approaches and methods for the sos modeling purpose broadly speaking these approaches can be described as being subject to socio technical considerations which are the focus of the discussion in this paper the social human aspect of modeling includes the circumstances of collaboration project management and participatory processes as well as those settings influencing the technical aspects including modeling and computational considerations the spatial and temporal features of a system are often the primary aspects around which scale is traditionally considered and framed these define the time and space of interest both their horizons and discretization and the events and processes that are considered important to represent cash et al 2006 the spatial scales selected may be influenced by the temporal scales of interest and vice versa their dependence can be intensified by the fact that spatio temporal scales are often influenced by factors outside their defined boundaries such influences may be important but may not be well understood or ignored zhang et al 2014b 2014a resolution defines the granularity of system representation and refers to the unit of spatial temporal scale represented in each system resolution may be spatial or temporal in nature but extends in other ways such as to social units individuals to families to communities etc and thus may be represented so as to conform to a semantic or conceptual hierarchy cash et al 2006 choice of resolution is highly dependent on the modeling context generally informed by the availability of data the needs of the model including for numerical stability sensitivity and model identifiability and model purpose hierarchy and their respective levels of organization relate to the representation of nested relationships among systems ostrom 2007 for example various governance systems may co exist at a range of scales with separate administrative or institutional concerns daniell and barreteau 2014 team based organizations are one example where the hierarchical scales may not be constrained to specific locations with members performing a variety of roles within an organization that may be geographically spread across different time zones actors influence and define the aspects of scale that are considered and may be both human and non human entities which affect or influence one another the term has its roots in the social sciences an example may be found in wessells 2007 actors have roles and carry out one or more activities in the system and can be represented individually or collectively human actors have attributes such as values goals and mental models which influence their behavior pahl wostl 2007 non human actors are defined here in a literal sense i e not an individual biological person such that organizations flora and fauna are non human actors but may still exhibit collective culture and personalities hobday et al 2018 schneider et al 2013 a system can encapsulate many actors and may be an actor itself the different types of system modeling encompass many terms that are often used interchangeably across the sciences as alluded to in the introduction we are guided by but do not directly adopt definitions as applied in system of systems engineering cf dahmann and baldwin 2008 here a single system model targets a specific system for instance an agricultural system without explicit representation of the hydrological dynamics or climatic influences consequently single system models constrain themselves to the concerns and considerations of a single sector models concerned with a single system may of course use several models internally e g crop growth soil water properties etc and these are referred to here as component models a direct approach to representing additional systems can be accomplished by applying albeit separately a selection of single system models for a given problem domain in such cases knowledge gained in the application of a model may inform the use of another data from one model may be fed into another and vice versa typically via manual processes for example a weather forecast model may be used to provide inputs to an agricultural model to determine seasonal effects on crops and the agricultural model may provide land surface boundaries to the weather forecast model multi system representations can be integrated by coupling models together such that data interoperation occurs in an automated fashion individual system level models are then referred to as constituent models the advantage of multi system models over their single system relatives is that the impacts and feedback mechanisms can be represented across between their individual scales elag et al 2011 tscheikner gratl et al 2019 wang et al 2019 multi system models with their explicit representation of system interactions are therefore capable of providing more holistic assessment compared to the use of individual models in isolation kelly letcher et al 2013 component based modeling stems from component based software engineering vale et al 2016 hutton et al 2020 and common usage in environmental modeling typically makes no distinction between constituent and component models e g malard et al 2017 a conscious decision has been made here to adopt the term constituent from the systems engineering field nielsen et al 2015 to convey this distinction it is important to note that integrated and multi system models could then equally apply to both single system models with several component or constituent models the requirement for a model to be regarded as integrated is that its component or constituent models are coupled together through the use of a common automated infrastructure to facilitate data interoperation see for example malard et al 2017 whelan et al 2014 by necessity multi system integrated models are more complex and may involve a variety of modeling paradigms e g bayesian networks agent based system dynamics etc and their combinations an sos model is then regarded here as an integrated model with constituent models each constituent model may be a single system or another sos model such that a tiered network of relationships between models is formed with each representing a layer of abstraction in sos modeling each constituent model may operate across different spatial temporal scales hierarchical levels and resolutions to incorporate multiple aspects of distinctly separate disciplinary or sectoral domains and modeling paradigms an sos perspective allows but does not prescribe consideration of complex system properties including nonlinearities interdependencies feedback loops thresholds and emergence 3 scale issues to consider models are developed through a life cycle of various phases each with specific considerations and steps the modeling cycle grimm and railsback 2012 hamilton et al 2015 jakeman et al 2006 sos modeling is more complex compared to single system models due to the number of people and disciplines involved as well as the dependencies between the constituent models similarly management of the modeling process is made more complex as there is not a single modeling cycle but multiple cycles occurring asynchronously each actor and model may have separate objectives and purposes priorities and differing levels of available resources not to mention the need to consider the availability of resources for the sos modeling as a whole the sections below are adapted from the modeling phases identified in badham et al 2019 and hamilton et al 2015 wherein the actions undertaken in each modeling phase are described in contrast we identify the relevant phases within an sos context and outline the considerations with respect to scale issues fig 1 depicts the high level considerations objectives within each phase while the sections below are presented in a sequential manner we stress that modeling is an iterative and concurrent process 3 1 scoping phase in this phase the objectives of the modeling are clarified by defining the problem and how modeling is intended to address it examples of model or modeling purpose could be to fill gaps in knowledge to support learning and communication processes to validate current understandings and assumptions to predict what might happen in the future or to carry out scenario analysis badham et al 2019 kelly letcher et al 2013 ideally this scoping phase results in a clear understanding of the model types and components that need to be developed or in later iterations their limitations with respect to the model purpose and how to address these 3 1 1 problem definition and scoping while the overarching purpose of the sos model may be known the specifics may be less clear at the outset development of a consistent and shared view of the scales to be considered involves communication of the scope and interactions across the constituent systems between all involved see fig 2 this process can aid in identifying and addressing areas that require reconciliation of different views that often exist across the stakeholders awareness of the scale issues will likely evolve as the modeling progresses through the iterations the choice of modeling pathways and methodological framework employed is heavily informed by this awareness macleod and nagatsu 2018 involvement of stakeholders including domain experts through participatory processes can inform the identification of relevant scales in the face of uncertainty and poor data availability hamilton et al 2015 kragt et al 2013 stakeholders can also play a role in selecting and combining data furthering holistic consideration of system actors and aid in developing the model purpose the relationship between actors and their roles in framing the scale scope and purpose of the modeling has been previously recognized kragt et al 2013 refsgaard et al 2007 and is further explored in the next subsection insufficient consideration or agreement regarding the overarching purpose of the sos model may ultimately affect model performance and outcomes connor et al 2019 the higher number of actors in sos modeling increases the difficulty in reconciling different or mismatched perspectives requirements and purposes this is a problem of heterogeneity o connell and todini 1996 and is not restricted to any single discipline often and by necessity the scale of the modeling is to be commensurate with its purpose including the level of certainty being sought and the available resources purpose and use of constituent models may be mismatched if conflicting perspectives over the scope of the modeling are not addressed modelers that have different goals in mind may only consider scales relevant to their immediate and often discipline specific concerns leading to an improper selection of constituent models there is potential for a high degree of mismatch between constituent models even if modelers coordinate their efforts unexpected cascades of effects through scales is commonplace in complex systems tranquillo 2019 and could arguably be taken as the rule rather than the exception change in scale may also occur during the modeling process due to new information that triggers a necessary change in model context the scale of model interactions to be represented can also influence the number and type of constituent models included and overall system complexity the choices regarding scale then have implications for how well interactions among systems can be represented with respect to the model purpose scope creep wherein the scale of the modeling is continually extended to cover contexts not originally envisioned cf barton and shan 2017 may eventually compromise modeling efforts as available resources get stretched too thinly to achieve sufficient progress sarosa and tatnall 2015 choice of scales is further compounded in cases where system bounds cannot be clearly and definitively defined coastal zones atmospheric systems and natural resource management systems are examples of systems with ambiguous system boundaries social systems and their dynamic structures are another example that do not have clear boundaries yet place important even governing conditions on system behavior such social systems and their influences are so far under represented in current integrated assessment efforts zare et al 2017 the lack of clear boundaries of such systems are often considered to be part of the problem voinov and bousquet 2010 reconciling conceptual differences and perspectives between human actors can be demanding but not insurmountable there are various methods available for group decision making such as the delphi technique gokhale 2001 which can be used to help the group reach agreement on the definition of the problem and or the system boundaries the subsequent modeling itself can be used to combine and reconcile different views among stakeholders and may be useful in cross cultural or particularly contentious settings cf potter et al 2016 the influence of modeler and stakeholder bias can also be constrained such as by using numerical optimization and or exploratory modeling processes martin et al 2017 reichert 2020 the influence of personal preferences is restricted by using the exploratory approach as it focuses on identifying the relevant scales and conditions or combinations of conditions that normally lead to desirable outcomes 3 1 2 stakeholder planning here stakeholder refers to the individual or groups that may affect or be affected by the modeling or have an interest in its outcomes freeman 2010 thus in this context the modelers and teams of modelers are also stakeholders there is a plethora of stakeholder focused approaches e g in integrated modeling participatory modeling but these methodologies are still limited in their capacity to deal with scale specific questions and challenges brought by sos modeling jordan et al 2018 generally participatory approaches aim to bring together the multiple goals issues and concerns of interest from multiple scales and governance systems by developing a mutually beneficial relationship between stakeholders thompson 2009 thoughtful consideration of transparency traceability and governance issues in engagement and participatory processes cockerill et al 2019 glynn et al 2017 will be essential for optimizing saliency legitimacy and credibility of the sos modeling cash et al 2003 the participation of a higher diversity of stakeholders in such processes allows for a more holistic representation to be developed covering potential blind spots in the system conceptualization and avoiding the siloing of knowledge hoekstra et al 2014 including further perspectives may increase the complexity of the modeling and so requires careful management of individual expectations and biases martin et al 2017 management of an sos may at times be predicated on effective management of stakeholders and their level and capacity of involvement ostrom 2007 boone and fragaszy 2018 increases in the variety of perspectives also increases potential for conflict defined here as disagreements of any degree between teams team members and or stakeholders on the one hand there is evidence that conflict plays a positive role in learning and effective teamwork tjosvold et al 2003 such positive benefits however may only occur in cases where there are high levels of pre existing trust within the group and when the conflict is task related rather than interpersonal de dreu 2008 power dynamics within teams and stakeholders therefore need to be considered national research council 2013 identification and focus on objectives that require participants to work together known as goal interdependence is an identified foundation towards project success and may additionally help in avoiding conflict knight et al 2001 lee et al 2015 tjosvold et al 2003 careful design and management of interactions between teams and stakeholders requires an explicit consideration of how the multiple and at times contradictory objectives might align or connect approaches to conflict resolution and prevention e g boundary critiquing midgley and pinzón 2011 are promising but still under utilized techniques effective stakeholder engagement will in practice be impacted by geographic spread allen and henn 2006 as the realities of scheduling rarely allow all stakeholders to be engaged at the same time and place additionally a diversity of stakeholders e g policy makers scientists and the public mean material and modes of communication may need to be tailored for each online participation platforms and technologies extends the reach to participants and are appealing for their asynchronous and distributed modes of engagement yearworth and white 2018 these relatively new technologies are simply tools however and a capacity to both use and leverage their advantages is also required cooke et al 2015 regardless of how interactions are to occur without documenting a record of engagement and decision making roed cockerill et al 2019 the original purpose assumptions and social and biophysical context of the engagement and resulting model choices might be lost leading to mismatches in understanding conceptualization and implementation the literature is still limited on the effectiveness of using different participatory methods for different purposes and audiences voinov et al 2018 nevertheless plans for stakeholder engagement for sos modeling should explicitly consider the scaling challenges and devise strategies to deal with these 3 1 3 preliminary conceptual model the preliminary conceptual model represents the current understanding of the system and the relationship between constituents including identification of key drivers interactions and outputs of interest badham et al 2019 in describing and capturing the essence of the system development of the conceptual model helps with the design of the subsequent computational model as well as making concrete the model purpose two scale specific aspects are to be considered here the approach used for conceptual model development see table 2 for a general overview and the formal representation e g equations technical specifications etc the processes that are included or excluded based on actors perceptions priorities beliefs and values under the sos context will inevitably influence the data leveraged the properties of the computational model and therefore the paths taken few mapping techniques exist that focus on illustrating multi scale representations scale separation maps hoekstra et al 2007 or stommel diagrams scholes et al 2013 represent the scales of the constituent systems on a two dimensional space time map system diagrams such as the representations used in van delden et al 2011 and oxley and apsimon 2007 organize the system components according to their spatial and or temporal scales and show the interactions between these components on the other hand coupling diagrams falcone et al 2010 show the flow of data between models a further approach is to use the odd protocol named after its three blocks overview design concepts and details grimm et al 2006 the original purpose of the odd protocol was to describe and enable transparent communication of agent based models abms to ensure their replication and the reproducibility of results based solely on the model description grimm et al 2020 the conceptualization involved in the overview block mandates identifying the scales of the processes or system components to ensure a shared understanding of the system being modeled this is further complemented with the identification of relevant resolutions and spatial temporal bounds at this stage the bounds can be vaguely defined e g local regional global this initial assessment of the scales involved may be revised throughout the modeling process as understanding improves the odd protocol is under continual development and planned additions extend its consideration and applicability of use to other areas not previously considered as outlined in grimm et al 2020 if differences in conceptual understanding of the scales and their interactions cannot be reconciled at this stage it is possible to create multiple alternative models representing the different hypotheses which can be tested in later stages of the modeling process such an approach can also assist in assessing uncertainty rooted in model building choices as the treatment of scale may affect model outputs and outcomes further discussed in section 3 2 4 although conceptual diagrams can be developed without specifying the scales involved explicit consideration of scale is valuable for avoiding misinterpretation of the conceptualization and ensuring key variables and processes are included a useful reflexive exercise not usually reported but aiding transparency is to identify what alternative approaches were considered or could have been considered and how these may have affected results and outcomes if adopted 3 2 development phase 3 2 1 collecting data information and knowledge data information and knowledge for each constituent model may come from the field or through literature solicited through expert and stakeholder engagement or collected through analysis considerations towards data collection in the integrated setting have been previously explored in badham et al 2019 correctly communicating and interpreting data across heterogeneous systems however requires that the data are interoperated between constituent models and that model behavior across scales remains valid and meaningful renner 2001 for this purpose metadata serves an essential role transparency in the collection process and approval from those involved in the modeling are necessary to ensure that collected data remain conceptually relevant across scales furthermore transparency in the context of data collection and usage is a key factor to develop trust among stakeholders and model users and future adoption of the constituent models barba 2019 gray and marwick 2019 data may need to be transformed to be fully relevant for the context of its intended use such as up or downscaling to ensure compatibility with other processes ideally metadata would include information on the data collection uncertainty and transformation process which aids in determining the appropriateness of data for the sos model explicit descriptors of both input and output data can assist in identifying the commensurate level of data collection with respect to available resources modeler bias can have a compounding effect as the choice of data collection as well as the metadata that describes the data influences how system interactions are perceived and thus conceptualized bhattacherjee et al 2008 what may be considered irrelevant in one field may dictate modeling pathways in another in an sos setting there are many more participants involved and so there is a high degree of uncertainty stemming from the decisions made as a result data quality and informativeness e g accuracy or precision provided by constituent models may also be diverse diversity of data obtained from a diversity of sources however runs the risk of conflicting information gray et al 2012 modelers from different disciplines may also utilize different scales for the same process resulting in inconsistencies and thus errors the sources of which are difficult to identify in this regard non quantitative sources of information gathered from literature and or through stakeholder engagement may become key assets that resolve such issues grant and swannack 2007 in cases where data describing a particular linkage in an sos model are not available theoretical relationships generally applicable empirical relationships or model process and output can be useful representations for the purpose of the sos model rai et al 2002 the documentation developed in the scoping phase can be leveraged to ensure applicability and validity with regard to the model purpose 3 2 2 construction construction of computational sos models requires the marrying of domain expertise from across the various disciplines involved with technical software development knowledge while the overarching context may be well defined within the scoping phase it is in this construction step that the individual components and the scales they represent are developed and coupled tested and validated here existing models may be repurposed or new models developed the specifics of their initialization interoperation method of execution and management of the data involved are to be determined and prototyped in this phase igamberdiev et al 2018 madni and sievers 2014 a balanced approach is needed in sos model development that takes several factors into account there is a danger that the models themselves become treated as pieces of software that merely require connection ignoring the socio technical context for their intended use voinov and shugart 2013 another issue is the overparameterization of constituent and component models brun et al 2001 nossent and bauwens 2012 as simply integrating these models to form an sos model exacerbates issues of uncertainty and identifiability considerations of which are explored in the following sections at the same time ignoring the technical considerations of integration is also inadvisable verweij et al 2010 mitigating the issues that consequently arise becomes increasingly difficult as more systems and scales are included voinov and shugart 2013 wirtz and nowak 2017 requisite systems could be represented at the level of detail necessary for the sos model purpose through a tiered modeling structure little et al 2019 implementation of such a tiered approach can involve developing metamodels or entirely different system models metamodels being simplified representations of more complex models revisited in section 3 3 two pertinent issues in sos model construction are the focus below managing the conceptual inter connection between models and the process of integration 3 2 2 1 conceptual integration conceptual integration of constituent models can benefit from requiring that constituent models be mechanistic as opposed to black boxes when a model is implemented as a black box it becomes difficult to evaluate and understand lorek and sonnenschein 1999 sos modeling may make use of pre existing models which constitutes re purposing implying the transference of the model assumptions limitations and scale to a new context it is emphasized here that model suitability within its original context is not necessarily applicable to the new context ayllón et al 2018 belete et al 2017 voinov and shugart 2013 availability of code alone for example does not imply transparency what is important is the contextual information that is necessary to assess the suitability of the model purpose and functionality a key challenge then is ensuring the box remains open and transparent rather than closed and opaque opaque development can be attributed to the modular nature of constituent model development with the teams working separately both conceptually and geographically and often split along disciplinary lines such teams can be described as self organizing sletholt et al 2012 but may lack cross disciplinary knowledge cross functionality as in hidalgo 2019 hoda et al 2013 the lack of interdisciplinary communication between teams then results in black or at best gray box models to those not involved in their development what is important in this interdisciplinary context is clear documentation and an organizational culture that supports the perpetuation of the relevant contextual knowledge as previously mentioned in section 3 1 3 describing the model and its conceptual linkages in a single canonical document via the odd protocol introduced in section 3 1 3 is one approach that could be leveraged furthermore a nested odd approach may be adopted in the case of complex sos models wherein the constituent models may be another sos model 3 2 2 2 technical integration technical integration refers to the correctness of model interactions recognizing the distinction between conceptual or abstract representation e g an equation or flow diagram and its implementation as software successful technical integration of computational models requires the necessary engineering expertise to be available knapen et al 2013 crucial considerations are that constituent models interact and accordingly that errors will propagate cf dunford et al 2015 and that each constituent model may undergo its own separate development cycle which invariably necessitates continual adjustments to be made flexibility of integration is often desirable as it allows the model to be resilient against changes in the modeling scope flexibility facilitates investigations into model structure of both constituent and component models and the technical design considerations that lead to flexibility allows for the composition of different combinations of relevant code and data represented through a nested hierarchy e g loose coupling elag et al 2011 vale et al 2016 whelan et al 2014 use of integration frameworks are helpful in that they allow the treatment of individual models as loose composable modules that provide some flexibility in dealing with the range of scales involved current integration frameworks typically have their roots in specific disciplines and tend to focus on physical processes cf ayllón et al 2018 the open modeling interface openmi moore and tindall 2005 for example has had to evolve from its initial focus in the hydrological sciences to accommodate an interdisciplinary modeling process buahin and horsburgh 2018 thus while the processes and requirements of such frameworks may be generally applicable there remains some difficulty in their generic implementation and adoption within the interdisciplinary context of sos modeling in some cases such frameworks may be overly complex or otherwise unsuitable for the purpose and context in which the modeling is being conducted such difficulties may be resolved in the future as improvements to these frameworks are ongoing voinov and shugart 2013 often modelers adopt a less formalized approach to avoid an inappropriate or constraining framework in either case ensuring semantic and conceptual correctness between models is typically left to the modelers themselves cf hutton et al 2020 direct manual tight coupling of models without the use of integration frameworks is still very much the norm more recent efforts include a collaborative web based platform through which the conceptual semantic and technical integration occurs opengms in chen et al 2019 chen et al 2020 faster feedback between participants then allows identified issues to be addressed earlier other approaches provide a curated ontological set of descriptors for common phenomena of interest e g snowmelt or rainfall these can be referred to as system variables as in pacheco romero et al 2020 and efforts to record their quantities e g centimetre grams etc and relevant operators in a specific metadata format have also been undertaken e g the standard names in hobley et al 2017 having the inputs and outputs described and documented in such a way aids in reducing potential mismatches in later re use and could be used to enable later automated model coupling frameworks do not yet fully automate conversions or identify incompatible or inconsistent usage e g litres per second to degrees celsius although this is likely to change in the near future both the selected framework and constituent models may change over the course of the modeling cycle along with the scales represented such changes may affect its appropriateness with respect to the model purpose for example adoption of a particular framework or model may increase the computational requirements or necessitate changes to constituent models to allow interoperation inadequate consideration of the concerns and requirements of the modeling as a whole may occur in cases where cognitive constraints are still in place the modeling process may be smoothed if requirements of the later phases are kept in mind during the design construction or selection of models and the resources allocated including the availability of expertise to each of these activities 3 2 3 model calibration calibration is the process of tuning parameters or altering the functional forms of equations or relations to achieve desired model behavior bennett et al 2013 in sos modeling issues such as non identifiability and equifinality beven and freer 2001 guillaume et al 2019 curse of dimensionality bellman 2015 computational burden razavi et al 2010 and data representativeness beven and westerberg 2011 singh and bárdossy 2012 may all be amplified calibration implies the existence of appropriate and sufficient data to calibrate models against availability of data relevant for the modeling purpose is a requirement no matter how perfect the model may be conversely a lack of data does not imply subsequent modeling is not useful a model with high uncertainty may still characterize uncertainty in a way that is meaningful to decision makers for example indicating the comparative tradeoffs between available management options reichert and borsuk 2005 assessment of uncertainty can be helpful in determining the relative worth of data to be collected to better characterize uncertainty and inform future modeling or research lópez fidalgo and tommasi 2018 partington et al 2020 such optimal experiment design approaches may also be leveraged to maximize the use of available data bandara et al 2009 lópez fidalgo and tommasi 2018 vanlier et al 2014 arguably model calibration within the sos paradigm can take three general approaches 1 calibration of each constituent model independently before integration 2 calibration of all models together after integration or 3 a combination thereof the first approach is the simplest and most straightforward as each constituent model would be calibrated within its own domain phillips et al 2001 while pragmatic it ignores the effect of representing different scales across the represented sos and system system interactions which in turn affects model behavior and performance of the individual constituent model if a model is considered calibrated when both an acceptable level of fit and reasonable parameter values are found as in anderson et al 2015 calibration in the disintegrated context does not necessarily transfer to the integrated context in other words what is reasonable in one context may not be so in another and the selected parameter values may not be robust to the change in context that integration brings due to the different scales interactions and data space involved the second approach is seemingly the most comprehensive approach to model calibration as every possible interaction between models could be present in the process of model calibration huang et al 2013 interdisciplinary knowledge is leveraged to ensure calibrated values are both reasonable for the expanded operationalization this then enriches the data space for individual constituent models and improves their performance jones et al 2017 the approach however has the following major barriers the search space for model calibration will be excessively large ling et al 2012 in addition new possibly erroneous interaction effects might emerge between the parameters of one model with those of another model especially with different scales of information which makes the response surface extremely complex for model calibration the calibration process might then become computationally cumbersome and or infeasible the available data with different scales may not be enough to properly constrain the model in the process of calibration ingwersen et al 2018 as it is not identifiable from the data guillaume et al 2019 there is a risk of overfitting as well as the available data might be insufficient to produce a generalized model that covers the integrated domain expert knowledge for each model may have scale constraints and may not be easily transferable to the full sos domain howard and derek 2016 in the third approach models are integrated one at a time incrementally adding complexity so that the influence of each constituent model can be directly attributed and subsequent issues can be addressed this approach may include modifying the conceptualization as necessary and sequentially calibrating the resulting integrated configurations duchin 2016 duchin and levine 2019 while this approach may be as pragmatic as the first and perhaps as comprehensive as the second the disadvantage is the time and computational cost to perform sequential coupling and calibration such an approach would seem more practical in cases where there is little disciplinary friction and a relatively small number of models to be integrated in all approaches above the role of expert knowledge in determining the acceptability of the calibration cannot be understated in management contexts for example change in policy e g the governing rulesets may impart shifts in system behavior that may be hard to discern by examining quantitative data alone and even more difficult to represent machine learning approaches may assist in identifying and representing non stationary system behavior e g rui wu et al 2019 razavi and tolson 2013 but still require intensive data for training and validation by experts where possible razavi and tolson 2013 and scale issues still exist between different single system models or different levels of model integration such information in one system may have implications for how other constituent models are calibrated and so interdisciplinary communication awareness and consideration of the intertwining issues is necessary to safeguard against mismatches a calibration method which seems not to have been used explicitly for sos models is pattern oriented modeling grimm and railsback 2012 railsback and grimm 2019 wiegand et al 2004 2003 here a set of patterns observed at different scales and levels of organization is used to reject as a set of filters unsuitable parameter combinations and process representations and may be closely related to the use of hydrologic signatures for hydrological model calibration and testing gupta et al 2008 as for parameters this approach corresponds to the rejection method in approximate bayesian computing van der vaart et al 2016 the basic idea is that a combination of weak patterns which by themselves do not contain much information and thus would not reject many parameter combinations can be as efficient as using a strong pattern which is highly distinctive but might not be available for models with multiple scales this approach holds high potential as it would help to keep both the sos and constituent models within realistic operation spaces 3 2 4 uncertainty analysis sos models often target large problem domains which necessitate complex models for their assessment and by their nature have a high degree of uncertainty for the discussion here we speak to the quantitative and qualitative aspects of uncertainty which may be further classified based on their source or primary influence prior literature for example speaks of model structure technical parameter scenario contextual and predictive uncertainty for further description see beven 2009 pianosi et al 2016 walker et al 2003 quantitative approaches aim to measure the effect of uncertainty in a specific parameter input or assumption on an output and allow the numerical characterization of the output distribution and therefore model behavior saltelli et al 2019 zimmermann 2000 qualitative uncertainty however cannot be characterized with a value and arises from sources such as the biases and subjective beliefs of human actors chen et al 2007 qualitative uncertainty can also arise from the modelers subjective judgment linguistic imprecision and disagreement across actors involved linkov and burmistrov 2003 refsgaard et al 2007 one reason for increased model uncertainty in sos modeling is the complexity that is largely a result of the increased scope of modeling which comes with a larger number of models and people and their perspectives involved the increase in the number of actors typically results in an increase in the overall number of parameters and their possible interactions oreskes 2003 the number of possible decision pathways in the modeling process lahtinen et al 2017 and the level of stakeholder influence at each decision fork ostrom 2007 increasing model complexity allows for a higher fidelity model but can also increase the perceived uncertainty in a traditional sense known as the complexity paradox oreskes 2003 characterizing true uncertainty in an sos model however is impossible as it requires a model that represents everything perfectly including unknown unknowns hunt 2017 uncertainty may then compound with each interaction across constituent models in the sos framework propagating some amount of error dunford et al 2015 thus it becomes progressively difficult to gain insights as to what effect and influence the combinations of these have structural and parameter identifiability as in bellman and åström 1970 guillaume et al 2019 high levels of model uncertainty need not be a barrier to effective decision support however and is ameliorated by providing estimates or assessments of such uncertainties reichert and borsuk 2005 both quantitative and qualitative different strategies and further considerations for uncertainty assessment are needed in sos modeling compared to single system modeling one commonly suggested approach to restricting model complexity and possibly runtime is to screen for insensitive parameters pianosi et al 2016 such parameters are said to have negligible influence on model output and so may be fixed i e made static in subsequent analyses or otherwise removed from the model another is to tie related parameters so that they may be represented by a single hyperparameter raick et al 2006 reducing the number of parameters however does not necessarily equate to a reduction in uncertainty rather it may simply mean consideration of an uncertainty source is determined to be unimportant for a given context or purpose pianosi et al 2016 and doing so may trade off model fidelity under new unseen conditions use of a constituent model within an sos model as opposed to its individual operation or its modification or simplification through parameter screening and tying constitutes a change in context therefore parameters initially found to be influential might become inactive and non influential and vice versa or the relationships that led to parameters being tied may change the change of context also changes the relevance of the assumptions and objectives and what constitutes an appropriate uncertainty analysis song et al 2015 uncertainty analysis conducted in one context is not valid across all scales thus premature model simplification may ultimately affect the appropriateness of the sos model for its overarching purpose a comprehensive sensitivity analysis under current and possibly alternative conditions can provide valuable insights into a key question when and how does uncertainty matter as discussed in razavi et al 2019 an alternate view is that given the likelihood of limited computational resources efforts to characterize and communicate uncertainties to stakeholders may be more beneficial than an exhaustive sensitivity analysis reichert 2020 anderson et al 2015 an additional consideration is that a constituent model may be a legacy or third party model that cannot be modified e g due to lack of access to the underlying code this would introduce some hidden or uncharacterized uncertainty into the sos modeling in this case metamodeling expanded on in the next subsection might provide some help in simplifying the model explicit documentation of the criteria used for each constituent model can ensure relevance of its application and reduce contextual uncertainty see walker et al 2003 across all the scales involved accordingly in the recent update of the odd protocol grimm et al 2020 a standard format for describing models the element purpose has been changed to purpose and patterns with patterns being the multiple criteria for ensuring a model s structural realism as defined in the pattern oriented modeling strategy grimm 2005 grimm and railsback 2012 the effect and relative importance of model structure uncertainty may be assessed through expert and stakeholder knowledge of alternate models van der sluijs 2007 and bayesian approaches could be applied to characterize the known unknowns clark 2005 uncertainty matrices have also been suggested as a tool to qualitatively identify and document the source type and nature of uncertainty and assess its relative priority in a table like format see refsgaard et al 2007 koo et al 2020 increased consideration of technical uncertainty adopting the term from walker et al 2003 is another area which warrants further consideration in the sos modeling context choice of what infrastructure and technologies to use is likely to stem from the prior experiences of the team s involved constituent models may be run on different infrastructure than was originally intended especially as issues around computational reproducibility are addressed barba 2019 hutton et al 2016 identical code run under different computational environments may produce different results see for example bhandari neupane et al 2019 such infrastructure may differ in physical or virtual architecture e g laptop supercomputer or operating systems or method of generating interpreting code e g different languages compilers package versions various combinations of these may be used and may also differ in the development and application phases for these reasons the influences of different and interoperating infrastructure are important considerations iwanaga et al 2020 correlation between parameters is another issue that is often ignored in the characterization and attribution of uncertainty do and razavi 2020 correlation refers to statistical dependency between parameters it is different from interaction effects which refer to the presence of non additive operations among two or more factors embedded in constitutive equations of the model in sos modeling the issue is further escalated as possible correlations between the factors of different models needs to be accounted for ignoring correlations can falsify any estimation of uncertainty do and razavi 2020 3 2 5 testing and evaluation testing and evaluation can assist in the assessment of the ramifications of scale choice in this step reasonableness of model structure and interpretability of relationships within models are assessed along with the traditional analysis of model behavior not all outputs produced by the constituent models may be relevant for the sos model purpose and the validity of their outputs are affected due to the integrated nature of sos modeling for any evaluation to be effective the specific model outputs of interest that are relevant for the model purpose must be well understood outputs may be at a particular spatio temporal scale for instance a long term average of a model output over a large spatial domain or an extreme event at a specific point location issues may also stem from the conceptual suitability of constituent models as uncertainty may be propagated throughout and may compound as more models are integrated dunford et al 2015 thus the first step in testing and evaluation involves attempting to refute aspects of sos model structure and functional relationships within the model based on their lack of correspondence with the represented system and the model outputs stakeholders could be leveraged to evaluate the conceptual alignment and appropriateness of the sos representation at the selected scales evaluation of the behavioral relationships at the integrated level is similar to scientific hypothesis testing wilson et al 2017 or conceptual testing iwanaga et al 2020 wherein functional relationships within the sos model are examined such tests may be especially useful in cases where the internal workings of a model are inaccessible or otherwise unknown but expected behavior of the constituent model in the integrated context can be characterized iwanaga et al 2020 these approaches can be used to identify impossible or implausible aspects of the sos model output if any aspect of model structure or any functional relationship within the model can be shown to be an inadequate representation of the corresponding aspects of the real system then that particular portion of the model is refuted li et al 2016 examination of model behavior over a range of inputs will also help to expose additional inadequacies in the model bennett et al 2013 the interesting aspect in this regard is that successful testing and evaluation of the constituent models does not guarantee correctness of the sos model and vice versa testing and evaluation may happen at different scale levels and acceptable model behavior depends on the model purpose and consequent measures or indicators of interest model behavior of constituent models could be examined quantitatively through assessment of the intermediate data in the models to ensure their behavior is consistent with a priori expectations it is necessary to test the software used to interoperate data across the different hierarchical levels using relevant testing approaches these include checking the mapping of input outputs between models conversion of units use of metadata to perform semantic operations and translation of spatial temporal dimensions ayllón et al 2018 belete et al 2017 voinov and shugart 2013 testing processes found in software engineering may additionally aid in conducting such checks see for example laukkanen et al 2017 verweij et al 2010 yoo and harman 2012 it may also be possible that some data gaps or uncertainties from constituent models have a lesser or negligible effect on the sos model depending on how the constituent model is leveraged at the sos level furthermore constituent models may present overlapping and or conflicting data or assumptions that will only be revealed when testing and evaluating their integration a common example is double counting uncertainty due to embedded assumptions in the model or failure to detect correlated variables with a common cause the next step focuses more specifically on the correspondence between model projections and observed data strictly speaking data used in model testing and evaluation must be independent of data used to develop the model raick et al 2006 a variety of visual statistical and machine learning methods are widely used to evaluate sos models the choice of method however should be based on the fundamental questions of what scenarios and observations to use in the evaluation evaluation of models under the range of conditions similar to those of interest can aid in identifying limitations of the model ramaswami et al 2005 sensitivity analysis is now regarded as standard practice in modeling norton 2015 pianosi et al 2016 razavi and gupta 2015 the sensitivity of sos model behavior to changes to its constituents and their interactions is the target of the assessment moriasi et al 2007 an issue stemming from the likely overparameterization of constituent models is equifinality and the lack of identifiability equifinality refers to the phenomenon of different implementations or combinations of model structure parameter values and their interactions producing equally acceptable results wagener et al 2003 beven 2006 identifiability then refers to the ability to attribute the influence on model outputs to unique model parameters or structure muñoz et al 2014 guillaume et al 2019 therefore the greater the number of parameters the less identifiable the model becomes sensitivities are assessed as part of identifiability analysis typically by ranking parameters based on their influence on outputs which can aid in determining what parameters require focused efforts to reduce uncertainty or improve identifiability e g factor prioritization nossent and bauwens 2012 information from sensitivity and identifiability analysis can then aid in simplifying the model as discussed in the previous section similar to what was noted in section 3 2 3 naively applying sensitivity and identifiability analysis without consideration of the sos context may adversely affect modeling outcomes assessment of sensitivities would ideally rely on global rather than local analyses for reasons that have been expounded in prior literature see for example pianosi et al 2016 saltelli and annoni 2010 use of global sensitivity analyses in model assessment has seen increasing use despite the lack of uptake or reported use of available software tools to conduct such analyses douglas smith et al 2020 still the importance of such analyses tends to be under appreciated saltelli et al 2019 one practical reason for the lack of global sensitivity analyses is that they are typically computationally expensive to perform and the sos models themselves typically exhibit long runtimes dependencies and correlations between parameters across constituent models and their respective scales pose another challenge koo et al 2020 metamodeling expanded on in the next section along with recently developed sampling and analysis methods may be more amenable to the sos context examples of such methods that warrant further investigation include moment independent methods such as pawn pianosi and wagener 2015 which can be applied independent of the sampling scheme used and variogram based approaches e g star vars razavi and gupta 2015 which can reportedly account for temporal and spatial correlations adaptive sampling of the parameter space through sparse grids for example in combination with these analysis techniques may also aid in reducing the computational costs associated with sensitivity and uncertainty analyses buzzard and xiu 2011 xiong et al 2010 3 3 application phase a critical aspect in the application of sos models is that constituent models evolve independently development of each constituent model by necessity is led by disciplinary experts and undergoes separate asynchronous development cycles as each model may come from different paradigms and sources of knowledge the implementation may be adjusted over time or even replaced in response to newly acquired knowledge advancing towards trial model applications using the expected type and volume of data as early quickly and often as possible allows modelers to encounter issues in the model application earlier in the process warren 2014 experience gained with each iteration subsequently serves to rectify and protect against future application challenges application of the model then requires monitoring and scrutinizing to ensure the underlying models including their metadata represented knowledge and application context remain current and appropriate when models are integrated the runtime may prevent practical application for its primary purpose such as social learning through interactive use with stakeholders or for global sensitivity analyses one option to overcome this problem is to simplify the constituent models for the specific purpose doing so requires a high degree of knowledge of the constituent models however and may not be practical in cases where legacy models are used spatially explicit models can especially be a problem in regard to runtime and a solution for reduction in computational burden may be achieved through aggregating grid cells into similar zones e g groundwater model aggregated into hydraulic conductivity zones elsawah et al 2017 in cases of high runtime replacing the most computationally expensive constituent models with metamodels may be a viable option metamodels approximate the input output behavior of the original model castelletti et al 2012 christelis and hughes 2018 pietzsch et al 2020 and therefore provide simplified representation s of more complex models asher et al 2015 razavi et al 2012 metamodels leverage the emergent simplicity of complex systems and although there are a variety of methods available to accomplish this generally metamodels require the complex models i e the original constituent models to be available beforehand metamodels being approximations of an original model s response surface are most relevant to the conditions existing in the datasets upon which they are tuned so care needs to be taken if using them under conditions that transcend those extant in the data system forcing data beyond that experienced such as climate change or groundwater extractions are of particular concern in this regard if possible simply allocating more computational resources e g supercomputers may be the most pragmatic and resource efficient alternative especially considering the time taken to investigate and implement the options listed above it is acknowledged however that more computational capacity may not be available 3 3 1 analysis and visualization in the management context where sos models are typically applied there is a need to adequately describe the level of uncertainties in the sos model and its predictions individual stakeholders may react differently to uncertainties and levels of uncertainty cockerill et al 2019 presenting scenario results relative to the modeled baseline neatly reduces the inherent biases that come with relying on stakeholder preferences to inform desirable thresholds as would usually occur in multi criteria or multi objective analysis approaches maier et al 2016 martin et al 2017 reichert and borsuk 2005 with such an approach the acceptability of a possible maximum or minimum relative change becomes the focus of stakeholder discussion software tooling for supporting analyses of model results including sensitivity and uncertainty analyses typically necessitates interaction between the analysis software and the model s which may require the development of additional interfaces i e code or supporting software due to the number of models involved the associated parameters and the possibly dynamic model structure wirtz and nowak 2017 maintaining these interfaces in the sos context may quickly become unwieldy additionally it may be desirable to replace entire models to analyze the influence of model structure and the scales they represent ewert et al 2011 thus potentially rendering existing interfaces obsolete recent efforts circumvent this issue by supporting the near seamless transition between the nested hierarchical representation common in sos design to the conceptually simpler flat structure expected in typical analyses e g schouten and deits 2020 an example of nested and flattened representations of a node network is provided in appendix 1 a common requirement shared with tooling for conducting analyses e g for sensitivity and uncertainty analysis and exploratory modeling is the provision and definition of parameter values these may consist of a default value a range within which values may vary whether these values are categorical scalar or regarded as constants examples may be found in adams et al 2014 kwakkel 2017 pianosi et al 2015 razavi et al 2019 categorical values may indicate substitution with other data types or a collection of data types e g rasters climate sequences etc such information may be the minimum necessary to conduct such analyses to reproduce and replicate results and to support later automation of these activities parameter values in effect represent dimensions of scale and the inappropriate selection of their values and ranges may result in misleading results shin et al 2013 wagener and pianosi 2019 3 4 perpetuation phase as in badham et al 2019 perpetuation is about the intended influence the modeling is to have into the future the focus here is on the scale of documentation and process evaluation in sos modeling which is informed by the level of consensus among stakeholders and modelers as to its purpose in the research context for example there is a newfound expectation that the model be developed and provided in a manner that supports reproducibility and replicability reproducibility is the ability to recreate results whereas replicability captures the ability of the model to generate new but consistent data in other applications patil et al 2016 where sos models are used by external stakeholders some amount of technical support is likely expected without this use of the model and thus its impact is likely to be minimal computational models are software in that they are made of code and so continued use comes with a baseline cost to cover maintenance improvements and updating of documentation such capacity is crucial in contexts where long term management and decision support is an acknowledged requirement in such cases the design implementation and documentation of the model should plan for these long term activities from the beginning in the sos context this implies retaining the interdisciplinary knowledge within a team or organization e g cockerill et al 2019 kragt et al 2013 3 4 1 documentation whereas earlier sections spoke to the content of documentation this section focuses on the role of documentation in an interdisciplinary setting such as sos modeling documentation is a conduit through which information and knowledge are propagated and provides the necessary context for model evaluation cockerill et al 2019 without sufficient documentation it is difficult to understand the context that led to any specific issue including mismatches between constituent models lack of context then affects the perceived validity of the model conceptualization restricts model use rendering the model inappropriate or invalid for its purpose the act of documenting itself allows for reflexive and transparent communication and for new insights to be gained undocumented assumptions regarding scale and their influence may compromise other constituent models thus holistic awareness of the sos issues can be obstructed by a lack of documentation long term maintenance and use of the model may also be impeded ahalt et al 2014 no individual holds the knowledge and awareness of the modeling details in their entirety let alone the effects of interactions between models it is therefore important to recognize that writing and maintaining documentation should be a team effort and a culture to support this should be fostered in practice there are few incentives for documenting models to such an extent a key problem in sos model documentation is that details of the constituent models important for the sos team may be considered unnecessary for the teams developing the constituent models once again this stems from potential disconnects between the purpose of the sos model and the individual or original objectives of each constituent model in the sciences the focus is often on the publication of papers at the expense of ensuring model reuse or reproducibility and replicability easterbrook 2014 joppa et al 2013 peng 2011 schnell 2018 there is an increasing push to change the culture surrounding the publication process however to better recognize credit and incentivize model code publication for example a number of organizations have begun supporting open code badges to highlight reproducible work https www comses net resources open code badge 3 4 2 process evaluation the extent to which the modeling has achieved its overarching purpose is evaluated in this step badham et al 2019 this evaluation extends beyond the technical performance of the sos model bennett et al 2013 to consider outcomes of modeling as a social process success of a model depends on the beliefs and expectations of the intended users and in their satisfaction with the model and its results hamilton et al 2019 it may also depend on the biases and beliefs of the model creators glynn et al 2017 and in an alignment of expectations between creators and users sterling et al 2019 the suitability of the success criteria is dependent on the context of the project including not only the model purpose but also the characteristics of the problem such as its complexity and the resources that were available hamilton et al 2019 process evaluation in sos focuses on two facets achievement of goals and longevity of the models in terms of goal achievement process evaluation considers whether the goals of the sos model were supported by its constituent models and where applicable whether constituent models achieved their own goals although satisfying the goals of the constituent models may seem an indirect path to satisfying the goals of the sos model this interpretation is misleading an sos approach to modeling instead of simply a multi modeling approach leverages the autonomy and independence of the constituent models constituent models still need to be capable of yielding their own outcomes regardless of how those models are used in the context of the sos model salado 2015 evaluation of the longevity of the sos model referring to the ability to leverage or reuse the sos model over time requires the development and assessment of a targeted plan for its sustainment that includes 1 monitoring the evolution of the constituent models 2 identifying alternatives for models that may cease their validity availability or accessibility during the lifetime of the sos model 3 establishing a strategy for the continued evolution of the sos model including the development of potential transformation frameworks and implementations and 4 identifying opportunities to facilitate the sustainment of constituent systems aligned with the sustainment of the sos model process evaluation for sos models may consider adopting a reflexive process in which questions are asked of those involved in the modeling such as did the modeling process help to improve understanding of the system problem or did the modeling process help facilitate communication between stakeholders hamilton et al 2019 the line of questioning can then leverage input from the various perspectives available including those of experts and stakeholders for the different constituent systems of an sos bias in the model such as whether their respective positions were adequately represented may then be assessed alternative conceptions and processes of the system and their scales could also be assessed at this stage voinov et al 2016 4 the paths forward 4 1 a grander vision and commensurate funding addressing all the scale related issues outlined in the paper requires a level of cooperation and concerted integrative effort that is by and large not possible given the usual short term funding of the sciences e g saltelli 2018 recent publications have also brought attention to deficiencies in the current science resourcing structure characterized in part by competition over limited funding and an emphasis on number and citation counts of publications existing funding mechanisms may well be detrimental to the quality of science produced binswanger 2014 sandström and besselaar 2018 limited resourcing is one reason for the multiple albeit siloed efforts with a focus on single case studies pulver et al 2018 hoekstra et al 2014 and the necessity of excluding salient aspects of the modeling such as adequate participatory processes eker et al 2018 or making less than ideal choices about the model or data e g using existing coarser scale data rather than collecting new data at a finer scale commentary by researchers highlight the importance of interdisciplinary work kretser et al 2019 meirmans et al 2019 which is typically not funded to the same extent as monodisciplinary efforts kwon et al 2017 bromham et al 2016 regardless of the importance of such holistic assessments these real world constraints essentially make holistic sos modeling and analyses unrealistic on the other hand examples of large concerted efforts can be found such as in astronomy and physics which have produced groundbreaking work with the event horizon telescope e g first photograph of a blackhole akiyama 2019 and the large hadron collider e g discovery of the higgs boson aad et al 2012 these resource intensive projects are important and could substantially influence future societal development at the same time lesser importance is placed by funding organizations on interdisciplinary socio environmental works which arguably have a more immediate impact and benefit to society a grander vision for sos research in line with large scale collaborations in other fields is vital to achieve a truly holistic consideration of sos modeling for resolving socio environmental issues realizing this vision itself requires fundamental shifts in how such interdisciplinary work and associated expertise are viewed and funded elsawah et al 2020 greater funding focused on education and training of interdisciplinary system practitioners is fundamental for greater cohesion and consensus in the socio environmental sciences little et al 2019 while alternative funding models have been suggested for the sciences see for example meirmans et al 2019 higginson and munafò 2016 the current state of affairs is unlikely to change in the near future thus any benefits from a systemic change if they occur at all will be experienced only in the long term although disciplinary experts may collaborate pool resources engage with stakeholders and gain experience in interdisciplinary work in the process of investigating a socio environmental issue this is not an effective way forward in the medium term existing case studies could be leveraged to perform a comparative meta analysis to determine the level of influence system connections have and the scales at which such connections matter pulver et al 2018 such meta analyses could extend to the practices used to manage the socio technical influences in the modeling process shifts towards leveraging collections of studies for meta analyses are emerging in fields such as psychology to allow for what is known as statistical objectivity towards reported findings in the literature freese and peterson 2018 although the focus there is in resolving issues of replicability the same approach can be additionally leveraged to characterize scale commonalities we conclude here by re emphasizing three key considerations which can reinforce current sos modeling efforts in a move towards the larger consensus needed for this grander vision 4 2 strengthen interdisciplinary communication here lies the crux of the challenge in developing a tiered sos model it is not only necessary for the science and engineering to mesh together appropriately but it is fundamental that the modeling process also consider and embed the socio technical considerations while we as modelers struggle with the former the latter is too often ignored as there are a variety of participants and therefore disciplinary perspectives involved a key set of considerations are in the social dimensions that provide the interface between modeling efforts integrating multiple perspectives requires an integrative approach which is ultimately necessary to navigate towards a beneficial system change why else do we model choices made in the treatment of scale are unavoidable and may result in conflicting decisions with separate implications just to name one members of teams may have a path pre selected without full consideration of the implications on the system representations leading to further issues when such decisions are not communicated the next generation of systems modelers would ideally embody a culture that is cognizant of the socio technical issues considerations and their influences throughout the modeling process e g little et al 2019 such a systemic cultural shift can only be developed in the longer term however and so in the meantime clearer communication requires adequate resourcing for documenting decisions made and code and data used including their maintenance practices for the co production of knowledge to fulfill the needs and requirements of the modeling is necessary for advances to be made norström et al 2020 there is often a preference for face to face meetings to facilitate the necessary level of communication but that may not always be possible geographic distance scheduling conflicts travel restrictions and other factors may preclude such activities communication technologies play a critical role in mitigating some aspects of the issue for example travel and social distancing restrictions during the covid 19 pandemic has prohibited many teams from meeting in person forcing reliance on technologies such as video conferencing regardless of the mode of communication a team and organizational culture of consistent and continual communication is one necessity repeatedly highlighted to resolve a variety of scale issues and the conflict that may arise between actors throughout the modeling process incorporating knowledge beyond the bounds of one s own disciplinary training is crucial to the holistic attention to and incorporation of scales and to avoid the siloing of information and knowledge and to break down cognitive constraints 4 3 improve documentation processes the importance of documentation is another aspect that was repeatedly raised throughout this paper documentation of the modeling process communicates and makes accessible the decisions actions the context of those decisions and actions and reflection on those choices to those who may or may not have been active participants in their making insufficient documentation affects many aspects from the pace of model development throughout the modeling cycle quality of model integration especially across disciplinary boundaries and the perceived quality of the modeling conducted a lack of documentation accessibility additionally affects the re use and maintenance of the sos model or its constituents and so could lead to duplication of effort across those involved in modeling sess one approach to ensure that documentation is made a priority is to adopt a documentation driven development and design approach heeager 2012 such approaches are exemplified by the odd protocol grimm et al 2020 2014 2010 in this paradigm documentation is developed first serving as a vehicle for discussion ideally prior to any model development heeager 2012 ambiguities in the documentation and thus the modeling may be addressed earlier in the process as a result and documentation could be iteratively revised commensurate with any changes to modeling scale furthermore maintaining records of engagement and decision making roed cockerill et al 2019 to document the process and pathway decisions were made in a context appropriate manner may be crucial to ensuring conceptual and technical validity throughout the modeling cycle sufficient rather than exhaustive documentation to describe model context would be preferred ambler 2002 cockerill et al 2019 4 4 explicit consideration of scale and uncertainty there is an increasing expectation that sos models can more completely represent processes within an ses however it is impossible to model everything for all purposes further explicit consideration of the inter relationships between scales choices made in representing scale and their influence on uncertainty is paramount in the sos context identifying managing and reconciling the disparate treatment of scale is a key step towards a holistic approach as opposed to the concurrent but separate processes currently applied cheong et al 2012 elsawah et al 2020 as noted several times throughout this paper the socio technical context has an inordinate influence on uncertainty in addition to the communication and documentation considerations outlined above an avenue for a more holistic assessment of uncertainty includes the use of robustness analysis grimm and berger 2016 in such analysis a model with multiple systems is systematically deconstructed through forceful changes to the model parameters structure and process representations within each system to assess uncertainty use of these approaches with pattern oriented modeling processes which filter unsuitable representations across scales may also be helpful in this regard grimm and railsback 2012 gupta et al 2008 additionally qualitative and quantitative uncertainties could be jointly assessed through the representation of multiple plausible futures that stem from different sets of assumptions through exploratory approaches maier et al 2016 roberts et al 2018 rounsevell and metzger 2010 a related approach is a multi model approach wherein an ensemble of equally plausible models are applied to identify the influence of structural and qualitative uncertainty matott et al 2009 tebaldi and knutti 2007 uusitalo et al 2015 using an ensemble of estimates such as the average or median of model outputs may have the benefit of providing more robust and accurate forecasts willcock et al 2020 applying these on different computational platforms may additionally assist in identifying technical uncertainties iwanaga et al 2020 it was noted throughout this paper that the scale of the modeling itself should be commensurate with the available resources and purpose a holistic sos model may not be entirely possible given resource constraints however relationships between systems can still be acknowledged and represented albeit simplistically doing so allows some assessment of the uncertainties at least and constitutes a step towards holistic sos modeling so long as the underlying assumptions are explicitly documented e g kloprogge et al 2011 declaration of competing interest none to declare acknowledgements this work was supported by the national socio environmental synthesis center sesync under funding received from the national science foundation dbi 1639145 the primary author takuya iwanaga is supported through an australian government research training program agrtp scholarship and a top up scholarship from the anu hilda john endowment fund hsiao hsuan wang and tomasz e koralewski acknowledge partial support from usda ars agreement no 58 3091 6 035 with texas a m agrilife research titled areawide pest management of the invasive sugarcane aphid in grain sorghum regional population monitoring and forecasting min chen is supported by the key program of nsf of china no 41930648 john little acknowledges partial support from nsf award eec 1937012 the authors would like to thank the three anonymous reviewers and prof randall hunt usgs for their constructive feedback and comments the authors additionally thank faye duchin and adrian hindes for comments provided on an earlier draft any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government appendix 1 example of hypothetical model inputs for a hydrological routing model provided in a nested data structure left column compared to a more traditional flat format right column nested structures are arguably better suited for representing collections of data structures and their relationships e g a network or graph structure and pragmatically are typically more amenable to the inclusion of comments and multiple values associated with specific parameters reducing cognitive overhead while perhaps more readable a disadvantage of nested representations is the additional complexity that may be perceived image 1 
25905,system of systems approaches for integrated assessments have become prevalent in recent years such approaches integrate a variety of models from different disciplines and modeling paradigms to represent a socio environmental or social ecological system aiming to holistically inform policy and decision making processes central to the system of systems approaches is the representation of systems in a multi tier framework with nested scales current modeling paradigms however have disciplinary specific lineage leading to inconsistencies in the conceptualization and integration of socio environmental systems in this paper a multidisciplinary team of researchers from engineering natural and social sciences have come together to detail socio technical practices and challenges that arise in the consideration of scale throughout the socio environmental modeling process we identify key paths forward focused on explicit consideration of scale and uncertainty strengthening interdisciplinary communication and improvement of the documentation process we call for a grand vision and commensurate funding for holistic system of systems research that engages researchers stakeholders and policy makers in a multi tiered process for the co creation of knowledge and solutions to major socio environmental problems keywords social ecological modeling interdisciplinary modeling integrated modeling scale issues system of systems approach 1 introduction socio environmental systems ses function across a range of inter related scales that collectively represent a system of systems sos the term sos has been used since the 1950s and various definitions exist nielsen et al 2015 in this paper we distinguish between an sos as a collection of human and natural systems and sos models which are engineered representations of an sos the former is defined as an interconnected collection of multiple heterogeneous distributed systems that collectively may give rise to emergent behavior where each system represents a process or set of processes in the modeling of sos we follow little et al 2019 who define sos models as a collection of independent constituent systems in which each fulfills its own purpose while acting jointly towards a common goal p 84 in environmental modeling sos models may take the form of integrated assessment models iams or more generally integrated environmental models iems which are commonly applied to inform environmental management processes ewert et al 2011 iwanaga et al 2020 letcher kelly et al 2013 matott et al 2009 central to sos modeling is the view of system representations as a multi tier structure with different levels of abstraction where systems and indicators at lower levels can be scaled up to higher levels these representations capture processes that operate at different scales e g temporal spatial organizational in contrast to single system approaches which assume such drivers to be exogenous and crucially do not account for any feedback mechanisms between the represented systems this view also sets the focus on how to integrate knowledge from the different disciplines involved and coordinate information exchange among these in a consistent and meaningful way knowledge integration is not limited to the technical coupling of models but to integration among multi scale stakeholder and expert processes this combined socio technical focus makes scale issues and their treatment a core consideration of sos modeling 1 1 the need for a holistic treatment of scale a crucial ingredient in sos modeling is attending to the socio technical processes involved representation of scales is defined by modelers for a particular purpose and is ultimately subject to human processes meadows 2008 accordingly the representation of an sos is the end product of what the people involved implicitly or explicitly have chosen to represent and how they implemented their choices these then influence the model structure and uncertainties embedded and the consideration of its different dimensions analyses conducted and data and methods used glynn et al 2017 gorddard et al 2016 voinov et al 2018 such choices are subject to the available knowledge experiences biases beliefs heuristics and social values as well as the perceived purpose s of the modeling a key scale issue in sos modeling is the development of a consistent and defensible characterization of scale elsawah et al 2020 existing systems analysis and modeling approaches tend to come from entrenched disciplinary paradigms and so with a specific focus on their scales and facets and embedded language and terms inconsistencies then manifest in the conceptualization and treatment of scale in sos approaches which prevent researchers from 1 understanding the implications of scale choices 2 formulating implementing and validating models that are relevant to the questions of interest 3 predicting future sos responses in support of decision making elsawah et al 2020 little et al 2019 razavi et al 2020 and 4 communicating modeling results in ways that help identify trade offs and synergies within an sos and among the systems under investigation fridman and kissinger 2019 miyasaka et al 2017 addressing issues that arise from the conceptualization and representation of multiple scales are often omitted or left for future discussion ayllón et al 2018 discrepancies in the treatment of scale can be addressed firstly by developing a shared understanding of the system s being analyzed through a holistic interdisciplinary process thompson 2009 white et al 2019 there is increasing recognition that holistic approaches are necessary to enable an integrated assessment of scale issues in socio environmental social ecological systems schlüter et al 2019a 2019b hoekstra et al 2014 the rise of inter multidisciplinary fields such as socio hydrology elshafei et al 2014 sivapalan et al 2012 and eco hydrology hannah et al 2004 porporato and rodriguez iturbe 2002 gives further credence to this need for soss in particular it is necessary to additionally acknowledge the socio technical influences on their modeling explicit inclusion of the socio technical perspective pushes beyond traditional modeling approaches as it advocates assimilation of not only the data and mechanistic processes across different systems but also includes the knowledge and information held in the social institutions involved in the modeling 1 2 purpose the purpose of this paper is to advance knowledge and implementation of interdisciplinary sos modeling by identifying and articulating the practices issues and challenges involved with respect to issues of scale central to this interdisciplinary lens is making concrete the multidimensional nature of scale issues and the interplay among these here the term interdisciplinary is favored over trans or multi disciplinary as the focus is on the blending of disciplinary knowledge white et al 2019 the primary audience of the paper is modelers albeit in different domains and scientific disciplines with an interest in adopting an sos approach as a methodological framework in ses modeling in the following section 2 we first provide definitions for the key terminology used throughout this paper these definitions are not intended to be universal but are provided to contextualize and aid in communication given the range of disciplines involved in ses modeling in section 3 we explore issues of scale which need to be considered throughout the modeling we then describe in section 4 the long term challenges towards resolving such scale issues and suggest paths to be taken in the shorter term 2 concepts and definitions of scale 2 1 the process of defining scales sos models principally provide a representation of the interactions that occur between the systems involved holistic integration of knowledge from the various disciplines involved is necessary so that the implications of the different methodological choices on scale can be understood elsawah et al 2020 to this end a three day workshop was held in october 2019 in which a culturally and disciplinary diverse group of 20 participants convened to share their knowledge an additional 3 contributed in complementary ways to the drafting of this paper contributors originated from europe north america and the asia pacific and included engineers economists social scientists mathematicians physicists hydrologists computer scientists and ecologists to prevent miscommunication we developed a set of terms outlined in section 2 2 to build a shared language rubin et al 2010 spitzberg and cupach 1989 thompson 2009 although prior definitions of scale are available see for example cash et al 2006 gibson et al 2000 it was considered useful to develop a shared empathetic understanding of each other s perspectives banerjee et al 2019 thomas and mcdonagh 2013 the process additionally served to break down cognitive constraints macleod and nagatsu 2018 which may otherwise blind researchers to relevant notions of scale allowing disciplinary bias to creep in and knowledge gaps to form the range of disciplines involved in ses modeling often makes addressing cognitive constraints difficult as there are different notions of scale and related terms are used in different ways depending on context this variance has been observed in the use of common terms with conflicting definitions between and sometimes within disciplinary fields bridle et al 2013 2 2 scale terminology in sos modeling defining the terminology associated with scales was an arduous process at first owing to the diversity amongst workshop participants a brief overview of the resulting primary terms used in this paper is provided in table 1 for the discussion here scale is taken to have an expansive definition covering the scope of work to be conducted in the treatment and representation of system processes aspects of scale that had unanimous consensus included the commensurability of the choice of scale within the purpose of the modeling and the consistency of spatial and temporal scales across models it was also acknowledged that scale can mean many things beyond the spatial and temporal for example the less tangible such as treatment of ethical considerations within the modeling process e g häyhä et al 2016 regardless of definitions treatment of scales and the choices made in this treatment influences the model uncertainties and the outcomes of the modeling commensurability refers to the appropriateness of the selected approaches and methods for the sos modeling purpose broadly speaking these approaches can be described as being subject to socio technical considerations which are the focus of the discussion in this paper the social human aspect of modeling includes the circumstances of collaboration project management and participatory processes as well as those settings influencing the technical aspects including modeling and computational considerations the spatial and temporal features of a system are often the primary aspects around which scale is traditionally considered and framed these define the time and space of interest both their horizons and discretization and the events and processes that are considered important to represent cash et al 2006 the spatial scales selected may be influenced by the temporal scales of interest and vice versa their dependence can be intensified by the fact that spatio temporal scales are often influenced by factors outside their defined boundaries such influences may be important but may not be well understood or ignored zhang et al 2014b 2014a resolution defines the granularity of system representation and refers to the unit of spatial temporal scale represented in each system resolution may be spatial or temporal in nature but extends in other ways such as to social units individuals to families to communities etc and thus may be represented so as to conform to a semantic or conceptual hierarchy cash et al 2006 choice of resolution is highly dependent on the modeling context generally informed by the availability of data the needs of the model including for numerical stability sensitivity and model identifiability and model purpose hierarchy and their respective levels of organization relate to the representation of nested relationships among systems ostrom 2007 for example various governance systems may co exist at a range of scales with separate administrative or institutional concerns daniell and barreteau 2014 team based organizations are one example where the hierarchical scales may not be constrained to specific locations with members performing a variety of roles within an organization that may be geographically spread across different time zones actors influence and define the aspects of scale that are considered and may be both human and non human entities which affect or influence one another the term has its roots in the social sciences an example may be found in wessells 2007 actors have roles and carry out one or more activities in the system and can be represented individually or collectively human actors have attributes such as values goals and mental models which influence their behavior pahl wostl 2007 non human actors are defined here in a literal sense i e not an individual biological person such that organizations flora and fauna are non human actors but may still exhibit collective culture and personalities hobday et al 2018 schneider et al 2013 a system can encapsulate many actors and may be an actor itself the different types of system modeling encompass many terms that are often used interchangeably across the sciences as alluded to in the introduction we are guided by but do not directly adopt definitions as applied in system of systems engineering cf dahmann and baldwin 2008 here a single system model targets a specific system for instance an agricultural system without explicit representation of the hydrological dynamics or climatic influences consequently single system models constrain themselves to the concerns and considerations of a single sector models concerned with a single system may of course use several models internally e g crop growth soil water properties etc and these are referred to here as component models a direct approach to representing additional systems can be accomplished by applying albeit separately a selection of single system models for a given problem domain in such cases knowledge gained in the application of a model may inform the use of another data from one model may be fed into another and vice versa typically via manual processes for example a weather forecast model may be used to provide inputs to an agricultural model to determine seasonal effects on crops and the agricultural model may provide land surface boundaries to the weather forecast model multi system representations can be integrated by coupling models together such that data interoperation occurs in an automated fashion individual system level models are then referred to as constituent models the advantage of multi system models over their single system relatives is that the impacts and feedback mechanisms can be represented across between their individual scales elag et al 2011 tscheikner gratl et al 2019 wang et al 2019 multi system models with their explicit representation of system interactions are therefore capable of providing more holistic assessment compared to the use of individual models in isolation kelly letcher et al 2013 component based modeling stems from component based software engineering vale et al 2016 hutton et al 2020 and common usage in environmental modeling typically makes no distinction between constituent and component models e g malard et al 2017 a conscious decision has been made here to adopt the term constituent from the systems engineering field nielsen et al 2015 to convey this distinction it is important to note that integrated and multi system models could then equally apply to both single system models with several component or constituent models the requirement for a model to be regarded as integrated is that its component or constituent models are coupled together through the use of a common automated infrastructure to facilitate data interoperation see for example malard et al 2017 whelan et al 2014 by necessity multi system integrated models are more complex and may involve a variety of modeling paradigms e g bayesian networks agent based system dynamics etc and their combinations an sos model is then regarded here as an integrated model with constituent models each constituent model may be a single system or another sos model such that a tiered network of relationships between models is formed with each representing a layer of abstraction in sos modeling each constituent model may operate across different spatial temporal scales hierarchical levels and resolutions to incorporate multiple aspects of distinctly separate disciplinary or sectoral domains and modeling paradigms an sos perspective allows but does not prescribe consideration of complex system properties including nonlinearities interdependencies feedback loops thresholds and emergence 3 scale issues to consider models are developed through a life cycle of various phases each with specific considerations and steps the modeling cycle grimm and railsback 2012 hamilton et al 2015 jakeman et al 2006 sos modeling is more complex compared to single system models due to the number of people and disciplines involved as well as the dependencies between the constituent models similarly management of the modeling process is made more complex as there is not a single modeling cycle but multiple cycles occurring asynchronously each actor and model may have separate objectives and purposes priorities and differing levels of available resources not to mention the need to consider the availability of resources for the sos modeling as a whole the sections below are adapted from the modeling phases identified in badham et al 2019 and hamilton et al 2015 wherein the actions undertaken in each modeling phase are described in contrast we identify the relevant phases within an sos context and outline the considerations with respect to scale issues fig 1 depicts the high level considerations objectives within each phase while the sections below are presented in a sequential manner we stress that modeling is an iterative and concurrent process 3 1 scoping phase in this phase the objectives of the modeling are clarified by defining the problem and how modeling is intended to address it examples of model or modeling purpose could be to fill gaps in knowledge to support learning and communication processes to validate current understandings and assumptions to predict what might happen in the future or to carry out scenario analysis badham et al 2019 kelly letcher et al 2013 ideally this scoping phase results in a clear understanding of the model types and components that need to be developed or in later iterations their limitations with respect to the model purpose and how to address these 3 1 1 problem definition and scoping while the overarching purpose of the sos model may be known the specifics may be less clear at the outset development of a consistent and shared view of the scales to be considered involves communication of the scope and interactions across the constituent systems between all involved see fig 2 this process can aid in identifying and addressing areas that require reconciliation of different views that often exist across the stakeholders awareness of the scale issues will likely evolve as the modeling progresses through the iterations the choice of modeling pathways and methodological framework employed is heavily informed by this awareness macleod and nagatsu 2018 involvement of stakeholders including domain experts through participatory processes can inform the identification of relevant scales in the face of uncertainty and poor data availability hamilton et al 2015 kragt et al 2013 stakeholders can also play a role in selecting and combining data furthering holistic consideration of system actors and aid in developing the model purpose the relationship between actors and their roles in framing the scale scope and purpose of the modeling has been previously recognized kragt et al 2013 refsgaard et al 2007 and is further explored in the next subsection insufficient consideration or agreement regarding the overarching purpose of the sos model may ultimately affect model performance and outcomes connor et al 2019 the higher number of actors in sos modeling increases the difficulty in reconciling different or mismatched perspectives requirements and purposes this is a problem of heterogeneity o connell and todini 1996 and is not restricted to any single discipline often and by necessity the scale of the modeling is to be commensurate with its purpose including the level of certainty being sought and the available resources purpose and use of constituent models may be mismatched if conflicting perspectives over the scope of the modeling are not addressed modelers that have different goals in mind may only consider scales relevant to their immediate and often discipline specific concerns leading to an improper selection of constituent models there is potential for a high degree of mismatch between constituent models even if modelers coordinate their efforts unexpected cascades of effects through scales is commonplace in complex systems tranquillo 2019 and could arguably be taken as the rule rather than the exception change in scale may also occur during the modeling process due to new information that triggers a necessary change in model context the scale of model interactions to be represented can also influence the number and type of constituent models included and overall system complexity the choices regarding scale then have implications for how well interactions among systems can be represented with respect to the model purpose scope creep wherein the scale of the modeling is continually extended to cover contexts not originally envisioned cf barton and shan 2017 may eventually compromise modeling efforts as available resources get stretched too thinly to achieve sufficient progress sarosa and tatnall 2015 choice of scales is further compounded in cases where system bounds cannot be clearly and definitively defined coastal zones atmospheric systems and natural resource management systems are examples of systems with ambiguous system boundaries social systems and their dynamic structures are another example that do not have clear boundaries yet place important even governing conditions on system behavior such social systems and their influences are so far under represented in current integrated assessment efforts zare et al 2017 the lack of clear boundaries of such systems are often considered to be part of the problem voinov and bousquet 2010 reconciling conceptual differences and perspectives between human actors can be demanding but not insurmountable there are various methods available for group decision making such as the delphi technique gokhale 2001 which can be used to help the group reach agreement on the definition of the problem and or the system boundaries the subsequent modeling itself can be used to combine and reconcile different views among stakeholders and may be useful in cross cultural or particularly contentious settings cf potter et al 2016 the influence of modeler and stakeholder bias can also be constrained such as by using numerical optimization and or exploratory modeling processes martin et al 2017 reichert 2020 the influence of personal preferences is restricted by using the exploratory approach as it focuses on identifying the relevant scales and conditions or combinations of conditions that normally lead to desirable outcomes 3 1 2 stakeholder planning here stakeholder refers to the individual or groups that may affect or be affected by the modeling or have an interest in its outcomes freeman 2010 thus in this context the modelers and teams of modelers are also stakeholders there is a plethora of stakeholder focused approaches e g in integrated modeling participatory modeling but these methodologies are still limited in their capacity to deal with scale specific questions and challenges brought by sos modeling jordan et al 2018 generally participatory approaches aim to bring together the multiple goals issues and concerns of interest from multiple scales and governance systems by developing a mutually beneficial relationship between stakeholders thompson 2009 thoughtful consideration of transparency traceability and governance issues in engagement and participatory processes cockerill et al 2019 glynn et al 2017 will be essential for optimizing saliency legitimacy and credibility of the sos modeling cash et al 2003 the participation of a higher diversity of stakeholders in such processes allows for a more holistic representation to be developed covering potential blind spots in the system conceptualization and avoiding the siloing of knowledge hoekstra et al 2014 including further perspectives may increase the complexity of the modeling and so requires careful management of individual expectations and biases martin et al 2017 management of an sos may at times be predicated on effective management of stakeholders and their level and capacity of involvement ostrom 2007 boone and fragaszy 2018 increases in the variety of perspectives also increases potential for conflict defined here as disagreements of any degree between teams team members and or stakeholders on the one hand there is evidence that conflict plays a positive role in learning and effective teamwork tjosvold et al 2003 such positive benefits however may only occur in cases where there are high levels of pre existing trust within the group and when the conflict is task related rather than interpersonal de dreu 2008 power dynamics within teams and stakeholders therefore need to be considered national research council 2013 identification and focus on objectives that require participants to work together known as goal interdependence is an identified foundation towards project success and may additionally help in avoiding conflict knight et al 2001 lee et al 2015 tjosvold et al 2003 careful design and management of interactions between teams and stakeholders requires an explicit consideration of how the multiple and at times contradictory objectives might align or connect approaches to conflict resolution and prevention e g boundary critiquing midgley and pinzón 2011 are promising but still under utilized techniques effective stakeholder engagement will in practice be impacted by geographic spread allen and henn 2006 as the realities of scheduling rarely allow all stakeholders to be engaged at the same time and place additionally a diversity of stakeholders e g policy makers scientists and the public mean material and modes of communication may need to be tailored for each online participation platforms and technologies extends the reach to participants and are appealing for their asynchronous and distributed modes of engagement yearworth and white 2018 these relatively new technologies are simply tools however and a capacity to both use and leverage their advantages is also required cooke et al 2015 regardless of how interactions are to occur without documenting a record of engagement and decision making roed cockerill et al 2019 the original purpose assumptions and social and biophysical context of the engagement and resulting model choices might be lost leading to mismatches in understanding conceptualization and implementation the literature is still limited on the effectiveness of using different participatory methods for different purposes and audiences voinov et al 2018 nevertheless plans for stakeholder engagement for sos modeling should explicitly consider the scaling challenges and devise strategies to deal with these 3 1 3 preliminary conceptual model the preliminary conceptual model represents the current understanding of the system and the relationship between constituents including identification of key drivers interactions and outputs of interest badham et al 2019 in describing and capturing the essence of the system development of the conceptual model helps with the design of the subsequent computational model as well as making concrete the model purpose two scale specific aspects are to be considered here the approach used for conceptual model development see table 2 for a general overview and the formal representation e g equations technical specifications etc the processes that are included or excluded based on actors perceptions priorities beliefs and values under the sos context will inevitably influence the data leveraged the properties of the computational model and therefore the paths taken few mapping techniques exist that focus on illustrating multi scale representations scale separation maps hoekstra et al 2007 or stommel diagrams scholes et al 2013 represent the scales of the constituent systems on a two dimensional space time map system diagrams such as the representations used in van delden et al 2011 and oxley and apsimon 2007 organize the system components according to their spatial and or temporal scales and show the interactions between these components on the other hand coupling diagrams falcone et al 2010 show the flow of data between models a further approach is to use the odd protocol named after its three blocks overview design concepts and details grimm et al 2006 the original purpose of the odd protocol was to describe and enable transparent communication of agent based models abms to ensure their replication and the reproducibility of results based solely on the model description grimm et al 2020 the conceptualization involved in the overview block mandates identifying the scales of the processes or system components to ensure a shared understanding of the system being modeled this is further complemented with the identification of relevant resolutions and spatial temporal bounds at this stage the bounds can be vaguely defined e g local regional global this initial assessment of the scales involved may be revised throughout the modeling process as understanding improves the odd protocol is under continual development and planned additions extend its consideration and applicability of use to other areas not previously considered as outlined in grimm et al 2020 if differences in conceptual understanding of the scales and their interactions cannot be reconciled at this stage it is possible to create multiple alternative models representing the different hypotheses which can be tested in later stages of the modeling process such an approach can also assist in assessing uncertainty rooted in model building choices as the treatment of scale may affect model outputs and outcomes further discussed in section 3 2 4 although conceptual diagrams can be developed without specifying the scales involved explicit consideration of scale is valuable for avoiding misinterpretation of the conceptualization and ensuring key variables and processes are included a useful reflexive exercise not usually reported but aiding transparency is to identify what alternative approaches were considered or could have been considered and how these may have affected results and outcomes if adopted 3 2 development phase 3 2 1 collecting data information and knowledge data information and knowledge for each constituent model may come from the field or through literature solicited through expert and stakeholder engagement or collected through analysis considerations towards data collection in the integrated setting have been previously explored in badham et al 2019 correctly communicating and interpreting data across heterogeneous systems however requires that the data are interoperated between constituent models and that model behavior across scales remains valid and meaningful renner 2001 for this purpose metadata serves an essential role transparency in the collection process and approval from those involved in the modeling are necessary to ensure that collected data remain conceptually relevant across scales furthermore transparency in the context of data collection and usage is a key factor to develop trust among stakeholders and model users and future adoption of the constituent models barba 2019 gray and marwick 2019 data may need to be transformed to be fully relevant for the context of its intended use such as up or downscaling to ensure compatibility with other processes ideally metadata would include information on the data collection uncertainty and transformation process which aids in determining the appropriateness of data for the sos model explicit descriptors of both input and output data can assist in identifying the commensurate level of data collection with respect to available resources modeler bias can have a compounding effect as the choice of data collection as well as the metadata that describes the data influences how system interactions are perceived and thus conceptualized bhattacherjee et al 2008 what may be considered irrelevant in one field may dictate modeling pathways in another in an sos setting there are many more participants involved and so there is a high degree of uncertainty stemming from the decisions made as a result data quality and informativeness e g accuracy or precision provided by constituent models may also be diverse diversity of data obtained from a diversity of sources however runs the risk of conflicting information gray et al 2012 modelers from different disciplines may also utilize different scales for the same process resulting in inconsistencies and thus errors the sources of which are difficult to identify in this regard non quantitative sources of information gathered from literature and or through stakeholder engagement may become key assets that resolve such issues grant and swannack 2007 in cases where data describing a particular linkage in an sos model are not available theoretical relationships generally applicable empirical relationships or model process and output can be useful representations for the purpose of the sos model rai et al 2002 the documentation developed in the scoping phase can be leveraged to ensure applicability and validity with regard to the model purpose 3 2 2 construction construction of computational sos models requires the marrying of domain expertise from across the various disciplines involved with technical software development knowledge while the overarching context may be well defined within the scoping phase it is in this construction step that the individual components and the scales they represent are developed and coupled tested and validated here existing models may be repurposed or new models developed the specifics of their initialization interoperation method of execution and management of the data involved are to be determined and prototyped in this phase igamberdiev et al 2018 madni and sievers 2014 a balanced approach is needed in sos model development that takes several factors into account there is a danger that the models themselves become treated as pieces of software that merely require connection ignoring the socio technical context for their intended use voinov and shugart 2013 another issue is the overparameterization of constituent and component models brun et al 2001 nossent and bauwens 2012 as simply integrating these models to form an sos model exacerbates issues of uncertainty and identifiability considerations of which are explored in the following sections at the same time ignoring the technical considerations of integration is also inadvisable verweij et al 2010 mitigating the issues that consequently arise becomes increasingly difficult as more systems and scales are included voinov and shugart 2013 wirtz and nowak 2017 requisite systems could be represented at the level of detail necessary for the sos model purpose through a tiered modeling structure little et al 2019 implementation of such a tiered approach can involve developing metamodels or entirely different system models metamodels being simplified representations of more complex models revisited in section 3 3 two pertinent issues in sos model construction are the focus below managing the conceptual inter connection between models and the process of integration 3 2 2 1 conceptual integration conceptual integration of constituent models can benefit from requiring that constituent models be mechanistic as opposed to black boxes when a model is implemented as a black box it becomes difficult to evaluate and understand lorek and sonnenschein 1999 sos modeling may make use of pre existing models which constitutes re purposing implying the transference of the model assumptions limitations and scale to a new context it is emphasized here that model suitability within its original context is not necessarily applicable to the new context ayllón et al 2018 belete et al 2017 voinov and shugart 2013 availability of code alone for example does not imply transparency what is important is the contextual information that is necessary to assess the suitability of the model purpose and functionality a key challenge then is ensuring the box remains open and transparent rather than closed and opaque opaque development can be attributed to the modular nature of constituent model development with the teams working separately both conceptually and geographically and often split along disciplinary lines such teams can be described as self organizing sletholt et al 2012 but may lack cross disciplinary knowledge cross functionality as in hidalgo 2019 hoda et al 2013 the lack of interdisciplinary communication between teams then results in black or at best gray box models to those not involved in their development what is important in this interdisciplinary context is clear documentation and an organizational culture that supports the perpetuation of the relevant contextual knowledge as previously mentioned in section 3 1 3 describing the model and its conceptual linkages in a single canonical document via the odd protocol introduced in section 3 1 3 is one approach that could be leveraged furthermore a nested odd approach may be adopted in the case of complex sos models wherein the constituent models may be another sos model 3 2 2 2 technical integration technical integration refers to the correctness of model interactions recognizing the distinction between conceptual or abstract representation e g an equation or flow diagram and its implementation as software successful technical integration of computational models requires the necessary engineering expertise to be available knapen et al 2013 crucial considerations are that constituent models interact and accordingly that errors will propagate cf dunford et al 2015 and that each constituent model may undergo its own separate development cycle which invariably necessitates continual adjustments to be made flexibility of integration is often desirable as it allows the model to be resilient against changes in the modeling scope flexibility facilitates investigations into model structure of both constituent and component models and the technical design considerations that lead to flexibility allows for the composition of different combinations of relevant code and data represented through a nested hierarchy e g loose coupling elag et al 2011 vale et al 2016 whelan et al 2014 use of integration frameworks are helpful in that they allow the treatment of individual models as loose composable modules that provide some flexibility in dealing with the range of scales involved current integration frameworks typically have their roots in specific disciplines and tend to focus on physical processes cf ayllón et al 2018 the open modeling interface openmi moore and tindall 2005 for example has had to evolve from its initial focus in the hydrological sciences to accommodate an interdisciplinary modeling process buahin and horsburgh 2018 thus while the processes and requirements of such frameworks may be generally applicable there remains some difficulty in their generic implementation and adoption within the interdisciplinary context of sos modeling in some cases such frameworks may be overly complex or otherwise unsuitable for the purpose and context in which the modeling is being conducted such difficulties may be resolved in the future as improvements to these frameworks are ongoing voinov and shugart 2013 often modelers adopt a less formalized approach to avoid an inappropriate or constraining framework in either case ensuring semantic and conceptual correctness between models is typically left to the modelers themselves cf hutton et al 2020 direct manual tight coupling of models without the use of integration frameworks is still very much the norm more recent efforts include a collaborative web based platform through which the conceptual semantic and technical integration occurs opengms in chen et al 2019 chen et al 2020 faster feedback between participants then allows identified issues to be addressed earlier other approaches provide a curated ontological set of descriptors for common phenomena of interest e g snowmelt or rainfall these can be referred to as system variables as in pacheco romero et al 2020 and efforts to record their quantities e g centimetre grams etc and relevant operators in a specific metadata format have also been undertaken e g the standard names in hobley et al 2017 having the inputs and outputs described and documented in such a way aids in reducing potential mismatches in later re use and could be used to enable later automated model coupling frameworks do not yet fully automate conversions or identify incompatible or inconsistent usage e g litres per second to degrees celsius although this is likely to change in the near future both the selected framework and constituent models may change over the course of the modeling cycle along with the scales represented such changes may affect its appropriateness with respect to the model purpose for example adoption of a particular framework or model may increase the computational requirements or necessitate changes to constituent models to allow interoperation inadequate consideration of the concerns and requirements of the modeling as a whole may occur in cases where cognitive constraints are still in place the modeling process may be smoothed if requirements of the later phases are kept in mind during the design construction or selection of models and the resources allocated including the availability of expertise to each of these activities 3 2 3 model calibration calibration is the process of tuning parameters or altering the functional forms of equations or relations to achieve desired model behavior bennett et al 2013 in sos modeling issues such as non identifiability and equifinality beven and freer 2001 guillaume et al 2019 curse of dimensionality bellman 2015 computational burden razavi et al 2010 and data representativeness beven and westerberg 2011 singh and bárdossy 2012 may all be amplified calibration implies the existence of appropriate and sufficient data to calibrate models against availability of data relevant for the modeling purpose is a requirement no matter how perfect the model may be conversely a lack of data does not imply subsequent modeling is not useful a model with high uncertainty may still characterize uncertainty in a way that is meaningful to decision makers for example indicating the comparative tradeoffs between available management options reichert and borsuk 2005 assessment of uncertainty can be helpful in determining the relative worth of data to be collected to better characterize uncertainty and inform future modeling or research lópez fidalgo and tommasi 2018 partington et al 2020 such optimal experiment design approaches may also be leveraged to maximize the use of available data bandara et al 2009 lópez fidalgo and tommasi 2018 vanlier et al 2014 arguably model calibration within the sos paradigm can take three general approaches 1 calibration of each constituent model independently before integration 2 calibration of all models together after integration or 3 a combination thereof the first approach is the simplest and most straightforward as each constituent model would be calibrated within its own domain phillips et al 2001 while pragmatic it ignores the effect of representing different scales across the represented sos and system system interactions which in turn affects model behavior and performance of the individual constituent model if a model is considered calibrated when both an acceptable level of fit and reasonable parameter values are found as in anderson et al 2015 calibration in the disintegrated context does not necessarily transfer to the integrated context in other words what is reasonable in one context may not be so in another and the selected parameter values may not be robust to the change in context that integration brings due to the different scales interactions and data space involved the second approach is seemingly the most comprehensive approach to model calibration as every possible interaction between models could be present in the process of model calibration huang et al 2013 interdisciplinary knowledge is leveraged to ensure calibrated values are both reasonable for the expanded operationalization this then enriches the data space for individual constituent models and improves their performance jones et al 2017 the approach however has the following major barriers the search space for model calibration will be excessively large ling et al 2012 in addition new possibly erroneous interaction effects might emerge between the parameters of one model with those of another model especially with different scales of information which makes the response surface extremely complex for model calibration the calibration process might then become computationally cumbersome and or infeasible the available data with different scales may not be enough to properly constrain the model in the process of calibration ingwersen et al 2018 as it is not identifiable from the data guillaume et al 2019 there is a risk of overfitting as well as the available data might be insufficient to produce a generalized model that covers the integrated domain expert knowledge for each model may have scale constraints and may not be easily transferable to the full sos domain howard and derek 2016 in the third approach models are integrated one at a time incrementally adding complexity so that the influence of each constituent model can be directly attributed and subsequent issues can be addressed this approach may include modifying the conceptualization as necessary and sequentially calibrating the resulting integrated configurations duchin 2016 duchin and levine 2019 while this approach may be as pragmatic as the first and perhaps as comprehensive as the second the disadvantage is the time and computational cost to perform sequential coupling and calibration such an approach would seem more practical in cases where there is little disciplinary friction and a relatively small number of models to be integrated in all approaches above the role of expert knowledge in determining the acceptability of the calibration cannot be understated in management contexts for example change in policy e g the governing rulesets may impart shifts in system behavior that may be hard to discern by examining quantitative data alone and even more difficult to represent machine learning approaches may assist in identifying and representing non stationary system behavior e g rui wu et al 2019 razavi and tolson 2013 but still require intensive data for training and validation by experts where possible razavi and tolson 2013 and scale issues still exist between different single system models or different levels of model integration such information in one system may have implications for how other constituent models are calibrated and so interdisciplinary communication awareness and consideration of the intertwining issues is necessary to safeguard against mismatches a calibration method which seems not to have been used explicitly for sos models is pattern oriented modeling grimm and railsback 2012 railsback and grimm 2019 wiegand et al 2004 2003 here a set of patterns observed at different scales and levels of organization is used to reject as a set of filters unsuitable parameter combinations and process representations and may be closely related to the use of hydrologic signatures for hydrological model calibration and testing gupta et al 2008 as for parameters this approach corresponds to the rejection method in approximate bayesian computing van der vaart et al 2016 the basic idea is that a combination of weak patterns which by themselves do not contain much information and thus would not reject many parameter combinations can be as efficient as using a strong pattern which is highly distinctive but might not be available for models with multiple scales this approach holds high potential as it would help to keep both the sos and constituent models within realistic operation spaces 3 2 4 uncertainty analysis sos models often target large problem domains which necessitate complex models for their assessment and by their nature have a high degree of uncertainty for the discussion here we speak to the quantitative and qualitative aspects of uncertainty which may be further classified based on their source or primary influence prior literature for example speaks of model structure technical parameter scenario contextual and predictive uncertainty for further description see beven 2009 pianosi et al 2016 walker et al 2003 quantitative approaches aim to measure the effect of uncertainty in a specific parameter input or assumption on an output and allow the numerical characterization of the output distribution and therefore model behavior saltelli et al 2019 zimmermann 2000 qualitative uncertainty however cannot be characterized with a value and arises from sources such as the biases and subjective beliefs of human actors chen et al 2007 qualitative uncertainty can also arise from the modelers subjective judgment linguistic imprecision and disagreement across actors involved linkov and burmistrov 2003 refsgaard et al 2007 one reason for increased model uncertainty in sos modeling is the complexity that is largely a result of the increased scope of modeling which comes with a larger number of models and people and their perspectives involved the increase in the number of actors typically results in an increase in the overall number of parameters and their possible interactions oreskes 2003 the number of possible decision pathways in the modeling process lahtinen et al 2017 and the level of stakeholder influence at each decision fork ostrom 2007 increasing model complexity allows for a higher fidelity model but can also increase the perceived uncertainty in a traditional sense known as the complexity paradox oreskes 2003 characterizing true uncertainty in an sos model however is impossible as it requires a model that represents everything perfectly including unknown unknowns hunt 2017 uncertainty may then compound with each interaction across constituent models in the sos framework propagating some amount of error dunford et al 2015 thus it becomes progressively difficult to gain insights as to what effect and influence the combinations of these have structural and parameter identifiability as in bellman and åström 1970 guillaume et al 2019 high levels of model uncertainty need not be a barrier to effective decision support however and is ameliorated by providing estimates or assessments of such uncertainties reichert and borsuk 2005 both quantitative and qualitative different strategies and further considerations for uncertainty assessment are needed in sos modeling compared to single system modeling one commonly suggested approach to restricting model complexity and possibly runtime is to screen for insensitive parameters pianosi et al 2016 such parameters are said to have negligible influence on model output and so may be fixed i e made static in subsequent analyses or otherwise removed from the model another is to tie related parameters so that they may be represented by a single hyperparameter raick et al 2006 reducing the number of parameters however does not necessarily equate to a reduction in uncertainty rather it may simply mean consideration of an uncertainty source is determined to be unimportant for a given context or purpose pianosi et al 2016 and doing so may trade off model fidelity under new unseen conditions use of a constituent model within an sos model as opposed to its individual operation or its modification or simplification through parameter screening and tying constitutes a change in context therefore parameters initially found to be influential might become inactive and non influential and vice versa or the relationships that led to parameters being tied may change the change of context also changes the relevance of the assumptions and objectives and what constitutes an appropriate uncertainty analysis song et al 2015 uncertainty analysis conducted in one context is not valid across all scales thus premature model simplification may ultimately affect the appropriateness of the sos model for its overarching purpose a comprehensive sensitivity analysis under current and possibly alternative conditions can provide valuable insights into a key question when and how does uncertainty matter as discussed in razavi et al 2019 an alternate view is that given the likelihood of limited computational resources efforts to characterize and communicate uncertainties to stakeholders may be more beneficial than an exhaustive sensitivity analysis reichert 2020 anderson et al 2015 an additional consideration is that a constituent model may be a legacy or third party model that cannot be modified e g due to lack of access to the underlying code this would introduce some hidden or uncharacterized uncertainty into the sos modeling in this case metamodeling expanded on in the next subsection might provide some help in simplifying the model explicit documentation of the criteria used for each constituent model can ensure relevance of its application and reduce contextual uncertainty see walker et al 2003 across all the scales involved accordingly in the recent update of the odd protocol grimm et al 2020 a standard format for describing models the element purpose has been changed to purpose and patterns with patterns being the multiple criteria for ensuring a model s structural realism as defined in the pattern oriented modeling strategy grimm 2005 grimm and railsback 2012 the effect and relative importance of model structure uncertainty may be assessed through expert and stakeholder knowledge of alternate models van der sluijs 2007 and bayesian approaches could be applied to characterize the known unknowns clark 2005 uncertainty matrices have also been suggested as a tool to qualitatively identify and document the source type and nature of uncertainty and assess its relative priority in a table like format see refsgaard et al 2007 koo et al 2020 increased consideration of technical uncertainty adopting the term from walker et al 2003 is another area which warrants further consideration in the sos modeling context choice of what infrastructure and technologies to use is likely to stem from the prior experiences of the team s involved constituent models may be run on different infrastructure than was originally intended especially as issues around computational reproducibility are addressed barba 2019 hutton et al 2016 identical code run under different computational environments may produce different results see for example bhandari neupane et al 2019 such infrastructure may differ in physical or virtual architecture e g laptop supercomputer or operating systems or method of generating interpreting code e g different languages compilers package versions various combinations of these may be used and may also differ in the development and application phases for these reasons the influences of different and interoperating infrastructure are important considerations iwanaga et al 2020 correlation between parameters is another issue that is often ignored in the characterization and attribution of uncertainty do and razavi 2020 correlation refers to statistical dependency between parameters it is different from interaction effects which refer to the presence of non additive operations among two or more factors embedded in constitutive equations of the model in sos modeling the issue is further escalated as possible correlations between the factors of different models needs to be accounted for ignoring correlations can falsify any estimation of uncertainty do and razavi 2020 3 2 5 testing and evaluation testing and evaluation can assist in the assessment of the ramifications of scale choice in this step reasonableness of model structure and interpretability of relationships within models are assessed along with the traditional analysis of model behavior not all outputs produced by the constituent models may be relevant for the sos model purpose and the validity of their outputs are affected due to the integrated nature of sos modeling for any evaluation to be effective the specific model outputs of interest that are relevant for the model purpose must be well understood outputs may be at a particular spatio temporal scale for instance a long term average of a model output over a large spatial domain or an extreme event at a specific point location issues may also stem from the conceptual suitability of constituent models as uncertainty may be propagated throughout and may compound as more models are integrated dunford et al 2015 thus the first step in testing and evaluation involves attempting to refute aspects of sos model structure and functional relationships within the model based on their lack of correspondence with the represented system and the model outputs stakeholders could be leveraged to evaluate the conceptual alignment and appropriateness of the sos representation at the selected scales evaluation of the behavioral relationships at the integrated level is similar to scientific hypothesis testing wilson et al 2017 or conceptual testing iwanaga et al 2020 wherein functional relationships within the sos model are examined such tests may be especially useful in cases where the internal workings of a model are inaccessible or otherwise unknown but expected behavior of the constituent model in the integrated context can be characterized iwanaga et al 2020 these approaches can be used to identify impossible or implausible aspects of the sos model output if any aspect of model structure or any functional relationship within the model can be shown to be an inadequate representation of the corresponding aspects of the real system then that particular portion of the model is refuted li et al 2016 examination of model behavior over a range of inputs will also help to expose additional inadequacies in the model bennett et al 2013 the interesting aspect in this regard is that successful testing and evaluation of the constituent models does not guarantee correctness of the sos model and vice versa testing and evaluation may happen at different scale levels and acceptable model behavior depends on the model purpose and consequent measures or indicators of interest model behavior of constituent models could be examined quantitatively through assessment of the intermediate data in the models to ensure their behavior is consistent with a priori expectations it is necessary to test the software used to interoperate data across the different hierarchical levels using relevant testing approaches these include checking the mapping of input outputs between models conversion of units use of metadata to perform semantic operations and translation of spatial temporal dimensions ayllón et al 2018 belete et al 2017 voinov and shugart 2013 testing processes found in software engineering may additionally aid in conducting such checks see for example laukkanen et al 2017 verweij et al 2010 yoo and harman 2012 it may also be possible that some data gaps or uncertainties from constituent models have a lesser or negligible effect on the sos model depending on how the constituent model is leveraged at the sos level furthermore constituent models may present overlapping and or conflicting data or assumptions that will only be revealed when testing and evaluating their integration a common example is double counting uncertainty due to embedded assumptions in the model or failure to detect correlated variables with a common cause the next step focuses more specifically on the correspondence between model projections and observed data strictly speaking data used in model testing and evaluation must be independent of data used to develop the model raick et al 2006 a variety of visual statistical and machine learning methods are widely used to evaluate sos models the choice of method however should be based on the fundamental questions of what scenarios and observations to use in the evaluation evaluation of models under the range of conditions similar to those of interest can aid in identifying limitations of the model ramaswami et al 2005 sensitivity analysis is now regarded as standard practice in modeling norton 2015 pianosi et al 2016 razavi and gupta 2015 the sensitivity of sos model behavior to changes to its constituents and their interactions is the target of the assessment moriasi et al 2007 an issue stemming from the likely overparameterization of constituent models is equifinality and the lack of identifiability equifinality refers to the phenomenon of different implementations or combinations of model structure parameter values and their interactions producing equally acceptable results wagener et al 2003 beven 2006 identifiability then refers to the ability to attribute the influence on model outputs to unique model parameters or structure muñoz et al 2014 guillaume et al 2019 therefore the greater the number of parameters the less identifiable the model becomes sensitivities are assessed as part of identifiability analysis typically by ranking parameters based on their influence on outputs which can aid in determining what parameters require focused efforts to reduce uncertainty or improve identifiability e g factor prioritization nossent and bauwens 2012 information from sensitivity and identifiability analysis can then aid in simplifying the model as discussed in the previous section similar to what was noted in section 3 2 3 naively applying sensitivity and identifiability analysis without consideration of the sos context may adversely affect modeling outcomes assessment of sensitivities would ideally rely on global rather than local analyses for reasons that have been expounded in prior literature see for example pianosi et al 2016 saltelli and annoni 2010 use of global sensitivity analyses in model assessment has seen increasing use despite the lack of uptake or reported use of available software tools to conduct such analyses douglas smith et al 2020 still the importance of such analyses tends to be under appreciated saltelli et al 2019 one practical reason for the lack of global sensitivity analyses is that they are typically computationally expensive to perform and the sos models themselves typically exhibit long runtimes dependencies and correlations between parameters across constituent models and their respective scales pose another challenge koo et al 2020 metamodeling expanded on in the next section along with recently developed sampling and analysis methods may be more amenable to the sos context examples of such methods that warrant further investigation include moment independent methods such as pawn pianosi and wagener 2015 which can be applied independent of the sampling scheme used and variogram based approaches e g star vars razavi and gupta 2015 which can reportedly account for temporal and spatial correlations adaptive sampling of the parameter space through sparse grids for example in combination with these analysis techniques may also aid in reducing the computational costs associated with sensitivity and uncertainty analyses buzzard and xiu 2011 xiong et al 2010 3 3 application phase a critical aspect in the application of sos models is that constituent models evolve independently development of each constituent model by necessity is led by disciplinary experts and undergoes separate asynchronous development cycles as each model may come from different paradigms and sources of knowledge the implementation may be adjusted over time or even replaced in response to newly acquired knowledge advancing towards trial model applications using the expected type and volume of data as early quickly and often as possible allows modelers to encounter issues in the model application earlier in the process warren 2014 experience gained with each iteration subsequently serves to rectify and protect against future application challenges application of the model then requires monitoring and scrutinizing to ensure the underlying models including their metadata represented knowledge and application context remain current and appropriate when models are integrated the runtime may prevent practical application for its primary purpose such as social learning through interactive use with stakeholders or for global sensitivity analyses one option to overcome this problem is to simplify the constituent models for the specific purpose doing so requires a high degree of knowledge of the constituent models however and may not be practical in cases where legacy models are used spatially explicit models can especially be a problem in regard to runtime and a solution for reduction in computational burden may be achieved through aggregating grid cells into similar zones e g groundwater model aggregated into hydraulic conductivity zones elsawah et al 2017 in cases of high runtime replacing the most computationally expensive constituent models with metamodels may be a viable option metamodels approximate the input output behavior of the original model castelletti et al 2012 christelis and hughes 2018 pietzsch et al 2020 and therefore provide simplified representation s of more complex models asher et al 2015 razavi et al 2012 metamodels leverage the emergent simplicity of complex systems and although there are a variety of methods available to accomplish this generally metamodels require the complex models i e the original constituent models to be available beforehand metamodels being approximations of an original model s response surface are most relevant to the conditions existing in the datasets upon which they are tuned so care needs to be taken if using them under conditions that transcend those extant in the data system forcing data beyond that experienced such as climate change or groundwater extractions are of particular concern in this regard if possible simply allocating more computational resources e g supercomputers may be the most pragmatic and resource efficient alternative especially considering the time taken to investigate and implement the options listed above it is acknowledged however that more computational capacity may not be available 3 3 1 analysis and visualization in the management context where sos models are typically applied there is a need to adequately describe the level of uncertainties in the sos model and its predictions individual stakeholders may react differently to uncertainties and levels of uncertainty cockerill et al 2019 presenting scenario results relative to the modeled baseline neatly reduces the inherent biases that come with relying on stakeholder preferences to inform desirable thresholds as would usually occur in multi criteria or multi objective analysis approaches maier et al 2016 martin et al 2017 reichert and borsuk 2005 with such an approach the acceptability of a possible maximum or minimum relative change becomes the focus of stakeholder discussion software tooling for supporting analyses of model results including sensitivity and uncertainty analyses typically necessitates interaction between the analysis software and the model s which may require the development of additional interfaces i e code or supporting software due to the number of models involved the associated parameters and the possibly dynamic model structure wirtz and nowak 2017 maintaining these interfaces in the sos context may quickly become unwieldy additionally it may be desirable to replace entire models to analyze the influence of model structure and the scales they represent ewert et al 2011 thus potentially rendering existing interfaces obsolete recent efforts circumvent this issue by supporting the near seamless transition between the nested hierarchical representation common in sos design to the conceptually simpler flat structure expected in typical analyses e g schouten and deits 2020 an example of nested and flattened representations of a node network is provided in appendix 1 a common requirement shared with tooling for conducting analyses e g for sensitivity and uncertainty analysis and exploratory modeling is the provision and definition of parameter values these may consist of a default value a range within which values may vary whether these values are categorical scalar or regarded as constants examples may be found in adams et al 2014 kwakkel 2017 pianosi et al 2015 razavi et al 2019 categorical values may indicate substitution with other data types or a collection of data types e g rasters climate sequences etc such information may be the minimum necessary to conduct such analyses to reproduce and replicate results and to support later automation of these activities parameter values in effect represent dimensions of scale and the inappropriate selection of their values and ranges may result in misleading results shin et al 2013 wagener and pianosi 2019 3 4 perpetuation phase as in badham et al 2019 perpetuation is about the intended influence the modeling is to have into the future the focus here is on the scale of documentation and process evaluation in sos modeling which is informed by the level of consensus among stakeholders and modelers as to its purpose in the research context for example there is a newfound expectation that the model be developed and provided in a manner that supports reproducibility and replicability reproducibility is the ability to recreate results whereas replicability captures the ability of the model to generate new but consistent data in other applications patil et al 2016 where sos models are used by external stakeholders some amount of technical support is likely expected without this use of the model and thus its impact is likely to be minimal computational models are software in that they are made of code and so continued use comes with a baseline cost to cover maintenance improvements and updating of documentation such capacity is crucial in contexts where long term management and decision support is an acknowledged requirement in such cases the design implementation and documentation of the model should plan for these long term activities from the beginning in the sos context this implies retaining the interdisciplinary knowledge within a team or organization e g cockerill et al 2019 kragt et al 2013 3 4 1 documentation whereas earlier sections spoke to the content of documentation this section focuses on the role of documentation in an interdisciplinary setting such as sos modeling documentation is a conduit through which information and knowledge are propagated and provides the necessary context for model evaluation cockerill et al 2019 without sufficient documentation it is difficult to understand the context that led to any specific issue including mismatches between constituent models lack of context then affects the perceived validity of the model conceptualization restricts model use rendering the model inappropriate or invalid for its purpose the act of documenting itself allows for reflexive and transparent communication and for new insights to be gained undocumented assumptions regarding scale and their influence may compromise other constituent models thus holistic awareness of the sos issues can be obstructed by a lack of documentation long term maintenance and use of the model may also be impeded ahalt et al 2014 no individual holds the knowledge and awareness of the modeling details in their entirety let alone the effects of interactions between models it is therefore important to recognize that writing and maintaining documentation should be a team effort and a culture to support this should be fostered in practice there are few incentives for documenting models to such an extent a key problem in sos model documentation is that details of the constituent models important for the sos team may be considered unnecessary for the teams developing the constituent models once again this stems from potential disconnects between the purpose of the sos model and the individual or original objectives of each constituent model in the sciences the focus is often on the publication of papers at the expense of ensuring model reuse or reproducibility and replicability easterbrook 2014 joppa et al 2013 peng 2011 schnell 2018 there is an increasing push to change the culture surrounding the publication process however to better recognize credit and incentivize model code publication for example a number of organizations have begun supporting open code badges to highlight reproducible work https www comses net resources open code badge 3 4 2 process evaluation the extent to which the modeling has achieved its overarching purpose is evaluated in this step badham et al 2019 this evaluation extends beyond the technical performance of the sos model bennett et al 2013 to consider outcomes of modeling as a social process success of a model depends on the beliefs and expectations of the intended users and in their satisfaction with the model and its results hamilton et al 2019 it may also depend on the biases and beliefs of the model creators glynn et al 2017 and in an alignment of expectations between creators and users sterling et al 2019 the suitability of the success criteria is dependent on the context of the project including not only the model purpose but also the characteristics of the problem such as its complexity and the resources that were available hamilton et al 2019 process evaluation in sos focuses on two facets achievement of goals and longevity of the models in terms of goal achievement process evaluation considers whether the goals of the sos model were supported by its constituent models and where applicable whether constituent models achieved their own goals although satisfying the goals of the constituent models may seem an indirect path to satisfying the goals of the sos model this interpretation is misleading an sos approach to modeling instead of simply a multi modeling approach leverages the autonomy and independence of the constituent models constituent models still need to be capable of yielding their own outcomes regardless of how those models are used in the context of the sos model salado 2015 evaluation of the longevity of the sos model referring to the ability to leverage or reuse the sos model over time requires the development and assessment of a targeted plan for its sustainment that includes 1 monitoring the evolution of the constituent models 2 identifying alternatives for models that may cease their validity availability or accessibility during the lifetime of the sos model 3 establishing a strategy for the continued evolution of the sos model including the development of potential transformation frameworks and implementations and 4 identifying opportunities to facilitate the sustainment of constituent systems aligned with the sustainment of the sos model process evaluation for sos models may consider adopting a reflexive process in which questions are asked of those involved in the modeling such as did the modeling process help to improve understanding of the system problem or did the modeling process help facilitate communication between stakeholders hamilton et al 2019 the line of questioning can then leverage input from the various perspectives available including those of experts and stakeholders for the different constituent systems of an sos bias in the model such as whether their respective positions were adequately represented may then be assessed alternative conceptions and processes of the system and their scales could also be assessed at this stage voinov et al 2016 4 the paths forward 4 1 a grander vision and commensurate funding addressing all the scale related issues outlined in the paper requires a level of cooperation and concerted integrative effort that is by and large not possible given the usual short term funding of the sciences e g saltelli 2018 recent publications have also brought attention to deficiencies in the current science resourcing structure characterized in part by competition over limited funding and an emphasis on number and citation counts of publications existing funding mechanisms may well be detrimental to the quality of science produced binswanger 2014 sandström and besselaar 2018 limited resourcing is one reason for the multiple albeit siloed efforts with a focus on single case studies pulver et al 2018 hoekstra et al 2014 and the necessity of excluding salient aspects of the modeling such as adequate participatory processes eker et al 2018 or making less than ideal choices about the model or data e g using existing coarser scale data rather than collecting new data at a finer scale commentary by researchers highlight the importance of interdisciplinary work kretser et al 2019 meirmans et al 2019 which is typically not funded to the same extent as monodisciplinary efforts kwon et al 2017 bromham et al 2016 regardless of the importance of such holistic assessments these real world constraints essentially make holistic sos modeling and analyses unrealistic on the other hand examples of large concerted efforts can be found such as in astronomy and physics which have produced groundbreaking work with the event horizon telescope e g first photograph of a blackhole akiyama 2019 and the large hadron collider e g discovery of the higgs boson aad et al 2012 these resource intensive projects are important and could substantially influence future societal development at the same time lesser importance is placed by funding organizations on interdisciplinary socio environmental works which arguably have a more immediate impact and benefit to society a grander vision for sos research in line with large scale collaborations in other fields is vital to achieve a truly holistic consideration of sos modeling for resolving socio environmental issues realizing this vision itself requires fundamental shifts in how such interdisciplinary work and associated expertise are viewed and funded elsawah et al 2020 greater funding focused on education and training of interdisciplinary system practitioners is fundamental for greater cohesion and consensus in the socio environmental sciences little et al 2019 while alternative funding models have been suggested for the sciences see for example meirmans et al 2019 higginson and munafò 2016 the current state of affairs is unlikely to change in the near future thus any benefits from a systemic change if they occur at all will be experienced only in the long term although disciplinary experts may collaborate pool resources engage with stakeholders and gain experience in interdisciplinary work in the process of investigating a socio environmental issue this is not an effective way forward in the medium term existing case studies could be leveraged to perform a comparative meta analysis to determine the level of influence system connections have and the scales at which such connections matter pulver et al 2018 such meta analyses could extend to the practices used to manage the socio technical influences in the modeling process shifts towards leveraging collections of studies for meta analyses are emerging in fields such as psychology to allow for what is known as statistical objectivity towards reported findings in the literature freese and peterson 2018 although the focus there is in resolving issues of replicability the same approach can be additionally leveraged to characterize scale commonalities we conclude here by re emphasizing three key considerations which can reinforce current sos modeling efforts in a move towards the larger consensus needed for this grander vision 4 2 strengthen interdisciplinary communication here lies the crux of the challenge in developing a tiered sos model it is not only necessary for the science and engineering to mesh together appropriately but it is fundamental that the modeling process also consider and embed the socio technical considerations while we as modelers struggle with the former the latter is too often ignored as there are a variety of participants and therefore disciplinary perspectives involved a key set of considerations are in the social dimensions that provide the interface between modeling efforts integrating multiple perspectives requires an integrative approach which is ultimately necessary to navigate towards a beneficial system change why else do we model choices made in the treatment of scale are unavoidable and may result in conflicting decisions with separate implications just to name one members of teams may have a path pre selected without full consideration of the implications on the system representations leading to further issues when such decisions are not communicated the next generation of systems modelers would ideally embody a culture that is cognizant of the socio technical issues considerations and their influences throughout the modeling process e g little et al 2019 such a systemic cultural shift can only be developed in the longer term however and so in the meantime clearer communication requires adequate resourcing for documenting decisions made and code and data used including their maintenance practices for the co production of knowledge to fulfill the needs and requirements of the modeling is necessary for advances to be made norström et al 2020 there is often a preference for face to face meetings to facilitate the necessary level of communication but that may not always be possible geographic distance scheduling conflicts travel restrictions and other factors may preclude such activities communication technologies play a critical role in mitigating some aspects of the issue for example travel and social distancing restrictions during the covid 19 pandemic has prohibited many teams from meeting in person forcing reliance on technologies such as video conferencing regardless of the mode of communication a team and organizational culture of consistent and continual communication is one necessity repeatedly highlighted to resolve a variety of scale issues and the conflict that may arise between actors throughout the modeling process incorporating knowledge beyond the bounds of one s own disciplinary training is crucial to the holistic attention to and incorporation of scales and to avoid the siloing of information and knowledge and to break down cognitive constraints 4 3 improve documentation processes the importance of documentation is another aspect that was repeatedly raised throughout this paper documentation of the modeling process communicates and makes accessible the decisions actions the context of those decisions and actions and reflection on those choices to those who may or may not have been active participants in their making insufficient documentation affects many aspects from the pace of model development throughout the modeling cycle quality of model integration especially across disciplinary boundaries and the perceived quality of the modeling conducted a lack of documentation accessibility additionally affects the re use and maintenance of the sos model or its constituents and so could lead to duplication of effort across those involved in modeling sess one approach to ensure that documentation is made a priority is to adopt a documentation driven development and design approach heeager 2012 such approaches are exemplified by the odd protocol grimm et al 2020 2014 2010 in this paradigm documentation is developed first serving as a vehicle for discussion ideally prior to any model development heeager 2012 ambiguities in the documentation and thus the modeling may be addressed earlier in the process as a result and documentation could be iteratively revised commensurate with any changes to modeling scale furthermore maintaining records of engagement and decision making roed cockerill et al 2019 to document the process and pathway decisions were made in a context appropriate manner may be crucial to ensuring conceptual and technical validity throughout the modeling cycle sufficient rather than exhaustive documentation to describe model context would be preferred ambler 2002 cockerill et al 2019 4 4 explicit consideration of scale and uncertainty there is an increasing expectation that sos models can more completely represent processes within an ses however it is impossible to model everything for all purposes further explicit consideration of the inter relationships between scales choices made in representing scale and their influence on uncertainty is paramount in the sos context identifying managing and reconciling the disparate treatment of scale is a key step towards a holistic approach as opposed to the concurrent but separate processes currently applied cheong et al 2012 elsawah et al 2020 as noted several times throughout this paper the socio technical context has an inordinate influence on uncertainty in addition to the communication and documentation considerations outlined above an avenue for a more holistic assessment of uncertainty includes the use of robustness analysis grimm and berger 2016 in such analysis a model with multiple systems is systematically deconstructed through forceful changes to the model parameters structure and process representations within each system to assess uncertainty use of these approaches with pattern oriented modeling processes which filter unsuitable representations across scales may also be helpful in this regard grimm and railsback 2012 gupta et al 2008 additionally qualitative and quantitative uncertainties could be jointly assessed through the representation of multiple plausible futures that stem from different sets of assumptions through exploratory approaches maier et al 2016 roberts et al 2018 rounsevell and metzger 2010 a related approach is a multi model approach wherein an ensemble of equally plausible models are applied to identify the influence of structural and qualitative uncertainty matott et al 2009 tebaldi and knutti 2007 uusitalo et al 2015 using an ensemble of estimates such as the average or median of model outputs may have the benefit of providing more robust and accurate forecasts willcock et al 2020 applying these on different computational platforms may additionally assist in identifying technical uncertainties iwanaga et al 2020 it was noted throughout this paper that the scale of the modeling itself should be commensurate with the available resources and purpose a holistic sos model may not be entirely possible given resource constraints however relationships between systems can still be acknowledged and represented albeit simplistically doing so allows some assessment of the uncertainties at least and constitutes a step towards holistic sos modeling so long as the underlying assumptions are explicitly documented e g kloprogge et al 2011 declaration of competing interest none to declare acknowledgements this work was supported by the national socio environmental synthesis center sesync under funding received from the national science foundation dbi 1639145 the primary author takuya iwanaga is supported through an australian government research training program agrtp scholarship and a top up scholarship from the anu hilda john endowment fund hsiao hsuan wang and tomasz e koralewski acknowledge partial support from usda ars agreement no 58 3091 6 035 with texas a m agrilife research titled areawide pest management of the invasive sugarcane aphid in grain sorghum regional population monitoring and forecasting min chen is supported by the key program of nsf of china no 41930648 john little acknowledges partial support from nsf award eec 1937012 the authors would like to thank the three anonymous reviewers and prof randall hunt usgs for their constructive feedback and comments the authors additionally thank faye duchin and adrian hindes for comments provided on an earlier draft any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government appendix 1 example of hypothetical model inputs for a hydrological routing model provided in a nested data structure left column compared to a more traditional flat format right column nested structures are arguably better suited for representing collections of data structures and their relationships e g a network or graph structure and pragmatically are typically more amenable to the inclusion of comments and multiple values associated with specific parameters reducing cognitive overhead while perhaps more readable a disadvantage of nested representations is the additional complexity that may be perceived image 1 
25906,in this study a novel density based spatial clustering method is developed to maintain a diverse set of solutions for stochastic multi objective optimization algorithms this method dynamically clusters solutions in the decision space after solutions evaluations dominance check is localized to maintain solutions that are globally dominated but locally non dominated in their cluster unlike the original solution archiving the proposed method implemented for pareto archived dynamically dimensioned search successfully finds optimal and near optimal fronts with different cluster labels in two mathematical case studies two environmental benchmark problems are also solved and a three stage screening process is applied to their archive sets to identify the number of dissimilar options the dissimilarity index devised for this study shows a significantly higher distinction level and archive size for the cluster based solution archiving which allows decision makers to have higher flexibility in refining their preferences for robust decision making in the environmental problems compared with the original archiving keywords multi objective optimization decision space diversity dynamic clustering dbscan water resources engineering design 1 introduction engineering design problems are perceived as multi objective decision making problems to mitigate the needs of all parties by enhancing the transparency auditability and reliability of these decisions dunning et al 2000 hajkowicz and collins 2007 explicit consideration of all objectives simultaneously can help stakeholders to avoid decision biases particularly when planners inadvertently neglect aspects of the problem by concentrating on a narrow definition of problem optimality kasprzyk et al 2013 reed et al 2013 however due to the system complexity and various sources of uncertainty in the decision making and future condition in water resources engineering systems such as reservoir operation asadzadeh et al 2014a erfani et al 2020 geressu and harou 2019 zatarain salazar et al 2017 hydrologic models ahmadi et al 2014 asadzadeh et al 2016 koppa et al 2019 sahraei et al 2020 sikorska and renard 2017 and system design asadzadeh and tolson 2012 bode et al 2019 marques et al 2015 zheng et al 2015 the multi objective techniques require a tool that provides a flexibile environment for stakeholders to select among a large number of distinct options access to multiple distinct optimal and or near optimal alternatives in environmental systems optimization is important in many real world circumstances as these types of optimizations are mathematically modelled in a simplfied fashion considering highly important objectives only these problems are under modelling uncertainties due to data unavailability unmodelled objectives objective prioritization and other modelling limitations which changes their behavior to a multi modal optimization brill et al 1982 burton et al 1987 chang et al 1982 harrington and gidley 1985 rogers and fiering 1986 rosenberg 2015 voll et al 2015 zechman et al 2013 and or interval multi objective optimization gong et al 2020 2013 sun et al 2020 multi modality refers to a situation that different solutions perform similarly in the objective space interval multi objective optimization refers to a problem that at least one objective or constraint depends on an uncertain coefficient whose upper and lower bounds are known or can be identified a priori with a high confidence level making the objective function have an interval characteristic for each point in the decision space gong et al 2020 2013 sun et al 2020 therefore other criteria such as stakeholders perspectives about social environmental and economical issues that cannot be mathematically formulated are involved in the post processing stage for selecting most desirable solution s through subjective judgements liebman 1976 examples are synthesis of distributed energy supply systems voll et al 2015 pollution control rosenberg 2015 reservoir operation liu et al 2011 and aircraft engine design problems zadorojniy et al 2012 this paper proposes a novel approach for identifying distinct solutions of multi objective optimization mo problems including the ones on the best pareto front and the ones on the near optimal front moreover the proposed approach can identify and archive distinct solutions of multi modal problems the past twenty years have seen increasingly rapid advancements in the performance of mo algorithms to efficiently identify multiple optimal solutions pareto set which form an optimal set of points tradeoff or pareto front in the objective space asadzadeh and tolson 2013 deb et al 2002 macro et al 2019 reed et al 2013 sahinidis 2004 vrugt and robinson 2007 zhang and li 2007 mainstream mo algorithms are equipped with a dominance check and selection strategies that often look at the distribution of points in the objective space to approximate the entire pareto front but they overlook the position of the solutions in the decision space as a result they are not able to preserve near optimal solutions that have completely distinct decision variables compared to pareto solutions since they will be dominated by optimal solutions fig 1 demonstrates a hypothetical bi objective example with 2 dimensional decision space containing a cluster of solutions that is distant from the pareto set but form a near optimal front in the objective space a mo algorithm as a posteriori decision making approach that is able to identify distant local pareto sets and preserve near optimal solutions with distinct design or modelling characteristics from the globally optimal pareto set s has a higher chance to locate the most desirable solution this study focuses on decision space diversity maintenance for finding distinct decision design options to have a flexible and robust decision making analysis to this end a novel dynamic clustering approach coupled with a localized dominance relation is introduced to retain near optimal fronts along with the optimal front for mo algorithms with an unbounded archive set solutions residing in one cluster are mutually compared in terms of dominance and those that have different cluster labels are not compared for domination the proposed methodology is applied to a pareto archived dynamically dimensioned search pa dds algorithm asadzadeh and tolson 2013 and its performance is compared to pa dds with original archiving and the omni optimizer deb and tiwari 2008 as a reference algorithm that considers decision space diversity in multi modal situation 2 related work the stochastic mo algorithms in the literature are classified into three categories in terms of diversity maintenance in the decision space these algorithms are capable of solving multi modal mo problems effectively but not designed to preserve near optimal solutions in most cases 2 1 decision space diversity as an optimization criterion a group of indicator based mo algorithms use a diversity metric calculated in the decision space to guide their search the genetic diversity evolutionary algorithm by toffolo and benini 2003 is equipped with two metrics used as optimization criteria one metric is maximizing the shortest distance to neighbor solutions to encourage exploration of the decision space the second metric is maximizing the dominance rank of each solution with respect to the original objectives of the mo problem to exploit the promising regions of the decision space the former metric is designed to keep a diverse set of solutions while the latter emphasizes the optimality or convergence ulrich et al 2010 developed an indicator based evolutionary mo algorithm that integrates the decision space diversity metric into the objective space hypervolume metric the diversity metric sums the distance of solutions from the median of the non dominated set in the decision space that monotonically increases as new solutions are added to the non dominated solution set the modified metric is a weighted hypervolume measure that divides the dominated portion of the objective space into hypercubical segments where each segment is dominated by a specific subset of the entire population the hypervolume partition for each segment is thus weighted by the diversity of its dominating solutions and their summation is maximized the aforementioned methods solely aim at enhancing relative distribution of solutions with each other in the decision space but they are not designed to retain solutions that are distant from the optimal solution in the decision space but have a similar quality in the objective space zadorojniy et al 2012 suggested two algorithms for product design problems to find near optimal options by considering a degree of compromise of the known global optima the first algorithm maximizes the diversity of solutions in the decision space constrained by a maximum allowable compromise for example 2 5 optimality violation from the pareto optimal front in the objective space in the second algorithm violation from the pareto optimal front is minimized subject to a required decision space diversity this method however requires a prior knowledge of the pareto optimal front to commence the search for finding solutions with maximum decision space diversity and it requires a careful setting of optimality violation and decision space diversity thresholds which are case specific 2 2 decision space diversity as selection operator the selection operators in mainstream mo optimization algorithms focus on the diversity of points in the objective space and or the convergence toward the pareto optimal front therefore it is a challenge for them to maintain different solutions that have similar objective vectors to address this challenge deb and tiwari 2008 developed a toolkit known as omni optimizer a type of generational genetic algorithm vavak and fogarty 1996 that uses the crowding distance calculated in both objective and decision spaces deb et al 2002 for diversity preservation in multi modal problems crowding distance is a measure of the solution density around a particular solution and a higher importance should be given to solutions in a less crowded region i e higher crowding distance value if two solutions are identical in the objective space but are distant in the decision space the omni optimizer retains both of them since they are two different local optima this algorithm nevertheless gives a superior weight to the diversity in the objective space by considering the crowding distance in the decision space only if there is a tie in the crowding distance in the objective space chmielewski 2013 introduced a diversity ranking evolutionary mo algorithm drema with a similar non dominated sorting method to deb et al 2002 that uses the hypervolume contribution as the objective space fitness metric for sorting solutions in each non dominated front drema sorts solutions based on three diversity metrics calculated in the decision space to find distinct solutions a dispersion that is the sum of euclidean distances of a solution from two neighboring solutions in the decision space b remoteness that is the distance to the nearest solution in the decision space and c alternate ranking that is the euclidean distance in the decision space between two nearest solutions in the objective space solutions situated in the above average portion of solution ranking list based upon at least one out of three decision space diversity metrics and above average hypervolume contribution are given more opportunity to be chosen for generating new solutions nonetheless this method is algorithm specific and applicable only to population based optimization algorithms that use solution sorting strategies the concern about multi modality led cuate and schütze 2019 to define the inverse variation rate that assigns each non dominated solution a combined measure of proximity in the objective space such as crowding distance hypervolume contribution or weighted aggregation of objectives divided by the average distance from other solutions in the decision space a solution with lower variation rate or higher value for its inverse version has a higher chance to be selected for generating new solutions since it is distant from other solutions in the decision space while having the same proximity in the objective space however this metric cannot preserve potentially useful solutions that are nearly optimal in terms of objectives but completely distinct in the decision space 2 3 niching method the concept of niching that was first introduced by cavicchio 1970 formed the basis for developing different conventional approaches such as crowding factor de jong 1975 fitness sharing goldberg and richardson 1987 and speciation li et al 2002 petrowski 1996 for solving multi modal single objective problems by giving a higher importance to solutions in less crowded regions of the decision space in light of the fact that the fitness sharing gives a higher chance of selecting solutions in smaller niches their preservation is not guaranteed li et al 2002 each niche in speciation method is called a species the dominating solution in each species is called species seed and all the solutions fall within a pre defined neighborhood radius from the species seed belong to the same species the seeds belonging to different species that are locally non dominated solutions are copied into the next generation of solutions to maintain elite solutions defining a suitable species radius as a measure of dissimilarity requires a great knowledge of the optimization problem at hand and the relationship between decision variables and the objective function li et al 2002 the niching method was first developed for single objective optimization algorithms it was first utilized in a mo problem by horn et al 1994 in an early study of mo optimization zitzler and thiele 1998 used a distance based niching approach for decreasing the size of archive set in strength pareto evolutionary algorithm spea based on the objective space diversity however they stated that niching could be based on the distribution of solutions in the decision space deb 2001 shir et al 2010 proposed a dynamic niching framework for the covariance matrix adaptation evolution strategy the niching framework decreases the contribution of the domination ranking in the selection process that uses a dynamically adjustable niching radius in order to form a pre specified number of niches the solutions are checked in their neighborhood after non dominated sorting and the ones belonging to the same niche fall within a hyper sphere the euclidean distance between solutions is calculated in the aggregated space i e the decision and objective spaces the fittest solution in each niche is the representative of that niche and they are retained to guarantee elitism zechman et al 2013 proposed a mo niching co evolutionary algorithm that creates independent multi sets of solutions in parallel in each generation assuming that each set of solutions represents distinct non dominated sets in the decision space the original procedure in non dominated sorting genetic algorithm nsga ii deb et al 2002 is applied to the primary set to detect non dominated front as a reference for other parallel sets and ensure convergence the algorithm then combines all the solution sets and groups solutions using the k means clustering approach macqueen 1967 in the objective space therefore solutions with the same cluster label in the same solution set reside in the same niche in the decision space the algorithm prefers selecting a solution distant from its niche center in less crowded regions within the t percent optimality of the non dominated front of the primary set despite finding near optimal fronts some niches in different sets happen to reside in the same region of the decision space since independent solution sets independently form and evolve niches which wastes the computational budget kramer and danielsiek 2010 developed an evolutionary optimization strategy that uses reference lines for attaining uniform distribution of non dominated solutions in the objective space kramer and koch 2009 and uses a density based clustering approach ester et al 1996 to preserve diversity in the decision space niches are identified in the decision space and are evolved independently for a specific number of generations until two neighboring solutions in the objective space belonging to one niche have a distance higher than a user specified threshold in the decision space then all the niches are combined and re clustered the re clustering threshold is problem dependent and one cluster may be excessively expanded during evolvement leading to a merge with other niches in re clustering stage and losing useful local optimal fronts for the sake of preservation of local optimal solutions along with global optimal pareto front pajares et al 2018 introduced a new concept of domination for the mo genetic algorithm that considers closeness of solutions in screening process i e dominance localization two archives are provided for this algorithm near optimal solutions are preserved in a separate archive set and the archived solutions are mutually distant if a dominated solution is far enough from the non dominated ones based on a pre defined dissimilarity vector for decision variable vector it moves into the second archive for dominance and closeness check with near optimal solutions as a result a dominated solution that is not nearby any archived solution is retained the emphasis is given to less crowded regions of decision space for selecting from the archive by an assignment of sharing fitness and niche count sareni and krähenbühl 1998 as a measure of neighborhood density this algorithm however has a bounded archive and requires a careful tuning of a user specified vector for all dimensions of the decision space as a measure of dissimilarity for removing similar solutions if decision variables have no physical meaning a complete survey on niching based optimization can be found in li et al 2017 cheng et al 2018 and tanabe and ishibuchi 2019 liu et al 2019 devised an evolutionary mo algorithm with two bounded archives and a recombination method for multi modal problems it can also find near optimal solutions that are distant from the optimal solutions in the decision space decision variables that contribute to convergence only are identified with an analytical technique and separated from convergence independent decision variables one archive is assigned to convergence related decision sub space and a second archive is used to retain convergence independent decision variables for diversity maintenance parent solutions are then chosen from both archives using a tournament selection for reproducing offspring solutions and updating the archives accordingly solutions in the convergence archive are ranked based on a convergence indicator solutions nearby another solution in the convergence related decision sub space are de emphasized from the archive if they have the same convergence rank for the sake of diversity preservation in the objective and decision spaces solutions nearby another solution in the convergence independent decision sub space are de emphasized and given a lower chance of selection if they are clustered around a reference vector in the objective space after the termination of optimization two archives are recombined to obtain a final set for a posteriori decision making they stated that the basic clearing technique that de emphasizes the neighbor solutions may not find all local optima regions in the decision space if the spacing between these regions are different the remainder of this paper is organized as follows next section begins with a description of the density based spatial clustering method and its application for a cluster based solution archiving in mo algorithms with an implementation example on pa dds the benchmark optimization problems and numerical experiment settings are explained in section 3 5 and section 3 6 respectively the results of the proposed methodology are presented in section 4 and a comparison discussion is made with original version of pa dds for each problem followed by the concluding remarks section 5 3 materials and methods the density based spatial clustering is restructured to dynamically revise the clusters as new solutions are archived by the mo algorithm the dominance check is decentralized to eliminate solutions from the archive only if they are dominated within their own cluster the advantage of the introduced clustering approach against k mean hierarchical method and distribution based methods is that it does not require a priori information about the number of clusters or the relationship between the objective space and decision space 3 1 density based spatial clustering of applications with noise dbscan dbscan introduced by ester et al 1996 is an unsupervised clustering method for data mining that looks at the neighboring density of each data point in large spatial databases with minimal domain knowledge requirements each cluster identified by dbscan contains at least one core point blue doughnuts in fig 2 that has to be within a user defined neighboring distance ε i e the adjacency from a pre defined minimum number of points including itself minpts ester et al 1996 the value of ε in fig 2 is equal to the radius of the circles and minpts is set to four or five each member of a cluster blue and red doughnuts in fig 2 is therefore density reachable from at least one core member two members of a cluster are called density connected if they are neighbors of density reachable from a core a point that is not a core member of a cluster but is density reachable from a core is called a border member red doughnuts an unreachable point from any core is an unclassified data point and called noise in dbscan green doughnut in fig 2 3 2 cluster based solution archiving fig 3 illustrates the general structure of a mo algorithm equipped with the cluster based solution archiving in order to eliminate the scaling effect of decision variables for clustering the decision space is normalized to a hypercube of size one coordinates of each solution x x 1 x n in the normalized decision space is calculated by equation 1 based on the preset lower bounds x min and upper bounds x max for decision variables the euclidean distance is used for clustering but other measures such as manhattan and minkowski distances are applicable xu and wunsch 2008 after clustering update in every iteration solutions are converted back to their original ranges for model simulation and solution archiving 1 x normalized x x min x max x min x min x 1 m i n x n m i n x max x 1 m a x x n m a x the original dbscan is used to cluster initial solutions generated by the mo algorithm in the normalized decision space however it is modified to dynamically evolve clusters as new solutions are introduced by the optimization algorithm upon generating new solutions their neighborhood is checked for forming a new cluster or expanding the so far clustered solutions to prevent the formation of only one pareto set in the decision space the dominance and archiving strategy of the algorithm are decentralized from the global to a local dominance check equation 2 shows that solution x a cluster dominates solution x b x a c x b if and only if three conditions are met the first condition is extra to the regular dominance relation second and third conditions and ensures that both solutions reside in the same cluster to this end solutions with the same cluster label are mutually compared and solutions belonging to different groups are not compared by the dominance check if a solution is not classified yet its cluster label is assumed to be zero and a dominance check is not applied to it the cluster dominated solutions are eliminated from the cluster and the cluster dominating and cluster non dominated solutions are archived as representative of the cluster 2 x a c x b i f f 1 c l u s t e r i d x a c l u s t e r i d x b 0 2 f i x a f i x b i ε 1 m 3 f j x a f j x b j ε 1 m archived solutions are assigned a memory to store their coverage history for subsequent cluster expansions coverage history is the cumulative adjacency density that represents the number of reachable solutions to a cluster non dominated solution from the beginning of optimization this memory helps the border points of a cluster to increase their coverage history and turn to a core point which aids in cluster expansion by connecting nearby unclassified solutions if any to the cluster and their involvement in cluster dominance each cluster forms a tradeoff in the objective space fig 4 demonstrates a schematic example with two decision variables and two objective functions that are minimized assume that step1 shows the current set of archived solutions two of which have not yet been clustered and the rest are classified into one cluster shown as cluster1 in fig 4 solutions in the cluster are called cluster non dominated because they are not dominated by any other member of the cluster in step2 a new solution is added to the cluster this new solution is reachable from the core of the cluster and one border point therefore the coverage values of the core and border point are updated to 5 and 3 respectively however one of the cluster members the blue point in step2 is dominated by the new solution therefore it is omitted from the cluster and the archive the coverage values of other solutions in the cluster that are cluster non dominated remain unchanged in order to keep track of the coverage history in other words there remain four members inside the cluster in step3 but the core solution keeps its coverage history of 5 solutions that have not been clustered yet are retained in the archive disregarding their objective values and are called noises to be consistent with the dbscan terminologies see noises in fig 4 the reason for preserving unclassified solutions is to give an opportunity to the optimization algorithm to produce more solutions nearby the unclassified ones to subsequently join previous clusters or form a new cluster a new cluster forms when a density unreachable noise from the core s of other clusters becomes a core two or multiple nearby clusters complementing one local pareto set are merged if they are sufficiently populated during the optimization the proposed cluster archiving inherits the two parameters of dbscan cluster radius ε and minimum solutions coverage minpts assuming a constant value for minpts defining a small radius for the initial formation of clusters in a limited computational budget may result in appearing many small clusters at the end of the optimization which could be merged at some point if the number of function evaluations was not low ester et al 1996 suggested that the results of clustering do not significantly change for minpts higher than or equal to four and increasing minpts increases the computational time minpts is recommended to be set to four or five ester et al 1996 however ε is a case specific parameter that has to be defined based on the decision space dimension and the number of function evaluations a very high ε value results in one cluster covering the entire or a large portion of decision space and preserving only the globally non dominated solutions since all distant and nearby solutions lie in one cluster by contrast a very low ε leads to the identification and maintenance of all or the majority of generated solutions as unclassified solutions with no opportunity for new cluster formations and dominance check according to our experience it is recommended to consult with decision makers prior to the optimization process about the least meaningful discrepancy percentage d i for each decision variable and approximate ε as in equation 3 the reason for defining a range for ε in equation 3 is that the available computational budget and the number of dimensions in the decision and objective spaces affect the value of ε for an efficient and effective optimization using the cluster based solutions archiving 3 ε 0 8 z 1 2 z z 1 100 d 1 2 d 2 2 d n 2 3 3 cluster based multi objective optimization the proposed solution archiving strategy is implemented for the pareto archiving dynamically dimensioned search pa dds algorithm to find high quality solutions that are distinct in terms of their design characteristics for mo design problems 3 3 1 pa dds algorithm pa dds stochastic mo algorithm generates solutions one at a time the optimization is initialized with a set of randomly generated solutions within the decision variable boundaries with a budget of the higher value of five solutions and 0 5 percent of the total number of evaluations the dominance check is then applied to identify and archive the non dominated solutions in an unbound archive one solution from the archive is selected for generating one new solution pa dds commences the heuristic optimization by perturbing all decision variables sampled from normal distributions centered at the current value of each decision variables and dynamically reduces the number of perturbed decision variables to transform from a global search to a local search near the end of the computational budget if the new solution is non dominated or dominating pa dds archives it and selects it for generating the next solution otherwise it selects one of the archived solutions asadzadeh and tolson 2013 and asadzadeh et al 2014b recommended the hypervolume contribution as the selection metric for solving general mo problems and the convex hull contribution for solving mo problems with expected convex pareto front this process continues until the maximum number of function evaluation condition is met the dominance relation and selection metrics constitute the principal components of the pa dds structure the process of the domination check and selection strategies are based on the objective space proximity of solutions therefore a near optimal solution is discarded by pa dds even if it is far distant from the dominating solutions 3 3 2 cluster based archiving in pa dds algorithm in order to incorporate the proposed cluster based solution archiving in pa dds the dominance check from a global comparison is reduced to local comparison of solutions with equal cluster tags once a new solution is generated its neighboring solutions are identified based on the minimum adjacency radius and their coverage history are inspected for forming a new or joining a previous cluster or linking two or more cores with different labels and merging the associated clusters if none of the above cases occurs the generated solution will be stored as noise or unclassified solution in the archive set for possibly subsequent cluster expansions 3 3 3 selection operator to improve the diversity in the decision space dissimilar solutions should be retained and selected for generating new solutions even if they are dominated by other dissimilar solutions moreover the selection operator should consider the convergence in the objective space in this study a selection indicator is introduced to promote the solution diversity in the decision space and the convergence in the objective space the proposed selection indicator is a summation of two metrics that range between zero and one resulting in a total value of the indicator between zero and two a higher value gives a higher chance of selection for subsequent solution generation a suitable metric that describes the proximity of a cluster of solutions is the distance of its closest solution called knee point hereafter to the utopia point which has the best value of each objective function the distance from each noise unclassified solution to the utopia is also calculated the groupmates of a knee point have equal normalized convergence and equal chance of selection solutions belonging to a cluster with higher normalized convergence indicator are given a higher chance to be selected for generating new solutions the decision space dissimilarity index is the second term in the proposed selection indicator the dissimilarity index value for clusters is a measure of euclidean distance between knee points with different cluster labels the dissimilarity of a cluster is calculated as the summation of pairwise distances from its knee point to the knee points of other clusters and to unclassified solutions in the decision space this metric is scaled to between zero and one based on its maximum and minimum value in each step of optimization if some of clusters and or unclassified solutions are packed in one region of the decision space they will get a low dissimilarity index solutions in clusters and those that are unclassified in less crowded regions receive a higher normalized dissimilarity index of close to one fig 5 illustrates the process of calculating the proposed selection metric for a hypothetical situation where there are two clusters of locally non dominated solutions along with two unclassified solutions in a two dimensional decision and objective spaces for a minimization problem in each step of optimization the nadir and utopia points are determined based on the currently generated and archived solutions the nadir point correspond to an ever dominated point whose objectives equal to the maximum objective values of the solutions for a minimization problem the objective space is then normalized to zero and one based on the nadir and utopia points and the knee point for each cluster is found shown with larger marker sizes in fig 5 b the selection metric for each cluster and each unclassified solution is calculated as equation 4 the first term is the convergence term that is scaled by diagonal of the normalized objective space i e 2 in a bi objective space and m in an m dimensional space for a convergence between zero and one the shorter the distance to the origin the better convergence term and the better quality solution once the knee point for each cluster is identified sum of distance of each knee point from other knee points and from unclassified solutions is calculated in the normalized decision space and it is scaled by the maximum distance summation to have a dissimilarity index between zero and one knee points represent their cluster and their groupmates are not involved in the calculation of the proposed selection metric after computing the selection metric value for a knee point its groupmates will get the same metric value and one of them in a cluster is randomly selected for generating the subsequent solution 4 s c l u s t e r 1 1 a 2 i 1 3 a i max a i b i c i d i s c l u s t e r 2 1 b 2 i 1 3 b i max a i b i c i d i s n o i s e 1 1 c 2 i 1 3 c i max a i b i c i d i s noise 2 1 d 2 i 1 3 d i max a i b i c i d i 3 4 omni optimizer algorithm omni optimizer is a population based evolutionary optimization algorithm with a bounded archive developed by deb and tiwari 2008 the structure of omni optimizer is similar to that of nsga ii algorithm with additional operators to help the algorithm solve multi objective multi modal problems it uses a hypercube sampling to generate the initial population and constructs a bigger set by two random ordering of the current population in each iteration it chooses four solutions from the bigger set using the nearest neighbor based method in the objective space for encouraging convergence to determine two parent solutions using binary tournament selection the parent solutions are then recombined and mutated by simulated binary crossover and polynomial mutation for generating offspring solutions the parent and offspring solutions are then combined and ranked based on non dominated sorting and crowding distance the omni optimizer favors non dominated over dominated solutions and less crowded solutions to more crowded solutions an important feature of the omni optimizer is that in multi modal situation where two solutions have identical objective vectors the crowding distance metric is calculated in the decision space instead of the objective space the latter becomes zero while the former has a non zero value omni optimizer was not designed to identify and maintain near optimal solutions however it is used in this study as a reference algorithm for results comparison since it considers crowding distance calculation in the decision space among solutions with identical objective vectors readers are referred to deb and tiwari 2008 for more detail about omni optimizer s structure and its performance 3 5 optimization problems the cluster based pa dds algorithm is compared to the original version of pa dds for solving two bi objective mathematical test problems a bi objective sorptive barrier design problem and a lake pollution control problem introduced in this section for the identification of distinct optimal and near optimal solutions 3 5 1 modified sym part test problem the sym part bi objective bi variable problem was first introduced by rudolph et al 2007 as a multi modal test problem schütze et al 2011 modified sym part to make it have one pareto optimal front and eight near optimal fronts shown in problem formulation 5 where x 1 and x 2 are real valued decision variables ranging in 8 8 f 1 and f 2 are the objective functions and t 1 and δ t are auxiliary variables 5 f 1 x x 1 t 1 c 2 a a 2 x 2 t 2 b 2 δ t f 1 x x 1 t 1 c 2 a a 2 x 2 t 2 b 2 δ t t 1 s g n x 1 m i n x 1 a 0 5 c 2 a c 1 t 2 s g n x 2 m i n x 2 0 5 b b 1 δ t 0 i f t i 0 i 1 2 0 1 o t h e r w i s e as shown in fig 6 a b and c are constant and if they are respectively set to 0 5 5 and 5 the global continuous red line and local optimum dashed black lines pareto sets and fronts are formed 3 5 2 modified omni test the scalable omni test problem in problem formulation 6 is adopted from deb and tiwari 2008 with a slight modification this minimization mo problem is slightly modified in this paper to a test with one global optimum and multiple local pareto subsets by introducing a new constant δ increasing the range or the number of decision variables increases the multi modality of the problem deb and tiwari 2008 three decision variables with a range between 0 and 4 9 are defined in this paper that contains eight near optimal subsets 6 f 1 x i 1 3 sin π x i δ f 2 x i 1 3 cos π x i δ δ 0 i f x i 2 0 1 o t h e r w i s e the global and local pareto sets occur where decision variables are between 1 and 1 5 or between 3 and 3 5 if all decision variables range from 1 to 1 5 the pareto optimal front continuous red line in fig 7 is created and the local minimal front dashed black line in fig 7 is produced with other combinations of the mentioned extremum intervals 3 5 3 lake pollution control problem ward et al 2015 developed a scalable four objective benchmark optimization problem from a modelling study conducted by carpenter et al 1999 for the management of eutrophication of a shallow lake this problem also known as the lake problem aims to maximize the economic profits of a town by finding the amount of yearly anthropogenic phosphorous release a t while maintaining the reliability and control policy inertia the dimensionless total mass concentration of phosphorous p t at annual time step t is calculated using equation 7 besides the annual phosphorous release from the town p t in each time step also depends on p t 1 and the uncertain non point natural sources of pollution flowing into the lake ε t that is emulated by a random number sampled from a log normal distribution with a mean of 0 02 and a log10 variance of 5 5 as reported in ward et al 2015 this problem is a monte carlo simulation based function evaluation due to the uncertainty in the uncontrolled pollution term ε t unlike its original variant in ward et al 2015 that considered 100 year pollution management twenty decision variables 20 year management policy a 1 a 2 a 20 varying from 0 to 0 1 dimensionless are considered in this problem there are two parameters b and q that are associated with phosphorous recycling and decaying rates in the lake these parameters are respectively fixed to 0 42 and 2 ward et al 2015 to impose an irreversible eutrophic state on the lake if p t exceeds a pre defined threshold p c r t c l that is a function of b and q 7 p t p t 1 a t b p t 1 p t 1 q 1 p t 1 q ε t 8 f 1 a 1 k i 1 k t 0 t 1 α a t i δ t t 20 k 100 as shown in equation 8 the first objective is to maximize the average economic benefit across k simulations of t years of random ε t as in ward et al 2015 α and δ are dimensionless parameters that are set to 0 4 and 0 98 respectively representing the town s desire to pay for pollution control and the discount factor to convert future profits to present utilities the second objective equation 9 is to minimize the highest total phosphorous concentration p t averaged across k simulations stability in anthropogenic phosphorous rate over time is another important criterion that needs to be taken into consideration since rapid reduction in phosphorous in the lake requires large infrastructural investments and it is best to preserve policy inertia for this reason the difference between two consecutive release rates should be less than a pre specified threshold i c r t c l 0 02 as in ward et al 2015 in equation 10 the second term in equation 10 is designed to find a reliable management policy to prevent from permanent eutrophication in the lake by keeping p t below a critical value i e irreversible threshold p c r t c l 0 5 9 f 2 a max t 1 2 t 1 k i 1 k p t i 10 f 3 a 1 k i 1 k 1 t 1 t 1 t 1 θ t i 1 1 k t i 1 k t 1 t ϑ t i θ t i 0 i f a t 1 i a t i i c r t c l 1 i f a t 1 i a t i i c r t c l ϑ t i 0 i f p t i p c r t c l 1 i f p t i p c r t c l 3 5 4 sorptive barrier design the sorptive barrier design problem introduced by bartelt hunt et al 2006 is a combinatorial simulation optimization problem that seeks the cheapest option s for waste management while mitigating the migration of contaminants from organic wastes through a multi layer sorptive liner the landfill liner design is converted to a single objective constrained optimization benchmark problem by matott et al 2012 consisting of six integer valued decision variables thirteen alternative 15 cm layers with coded values from one to thirteen are available for each decision variable that is made of variable mixture of sand bentonite benzyltriethylammonium bentonite hexadecyltrimethylammonium bentonite shale and granular activated carbon the fifth and sixth decision variables can take a fourteenth option which is a no layer option to allow for a design with variable number of layers l the first objective function f 1 in problem formulation 11 is the design cost that has to be minimized m 2 if the number of layers is higher than four the opportunity cost c o s t 1 will be added to the material cost c o s t 2 this problem also uses a one dimensional numerical model to simulate the cumulative amount of 1 2 dichlorobenzene 1 2 dcb contaminants a x infiltrated into the ground from the liner over time the second objective function used in this study in problem formulation 11 that was a constraint in matott et al 2012 is to keep cumulative amount of contaminants below a pre defined allowable amount of 5 0 μ g m 2 over 100 years design lifetime readers are referred to matott et al 2012 regarding layer compositions and the associated costs 11 f 1 x c o s t 1 l c o s t 2 x c o s t 1 l 5 625 l 4 c o s t 2 x i 1 6 l a y e r c o s t x i f 2 x a x 5 3 6 numerical experiment setup and results comparison approach pa dds is a stochastic mo optimization algorithm in that its solution differs in different trials therefore in order to compare the distribution and proximity of the original pa dds algorithm with hypervolume contribution metric to its cluster based variant introduced in this study ten independent trial runs are conducted on each of the mo optimization problems the number of function evaluations is different for each type of problem mentioned in table 1 depending on the computational complexity and search objective space dimensionality of the problem moreover determining the value of minimum adjacency density is highly dependent on the available computational budget and the results will be highly different for different values of ε due to their simple structure the value of ε is set to 0 09 for the benchmark test problems which is equivalent to 5 to 6 5 percent discrepancy for each decision variable it is assumed if the difference between two solutions in the lake problem is at least 15 percent for each of their decision variables they are two dissimilar options and the value of ε is equal to 0 67 for a decision space normalized to between zero and one the decision space in the sorptive barrier design problem is integer and if the values of four out of six decision variables are one step higher or lower between two solutions they are considered as distinct design options resulting in ε equal to 0 16 when the design space is scaled to between zero and one for each decision variable the omni optimizer algorithm is compared with the cluster based pa dds in terms of decision space diversity and near optimal solution preservation the same computational budget and post processing procedure is used for both of these optimization algorithms the omni optimizer however has multiple parameters that influence its performance they are the population size number of generation probability of crossover probability of mutation distribution indices for crossover and mutation that can take different values for different optimization problems following deb and tiwari 2008 the crossover probability and index for mutation are respectively set on 0 9 and 20 for all optimization cases the distribution index for crossover is subjectively set to 15 for the sym part and sorptive barrier design problem and 10 for the modified omni test and lake problems a higher distribution index value aids in escaping local optima and producing an offspring that is far away from its parent while a lower value helps fine tuning and convergence deb and beyer 2001 the mutation probability is set to 1 n where n is the number of decision variables based on the recommendations by deb and tiwari 2008 the population size and number of generations are shown in table 2 based on the set computational budget in this study the resulted archive in the case of the modified sym part and omni test problems are analyzed and processed based on the closeness to their mathematically known global and local optimal regions the optimization results for the modified sym part and omni test problems are visualized and compared in their 2d and 3d decision spaces unlike the mathematical test problems there is no prior information about the global or local optimal solution sets in the lake problem and the sorptive barrier design problem therefore a three stage post processing analysis is performed on their archives to identify the desirable solution s based on decision makers preference the first stage is to re cluster archived solutions using dbscan in the decision space to distinguish similar solutions from dissimilar solutions based on their cluster labels the same adjacency density parametrizations in table 1 are considered for re clustering the archives in each type of problem the second stage is to identify and retain dissimilar solutions that include the unclassified archived solutions and only one solution in each cluster which is closest to a reference point in the objective space among its groupmates if there are also constraints the closest solution to the reference point must meet the constraints the reference point can be an ideal objective vector utopia or any desirable values of objectives defined by a decision maker the third stage is to define an acceptable threshold for each objective function and identify desirable solutions among dissimilar solutions in the second stage that have better objective values than the thresholds in the end the performance of two versions of pa dds algorithms are compared based on the solutions that passed the screening process in the third stage by showing their decision variable vectors on a parallel coordinate plot and a dissimilarity index shown in equation 12 12 i s i z e s c r e e n i 1 10 a r c h i v e i i 1 10 s i z e s c r e e n a r c h i v e i in order to calculate the dissimilarity index in equation 12 the archives of all optimization trial runs are combined and the three stage screening process is applied to the combined archive the dissimilarity index value is equal to the number of screened dissimilar solutions after combining the archive sets of all individual trials divided by sum of number of dissimilar screened solutions for each trial the higher the value of dissimilarity index the better the performance of the optimization algorithm in terms of identifying distinct desirable options 4 results and discussion 4 1 mathematical test functions the cluster based pa dds identifies all optimal and near optimal regions in each trial in the modified sym part and omni test problems while the original pa dds and omni optimizer algorithms only find the optimal pareto front in each trial and they are unable to detect distinct near optimal solution sets fig 8 and fig 9 demonstrate the results for one trial respectively for the modified sym part and omni test problems and the cluster based pa dds is able to find and assign a unique cluster tag to each locally optimal region the archive set also contains other low quality clusters each of which form a unique tradeoff along with unclassified solutions in other regions of decision space but they are filtered out after post processing based on the information we have about the location of the near optimal and optimal regions in the decision space for these test problems the post processed results for other trial runs are similar to figs 8 and 9 but with different cluster tags the original pa dds concentrates on global convergence and uniform distribution of non dominated solutions in the objective space figs 8 and 9 show that a large number of solutions are produced and retained by the original pa dds that are well diversified along the optimal pareto front with no solution representing the near optimal front this does not mean the pa dds algorithm does not explore the near optimal regions in the decision space it may find and preserve distant near optimal solutions in the archive set however since a global dominance check is carried out based on the objective values in the traditional solution archiving they are eliminated from the archive when a new dominating solution is produced therefore the original pa dds does not consider the decision space diversity in the optimization process and therefore does not guarantee maintenance of distinct near optimal solutions the result of the omni optimizer algorithm is similar to that of original pa dds with the difference that the omni optimizer has a bounded archive i e 40 and 100 solutions respectively for the sym part and omni test problems in light of the fact that the multi modality of the mathematical test problems are modified to a global and multi local optimal regions the decision space crowding measure in the omni optimizer cannot help retain solutions representing local optima in fact that the decision space crowding measure is activated when there are at least two solutions with identical objective vectors that never happened in problems solved in this study on the contrary most of the searching power in the cluster based pa dds is used for the identification of local optimal regions and dissimilarity maintenance but distribution of solutions along the optimal or near optimal fronts in the objective space is not in priority performing a local gradient based strategy on the desirable clusters or solutions in the archive of the cluster based pa dds after the termination of optimization may help to sufficiently populate each cluster for having a uniform distribution in the objective space 4 2 pollution control problem table 3 presents the performance of two variants of the pa dds algorithm along with the omni optimizer algorithm in terms of distinction level of their archives for the lake problem the archive set for the original pa dds algorithm has a very small size ranging from 36 to 87 solutions out of 20000 generated solutions compared to the cluster based version and it has a constant size of 100 in the omni optimizer algorithm for each optimization trial the size of archive in the cluster based pa dds is close to 3000 solutions seeking a desirable solution in an archive by comparing the values of decision variables and the corresponding objectives is laborious if not impossible as a result a systematic post processing strategy is utilized in this study to further screen the archived solutions based on a series of subjective judgements that often require consulting with decision makers the first screening stage is to re cluster archived solutions using dbscan with the same adjacency density parameters used by the cluster based pa dds the nearest solution to the reference point in each cluster is maintained along with unclassified solutions and the rest of the archived solutions are filtered out in the second screening stage a subjective threshold is set for each objective function this threshold should be defined based on the decision makers desirable range for each objective function for example the lake problem is assumed to have an irreversible eutrophic condition meaning that the reliability index should be 1 0 otherwise the corresponding 20 year pollution management policy leads to a permanent eutrophic state in the lake with no possible water quality restoration by solely reducing phosphorous loading therefore solutions that have a reliability index of less than 1 0 are removed from the archive the numbers highlighted by the bold font in columns five to seven of table 3 show the number of dissimilar design options solutions that do not tip into irreversible polluted state according to the adjacency density radius in table 1 for the lake problem the higher number of archived solutions by the cluster based pa dds suggests that each of its trial identified a significantly larger number of distinct design options that are considered reliable in the lake problem in the third screening stage it is assumed that only design options solutions that score more than 85 inertia index require acceptable infrastructural investments for phosphorous reduction in the lake therefore solutions not meeting this screening criterion are filtered out and the archive size is reduced to 2 to 8 solutions for trials of the original pa dds and 1 to 4 solutions for the cluster based pa dds depending on the trial number which is displayed inside parenthesis in table 3 it is interesting that only a few distinct archived solutions with reliability value of 1 0 are identified by the original pa dds 3 10 solutions and the majority or all of them have inertial maintenance index of higher than 85 further analysis of the results shows that solutions identified by all trials of the original pa dds are highly similar this means that most of the computational budget is consumed by all trials of pa dds to converge to a common location in the decision space despite the fact that the original pa dds is equipped with an unbounded archive that is expected to archive a high number of solutions the omni optimizer has a higher number of archived solutions and offers higher number of solutions with reliability index of one in each optimization trial the ratio of solutions with inertia index higher than 85 percent among reliable solutions is lower for the omni optimizer compared to that for the original pa dds in all trials for example 12 28 versus 7 7 in the first trial see table 3 this is due to considering decision space crowding distance in the structure of omni optimizer this ratio is the lowest for the cluster based pa dds in all trial runs since the algorithm does not solely focus on a specific region in the search space the archive sets of all trials for the omni optimizer algorithm and each variant of pa dds are combined resulting in 1000 solutions for the omni optimizer 625 solutions for the original pa dds and 29808 solutions for the cluster based pa dds the aforementioned three stage screening procedure is applied to these three archive sets separately only five reliable dissimilar solutions are identified by original pa dds and only three of these solutions have policy inertia index of higher than 85 therefore most of the high quality solutions identified by different trials of the original pa dds are similar and only 9 percent of them are dissimilar giving different design options this number is 149 reliable dissimilar solutions for the omni optimizer algorithm 32 of which having inertia index of higher than 85 as a result the dissimilarity index of the reliable solutions in the omni optimizer algorithm is 60 considerably higher than the original pa dds on the other hand the cluster based pa dds identified 1358 dissimilar solutions with reliability of 1 0 23 of which have policy inertia higher than 85 these 23 solutions are the union of solutions identified by all trials of the cluster based pa dds and none of them have similar cluster tags interestingly the dissimilarity index increases from 98 to 100 with fine filtering the archived solutions for the proposed pa dds structure while it decreases from 60 to 9 45 and 6 respectively for the omni optimizer and the original pa dds algorithms therefore it is concluded that the proposed solutions archiving helps the optimization algorithm identify dissimilar solutions identifying higher number of dissimilar solutions after combining the archives of the cluster based pa dds gives a higher flexibility to decision makers to choose among higher number of solutions with a wider range of benefit f 2 and maximum annual pollution f 1 compared to the solutions identified by the conventional algorithm the range of the first and the second objectives for the omni optimizer algorithm is wider than the proposed pa dds structure among reliable solutions with inertia maintenance index of higher than 85 this is mainly due to the structure of the omni optimizer that is a population based algorithm while pa dds is a single solution based method for a fair comparison about the range of the objectives the omni optimizer needs to be compared to its cluster based variant with an adaptive archive size which is outside the scope of this paper fig 10 demonstrates the parallel coordinate plot of the remaining solutions f 4 1 0 and f 3 0 85 after screening the combined archives for the omni optimizer original and cluster based pa dds respectively the cluster based pa dds provides different values of annual phosphorous release from the town that are scattered all over their defined range while if the decision maker relies on the original pa dds they are limited to options with annual release values in the lower half of their range especially from the fourth year to the sixteenth year similarly the majority of the post processed solutions in the omni optimizer are clustered in the lower half of the range in a sub space containing decision variables from four to sixteen therefore the original pa dds discards many solutions that could be interesting for the decision maker because those solutions are dominated but near optimal in the objective space 4 3 sorptive barrier design problem a similar filtering process to the lake problem is conducted for the results of the sorptive barrier design problem the first stage is to employ dbscan for clustering and re clustering archived solutions of the omni optimizer algorithm and pa dds with the traditional and cluster based archiving the second stage is to screen the solutions based on their euclidean distance to the reference point see table 1 in a semi logarithmic objective space for this problem which is an ideal zero cost and allowable contaminant rate transported to the soil in the barrier design lifetime the last three columns of table 4 shows the number of dissimilar solutions closest to the reference point in the objective space that have a cost f 1 of less than 50 and satisfy the contaminant rate f 2 of 5 0 μ g m 2 for each optimization trial and the combined archive of all trials apparently the original pa dds is able to identify multiple distinct design options after the filtering process and offer more number of distinct solutions compared to the omni optimizer algorithm the reason can be attributed to the discontinuity of the decision space however the dissimilarity level of these algorithms is still less than the cluster based version when the archive sets are combined these algorithms result in dissimilarity index values of 0 53 and 0 61 versus 0 76 for the cost values less than 50 similar to the lake problem fine filtering of the archives by considering a lower threshold for the maximum acceptable cost increases the dissimilarity ratio for the cluster based pa dds to 88 on the contrary the dissimilarity ratio for the omni optimizer and original pa dds decreases to 32 and 51 the high dissimilarity index in the results of the sorptive barrier design problem compared to the lake problem has increased when the original pa dds is used and reduced when the omni optimizer is used this is mainly due to two reasons one reason is that the archive size resulted from the original pa dds for each trial of this type of problem with discrete decision variables is so small that dbscan identifies some solutions as noise since they do not meet the minimum adjacency density requirement for cluster formation despite having high similarity to other archived solutions which should not be considered as dissimilar options the second reason is that the discrete nature of this problem does not let the original pa dds algorithm fine tune decision variable values to decimal places the integer valued decision variables also aid in small archive size that exacerbates the situation for dbscan method to find really dissimilar design options in the post processing stage as it needs a sufficiently populated archive for more accurate clustering the integer valued decision space also causes low distinction level in the archive of the omni optimizer since this type of problem does not have a multi modal characteristic to make the algorithm activate its decision space crowding measure since there exists no solution to have an identical objective vector with another solution in the decision space fig 11 demonstrates the joint parallel coordinate plot of the decision variable vectors and the corresponding scaled objective vectors f 1 10 log f 2 log 5 e 6 for solutions with cost of less than 50 and the contamination of less than 5 0 μ g m 2 the red lines represent solutions with cost of less than 25 according to fig 11 all the solutions found by the omni optimizer and original pa dds algorithms have decision variable values of equal to or higher than seven providing no option with decision variable values of one to six however the cluster based pa dds allows decision makers to choose from design options that have decision variable of one to six when they are interested to include these layers for the barrier design provided they are flexible with a higher design cost a more detailed inspection of fig 11 reveals that the cluster based pa dds shows a higher distinction among solutions costing less than 25 compared to its original counterpart and the omni optimizer algorithm for instance the original pa dds cannot identify alternate designs having option no 13 for the fifth layer fig 11 also illustrates that among less costly solutions the proposed pa dds structure offers five options for the next layer if layer no 13 is used for a layer while the omni optimizer offers one to maximum three options for the next layer 5 conclusion this work has laid out a foundation for incorporating decision space diversity maintenance within stochastic mo algorithms a novel cluster based solution archiving approach is introduced by restructuring the dbscan clustering strategy to provide multiple distinct optimal and near optimal options to decision makers for a robust decision making similar solutions are clustered in the decision space if they meet the pre defined solution density requirement for initial cluster formation the dominance check is decentralized such that solutions are compared only within their own cluster dominated clustered solutions are discarded from the archive after a mutual comparison with their groupmates a selection metric is also devised that gives higher chance of selection to distant clusters with higher normalized hypervolume this solution archiving method adds two parameters to the mo algorithms that represent the minimum adjacency density for cluster formation and recommendations has been given with regard to pre setting their values the proposed cluster based pa dds was successfully applied to two bi objective mathematical tests a three objective lake pollution control and a bi objective integer valued barrier design problems and compared its performance to the original version and the omni optimizer as a reference algorithm that considers decision space diversity in multi modal optimization the original pa dds and the omni optimizer algorithms provided better distributed optimal fronts in the objective space with much lower archive size in a single trial but they were highly similar and clustered in one region in the decision space the omni optimizer and pa dds with the traditional archiving strategy were also unable to preserve dissimilar near optimal solutions in their archive with the progress of optimization due to global domination check among solutions while the cluster based pa dds performed better in terms of diversity maintenance in the decision space the proposed archiving approach was developed for the single solution based pa dds algorithm in this study but it can be also developed for population based mo algorithms provided they are equipped with an unbounded archive or an adaptive archive size such as borg moea hadka and reed 2013 in a single solution based mo algorithm such as pa dds clustering should be performed as soon as a solution is generated i e 10000 times clustering update for 10000 function evaluations however in a population based optimization algorithm clustering needs to be performed only after a new generation of solutions are evaluated e g 100 times clustering update is required for 10000 solution evaluations when the population size is 100 in addition selection metrics such as the objective space crowding distance hypervolume contribution or convex hull contributions are not immediately applicable when solutions are clustered instead a simple random selection metric or more advanced effective metrics need to be introduced to jointly consider the decision space and objective space diversities such as the one used for the cluster based pa dds in this work or variation rate in cuate and schütze 2019 is suggested for the cluster based solution archiving the cluster based pa dds provides a large archive of solutions that gives more flexibility to the decision maker in refining their preferences there are also objectives in the real world that are not quantifiable and cannot be formulated definitively these types of objectives are evaluated through subjective judgements in the post processing stage liebman 1976 in addition decision makers may change their preferences in the post processing stage after interacting with stakeholders for example if a decision maker is interested in benefit values higher than a specified threshold instead of an inertial maintenance index of higher than 0 85 in the lake problem other new distinct or dissimilar solutions will be found therefore we recommend archiving and presenting all clustered solutions for the decision makers advanced visualization techniques such as video kollat and reed 2007 and mograms trawiński et al 2018 should be used for presenting the large number of design options for the decision makers and helping them find their most desirable option declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by dr asadzadeh s natural sciences and engineering research council of canada nserc discovery grant rgpin 2016 05896 and graduate enhancement of tri council stipends gets university of manitoba winnipeg mb appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104902 
25906,in this study a novel density based spatial clustering method is developed to maintain a diverse set of solutions for stochastic multi objective optimization algorithms this method dynamically clusters solutions in the decision space after solutions evaluations dominance check is localized to maintain solutions that are globally dominated but locally non dominated in their cluster unlike the original solution archiving the proposed method implemented for pareto archived dynamically dimensioned search successfully finds optimal and near optimal fronts with different cluster labels in two mathematical case studies two environmental benchmark problems are also solved and a three stage screening process is applied to their archive sets to identify the number of dissimilar options the dissimilarity index devised for this study shows a significantly higher distinction level and archive size for the cluster based solution archiving which allows decision makers to have higher flexibility in refining their preferences for robust decision making in the environmental problems compared with the original archiving keywords multi objective optimization decision space diversity dynamic clustering dbscan water resources engineering design 1 introduction engineering design problems are perceived as multi objective decision making problems to mitigate the needs of all parties by enhancing the transparency auditability and reliability of these decisions dunning et al 2000 hajkowicz and collins 2007 explicit consideration of all objectives simultaneously can help stakeholders to avoid decision biases particularly when planners inadvertently neglect aspects of the problem by concentrating on a narrow definition of problem optimality kasprzyk et al 2013 reed et al 2013 however due to the system complexity and various sources of uncertainty in the decision making and future condition in water resources engineering systems such as reservoir operation asadzadeh et al 2014a erfani et al 2020 geressu and harou 2019 zatarain salazar et al 2017 hydrologic models ahmadi et al 2014 asadzadeh et al 2016 koppa et al 2019 sahraei et al 2020 sikorska and renard 2017 and system design asadzadeh and tolson 2012 bode et al 2019 marques et al 2015 zheng et al 2015 the multi objective techniques require a tool that provides a flexibile environment for stakeholders to select among a large number of distinct options access to multiple distinct optimal and or near optimal alternatives in environmental systems optimization is important in many real world circumstances as these types of optimizations are mathematically modelled in a simplfied fashion considering highly important objectives only these problems are under modelling uncertainties due to data unavailability unmodelled objectives objective prioritization and other modelling limitations which changes their behavior to a multi modal optimization brill et al 1982 burton et al 1987 chang et al 1982 harrington and gidley 1985 rogers and fiering 1986 rosenberg 2015 voll et al 2015 zechman et al 2013 and or interval multi objective optimization gong et al 2020 2013 sun et al 2020 multi modality refers to a situation that different solutions perform similarly in the objective space interval multi objective optimization refers to a problem that at least one objective or constraint depends on an uncertain coefficient whose upper and lower bounds are known or can be identified a priori with a high confidence level making the objective function have an interval characteristic for each point in the decision space gong et al 2020 2013 sun et al 2020 therefore other criteria such as stakeholders perspectives about social environmental and economical issues that cannot be mathematically formulated are involved in the post processing stage for selecting most desirable solution s through subjective judgements liebman 1976 examples are synthesis of distributed energy supply systems voll et al 2015 pollution control rosenberg 2015 reservoir operation liu et al 2011 and aircraft engine design problems zadorojniy et al 2012 this paper proposes a novel approach for identifying distinct solutions of multi objective optimization mo problems including the ones on the best pareto front and the ones on the near optimal front moreover the proposed approach can identify and archive distinct solutions of multi modal problems the past twenty years have seen increasingly rapid advancements in the performance of mo algorithms to efficiently identify multiple optimal solutions pareto set which form an optimal set of points tradeoff or pareto front in the objective space asadzadeh and tolson 2013 deb et al 2002 macro et al 2019 reed et al 2013 sahinidis 2004 vrugt and robinson 2007 zhang and li 2007 mainstream mo algorithms are equipped with a dominance check and selection strategies that often look at the distribution of points in the objective space to approximate the entire pareto front but they overlook the position of the solutions in the decision space as a result they are not able to preserve near optimal solutions that have completely distinct decision variables compared to pareto solutions since they will be dominated by optimal solutions fig 1 demonstrates a hypothetical bi objective example with 2 dimensional decision space containing a cluster of solutions that is distant from the pareto set but form a near optimal front in the objective space a mo algorithm as a posteriori decision making approach that is able to identify distant local pareto sets and preserve near optimal solutions with distinct design or modelling characteristics from the globally optimal pareto set s has a higher chance to locate the most desirable solution this study focuses on decision space diversity maintenance for finding distinct decision design options to have a flexible and robust decision making analysis to this end a novel dynamic clustering approach coupled with a localized dominance relation is introduced to retain near optimal fronts along with the optimal front for mo algorithms with an unbounded archive set solutions residing in one cluster are mutually compared in terms of dominance and those that have different cluster labels are not compared for domination the proposed methodology is applied to a pareto archived dynamically dimensioned search pa dds algorithm asadzadeh and tolson 2013 and its performance is compared to pa dds with original archiving and the omni optimizer deb and tiwari 2008 as a reference algorithm that considers decision space diversity in multi modal situation 2 related work the stochastic mo algorithms in the literature are classified into three categories in terms of diversity maintenance in the decision space these algorithms are capable of solving multi modal mo problems effectively but not designed to preserve near optimal solutions in most cases 2 1 decision space diversity as an optimization criterion a group of indicator based mo algorithms use a diversity metric calculated in the decision space to guide their search the genetic diversity evolutionary algorithm by toffolo and benini 2003 is equipped with two metrics used as optimization criteria one metric is maximizing the shortest distance to neighbor solutions to encourage exploration of the decision space the second metric is maximizing the dominance rank of each solution with respect to the original objectives of the mo problem to exploit the promising regions of the decision space the former metric is designed to keep a diverse set of solutions while the latter emphasizes the optimality or convergence ulrich et al 2010 developed an indicator based evolutionary mo algorithm that integrates the decision space diversity metric into the objective space hypervolume metric the diversity metric sums the distance of solutions from the median of the non dominated set in the decision space that monotonically increases as new solutions are added to the non dominated solution set the modified metric is a weighted hypervolume measure that divides the dominated portion of the objective space into hypercubical segments where each segment is dominated by a specific subset of the entire population the hypervolume partition for each segment is thus weighted by the diversity of its dominating solutions and their summation is maximized the aforementioned methods solely aim at enhancing relative distribution of solutions with each other in the decision space but they are not designed to retain solutions that are distant from the optimal solution in the decision space but have a similar quality in the objective space zadorojniy et al 2012 suggested two algorithms for product design problems to find near optimal options by considering a degree of compromise of the known global optima the first algorithm maximizes the diversity of solutions in the decision space constrained by a maximum allowable compromise for example 2 5 optimality violation from the pareto optimal front in the objective space in the second algorithm violation from the pareto optimal front is minimized subject to a required decision space diversity this method however requires a prior knowledge of the pareto optimal front to commence the search for finding solutions with maximum decision space diversity and it requires a careful setting of optimality violation and decision space diversity thresholds which are case specific 2 2 decision space diversity as selection operator the selection operators in mainstream mo optimization algorithms focus on the diversity of points in the objective space and or the convergence toward the pareto optimal front therefore it is a challenge for them to maintain different solutions that have similar objective vectors to address this challenge deb and tiwari 2008 developed a toolkit known as omni optimizer a type of generational genetic algorithm vavak and fogarty 1996 that uses the crowding distance calculated in both objective and decision spaces deb et al 2002 for diversity preservation in multi modal problems crowding distance is a measure of the solution density around a particular solution and a higher importance should be given to solutions in a less crowded region i e higher crowding distance value if two solutions are identical in the objective space but are distant in the decision space the omni optimizer retains both of them since they are two different local optima this algorithm nevertheless gives a superior weight to the diversity in the objective space by considering the crowding distance in the decision space only if there is a tie in the crowding distance in the objective space chmielewski 2013 introduced a diversity ranking evolutionary mo algorithm drema with a similar non dominated sorting method to deb et al 2002 that uses the hypervolume contribution as the objective space fitness metric for sorting solutions in each non dominated front drema sorts solutions based on three diversity metrics calculated in the decision space to find distinct solutions a dispersion that is the sum of euclidean distances of a solution from two neighboring solutions in the decision space b remoteness that is the distance to the nearest solution in the decision space and c alternate ranking that is the euclidean distance in the decision space between two nearest solutions in the objective space solutions situated in the above average portion of solution ranking list based upon at least one out of three decision space diversity metrics and above average hypervolume contribution are given more opportunity to be chosen for generating new solutions nonetheless this method is algorithm specific and applicable only to population based optimization algorithms that use solution sorting strategies the concern about multi modality led cuate and schütze 2019 to define the inverse variation rate that assigns each non dominated solution a combined measure of proximity in the objective space such as crowding distance hypervolume contribution or weighted aggregation of objectives divided by the average distance from other solutions in the decision space a solution with lower variation rate or higher value for its inverse version has a higher chance to be selected for generating new solutions since it is distant from other solutions in the decision space while having the same proximity in the objective space however this metric cannot preserve potentially useful solutions that are nearly optimal in terms of objectives but completely distinct in the decision space 2 3 niching method the concept of niching that was first introduced by cavicchio 1970 formed the basis for developing different conventional approaches such as crowding factor de jong 1975 fitness sharing goldberg and richardson 1987 and speciation li et al 2002 petrowski 1996 for solving multi modal single objective problems by giving a higher importance to solutions in less crowded regions of the decision space in light of the fact that the fitness sharing gives a higher chance of selecting solutions in smaller niches their preservation is not guaranteed li et al 2002 each niche in speciation method is called a species the dominating solution in each species is called species seed and all the solutions fall within a pre defined neighborhood radius from the species seed belong to the same species the seeds belonging to different species that are locally non dominated solutions are copied into the next generation of solutions to maintain elite solutions defining a suitable species radius as a measure of dissimilarity requires a great knowledge of the optimization problem at hand and the relationship between decision variables and the objective function li et al 2002 the niching method was first developed for single objective optimization algorithms it was first utilized in a mo problem by horn et al 1994 in an early study of mo optimization zitzler and thiele 1998 used a distance based niching approach for decreasing the size of archive set in strength pareto evolutionary algorithm spea based on the objective space diversity however they stated that niching could be based on the distribution of solutions in the decision space deb 2001 shir et al 2010 proposed a dynamic niching framework for the covariance matrix adaptation evolution strategy the niching framework decreases the contribution of the domination ranking in the selection process that uses a dynamically adjustable niching radius in order to form a pre specified number of niches the solutions are checked in their neighborhood after non dominated sorting and the ones belonging to the same niche fall within a hyper sphere the euclidean distance between solutions is calculated in the aggregated space i e the decision and objective spaces the fittest solution in each niche is the representative of that niche and they are retained to guarantee elitism zechman et al 2013 proposed a mo niching co evolutionary algorithm that creates independent multi sets of solutions in parallel in each generation assuming that each set of solutions represents distinct non dominated sets in the decision space the original procedure in non dominated sorting genetic algorithm nsga ii deb et al 2002 is applied to the primary set to detect non dominated front as a reference for other parallel sets and ensure convergence the algorithm then combines all the solution sets and groups solutions using the k means clustering approach macqueen 1967 in the objective space therefore solutions with the same cluster label in the same solution set reside in the same niche in the decision space the algorithm prefers selecting a solution distant from its niche center in less crowded regions within the t percent optimality of the non dominated front of the primary set despite finding near optimal fronts some niches in different sets happen to reside in the same region of the decision space since independent solution sets independently form and evolve niches which wastes the computational budget kramer and danielsiek 2010 developed an evolutionary optimization strategy that uses reference lines for attaining uniform distribution of non dominated solutions in the objective space kramer and koch 2009 and uses a density based clustering approach ester et al 1996 to preserve diversity in the decision space niches are identified in the decision space and are evolved independently for a specific number of generations until two neighboring solutions in the objective space belonging to one niche have a distance higher than a user specified threshold in the decision space then all the niches are combined and re clustered the re clustering threshold is problem dependent and one cluster may be excessively expanded during evolvement leading to a merge with other niches in re clustering stage and losing useful local optimal fronts for the sake of preservation of local optimal solutions along with global optimal pareto front pajares et al 2018 introduced a new concept of domination for the mo genetic algorithm that considers closeness of solutions in screening process i e dominance localization two archives are provided for this algorithm near optimal solutions are preserved in a separate archive set and the archived solutions are mutually distant if a dominated solution is far enough from the non dominated ones based on a pre defined dissimilarity vector for decision variable vector it moves into the second archive for dominance and closeness check with near optimal solutions as a result a dominated solution that is not nearby any archived solution is retained the emphasis is given to less crowded regions of decision space for selecting from the archive by an assignment of sharing fitness and niche count sareni and krähenbühl 1998 as a measure of neighborhood density this algorithm however has a bounded archive and requires a careful tuning of a user specified vector for all dimensions of the decision space as a measure of dissimilarity for removing similar solutions if decision variables have no physical meaning a complete survey on niching based optimization can be found in li et al 2017 cheng et al 2018 and tanabe and ishibuchi 2019 liu et al 2019 devised an evolutionary mo algorithm with two bounded archives and a recombination method for multi modal problems it can also find near optimal solutions that are distant from the optimal solutions in the decision space decision variables that contribute to convergence only are identified with an analytical technique and separated from convergence independent decision variables one archive is assigned to convergence related decision sub space and a second archive is used to retain convergence independent decision variables for diversity maintenance parent solutions are then chosen from both archives using a tournament selection for reproducing offspring solutions and updating the archives accordingly solutions in the convergence archive are ranked based on a convergence indicator solutions nearby another solution in the convergence related decision sub space are de emphasized from the archive if they have the same convergence rank for the sake of diversity preservation in the objective and decision spaces solutions nearby another solution in the convergence independent decision sub space are de emphasized and given a lower chance of selection if they are clustered around a reference vector in the objective space after the termination of optimization two archives are recombined to obtain a final set for a posteriori decision making they stated that the basic clearing technique that de emphasizes the neighbor solutions may not find all local optima regions in the decision space if the spacing between these regions are different the remainder of this paper is organized as follows next section begins with a description of the density based spatial clustering method and its application for a cluster based solution archiving in mo algorithms with an implementation example on pa dds the benchmark optimization problems and numerical experiment settings are explained in section 3 5 and section 3 6 respectively the results of the proposed methodology are presented in section 4 and a comparison discussion is made with original version of pa dds for each problem followed by the concluding remarks section 5 3 materials and methods the density based spatial clustering is restructured to dynamically revise the clusters as new solutions are archived by the mo algorithm the dominance check is decentralized to eliminate solutions from the archive only if they are dominated within their own cluster the advantage of the introduced clustering approach against k mean hierarchical method and distribution based methods is that it does not require a priori information about the number of clusters or the relationship between the objective space and decision space 3 1 density based spatial clustering of applications with noise dbscan dbscan introduced by ester et al 1996 is an unsupervised clustering method for data mining that looks at the neighboring density of each data point in large spatial databases with minimal domain knowledge requirements each cluster identified by dbscan contains at least one core point blue doughnuts in fig 2 that has to be within a user defined neighboring distance ε i e the adjacency from a pre defined minimum number of points including itself minpts ester et al 1996 the value of ε in fig 2 is equal to the radius of the circles and minpts is set to four or five each member of a cluster blue and red doughnuts in fig 2 is therefore density reachable from at least one core member two members of a cluster are called density connected if they are neighbors of density reachable from a core a point that is not a core member of a cluster but is density reachable from a core is called a border member red doughnuts an unreachable point from any core is an unclassified data point and called noise in dbscan green doughnut in fig 2 3 2 cluster based solution archiving fig 3 illustrates the general structure of a mo algorithm equipped with the cluster based solution archiving in order to eliminate the scaling effect of decision variables for clustering the decision space is normalized to a hypercube of size one coordinates of each solution x x 1 x n in the normalized decision space is calculated by equation 1 based on the preset lower bounds x min and upper bounds x max for decision variables the euclidean distance is used for clustering but other measures such as manhattan and minkowski distances are applicable xu and wunsch 2008 after clustering update in every iteration solutions are converted back to their original ranges for model simulation and solution archiving 1 x normalized x x min x max x min x min x 1 m i n x n m i n x max x 1 m a x x n m a x the original dbscan is used to cluster initial solutions generated by the mo algorithm in the normalized decision space however it is modified to dynamically evolve clusters as new solutions are introduced by the optimization algorithm upon generating new solutions their neighborhood is checked for forming a new cluster or expanding the so far clustered solutions to prevent the formation of only one pareto set in the decision space the dominance and archiving strategy of the algorithm are decentralized from the global to a local dominance check equation 2 shows that solution x a cluster dominates solution x b x a c x b if and only if three conditions are met the first condition is extra to the regular dominance relation second and third conditions and ensures that both solutions reside in the same cluster to this end solutions with the same cluster label are mutually compared and solutions belonging to different groups are not compared by the dominance check if a solution is not classified yet its cluster label is assumed to be zero and a dominance check is not applied to it the cluster dominated solutions are eliminated from the cluster and the cluster dominating and cluster non dominated solutions are archived as representative of the cluster 2 x a c x b i f f 1 c l u s t e r i d x a c l u s t e r i d x b 0 2 f i x a f i x b i ε 1 m 3 f j x a f j x b j ε 1 m archived solutions are assigned a memory to store their coverage history for subsequent cluster expansions coverage history is the cumulative adjacency density that represents the number of reachable solutions to a cluster non dominated solution from the beginning of optimization this memory helps the border points of a cluster to increase their coverage history and turn to a core point which aids in cluster expansion by connecting nearby unclassified solutions if any to the cluster and their involvement in cluster dominance each cluster forms a tradeoff in the objective space fig 4 demonstrates a schematic example with two decision variables and two objective functions that are minimized assume that step1 shows the current set of archived solutions two of which have not yet been clustered and the rest are classified into one cluster shown as cluster1 in fig 4 solutions in the cluster are called cluster non dominated because they are not dominated by any other member of the cluster in step2 a new solution is added to the cluster this new solution is reachable from the core of the cluster and one border point therefore the coverage values of the core and border point are updated to 5 and 3 respectively however one of the cluster members the blue point in step2 is dominated by the new solution therefore it is omitted from the cluster and the archive the coverage values of other solutions in the cluster that are cluster non dominated remain unchanged in order to keep track of the coverage history in other words there remain four members inside the cluster in step3 but the core solution keeps its coverage history of 5 solutions that have not been clustered yet are retained in the archive disregarding their objective values and are called noises to be consistent with the dbscan terminologies see noises in fig 4 the reason for preserving unclassified solutions is to give an opportunity to the optimization algorithm to produce more solutions nearby the unclassified ones to subsequently join previous clusters or form a new cluster a new cluster forms when a density unreachable noise from the core s of other clusters becomes a core two or multiple nearby clusters complementing one local pareto set are merged if they are sufficiently populated during the optimization the proposed cluster archiving inherits the two parameters of dbscan cluster radius ε and minimum solutions coverage minpts assuming a constant value for minpts defining a small radius for the initial formation of clusters in a limited computational budget may result in appearing many small clusters at the end of the optimization which could be merged at some point if the number of function evaluations was not low ester et al 1996 suggested that the results of clustering do not significantly change for minpts higher than or equal to four and increasing minpts increases the computational time minpts is recommended to be set to four or five ester et al 1996 however ε is a case specific parameter that has to be defined based on the decision space dimension and the number of function evaluations a very high ε value results in one cluster covering the entire or a large portion of decision space and preserving only the globally non dominated solutions since all distant and nearby solutions lie in one cluster by contrast a very low ε leads to the identification and maintenance of all or the majority of generated solutions as unclassified solutions with no opportunity for new cluster formations and dominance check according to our experience it is recommended to consult with decision makers prior to the optimization process about the least meaningful discrepancy percentage d i for each decision variable and approximate ε as in equation 3 the reason for defining a range for ε in equation 3 is that the available computational budget and the number of dimensions in the decision and objective spaces affect the value of ε for an efficient and effective optimization using the cluster based solutions archiving 3 ε 0 8 z 1 2 z z 1 100 d 1 2 d 2 2 d n 2 3 3 cluster based multi objective optimization the proposed solution archiving strategy is implemented for the pareto archiving dynamically dimensioned search pa dds algorithm to find high quality solutions that are distinct in terms of their design characteristics for mo design problems 3 3 1 pa dds algorithm pa dds stochastic mo algorithm generates solutions one at a time the optimization is initialized with a set of randomly generated solutions within the decision variable boundaries with a budget of the higher value of five solutions and 0 5 percent of the total number of evaluations the dominance check is then applied to identify and archive the non dominated solutions in an unbound archive one solution from the archive is selected for generating one new solution pa dds commences the heuristic optimization by perturbing all decision variables sampled from normal distributions centered at the current value of each decision variables and dynamically reduces the number of perturbed decision variables to transform from a global search to a local search near the end of the computational budget if the new solution is non dominated or dominating pa dds archives it and selects it for generating the next solution otherwise it selects one of the archived solutions asadzadeh and tolson 2013 and asadzadeh et al 2014b recommended the hypervolume contribution as the selection metric for solving general mo problems and the convex hull contribution for solving mo problems with expected convex pareto front this process continues until the maximum number of function evaluation condition is met the dominance relation and selection metrics constitute the principal components of the pa dds structure the process of the domination check and selection strategies are based on the objective space proximity of solutions therefore a near optimal solution is discarded by pa dds even if it is far distant from the dominating solutions 3 3 2 cluster based archiving in pa dds algorithm in order to incorporate the proposed cluster based solution archiving in pa dds the dominance check from a global comparison is reduced to local comparison of solutions with equal cluster tags once a new solution is generated its neighboring solutions are identified based on the minimum adjacency radius and their coverage history are inspected for forming a new or joining a previous cluster or linking two or more cores with different labels and merging the associated clusters if none of the above cases occurs the generated solution will be stored as noise or unclassified solution in the archive set for possibly subsequent cluster expansions 3 3 3 selection operator to improve the diversity in the decision space dissimilar solutions should be retained and selected for generating new solutions even if they are dominated by other dissimilar solutions moreover the selection operator should consider the convergence in the objective space in this study a selection indicator is introduced to promote the solution diversity in the decision space and the convergence in the objective space the proposed selection indicator is a summation of two metrics that range between zero and one resulting in a total value of the indicator between zero and two a higher value gives a higher chance of selection for subsequent solution generation a suitable metric that describes the proximity of a cluster of solutions is the distance of its closest solution called knee point hereafter to the utopia point which has the best value of each objective function the distance from each noise unclassified solution to the utopia is also calculated the groupmates of a knee point have equal normalized convergence and equal chance of selection solutions belonging to a cluster with higher normalized convergence indicator are given a higher chance to be selected for generating new solutions the decision space dissimilarity index is the second term in the proposed selection indicator the dissimilarity index value for clusters is a measure of euclidean distance between knee points with different cluster labels the dissimilarity of a cluster is calculated as the summation of pairwise distances from its knee point to the knee points of other clusters and to unclassified solutions in the decision space this metric is scaled to between zero and one based on its maximum and minimum value in each step of optimization if some of clusters and or unclassified solutions are packed in one region of the decision space they will get a low dissimilarity index solutions in clusters and those that are unclassified in less crowded regions receive a higher normalized dissimilarity index of close to one fig 5 illustrates the process of calculating the proposed selection metric for a hypothetical situation where there are two clusters of locally non dominated solutions along with two unclassified solutions in a two dimensional decision and objective spaces for a minimization problem in each step of optimization the nadir and utopia points are determined based on the currently generated and archived solutions the nadir point correspond to an ever dominated point whose objectives equal to the maximum objective values of the solutions for a minimization problem the objective space is then normalized to zero and one based on the nadir and utopia points and the knee point for each cluster is found shown with larger marker sizes in fig 5 b the selection metric for each cluster and each unclassified solution is calculated as equation 4 the first term is the convergence term that is scaled by diagonal of the normalized objective space i e 2 in a bi objective space and m in an m dimensional space for a convergence between zero and one the shorter the distance to the origin the better convergence term and the better quality solution once the knee point for each cluster is identified sum of distance of each knee point from other knee points and from unclassified solutions is calculated in the normalized decision space and it is scaled by the maximum distance summation to have a dissimilarity index between zero and one knee points represent their cluster and their groupmates are not involved in the calculation of the proposed selection metric after computing the selection metric value for a knee point its groupmates will get the same metric value and one of them in a cluster is randomly selected for generating the subsequent solution 4 s c l u s t e r 1 1 a 2 i 1 3 a i max a i b i c i d i s c l u s t e r 2 1 b 2 i 1 3 b i max a i b i c i d i s n o i s e 1 1 c 2 i 1 3 c i max a i b i c i d i s noise 2 1 d 2 i 1 3 d i max a i b i c i d i 3 4 omni optimizer algorithm omni optimizer is a population based evolutionary optimization algorithm with a bounded archive developed by deb and tiwari 2008 the structure of omni optimizer is similar to that of nsga ii algorithm with additional operators to help the algorithm solve multi objective multi modal problems it uses a hypercube sampling to generate the initial population and constructs a bigger set by two random ordering of the current population in each iteration it chooses four solutions from the bigger set using the nearest neighbor based method in the objective space for encouraging convergence to determine two parent solutions using binary tournament selection the parent solutions are then recombined and mutated by simulated binary crossover and polynomial mutation for generating offspring solutions the parent and offspring solutions are then combined and ranked based on non dominated sorting and crowding distance the omni optimizer favors non dominated over dominated solutions and less crowded solutions to more crowded solutions an important feature of the omni optimizer is that in multi modal situation where two solutions have identical objective vectors the crowding distance metric is calculated in the decision space instead of the objective space the latter becomes zero while the former has a non zero value omni optimizer was not designed to identify and maintain near optimal solutions however it is used in this study as a reference algorithm for results comparison since it considers crowding distance calculation in the decision space among solutions with identical objective vectors readers are referred to deb and tiwari 2008 for more detail about omni optimizer s structure and its performance 3 5 optimization problems the cluster based pa dds algorithm is compared to the original version of pa dds for solving two bi objective mathematical test problems a bi objective sorptive barrier design problem and a lake pollution control problem introduced in this section for the identification of distinct optimal and near optimal solutions 3 5 1 modified sym part test problem the sym part bi objective bi variable problem was first introduced by rudolph et al 2007 as a multi modal test problem schütze et al 2011 modified sym part to make it have one pareto optimal front and eight near optimal fronts shown in problem formulation 5 where x 1 and x 2 are real valued decision variables ranging in 8 8 f 1 and f 2 are the objective functions and t 1 and δ t are auxiliary variables 5 f 1 x x 1 t 1 c 2 a a 2 x 2 t 2 b 2 δ t f 1 x x 1 t 1 c 2 a a 2 x 2 t 2 b 2 δ t t 1 s g n x 1 m i n x 1 a 0 5 c 2 a c 1 t 2 s g n x 2 m i n x 2 0 5 b b 1 δ t 0 i f t i 0 i 1 2 0 1 o t h e r w i s e as shown in fig 6 a b and c are constant and if they are respectively set to 0 5 5 and 5 the global continuous red line and local optimum dashed black lines pareto sets and fronts are formed 3 5 2 modified omni test the scalable omni test problem in problem formulation 6 is adopted from deb and tiwari 2008 with a slight modification this minimization mo problem is slightly modified in this paper to a test with one global optimum and multiple local pareto subsets by introducing a new constant δ increasing the range or the number of decision variables increases the multi modality of the problem deb and tiwari 2008 three decision variables with a range between 0 and 4 9 are defined in this paper that contains eight near optimal subsets 6 f 1 x i 1 3 sin π x i δ f 2 x i 1 3 cos π x i δ δ 0 i f x i 2 0 1 o t h e r w i s e the global and local pareto sets occur where decision variables are between 1 and 1 5 or between 3 and 3 5 if all decision variables range from 1 to 1 5 the pareto optimal front continuous red line in fig 7 is created and the local minimal front dashed black line in fig 7 is produced with other combinations of the mentioned extremum intervals 3 5 3 lake pollution control problem ward et al 2015 developed a scalable four objective benchmark optimization problem from a modelling study conducted by carpenter et al 1999 for the management of eutrophication of a shallow lake this problem also known as the lake problem aims to maximize the economic profits of a town by finding the amount of yearly anthropogenic phosphorous release a t while maintaining the reliability and control policy inertia the dimensionless total mass concentration of phosphorous p t at annual time step t is calculated using equation 7 besides the annual phosphorous release from the town p t in each time step also depends on p t 1 and the uncertain non point natural sources of pollution flowing into the lake ε t that is emulated by a random number sampled from a log normal distribution with a mean of 0 02 and a log10 variance of 5 5 as reported in ward et al 2015 this problem is a monte carlo simulation based function evaluation due to the uncertainty in the uncontrolled pollution term ε t unlike its original variant in ward et al 2015 that considered 100 year pollution management twenty decision variables 20 year management policy a 1 a 2 a 20 varying from 0 to 0 1 dimensionless are considered in this problem there are two parameters b and q that are associated with phosphorous recycling and decaying rates in the lake these parameters are respectively fixed to 0 42 and 2 ward et al 2015 to impose an irreversible eutrophic state on the lake if p t exceeds a pre defined threshold p c r t c l that is a function of b and q 7 p t p t 1 a t b p t 1 p t 1 q 1 p t 1 q ε t 8 f 1 a 1 k i 1 k t 0 t 1 α a t i δ t t 20 k 100 as shown in equation 8 the first objective is to maximize the average economic benefit across k simulations of t years of random ε t as in ward et al 2015 α and δ are dimensionless parameters that are set to 0 4 and 0 98 respectively representing the town s desire to pay for pollution control and the discount factor to convert future profits to present utilities the second objective equation 9 is to minimize the highest total phosphorous concentration p t averaged across k simulations stability in anthropogenic phosphorous rate over time is another important criterion that needs to be taken into consideration since rapid reduction in phosphorous in the lake requires large infrastructural investments and it is best to preserve policy inertia for this reason the difference between two consecutive release rates should be less than a pre specified threshold i c r t c l 0 02 as in ward et al 2015 in equation 10 the second term in equation 10 is designed to find a reliable management policy to prevent from permanent eutrophication in the lake by keeping p t below a critical value i e irreversible threshold p c r t c l 0 5 9 f 2 a max t 1 2 t 1 k i 1 k p t i 10 f 3 a 1 k i 1 k 1 t 1 t 1 t 1 θ t i 1 1 k t i 1 k t 1 t ϑ t i θ t i 0 i f a t 1 i a t i i c r t c l 1 i f a t 1 i a t i i c r t c l ϑ t i 0 i f p t i p c r t c l 1 i f p t i p c r t c l 3 5 4 sorptive barrier design the sorptive barrier design problem introduced by bartelt hunt et al 2006 is a combinatorial simulation optimization problem that seeks the cheapest option s for waste management while mitigating the migration of contaminants from organic wastes through a multi layer sorptive liner the landfill liner design is converted to a single objective constrained optimization benchmark problem by matott et al 2012 consisting of six integer valued decision variables thirteen alternative 15 cm layers with coded values from one to thirteen are available for each decision variable that is made of variable mixture of sand bentonite benzyltriethylammonium bentonite hexadecyltrimethylammonium bentonite shale and granular activated carbon the fifth and sixth decision variables can take a fourteenth option which is a no layer option to allow for a design with variable number of layers l the first objective function f 1 in problem formulation 11 is the design cost that has to be minimized m 2 if the number of layers is higher than four the opportunity cost c o s t 1 will be added to the material cost c o s t 2 this problem also uses a one dimensional numerical model to simulate the cumulative amount of 1 2 dichlorobenzene 1 2 dcb contaminants a x infiltrated into the ground from the liner over time the second objective function used in this study in problem formulation 11 that was a constraint in matott et al 2012 is to keep cumulative amount of contaminants below a pre defined allowable amount of 5 0 μ g m 2 over 100 years design lifetime readers are referred to matott et al 2012 regarding layer compositions and the associated costs 11 f 1 x c o s t 1 l c o s t 2 x c o s t 1 l 5 625 l 4 c o s t 2 x i 1 6 l a y e r c o s t x i f 2 x a x 5 3 6 numerical experiment setup and results comparison approach pa dds is a stochastic mo optimization algorithm in that its solution differs in different trials therefore in order to compare the distribution and proximity of the original pa dds algorithm with hypervolume contribution metric to its cluster based variant introduced in this study ten independent trial runs are conducted on each of the mo optimization problems the number of function evaluations is different for each type of problem mentioned in table 1 depending on the computational complexity and search objective space dimensionality of the problem moreover determining the value of minimum adjacency density is highly dependent on the available computational budget and the results will be highly different for different values of ε due to their simple structure the value of ε is set to 0 09 for the benchmark test problems which is equivalent to 5 to 6 5 percent discrepancy for each decision variable it is assumed if the difference between two solutions in the lake problem is at least 15 percent for each of their decision variables they are two dissimilar options and the value of ε is equal to 0 67 for a decision space normalized to between zero and one the decision space in the sorptive barrier design problem is integer and if the values of four out of six decision variables are one step higher or lower between two solutions they are considered as distinct design options resulting in ε equal to 0 16 when the design space is scaled to between zero and one for each decision variable the omni optimizer algorithm is compared with the cluster based pa dds in terms of decision space diversity and near optimal solution preservation the same computational budget and post processing procedure is used for both of these optimization algorithms the omni optimizer however has multiple parameters that influence its performance they are the population size number of generation probability of crossover probability of mutation distribution indices for crossover and mutation that can take different values for different optimization problems following deb and tiwari 2008 the crossover probability and index for mutation are respectively set on 0 9 and 20 for all optimization cases the distribution index for crossover is subjectively set to 15 for the sym part and sorptive barrier design problem and 10 for the modified omni test and lake problems a higher distribution index value aids in escaping local optima and producing an offspring that is far away from its parent while a lower value helps fine tuning and convergence deb and beyer 2001 the mutation probability is set to 1 n where n is the number of decision variables based on the recommendations by deb and tiwari 2008 the population size and number of generations are shown in table 2 based on the set computational budget in this study the resulted archive in the case of the modified sym part and omni test problems are analyzed and processed based on the closeness to their mathematically known global and local optimal regions the optimization results for the modified sym part and omni test problems are visualized and compared in their 2d and 3d decision spaces unlike the mathematical test problems there is no prior information about the global or local optimal solution sets in the lake problem and the sorptive barrier design problem therefore a three stage post processing analysis is performed on their archives to identify the desirable solution s based on decision makers preference the first stage is to re cluster archived solutions using dbscan in the decision space to distinguish similar solutions from dissimilar solutions based on their cluster labels the same adjacency density parametrizations in table 1 are considered for re clustering the archives in each type of problem the second stage is to identify and retain dissimilar solutions that include the unclassified archived solutions and only one solution in each cluster which is closest to a reference point in the objective space among its groupmates if there are also constraints the closest solution to the reference point must meet the constraints the reference point can be an ideal objective vector utopia or any desirable values of objectives defined by a decision maker the third stage is to define an acceptable threshold for each objective function and identify desirable solutions among dissimilar solutions in the second stage that have better objective values than the thresholds in the end the performance of two versions of pa dds algorithms are compared based on the solutions that passed the screening process in the third stage by showing their decision variable vectors on a parallel coordinate plot and a dissimilarity index shown in equation 12 12 i s i z e s c r e e n i 1 10 a r c h i v e i i 1 10 s i z e s c r e e n a r c h i v e i in order to calculate the dissimilarity index in equation 12 the archives of all optimization trial runs are combined and the three stage screening process is applied to the combined archive the dissimilarity index value is equal to the number of screened dissimilar solutions after combining the archive sets of all individual trials divided by sum of number of dissimilar screened solutions for each trial the higher the value of dissimilarity index the better the performance of the optimization algorithm in terms of identifying distinct desirable options 4 results and discussion 4 1 mathematical test functions the cluster based pa dds identifies all optimal and near optimal regions in each trial in the modified sym part and omni test problems while the original pa dds and omni optimizer algorithms only find the optimal pareto front in each trial and they are unable to detect distinct near optimal solution sets fig 8 and fig 9 demonstrate the results for one trial respectively for the modified sym part and omni test problems and the cluster based pa dds is able to find and assign a unique cluster tag to each locally optimal region the archive set also contains other low quality clusters each of which form a unique tradeoff along with unclassified solutions in other regions of decision space but they are filtered out after post processing based on the information we have about the location of the near optimal and optimal regions in the decision space for these test problems the post processed results for other trial runs are similar to figs 8 and 9 but with different cluster tags the original pa dds concentrates on global convergence and uniform distribution of non dominated solutions in the objective space figs 8 and 9 show that a large number of solutions are produced and retained by the original pa dds that are well diversified along the optimal pareto front with no solution representing the near optimal front this does not mean the pa dds algorithm does not explore the near optimal regions in the decision space it may find and preserve distant near optimal solutions in the archive set however since a global dominance check is carried out based on the objective values in the traditional solution archiving they are eliminated from the archive when a new dominating solution is produced therefore the original pa dds does not consider the decision space diversity in the optimization process and therefore does not guarantee maintenance of distinct near optimal solutions the result of the omni optimizer algorithm is similar to that of original pa dds with the difference that the omni optimizer has a bounded archive i e 40 and 100 solutions respectively for the sym part and omni test problems in light of the fact that the multi modality of the mathematical test problems are modified to a global and multi local optimal regions the decision space crowding measure in the omni optimizer cannot help retain solutions representing local optima in fact that the decision space crowding measure is activated when there are at least two solutions with identical objective vectors that never happened in problems solved in this study on the contrary most of the searching power in the cluster based pa dds is used for the identification of local optimal regions and dissimilarity maintenance but distribution of solutions along the optimal or near optimal fronts in the objective space is not in priority performing a local gradient based strategy on the desirable clusters or solutions in the archive of the cluster based pa dds after the termination of optimization may help to sufficiently populate each cluster for having a uniform distribution in the objective space 4 2 pollution control problem table 3 presents the performance of two variants of the pa dds algorithm along with the omni optimizer algorithm in terms of distinction level of their archives for the lake problem the archive set for the original pa dds algorithm has a very small size ranging from 36 to 87 solutions out of 20000 generated solutions compared to the cluster based version and it has a constant size of 100 in the omni optimizer algorithm for each optimization trial the size of archive in the cluster based pa dds is close to 3000 solutions seeking a desirable solution in an archive by comparing the values of decision variables and the corresponding objectives is laborious if not impossible as a result a systematic post processing strategy is utilized in this study to further screen the archived solutions based on a series of subjective judgements that often require consulting with decision makers the first screening stage is to re cluster archived solutions using dbscan with the same adjacency density parameters used by the cluster based pa dds the nearest solution to the reference point in each cluster is maintained along with unclassified solutions and the rest of the archived solutions are filtered out in the second screening stage a subjective threshold is set for each objective function this threshold should be defined based on the decision makers desirable range for each objective function for example the lake problem is assumed to have an irreversible eutrophic condition meaning that the reliability index should be 1 0 otherwise the corresponding 20 year pollution management policy leads to a permanent eutrophic state in the lake with no possible water quality restoration by solely reducing phosphorous loading therefore solutions that have a reliability index of less than 1 0 are removed from the archive the numbers highlighted by the bold font in columns five to seven of table 3 show the number of dissimilar design options solutions that do not tip into irreversible polluted state according to the adjacency density radius in table 1 for the lake problem the higher number of archived solutions by the cluster based pa dds suggests that each of its trial identified a significantly larger number of distinct design options that are considered reliable in the lake problem in the third screening stage it is assumed that only design options solutions that score more than 85 inertia index require acceptable infrastructural investments for phosphorous reduction in the lake therefore solutions not meeting this screening criterion are filtered out and the archive size is reduced to 2 to 8 solutions for trials of the original pa dds and 1 to 4 solutions for the cluster based pa dds depending on the trial number which is displayed inside parenthesis in table 3 it is interesting that only a few distinct archived solutions with reliability value of 1 0 are identified by the original pa dds 3 10 solutions and the majority or all of them have inertial maintenance index of higher than 85 further analysis of the results shows that solutions identified by all trials of the original pa dds are highly similar this means that most of the computational budget is consumed by all trials of pa dds to converge to a common location in the decision space despite the fact that the original pa dds is equipped with an unbounded archive that is expected to archive a high number of solutions the omni optimizer has a higher number of archived solutions and offers higher number of solutions with reliability index of one in each optimization trial the ratio of solutions with inertia index higher than 85 percent among reliable solutions is lower for the omni optimizer compared to that for the original pa dds in all trials for example 12 28 versus 7 7 in the first trial see table 3 this is due to considering decision space crowding distance in the structure of omni optimizer this ratio is the lowest for the cluster based pa dds in all trial runs since the algorithm does not solely focus on a specific region in the search space the archive sets of all trials for the omni optimizer algorithm and each variant of pa dds are combined resulting in 1000 solutions for the omni optimizer 625 solutions for the original pa dds and 29808 solutions for the cluster based pa dds the aforementioned three stage screening procedure is applied to these three archive sets separately only five reliable dissimilar solutions are identified by original pa dds and only three of these solutions have policy inertia index of higher than 85 therefore most of the high quality solutions identified by different trials of the original pa dds are similar and only 9 percent of them are dissimilar giving different design options this number is 149 reliable dissimilar solutions for the omni optimizer algorithm 32 of which having inertia index of higher than 85 as a result the dissimilarity index of the reliable solutions in the omni optimizer algorithm is 60 considerably higher than the original pa dds on the other hand the cluster based pa dds identified 1358 dissimilar solutions with reliability of 1 0 23 of which have policy inertia higher than 85 these 23 solutions are the union of solutions identified by all trials of the cluster based pa dds and none of them have similar cluster tags interestingly the dissimilarity index increases from 98 to 100 with fine filtering the archived solutions for the proposed pa dds structure while it decreases from 60 to 9 45 and 6 respectively for the omni optimizer and the original pa dds algorithms therefore it is concluded that the proposed solutions archiving helps the optimization algorithm identify dissimilar solutions identifying higher number of dissimilar solutions after combining the archives of the cluster based pa dds gives a higher flexibility to decision makers to choose among higher number of solutions with a wider range of benefit f 2 and maximum annual pollution f 1 compared to the solutions identified by the conventional algorithm the range of the first and the second objectives for the omni optimizer algorithm is wider than the proposed pa dds structure among reliable solutions with inertia maintenance index of higher than 85 this is mainly due to the structure of the omni optimizer that is a population based algorithm while pa dds is a single solution based method for a fair comparison about the range of the objectives the omni optimizer needs to be compared to its cluster based variant with an adaptive archive size which is outside the scope of this paper fig 10 demonstrates the parallel coordinate plot of the remaining solutions f 4 1 0 and f 3 0 85 after screening the combined archives for the omni optimizer original and cluster based pa dds respectively the cluster based pa dds provides different values of annual phosphorous release from the town that are scattered all over their defined range while if the decision maker relies on the original pa dds they are limited to options with annual release values in the lower half of their range especially from the fourth year to the sixteenth year similarly the majority of the post processed solutions in the omni optimizer are clustered in the lower half of the range in a sub space containing decision variables from four to sixteen therefore the original pa dds discards many solutions that could be interesting for the decision maker because those solutions are dominated but near optimal in the objective space 4 3 sorptive barrier design problem a similar filtering process to the lake problem is conducted for the results of the sorptive barrier design problem the first stage is to employ dbscan for clustering and re clustering archived solutions of the omni optimizer algorithm and pa dds with the traditional and cluster based archiving the second stage is to screen the solutions based on their euclidean distance to the reference point see table 1 in a semi logarithmic objective space for this problem which is an ideal zero cost and allowable contaminant rate transported to the soil in the barrier design lifetime the last three columns of table 4 shows the number of dissimilar solutions closest to the reference point in the objective space that have a cost f 1 of less than 50 and satisfy the contaminant rate f 2 of 5 0 μ g m 2 for each optimization trial and the combined archive of all trials apparently the original pa dds is able to identify multiple distinct design options after the filtering process and offer more number of distinct solutions compared to the omni optimizer algorithm the reason can be attributed to the discontinuity of the decision space however the dissimilarity level of these algorithms is still less than the cluster based version when the archive sets are combined these algorithms result in dissimilarity index values of 0 53 and 0 61 versus 0 76 for the cost values less than 50 similar to the lake problem fine filtering of the archives by considering a lower threshold for the maximum acceptable cost increases the dissimilarity ratio for the cluster based pa dds to 88 on the contrary the dissimilarity ratio for the omni optimizer and original pa dds decreases to 32 and 51 the high dissimilarity index in the results of the sorptive barrier design problem compared to the lake problem has increased when the original pa dds is used and reduced when the omni optimizer is used this is mainly due to two reasons one reason is that the archive size resulted from the original pa dds for each trial of this type of problem with discrete decision variables is so small that dbscan identifies some solutions as noise since they do not meet the minimum adjacency density requirement for cluster formation despite having high similarity to other archived solutions which should not be considered as dissimilar options the second reason is that the discrete nature of this problem does not let the original pa dds algorithm fine tune decision variable values to decimal places the integer valued decision variables also aid in small archive size that exacerbates the situation for dbscan method to find really dissimilar design options in the post processing stage as it needs a sufficiently populated archive for more accurate clustering the integer valued decision space also causes low distinction level in the archive of the omni optimizer since this type of problem does not have a multi modal characteristic to make the algorithm activate its decision space crowding measure since there exists no solution to have an identical objective vector with another solution in the decision space fig 11 demonstrates the joint parallel coordinate plot of the decision variable vectors and the corresponding scaled objective vectors f 1 10 log f 2 log 5 e 6 for solutions with cost of less than 50 and the contamination of less than 5 0 μ g m 2 the red lines represent solutions with cost of less than 25 according to fig 11 all the solutions found by the omni optimizer and original pa dds algorithms have decision variable values of equal to or higher than seven providing no option with decision variable values of one to six however the cluster based pa dds allows decision makers to choose from design options that have decision variable of one to six when they are interested to include these layers for the barrier design provided they are flexible with a higher design cost a more detailed inspection of fig 11 reveals that the cluster based pa dds shows a higher distinction among solutions costing less than 25 compared to its original counterpart and the omni optimizer algorithm for instance the original pa dds cannot identify alternate designs having option no 13 for the fifth layer fig 11 also illustrates that among less costly solutions the proposed pa dds structure offers five options for the next layer if layer no 13 is used for a layer while the omni optimizer offers one to maximum three options for the next layer 5 conclusion this work has laid out a foundation for incorporating decision space diversity maintenance within stochastic mo algorithms a novel cluster based solution archiving approach is introduced by restructuring the dbscan clustering strategy to provide multiple distinct optimal and near optimal options to decision makers for a robust decision making similar solutions are clustered in the decision space if they meet the pre defined solution density requirement for initial cluster formation the dominance check is decentralized such that solutions are compared only within their own cluster dominated clustered solutions are discarded from the archive after a mutual comparison with their groupmates a selection metric is also devised that gives higher chance of selection to distant clusters with higher normalized hypervolume this solution archiving method adds two parameters to the mo algorithms that represent the minimum adjacency density for cluster formation and recommendations has been given with regard to pre setting their values the proposed cluster based pa dds was successfully applied to two bi objective mathematical tests a three objective lake pollution control and a bi objective integer valued barrier design problems and compared its performance to the original version and the omni optimizer as a reference algorithm that considers decision space diversity in multi modal optimization the original pa dds and the omni optimizer algorithms provided better distributed optimal fronts in the objective space with much lower archive size in a single trial but they were highly similar and clustered in one region in the decision space the omni optimizer and pa dds with the traditional archiving strategy were also unable to preserve dissimilar near optimal solutions in their archive with the progress of optimization due to global domination check among solutions while the cluster based pa dds performed better in terms of diversity maintenance in the decision space the proposed archiving approach was developed for the single solution based pa dds algorithm in this study but it can be also developed for population based mo algorithms provided they are equipped with an unbounded archive or an adaptive archive size such as borg moea hadka and reed 2013 in a single solution based mo algorithm such as pa dds clustering should be performed as soon as a solution is generated i e 10000 times clustering update for 10000 function evaluations however in a population based optimization algorithm clustering needs to be performed only after a new generation of solutions are evaluated e g 100 times clustering update is required for 10000 solution evaluations when the population size is 100 in addition selection metrics such as the objective space crowding distance hypervolume contribution or convex hull contributions are not immediately applicable when solutions are clustered instead a simple random selection metric or more advanced effective metrics need to be introduced to jointly consider the decision space and objective space diversities such as the one used for the cluster based pa dds in this work or variation rate in cuate and schütze 2019 is suggested for the cluster based solution archiving the cluster based pa dds provides a large archive of solutions that gives more flexibility to the decision maker in refining their preferences there are also objectives in the real world that are not quantifiable and cannot be formulated definitively these types of objectives are evaluated through subjective judgements in the post processing stage liebman 1976 in addition decision makers may change their preferences in the post processing stage after interacting with stakeholders for example if a decision maker is interested in benefit values higher than a specified threshold instead of an inertial maintenance index of higher than 0 85 in the lake problem other new distinct or dissimilar solutions will be found therefore we recommend archiving and presenting all clustered solutions for the decision makers advanced visualization techniques such as video kollat and reed 2007 and mograms trawiński et al 2018 should be used for presenting the large number of design options for the decision makers and helping them find their most desirable option declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by dr asadzadeh s natural sciences and engineering research council of canada nserc discovery grant rgpin 2016 05896 and graduate enhancement of tri council stipends gets university of manitoba winnipeg mb appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104902 
25907,the phenological and growth parameters of ceres maize were estimated using data from field variety trials in 2017 and 2018 to simulate hybrid maize zea mays l grown in michigan multiple calibration methods used include gencalc genotype coefficient calculator glue generalized likelihood uncertainty estimate nmcga noisy monte carlo genetic algorithm and ensembling approach three irrigated sites were used for calibration while six rainfed sites for evaluation better results were obtained when using multiple years of data in calibration than using only a single year model evaluation also suggests that fixed soil root growth factor srgf used in calibration irrigated condition tended to restrict root dynamics under rainfed condition this resulted in substantial yield mismatch due to poorly modeled yields although phenology was better predicted adjusting srgf under rainfed condition resulted in better model evaluation for both years moreover weighted averaging of genetic coefficients resulted in better predictions of phenology and yields keywords gencalc glue nmcga ensemble methods dssat software and or data availability name of software nmcga noisy monte carlo genetic algorithm developer amor ines contact amor ines michigan state university usa e mail address inesamor msu edu year first available 2008 hardware required pc software required windows 2007 or higher version program language fortran availability nmcga is available upon request cost n a 1 introduction maize zea mays l is a major crop grown in the midwestern us most of the maize grown in the state of michigan is rainfed and used for biofuel and animal feeds however farmers in the lower part of southwest michigan having fertile and sandy soils grow maize under intensive irrigation to meet the growing demand for maize its genetic performance must be improved and agronomic practices be optimized varietal improvements and development however require a lot of time and resources since the mid 20th century yield increments in maize have been attributed 50 60 to improved genetics and 40 50 to management practices duvick 2005 kucharik and ramankutty 2005 lee and tollenaar 2007 egli 2008 and sacks and kucharik 2011 although plant breeders have leveraged the genotype x environment g x e interactions elias et al 2016 yan 2016 sound agronomic management within the season can help overcome these challenges and minimize yield gap i e difference of potential and actual yields however improving management and cultivar development by traditional agronomic research methods is constrained by time vilayvong et al 2015 crop simulation models can facilitate the cultivar development process by virtual extrapolations of field experiments at multiple locations and seasons lobell et al 2009 crop simulation models have been used to study growth and development of crops under different environments white and hoogenboom 2010 asseng et al 2013 maccarthy et al 2017 jha et al 2018 they integrate knowledge in soil science plant physiology micrometeorology and agronomy to simulate crop growth and development van ittersum et al 2003 yin et al 2004 löffler et al 2005 cooper et al 2009 specific cultivar traits control the interactions of environmental factors e g temperature daylength light and plant processes growth and development these crop model parameters called genetic coefficients describe specific growth and development characteristics of a crop cultivar white and hoogenboom 1996 boote et al 2003 hoogenboom et al 2004 the first step in the applications of crop models is the estimation of these genetic coefficients wallach et al 2001 jones et al 2011 they can be estimated by extensive and exhaustive field and laboratory experiments du toit 2002 suriharn et al 2007 however this process can be hastened and simplified using carefully designed model calibration and evaluation several approaches have been used to estimate crop model parameters grimm et al 1993 used downhill simplex to estimate phenological parameters of soybean cultivars simulated annealing was used to estimate soil and root parameters of a soybean model calmon et al 1999 mavromatis et al 2002 hunt et al 1993 developed gencalc genotype coefficient calculator based on sequential search method that estimates coefficients by multiple iterations of approximate coefficients in a pre set sequence and compares outputs based on the difference between simulated and observed values e g anthesis and maturity dates yields the genotype coefficients are altered until a good model fit is found hunt et al 1993 the genetic coefficients of groundnut arachis hypogea l anothai et al 2008 soybean glycine max l bao et al 2015 wheat triticum aestivum l ibrahim et al 2016 and maize román paoli et al 2000 hassanien et al 2007 yang et al 2009 bao et al 2017 adnan et al 2019 have been estimated using gencalc however gencalc does not estimate uncertainties of the derived parameters he et al 2010 a detailed review of methods for calibrating model parameters and discussions about the past present and future of model calibrations can be found in siedel et al 2018 model outputs are prone to errors due to uncertainties in data inputs and model parameters and it is unrealistic to conclude that one set of parameters represents the model behaviour rather it is better to assess likelihood weights of the parameters which can better predict the model behaviour beven and binley 1992 a bayesian framework that assesses uncertainty of parameters using monte carlo technique called generalized likelihood uncertainty estimation glue overcomes gencalc s limitation campbell et al 1999 mertens et al 2004 candela et al 2005 he et al 2009 it uses observed data to develop prior parameter distributions he 2010 the posterior distribution is computed based on bayes theorem makowski et al 2006 glue has been used in the field of hydrology beven 2018 and crop sciences he et al 2010 for parameter estimation several works have compared gencalc and glue which are already embedded in the decision support system for agrotechnology transfer dssat model for wheat rice and maize calibrations but did not find any significant differences in performance between the methods e g ibrahim et al 2016 budhhaboon et al 2018 adnan et al 2019 genetic algorithm ga goldberg 1989 has been used also in crop model calibration sometimes outperforming other gradients or bayesian based optimization methods because of its ability to search through vast search spaces miller and goldberg 1996 smalley et al 2000 gopalakrishnan et al 2003 wu et al 2006 pabico et al 1999 used ga to determine genetic coefficients of cultivars mapping coefficients as a single chromosome the noisy monte carlo genetic algorithm nmcga ines and mohanty 2008a evaluates realizations of strings analogous to chromosome of model parameters with given distributions using a monte carlo approach wang 1991 ines and droogers 2002 pabico 2007 ines and mohanty 2008b dai et al 2009 in each generation fitter chromosomes are selected then undergo the process of crossover and mutation until a solution is achieved shin et al 2013 used nmcga to estimate soil hydraulic parameters of an agro hydrological model swap van dam 2000 and performed reasonably well on the other hand estimated parameters using individual methods always possess some levels of uncertainty due to errors in initial conditions data and model structure ensemble based parameter estimation methods can improve accuracy and account for different sources of uncertainties in model calibration chen et al 2015 they provide more robust estimates of model parameters e g vrugt and robinson 2007 parameters estimated by one method can better predict phenology other methods can better predict growth ensembling parameters derived from different methods can account for biases associated with the methods and can better predict phenology and yields in order to achieve robust genetic coefficients which can simulate phenology and yield of maize in michigan we designed this study to ensemble genetic coefficients from multiple methods and evaluate model performance the specific objectives of the study are i to calibrate genetic coefficients of a maize hybrid in michigan using multiple methods and ii to evaluate ceres maize model for simulating maize hybrid phenology and yields in michigan in this study we used three methods for estimating genetic coefficients of ceres maize namely gencalc glue and nmcga and employed an ensembling approach for genetic coefficients then assessed their performance we used maize performance trial locations in michigan in the model calibration irrigated and evaluation rainfed 2 materials and methods 2 1 study area data for this study were collected from the field experiments conducted at michigan maize performance trial locations mmpt singh et al 2018 performances of commercial corn hybrids are evaluated in mmpt annually in zones ranging from south to north across michigan fig 1 climatic conditions are similar within each zone and there are three trial locations per zone these zones were established based on long term accumulated growing degree days gdd the 30 year 1981 2010 normal accumulated gddf from may 1 to october 31 were 2557 f 1402 c 2478 f 1359 c and 2342 f 1283 c for zone 1 2 and 3 respectively gdd for maize growth and development are calculated by deducting the base temperature for maize growth 50 ᵒf 10 ᵒc cross and zuber 1972 stewart et al 1998 from the average air temperature in a daily 24 h period from the emergence date abendroth et al 2010 angel et al 2017 the upper threshold for optimum growth in maize is considered 86 ᵒf 30 ᵒc for calculating average temperature it means that whenever air temperature goes beyond 86 ᵒf 30 ᵒc the daily maximum temperature has to be set equal to 86 ᵒf 30 ᵒc however ceres maize model considers 8 ᵒc as base temperature and 34 ᵒc as optimum temperature for maize growth jones and kiniry 1986 based on heat accumulation seed companies provide information of relative maturity to understand the crop maturity period planting to physiological maturity with specified gdd numbers from planting to silking and to maturity respectively the zones coordinates major soil types and management practices of the field trials used in this study are given in table 1 2 2 ceres maize model ceres maize is a crop module embedded within the suite of cropping system model csm in dssat v4 6 jones and kiniry 1986 boote et al 2010 hoogenboom et al 2013 dssat facilitates the assessment and evaluation of different management practices on crop growth and development with the goal of enhancing current knowledge on genotype x environment management interactions boote et al 2010 elias et al 2016 ceres maize a fortran based process oriented model utilizes daily weather data to simulate crop growth stages on a daily basis integrating soil water and nitrogen balance associated with maize growth therefore csm in dssat integrates the interaction and effects of climate soil and the management practices and can be used to predict assess their impacts on crop growth and development in the past present and future lobell et al 2009 based on heat accumulation and photoperiod the model assumes that the rate of development increases linearly above a base temperature 8 c until 34 c remaining at plateau above 34 ᵒc which are governed by genetic coefficients p1 p2 p5 phint table 4 the phenological development also influences the process of morphological development of leaves stems and roots resulting in biomass accumulation and partitioning the phenological stages and growth parameters are governed by genetic coefficients that depend mostly on temperature and daylength kiniry and bonhomme 1991 2 2 1 input data for ceres maize 2 2 1 1 weather data weather data were collected from msu enviro weather network a weather based information system that helps growers and stakeholders in making farm related decisions in michigan https enviroweather msu edu all the weather data were ingested in the weather database of dssat using weatherman which converts data to dssat weather format pickering et al 1994 average seasonal values of total rainfall solar radiation maximum and minimum temperatures are given in table 2 except for huron all stations had greater total solar radiation in 2017 growing season than 2018 which could benefit the crops to achieve their potential biomass productions however 2018 growing season was wetter than 2017 2 2 1 2 soil data and agronomic management all nine locations were categorized under two major soil types i e loam and sandy loam based on major soil type according to wss ssurgo database table 1 initial conditions for sandy loam soil volumetric water 0 22 cm3 cm 3 soil n nh4 3 g n mg 1 soil and soil n no3 5 g n mg 1 soil and for loam soil volumetric water 0 40 cm3 cm 3 soil n nh4 5 g n mg 1 soil and soil n no3 6 g n mg 1 soil were kept in the medium range rutan and steinke 2017 field data such as fertilizer applications planting and anthesis 75 of silking dates and yields are given in table 3 however due to limited data availability on maturity dates black layer they were estimated based on degree days accumulation for 2500 gddf using u2u tool angel et al 2017 for both years and all locations table 1 the split doses of n were applied uniformly at each location around 15 of n as 10 34 0 was applied during planting and the remaining n was applied as urea ammonium nitrate solution during v6 six leaf stage to v8 growth stage farm yard manure fym was applied before planting at allegan ingham huron and mason sites the harvested yields were estimated from the center two rows of four row plot with 6 7 m row length and 0 76 m row spacing harvesting occurred using a kincaid 8 xp plot combine after physiological maturity to collect data on grain yield and moisture content the final yields were estimated on dry basis adjusted to 0 moisture 2 3 calibration methods 2 3 1 genetic coefficients maize hybrid of comparative relative maturity crm 103 lauer 1998 that requires around 2500 gddf from planting to physiological maturity was selected for the model calibration calibrations were done using all three irrigated locations branch cass and mason out of total nine varietal trial locations during 2017 and 2018 growing seasons maize grown at irrigated locations were selected for calibration as optimal crop management conditions water and nutrients are supposed to be required for the calibration process lobell et al 2009 the starting cultivar selected for gencalc was pc0001 2500 2600 gddf from the dssat database dssat v4 6 hoogenboom et al 2013 which is suitable for zones in michigan based on gddf accumulation descriptions of genetic coefficients in ceres maize are listed in table 4 p1 and p2 determine anthesis tassel initiation is controlled by both p1 and p2 p1 p2 and p5 determine maturity dates while p5 g2 g3 and phint directly indirectly control yield and its components e g dry matter grain size and canopy weight through their impacts on leaf area index lai grain number grain size and duration of grain growth jones and kiniry 1986 hanks and ritchie 1991 román paoli et al 2000 du toit 2002 phint controls phenology and growth as well through determination of the timing to leaf tip appearance hammad et al 2018 phint was kept fixed in the calibration phnt 49 while the other five parameters described above were calibrated it is recommended not to change phint unless sufficient field data of leaf numbers are available dssat v 4 6 hoogenboom et al 2013 2 3 2 gencalc genetic calculator gencalc is a built in software in dssat which estimates genetic coefficients using a gradient search method hunt et al 1993 adnan et al 2019 with pre defined set of experiments and starting cultivar coefficients of a selected maize variety it runs ceres maize iteratively to search for the best parameter estimates with an initial value for each parameter it adjusts genetic coefficients until it fits with the provided observed value within the range of their physiological characteristics i e flowering date maturity date daylength kernel size etc hunt et al 1993 the algorithm in the software exploits a search space that depends on a starting point i e coefficients of starting cultivar and based on the differences between simulated and observed values it adjusts the coefficients it minimizes the error between simulated and observed values in each run hunt et al 1993 because of a small sampling area of the search space the final coefficients cannot be optimized for wide ranges of physiological characteristics pabico et al 1999 pre defined targets a measured crop traits and rules which govern the sequence of genotype coefficient calculation govern the search until the best fit to each observation is found after multiple iterations parameter set that gives the best fit with observed anthesis date represented as anthesis day after planting adap maturity date represented as maturity day after planting mdap and yield at harvest maturity hwam are stored gencalc estimates genetic coefficients in two steps optimizing phenology parameters p1 p2 p5 first and then growth parameters g2 g3 phenological development depends on degree day accumulation and growth depends on phenology fig 4 hence it is logical that phenology parameter is derived first and then growth parameters gencalc does not estimate uncertainties of parameters 2 3 3 generalized likelihood uncertainty estimate glue glue estimates parameters using a bayesian approach glue first develops the prior parameter distributions using range of genetic coefficients from the dssat database hoogenboom et al 2013 by fitting them to a multivariate normal distribution and then estimates the posterior distributions of each parameter using bayes theorem eq 1 1 p θ o p o θ p θ p o where θ and o represent the parameter set and observations respectively p θ o is the posterior distribution p o θ is the likelihood p θ is the prior probability and p o is a normalizing constant to calculate likelihood values random parameter sets θi are generated from the prior distributions the more the number of parameter set realizations the more stable results can be obtained he et al 2010 for stability in results we selected 30 000 runs for glue a likelihood value l θi o for each observation anthesis date maturity date and yield is estimated based on gaussian likelihood function eq 2 he et al 2010 2 l θ i o j 1 m 1 2 π σ o 2 exp o j y θ i 2 2 σ o 2 where θi is the ith parameter set m is the number of observations oj is the jth observation σo 2 is the variance of model error and y θi is the output of the model in addition eq 3 calculates the probability of the parameter set 3 p θ i l θ i o j 1 n l θ i o where p θi is the probability or likelihood weight of the ith parameter set θi l θi o is the likelihood value of parameter set θi given observations o he et al 2010 the empirical posterior distributions were constructed from the pairs of parameter set and probabilities θi p θi i 1 n the means and variances of those chosen parameters were calculated as in eqs 4 and 5 he et al 2010 4 μ p o s t θ i 1 n p θ i θ i 5 σ p o s t 2 θ i 1 n p θ i θ i μ p o s t θ 2 where μpost θ and σ2 post θ are the mean and variance of the posterior distribution of parameters θ and p θi is the probability of the ith parameter set the parameter estimation in glue follows a similar step as in gencalc i e estimate first the phenology parameters p1 p2 p5 and then the growth parameters g2 g3 the value of phint was kept constant phnt 49 for model evaluation the parameter set that gives the maximum likelihood value is selected 2 3 4 noisy monte carlo genetic algorithm nmcga there are multiple methods of parameter estimation each has its own advantages and disadvantages makowski et al 2006 ensemble based parameter estimation methods tend to improve model accuracy and account for different sources of uncertainty providing more robust estimates of model parameters e g vrugt and robinson 2007 along with gencalc and glue we also employed the noisy monte carlo genetic algorithm nmcga ines and mohanty 2008a to estimate maize genetic coefficients here genetic algorithm ga estimates combination of parameters i e means and standard deviations and evaluate their fitness based on a priori distributions and a priori range of parameter values from dssat cultivar database parameter sets are evaluated using monte carlo resampling resampled parameters sets are then passed to ceres maize to evaluate the fitness of that parameter set the fitness of the parameters are tested by evaluating the difference between simulated and observed values nmcga being a noisy ga evaluates the fitness of a parameter set under a noisy fitness space wu et al 2006 thus an overall fitness of a parameter set is evaluated from the average fitness of several ensemble runs from parameter set realizations the fittest parameters are selected and allowed to reproduce for multiple generations undergoing crossovers and mutations until an optimal solution is achieved for consistency we also employed a two step parameter estimation technique like in gencalc and glue i e estimating phenology parameters first then growth parameters the coefficients first and second moments of phenology p1 p2 and p5 and growth g2 and g3 were arranged as a set of genes in a chromosome during those steps respectively the value of phint was also kept constant phnt 49 for model evaluation the objective function of the parameter set for the ith ensemble is formulated as eq 6 6 o b j k i m i n 1 t t 1 t 1 n r e s a m p l e r 1 n r e s a m p l e s i m k r t i o b s t i where kr is set of k parameters combinations with r realizations generated from monte carlo resampling and nresample is the total number of realizations for simulated sim kr and observed variables obst ti is running index for time t ines and mohanty 2008a noisy fitness is calculated using the inverse of the modified penalty approach of hilton and culver 2000 eqs 7 and 8 7 z k i o b j k i 1 p e n a l t y k i i 8 f i t n e s s p i 1 z k i i where p is the chromosome and fitness p is the noisy fitness of that chromosome sampled from each ensemble i from the monte carlo resampling a chromosome realization is penalized penalty k if its predicted variables violate some preset rules against the goodness of fit evaluation ines and mohanty 2008a sampling fitness is calculated based on eq 9 to reduce the noise in fitness 9 s f i t n e s s p 1 r i 1 r f i t n e s s p i where r is total number of ensemble i the arrays of parameters sets chromosome of means and standard deviations undergo through the search process until the best chromosome is generated when calibrating for phenology adap and mdap were given the same weights while hwam was not used in the objective function eq 6 when calibrating for growth hwam was used in the objective function while adap and mdap were not moreover we ran nmcga in two ways one by estimating only the means of parameters nmcga no sd and other by estimating both the means and standard deviations of the parameters nmcga sd the representations of p as used in this study are given in table 5 2 3 5 ensembling approach along with the comparisons of gencalc glue and two variants of nmcga we evaluated an ensembling approach of estimating crop model parameters the purpose of ensembling was to integrate the strengths of the three methods with a goal of achieving a more robust set of genetic coefficients the general framework is shown in eqs 10 12 10 p j i 1 n w i p i j j 11 w i w i i 1 n w i i 12 w i k 1 k β k 1 a p i j k a o k 2 i j where pj is the ensembled value of a phenology or growth parameter wi is the weight of a parameter from method i pij is the parameter value of a phenology or growth parameter from method i j is an index of a phenology or growth parameter wi is the inverse squared distance between the predicted apij k and observed ao k variable s e g adap mdap or hwam substantially impacted by that phenology or growth parameter n is the number of methods here n 4 k is an index for predicted or observed variable s k is the number of variable s impacted by pij and β k is the weight for that variable k however since hwam and mdap have different units eq 12 is transformed to eq 13 13 w i k 1 k β k 1 a p i j k a o k a o k 2 i j for arithmetic average wi 1 n for all i s in order to improve model calibration parameter uncertainty has to be minimized which can be done by reducing uncertainty in input like soil properties initialization variables and management practices wallach et al 2012 dzotsi et al 2015 varella et al 2012 roux et al 2014 waha et al 2015 we improve this further by using multiple calibration methods and combine the best information coming from those methods 2 4 evaluation and statistical analysis before model evaluation the calibrated model was used to estimate potential productions no water and nutrient stresses for all locations to analyze the genetic potentials of the calibrated cultivar in those locations all rainfed locations washtenaw allegan ingham saginaw huron and montcalm were used for model evaluation table 1 genetic coefficients derived from gencalc glue two variants of nmcga and the ensembling approach were used to evaluate ceres maize under rainfed conditions these were done for both growing seasons 2017 and 2018 we compared predicted and observed adap mdap and hwam and used the coefficient of determination r2 eq 14 mean bias error mbe eq 15 root mean square error rmse eq 16 and index of agreement d index eq 17 willmott 1982 to measure the performances of the calibration methods 14 r 2 i 0 n o o m m 2 i 0 n o o 2 i 0 n m m 2 15 m b e 1 n 1 n m o 16 r o o t m e a n s q u a r e e r r o r 1 n m o 2 n 17 i n d e x o f a g r e e m e n t d i n d e x 1 1 n o m 2 i 0 n m o o o 2 where m and o are simulated and observed variables respectively 3 results and discussion 3 1 calibration well watered and well fertilized crops are suggested to be used in crop model calibration grassini et al 2015 initially we calibrated ceres maize using only 2017 data under irrigated locations branch cass and mason and results are shown in table 6 according to gdd requirement maize hybrid pc0001 2500 2600 gdd selected for starting the search of parameters in gencalc was suitable for the study area prokopy et al 2017 tables 4 and 6 show that gencalc only changed p1 and p2 values from pc0001 2500 2600 gdd in glue the calibrated value of p1 was reduced compared to the selected maize hybrid which suggests that juvenile stage should end three to four days earlier and hence adjusting silking as well however nmcga sd and no sd estimated an increase in p1 value which suggests that the juvenile stage needs more degree days to complete hence anthesis is delayed furthermore a lower p2 value was estimated which signifies that any delay in anthesis is compensated as there is a delay in the developmental process if photoperiod is increased above the maximum physiological limit of 12 5 h jones and kiniry 1986 hanks and ritchie 1991 román paoli et al 2000 for all methods it was observed that there were low estimates of p5 which signify that the calibrated cultivar requires lesser thermal time from silking to physiological maturity compared to pc0001 2500 2600 gdd g2 values are relatively low especially for nmcga no sd modern hybrids should have a maximum kernel number as close to 800 du toit 2002 based on inverse square distance between observed and predicted variables anthesis maturity and yield the highest weights for p1 p2 and p5 were given to nmcga no sd 0 40 method while for g2 and g3 were given to gencalc 0 36 not shown for a more objective comparison we evaluated ceres maize performance using the calibrated crop coefficients table 7 shows that all methods performed well in simulating adap however most of the methods did not perform well in simulating mdap and yield this modest performance could be attributed to the amount of data used in model calibration only 2017 estimation of genetic coefficients of a cultivar for a specific agro climatic environment requires adequate data from multiple locations and cropping seasons to include environmental variability of the genotype environment and management interactions kersebaum et al 2015 he et al 2017 to obtain better representative parameter estimates it is recommended to use multiple years of calibration data bulatewicz et al 2009 confalonieri et al 2016 seidel et al 2018 we then re run the calibrations using 2017 and 2018 data for the irrigated locations the genetic coefficients calibrated from the two year datasets are given in table 8 and ceres maize performances using those are shown in table 9 3 1 1 anthesis and physiological maturity calibration results overall ceres maize performance in simulating phenology improved after calibration using two years data see tables 7 and 9 due to lower p1 values in the new calibration see tables 6 and 8 anthesis dates adap were slightly under predicted in all the methods which is reflected by the negative mbe tables 7 and 9 gencalc glue nmcga sd and nmcga no sd under predicted anthesis except for nmcga no sd all the methods over predicted maturity table 9 however d index for adap and mdap improved substantially for all methods poor optimization of parameters p1 p2 and p5 possibly caused deviations in the predicted phenology especially for p5 which determines the development of the cultivar after anthesis and hence maturity might not be optimized see hanks and ritchie 1991 román paoli et al 2000 p5 represents thermal time from silking to physiological maturity and it varies from 700 to 1000 gdd for modern cultivars román paoli et al 2000 phenological impacts however are the results of the combined effects of all phenological parameters hence improper calibration of the phenological parameters may create a slight difference in gdd that can affect anthesis and maturity román paoli et al 2000 coefficients of determination r2 between predicted and observed anthesis dates were found moderately high for all the methods fig 2 overall however all the methods under predicted anthesis in 2017 and 2018 at branch and cass but over predicted in mason for both years this result reflects the inter annual variability in predicting anthesis variability in genetic expressions depends greatly on the variations in weather parameters especially solar radiation during the growing season lee et al 2016 mason has higher amount of accumulated solar radiation than branch and cass table 2 coefficients of determination r2 for maturity dates were found higher for all methods fig 2 3 1 2 yield calibration results growth parameters g2 and g3 control yield directly whereas p1 p2 and p5 control it indirectly the reproductive growth stages and yield components e g dry matter grain size and canopy weight are controlled by p5 g2 g3 and phint and the interactions of these parameters hammad et al 2018 however g2 is the most critical parameter in predicting yield ritchie and wei 2000 du toit 2002 ritchie and alagarswamy 2003 lizaso et al 2007 which is dependent on the sowing date and associated weather variability zhou et al 2017 g2 values increased during re calibration using two years data and its impact is manifested in yield improvements in all the methods this result might be due to favourable weather conditions in 2018 as weather conditions during grain filling has significant impact on kernel weights tables 8 and 9 gencalc predicted yield with d index of 0 96 and rmse of 784 kg ha 1 while glue predicted yield with d index of 0 93 and rmse of 1093 kg ha 1 nmcga sd outperformed all the methods with predicted yield d index of 0 97 and rmse of 665 kg ha 1 table 9 fig 2 coefficients of determination r2 of yield are high showing a strong confidence to the model in the simulation of yields fig 2 yearly variations of adap mdap and yield were attributed to the variability in weather and other management practices e g confalonieri et al 2016 waha et al 2015 3 1 3 calibration results from ensemble of methods as shown above the individual methods performed differently in predicting phenology and yield some are better in predicting phenology and some are better in predicting yield however we wanted to calibrate genetic coefficients that are robust and resistant as vrugt and robinson 2007 noted the advantage of ensembling models we performed ensembling of the genetic coefficients run them in ceres maize and compared their performances from the individual methods we employed weighted and arithmetic averaging to ensemble the coefficients section 2 3 5 in the weighted averaging method weights were assigned to the parameters based on the distance between the predicted and observed variables that they mostly influenced e g adap mdap and hwam in the initial calibration i e using only 2017 data ensembled coefficients performed poorly especially for mdap and yield as did individual methods with the re calibration using 2017 and 2018 data their performance improved substantially outperforming some of the methods tables 7 and 9 overall nmcga sd performed best in predicting yield d index 0 97 table 8 however weighted averaging performed relatively better in predicting phenology anthesis and maturity and comparable in predicting yield table 8 arithmetic averaging of the coefficients also performed well these results corroborate the value of using multiple methods e g vrugt and robinson 2007 in crop model calibration and ensembling the derived parameters for better performance 3 2 evaluation the calibrated crop genetic coefficients were evaluated for two years 2017 and 2018 at the six rainfed locations washtenaw allegan ingham saginaw huron and montcalm see table 1 phenology anthesis and maturity results showed that the model could perform well with d index varying from 0 84 to 0 95 for all methods table 10 however yields were substantially under predicted with high mbe negative and rmse and low d index table 10 there could be other factors that affected this yield performance we reviewed the literature and found some related works pointing out about the adaptive capacity of the crops to their environments if they were source limited such as root growth sharp and davies 1985 lorens et al 1987 vamerali et al 2003 hund et al 2009 although dssat simulates the root dynamics of the plant there are exogenous factors that are set by users that can possibly restrict that growth simulated by the model this must be evaluated because we are modeling a real world system 3 2 1 soil root growth factor adjustment soil root growth factor srgf is a soil input parameter in dssat which controls the maximum rooting depth and root mass distribution with depth in the soil profile jones and kiniry 1986 the root growth distribution function in ceres maize can be calibrated for different soil types as root hospitality factor which gives flexibility to root growth in the model according to soil water availability and structure friasse et al 2001 in dssat v4 6 srgf with a value of 1 0 allows the root to grow equally in the soil layer and gradually decreases to zero in tapered form through the deeper layer yang et al 2017 table 11 shows the srgf vertical distributions for the sandy loam and loam soils used in the calibration irrigated locations ibpt910006 and mskb890006 in dssat v4 6 soil sol respectively and evaluation rainfed locations the srgf distributions for the irrigated conditions were adopted from these two soil types listed in dssat v4 6 soil database hoogenboom et al 2013 the final srgf distributions used for evaluation rainfed as shown in the table did not come from dssat sbuild program their determination is discussed below based on a loam soil profile developed for iowa u s a see dssat v4 6 soil sol iubf970211 by ritchie hoogenboom et al 2013 roots were allowed to fully grow by keeping srgf 1 0 until the soil depth of 90 cm then slowly tapering down until the depth of 190 cm where srgf 0 for the sandy loam soil ibpt910006 dssat v4 6 we revised the srgf parameters by relaxing srgf to 1 0 until 40 cm depth then slowly tapering down to 0 2 until 140 cm table 11 for the loam soil mskb890006 dssat v4 6 srgf values were relaxed to 1 0 until 89 cm then slowly tapering down to 0 2 until 160 cm in addition to modifying srgf for rainfed condition we extended the maximum soil depth to 215 cm the srgf used in irrigated conditions old soil profile structure when applied to rainfed conditions could restrict the crop from exploring available resources from the soil note that the srgf in dssat sbuild is static varella et al 2012 explained that uncertainties in soil input parameters can influence model performances water and nutrient availability in the root zone have a substantial impact on root geometry dynamics and physiology therefore influencing plant water and nutrient uptakes and yields ritchie et al 1998 ma et al 2006 maize root grows deeper in the soil to extract more water per unit length of root in rainfed than in irrigated conditions sharp and davies 1985 these root dynamics are more prevalent in the event of stress during critical periods e g tasselling to grain filling in maize lorens et al 1987 vamerali et al 2003 hund et al 2009 lenka et al 2009 panda et al 2004 and djaman and irmak 2012 noted that in well irrigated conditions root water extraction mostly takes place from the top soil layers based on our literature review and multiple model iterations we suggest that the srgf in the soil input file may be restricting the roots to grow deeper when applied under water stressed conditions e g lópez cedrón et al 2008 yang et al 2009 e g in this case study the revised soil profiles were used in the model evaluation and ceres maize yield predictions substantially improved table 12 we analyzed the new evaluation results to test our hypothesis about the adaptive capacity of the plant under stressed environments that the static soil file input parameters may restrict root growth fig 3 a suggests yield improvement from 4058 kg ha 1 with the old soil profile used for calibration to 11 137 kg ha 1 with the revised one used for evaluation which is close to the 10 984 kg ha 1 observed yield the improvement in yield prediction after modifying srgf suggests that soil profile data need careful consideration when applying the model to simulate hybrid maize under rainfed conditions the plant s feedback or response mechanisms to water stressed environment e g root expansion and lengthening might not be accounted for by the model particularly being restricted by the soil capacity set up by the user to allow roots to wander deeper and wider in the rootzone in dssat sbuild srgf is currently static and should be replaced by a dynamic rooting distribution for different soils and crops such as the one developed by jones et al 1991 here the observed yields are our gold standard data observed by standard agronomic protocol and matching them with the model outputs together with observed phenology supports our attempt to adjust a static srgf profile to get a better fit of the data physiologically maximum root growth occurs during pre silking period to provide enough water for crop growth and development liu et al 2017 yang et al 2017 in the revised soil profile root ceased to grow after silking 78 dap which is typically 5 7 days after tasselling fig 3a c which helps in the reduction of water stress during tassel initiation to early grain filling period e g lizaso et al 2018 the root length density for each soil layer is more uniform in the case of the revised soil profile fig 3b and c the root layer density suggests that the deeper root layers 8 and 9 are able to extract water and nutrients during the peak demand period of crop growth fig 3c this is restricted in the old soil profile configuration fig 3b as roots in soil layers 7 and 8 grew only after tasselling and silking and did not grow deep enough to support the peak demands of water and nutrient in the growing season fig 3a and b one of the plausible reasons could be that the grain number function and grain growth rate have too much water stress effect built in more typically the crop should have set a full grain number thus no assimilate left over for the roots which probably what happened on the revised soil profile case that minimized water stress the srgf values used for the rainfed treatments can be used for the irrigated as well so we also performed simulations under irrigation with the revised soil profile to evaluate if relaxing srgf contributed to yield improvement when water is readily available we found that there was no substantial difference in predicted yields under irrigated conditions before and after relaxing srgf values fig 4a root length density in the deeper layers however became more uniform after tasselling after relaxing srgf values fig 4c while the deeper roots in layers 9 and 10 continued growing after silking with the older soil profile but that did not contribute much to the yield dynamics as sufficient water was already provided by irrigation fig 4a and b the revised soil profiles were used in the evaluation of the derived genetic coefficients for all rainfed locations in 2017 and 2018 sections 3 2 2 and 3 2 3 show the performance of phenology and yield predictions with the revised soil profile configurations 3 2 2 anthesis and maturity evaluation results in the model evaluation predictions of anthesis adap and maturity dates mdap did not change substantially when using the old table 10 or new soil profiles table 12 for all the methods anthesis dates were slightly underpredicted but with high d index table 12 it suggests that the calibrated p1 and p2 coefficients performed well in predicting anthesis at the rainfed locations overpredictions of maturity dates by 3 days glue and nmcg sd to a week gencalc and underprediction by 4 days nmcga no sd were also observed table 12 ensembling methods performed better than individual methods in predicting maturity dates table 12 coefficients of determination r2 in fig 5 also show that the model could predict anthesis with high accuracy overlapping phenological data in some locations can be observed in the case of saginaw in2018 and huron in 2017 and is mainly due to the inter annual variability of crop response to planting dates and weather patterns ensembling methods have the highest r2 values for mdap prediction fig 5 3 2 3 yield evaluation results tables 10 and 12 suggest that majority of the impacts of enhancing root dynamics is accounted for by the improvements in yields and not on phenology see section 3 2 2 the yield d index for all methods improved from 0 3 table 10 to 0 9 table 12 and r2 0 8 fig 5 yields were still overpredicted with rmse ranging from 536 to 774 kg ha 1 table 12 allowing the root system to explore more resources allowed ceres maize to match better the observed yields under rainfed conditions which as mentioned above is our gold standard data to fitting the model this result suggests the intrinsic limitation of a rigid soil based root growth factor srgf i e not dynamic if one assumes it blindly and does not account for the adaptive capacity of the crops and of the unique characteristics of the soil when setting up a soil profile data under water stress conditions 4 summary and conclusions in this study we employed existing gencalc glue and new methods nmcga sd and nmcga no sd of crop model calibrations ensembled them to get robust estimates of a hybrid maize genetic coefficients in michigan genetic coefficients of ceres maize were calibrated for a hybrid using three irrigated locations branch cass and mason during 2017 and 2018 growing seasons results suggest that using two years 2017 2018 data for calibration gave better results than by using one year data alone 2017 the phenological parameters were mostly affected by the inclusion of 2018 data in the calibration the calibrated coefficients were used to evaluate ceres maize performance across six rainfed locations washtenaw allegan ingham saginaw huron and montcalm during 2017 and 2018 growing seasons the crop genetic coefficients as used in ceres maize were able to produce potential yields that are higher than observed yields suggesting that the model can generate those levels of yields if applied in real world conditions under optimal management however under stressed environments evaluation results suggest that there are mechanisms that are not accounted for or permitted by the irrigated model setup used in calibration for a crop grown under rainfed conditions particularly on root dynamics based on literature reviews and above hypothesis we relaxed the capacity of the soil to allow the root geometry to change from the irrigated setup which resulted in a better model fit using similar soil file inputs in 2017 and 2018 the calibrated crop genetic coefficients with revised soil input parameters performed well under rainfed conditions the revised soil file inputs were evaluated under irrigated conditions with nearly similar results as the old soil file inputs suggesting that the root system did not change substantially between the two soil file inputs when the crops were irrigated this result is possibly due to the abundance of water in the upper rootzone which indicates that there should be one soil input file and rooting behaviour that apply for both irrigated and rainfed crops our results suggest that the calibration of genetic coefficients benefits greatly from multiple seasons of data and on the ensembling of calibrated genetic coefficients from different methods than using only one method the purpose of ensembling genetic coefficients from several methods was to integrate the strengths of the methods with the goal of achieving a set of genetic coefficients that is robust and resistant we found that ensembling coefficients by means of weighted averaging outperformed most of the other ensembling and calibration methods declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this manuscript acknowledgment this work was partly funded by the corn marketing program of michigan cmpm and msu agbioresearch usa we acknowledge chubu university japan listenfield japan usaid philippines and nasa servir usa for partly funding the first author s research work at msu we also acknowledge bill widdicombe for the michigan maize performance trial mmpt experiments and katlin fusilier for collecting specific field data we thank the editors and reviewers for helping us improve the quality of the work 
25907,the phenological and growth parameters of ceres maize were estimated using data from field variety trials in 2017 and 2018 to simulate hybrid maize zea mays l grown in michigan multiple calibration methods used include gencalc genotype coefficient calculator glue generalized likelihood uncertainty estimate nmcga noisy monte carlo genetic algorithm and ensembling approach three irrigated sites were used for calibration while six rainfed sites for evaluation better results were obtained when using multiple years of data in calibration than using only a single year model evaluation also suggests that fixed soil root growth factor srgf used in calibration irrigated condition tended to restrict root dynamics under rainfed condition this resulted in substantial yield mismatch due to poorly modeled yields although phenology was better predicted adjusting srgf under rainfed condition resulted in better model evaluation for both years moreover weighted averaging of genetic coefficients resulted in better predictions of phenology and yields keywords gencalc glue nmcga ensemble methods dssat software and or data availability name of software nmcga noisy monte carlo genetic algorithm developer amor ines contact amor ines michigan state university usa e mail address inesamor msu edu year first available 2008 hardware required pc software required windows 2007 or higher version program language fortran availability nmcga is available upon request cost n a 1 introduction maize zea mays l is a major crop grown in the midwestern us most of the maize grown in the state of michigan is rainfed and used for biofuel and animal feeds however farmers in the lower part of southwest michigan having fertile and sandy soils grow maize under intensive irrigation to meet the growing demand for maize its genetic performance must be improved and agronomic practices be optimized varietal improvements and development however require a lot of time and resources since the mid 20th century yield increments in maize have been attributed 50 60 to improved genetics and 40 50 to management practices duvick 2005 kucharik and ramankutty 2005 lee and tollenaar 2007 egli 2008 and sacks and kucharik 2011 although plant breeders have leveraged the genotype x environment g x e interactions elias et al 2016 yan 2016 sound agronomic management within the season can help overcome these challenges and minimize yield gap i e difference of potential and actual yields however improving management and cultivar development by traditional agronomic research methods is constrained by time vilayvong et al 2015 crop simulation models can facilitate the cultivar development process by virtual extrapolations of field experiments at multiple locations and seasons lobell et al 2009 crop simulation models have been used to study growth and development of crops under different environments white and hoogenboom 2010 asseng et al 2013 maccarthy et al 2017 jha et al 2018 they integrate knowledge in soil science plant physiology micrometeorology and agronomy to simulate crop growth and development van ittersum et al 2003 yin et al 2004 löffler et al 2005 cooper et al 2009 specific cultivar traits control the interactions of environmental factors e g temperature daylength light and plant processes growth and development these crop model parameters called genetic coefficients describe specific growth and development characteristics of a crop cultivar white and hoogenboom 1996 boote et al 2003 hoogenboom et al 2004 the first step in the applications of crop models is the estimation of these genetic coefficients wallach et al 2001 jones et al 2011 they can be estimated by extensive and exhaustive field and laboratory experiments du toit 2002 suriharn et al 2007 however this process can be hastened and simplified using carefully designed model calibration and evaluation several approaches have been used to estimate crop model parameters grimm et al 1993 used downhill simplex to estimate phenological parameters of soybean cultivars simulated annealing was used to estimate soil and root parameters of a soybean model calmon et al 1999 mavromatis et al 2002 hunt et al 1993 developed gencalc genotype coefficient calculator based on sequential search method that estimates coefficients by multiple iterations of approximate coefficients in a pre set sequence and compares outputs based on the difference between simulated and observed values e g anthesis and maturity dates yields the genotype coefficients are altered until a good model fit is found hunt et al 1993 the genetic coefficients of groundnut arachis hypogea l anothai et al 2008 soybean glycine max l bao et al 2015 wheat triticum aestivum l ibrahim et al 2016 and maize román paoli et al 2000 hassanien et al 2007 yang et al 2009 bao et al 2017 adnan et al 2019 have been estimated using gencalc however gencalc does not estimate uncertainties of the derived parameters he et al 2010 a detailed review of methods for calibrating model parameters and discussions about the past present and future of model calibrations can be found in siedel et al 2018 model outputs are prone to errors due to uncertainties in data inputs and model parameters and it is unrealistic to conclude that one set of parameters represents the model behaviour rather it is better to assess likelihood weights of the parameters which can better predict the model behaviour beven and binley 1992 a bayesian framework that assesses uncertainty of parameters using monte carlo technique called generalized likelihood uncertainty estimation glue overcomes gencalc s limitation campbell et al 1999 mertens et al 2004 candela et al 2005 he et al 2009 it uses observed data to develop prior parameter distributions he 2010 the posterior distribution is computed based on bayes theorem makowski et al 2006 glue has been used in the field of hydrology beven 2018 and crop sciences he et al 2010 for parameter estimation several works have compared gencalc and glue which are already embedded in the decision support system for agrotechnology transfer dssat model for wheat rice and maize calibrations but did not find any significant differences in performance between the methods e g ibrahim et al 2016 budhhaboon et al 2018 adnan et al 2019 genetic algorithm ga goldberg 1989 has been used also in crop model calibration sometimes outperforming other gradients or bayesian based optimization methods because of its ability to search through vast search spaces miller and goldberg 1996 smalley et al 2000 gopalakrishnan et al 2003 wu et al 2006 pabico et al 1999 used ga to determine genetic coefficients of cultivars mapping coefficients as a single chromosome the noisy monte carlo genetic algorithm nmcga ines and mohanty 2008a evaluates realizations of strings analogous to chromosome of model parameters with given distributions using a monte carlo approach wang 1991 ines and droogers 2002 pabico 2007 ines and mohanty 2008b dai et al 2009 in each generation fitter chromosomes are selected then undergo the process of crossover and mutation until a solution is achieved shin et al 2013 used nmcga to estimate soil hydraulic parameters of an agro hydrological model swap van dam 2000 and performed reasonably well on the other hand estimated parameters using individual methods always possess some levels of uncertainty due to errors in initial conditions data and model structure ensemble based parameter estimation methods can improve accuracy and account for different sources of uncertainties in model calibration chen et al 2015 they provide more robust estimates of model parameters e g vrugt and robinson 2007 parameters estimated by one method can better predict phenology other methods can better predict growth ensembling parameters derived from different methods can account for biases associated with the methods and can better predict phenology and yields in order to achieve robust genetic coefficients which can simulate phenology and yield of maize in michigan we designed this study to ensemble genetic coefficients from multiple methods and evaluate model performance the specific objectives of the study are i to calibrate genetic coefficients of a maize hybrid in michigan using multiple methods and ii to evaluate ceres maize model for simulating maize hybrid phenology and yields in michigan in this study we used three methods for estimating genetic coefficients of ceres maize namely gencalc glue and nmcga and employed an ensembling approach for genetic coefficients then assessed their performance we used maize performance trial locations in michigan in the model calibration irrigated and evaluation rainfed 2 materials and methods 2 1 study area data for this study were collected from the field experiments conducted at michigan maize performance trial locations mmpt singh et al 2018 performances of commercial corn hybrids are evaluated in mmpt annually in zones ranging from south to north across michigan fig 1 climatic conditions are similar within each zone and there are three trial locations per zone these zones were established based on long term accumulated growing degree days gdd the 30 year 1981 2010 normal accumulated gddf from may 1 to october 31 were 2557 f 1402 c 2478 f 1359 c and 2342 f 1283 c for zone 1 2 and 3 respectively gdd for maize growth and development are calculated by deducting the base temperature for maize growth 50 ᵒf 10 ᵒc cross and zuber 1972 stewart et al 1998 from the average air temperature in a daily 24 h period from the emergence date abendroth et al 2010 angel et al 2017 the upper threshold for optimum growth in maize is considered 86 ᵒf 30 ᵒc for calculating average temperature it means that whenever air temperature goes beyond 86 ᵒf 30 ᵒc the daily maximum temperature has to be set equal to 86 ᵒf 30 ᵒc however ceres maize model considers 8 ᵒc as base temperature and 34 ᵒc as optimum temperature for maize growth jones and kiniry 1986 based on heat accumulation seed companies provide information of relative maturity to understand the crop maturity period planting to physiological maturity with specified gdd numbers from planting to silking and to maturity respectively the zones coordinates major soil types and management practices of the field trials used in this study are given in table 1 2 2 ceres maize model ceres maize is a crop module embedded within the suite of cropping system model csm in dssat v4 6 jones and kiniry 1986 boote et al 2010 hoogenboom et al 2013 dssat facilitates the assessment and evaluation of different management practices on crop growth and development with the goal of enhancing current knowledge on genotype x environment management interactions boote et al 2010 elias et al 2016 ceres maize a fortran based process oriented model utilizes daily weather data to simulate crop growth stages on a daily basis integrating soil water and nitrogen balance associated with maize growth therefore csm in dssat integrates the interaction and effects of climate soil and the management practices and can be used to predict assess their impacts on crop growth and development in the past present and future lobell et al 2009 based on heat accumulation and photoperiod the model assumes that the rate of development increases linearly above a base temperature 8 c until 34 c remaining at plateau above 34 ᵒc which are governed by genetic coefficients p1 p2 p5 phint table 4 the phenological development also influences the process of morphological development of leaves stems and roots resulting in biomass accumulation and partitioning the phenological stages and growth parameters are governed by genetic coefficients that depend mostly on temperature and daylength kiniry and bonhomme 1991 2 2 1 input data for ceres maize 2 2 1 1 weather data weather data were collected from msu enviro weather network a weather based information system that helps growers and stakeholders in making farm related decisions in michigan https enviroweather msu edu all the weather data were ingested in the weather database of dssat using weatherman which converts data to dssat weather format pickering et al 1994 average seasonal values of total rainfall solar radiation maximum and minimum temperatures are given in table 2 except for huron all stations had greater total solar radiation in 2017 growing season than 2018 which could benefit the crops to achieve their potential biomass productions however 2018 growing season was wetter than 2017 2 2 1 2 soil data and agronomic management all nine locations were categorized under two major soil types i e loam and sandy loam based on major soil type according to wss ssurgo database table 1 initial conditions for sandy loam soil volumetric water 0 22 cm3 cm 3 soil n nh4 3 g n mg 1 soil and soil n no3 5 g n mg 1 soil and for loam soil volumetric water 0 40 cm3 cm 3 soil n nh4 5 g n mg 1 soil and soil n no3 6 g n mg 1 soil were kept in the medium range rutan and steinke 2017 field data such as fertilizer applications planting and anthesis 75 of silking dates and yields are given in table 3 however due to limited data availability on maturity dates black layer they were estimated based on degree days accumulation for 2500 gddf using u2u tool angel et al 2017 for both years and all locations table 1 the split doses of n were applied uniformly at each location around 15 of n as 10 34 0 was applied during planting and the remaining n was applied as urea ammonium nitrate solution during v6 six leaf stage to v8 growth stage farm yard manure fym was applied before planting at allegan ingham huron and mason sites the harvested yields were estimated from the center two rows of four row plot with 6 7 m row length and 0 76 m row spacing harvesting occurred using a kincaid 8 xp plot combine after physiological maturity to collect data on grain yield and moisture content the final yields were estimated on dry basis adjusted to 0 moisture 2 3 calibration methods 2 3 1 genetic coefficients maize hybrid of comparative relative maturity crm 103 lauer 1998 that requires around 2500 gddf from planting to physiological maturity was selected for the model calibration calibrations were done using all three irrigated locations branch cass and mason out of total nine varietal trial locations during 2017 and 2018 growing seasons maize grown at irrigated locations were selected for calibration as optimal crop management conditions water and nutrients are supposed to be required for the calibration process lobell et al 2009 the starting cultivar selected for gencalc was pc0001 2500 2600 gddf from the dssat database dssat v4 6 hoogenboom et al 2013 which is suitable for zones in michigan based on gddf accumulation descriptions of genetic coefficients in ceres maize are listed in table 4 p1 and p2 determine anthesis tassel initiation is controlled by both p1 and p2 p1 p2 and p5 determine maturity dates while p5 g2 g3 and phint directly indirectly control yield and its components e g dry matter grain size and canopy weight through their impacts on leaf area index lai grain number grain size and duration of grain growth jones and kiniry 1986 hanks and ritchie 1991 román paoli et al 2000 du toit 2002 phint controls phenology and growth as well through determination of the timing to leaf tip appearance hammad et al 2018 phint was kept fixed in the calibration phnt 49 while the other five parameters described above were calibrated it is recommended not to change phint unless sufficient field data of leaf numbers are available dssat v 4 6 hoogenboom et al 2013 2 3 2 gencalc genetic calculator gencalc is a built in software in dssat which estimates genetic coefficients using a gradient search method hunt et al 1993 adnan et al 2019 with pre defined set of experiments and starting cultivar coefficients of a selected maize variety it runs ceres maize iteratively to search for the best parameter estimates with an initial value for each parameter it adjusts genetic coefficients until it fits with the provided observed value within the range of their physiological characteristics i e flowering date maturity date daylength kernel size etc hunt et al 1993 the algorithm in the software exploits a search space that depends on a starting point i e coefficients of starting cultivar and based on the differences between simulated and observed values it adjusts the coefficients it minimizes the error between simulated and observed values in each run hunt et al 1993 because of a small sampling area of the search space the final coefficients cannot be optimized for wide ranges of physiological characteristics pabico et al 1999 pre defined targets a measured crop traits and rules which govern the sequence of genotype coefficient calculation govern the search until the best fit to each observation is found after multiple iterations parameter set that gives the best fit with observed anthesis date represented as anthesis day after planting adap maturity date represented as maturity day after planting mdap and yield at harvest maturity hwam are stored gencalc estimates genetic coefficients in two steps optimizing phenology parameters p1 p2 p5 first and then growth parameters g2 g3 phenological development depends on degree day accumulation and growth depends on phenology fig 4 hence it is logical that phenology parameter is derived first and then growth parameters gencalc does not estimate uncertainties of parameters 2 3 3 generalized likelihood uncertainty estimate glue glue estimates parameters using a bayesian approach glue first develops the prior parameter distributions using range of genetic coefficients from the dssat database hoogenboom et al 2013 by fitting them to a multivariate normal distribution and then estimates the posterior distributions of each parameter using bayes theorem eq 1 1 p θ o p o θ p θ p o where θ and o represent the parameter set and observations respectively p θ o is the posterior distribution p o θ is the likelihood p θ is the prior probability and p o is a normalizing constant to calculate likelihood values random parameter sets θi are generated from the prior distributions the more the number of parameter set realizations the more stable results can be obtained he et al 2010 for stability in results we selected 30 000 runs for glue a likelihood value l θi o for each observation anthesis date maturity date and yield is estimated based on gaussian likelihood function eq 2 he et al 2010 2 l θ i o j 1 m 1 2 π σ o 2 exp o j y θ i 2 2 σ o 2 where θi is the ith parameter set m is the number of observations oj is the jth observation σo 2 is the variance of model error and y θi is the output of the model in addition eq 3 calculates the probability of the parameter set 3 p θ i l θ i o j 1 n l θ i o where p θi is the probability or likelihood weight of the ith parameter set θi l θi o is the likelihood value of parameter set θi given observations o he et al 2010 the empirical posterior distributions were constructed from the pairs of parameter set and probabilities θi p θi i 1 n the means and variances of those chosen parameters were calculated as in eqs 4 and 5 he et al 2010 4 μ p o s t θ i 1 n p θ i θ i 5 σ p o s t 2 θ i 1 n p θ i θ i μ p o s t θ 2 where μpost θ and σ2 post θ are the mean and variance of the posterior distribution of parameters θ and p θi is the probability of the ith parameter set the parameter estimation in glue follows a similar step as in gencalc i e estimate first the phenology parameters p1 p2 p5 and then the growth parameters g2 g3 the value of phint was kept constant phnt 49 for model evaluation the parameter set that gives the maximum likelihood value is selected 2 3 4 noisy monte carlo genetic algorithm nmcga there are multiple methods of parameter estimation each has its own advantages and disadvantages makowski et al 2006 ensemble based parameter estimation methods tend to improve model accuracy and account for different sources of uncertainty providing more robust estimates of model parameters e g vrugt and robinson 2007 along with gencalc and glue we also employed the noisy monte carlo genetic algorithm nmcga ines and mohanty 2008a to estimate maize genetic coefficients here genetic algorithm ga estimates combination of parameters i e means and standard deviations and evaluate their fitness based on a priori distributions and a priori range of parameter values from dssat cultivar database parameter sets are evaluated using monte carlo resampling resampled parameters sets are then passed to ceres maize to evaluate the fitness of that parameter set the fitness of the parameters are tested by evaluating the difference between simulated and observed values nmcga being a noisy ga evaluates the fitness of a parameter set under a noisy fitness space wu et al 2006 thus an overall fitness of a parameter set is evaluated from the average fitness of several ensemble runs from parameter set realizations the fittest parameters are selected and allowed to reproduce for multiple generations undergoing crossovers and mutations until an optimal solution is achieved for consistency we also employed a two step parameter estimation technique like in gencalc and glue i e estimating phenology parameters first then growth parameters the coefficients first and second moments of phenology p1 p2 and p5 and growth g2 and g3 were arranged as a set of genes in a chromosome during those steps respectively the value of phint was also kept constant phnt 49 for model evaluation the objective function of the parameter set for the ith ensemble is formulated as eq 6 6 o b j k i m i n 1 t t 1 t 1 n r e s a m p l e r 1 n r e s a m p l e s i m k r t i o b s t i where kr is set of k parameters combinations with r realizations generated from monte carlo resampling and nresample is the total number of realizations for simulated sim kr and observed variables obst ti is running index for time t ines and mohanty 2008a noisy fitness is calculated using the inverse of the modified penalty approach of hilton and culver 2000 eqs 7 and 8 7 z k i o b j k i 1 p e n a l t y k i i 8 f i t n e s s p i 1 z k i i where p is the chromosome and fitness p is the noisy fitness of that chromosome sampled from each ensemble i from the monte carlo resampling a chromosome realization is penalized penalty k if its predicted variables violate some preset rules against the goodness of fit evaluation ines and mohanty 2008a sampling fitness is calculated based on eq 9 to reduce the noise in fitness 9 s f i t n e s s p 1 r i 1 r f i t n e s s p i where r is total number of ensemble i the arrays of parameters sets chromosome of means and standard deviations undergo through the search process until the best chromosome is generated when calibrating for phenology adap and mdap were given the same weights while hwam was not used in the objective function eq 6 when calibrating for growth hwam was used in the objective function while adap and mdap were not moreover we ran nmcga in two ways one by estimating only the means of parameters nmcga no sd and other by estimating both the means and standard deviations of the parameters nmcga sd the representations of p as used in this study are given in table 5 2 3 5 ensembling approach along with the comparisons of gencalc glue and two variants of nmcga we evaluated an ensembling approach of estimating crop model parameters the purpose of ensembling was to integrate the strengths of the three methods with a goal of achieving a more robust set of genetic coefficients the general framework is shown in eqs 10 12 10 p j i 1 n w i p i j j 11 w i w i i 1 n w i i 12 w i k 1 k β k 1 a p i j k a o k 2 i j where pj is the ensembled value of a phenology or growth parameter wi is the weight of a parameter from method i pij is the parameter value of a phenology or growth parameter from method i j is an index of a phenology or growth parameter wi is the inverse squared distance between the predicted apij k and observed ao k variable s e g adap mdap or hwam substantially impacted by that phenology or growth parameter n is the number of methods here n 4 k is an index for predicted or observed variable s k is the number of variable s impacted by pij and β k is the weight for that variable k however since hwam and mdap have different units eq 12 is transformed to eq 13 13 w i k 1 k β k 1 a p i j k a o k a o k 2 i j for arithmetic average wi 1 n for all i s in order to improve model calibration parameter uncertainty has to be minimized which can be done by reducing uncertainty in input like soil properties initialization variables and management practices wallach et al 2012 dzotsi et al 2015 varella et al 2012 roux et al 2014 waha et al 2015 we improve this further by using multiple calibration methods and combine the best information coming from those methods 2 4 evaluation and statistical analysis before model evaluation the calibrated model was used to estimate potential productions no water and nutrient stresses for all locations to analyze the genetic potentials of the calibrated cultivar in those locations all rainfed locations washtenaw allegan ingham saginaw huron and montcalm were used for model evaluation table 1 genetic coefficients derived from gencalc glue two variants of nmcga and the ensembling approach were used to evaluate ceres maize under rainfed conditions these were done for both growing seasons 2017 and 2018 we compared predicted and observed adap mdap and hwam and used the coefficient of determination r2 eq 14 mean bias error mbe eq 15 root mean square error rmse eq 16 and index of agreement d index eq 17 willmott 1982 to measure the performances of the calibration methods 14 r 2 i 0 n o o m m 2 i 0 n o o 2 i 0 n m m 2 15 m b e 1 n 1 n m o 16 r o o t m e a n s q u a r e e r r o r 1 n m o 2 n 17 i n d e x o f a g r e e m e n t d i n d e x 1 1 n o m 2 i 0 n m o o o 2 where m and o are simulated and observed variables respectively 3 results and discussion 3 1 calibration well watered and well fertilized crops are suggested to be used in crop model calibration grassini et al 2015 initially we calibrated ceres maize using only 2017 data under irrigated locations branch cass and mason and results are shown in table 6 according to gdd requirement maize hybrid pc0001 2500 2600 gdd selected for starting the search of parameters in gencalc was suitable for the study area prokopy et al 2017 tables 4 and 6 show that gencalc only changed p1 and p2 values from pc0001 2500 2600 gdd in glue the calibrated value of p1 was reduced compared to the selected maize hybrid which suggests that juvenile stage should end three to four days earlier and hence adjusting silking as well however nmcga sd and no sd estimated an increase in p1 value which suggests that the juvenile stage needs more degree days to complete hence anthesis is delayed furthermore a lower p2 value was estimated which signifies that any delay in anthesis is compensated as there is a delay in the developmental process if photoperiod is increased above the maximum physiological limit of 12 5 h jones and kiniry 1986 hanks and ritchie 1991 román paoli et al 2000 for all methods it was observed that there were low estimates of p5 which signify that the calibrated cultivar requires lesser thermal time from silking to physiological maturity compared to pc0001 2500 2600 gdd g2 values are relatively low especially for nmcga no sd modern hybrids should have a maximum kernel number as close to 800 du toit 2002 based on inverse square distance between observed and predicted variables anthesis maturity and yield the highest weights for p1 p2 and p5 were given to nmcga no sd 0 40 method while for g2 and g3 were given to gencalc 0 36 not shown for a more objective comparison we evaluated ceres maize performance using the calibrated crop coefficients table 7 shows that all methods performed well in simulating adap however most of the methods did not perform well in simulating mdap and yield this modest performance could be attributed to the amount of data used in model calibration only 2017 estimation of genetic coefficients of a cultivar for a specific agro climatic environment requires adequate data from multiple locations and cropping seasons to include environmental variability of the genotype environment and management interactions kersebaum et al 2015 he et al 2017 to obtain better representative parameter estimates it is recommended to use multiple years of calibration data bulatewicz et al 2009 confalonieri et al 2016 seidel et al 2018 we then re run the calibrations using 2017 and 2018 data for the irrigated locations the genetic coefficients calibrated from the two year datasets are given in table 8 and ceres maize performances using those are shown in table 9 3 1 1 anthesis and physiological maturity calibration results overall ceres maize performance in simulating phenology improved after calibration using two years data see tables 7 and 9 due to lower p1 values in the new calibration see tables 6 and 8 anthesis dates adap were slightly under predicted in all the methods which is reflected by the negative mbe tables 7 and 9 gencalc glue nmcga sd and nmcga no sd under predicted anthesis except for nmcga no sd all the methods over predicted maturity table 9 however d index for adap and mdap improved substantially for all methods poor optimization of parameters p1 p2 and p5 possibly caused deviations in the predicted phenology especially for p5 which determines the development of the cultivar after anthesis and hence maturity might not be optimized see hanks and ritchie 1991 román paoli et al 2000 p5 represents thermal time from silking to physiological maturity and it varies from 700 to 1000 gdd for modern cultivars román paoli et al 2000 phenological impacts however are the results of the combined effects of all phenological parameters hence improper calibration of the phenological parameters may create a slight difference in gdd that can affect anthesis and maturity román paoli et al 2000 coefficients of determination r2 between predicted and observed anthesis dates were found moderately high for all the methods fig 2 overall however all the methods under predicted anthesis in 2017 and 2018 at branch and cass but over predicted in mason for both years this result reflects the inter annual variability in predicting anthesis variability in genetic expressions depends greatly on the variations in weather parameters especially solar radiation during the growing season lee et al 2016 mason has higher amount of accumulated solar radiation than branch and cass table 2 coefficients of determination r2 for maturity dates were found higher for all methods fig 2 3 1 2 yield calibration results growth parameters g2 and g3 control yield directly whereas p1 p2 and p5 control it indirectly the reproductive growth stages and yield components e g dry matter grain size and canopy weight are controlled by p5 g2 g3 and phint and the interactions of these parameters hammad et al 2018 however g2 is the most critical parameter in predicting yield ritchie and wei 2000 du toit 2002 ritchie and alagarswamy 2003 lizaso et al 2007 which is dependent on the sowing date and associated weather variability zhou et al 2017 g2 values increased during re calibration using two years data and its impact is manifested in yield improvements in all the methods this result might be due to favourable weather conditions in 2018 as weather conditions during grain filling has significant impact on kernel weights tables 8 and 9 gencalc predicted yield with d index of 0 96 and rmse of 784 kg ha 1 while glue predicted yield with d index of 0 93 and rmse of 1093 kg ha 1 nmcga sd outperformed all the methods with predicted yield d index of 0 97 and rmse of 665 kg ha 1 table 9 fig 2 coefficients of determination r2 of yield are high showing a strong confidence to the model in the simulation of yields fig 2 yearly variations of adap mdap and yield were attributed to the variability in weather and other management practices e g confalonieri et al 2016 waha et al 2015 3 1 3 calibration results from ensemble of methods as shown above the individual methods performed differently in predicting phenology and yield some are better in predicting phenology and some are better in predicting yield however we wanted to calibrate genetic coefficients that are robust and resistant as vrugt and robinson 2007 noted the advantage of ensembling models we performed ensembling of the genetic coefficients run them in ceres maize and compared their performances from the individual methods we employed weighted and arithmetic averaging to ensemble the coefficients section 2 3 5 in the weighted averaging method weights were assigned to the parameters based on the distance between the predicted and observed variables that they mostly influenced e g adap mdap and hwam in the initial calibration i e using only 2017 data ensembled coefficients performed poorly especially for mdap and yield as did individual methods with the re calibration using 2017 and 2018 data their performance improved substantially outperforming some of the methods tables 7 and 9 overall nmcga sd performed best in predicting yield d index 0 97 table 8 however weighted averaging performed relatively better in predicting phenology anthesis and maturity and comparable in predicting yield table 8 arithmetic averaging of the coefficients also performed well these results corroborate the value of using multiple methods e g vrugt and robinson 2007 in crop model calibration and ensembling the derived parameters for better performance 3 2 evaluation the calibrated crop genetic coefficients were evaluated for two years 2017 and 2018 at the six rainfed locations washtenaw allegan ingham saginaw huron and montcalm see table 1 phenology anthesis and maturity results showed that the model could perform well with d index varying from 0 84 to 0 95 for all methods table 10 however yields were substantially under predicted with high mbe negative and rmse and low d index table 10 there could be other factors that affected this yield performance we reviewed the literature and found some related works pointing out about the adaptive capacity of the crops to their environments if they were source limited such as root growth sharp and davies 1985 lorens et al 1987 vamerali et al 2003 hund et al 2009 although dssat simulates the root dynamics of the plant there are exogenous factors that are set by users that can possibly restrict that growth simulated by the model this must be evaluated because we are modeling a real world system 3 2 1 soil root growth factor adjustment soil root growth factor srgf is a soil input parameter in dssat which controls the maximum rooting depth and root mass distribution with depth in the soil profile jones and kiniry 1986 the root growth distribution function in ceres maize can be calibrated for different soil types as root hospitality factor which gives flexibility to root growth in the model according to soil water availability and structure friasse et al 2001 in dssat v4 6 srgf with a value of 1 0 allows the root to grow equally in the soil layer and gradually decreases to zero in tapered form through the deeper layer yang et al 2017 table 11 shows the srgf vertical distributions for the sandy loam and loam soils used in the calibration irrigated locations ibpt910006 and mskb890006 in dssat v4 6 soil sol respectively and evaluation rainfed locations the srgf distributions for the irrigated conditions were adopted from these two soil types listed in dssat v4 6 soil database hoogenboom et al 2013 the final srgf distributions used for evaluation rainfed as shown in the table did not come from dssat sbuild program their determination is discussed below based on a loam soil profile developed for iowa u s a see dssat v4 6 soil sol iubf970211 by ritchie hoogenboom et al 2013 roots were allowed to fully grow by keeping srgf 1 0 until the soil depth of 90 cm then slowly tapering down until the depth of 190 cm where srgf 0 for the sandy loam soil ibpt910006 dssat v4 6 we revised the srgf parameters by relaxing srgf to 1 0 until 40 cm depth then slowly tapering down to 0 2 until 140 cm table 11 for the loam soil mskb890006 dssat v4 6 srgf values were relaxed to 1 0 until 89 cm then slowly tapering down to 0 2 until 160 cm in addition to modifying srgf for rainfed condition we extended the maximum soil depth to 215 cm the srgf used in irrigated conditions old soil profile structure when applied to rainfed conditions could restrict the crop from exploring available resources from the soil note that the srgf in dssat sbuild is static varella et al 2012 explained that uncertainties in soil input parameters can influence model performances water and nutrient availability in the root zone have a substantial impact on root geometry dynamics and physiology therefore influencing plant water and nutrient uptakes and yields ritchie et al 1998 ma et al 2006 maize root grows deeper in the soil to extract more water per unit length of root in rainfed than in irrigated conditions sharp and davies 1985 these root dynamics are more prevalent in the event of stress during critical periods e g tasselling to grain filling in maize lorens et al 1987 vamerali et al 2003 hund et al 2009 lenka et al 2009 panda et al 2004 and djaman and irmak 2012 noted that in well irrigated conditions root water extraction mostly takes place from the top soil layers based on our literature review and multiple model iterations we suggest that the srgf in the soil input file may be restricting the roots to grow deeper when applied under water stressed conditions e g lópez cedrón et al 2008 yang et al 2009 e g in this case study the revised soil profiles were used in the model evaluation and ceres maize yield predictions substantially improved table 12 we analyzed the new evaluation results to test our hypothesis about the adaptive capacity of the plant under stressed environments that the static soil file input parameters may restrict root growth fig 3 a suggests yield improvement from 4058 kg ha 1 with the old soil profile used for calibration to 11 137 kg ha 1 with the revised one used for evaluation which is close to the 10 984 kg ha 1 observed yield the improvement in yield prediction after modifying srgf suggests that soil profile data need careful consideration when applying the model to simulate hybrid maize under rainfed conditions the plant s feedback or response mechanisms to water stressed environment e g root expansion and lengthening might not be accounted for by the model particularly being restricted by the soil capacity set up by the user to allow roots to wander deeper and wider in the rootzone in dssat sbuild srgf is currently static and should be replaced by a dynamic rooting distribution for different soils and crops such as the one developed by jones et al 1991 here the observed yields are our gold standard data observed by standard agronomic protocol and matching them with the model outputs together with observed phenology supports our attempt to adjust a static srgf profile to get a better fit of the data physiologically maximum root growth occurs during pre silking period to provide enough water for crop growth and development liu et al 2017 yang et al 2017 in the revised soil profile root ceased to grow after silking 78 dap which is typically 5 7 days after tasselling fig 3a c which helps in the reduction of water stress during tassel initiation to early grain filling period e g lizaso et al 2018 the root length density for each soil layer is more uniform in the case of the revised soil profile fig 3b and c the root layer density suggests that the deeper root layers 8 and 9 are able to extract water and nutrients during the peak demand period of crop growth fig 3c this is restricted in the old soil profile configuration fig 3b as roots in soil layers 7 and 8 grew only after tasselling and silking and did not grow deep enough to support the peak demands of water and nutrient in the growing season fig 3a and b one of the plausible reasons could be that the grain number function and grain growth rate have too much water stress effect built in more typically the crop should have set a full grain number thus no assimilate left over for the roots which probably what happened on the revised soil profile case that minimized water stress the srgf values used for the rainfed treatments can be used for the irrigated as well so we also performed simulations under irrigation with the revised soil profile to evaluate if relaxing srgf contributed to yield improvement when water is readily available we found that there was no substantial difference in predicted yields under irrigated conditions before and after relaxing srgf values fig 4a root length density in the deeper layers however became more uniform after tasselling after relaxing srgf values fig 4c while the deeper roots in layers 9 and 10 continued growing after silking with the older soil profile but that did not contribute much to the yield dynamics as sufficient water was already provided by irrigation fig 4a and b the revised soil profiles were used in the evaluation of the derived genetic coefficients for all rainfed locations in 2017 and 2018 sections 3 2 2 and 3 2 3 show the performance of phenology and yield predictions with the revised soil profile configurations 3 2 2 anthesis and maturity evaluation results in the model evaluation predictions of anthesis adap and maturity dates mdap did not change substantially when using the old table 10 or new soil profiles table 12 for all the methods anthesis dates were slightly underpredicted but with high d index table 12 it suggests that the calibrated p1 and p2 coefficients performed well in predicting anthesis at the rainfed locations overpredictions of maturity dates by 3 days glue and nmcg sd to a week gencalc and underprediction by 4 days nmcga no sd were also observed table 12 ensembling methods performed better than individual methods in predicting maturity dates table 12 coefficients of determination r2 in fig 5 also show that the model could predict anthesis with high accuracy overlapping phenological data in some locations can be observed in the case of saginaw in2018 and huron in 2017 and is mainly due to the inter annual variability of crop response to planting dates and weather patterns ensembling methods have the highest r2 values for mdap prediction fig 5 3 2 3 yield evaluation results tables 10 and 12 suggest that majority of the impacts of enhancing root dynamics is accounted for by the improvements in yields and not on phenology see section 3 2 2 the yield d index for all methods improved from 0 3 table 10 to 0 9 table 12 and r2 0 8 fig 5 yields were still overpredicted with rmse ranging from 536 to 774 kg ha 1 table 12 allowing the root system to explore more resources allowed ceres maize to match better the observed yields under rainfed conditions which as mentioned above is our gold standard data to fitting the model this result suggests the intrinsic limitation of a rigid soil based root growth factor srgf i e not dynamic if one assumes it blindly and does not account for the adaptive capacity of the crops and of the unique characteristics of the soil when setting up a soil profile data under water stress conditions 4 summary and conclusions in this study we employed existing gencalc glue and new methods nmcga sd and nmcga no sd of crop model calibrations ensembled them to get robust estimates of a hybrid maize genetic coefficients in michigan genetic coefficients of ceres maize were calibrated for a hybrid using three irrigated locations branch cass and mason during 2017 and 2018 growing seasons results suggest that using two years 2017 2018 data for calibration gave better results than by using one year data alone 2017 the phenological parameters were mostly affected by the inclusion of 2018 data in the calibration the calibrated coefficients were used to evaluate ceres maize performance across six rainfed locations washtenaw allegan ingham saginaw huron and montcalm during 2017 and 2018 growing seasons the crop genetic coefficients as used in ceres maize were able to produce potential yields that are higher than observed yields suggesting that the model can generate those levels of yields if applied in real world conditions under optimal management however under stressed environments evaluation results suggest that there are mechanisms that are not accounted for or permitted by the irrigated model setup used in calibration for a crop grown under rainfed conditions particularly on root dynamics based on literature reviews and above hypothesis we relaxed the capacity of the soil to allow the root geometry to change from the irrigated setup which resulted in a better model fit using similar soil file inputs in 2017 and 2018 the calibrated crop genetic coefficients with revised soil input parameters performed well under rainfed conditions the revised soil file inputs were evaluated under irrigated conditions with nearly similar results as the old soil file inputs suggesting that the root system did not change substantially between the two soil file inputs when the crops were irrigated this result is possibly due to the abundance of water in the upper rootzone which indicates that there should be one soil input file and rooting behaviour that apply for both irrigated and rainfed crops our results suggest that the calibration of genetic coefficients benefits greatly from multiple seasons of data and on the ensembling of calibrated genetic coefficients from different methods than using only one method the purpose of ensembling genetic coefficients from several methods was to integrate the strengths of the methods with the goal of achieving a set of genetic coefficients that is robust and resistant we found that ensembling coefficients by means of weighted averaging outperformed most of the other ensembling and calibration methods declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this manuscript acknowledgment this work was partly funded by the corn marketing program of michigan cmpm and msu agbioresearch usa we acknowledge chubu university japan listenfield japan usaid philippines and nasa servir usa for partly funding the first author s research work at msu we also acknowledge bill widdicombe for the michigan maize performance trial mmpt experiments and katlin fusilier for collecting specific field data we thank the editors and reviewers for helping us improve the quality of the work 
25908,loadest is a program for estimating constituent loads in rivers and streams developed by the u s geological survey usgs but it does not have a graphical user interface gui that facilitates processing of large amounts of data therefore we present the load estimation loadest parallel data processing interface lpdpi lpdpi is unique as it features an easy to use workflow for data download and water quality estimations for numerous stations and multiple constituents and is readily applicable to any station with both flow and water quality data available lpdpi incorporates a parallel module for faster load estimation and can identify and fix errors that occur while running loadest by adjusting calibration and estimation data inputs lpdpi also includes an extension to extract and filter loadest output to facilitate further data analysis and use of the data to calibrate hydrologic models the tool is a standalone executable for windows and can be readily used without any additional packages or software installation keywords water quality pyqt5 graphic user interface massive data processing loadest 1 introduction water quality samples are usually collected less frequently than flow because they are much more costly and labor intensive to collect and analyze therefore various sampling strategies based on flow time or flow and time have been adopted to record water quality king and harmel 2003 brigham et al 2009 the most commonly used approaches include fixed frequency sampling strategies and stratified sampling strategies the former involves collecting samples with equal time intervals e g weekly biweekly or monthly valero 2012 sanders et al 2013 halliday et al 2014 while for the latter sampling is scheduled based on flow volume the water samples are usually measured in concentration but it is not reasonable to utilize these discrete records in time series to summarize daily monthly or annual load by a straightforward calculation because these samples are not consecutive or associated with the date range of flow data however these summarized loads are required to calibrate hydrologic models from the concentrations of discrete samples subsequently water quality for days when no samples were collected have to be estimated robertson and roerish 1999 load estimator loadest was developed by the united states geological survey usgs for estimating pollutant loads through regression models using streamflow and water quality concentration data runkel et al 2004 loadest is used to predict the concentrations and loads of water quality constituents on days without water quality observations so that the water quality data are sufficient to summarize annual pollutant load estimation runkel et al 2004 park and engel 2016 loadest offers three statistical methods adjusted maximum likelihood estimation amle maximum likelihood estimation mle and least absolute deviation lad for estimating the regression model coefficients during calibration the amle and mle methods are suitable when the calibration model error or residuals are normally distributed and lad is usually adopted when the errors are randomly distributed powell 1984 loadest has been used to estimate daily pollutant loads for several water quality parameters with various sample sizes or sampling strategies sprague and gronberg 2012 jones and schilling 2013 for instance loadest has been taken as an effective tool to estimate mercury brigham et al 2009 suspended sediment dornblaser and striegl 2009 and total nitrogen and phosphorus duan et al 2012 loadest has also been used to predict nutrient loads along with season ahead climate forecasts oh and sankarasubramanian 2011 loadest is an efficient software for estimating pollutant loads and requires only streamflow and water quality data as model inputs however loadest often requires significant effort to prepare the model inputs the most difficult steps typically are the editing and formatting of input files and finding the ideal time period or flow data range to correctly run the model without errors the more monitoring points are involved in the application the more effort is required for set up and calibration irregularity of available date ranges in various constituents also increases manual correction work when preparing input data for loadest a web based interface tool was developed by park et al 2015 to estimate pollutant loads associated with flow and provide flow and water quality data retrieval although the interface facilitates the use of loadest its applicability is limited most notably the interface does not have the capability to process large amounts of data from many stations and for multiple constituents in parallel and is not capable of fixing common errors that occur when running the model there is still a high level of expertise required for users to complete the preparation of data and the configuration and execution of loadest consequently there is a need for a comprehensive interface that handles the downloading preparing and formatting of input data and fixes errors while running loadest the conservation effects assessment project ceap is a national multiagency effort to quantify the environmental benefits of conservation practices across the conterminous united states us one critical task of this project is to use hydrologic models to scientifically assess the effects and benefits of conservation practices at national scale before creating reliable assessments from these hydrologic models these models have to be calibrated in flow and various constituents with observations from gages there are more than 6000 usgs gage stations collected in this national assessment involving flow and multiple constituents such as sediment total nitrogen n organic n ammonia nitrite and nitrate total phosphorus p and soluble p due to the higher probability of irregularity in terms of date ranges of various constituents it means 48 000 individual loadest models need to be built to fully take advantage of the data available performing this manually is intractable in which the main steps of data processing include downloading data formatting inputs and running models and collecting outputs it will be even harder and longer in this procession to fix common errors that occur when running the loadest model to enable a much wider and easier use of loadest we developed a graphical user interface gui called loadest parallel data processing interface lpdpi the interface is useful for preparing data inputs for loadest for large amounts of streamflow and water quality data lpdpi is able to run loadest in parallel and fix common errors that often occur when running the model a review of the tools in developing lpdpi is presented next this covers languages and associated packages as well as brief introduction on installation in section 3 software design and philosophy are first described and then presents how lpdpi works to facilitate the data processing for water quality section 4 presents a case study section 5 concludes with a conclusion and outlining future work 2 interface language and installation lpdpi was written in python 3 7 1 using several python packages mainly pp https www parallelpython com pandas https pandas pydata org numpy van der walt et al 2011 climata https pypi org project climata and the pyqt5 application framework pyqt5 is a comprehensive set of python bindings for the qt cross platform user interface and application toolkit it enables python to be used as an alternative application development language to c on all supported platforms including windows and other operation systems graphical user interfaces gui designed by pyqt5 have assisted in connecting the user interface with the supporting python scripts to ease model application for new model users the core loadest model in lpdpi acts as a stand alone executable that is included with the lpdpi software lpdpi is a stand alone tool for the 32 bit or 64 bit versions of windows which can be installed on most computers without any additional software needed 3 main functions in lpdpi the lpdpi workflow is intended to guide loadest users through all the required steps from obtaining data and preparing it for loadest to running the model and extracting results a key motivation in the development of lpdpi was the construction of a workflow that allows users to process massive amounts of data quickly and efficiently with a parallel function so all available information and data can be used maximally the interface is geared specifically to massive data processing needs the required inputs are usgs gage station numbers streamflow or water quality time period and the targeted constituents of water quality the general organization of lpdpi is illustrated in fig 1 based on the station list lpdpi downloads all needed data flow and water quality from the usgs website creates all required input files for running loadest to create scenarios based on the regression models and load constituents that users select a unique scenario is defined in lpdpi for each given station with a unique combination of regression models and load constituents the major reason for creating different scenarios for each station is that the availability of valid records for the different constituents may vary in addition some regression models may be inappropriate for the available water quality samples at a given station the definition of scenarios will enable users to take full advantage of all information available in the calibration and estimation data during load estimation lpdpi runs all scenarios selected by the user checks the status of initial results reruns loadest to fix errors reported in scenarios and extracts results from the model output files lpdpi is designed with a top situated tab structure where users can visit each tab from left to right to complete the setup and running of lpdpi fig 2 3 1 download data the first step in the lpdpi workflow is to download flow and water quality data for usgs streamflow gages the user must create two text files listing the usgs gage numbers and the corresponding state abbreviations two letters to download data one for streamflow and one for water quality fig 2 there are two types of data available from usgs daily time series and field measurement field lab samples data for flow daily time series is defined as official daily data values for the period of record and field measurement data means list of all streamflow measurements made at the site regarding water quality data daily time series is official daily values for the period of record and field measurement data field lab samples is defined as all water quality data taken from the site the detailed information can be referred to the official usgs website https help waterdata usgs gov tutorials overview navigating usgs water data for the nation therefore the user must specify which type to download per own requirements furthermore the user can choose to download all available data or specify the time period of interest if all available data is selected for daily data all available data from usgs will be downloaded for the stations included in the station list for water quality data the user can additionally decide whether to download all available constituents or whether to select specific ones from a list provided by the interface an empty file will be created if there is no data available for the stations listed finally the user must provide the directories for the usgs gage station list and for saving the data 3 2 format data for loadest in the first section of this tab read data the user must specify which data source daily time series or field measurements of the downloaded streamflow and water quality data to use for the loadest runs fig 3 by default lpdpi assumes that daily time series of flow will be read in whereas water quality data is assumed to come from less frequent field measurements if there is no usgs daily timeseries available for flow preprocessing of the flow data from alternative field measurements is required to convert it into a format acceptable for building loadest input files in the next section create loadest input files the user must provide the information needed for lpdpi to create the loadest input files i e a calibration file containing the data used for calibrating the regression model an estimation file containing the data for which loads are to be estimated a header file listing the constituents the regression model and load estimation options and a control file listing the names of the calibration estimation and header files if a usgs gage station only has water quality data but users still want to use this station in loadest they can use flow data from a different station e g one that is nearby to build a scenario using a customized lookup table a text file listing the water quality station number and the flow station number separated by a comma there are 11 models i e regression functions available in loadest that the user can select by entering values from 1 to 11 if the user enters 0 loadest will automatically select the best model out of models 1 9 models 10 and 11 use dummy explanatory variables to handle abrupt seasonal changes in some constituents helsel and hirsch 2002 judge et al 1985 the user must specify the months for which to include the dummy variable in the regression model detailed information about each model can be found in the loadest manual runkel et al 2004 the purpose of the header files is to set output options for loadest including print options of estimation values standard error options load output options and concentration and load unit settings user defined seasons can be defined if users select user defined seasons from the load output options drop down menu the units of concentration and load inputs must be defined in a pop up sub window after these initial settings all files required for running loadest will be ready if the user forgot to set the concentration and load units it will be highlighted in red the normal maximum time needed for successfully running loadest can be roughly determined by running the model for a station with a single constituent and the longest possible estimation and calibration data sets in our case it took about up to 2 min to finish a run in some cases loadest is unable to obtain meaningful results from the calibration and estimation data available and cannot finish the run to avoid this the user can define a time limit for completing a single job if a scenario runs beyond this limit lpdpi will force it to stop and report an error for running loadest in parallel mode lpdpi lets the user define the number of threads to be used fig 3 for a computer with four physical cores eight logical processors can be used to run multiple processes at the same time theoretically eight tasks can be working at the same time but users can assign more than eight tasks to run loadest in parallel by setting the number of threads in lpdpi 3 3 run and check initial results in the run loadest tab lpdpi provides the option to check if all parameters have been set correctly before starting to run loadest if loadest is run in parallel mode with erroneous settings the computer may crash due to too many threads working improperly which will require extensive manual work to fix after the parallel data processing lpdpi has a function to check the status of the loadest run check running results it will summarize the total number of scenarios that were run successfully or unsuccessfully and report a summary of scenarios and their respective error types which can be exported as a text file for the scenarios with errors reported users can adjust inputs in the next tab to take full advantage of all data information 3 4 rerun loadest in the rerun loadest tab the user can adjust the inputs to fix errors that commonly occur when running loadest many users reported that loadest was unable to finish a run https water usgs gov software loadest faq extrapolate this error is usually caused by one of two issues the first issue occurs when the estimation file contains stream flows that are much lower or higher than the lowest or highest streamflow in the calibration data set the second issue occurs when the estimation file includes time periods that are too long before or too long after the earliest or latest observation in the calibration data set if the estimation dataset includes time periods that are too long before or too long after the earliest or latest observation in the calibration data set loadest will show either error in function expon failure to converge or error in function expvar failure to converge this issue can be fixed by limiting the time period of flow records in the estimation data set to be closer to that in the calibration data set see cutting periods in section 3 4 1 since water quality data is usually monitored less frequently than streamflow there can be significantly less water quality data than flow data available for some stations in addition water quality data should include an appropriate number of water quality samples specifically from storm events rather than just a fixed number of water quality samples much of the constituent load is transported during these events and it is important that the constituent concentration during them is properly estimated in other words an appropriate water quality sampling strategy that targets some high flow events is required to accurately estimate annual pollutant loads using approaches like loadest park and engel 2014 if no water quality samples have been collected during large storm events the observed data cannot be used in loadest to simulate the load for the whole range of flows during the estimation period however the data may still be used for estimations during storm events of similar magnitude as the ones included in the calibration dataset thus this type of error can be partly resolved by removing the flow records that are much lower or much higher than the lowest or highest flow in the calibration data set see cutting values in section 3 4 2 to avoid re running successful scenarios lpdpi provides an option to get a list of scenarios with errors from the first loadest run get from the first run which will be posted in the scenario list needing to be checked field there are three options for fixing errors in lpdpi cut periods limit ranges and limit ranges and then cut periods it can take several iterations using the three options with different thresholds to fix the errors after each run with one of the three options the status of these scenarios can be checked by check running results after updating the scenario list by clicking update list the scenarios that were still unsuccessful can be updated for further processing by extract and will be posted in the list of still unsuccessful scenarios field in the check running status section and in the scenario list in the following section rerun loadest fig 4 before any input data is modified for rerunning loadest all data should be backed up in a separate file lpdpi has a button to allow easy backup of all data lpdpi has the capability to re run scenarios in parallel it is recommended to perform a single scenario test to make sure all settings are appropriate before running scenarios in parallel 3 4 1 cut periods the function of cutting periods in lpdpi was designed for automatically shrinking data by iterations from both ends of the estimation time period while minimizing the loss of information in the estimation fig 5 it is designed to avoid extreme extrapolation of data by loadest this function can be useful when the calibration period is significantly shorter than the estimation period e g if the calibration data spans from 2010 to 2015 but the estimation period extends from 1960 to 2018 if the calibration data set has a linear increasing trend e g a positive coefficient for the independent variable of decimal time models 3 5 and 7 will predict negative loads for days prior to the calibration period however models 1 2 4 and 6 may produce reasonable results as they use the sine and cosine of decimal time in some cases loadest will run very slowly and even if it finishes eventually the results are likely incorrect or meaningless to resolve this problem it is necessary to adjust the estimation period so it does not include dates too long before or too long after the earliest or latest observation in the calibration data set lpdpi provides the option to select an automatic setting or a customized setting for cutting periods fig 4 when the customized setting is selected the user can specify the percentage of days to cut from the estimation file before the start date in the calibration file in each iteration a in fig 5 and how many percent of the days in the estimation file after the end date in the calibration file to cut in each iteration b in fig 5 even if the calibration and estimation time periods are the same loadest will still not be able to run correctly if there are too few data records in the calibration data set there are two options to fix this problem first if loadest still reports errors after 10 iterations lpdpi can try to only include months in the estimation that have observed records in the calibration file c in fig 5 second if loadest is still unable to run the scenario successfully one year will be cut from the start and end of the estimation time period in each additional iteration d in fig 5 when the automatic setting is selected a and b are set to 50 and c and d are checked 3 4 2 limit flow ranges the function of limiting flow range values in the estimation data set was designed to exclude values that are significantly outside the range of the calibration data set loadest may report errors when the estimation file contains flows that are much lower or much higher than the lowest or highest flow in the calibration data set e g if the calibration data set has several minimum flow records of less than 2 cfs 0 06 m3 s while most flows in the estimation data are more than 2 cfs 0 06 m3 s in this case low flows may not be appropriately extrapolated this error can be fixed by eliminating the low flow values from the estimation data set figs 4 and 6 however it is important to note that this may result in inaccurate load estimations if the minimum observed flow rate in the calibration data set is very low the error may be negligible but in other cases it may significantly impact estimated loads when the high flows in the estimation data set are significantly higher than the highest flow in the calibration data set cutting these higher values in estimation will lead to a substantial underestimation of loads during these events however it is reasonable to use this function if the months or years with flows that are higher than the highest flow in the calibration data set are not considered in specific statistical analysis there are three options for this function in lpdpi to remove records figs 4 and 6 with a flow rate lower than 0 1 cfs 0 0028 m3 s from the calibration dataset flowcal in fig 6 from the estimation dataset based on a user defined ratio of maximum flow in the estimation dataset to maximum flow in the calibration dataset fmaxest in fig 6 and from the estimation dataset based on a user defined ratio of minimum flow in the estimation dataset to minimum flow in the calibration dataset fminest in fig 6 the first option was included in lpdpi because in some cases loadest is unable to run successfully when there are flows lower than 0 1 cfs 0 0028 m3 s in the calibration data set the other two options set by lpdpi are the maximum and minimum flow thresholds fmaxest and fminest mentioned above allowed in estimation data set if the automatic setting is selected flowcal will be checked and both fmaxest and fminest will be set to 1 0 3 4 3 limit flow ranges and then cut periods if there are still unsuccessful scenarios after cutting the values or the periods the option to limit ranges and then cut periods helps users to combine both methods of modifying the calibration and estimation data 3 5 extract outputs lpdpi creates an individual folder for each loadest scenario users can select all scenarios or a customized list for extracting data fig 7 lpdpi also offers the option to print a list of scenarios that did not have enough data to successfully run loadest in a separate file lpdpi will extract loads concentrations and streamflow parameters from loadest output files as listed in table 1 due to the natural variability in streamflow and water quality there will most likely always be some scenarios that do not result in satisfactory estimations even if the loadest runs were successful lpdpi provides a filtering function to extract the scenarios meeting user defined requirements in the current lpdpi version five statistical parameters pbias in load estimation ratio of maximum simulated and observed load nash sutcliffe efficiency of load estimation r squared of load estimation and significant level of ppcc test can be used as criteria to identify satisfactory scenarios in addition the final estimations of loads from loadest can be printed at three different time steps daily monthly and yearly 4 application of lpdpi for the purpose of demonstration in this paper lpdpi was applied to a 4 digit hydrologic unit code huc4 watershed usgs 0512 there were data for 106 flow gage stations and 289 water quality stations available to download from the usgs the time period chosen for this application was january 1 1960 to december 31 2018 three constituents p80154 suspended sediment concentration p00666 filtered phosphorus p00600 total nitrogen were selected as examples after extracting and formatting data by lpdpi 81 stations were identified as suitable for use in loadest 12 or more nonzero observations are required 7 or more of which must be uncensored but foor many of them there was only one constituent available fig 8 only one regression model the one that lets loadest pick one with the best statistical performance was used and 111 scenarios were created 20 for total nitrogen 20 for filtered phosphorus and 71 for suspended sediment concentration in the first iteration estimations were successfully made for 99 scenarios accordingly there were some scenarios that could potentially be fixed by the re run function in lpdpi all 12 scenarios 10 8 were fixed by cutting the time period with an automatic setting of thresholds so the other two functions limit ranges and then cut periods and limit ranges only in lpdpi were not needed in this application the date coverages in calibration and raw and final estimations in these 12 scenarios are presented in fig 9 it shows that there is a significantly longer period in the raw flow estimation than in the final flow estimation that is the reason why there were errors reported from these scenarios three scenarios 1 5 and 11 have very decent estimation periods about 31 25 and 24 years that are long enough for validating various hydrologic models this function from lpdpi will help users to find an appropriate estimation period extending a reasonable length via current trends in calibration for instance the calibration period of usgs gage 03342300 scenario 6 in fig 9 ranged from october 04 1977 to december 11 1981 while the original estimation data retrieved from the usgs extended from june 1 1966 to september 30 1986 after the initial loadest run an error was reported error in subroutine mvuephi argument aw is too large after re running using the cut periods only function in lpdpi the loadest calculations for this station were successfully executed with an optimized estimation period spanning from february 1 1972 to may 6 1984 in some scenarios the date coverage in estimation could be even shorter than that in calibration which was not found in our case study the reason is that there are fewer data records in calibration causing further cutting period in estimation till a successful loadest run is obtained this adjustment not only enables loadest to work correctly by appropriately cutting the estimation period but also results in a minimal reduction of estimation data thereby keeping the largest possible amount of information this feature can be very useful for users who need a long estimation period for calibrating hydrologic models to simulate water quality in lpdpi much time may be spent on iterations where estimation periods are revisited and adjusted followed by inspection of model running status once model output for a scenario is created correctly it will stop and go to the next scenario the built in checking result status designs in lpdpi are accessed through a table display that presently includes an altering scenario count with the type of errors and a specific error description the rerunning function in lpdpi is not limited to fixing loadest models created by lpdpi users may choose to add one s own scenarios list to rerun the model to find the optimized solutions to demonstrate the improved performance in handling massive load data by lpdp a comparison of time spent on data processing without lpdpi was carried out table 2 there are apparently difference for various users to process data for loadest so it was assumed that a user who was very familiar with loadest was working on whole steps to prepare data and run loadest there are comparisons with and without lpdpi in four steps including downloading data formatting data parallelly running data and rerunning scenarios there are 403 gages available from usgs which are related to flow and three constituents usually users need to find gages on the usgs website set up the data format and download data the total time spent was roughly calculated by 1 min for flow and water quality from one gage consequently it will need 403 min to collect flow and water quality data however lpdpi only spent 17 min to download all data from these usgs gages including 106 gages with flow data and 400 gages with water quality data almost 24 times faster than regular approaches to get these data ready table 2 the most time consuming step is to extract flow data and each constituent data find the matched records for each intended constituent with the existing flow records for calibration and then prepare the separated flow data for estimation last thing is to set up the header and control files for each loadest model it is assuming that the user can quickly and easily find the right data column and join the flow and water quality data we tried our best to do this processing finding out at least 2 min for a gage with consecutive time series of flow and water quality data since all three constituents are not always available for each gage it results in 111 total scenarios unique combination of gages and constituents created by lpdpi so the same scenarios were calculated in the absence of lpdpi and total time for all scenarios in the second step will need at least 222 min table 2 there is a function reading the data and formatting data for loadest in lpdpi which needs 2 min to finish all processing after having the paths filled it saves about 3 7 h 111 times faster to prepare data for a huc4 watershed if a study focuses on several huc4s it will facilitate them to get these data substantially faster than regular methods parallel running is another novel approach for handling data for loadest in lpdpi as we know in some cases loadest will run for a very long time running without finishing the estimation process in some cases like 12 scenarios mentioned before therefore approx 6 000 gages involved in our whole project were tested to find the maximum time needed for a scenario that is able to run successfully in a rational time period it was found that the longest scenario with a successful running needs about 2 min therefore loadest will be forced to suspend by lpdpi if it is running longer than 2 min regarding threads in parallel running more than 100 threads can be deployed with four cores configured in a regular computer in this case application 40 threads were tested resulting in 2 min for finishing 111 scenarios manually it will take 27 75 min with each consuming 15 s which includes locating the loadest project path and running it regardless of the intervals between two running scenarios table 2 the time to be consumed by lpdpi can be reduced further if more threads were deployed even in the current setting it is almost 14 times faster than running loadest manually with regard to the scenarios that have errors occurred or non stop running lpdpi provides a tool to fix them by rerun function which is to iteratively reduce the period of estimation or limit the flow range in estimation there are 41 iterations ranging from 1 to 11 in these 12 scenarios to fix errors three iterations were assumed to be taken in the absence of lpdpi in each iteration 30 s are usually needed because both the start and the end in estimation need to be reduced and at least 15 s are required for finishing loadest running accordingly the total time consumed is about 18 min while lpdpi only needs about 40 s for this process table 2 it is 26 times faster than manual rerunning and will save much more time if there are a lot of scenarios to be fixed the final date coverages of the 111 scenarios for calibration and final estimation in this case were presented in fig 10 for most scenarios there are longer date coverages in estimation than calibration some of which even cover the whole period in raw flow data from january 1 1960 to december 31 2018 at least lpdpi is able to cover the period in calibration helping to interpolate the discrete water quality records into time series of data for reasonably summarizing yearly or monthly metrics on water quality the final step is to extract statistical results and estimated loads at three temporal scales lpdpi also provides a function to collect all statistical metrics and selected equation model number by loadest in this case statistical metrics pbias nash sutcliffe efficiency nse and model flag equation number in loadest of regression models in these 111 scenarios were extracted as shown in fig 11 pbias 25 25 and nash sutcliffe efficiency 0 6 1 0 were chosen as criteria to extract scenarios that meet the requirements the final selection of three constituents included 70 scenarios 18 for total nitrogen 15 for filtered phosphorus and 37 for suspended sediment concentration it indicated that 63 of total scenarios 70 out of 111 have a good performance in estimating load by loadest in these scenarios the preferred equations were displayed by their count frequency in fig 12 model numbers in x axis are the same with numbers mentioned in loadest manual https water usgs gov software loadest doc regarding suspended sediment equations 9 7 and 8 are more popular to be the ideal models for estimating loads in loadest equations 6 8 and 9 are the best choices in simulating nitrogen load it will be better to choose equations 9 8 and 4 for filtered phosphorus estimation although these conclusions were drawn from only one huc4 watershed they suggest that each equation has its own advantage of estimating various constituents finally the estimations in these scenarios can be extracted in individual files at daily monthly and yearly time step in lpdpi which can subsequently be used for calibrating hydrologic models or performing further analysis in this case study we developed average annual nutrient and sediment loads for the calibration of a soil and water assessment tools swat model built in this huc4 watershed usgs 0512 5 conclusions and future work lpdpi is a useful tool for researchers or students who need to estimate loads at a large number of sites but do not have the efforts or time required to develop massive data processing programs and scripts lpdpi provides an easy to use workflow for users to automatically download and prepare inputs files for loadest the ability of lpdpi to rerun scenarios that otherwise fail with reduced data allows users to take full advantage of existing data and information available for the load estimation the option to run loadest in parallel across many cpu cores can save time when processing large amounts of data which is often required for calibration of large scale hydrologic models future plans for lpdpi development include focusing on gauge station selection through an interface via hydrologic unit or administrative boundaries software availability name of software lpdpi loadest parallel data processing interface developers jungang gao michael j white katrin bieger and jeffrey g arnold contact address blackland research extension center texas a m agrilife 702 e blackland road temple tx 76502 usa email jgao brc tamus edu availability https github com johng2014 lpdpi 1 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements usda is an equal opportunity employer and provider we highly appreciate the editor and three anonymous reviewers for their constructive comments that were very helpful for improving the manuscript this research was funded with a cooperative agreement fund by usda ars united states with nrcs united states agreement number 58 3098 8 002 
25908,loadest is a program for estimating constituent loads in rivers and streams developed by the u s geological survey usgs but it does not have a graphical user interface gui that facilitates processing of large amounts of data therefore we present the load estimation loadest parallel data processing interface lpdpi lpdpi is unique as it features an easy to use workflow for data download and water quality estimations for numerous stations and multiple constituents and is readily applicable to any station with both flow and water quality data available lpdpi incorporates a parallel module for faster load estimation and can identify and fix errors that occur while running loadest by adjusting calibration and estimation data inputs lpdpi also includes an extension to extract and filter loadest output to facilitate further data analysis and use of the data to calibrate hydrologic models the tool is a standalone executable for windows and can be readily used without any additional packages or software installation keywords water quality pyqt5 graphic user interface massive data processing loadest 1 introduction water quality samples are usually collected less frequently than flow because they are much more costly and labor intensive to collect and analyze therefore various sampling strategies based on flow time or flow and time have been adopted to record water quality king and harmel 2003 brigham et al 2009 the most commonly used approaches include fixed frequency sampling strategies and stratified sampling strategies the former involves collecting samples with equal time intervals e g weekly biweekly or monthly valero 2012 sanders et al 2013 halliday et al 2014 while for the latter sampling is scheduled based on flow volume the water samples are usually measured in concentration but it is not reasonable to utilize these discrete records in time series to summarize daily monthly or annual load by a straightforward calculation because these samples are not consecutive or associated with the date range of flow data however these summarized loads are required to calibrate hydrologic models from the concentrations of discrete samples subsequently water quality for days when no samples were collected have to be estimated robertson and roerish 1999 load estimator loadest was developed by the united states geological survey usgs for estimating pollutant loads through regression models using streamflow and water quality concentration data runkel et al 2004 loadest is used to predict the concentrations and loads of water quality constituents on days without water quality observations so that the water quality data are sufficient to summarize annual pollutant load estimation runkel et al 2004 park and engel 2016 loadest offers three statistical methods adjusted maximum likelihood estimation amle maximum likelihood estimation mle and least absolute deviation lad for estimating the regression model coefficients during calibration the amle and mle methods are suitable when the calibration model error or residuals are normally distributed and lad is usually adopted when the errors are randomly distributed powell 1984 loadest has been used to estimate daily pollutant loads for several water quality parameters with various sample sizes or sampling strategies sprague and gronberg 2012 jones and schilling 2013 for instance loadest has been taken as an effective tool to estimate mercury brigham et al 2009 suspended sediment dornblaser and striegl 2009 and total nitrogen and phosphorus duan et al 2012 loadest has also been used to predict nutrient loads along with season ahead climate forecasts oh and sankarasubramanian 2011 loadest is an efficient software for estimating pollutant loads and requires only streamflow and water quality data as model inputs however loadest often requires significant effort to prepare the model inputs the most difficult steps typically are the editing and formatting of input files and finding the ideal time period or flow data range to correctly run the model without errors the more monitoring points are involved in the application the more effort is required for set up and calibration irregularity of available date ranges in various constituents also increases manual correction work when preparing input data for loadest a web based interface tool was developed by park et al 2015 to estimate pollutant loads associated with flow and provide flow and water quality data retrieval although the interface facilitates the use of loadest its applicability is limited most notably the interface does not have the capability to process large amounts of data from many stations and for multiple constituents in parallel and is not capable of fixing common errors that occur when running the model there is still a high level of expertise required for users to complete the preparation of data and the configuration and execution of loadest consequently there is a need for a comprehensive interface that handles the downloading preparing and formatting of input data and fixes errors while running loadest the conservation effects assessment project ceap is a national multiagency effort to quantify the environmental benefits of conservation practices across the conterminous united states us one critical task of this project is to use hydrologic models to scientifically assess the effects and benefits of conservation practices at national scale before creating reliable assessments from these hydrologic models these models have to be calibrated in flow and various constituents with observations from gages there are more than 6000 usgs gage stations collected in this national assessment involving flow and multiple constituents such as sediment total nitrogen n organic n ammonia nitrite and nitrate total phosphorus p and soluble p due to the higher probability of irregularity in terms of date ranges of various constituents it means 48 000 individual loadest models need to be built to fully take advantage of the data available performing this manually is intractable in which the main steps of data processing include downloading data formatting inputs and running models and collecting outputs it will be even harder and longer in this procession to fix common errors that occur when running the loadest model to enable a much wider and easier use of loadest we developed a graphical user interface gui called loadest parallel data processing interface lpdpi the interface is useful for preparing data inputs for loadest for large amounts of streamflow and water quality data lpdpi is able to run loadest in parallel and fix common errors that often occur when running the model a review of the tools in developing lpdpi is presented next this covers languages and associated packages as well as brief introduction on installation in section 3 software design and philosophy are first described and then presents how lpdpi works to facilitate the data processing for water quality section 4 presents a case study section 5 concludes with a conclusion and outlining future work 2 interface language and installation lpdpi was written in python 3 7 1 using several python packages mainly pp https www parallelpython com pandas https pandas pydata org numpy van der walt et al 2011 climata https pypi org project climata and the pyqt5 application framework pyqt5 is a comprehensive set of python bindings for the qt cross platform user interface and application toolkit it enables python to be used as an alternative application development language to c on all supported platforms including windows and other operation systems graphical user interfaces gui designed by pyqt5 have assisted in connecting the user interface with the supporting python scripts to ease model application for new model users the core loadest model in lpdpi acts as a stand alone executable that is included with the lpdpi software lpdpi is a stand alone tool for the 32 bit or 64 bit versions of windows which can be installed on most computers without any additional software needed 3 main functions in lpdpi the lpdpi workflow is intended to guide loadest users through all the required steps from obtaining data and preparing it for loadest to running the model and extracting results a key motivation in the development of lpdpi was the construction of a workflow that allows users to process massive amounts of data quickly and efficiently with a parallel function so all available information and data can be used maximally the interface is geared specifically to massive data processing needs the required inputs are usgs gage station numbers streamflow or water quality time period and the targeted constituents of water quality the general organization of lpdpi is illustrated in fig 1 based on the station list lpdpi downloads all needed data flow and water quality from the usgs website creates all required input files for running loadest to create scenarios based on the regression models and load constituents that users select a unique scenario is defined in lpdpi for each given station with a unique combination of regression models and load constituents the major reason for creating different scenarios for each station is that the availability of valid records for the different constituents may vary in addition some regression models may be inappropriate for the available water quality samples at a given station the definition of scenarios will enable users to take full advantage of all information available in the calibration and estimation data during load estimation lpdpi runs all scenarios selected by the user checks the status of initial results reruns loadest to fix errors reported in scenarios and extracts results from the model output files lpdpi is designed with a top situated tab structure where users can visit each tab from left to right to complete the setup and running of lpdpi fig 2 3 1 download data the first step in the lpdpi workflow is to download flow and water quality data for usgs streamflow gages the user must create two text files listing the usgs gage numbers and the corresponding state abbreviations two letters to download data one for streamflow and one for water quality fig 2 there are two types of data available from usgs daily time series and field measurement field lab samples data for flow daily time series is defined as official daily data values for the period of record and field measurement data means list of all streamflow measurements made at the site regarding water quality data daily time series is official daily values for the period of record and field measurement data field lab samples is defined as all water quality data taken from the site the detailed information can be referred to the official usgs website https help waterdata usgs gov tutorials overview navigating usgs water data for the nation therefore the user must specify which type to download per own requirements furthermore the user can choose to download all available data or specify the time period of interest if all available data is selected for daily data all available data from usgs will be downloaded for the stations included in the station list for water quality data the user can additionally decide whether to download all available constituents or whether to select specific ones from a list provided by the interface an empty file will be created if there is no data available for the stations listed finally the user must provide the directories for the usgs gage station list and for saving the data 3 2 format data for loadest in the first section of this tab read data the user must specify which data source daily time series or field measurements of the downloaded streamflow and water quality data to use for the loadest runs fig 3 by default lpdpi assumes that daily time series of flow will be read in whereas water quality data is assumed to come from less frequent field measurements if there is no usgs daily timeseries available for flow preprocessing of the flow data from alternative field measurements is required to convert it into a format acceptable for building loadest input files in the next section create loadest input files the user must provide the information needed for lpdpi to create the loadest input files i e a calibration file containing the data used for calibrating the regression model an estimation file containing the data for which loads are to be estimated a header file listing the constituents the regression model and load estimation options and a control file listing the names of the calibration estimation and header files if a usgs gage station only has water quality data but users still want to use this station in loadest they can use flow data from a different station e g one that is nearby to build a scenario using a customized lookup table a text file listing the water quality station number and the flow station number separated by a comma there are 11 models i e regression functions available in loadest that the user can select by entering values from 1 to 11 if the user enters 0 loadest will automatically select the best model out of models 1 9 models 10 and 11 use dummy explanatory variables to handle abrupt seasonal changes in some constituents helsel and hirsch 2002 judge et al 1985 the user must specify the months for which to include the dummy variable in the regression model detailed information about each model can be found in the loadest manual runkel et al 2004 the purpose of the header files is to set output options for loadest including print options of estimation values standard error options load output options and concentration and load unit settings user defined seasons can be defined if users select user defined seasons from the load output options drop down menu the units of concentration and load inputs must be defined in a pop up sub window after these initial settings all files required for running loadest will be ready if the user forgot to set the concentration and load units it will be highlighted in red the normal maximum time needed for successfully running loadest can be roughly determined by running the model for a station with a single constituent and the longest possible estimation and calibration data sets in our case it took about up to 2 min to finish a run in some cases loadest is unable to obtain meaningful results from the calibration and estimation data available and cannot finish the run to avoid this the user can define a time limit for completing a single job if a scenario runs beyond this limit lpdpi will force it to stop and report an error for running loadest in parallel mode lpdpi lets the user define the number of threads to be used fig 3 for a computer with four physical cores eight logical processors can be used to run multiple processes at the same time theoretically eight tasks can be working at the same time but users can assign more than eight tasks to run loadest in parallel by setting the number of threads in lpdpi 3 3 run and check initial results in the run loadest tab lpdpi provides the option to check if all parameters have been set correctly before starting to run loadest if loadest is run in parallel mode with erroneous settings the computer may crash due to too many threads working improperly which will require extensive manual work to fix after the parallel data processing lpdpi has a function to check the status of the loadest run check running results it will summarize the total number of scenarios that were run successfully or unsuccessfully and report a summary of scenarios and their respective error types which can be exported as a text file for the scenarios with errors reported users can adjust inputs in the next tab to take full advantage of all data information 3 4 rerun loadest in the rerun loadest tab the user can adjust the inputs to fix errors that commonly occur when running loadest many users reported that loadest was unable to finish a run https water usgs gov software loadest faq extrapolate this error is usually caused by one of two issues the first issue occurs when the estimation file contains stream flows that are much lower or higher than the lowest or highest streamflow in the calibration data set the second issue occurs when the estimation file includes time periods that are too long before or too long after the earliest or latest observation in the calibration data set if the estimation dataset includes time periods that are too long before or too long after the earliest or latest observation in the calibration data set loadest will show either error in function expon failure to converge or error in function expvar failure to converge this issue can be fixed by limiting the time period of flow records in the estimation data set to be closer to that in the calibration data set see cutting periods in section 3 4 1 since water quality data is usually monitored less frequently than streamflow there can be significantly less water quality data than flow data available for some stations in addition water quality data should include an appropriate number of water quality samples specifically from storm events rather than just a fixed number of water quality samples much of the constituent load is transported during these events and it is important that the constituent concentration during them is properly estimated in other words an appropriate water quality sampling strategy that targets some high flow events is required to accurately estimate annual pollutant loads using approaches like loadest park and engel 2014 if no water quality samples have been collected during large storm events the observed data cannot be used in loadest to simulate the load for the whole range of flows during the estimation period however the data may still be used for estimations during storm events of similar magnitude as the ones included in the calibration dataset thus this type of error can be partly resolved by removing the flow records that are much lower or much higher than the lowest or highest flow in the calibration data set see cutting values in section 3 4 2 to avoid re running successful scenarios lpdpi provides an option to get a list of scenarios with errors from the first loadest run get from the first run which will be posted in the scenario list needing to be checked field there are three options for fixing errors in lpdpi cut periods limit ranges and limit ranges and then cut periods it can take several iterations using the three options with different thresholds to fix the errors after each run with one of the three options the status of these scenarios can be checked by check running results after updating the scenario list by clicking update list the scenarios that were still unsuccessful can be updated for further processing by extract and will be posted in the list of still unsuccessful scenarios field in the check running status section and in the scenario list in the following section rerun loadest fig 4 before any input data is modified for rerunning loadest all data should be backed up in a separate file lpdpi has a button to allow easy backup of all data lpdpi has the capability to re run scenarios in parallel it is recommended to perform a single scenario test to make sure all settings are appropriate before running scenarios in parallel 3 4 1 cut periods the function of cutting periods in lpdpi was designed for automatically shrinking data by iterations from both ends of the estimation time period while minimizing the loss of information in the estimation fig 5 it is designed to avoid extreme extrapolation of data by loadest this function can be useful when the calibration period is significantly shorter than the estimation period e g if the calibration data spans from 2010 to 2015 but the estimation period extends from 1960 to 2018 if the calibration data set has a linear increasing trend e g a positive coefficient for the independent variable of decimal time models 3 5 and 7 will predict negative loads for days prior to the calibration period however models 1 2 4 and 6 may produce reasonable results as they use the sine and cosine of decimal time in some cases loadest will run very slowly and even if it finishes eventually the results are likely incorrect or meaningless to resolve this problem it is necessary to adjust the estimation period so it does not include dates too long before or too long after the earliest or latest observation in the calibration data set lpdpi provides the option to select an automatic setting or a customized setting for cutting periods fig 4 when the customized setting is selected the user can specify the percentage of days to cut from the estimation file before the start date in the calibration file in each iteration a in fig 5 and how many percent of the days in the estimation file after the end date in the calibration file to cut in each iteration b in fig 5 even if the calibration and estimation time periods are the same loadest will still not be able to run correctly if there are too few data records in the calibration data set there are two options to fix this problem first if loadest still reports errors after 10 iterations lpdpi can try to only include months in the estimation that have observed records in the calibration file c in fig 5 second if loadest is still unable to run the scenario successfully one year will be cut from the start and end of the estimation time period in each additional iteration d in fig 5 when the automatic setting is selected a and b are set to 50 and c and d are checked 3 4 2 limit flow ranges the function of limiting flow range values in the estimation data set was designed to exclude values that are significantly outside the range of the calibration data set loadest may report errors when the estimation file contains flows that are much lower or much higher than the lowest or highest flow in the calibration data set e g if the calibration data set has several minimum flow records of less than 2 cfs 0 06 m3 s while most flows in the estimation data are more than 2 cfs 0 06 m3 s in this case low flows may not be appropriately extrapolated this error can be fixed by eliminating the low flow values from the estimation data set figs 4 and 6 however it is important to note that this may result in inaccurate load estimations if the minimum observed flow rate in the calibration data set is very low the error may be negligible but in other cases it may significantly impact estimated loads when the high flows in the estimation data set are significantly higher than the highest flow in the calibration data set cutting these higher values in estimation will lead to a substantial underestimation of loads during these events however it is reasonable to use this function if the months or years with flows that are higher than the highest flow in the calibration data set are not considered in specific statistical analysis there are three options for this function in lpdpi to remove records figs 4 and 6 with a flow rate lower than 0 1 cfs 0 0028 m3 s from the calibration dataset flowcal in fig 6 from the estimation dataset based on a user defined ratio of maximum flow in the estimation dataset to maximum flow in the calibration dataset fmaxest in fig 6 and from the estimation dataset based on a user defined ratio of minimum flow in the estimation dataset to minimum flow in the calibration dataset fminest in fig 6 the first option was included in lpdpi because in some cases loadest is unable to run successfully when there are flows lower than 0 1 cfs 0 0028 m3 s in the calibration data set the other two options set by lpdpi are the maximum and minimum flow thresholds fmaxest and fminest mentioned above allowed in estimation data set if the automatic setting is selected flowcal will be checked and both fmaxest and fminest will be set to 1 0 3 4 3 limit flow ranges and then cut periods if there are still unsuccessful scenarios after cutting the values or the periods the option to limit ranges and then cut periods helps users to combine both methods of modifying the calibration and estimation data 3 5 extract outputs lpdpi creates an individual folder for each loadest scenario users can select all scenarios or a customized list for extracting data fig 7 lpdpi also offers the option to print a list of scenarios that did not have enough data to successfully run loadest in a separate file lpdpi will extract loads concentrations and streamflow parameters from loadest output files as listed in table 1 due to the natural variability in streamflow and water quality there will most likely always be some scenarios that do not result in satisfactory estimations even if the loadest runs were successful lpdpi provides a filtering function to extract the scenarios meeting user defined requirements in the current lpdpi version five statistical parameters pbias in load estimation ratio of maximum simulated and observed load nash sutcliffe efficiency of load estimation r squared of load estimation and significant level of ppcc test can be used as criteria to identify satisfactory scenarios in addition the final estimations of loads from loadest can be printed at three different time steps daily monthly and yearly 4 application of lpdpi for the purpose of demonstration in this paper lpdpi was applied to a 4 digit hydrologic unit code huc4 watershed usgs 0512 there were data for 106 flow gage stations and 289 water quality stations available to download from the usgs the time period chosen for this application was january 1 1960 to december 31 2018 three constituents p80154 suspended sediment concentration p00666 filtered phosphorus p00600 total nitrogen were selected as examples after extracting and formatting data by lpdpi 81 stations were identified as suitable for use in loadest 12 or more nonzero observations are required 7 or more of which must be uncensored but foor many of them there was only one constituent available fig 8 only one regression model the one that lets loadest pick one with the best statistical performance was used and 111 scenarios were created 20 for total nitrogen 20 for filtered phosphorus and 71 for suspended sediment concentration in the first iteration estimations were successfully made for 99 scenarios accordingly there were some scenarios that could potentially be fixed by the re run function in lpdpi all 12 scenarios 10 8 were fixed by cutting the time period with an automatic setting of thresholds so the other two functions limit ranges and then cut periods and limit ranges only in lpdpi were not needed in this application the date coverages in calibration and raw and final estimations in these 12 scenarios are presented in fig 9 it shows that there is a significantly longer period in the raw flow estimation than in the final flow estimation that is the reason why there were errors reported from these scenarios three scenarios 1 5 and 11 have very decent estimation periods about 31 25 and 24 years that are long enough for validating various hydrologic models this function from lpdpi will help users to find an appropriate estimation period extending a reasonable length via current trends in calibration for instance the calibration period of usgs gage 03342300 scenario 6 in fig 9 ranged from october 04 1977 to december 11 1981 while the original estimation data retrieved from the usgs extended from june 1 1966 to september 30 1986 after the initial loadest run an error was reported error in subroutine mvuephi argument aw is too large after re running using the cut periods only function in lpdpi the loadest calculations for this station were successfully executed with an optimized estimation period spanning from february 1 1972 to may 6 1984 in some scenarios the date coverage in estimation could be even shorter than that in calibration which was not found in our case study the reason is that there are fewer data records in calibration causing further cutting period in estimation till a successful loadest run is obtained this adjustment not only enables loadest to work correctly by appropriately cutting the estimation period but also results in a minimal reduction of estimation data thereby keeping the largest possible amount of information this feature can be very useful for users who need a long estimation period for calibrating hydrologic models to simulate water quality in lpdpi much time may be spent on iterations where estimation periods are revisited and adjusted followed by inspection of model running status once model output for a scenario is created correctly it will stop and go to the next scenario the built in checking result status designs in lpdpi are accessed through a table display that presently includes an altering scenario count with the type of errors and a specific error description the rerunning function in lpdpi is not limited to fixing loadest models created by lpdpi users may choose to add one s own scenarios list to rerun the model to find the optimized solutions to demonstrate the improved performance in handling massive load data by lpdp a comparison of time spent on data processing without lpdpi was carried out table 2 there are apparently difference for various users to process data for loadest so it was assumed that a user who was very familiar with loadest was working on whole steps to prepare data and run loadest there are comparisons with and without lpdpi in four steps including downloading data formatting data parallelly running data and rerunning scenarios there are 403 gages available from usgs which are related to flow and three constituents usually users need to find gages on the usgs website set up the data format and download data the total time spent was roughly calculated by 1 min for flow and water quality from one gage consequently it will need 403 min to collect flow and water quality data however lpdpi only spent 17 min to download all data from these usgs gages including 106 gages with flow data and 400 gages with water quality data almost 24 times faster than regular approaches to get these data ready table 2 the most time consuming step is to extract flow data and each constituent data find the matched records for each intended constituent with the existing flow records for calibration and then prepare the separated flow data for estimation last thing is to set up the header and control files for each loadest model it is assuming that the user can quickly and easily find the right data column and join the flow and water quality data we tried our best to do this processing finding out at least 2 min for a gage with consecutive time series of flow and water quality data since all three constituents are not always available for each gage it results in 111 total scenarios unique combination of gages and constituents created by lpdpi so the same scenarios were calculated in the absence of lpdpi and total time for all scenarios in the second step will need at least 222 min table 2 there is a function reading the data and formatting data for loadest in lpdpi which needs 2 min to finish all processing after having the paths filled it saves about 3 7 h 111 times faster to prepare data for a huc4 watershed if a study focuses on several huc4s it will facilitate them to get these data substantially faster than regular methods parallel running is another novel approach for handling data for loadest in lpdpi as we know in some cases loadest will run for a very long time running without finishing the estimation process in some cases like 12 scenarios mentioned before therefore approx 6 000 gages involved in our whole project were tested to find the maximum time needed for a scenario that is able to run successfully in a rational time period it was found that the longest scenario with a successful running needs about 2 min therefore loadest will be forced to suspend by lpdpi if it is running longer than 2 min regarding threads in parallel running more than 100 threads can be deployed with four cores configured in a regular computer in this case application 40 threads were tested resulting in 2 min for finishing 111 scenarios manually it will take 27 75 min with each consuming 15 s which includes locating the loadest project path and running it regardless of the intervals between two running scenarios table 2 the time to be consumed by lpdpi can be reduced further if more threads were deployed even in the current setting it is almost 14 times faster than running loadest manually with regard to the scenarios that have errors occurred or non stop running lpdpi provides a tool to fix them by rerun function which is to iteratively reduce the period of estimation or limit the flow range in estimation there are 41 iterations ranging from 1 to 11 in these 12 scenarios to fix errors three iterations were assumed to be taken in the absence of lpdpi in each iteration 30 s are usually needed because both the start and the end in estimation need to be reduced and at least 15 s are required for finishing loadest running accordingly the total time consumed is about 18 min while lpdpi only needs about 40 s for this process table 2 it is 26 times faster than manual rerunning and will save much more time if there are a lot of scenarios to be fixed the final date coverages of the 111 scenarios for calibration and final estimation in this case were presented in fig 10 for most scenarios there are longer date coverages in estimation than calibration some of which even cover the whole period in raw flow data from january 1 1960 to december 31 2018 at least lpdpi is able to cover the period in calibration helping to interpolate the discrete water quality records into time series of data for reasonably summarizing yearly or monthly metrics on water quality the final step is to extract statistical results and estimated loads at three temporal scales lpdpi also provides a function to collect all statistical metrics and selected equation model number by loadest in this case statistical metrics pbias nash sutcliffe efficiency nse and model flag equation number in loadest of regression models in these 111 scenarios were extracted as shown in fig 11 pbias 25 25 and nash sutcliffe efficiency 0 6 1 0 were chosen as criteria to extract scenarios that meet the requirements the final selection of three constituents included 70 scenarios 18 for total nitrogen 15 for filtered phosphorus and 37 for suspended sediment concentration it indicated that 63 of total scenarios 70 out of 111 have a good performance in estimating load by loadest in these scenarios the preferred equations were displayed by their count frequency in fig 12 model numbers in x axis are the same with numbers mentioned in loadest manual https water usgs gov software loadest doc regarding suspended sediment equations 9 7 and 8 are more popular to be the ideal models for estimating loads in loadest equations 6 8 and 9 are the best choices in simulating nitrogen load it will be better to choose equations 9 8 and 4 for filtered phosphorus estimation although these conclusions were drawn from only one huc4 watershed they suggest that each equation has its own advantage of estimating various constituents finally the estimations in these scenarios can be extracted in individual files at daily monthly and yearly time step in lpdpi which can subsequently be used for calibrating hydrologic models or performing further analysis in this case study we developed average annual nutrient and sediment loads for the calibration of a soil and water assessment tools swat model built in this huc4 watershed usgs 0512 5 conclusions and future work lpdpi is a useful tool for researchers or students who need to estimate loads at a large number of sites but do not have the efforts or time required to develop massive data processing programs and scripts lpdpi provides an easy to use workflow for users to automatically download and prepare inputs files for loadest the ability of lpdpi to rerun scenarios that otherwise fail with reduced data allows users to take full advantage of existing data and information available for the load estimation the option to run loadest in parallel across many cpu cores can save time when processing large amounts of data which is often required for calibration of large scale hydrologic models future plans for lpdpi development include focusing on gauge station selection through an interface via hydrologic unit or administrative boundaries software availability name of software lpdpi loadest parallel data processing interface developers jungang gao michael j white katrin bieger and jeffrey g arnold contact address blackland research extension center texas a m agrilife 702 e blackland road temple tx 76502 usa email jgao brc tamus edu availability https github com johng2014 lpdpi 1 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements usda is an equal opportunity employer and provider we highly appreciate the editor and three anonymous reviewers for their constructive comments that were very helpful for improving the manuscript this research was funded with a cooperative agreement fund by usda ars united states with nrcs united states agreement number 58 3098 8 002 
25909,cyberinfrastructure needs to be advanced to enable open and reproducible environmental modeling research recent efforts toward this goal have focused on advancing online repositories for data and model sharing online computational environments along with containerization technology and notebooks for capturing reproducible computational studies and application programming interfaces apis for simulation models to foster intuitive programmatic control the objective of this research is to show how these efforts can be integrated to support reproducible environmental modeling we present first the high level concept and general approach for integrating these three components we then present one possible implementation that integrates hydroshare an online repository cuahsi jupyterhub and cybergis jupyter for water computational environments and pysumma a model api to support open and reproducible hydrologic modeling we apply the example implementation for a hydrologic modeling use case to demonstrate how the approach can advance reproducible environmental modeling through the seamless integration of cyberinfrastructure services keywords open hydrology reproducibility modeling frameworks jupyterhub containers 1 introduction there is a growing acknowledgment and awareness of the reproducibility challenge facing computational environmental modeling fields hutton et al 2016 stagge et al 2019 as well as in other computational modeling disciplines baker 2016 mcnutt 2014 national academies of sciences 2019 according to a survey of 1576 researchers about 70 had tried but failed to reproduce published research and 90 agreed that the problem of reproducibility is a critical problem for scientific advancement baker 2016 within the hydrology and water resources fields stagge et al 2019 analyzed 360 articles in six leading journals to understand if their data were available online and if the study results were reproducible their analysis showed that only 5 6 of the articles had data and model code available online along with directions for use and only 1 1 were fully reproducible while 0 6 were partially reproducible there are many possible reasons for this outcome however we argue along with others that advances in the cyberinfrastructure that enable modern computational science is critical to achieving reproducible research hut et al 2017 hutton et al 2016 reviewing recent research toward this goal of improving the underlying cyberinfrastructure necessary to support reproducible computational studies we see three distinct thrusts 1 open sharing of data and models online 2 encapsulating computational environments through containers and self documented computational notebooks and 3 creating application programming interfaces apis for programmatic control of complex computational models a major effort to improve the open sharing of data and models is the fair findable accessible interoperable reusable guiding principles for scientific data management and stewardship wilkinson et al 2016 however fair principles speak primarily to openness which is essential but insufficient on its own for addressing reproducibility of computational software and computational environments bast 2019 ince et al 2012 argued that even with well developed data and software sharing capabilities it remains challenging to reproduce published results due to difficulties in documenting computational environments needed to repeat past studies moreover they found this especially true for operating system environments and software dependencies that can cause unpredictable differences with even slight changes in model source code or configuration to address this need a second thrust in recent research is aimed at overcoming the difficulties with sharing complete computational software environments research that has focused on improving the sharing of well documented data and software workflows for computational studies includes stodden and miguez 2013 for example who proposed sharing data algorithms and workflows to utilize and verify published results similarly gil et al 2016 suggested the best practices of sharing data software and documents in an open and transparent way using a high level roadmap of approaches to strengthen reproducibility in the geosciences in the meantime the broader information technology community has introduced the concept of containers as a means for encapsulating computational environments kurtzer et al 2017 merkel 2014 the result of this work has benefited computational modeling fields and led to efforts to improve the preservation of operating system and software dependencies strengthening reproducibility in computational research boettiger 2015 brinckman et al 2019 containerization technologies such as docker merkel 2014 have been used to reproduce computational modeling environments without requiring users to install additional dependencies boettiger 2015 signell and pothina 2019 software tools like sciunit essawy et al 2018 yuan et al 2018 ease the process of containerizing sharing and tracking scientific applications lowering the barrier to entry for researchers to use containerization tools containerization has also led to the ability to create new modeling environments and deploy them through interactive online analysis environments such as jupyterhub kluyver et al 2016 jupyter notebooks are quickly growing in use and popularity in computational fields as a means to document studies as a mix of formatted text mathematical equations and executable code with in line visualizations resulting from the code kluyver et al 2016 jupyterhub is a cloud based software that utilizes containerization to support the execution of multiple jupyter notebooks simultaneously recent advances leveraging jupyter for environmental modeling include work by castronova et al 2018 who created the cuahsi jupyterhub to support online hydrologic modeling and analysis yin et al 2017 who created a taudem tarboton 1997 modeling environment with jupyterhub eynard bontemps et al 2019 who created the pangeo project that supports big data studies in the geosciences and heavily leverages jupyterhub and bandaragoda et al 2019 who used jupyterhub within a larger knowledge infrastructure to support earth system modeling recent work has also begun to explore combining external computational environments including high performance computing hpc and high throughput computing htc cyberinfrastructure for model execution directly through jupyter notebooks lyu et al 2019 that work also takes advantage of containerization concepts to easily port preconfigured model execution environments to available computational resources the third thrust we observe in recent research is efforts to create apis for computational environmental models while many models have graphical user interfaces guis for improving the usability of the models apis are different in that they facilitate programmatically interacting with a simulation model to configure input files execute models and analyze model outputs python https www python org and r https www r project org are common programming languages used for creating model apis python has examples including model apis for the stormwater management model pyswmm mcdonnell 2017 modflow flopy bakker et al 2016 hydrologic simulation program in fortran pyhspf lampert and wu 2015 and precipitation runoff modeling system prms python volk and turner 2019 r has examples including model apis for topmodel topmodel buytaert 2011 swat swatmodel fuka et al 2014 and tuw model tuwmodel viglione and parajka 2020 these model apis help by abstracting low level programmatic details of input file manipulation and model execution operations from end users in this way they are particularly useful when combined with computational notebooks for creating self documented modeling studies that can be more easily understood and reproduced by both modelers and non modelers alike while work along each of these thrusts online data repositories computational environments leveraging containerization and computational notebooks and model apis is important individually integrating these three thrusts offers a powerful approach for reproducible computational modeling recent research has started to explore this integration includes 1 the gi rhessys green infrastructure regional hydro ecological simulation system jupyter environment created for green infrastructure gi landscape designs and modeling output using jupyterhub leonard et al 2019 2 the landlab model hobley et al 2017 with recent work to implement landlab within jupyterhub as a knowledge infrastructure bandaragoda et al 2019 and 3 the hydroterre system leonard and duffy 2016 that links an online data repository with the penn state integrated hydrologic model pihm while these examples focused on supporting individual modeling use cases they reveal general patterns of infrastructure components necessary to implement their systems our aim is to build on this past work by first presenting this general pattern as a general approach that can be followed for building new modeling systems second we provide an example implementation of the general approach that can be easily expanded to support any computational environmental model that is containerized and has an accompanying model api the objective of this research is therefore to put forward a general approach or framework for integrating online data repositories computational environments and model apis to enable more open and reproducible environmental modeling in the methodology section we first present a high level design of the approach describing each of the three components in more detail while also discussing different options available for online repositories notebook based and containerized modeling environments and model apis we then present an example implementation that makes use of hydroshare as an online repository cuahsi jupyterhub and cybergis jupyter for water as computational environments and pysumma as an example model api in the results and discussion section we present the results of applying the example implementation to reproduce a prior hydrologic modeling study clark et al 2015b and discuss the difficulty and nuance in claiming to achieve reproducibility we also present limitations of the work that could be a focus of future research finally we conclude by summarizing the findings and emphasizing their contribution to the larger goal of making past and future studies simpler to reproduce through advances in cyberinfrastructure 2 methodology in this section we describe the general approach being put forward for open and reproducible environmental modeling section 2 1 and then present an example implementation of this general approach for hydrologic modeling section 2 2 2 1 overview of general approach and description of system components the general modeling system approach considered in this research consists of three primary components fig 1 component 1 is the online repository where data models and notebooks can be openly shared with the community component 2 is the jupyterhub computational environment where containerized models can be executed using notebooks component 3 consists of a collection of model apis one for each model supported within the system that allow for programmatic configuration execution and visualization through computational notebooks the three components are integrated through seamless data transfers to create a powerful framework for open and reproducible modeling analyses in practice we anticipate that this general approach or framework may have many different physical implementations where different technologies may serve the needs of specific subcommunities within the broader environmental modeling field we demonstrate one such implementation in section 2 2 for the hydrology community in the following subsections we describe each of these components in more detail while also providing examples of each that are available for integration 2 1 1 online repository online repositories allow for storing sharing and publishing data metadata and other resources required to reproduce computational research findings these online repositories often support a rich set of user friendly features such as metadata capture persistent digital object identifiers dois and extensive apis for programmatically creating updating and deleting resources they also often support various data types such as documents figures code audio and video with metadata tailored to each data type some examples of online repositories used by researchers include dataone member nodes https www dataone org figshare https figshare com harvard dataverse https dataverse harvard edu and hydroshare https www hydroshare org many online repositories serve broad scientific communities and therefore maintain only general and widely applicable capabilities others are more targeted to specific communities and as a result can offer more specific functionality environmental modeling for example is not a common use case for many repositories that focus on more general data sharing needs e g figshare environmental models however have their own characteristics that consist of software input and output files and data processing workflows morsy et al 2017 described these unique needs of models being stored in data repositories and presented a data model design including metadata descriptions for key modeling objects to support flexible and applicable model sharing framework this design is implemented within the hydroshare data repository allowing for describing and sharing more specific model resource types 2 1 2 computational environment a computational environment serves as a gateway for model configuration execution and post processing in the case of model execution environmental modeling often includes complex simulation models along with data pre and post processing software all with software dependencies that range from the operating system to modules used within a model engine to libraries used by data processing and analysis software e g python libraries without the ability to replicate a computational environment slight inconsistencies in software dependencies can result in well documented model studies failing when ported to a new machine without the use of recent innovations like containers documenting the exact computational environment used in an analysis is difficult time consuming and error prone to overcome these challenges docker merkel 2014 and singularity kurtzer et al 2017 have emerged as containerization techniques used to encapsulate a computational modeling environment as described further in the implementation see section 2 2 along with containers computational gateway interfaces are also critical to lowering the barrier to entry and supporting more open and reproducible modeling in online computational environments with the emergence of jupyterhub as a gateway innovation there has been an increased interest in cloud based modeling environments for creating editing and running computational notebooks markham 2019 reviewed five popular cloud services that support computational notebooks table 1 we reviewed two additional cloud services 1 cuahsi jupyterhub hereafter cuahsi jh and 2 cybergis jupyter for water hereafter cybergis jw and included them in table 1 as well the environments range from scientific services e g the cuahsi jh and cybergis jw that are used in this work to more general services such as binder jupyter project et al 2018 large technology companies including google and microsoft have provided notebook execution environments such as google colab and microsoft azure notebooks demonstrating the popularity and growing interest in a variety of fields many cloud services have adopted the default jupyter interface available from the jupyter project without modification while others have modified this interface to customize it for their own purposes markham 2019 furthermore many cloud services support python r and other languages as well interface similarity in table 1 considers available menus buttons and other visual elements that make up the user interface and how different they are from a default jupyter interface all services listed in table 1 are candidates for integration into an implementation of the modeling system described in this paper 2 1 3 model apis an api defines a set of protocols or tools to communicate with an operating system database network and other lower level aspects of a software system reddy 2011 the abstraction provided by an api has benefits brooks 2013 including 1 flexibility and efficiency for data access 2 personalization to customize the functionality that users access the most and 3 reusability of code to work more productively examples of widely used apis include the google maps api for map services and the twitter api for social networking services services also widely exist for scientific systems relevant to environmental modeling including the hydroshare rest representational state transfer api for sharing and publishing water data as well as apis for a growing number of environmental models in this study we focused on python based model apis and reviewed a series of model apis including prms python volk and turner 2019 and pyhspf lampert and wu 2015 to better understand how they are designed and structured doing this can help to inform the design and structure of future apis created to support specific environmental models we observed that model api functionalities fell into three categories model input model execution and model output table 2 for prms python as an example input files often have corresponding python modules that can be used for data manipulation for pyhspf as an example the python modules do not have a one to one correspondence with the core model files and modules instead the api designs include a higher level abstraction to consider core classes needed for interacting with the model from this review we suggest that communities of modelers e g researchers or groups of researchers who are considering building a model api for a specific environmental model begin with answering the following questions 1 what configuration and input files should be exposed through the api to allow for programmatic changes and what are the logical classes for organizing these model input configuration attributes 2 what methods and attributes should the api expose for executing the model and refining the model through for example calibration or sensitivity analysis 3 what are common visualizations of the model output that many users would wish to produce creating a model api with this functionality in a well thought through design will serve as a solid foundation for future extensions to the software furthermore the extent to which environmental model apis can adopt conventions for the organization of their design and structure will allow users to more easily learn new model apis by having some consistency across model apis 2 2 example implementation in this section we present one possible physical implementation of approach described in the prior section this example implementation uses hydroshare as the online repository cuahsi jh and cybergis jw the computational environments and pysumma as one of potentially many model apis within the system while this example implementation targets the needs of the hydrologic modeling community we anticipate that multiple other permutations of the technologies described in the prior section could be assembled to meet the needs of other environmental modeling communities 2 2 1 hydroshare as the online repository we used hydroshare as the data repository in our example implementation due to both its flexibility and tailored functionality for supporting environmental modeling use cases hydroshare is an online repository tailored for the needs of the hydrologic community but general enough to satisfy other environmental modeling needs tarboton et al 2014 hydroshare defines a resource as the fundamental unit of digital content in hydroshare that contains data and or model files and their corresponding metadata horsburgh et al 2016 hydroshare supports various data content types such as geographic raster geotiff multidimensional arrays netcdf geographic features shapefile and time series as aggregations within a single hydroshare resource hydroshare also supports collections as a resource type that holds a list of related hydroshare resources that can be referenced with a single unique identifier furthermore realizing that data associated with models have their own characteristics hydroshare defines specific resource types for a model program the software and a model instance the input and output files for a specific model run morsy et al 2017 resources with these two resource types are related through the executedby attribute of a model instance which points to the specific model program resource used to execute that model instance this design allows for a one to many link between a model program that is used to execute many different model instances built for different geographic locations or to address different research questions the methodology for sharing computational modeling resources is shown in fig 2 first the user creates a model program resource for each version of a model program software used in the analysis this resource can include the source code executable and container for the model program itself or a link to one or more of these resources shared in a system external to hydroshare e g in github binderhub or dockerhub second the model instance resources are created to store and describe the input data required to execute the model and can optionally store the output after the model is executed then the model instance is linked to a specific model program resource using the executedby metadata term a separate composite resource is used to store jupyter notebooks that describe the overall analysis workflow finally a collection resource is used to combine and conveniently share all of the resources used to complete the study 2 2 2 jupyterhub as the computational environment we integrated both the cuahsi jh and cybergis jw as computational environments in our example implementation we chose these environments because both are publicly available and aimed at scientific modeling in the water and environmental communities moreover both systems allow for seamless data transfer with hydroshare as a data repository supporting the necessary interoperability between these two components of the general framework this data transfer is enabled through the hydroshare rest api and the standardization of hydroshare resource data structures the cuahsi jh is a cloud computing environment on the google cloud platform specifically designed to support research and education in the water sciences fig 3 to support a variety of applications it leverages environment profiles that allow users to choose the ideal computing configuration for their work each of these profiles is a separate containerized environment that has been built with a specific set of software to support various water science use cases currently the cuahsi jh consists of seven profiles that range from scientific python and r to hydrolearn https www hydrolearn org educational modules and hydrologic modeling in addition the cuahsi jh supports persistent data meaning user created content is stored between sessions and shared between profile environments moreover this environment enables users to install custom software using conda virtual environments for this study we created a python 3 7 summa modeling profile to support summa 3 0 modeling environment using a dockerfile in cuahsi jh another model execution environment interoperable with hydroshare cybergis jw is a well tailored cybergisx https cybergisxhub cigi illinois edu instance to serve the fast emerging needs for data intensive and reproducible research in the environmental modeling community fig 3 overall cybergis jw is similar to cuahsi jh but cybergis jw also includes interoperability with advanced cyberinfrastructure resources such as virtual roger a cybergis supercomputer hosted by the cybergis center for advanced digital and spatial studies at the university of illinois and xsede comet an hpc resource on the extreme science and engineering discovery environment for model execution support lyu et al 2019 describe how to use htc through a jupyter notebook using summa as an example case in cybergis jupyter beta which is the previous version of cybergis jw currently cybergis jw is supporting landlab hobley et al 2017 and rhessys tague et al 2004 modeling environments for this study we created a summa modeling environment using a dockerfile users can use this summa modeling environment via a summa kernel for use of hpc resources cybergis jw requires a singularity image to support a computational modeling environment in xsede because cybergis jw and xsede are separately placed also cybergis jw needs a particular library to connect to computational resources for submitting jobs and data exchange in xsede 2 2 3 pysumma as the model api the model api pysumma was created through this research as an example model api pysumma wraps the hydrologic model structure for unifying multiple modeling alternative summa clark et al 2015a summa was selected for this study because it is a general hydrologic modeling environment offering the ability to conduct model experiments with controlled and systematic evaluation of multiple model representations of hydrologic processes and scaling behavior the summa model simulates both the thermodynamics the storage and flux of energy such as the heat balance of the vegetation canopy snow and soil affected by the radiative fluxes as well as the hydrology the storage and transmission of water for example vertical and lateral transmission of water through vegetation canopy snow soil aquifer and river within a catchment system the flexible hierarchical spatial structure of summa supports different spatial configurations including the size and shape of model elements with grouped response units grus kouwen et al 1993 and hydrologic response units hrus in addition the flexible structure enables researchers to consider the lateral flux of water across the model domain and complex topographical properties like hillslopes and riparian areas this flexibility within summa enables hydrologists to find solutions for the application of scaling behavior in relation to different physical processes summa also enables hydrologists to select the appropriate physical process methods and model complexity this process implements a modular structure that is supported by the conservation equations to calculate each process in a controlled and systematic way this unified process helps users to concentrate important physical parameterizations with higher complexity and conversely to simplify specific processes to minimize uncertainty according to the purpose and characteristics of biophysics and hydrology moreover the structure of summa which consists of a core solver and outer branches enables the output of a numerical solution from summa so that the user can evaluate the accuracy and efficiency of the model therefore summa supports flexibility to simulate different options of physical processes and numerical solutions we designed and implemented pysumma as a model api for summa using the questions proposed in section 2 1 3 for guiding the design of a new model api table 3 for the model input category there are six configuration files to manipulate summa input 1 file manager 2 decisions 3 forcing file list 4 model output 5 param trial and 6 local attribute files to expose the first four configuration files through the api we created file manger py decisions py force file list py output control py and option py for the rest of the configuration files we created assign trial params and assign attributes methods in simulation py in the model execution category we created simulation py to use the model execution command conveniently from the shell script format so that users do not need to edit manually every time we also created two options to execute the summa model local and docker to satisfy different user requirements finally the output format of summa is netcdf therefore we created plotting py for visualization considering the output variables and their output structure in netcdf the classes of pysumma are shown in fig 4 a simulation module simulation py is used as the initial python module to start a pysumma api and combine most pysumma functionalities such as manipulating configuration files and executing summa after creating a pysumma simulation object users can manipulate six configuration files a file manager module file manager py reads and manipulates a file manager file which controls the location of every configuration file for the summa model for example the file manager file sets the directory and configuration files including the decision forcing parameter and attribute files a decisions module decisions py reads and manipulates a decisions file which sets different physical process parameterizations through the available options object in decisions py users can determine what options are available for model parameterizations and select model parameterizations from a list of options for each physical process summa online document 2020 four input configuration modules file manager py decisions py force file list py and output control py have the same pattern of classes for example a file manager module file manager py is composed of filemanageroption and filemanager classes and a decisions module decisions py is composed of decisionoption and decision classes each class is connected to an option module option py to avoid repetition of functions such as comparing setting and writing each configuration file after setting the summa configuration the simulation module simulation py is used for model execution the run method of the simulation class is used to execute the summa model this execution can be done in both local and docker computational environments the environments are set using the run option parameter for the run method as discussed later in the results and discussion section once a summa model run has been completed the plotting module plotting py can be used to visualize the results there are two different data output structures for summa 1 time hru or gru number and variable 2 time hru or gru number soil or snow layer number and variable to visualize each of these output structures the plotting class consists of three methods ts plot ts plot layer and heatmap plot we used the seaborn library statistical data visualization library to create a 2d heat map with soil or snow layer and time as the axis for displaying a selected variable lastly the model output module output control py is used to manipulate the output variables of summa and the utilities module hydroshare utils py has functions to download test cases of summa model instance resources and execution files model program resources from hydroshare 3 results and discussion in this section we present a modeling case study application of the example implementation system described in section 2 2 then we discuss how this approach addresses the challenge of achieving more reproducible studies summarized in the introduction section by evaluating the approach against definitions concepts and metrics for reproducibility proposed by others lastly we discuss the limitations of our approach that present opportunities for future research 3 1 case study description clark et al 2015b describe a set of thirteen modeling experiments exploring various hydrologic modeling scenarios using summa the study area for these modeling experiments is the reynolds mountain east area a 32 7 km2 in the reynolds creek experimental watershed in idaho usa fig 5 in this paper we focus on these modeling experiments as a case study with the goal of applying our approach so that each clark et al 2015b experiment can be reconstructed and shared openly in a way that is easier to reproduce the first step toward this goal is the creation and organization of hydroshare resources to share all models and data files required for the analysis the second step is to create jupyter notebooks that describe the modeling experiments these notebooks include text and equations to describe the modeling experiments while also including executable python code using the pysumma api and inline visualizations that can be repeated and extended by others we created seven jupyter notebooks each one documenting an experiment in the clark et al 2015b study and published them through hydroshare as a collection resource choi et al 2020 3 2 model and data resources our first step in reproducing the clark et al 2015b modeling experiments was to publish the specific summa model version used in the analysis as a resource on hydroshare to do this we created a hydroshare resource using the model program resource type and uploaded the summa 3 0 0 source code to the resource we then published the resource through hydroshare so that it is persistent and immutable with a unique digital object identifier doi choi et al 2020 fig 6 shows the landing page for this resource on hydroshare that includes detailed metadata describing 1 the source code and compiled software engine 2 metadata for the software 3 a link showing the model was derived from a particular branch of a github repository for summa and 4 a citation for referencing the resource this same summa 3 0 0 was also installed on the cuahsi jh allowing users to execute the summa model directly from cuahsi jh we next created multiple resources in hydroshare to store the specific model inputs for each different summa model experiment used in the clark et al 2015b paper there were four synthetic and nine field study test cases available as an online supplement to the clark et al 2015b paper from these data we created seven unique model instance resources in hydroshare table 4 and grouped them into a collection resource choi et al 2020 each model instance resource includes 1 input data for the summa model 2 a reference to the clark et al 2015b paper 3 a composite resource link that points to the jupyter notebook used to execute the summa model and 4 a link to the model program resource used to execute the model instance once this step is complete the model and data resources required to reproduce the clark et al 2015b experiments are publicly accessible in hydroshare with metadata to describe each resource and a unique url to locate each resource hydroshare also allows for publishing these resources in which case the resources become immutable and are assigned a digital object identifier doi this pattern can be adopted by other environmental modeling studies whereby both the model and data resources required to reproduce the study are uploaded into hydroshare given metadata to describe each resource including relationships between resources such as the executedby relationship between model program and model instance resources and published with a doi 3 3 demonstrating reproducibility this section describes the steps that should be taken to reproduce one of the experiments described in clark et al 2015b as a preparation step before starting a summa simulation using jupyter notebooks on cuahsi jupyterhub we recommend creating a pysumma conda virtual environment by running the steps described in the notebook creating pysumma conda virtual environment in cuahsi jupyterhub ipynb in the hydroshare composite resource for cuahsi jh notebooks once this preparation step is completed the basic algorithm to run a notebook is shown in fig 7 first the pysumma hydroshare utils module is used to download the model instance that will be used in the notebook directly from hydroshare after downloading the summa model instance it is possible to create a pysumma simulation object using the simulation class of pysumma and supplying summa executable summa exe and the file manager file path after creating the pysumma simulation object the summa model can be executed using the run method which takes a run option argument as local when cuahsi jh was created by using docker summa was automatically complied and the summa executable was created in usr local bin summa exe therefore after setting the executable variable to the location of summa exe users can set a run option as local by changing the executable variable as usr bin summa exe it is possible to execute the same notebook on cybergis jw as an example we present here the results from running two different experiments included in the clark et al 2015b paper the first reproduces fig 7 from clark et al 2015b and is published as a hydroshare resource with the title the impact of stomatal resistance parameterization on et of summa model in aspen stand at reynolds mountain east choi et al 2020 the second reproduces fig 8 left from clark et al 2015b and is published as a hydroshare resource with the title the impact of root distributions parameters on et of summa model in aspen stand at reynolds mountain east fig 8 gives the results from the first experiment that explores the impact of three different stomatal resistance parameterizations on total evapotranspiration ball berry ball et al 1987 jarvis 1976 and the simple resistance method fig 8a is the original result from the summa paper clark et al 2015b and fig 8b is a reproduced figure resulting from applying this framework these stomatal resistance parameterizations have different physical characteristics both the jarvis and ball berry methods consider photosynthesis while the simple soil resistance method mainly considers the soil water conditions the results show that the simple soil resistance method is higher than the other methods during the night hours comparing the two plots shows the complexity associated with reproducing past computational modeling studies while the results are consistent they are not exact the precise reason for the differences in the model results is difficult to determine we suspect it is due in part to upgrades to summa or summa dependencies between the versions of the summa 2 0 used in the clark et al 2015b paper and the summa 3 0 used to create the newer plot more vexing is that some of the observed data points appear to have shifted with no good explanation for why one possible explanation could be the fact that different visualization tools were used to create each plot interactive data language idl for the plot on the left and matplotlib for the plot on the right we suspect differences like this would not be uncommon when trying to reproduce any past computational study given the difficulties in recreating the exact computational and analysis environment including data preparation routines computational modeling software and post processing analysis and visualization tools this in fact speaks to the difficulty of the problem and the need for innovation in cyberinfrastructure approaches that is at the heart of this study this said it is also important to stress that the goal of reproducibility may not be to obtain the exact same results but rather consistent results that would produce the same conclusion this is an idea expressed by high level reports on computational reproducibility national academies of sciences 2019 that we will discuss further in section 3 4 fig 9 shows the results from the second experiment from clark et al 2015b which explores the impact of the root distribution parameters with different stomatal resistance parameterizations for total evapotranspiration in this case we reproduced the plot that shows the impact of root distribution parameters fig 9b and compared it to the previous result fig 9a again we see consistent although not exact results between the two model runs given that the modeling experiment is now implemented within the system it is also possible to more easily extend and repurpose it for other purposes to this point we demonstrate reuse of past modeling studies by creating two additional plots for determining the effect of different root distribution fig 9c and stomatal resistance parameterizations fig 9d on total evapotranspiration these plots show how higher root distribution exponents in the soil profile indicate that the roots are deeper in the soil which makes it easier for plants to extract soil water as a result during the higher evapotranspiration periods 10 00 17 00 the jarvis method more closely matched the observation data however during the period when evapotranspiration is decreasing 17 00 20 00 the ball berry method was more precise compared to the simple resistance method over the complete time period the analysis shows that the jarvis method had the best fit with observations 3 4 evaluating reproducibility to evaluate if reproducibility was achieved we considered definitions and concepts for evaluating reproducibility being put forward by others for example the national academies of science engineering and medicine national academies of sciences 2019 define reproducibility focused on computational reproducibility as obtaining consistent results using the same input data computational steps methods and code and conditions of analysis to guarantee reproducibility the organization recommended delivering clear specific and complete information about any computational methods and data products to repeat the previous study and that information should include the data methods and computational environment fair principles wilkinson et al 2016 include 15 metrics that should be met as a minimum for reproducibility these metrics are a findable 4 metrics b accessible 4 metrics c interoperable 3 metrics and d reusable 4 metrics in the hydrology and water resources fields hutton et al 2016 recommended reproducible studies have 1 readable and reusable code 2 an unambiguous workflow 3 a repository to easily find data and code with associated metadata 4 use of unique persistent identifiers 5 new procedures to reproduce large scale studies using hpc additionally hut et al 2017 suggested the use of containers and open interfaces to guarantee stronger reproducibility as a response to hutton et al 2016 finally stagge et al 2019 proposed a set of survey questions to assess the reproducibility of a journal article the survey requires that eight elements be available for a study to be called reproducible 1 directions to run or reproduce the study 2 code model software files 3 input data 4 hardware software requirements 5 stated data persistence policy 6 materials linked by unique and persistent identifiers 7 metadata to describe the code and 8 common file format and instructions to open these files with these criteria in mind by simply using hydroshare as the data repository for all data and software used for the study we can support many of the metrics associated with reproducibility hydroshare supports fair principles tarboton et al 2018 for each resource that includes model input source code metadata and supplementary documents using jupyterhub as described in the paper provides a consistent computational environment and using jupyter notebooks and containerized model execution environments provides a clear and easy workflow to assure users can reproduce a published study finally using a model api makes it easier for a user to follow the logic and steps used to configure run and postprocess a modeling simulation allowing for not only reproducibility but also reuse and extension of prior work therefore if we compare these definitions and concepts for a validation of reproducibility to our approach and its example application we can claim that it satisfies the criteria for reproducible computational modeling still while the framework allows for satisfying the criteria it is still up to the user to ensure care is taken with sharing and documenting resources with adequate metadata and instructions to achieve reproducibility 3 5 approach limitations and opportunities for future research this research focuses on examples that assume model input files had already been processed and are available for use in the modeling analysis the preprocessing steps required to generate model input files from raw geospatial and time series observational data are a necessary component of longer term goals for creating so called end to end reproducible analysis workflows for example slater et al 2019 provided an end to end reproducible hydrology workflow using r for climate data retrieval spatial analysis modeling statistical analysis visualization and data publishing as another example of automated end to end workflows hydroterre leonard 2015 includes 1 data workflows leonard and duffy 2013 to create watershed models using essential terrestrial variables etv 2 data model workflows to transform watershed data into model inputs 3 model workflows leonard and duffy 2014 to execute models in hpc especially the penn state integrated hydrologic modeling system pihm and 4 visualization workflows to visualize the first three workflows to easily create and share model results for analysis currently pysumma has developed the functionalities of manipulating created model input executing summa and plotting model output to complete end to end workflows data preprocessing is critical for improving reproducibility as the steps to create model input files are often nontrivial and require a significant time investment prior work to address this challenge includes the ecohydrolib python library developed as a software framework for managing spatial data acquisition and preparation workflows for ecohydrology modeling miles and band 2015 ecohydrolib takes advantage of open source grass gis libraries to automate data gathering and preparation for environmental models it is a model agnostic approach for mapping a variety of data sources into input files required by environmental models alternative data processing workflows and pipelines such as hydroterre leonard 2015 could also be explored for bringing data preprocessing capabilities for environmental models into the general approach described through this work however just having new data processing pipelines alone will be insufficient we also need more detailed modeling protocols and procedures to replicate or even reproduce a study ceola et al 2015 because reproducibility is not just a technological problem it is equally an educational problem grüning et al 2018 post processing for visualization and model analysis procedures is also essential to creating a powerful modeling environment saving time when analyzing model output and strengthening reproducibility to grow use of model apis many analysis methods will be necessary such as plotting calibration optimization and uncertainty analysis while pysumma is still being developed toward these goals other model apis discussed in this paper and that could be used within the example modeling system do have more robust processing capabilities already one question that remains is the extent to which environmental model apis can reuse underlying software to support common model post processing routines general libraries in python such as pandas and matplotlib are universally applicable to environmental modeling post processing tasks however is a plotting or data analysis library more tailored for environmental modeling but still sufficiently general to serve many environmental models possible if so it could further reduce the duplication of code across environmental model apis and ultimately encourage more environmental model apis that are robust easier to maintain and feature rich the ability to include data pre and post processing within the framework would be an important step for moving from reproducibility to replicability within the framework replicability is defined by the national academies of science engineering and medicine national academies of sciences 2019 as obtaining consistent results across studies aimed at answering the same scientific question each of which has obtained its own data replication therefore can be thought of as a next step beyond reproducibility where a study is repeated using new data potentially from a new site or different time period but similar methods this work has focused on a general approach to support reproducibility of computational models the framework could be extended for replication by extending a model api like the pysumma api described in this paper to include not only functions for model configuration e g settings and parameter values assuming model input files have already been generated but also for model preprocessing where input files for the model are generated from raw data sources 4 conclusion computational irreproducibility is an important problem in many scientific fields recent research to improve computational reproducibility has focused on advancing the sharing of data used in studies using computational notebooks and containers for encapsulating complete computational environments and developing model apis for programmatically interacting with simulation models a contribution of this research is to present a general approach to integrate these three areas of past work into a general approach for supporting more open and reproducible environmental modeling we present an example implementation of this approach by leveraging 1 hydroshare as a data sharing repository 2 jupyterhub as a notebook based containerized and cloud based computational environment and 3 pysumma as an example model api able to abstract lower level details for model configuration execution and visualization from end users using the example implementation we demonstrate how modeling analyses can be completed in a more open and reproducible way building from a prior study presenting a series of modeling experiments applying summa at the reynolds mountain east area in the reynolds creek experimental watershed in idaho usa clark et al 2015b we first created and organized hydroshare resources to share data and model files next we created jupyter notebooks that leveraged the pysumma api introduced in this paper to reproduce and extend figures from the prior study each notebook a pulled required data from hydroshare into the computational environment b provided a notebook using text equations code and inline visualizations for documenting the experiment and c allowed for online execution of the notebook and sharing of modifications to the notebook through hydroshare finally we discussed how we evaluated that reproducibility was achieved and future steps that could be taken to further improve the proposed framework from this research we conclude that cyberinfrastructure is reaching a point where it is possible to build open and transparent environmental modeling systems online repositories are sufficiently mature where they can be relied upon for storing key data and software resources for studies computational environments able to execute containerized environmental models can be interlinked with data repositories and the ability for these computational environments to serve as gateways to high performance computing hpc resources is improving more models are being provided with apis that allow for programmatic control of the model configuration execution and visualization jupyter notebooks provide an important orchestration and documentation glue across these components where users can leverage apis to access and publish data from online repositories submit jobs to hpc resources and programmatically interact with state of the art environmental models linking these capabilities in a way that can be built upon and expanded as new models become available as demonstrated in this paper will move environmental modeling in a direction where open transparent reproducible reusable and replicable studies become the rule rather than the exception software and data availability all software and data used in this study were published with persistent dois on hydroshare a collection resource in hydroshare choi et al 2020 contains each of these resources in addition to these resources published through hydroshare the pysumma source code created though this study is available on github as detailed below product title pysumma v3 0 0 lead developers young don choi and andrew bennett contact email yc5ef virginia edu andrbenn uw edu tested platform hydroshare cuahsi jupyterhub cybergis jupyter for water software required python 3 5 or above availability the pysumma source code is publicly available through github https github com uw hydro pysumma releases tag 3 0 0 license bsd 3 clause license list of relevant urls cuahsi jupyterhub https jupyterhub cuahsi org cuahsi jupyterhub legacy environment https jupyter cuahsi org cuahsi jupyterhub github https github com hydroshare hydroshare jupyterhub cybergis jupyter beta https hsjupyter cigi illinois edu 8000 cybergis jupyter for water https go illinois edu cybergis jupyter water dataone https www dataone org docker https www docker com dockerhub https hub docker com ecohydrolib https github com selimnairb ecohydrolib facebook api https developers facebook com docs apis and sdks figshare https figshare com google api https developers google com apis explorer harvard dataverse https dataverse harvard edu hydroshare rest api https github com hydroshare hydroshare wiki hydroshare rest api netcdf4 github https github com unidata netcdf4 python numpy https www numpy org pandas https pandas pydata org seaborn https seaborn pydata org singularity https sylabs io summa on the ucar https ral ucar edu projects summa xarray http xarray pydata org xsede https www xsede org declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the national science foundation under collaborative grants oac 1664061 oac 1664018 oac 1664119 icer 1928369 and icer 1928315 we acknowledge the work of the larger hydroshare team https help hydroshare org about hydroshare team that made this research possible 
25909,cyberinfrastructure needs to be advanced to enable open and reproducible environmental modeling research recent efforts toward this goal have focused on advancing online repositories for data and model sharing online computational environments along with containerization technology and notebooks for capturing reproducible computational studies and application programming interfaces apis for simulation models to foster intuitive programmatic control the objective of this research is to show how these efforts can be integrated to support reproducible environmental modeling we present first the high level concept and general approach for integrating these three components we then present one possible implementation that integrates hydroshare an online repository cuahsi jupyterhub and cybergis jupyter for water computational environments and pysumma a model api to support open and reproducible hydrologic modeling we apply the example implementation for a hydrologic modeling use case to demonstrate how the approach can advance reproducible environmental modeling through the seamless integration of cyberinfrastructure services keywords open hydrology reproducibility modeling frameworks jupyterhub containers 1 introduction there is a growing acknowledgment and awareness of the reproducibility challenge facing computational environmental modeling fields hutton et al 2016 stagge et al 2019 as well as in other computational modeling disciplines baker 2016 mcnutt 2014 national academies of sciences 2019 according to a survey of 1576 researchers about 70 had tried but failed to reproduce published research and 90 agreed that the problem of reproducibility is a critical problem for scientific advancement baker 2016 within the hydrology and water resources fields stagge et al 2019 analyzed 360 articles in six leading journals to understand if their data were available online and if the study results were reproducible their analysis showed that only 5 6 of the articles had data and model code available online along with directions for use and only 1 1 were fully reproducible while 0 6 were partially reproducible there are many possible reasons for this outcome however we argue along with others that advances in the cyberinfrastructure that enable modern computational science is critical to achieving reproducible research hut et al 2017 hutton et al 2016 reviewing recent research toward this goal of improving the underlying cyberinfrastructure necessary to support reproducible computational studies we see three distinct thrusts 1 open sharing of data and models online 2 encapsulating computational environments through containers and self documented computational notebooks and 3 creating application programming interfaces apis for programmatic control of complex computational models a major effort to improve the open sharing of data and models is the fair findable accessible interoperable reusable guiding principles for scientific data management and stewardship wilkinson et al 2016 however fair principles speak primarily to openness which is essential but insufficient on its own for addressing reproducibility of computational software and computational environments bast 2019 ince et al 2012 argued that even with well developed data and software sharing capabilities it remains challenging to reproduce published results due to difficulties in documenting computational environments needed to repeat past studies moreover they found this especially true for operating system environments and software dependencies that can cause unpredictable differences with even slight changes in model source code or configuration to address this need a second thrust in recent research is aimed at overcoming the difficulties with sharing complete computational software environments research that has focused on improving the sharing of well documented data and software workflows for computational studies includes stodden and miguez 2013 for example who proposed sharing data algorithms and workflows to utilize and verify published results similarly gil et al 2016 suggested the best practices of sharing data software and documents in an open and transparent way using a high level roadmap of approaches to strengthen reproducibility in the geosciences in the meantime the broader information technology community has introduced the concept of containers as a means for encapsulating computational environments kurtzer et al 2017 merkel 2014 the result of this work has benefited computational modeling fields and led to efforts to improve the preservation of operating system and software dependencies strengthening reproducibility in computational research boettiger 2015 brinckman et al 2019 containerization technologies such as docker merkel 2014 have been used to reproduce computational modeling environments without requiring users to install additional dependencies boettiger 2015 signell and pothina 2019 software tools like sciunit essawy et al 2018 yuan et al 2018 ease the process of containerizing sharing and tracking scientific applications lowering the barrier to entry for researchers to use containerization tools containerization has also led to the ability to create new modeling environments and deploy them through interactive online analysis environments such as jupyterhub kluyver et al 2016 jupyter notebooks are quickly growing in use and popularity in computational fields as a means to document studies as a mix of formatted text mathematical equations and executable code with in line visualizations resulting from the code kluyver et al 2016 jupyterhub is a cloud based software that utilizes containerization to support the execution of multiple jupyter notebooks simultaneously recent advances leveraging jupyter for environmental modeling include work by castronova et al 2018 who created the cuahsi jupyterhub to support online hydrologic modeling and analysis yin et al 2017 who created a taudem tarboton 1997 modeling environment with jupyterhub eynard bontemps et al 2019 who created the pangeo project that supports big data studies in the geosciences and heavily leverages jupyterhub and bandaragoda et al 2019 who used jupyterhub within a larger knowledge infrastructure to support earth system modeling recent work has also begun to explore combining external computational environments including high performance computing hpc and high throughput computing htc cyberinfrastructure for model execution directly through jupyter notebooks lyu et al 2019 that work also takes advantage of containerization concepts to easily port preconfigured model execution environments to available computational resources the third thrust we observe in recent research is efforts to create apis for computational environmental models while many models have graphical user interfaces guis for improving the usability of the models apis are different in that they facilitate programmatically interacting with a simulation model to configure input files execute models and analyze model outputs python https www python org and r https www r project org are common programming languages used for creating model apis python has examples including model apis for the stormwater management model pyswmm mcdonnell 2017 modflow flopy bakker et al 2016 hydrologic simulation program in fortran pyhspf lampert and wu 2015 and precipitation runoff modeling system prms python volk and turner 2019 r has examples including model apis for topmodel topmodel buytaert 2011 swat swatmodel fuka et al 2014 and tuw model tuwmodel viglione and parajka 2020 these model apis help by abstracting low level programmatic details of input file manipulation and model execution operations from end users in this way they are particularly useful when combined with computational notebooks for creating self documented modeling studies that can be more easily understood and reproduced by both modelers and non modelers alike while work along each of these thrusts online data repositories computational environments leveraging containerization and computational notebooks and model apis is important individually integrating these three thrusts offers a powerful approach for reproducible computational modeling recent research has started to explore this integration includes 1 the gi rhessys green infrastructure regional hydro ecological simulation system jupyter environment created for green infrastructure gi landscape designs and modeling output using jupyterhub leonard et al 2019 2 the landlab model hobley et al 2017 with recent work to implement landlab within jupyterhub as a knowledge infrastructure bandaragoda et al 2019 and 3 the hydroterre system leonard and duffy 2016 that links an online data repository with the penn state integrated hydrologic model pihm while these examples focused on supporting individual modeling use cases they reveal general patterns of infrastructure components necessary to implement their systems our aim is to build on this past work by first presenting this general pattern as a general approach that can be followed for building new modeling systems second we provide an example implementation of the general approach that can be easily expanded to support any computational environmental model that is containerized and has an accompanying model api the objective of this research is therefore to put forward a general approach or framework for integrating online data repositories computational environments and model apis to enable more open and reproducible environmental modeling in the methodology section we first present a high level design of the approach describing each of the three components in more detail while also discussing different options available for online repositories notebook based and containerized modeling environments and model apis we then present an example implementation that makes use of hydroshare as an online repository cuahsi jupyterhub and cybergis jupyter for water as computational environments and pysumma as an example model api in the results and discussion section we present the results of applying the example implementation to reproduce a prior hydrologic modeling study clark et al 2015b and discuss the difficulty and nuance in claiming to achieve reproducibility we also present limitations of the work that could be a focus of future research finally we conclude by summarizing the findings and emphasizing their contribution to the larger goal of making past and future studies simpler to reproduce through advances in cyberinfrastructure 2 methodology in this section we describe the general approach being put forward for open and reproducible environmental modeling section 2 1 and then present an example implementation of this general approach for hydrologic modeling section 2 2 2 1 overview of general approach and description of system components the general modeling system approach considered in this research consists of three primary components fig 1 component 1 is the online repository where data models and notebooks can be openly shared with the community component 2 is the jupyterhub computational environment where containerized models can be executed using notebooks component 3 consists of a collection of model apis one for each model supported within the system that allow for programmatic configuration execution and visualization through computational notebooks the three components are integrated through seamless data transfers to create a powerful framework for open and reproducible modeling analyses in practice we anticipate that this general approach or framework may have many different physical implementations where different technologies may serve the needs of specific subcommunities within the broader environmental modeling field we demonstrate one such implementation in section 2 2 for the hydrology community in the following subsections we describe each of these components in more detail while also providing examples of each that are available for integration 2 1 1 online repository online repositories allow for storing sharing and publishing data metadata and other resources required to reproduce computational research findings these online repositories often support a rich set of user friendly features such as metadata capture persistent digital object identifiers dois and extensive apis for programmatically creating updating and deleting resources they also often support various data types such as documents figures code audio and video with metadata tailored to each data type some examples of online repositories used by researchers include dataone member nodes https www dataone org figshare https figshare com harvard dataverse https dataverse harvard edu and hydroshare https www hydroshare org many online repositories serve broad scientific communities and therefore maintain only general and widely applicable capabilities others are more targeted to specific communities and as a result can offer more specific functionality environmental modeling for example is not a common use case for many repositories that focus on more general data sharing needs e g figshare environmental models however have their own characteristics that consist of software input and output files and data processing workflows morsy et al 2017 described these unique needs of models being stored in data repositories and presented a data model design including metadata descriptions for key modeling objects to support flexible and applicable model sharing framework this design is implemented within the hydroshare data repository allowing for describing and sharing more specific model resource types 2 1 2 computational environment a computational environment serves as a gateway for model configuration execution and post processing in the case of model execution environmental modeling often includes complex simulation models along with data pre and post processing software all with software dependencies that range from the operating system to modules used within a model engine to libraries used by data processing and analysis software e g python libraries without the ability to replicate a computational environment slight inconsistencies in software dependencies can result in well documented model studies failing when ported to a new machine without the use of recent innovations like containers documenting the exact computational environment used in an analysis is difficult time consuming and error prone to overcome these challenges docker merkel 2014 and singularity kurtzer et al 2017 have emerged as containerization techniques used to encapsulate a computational modeling environment as described further in the implementation see section 2 2 along with containers computational gateway interfaces are also critical to lowering the barrier to entry and supporting more open and reproducible modeling in online computational environments with the emergence of jupyterhub as a gateway innovation there has been an increased interest in cloud based modeling environments for creating editing and running computational notebooks markham 2019 reviewed five popular cloud services that support computational notebooks table 1 we reviewed two additional cloud services 1 cuahsi jupyterhub hereafter cuahsi jh and 2 cybergis jupyter for water hereafter cybergis jw and included them in table 1 as well the environments range from scientific services e g the cuahsi jh and cybergis jw that are used in this work to more general services such as binder jupyter project et al 2018 large technology companies including google and microsoft have provided notebook execution environments such as google colab and microsoft azure notebooks demonstrating the popularity and growing interest in a variety of fields many cloud services have adopted the default jupyter interface available from the jupyter project without modification while others have modified this interface to customize it for their own purposes markham 2019 furthermore many cloud services support python r and other languages as well interface similarity in table 1 considers available menus buttons and other visual elements that make up the user interface and how different they are from a default jupyter interface all services listed in table 1 are candidates for integration into an implementation of the modeling system described in this paper 2 1 3 model apis an api defines a set of protocols or tools to communicate with an operating system database network and other lower level aspects of a software system reddy 2011 the abstraction provided by an api has benefits brooks 2013 including 1 flexibility and efficiency for data access 2 personalization to customize the functionality that users access the most and 3 reusability of code to work more productively examples of widely used apis include the google maps api for map services and the twitter api for social networking services services also widely exist for scientific systems relevant to environmental modeling including the hydroshare rest representational state transfer api for sharing and publishing water data as well as apis for a growing number of environmental models in this study we focused on python based model apis and reviewed a series of model apis including prms python volk and turner 2019 and pyhspf lampert and wu 2015 to better understand how they are designed and structured doing this can help to inform the design and structure of future apis created to support specific environmental models we observed that model api functionalities fell into three categories model input model execution and model output table 2 for prms python as an example input files often have corresponding python modules that can be used for data manipulation for pyhspf as an example the python modules do not have a one to one correspondence with the core model files and modules instead the api designs include a higher level abstraction to consider core classes needed for interacting with the model from this review we suggest that communities of modelers e g researchers or groups of researchers who are considering building a model api for a specific environmental model begin with answering the following questions 1 what configuration and input files should be exposed through the api to allow for programmatic changes and what are the logical classes for organizing these model input configuration attributes 2 what methods and attributes should the api expose for executing the model and refining the model through for example calibration or sensitivity analysis 3 what are common visualizations of the model output that many users would wish to produce creating a model api with this functionality in a well thought through design will serve as a solid foundation for future extensions to the software furthermore the extent to which environmental model apis can adopt conventions for the organization of their design and structure will allow users to more easily learn new model apis by having some consistency across model apis 2 2 example implementation in this section we present one possible physical implementation of approach described in the prior section this example implementation uses hydroshare as the online repository cuahsi jh and cybergis jw the computational environments and pysumma as one of potentially many model apis within the system while this example implementation targets the needs of the hydrologic modeling community we anticipate that multiple other permutations of the technologies described in the prior section could be assembled to meet the needs of other environmental modeling communities 2 2 1 hydroshare as the online repository we used hydroshare as the data repository in our example implementation due to both its flexibility and tailored functionality for supporting environmental modeling use cases hydroshare is an online repository tailored for the needs of the hydrologic community but general enough to satisfy other environmental modeling needs tarboton et al 2014 hydroshare defines a resource as the fundamental unit of digital content in hydroshare that contains data and or model files and their corresponding metadata horsburgh et al 2016 hydroshare supports various data content types such as geographic raster geotiff multidimensional arrays netcdf geographic features shapefile and time series as aggregations within a single hydroshare resource hydroshare also supports collections as a resource type that holds a list of related hydroshare resources that can be referenced with a single unique identifier furthermore realizing that data associated with models have their own characteristics hydroshare defines specific resource types for a model program the software and a model instance the input and output files for a specific model run morsy et al 2017 resources with these two resource types are related through the executedby attribute of a model instance which points to the specific model program resource used to execute that model instance this design allows for a one to many link between a model program that is used to execute many different model instances built for different geographic locations or to address different research questions the methodology for sharing computational modeling resources is shown in fig 2 first the user creates a model program resource for each version of a model program software used in the analysis this resource can include the source code executable and container for the model program itself or a link to one or more of these resources shared in a system external to hydroshare e g in github binderhub or dockerhub second the model instance resources are created to store and describe the input data required to execute the model and can optionally store the output after the model is executed then the model instance is linked to a specific model program resource using the executedby metadata term a separate composite resource is used to store jupyter notebooks that describe the overall analysis workflow finally a collection resource is used to combine and conveniently share all of the resources used to complete the study 2 2 2 jupyterhub as the computational environment we integrated both the cuahsi jh and cybergis jw as computational environments in our example implementation we chose these environments because both are publicly available and aimed at scientific modeling in the water and environmental communities moreover both systems allow for seamless data transfer with hydroshare as a data repository supporting the necessary interoperability between these two components of the general framework this data transfer is enabled through the hydroshare rest api and the standardization of hydroshare resource data structures the cuahsi jh is a cloud computing environment on the google cloud platform specifically designed to support research and education in the water sciences fig 3 to support a variety of applications it leverages environment profiles that allow users to choose the ideal computing configuration for their work each of these profiles is a separate containerized environment that has been built with a specific set of software to support various water science use cases currently the cuahsi jh consists of seven profiles that range from scientific python and r to hydrolearn https www hydrolearn org educational modules and hydrologic modeling in addition the cuahsi jh supports persistent data meaning user created content is stored between sessions and shared between profile environments moreover this environment enables users to install custom software using conda virtual environments for this study we created a python 3 7 summa modeling profile to support summa 3 0 modeling environment using a dockerfile in cuahsi jh another model execution environment interoperable with hydroshare cybergis jw is a well tailored cybergisx https cybergisxhub cigi illinois edu instance to serve the fast emerging needs for data intensive and reproducible research in the environmental modeling community fig 3 overall cybergis jw is similar to cuahsi jh but cybergis jw also includes interoperability with advanced cyberinfrastructure resources such as virtual roger a cybergis supercomputer hosted by the cybergis center for advanced digital and spatial studies at the university of illinois and xsede comet an hpc resource on the extreme science and engineering discovery environment for model execution support lyu et al 2019 describe how to use htc through a jupyter notebook using summa as an example case in cybergis jupyter beta which is the previous version of cybergis jw currently cybergis jw is supporting landlab hobley et al 2017 and rhessys tague et al 2004 modeling environments for this study we created a summa modeling environment using a dockerfile users can use this summa modeling environment via a summa kernel for use of hpc resources cybergis jw requires a singularity image to support a computational modeling environment in xsede because cybergis jw and xsede are separately placed also cybergis jw needs a particular library to connect to computational resources for submitting jobs and data exchange in xsede 2 2 3 pysumma as the model api the model api pysumma was created through this research as an example model api pysumma wraps the hydrologic model structure for unifying multiple modeling alternative summa clark et al 2015a summa was selected for this study because it is a general hydrologic modeling environment offering the ability to conduct model experiments with controlled and systematic evaluation of multiple model representations of hydrologic processes and scaling behavior the summa model simulates both the thermodynamics the storage and flux of energy such as the heat balance of the vegetation canopy snow and soil affected by the radiative fluxes as well as the hydrology the storage and transmission of water for example vertical and lateral transmission of water through vegetation canopy snow soil aquifer and river within a catchment system the flexible hierarchical spatial structure of summa supports different spatial configurations including the size and shape of model elements with grouped response units grus kouwen et al 1993 and hydrologic response units hrus in addition the flexible structure enables researchers to consider the lateral flux of water across the model domain and complex topographical properties like hillslopes and riparian areas this flexibility within summa enables hydrologists to find solutions for the application of scaling behavior in relation to different physical processes summa also enables hydrologists to select the appropriate physical process methods and model complexity this process implements a modular structure that is supported by the conservation equations to calculate each process in a controlled and systematic way this unified process helps users to concentrate important physical parameterizations with higher complexity and conversely to simplify specific processes to minimize uncertainty according to the purpose and characteristics of biophysics and hydrology moreover the structure of summa which consists of a core solver and outer branches enables the output of a numerical solution from summa so that the user can evaluate the accuracy and efficiency of the model therefore summa supports flexibility to simulate different options of physical processes and numerical solutions we designed and implemented pysumma as a model api for summa using the questions proposed in section 2 1 3 for guiding the design of a new model api table 3 for the model input category there are six configuration files to manipulate summa input 1 file manager 2 decisions 3 forcing file list 4 model output 5 param trial and 6 local attribute files to expose the first four configuration files through the api we created file manger py decisions py force file list py output control py and option py for the rest of the configuration files we created assign trial params and assign attributes methods in simulation py in the model execution category we created simulation py to use the model execution command conveniently from the shell script format so that users do not need to edit manually every time we also created two options to execute the summa model local and docker to satisfy different user requirements finally the output format of summa is netcdf therefore we created plotting py for visualization considering the output variables and their output structure in netcdf the classes of pysumma are shown in fig 4 a simulation module simulation py is used as the initial python module to start a pysumma api and combine most pysumma functionalities such as manipulating configuration files and executing summa after creating a pysumma simulation object users can manipulate six configuration files a file manager module file manager py reads and manipulates a file manager file which controls the location of every configuration file for the summa model for example the file manager file sets the directory and configuration files including the decision forcing parameter and attribute files a decisions module decisions py reads and manipulates a decisions file which sets different physical process parameterizations through the available options object in decisions py users can determine what options are available for model parameterizations and select model parameterizations from a list of options for each physical process summa online document 2020 four input configuration modules file manager py decisions py force file list py and output control py have the same pattern of classes for example a file manager module file manager py is composed of filemanageroption and filemanager classes and a decisions module decisions py is composed of decisionoption and decision classes each class is connected to an option module option py to avoid repetition of functions such as comparing setting and writing each configuration file after setting the summa configuration the simulation module simulation py is used for model execution the run method of the simulation class is used to execute the summa model this execution can be done in both local and docker computational environments the environments are set using the run option parameter for the run method as discussed later in the results and discussion section once a summa model run has been completed the plotting module plotting py can be used to visualize the results there are two different data output structures for summa 1 time hru or gru number and variable 2 time hru or gru number soil or snow layer number and variable to visualize each of these output structures the plotting class consists of three methods ts plot ts plot layer and heatmap plot we used the seaborn library statistical data visualization library to create a 2d heat map with soil or snow layer and time as the axis for displaying a selected variable lastly the model output module output control py is used to manipulate the output variables of summa and the utilities module hydroshare utils py has functions to download test cases of summa model instance resources and execution files model program resources from hydroshare 3 results and discussion in this section we present a modeling case study application of the example implementation system described in section 2 2 then we discuss how this approach addresses the challenge of achieving more reproducible studies summarized in the introduction section by evaluating the approach against definitions concepts and metrics for reproducibility proposed by others lastly we discuss the limitations of our approach that present opportunities for future research 3 1 case study description clark et al 2015b describe a set of thirteen modeling experiments exploring various hydrologic modeling scenarios using summa the study area for these modeling experiments is the reynolds mountain east area a 32 7 km2 in the reynolds creek experimental watershed in idaho usa fig 5 in this paper we focus on these modeling experiments as a case study with the goal of applying our approach so that each clark et al 2015b experiment can be reconstructed and shared openly in a way that is easier to reproduce the first step toward this goal is the creation and organization of hydroshare resources to share all models and data files required for the analysis the second step is to create jupyter notebooks that describe the modeling experiments these notebooks include text and equations to describe the modeling experiments while also including executable python code using the pysumma api and inline visualizations that can be repeated and extended by others we created seven jupyter notebooks each one documenting an experiment in the clark et al 2015b study and published them through hydroshare as a collection resource choi et al 2020 3 2 model and data resources our first step in reproducing the clark et al 2015b modeling experiments was to publish the specific summa model version used in the analysis as a resource on hydroshare to do this we created a hydroshare resource using the model program resource type and uploaded the summa 3 0 0 source code to the resource we then published the resource through hydroshare so that it is persistent and immutable with a unique digital object identifier doi choi et al 2020 fig 6 shows the landing page for this resource on hydroshare that includes detailed metadata describing 1 the source code and compiled software engine 2 metadata for the software 3 a link showing the model was derived from a particular branch of a github repository for summa and 4 a citation for referencing the resource this same summa 3 0 0 was also installed on the cuahsi jh allowing users to execute the summa model directly from cuahsi jh we next created multiple resources in hydroshare to store the specific model inputs for each different summa model experiment used in the clark et al 2015b paper there were four synthetic and nine field study test cases available as an online supplement to the clark et al 2015b paper from these data we created seven unique model instance resources in hydroshare table 4 and grouped them into a collection resource choi et al 2020 each model instance resource includes 1 input data for the summa model 2 a reference to the clark et al 2015b paper 3 a composite resource link that points to the jupyter notebook used to execute the summa model and 4 a link to the model program resource used to execute the model instance once this step is complete the model and data resources required to reproduce the clark et al 2015b experiments are publicly accessible in hydroshare with metadata to describe each resource and a unique url to locate each resource hydroshare also allows for publishing these resources in which case the resources become immutable and are assigned a digital object identifier doi this pattern can be adopted by other environmental modeling studies whereby both the model and data resources required to reproduce the study are uploaded into hydroshare given metadata to describe each resource including relationships between resources such as the executedby relationship between model program and model instance resources and published with a doi 3 3 demonstrating reproducibility this section describes the steps that should be taken to reproduce one of the experiments described in clark et al 2015b as a preparation step before starting a summa simulation using jupyter notebooks on cuahsi jupyterhub we recommend creating a pysumma conda virtual environment by running the steps described in the notebook creating pysumma conda virtual environment in cuahsi jupyterhub ipynb in the hydroshare composite resource for cuahsi jh notebooks once this preparation step is completed the basic algorithm to run a notebook is shown in fig 7 first the pysumma hydroshare utils module is used to download the model instance that will be used in the notebook directly from hydroshare after downloading the summa model instance it is possible to create a pysumma simulation object using the simulation class of pysumma and supplying summa executable summa exe and the file manager file path after creating the pysumma simulation object the summa model can be executed using the run method which takes a run option argument as local when cuahsi jh was created by using docker summa was automatically complied and the summa executable was created in usr local bin summa exe therefore after setting the executable variable to the location of summa exe users can set a run option as local by changing the executable variable as usr bin summa exe it is possible to execute the same notebook on cybergis jw as an example we present here the results from running two different experiments included in the clark et al 2015b paper the first reproduces fig 7 from clark et al 2015b and is published as a hydroshare resource with the title the impact of stomatal resistance parameterization on et of summa model in aspen stand at reynolds mountain east choi et al 2020 the second reproduces fig 8 left from clark et al 2015b and is published as a hydroshare resource with the title the impact of root distributions parameters on et of summa model in aspen stand at reynolds mountain east fig 8 gives the results from the first experiment that explores the impact of three different stomatal resistance parameterizations on total evapotranspiration ball berry ball et al 1987 jarvis 1976 and the simple resistance method fig 8a is the original result from the summa paper clark et al 2015b and fig 8b is a reproduced figure resulting from applying this framework these stomatal resistance parameterizations have different physical characteristics both the jarvis and ball berry methods consider photosynthesis while the simple soil resistance method mainly considers the soil water conditions the results show that the simple soil resistance method is higher than the other methods during the night hours comparing the two plots shows the complexity associated with reproducing past computational modeling studies while the results are consistent they are not exact the precise reason for the differences in the model results is difficult to determine we suspect it is due in part to upgrades to summa or summa dependencies between the versions of the summa 2 0 used in the clark et al 2015b paper and the summa 3 0 used to create the newer plot more vexing is that some of the observed data points appear to have shifted with no good explanation for why one possible explanation could be the fact that different visualization tools were used to create each plot interactive data language idl for the plot on the left and matplotlib for the plot on the right we suspect differences like this would not be uncommon when trying to reproduce any past computational study given the difficulties in recreating the exact computational and analysis environment including data preparation routines computational modeling software and post processing analysis and visualization tools this in fact speaks to the difficulty of the problem and the need for innovation in cyberinfrastructure approaches that is at the heart of this study this said it is also important to stress that the goal of reproducibility may not be to obtain the exact same results but rather consistent results that would produce the same conclusion this is an idea expressed by high level reports on computational reproducibility national academies of sciences 2019 that we will discuss further in section 3 4 fig 9 shows the results from the second experiment from clark et al 2015b which explores the impact of the root distribution parameters with different stomatal resistance parameterizations for total evapotranspiration in this case we reproduced the plot that shows the impact of root distribution parameters fig 9b and compared it to the previous result fig 9a again we see consistent although not exact results between the two model runs given that the modeling experiment is now implemented within the system it is also possible to more easily extend and repurpose it for other purposes to this point we demonstrate reuse of past modeling studies by creating two additional plots for determining the effect of different root distribution fig 9c and stomatal resistance parameterizations fig 9d on total evapotranspiration these plots show how higher root distribution exponents in the soil profile indicate that the roots are deeper in the soil which makes it easier for plants to extract soil water as a result during the higher evapotranspiration periods 10 00 17 00 the jarvis method more closely matched the observation data however during the period when evapotranspiration is decreasing 17 00 20 00 the ball berry method was more precise compared to the simple resistance method over the complete time period the analysis shows that the jarvis method had the best fit with observations 3 4 evaluating reproducibility to evaluate if reproducibility was achieved we considered definitions and concepts for evaluating reproducibility being put forward by others for example the national academies of science engineering and medicine national academies of sciences 2019 define reproducibility focused on computational reproducibility as obtaining consistent results using the same input data computational steps methods and code and conditions of analysis to guarantee reproducibility the organization recommended delivering clear specific and complete information about any computational methods and data products to repeat the previous study and that information should include the data methods and computational environment fair principles wilkinson et al 2016 include 15 metrics that should be met as a minimum for reproducibility these metrics are a findable 4 metrics b accessible 4 metrics c interoperable 3 metrics and d reusable 4 metrics in the hydrology and water resources fields hutton et al 2016 recommended reproducible studies have 1 readable and reusable code 2 an unambiguous workflow 3 a repository to easily find data and code with associated metadata 4 use of unique persistent identifiers 5 new procedures to reproduce large scale studies using hpc additionally hut et al 2017 suggested the use of containers and open interfaces to guarantee stronger reproducibility as a response to hutton et al 2016 finally stagge et al 2019 proposed a set of survey questions to assess the reproducibility of a journal article the survey requires that eight elements be available for a study to be called reproducible 1 directions to run or reproduce the study 2 code model software files 3 input data 4 hardware software requirements 5 stated data persistence policy 6 materials linked by unique and persistent identifiers 7 metadata to describe the code and 8 common file format and instructions to open these files with these criteria in mind by simply using hydroshare as the data repository for all data and software used for the study we can support many of the metrics associated with reproducibility hydroshare supports fair principles tarboton et al 2018 for each resource that includes model input source code metadata and supplementary documents using jupyterhub as described in the paper provides a consistent computational environment and using jupyter notebooks and containerized model execution environments provides a clear and easy workflow to assure users can reproduce a published study finally using a model api makes it easier for a user to follow the logic and steps used to configure run and postprocess a modeling simulation allowing for not only reproducibility but also reuse and extension of prior work therefore if we compare these definitions and concepts for a validation of reproducibility to our approach and its example application we can claim that it satisfies the criteria for reproducible computational modeling still while the framework allows for satisfying the criteria it is still up to the user to ensure care is taken with sharing and documenting resources with adequate metadata and instructions to achieve reproducibility 3 5 approach limitations and opportunities for future research this research focuses on examples that assume model input files had already been processed and are available for use in the modeling analysis the preprocessing steps required to generate model input files from raw geospatial and time series observational data are a necessary component of longer term goals for creating so called end to end reproducible analysis workflows for example slater et al 2019 provided an end to end reproducible hydrology workflow using r for climate data retrieval spatial analysis modeling statistical analysis visualization and data publishing as another example of automated end to end workflows hydroterre leonard 2015 includes 1 data workflows leonard and duffy 2013 to create watershed models using essential terrestrial variables etv 2 data model workflows to transform watershed data into model inputs 3 model workflows leonard and duffy 2014 to execute models in hpc especially the penn state integrated hydrologic modeling system pihm and 4 visualization workflows to visualize the first three workflows to easily create and share model results for analysis currently pysumma has developed the functionalities of manipulating created model input executing summa and plotting model output to complete end to end workflows data preprocessing is critical for improving reproducibility as the steps to create model input files are often nontrivial and require a significant time investment prior work to address this challenge includes the ecohydrolib python library developed as a software framework for managing spatial data acquisition and preparation workflows for ecohydrology modeling miles and band 2015 ecohydrolib takes advantage of open source grass gis libraries to automate data gathering and preparation for environmental models it is a model agnostic approach for mapping a variety of data sources into input files required by environmental models alternative data processing workflows and pipelines such as hydroterre leonard 2015 could also be explored for bringing data preprocessing capabilities for environmental models into the general approach described through this work however just having new data processing pipelines alone will be insufficient we also need more detailed modeling protocols and procedures to replicate or even reproduce a study ceola et al 2015 because reproducibility is not just a technological problem it is equally an educational problem grüning et al 2018 post processing for visualization and model analysis procedures is also essential to creating a powerful modeling environment saving time when analyzing model output and strengthening reproducibility to grow use of model apis many analysis methods will be necessary such as plotting calibration optimization and uncertainty analysis while pysumma is still being developed toward these goals other model apis discussed in this paper and that could be used within the example modeling system do have more robust processing capabilities already one question that remains is the extent to which environmental model apis can reuse underlying software to support common model post processing routines general libraries in python such as pandas and matplotlib are universally applicable to environmental modeling post processing tasks however is a plotting or data analysis library more tailored for environmental modeling but still sufficiently general to serve many environmental models possible if so it could further reduce the duplication of code across environmental model apis and ultimately encourage more environmental model apis that are robust easier to maintain and feature rich the ability to include data pre and post processing within the framework would be an important step for moving from reproducibility to replicability within the framework replicability is defined by the national academies of science engineering and medicine national academies of sciences 2019 as obtaining consistent results across studies aimed at answering the same scientific question each of which has obtained its own data replication therefore can be thought of as a next step beyond reproducibility where a study is repeated using new data potentially from a new site or different time period but similar methods this work has focused on a general approach to support reproducibility of computational models the framework could be extended for replication by extending a model api like the pysumma api described in this paper to include not only functions for model configuration e g settings and parameter values assuming model input files have already been generated but also for model preprocessing where input files for the model are generated from raw data sources 4 conclusion computational irreproducibility is an important problem in many scientific fields recent research to improve computational reproducibility has focused on advancing the sharing of data used in studies using computational notebooks and containers for encapsulating complete computational environments and developing model apis for programmatically interacting with simulation models a contribution of this research is to present a general approach to integrate these three areas of past work into a general approach for supporting more open and reproducible environmental modeling we present an example implementation of this approach by leveraging 1 hydroshare as a data sharing repository 2 jupyterhub as a notebook based containerized and cloud based computational environment and 3 pysumma as an example model api able to abstract lower level details for model configuration execution and visualization from end users using the example implementation we demonstrate how modeling analyses can be completed in a more open and reproducible way building from a prior study presenting a series of modeling experiments applying summa at the reynolds mountain east area in the reynolds creek experimental watershed in idaho usa clark et al 2015b we first created and organized hydroshare resources to share data and model files next we created jupyter notebooks that leveraged the pysumma api introduced in this paper to reproduce and extend figures from the prior study each notebook a pulled required data from hydroshare into the computational environment b provided a notebook using text equations code and inline visualizations for documenting the experiment and c allowed for online execution of the notebook and sharing of modifications to the notebook through hydroshare finally we discussed how we evaluated that reproducibility was achieved and future steps that could be taken to further improve the proposed framework from this research we conclude that cyberinfrastructure is reaching a point where it is possible to build open and transparent environmental modeling systems online repositories are sufficiently mature where they can be relied upon for storing key data and software resources for studies computational environments able to execute containerized environmental models can be interlinked with data repositories and the ability for these computational environments to serve as gateways to high performance computing hpc resources is improving more models are being provided with apis that allow for programmatic control of the model configuration execution and visualization jupyter notebooks provide an important orchestration and documentation glue across these components where users can leverage apis to access and publish data from online repositories submit jobs to hpc resources and programmatically interact with state of the art environmental models linking these capabilities in a way that can be built upon and expanded as new models become available as demonstrated in this paper will move environmental modeling in a direction where open transparent reproducible reusable and replicable studies become the rule rather than the exception software and data availability all software and data used in this study were published with persistent dois on hydroshare a collection resource in hydroshare choi et al 2020 contains each of these resources in addition to these resources published through hydroshare the pysumma source code created though this study is available on github as detailed below product title pysumma v3 0 0 lead developers young don choi and andrew bennett contact email yc5ef virginia edu andrbenn uw edu tested platform hydroshare cuahsi jupyterhub cybergis jupyter for water software required python 3 5 or above availability the pysumma source code is publicly available through github https github com uw hydro pysumma releases tag 3 0 0 license bsd 3 clause license list of relevant urls cuahsi jupyterhub https jupyterhub cuahsi org cuahsi jupyterhub legacy environment https jupyter cuahsi org cuahsi jupyterhub github https github com hydroshare hydroshare jupyterhub cybergis jupyter beta https hsjupyter cigi illinois edu 8000 cybergis jupyter for water https go illinois edu cybergis jupyter water dataone https www dataone org docker https www docker com dockerhub https hub docker com ecohydrolib https github com selimnairb ecohydrolib facebook api https developers facebook com docs apis and sdks figshare https figshare com google api https developers google com apis explorer harvard dataverse https dataverse harvard edu hydroshare rest api https github com hydroshare hydroshare wiki hydroshare rest api netcdf4 github https github com unidata netcdf4 python numpy https www numpy org pandas https pandas pydata org seaborn https seaborn pydata org singularity https sylabs io summa on the ucar https ral ucar edu projects summa xarray http xarray pydata org xsede https www xsede org declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the national science foundation under collaborative grants oac 1664061 oac 1664018 oac 1664119 icer 1928369 and icer 1928315 we acknowledge the work of the larger hydroshare team https help hydroshare org about hydroshare team that made this research possible 
