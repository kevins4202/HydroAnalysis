index,text
1955,seasonal hydrological forecasts play a critical role in water resources management the copernicus climate change service c3s data store provides open access to monthly hydrological forecasts for up to six months this study aims to evaluate for the first time 1 to 3 month runoff forecasts using the european centre for medium range weather forecasts ecmwf ensembles of precipitation runoff and temperature in 1981 2015 period over a total of 30 s level basins in iran we adopted the 5th 50th and 95th ecmwf ensemble quantiles for each variable that represent low medium and high probability of occurrence respectively pearson correlation analysis pca recursive feature elimination rfe via random forest rf model and bayesian networks bn feature selection algorithms were used in order to reduce input variable dimension and select potential predictors to be fed to the machine learning models multiple linear regression mlr artificial neural networks ann support vector regression svr random forest rf and extreme gradient boosting xgboost machine learning models were used with repeated k fold cross validation rk fold cv while model efficiency was evaluated using modified kling gupta efficiency coefficient kge nash sutcliffe efficiency coefficient nse and normalized root mean square error nrmse results of this study revealed that c3s runoff ensembles have the highest impact on forecast accuracy of streamflow followed by precipitation and temperature overall model performance yield a best to worst ranking of ann xgboost rf mlr and svr with kge values of 0 70 0 68 0 66 0 57 and 0 41 respectively the predictive performance of all models decreased with lead times beyond 1 month where ann and xgboost outperformed other models with kge of 0 65 for 2 month lead time and 0 60 for 3 month lead time the three superior models of xgboost ann and rf were employed with rfe and bn fsas most frequently across iran s 30 s level basins in all lead times almost all models in the arid central region of iran showed the lowest performance while highest skills were achieved in the western regions of iran finally for all models and over all regions the model performance reduced by increase in lead time keywords streamflow forecast c3s data store ecmwf ensemble recursive feature elimination rfe bayesian networks bn machine learning ml data availability data will be made available on request 1 introduction given global water scarcity particularly in the past decade it is crucial to adopt the best water resource management practices to handle or avert consequent water crises greve et al 2018 an important factor in water resource management is the accurate estimation of streamflow in order to plan for available water resources sharma machiwal 2021 long term forecasts include weekly monthly seasonal and even annual predictions and are crucial for operation of reservoirs irrigation management systems and hydropower generation liang et al 2018 improving the accuracy of long term forecasts is significantly dependent on improvements in availability and sustainability of climate datasets and modeling tools wegayehu and muluneh 2022 recent advances in meteorological forecasts open up new opportunities for improving long term hydrological forecasting capabilities kilinc haznedar 2022 long term forecasts are more complex to simulate compared to short term forecasts karimi et al 2016 the practical gap in access to accurate long term runoff predictions pushes research toward reliable hydroclimatological forecasts particularly given the benefits received from even a slight increase in the accuracy of these forecasts to water resource management cheng et al 2020 ensemble hydroclimatological forecasts are used to numerically produce a range of forecasts based on possible future atmospheric conditions unlike single predictions ensemble forecasting provides a more robust prediction by generating the range of possible outcomes and evaluating the confidence in future system states hawcroft et al 2021 a prominent provider of ensemble forecasts is the copernicus climate change service c3s multi system seasonal forecast https cds climate copernicus eu the service which has recently released several european center forecasts is run on behalf of the european union by ecmwf and offers seasonal forecasting protocols publicly available through the climate data store the database combines observations of weather systems and provides holistic past present and future information on atmospheric conditions ingleby 2015 lopez 2013 given the recent launch of the c3s database research on its evaluation is very limited in particular over iran the evaluation of performance of ensemble forecast can provide context to decision makers strategic choices for overcoming climate related risks nobakht et al 2021 crochemore et al 2017 evaluated precipitation and river flow forecasting performance in 16 basins in france the study evaluated and post processed seasonal precipitation forecasts from the open library of ecmwf system 4 with 90 day lead time they used linear scaling ls and different distribution mapping methods for post processing of monthly and annual raw precipitation data the results demonstrated that applying post processing techniques increase precipitation forecasting accuracy manzanas et al 2019 applied simple bias adjustment ba methods such as quantile mapping as well as complicated ensemble recalibration rc methods such as non homogenous gaussian regression to increase the accuracy of c3s forecasted precipitation and temperature in particular they evaluated uk met office ukmo glosea5 maclachlan et al 2015 météo france system5 descamps et al 2015 and ecmwf seas5 johnson et al 2019 seasonal forecasts with one month lead time the results revealed that both ba and rc methods correct large raw model biases effectively with high model skill in confined regions and seasons in these instances rc bias correction outperformed ba methods manzanas et al 2019 in another study gebrechorkos et al 2022 evaluated the performance of precipitation forecasts from five potential climate models namely ecmwf uk met office météo france deutscher wetterdienst kaspar et al 2015 and centro euro mediterraneo sui cambiamenti climatici nicolì et al 2023 multi source weighted ensemble precipitation was used as the reference data for model performance evaluation model performance was evaluated in daily weekly monthly and seasonal timeframes and at specific months and lead times all models showed reliable predictions for 1 month forecasts however they showed rapid decline in performance with increase in lead times in particular in drier regions and seasons it was found that ecmwf followed by uk met were the most accurate models among other c3s products gebrechorkos et al 2022 due to machine learning ml algorithms significant nonlinear modeling capabilities in complex problems they have been widely used for the monthly prediction of streamflow ali shahbaz 2020 the data preprocessing feature preprocessing ml algorithm selection and hyperparameter optimization stages are typically included in the machine learning based prediction process for streamflow prediction one of the more common forms of data driven models multiple linear regression mlr has been shown to perform effectively for long term forecasts krstanovic singh 1991 however due to the nature of the process it makes the assumption that the relationships between input and output data are linear and the data has no multicollinearity whereas streamflow prediction is a highly nonlinear process and depends on a variety of known and unknown factors sudheer et al 2014 when it comes to model selection the artificial neural networks anns kilinc haznedar 2022 deep neural networks dnns apaydin et al 2021 he et al 2022 kao et al 2020 2021 maddu et al 2022 support vector regression svr ni et al 2020 random forest rf tyralis et al 2021 adaptive boosting adaboost liu et al 2014 light gradient boosting method lgbm szczepanek 2022 and extreme gradient boosting xgboost ni et al 2020 are frequently used for streamflow prediction both dt and gradient boosting gb are utilized by xgboost it has many benefits including its predictive algorithms are straightforward but still effective simple to understand and don t need as much data preparation ni et al 2020 artificial neural network ann s ability to process and model complex nonlinear time series has recently enabled its widespread applications in hydroclimatological studies in contrast to application of physically based models kilinc haznedar 2022 the ability to work with large amounts of noisy data from nonlinear and dynamic systems especially when the fundamental physical relationships are unknown is one of the advantages ann has over traditional modeling anusree varghese 2016 ann has been successfully applied in various hydrological simulations known as an efficient and accurate tool in forecasting streamflow precipitation and water quality kilinc haznedar 2022 rf is a decision tree based model that addresses the overfitting issues with single decision trees while maintaining their prediction accuracy the rf approach in contrast to ann and svm offers excellent computing speed while being simple to use schoppa et al 2020 multiple studies have reported complexities associated with flow forecasting driven by the natural complexity nonlinearity and randomness of river systems smith et al 2007 in order to reduce the number of predictors different linear or non linear feature selection algorithms fsas such as pearson s correlation analysis pca djibo et al 2015 recursive feature elimination rfe ferreira et al 2021 and bayesian networks bn das et al 2022 are commonly used in terms of ml algorithm calibration the optimization of hyperparameters has a significant impact on the performance of ml models szczepanek 2022 as a result to optimize hyperparameters for ml algorithms researchers have used different methods such as grid search gs lavalle et al 2016 or metaheuristics malik et al 2020 in iran ml algorithms have also been used in many hydroclimatological studies for example forecasts from ecmwf ukmo and national centers for environmental prediction ncep were evaluated for 13 synoptic stations in eight precipitation zones aminyavari et al 2018 the evaluation was based on precipitation data from 2008 to 2016 with a 1 to 3 day lead times their results showed decrease in model performance with increase in lead time while different models performed differently over various regions i e ecmwf in most regions ukmo in mountainous regions and ncep around the persian gulf kolachian and saghafian 2019 used a number of deterministic and probabilistic criteria to evaluate the performance of precipitation forecast for ecmwf sub seasonal to seasonal s2s model with 1 month lead time for several synoptic stations and precipitation regimes in iran they found acceptable performance in wet months in most regions however absence of reliable raw forecasts in dry months was evident no significant relationship was found between the precipitation regime and prediction skill furthermore their results showed forecasting and post processing capabilities vary significantly in different seasons and locations nobakht et al 2021 evaluated the ensemble precipitation forecasts of ecmwf ukmo and météo france c3s models over 1993 2017 period in iran s eight classified precipitation clusters with 1 to 3 month lead times probabilistic and non probabilistic criteria were used for the evaluation the results indicated all models performed better in the western precipitation regions while in the northern region with humid climate models had poor skill scores all forecasts were better at predicting upper tercile events in dry seasons and lower tercile events in wet seasons moreover with increasing lead time the forecast skills worsened in terms of forecasting in dry and wet years the predictions were generally close to observations albeit they underestimated several severe dry periods and overestimated a few wet periods in another study meydani et al 2022 applied weather forecast downscaling and rainfall runoff modeling for daily reservoir inflow forecasts in the urmia lake basin in iran they utilized large scale weather forecasts from ecmwf and ncep to evaluate various downscaling methods including artificial intelligence ai and bayesian belief network bbn techniques to derive local reservoir inflow forecasts the results showed a hybrid downscaling approach that combined group method of data handling gmdh and support vector regression svr performed better than their non hybrid counterparts in downscaling precipitation furthermore the authors found bbn to outperform hybrid ai in forecasting the dynamics of precipitation in observed datasets in this study for the first time the performance of ecmwf runoff forecasts over iran is comprehensively assessed by evaluating the monthly ecmwf s seasonal forecasting system 5 seas5 and its potential in predicting streamflow over 30 s level basins in iran subject to different climate and hydrological regimes based on a set of initial potential predictors and their association high correlation and statistical significance with the target monthly streamflow a set of potential predictors is established the potential predictors are then put through three feature selection algorithms fsa including bayesian networks bn recursive feature elimination rfe and pearson correlation analysis pca to create a set of optimal predictors for each scenario these optimum predictors served as inputs for different ml algorithms the study is also the first to use ecmwf runoff forecasts to predict monthly streamflow in iran according to the authors knowledge additionally this study investigates the use of five ml algorithms notably xgboost a novel intelligent method based on the gradient boosting algorithm to forecast the streamflow across iran s 6 major and 30 s level basins 2 materials and methods this section describes the study area datasets pre processing and modeling techniques to develop streamflow forecasting with lead times from 1 to 3 months over iran the data used are from the latest generation of ecmwf s seasonal forecasting system 5 seas5 and consist of ensemble forecasts of precipitation runoff and temperature from 1991 to 2015 with 1 to 3 month lead time in order to reduce the dimension of input variables pearson s correlation analysis pca recursive feature elimination rfe and bayesian networks bn are used mlr ann rf svr and xgboost models with rk fold cross validation are adopted to model runoff in 30 iranian basins lastly recommendations are proposed for runoff forecast for different basins 2 1 study area iran with a total area of 1 648 195 km2 is located in the mid latitude northern region between 25 n and 40 n latitude and 44 e and 64 e longitude in southwest asia other than western and northern coastal areas iran s general climate is dominated by a mostly arid and semi arid character precipitation varies mainly with latitude and topographical altitude of the region mansouri daneshvar et al 2019 the whole country may be divided into six major basins that are broken into a total of 30 s level basins saatsaz 2020 fig 1 depicts the basins rivers and location of the hydrometeorological stations used in this study in total 571 hydrometric stations in the country have at least 240 months of data throughout the 1981 2015 study period in this study however we selected the stations with the highest elevation in each basin resulting in a total of 30 selected stations shown in fig 1 table 1 provides characteristics of the basins including basin name basin code area and climate class according to köppen geiger climate classification raziei 2022 2 2 datasets and pre processing 2 2 1 hydro meteorological and ensemble seasonal forecast data the observed station data spans the 1946 to 2015 period monthly averages of precipitation and runoff are shown in fig 2 for all 30 basins in percentage c3s data store launched in 2017 by ecmwf regularly releases seasonal forecast products the data store relies on forecasts frommultiple organizations across europe and updates monthly forecasts with up to six months lead time the data includes forecasts created in real time since 2017 and retrospective forecasts hindcasts initialized at equivalent intervals during the 1981 2016 period this study investigated monthly c3s precipitation runoff and temperature ensembles with 1 to 3 month lead time for the time period of 1981 2015 over the 25 40 n 44 64 e geographical area with a grid size of 0 25 0 25 with approximately 25 km spatial resolution the correlation coefficient between seas5 raw runoff average precipitation and temperature outputs with observed values over each basin is presented in fig 3 for 1 to 3 month lead time it shows runoff ensemble demonstrates the highest correlation withobserved values followed by precipitation 2 2 2 pre processing of raw product in order to work with ecmwf seas5 ensemble outputs the quantile classification methodwas appliedin a way that for each variable thecorresponding 5th 50th and 95th quantiles were extracted to represent low medium and high probability of occurrence values respectively these quantile values form potential predictors to drive machine learning ml models golian et al 2010 2011 used a similar quantile classification to derive rainfall thresholds with different probability of occurrence through a monte carlo framework fig 4 illustrates the end to end data preparation process implemented in this study starting with data retrieval from the ecmwf data store to preparation for runoff simulation ensemble precipitation runoff and temperature hindcasts were retrieved for each cell over the entirety of iran from cds website https cds climate copernicus eu for lead times from 1 to 3 months next data in gridded binary format grib were converted and saved in a comma separated values csv format these converted files include information on precipitation runoff and temperature containing all 25 ensembles with 1 to 3 month lead time then precipitation runoff and temperature were extracted for each cell and 5th 50th and 95th quantiles were extracted along with the average of ensembles for each cell inside a particular basin represented by ens 5 ens 50 ens 95 ens mean finally for each variable and across all cells in a basin the 5th 50th and 95th quantiles and the mean values were calculated for each basin represented by cell 5 cell 50 cell 95 cell mean 2 3 methods 2 3 1 selection of predictors given the large number ofpotential predictors i e 36 for each lead time dimension reduction of the input matrix is needed to simplify the model and reduce calculation time the feature selection algorithms used to select the optimum predictors has some limitations such as the risk of overfitting when the data size is small the high computation time when the variables are many and the trade off between reducing variance and increasing bias by eliminating some relevant features munson caruana 2009 predictor selection is herein implemented through two methods of highly correlated variable exclusion and elimination by importance where via a high correlation filter highly redundant predictors are identified and removed kuhn johnson 2013 in pearson correlation analysis pca method the cross correlation value is quantified between all potential predictor variable pairs simultaneously using pca each predictor pairing with a pearson s correlation of more than 95 is compared with other predictors keeping one predictor among those with the highest correlation ferreira et al 2021 thus for each predictor pair with a high pearson correlation those with the highest correlation with other predictors are dropped from further analyses this eliminates redundant input variable predictors that are already captured by other variables the second method elimination by importance is implemented for predictor selection by eliminating redundant and irrelevant inputs the predictability and robustness of machine learning methods are increased while reducing the computational costs a suitable method to identify the most important feature is the recursive feature elimination rfe algorithm in this approach all possible combinations of predictive variables are used to apply the models whereby the explanatory power of each predictor is identified through rfe the algorithm then repeatedly eliminates variables below an importance criterion admitted by the models in each step of searching guyon et al 2002 kuhn johnson 2013 overall rfe is performed on all variables considering 2 to 36 predictors as possible inputs to the simulation models selection of the optimal predictor set based on the leave one out cross validation loocv method is tested using rmse as a performance criterion for each set the ideal predictor set is selected as one with the least rmse and fewest predictors this study has used rfe algorithm based on random forest rf model directed acyclic graph dag is used to define the bayesian network bn it offers the joint probability distribution for a set of random variables and connected nodes that represent these random variables bn demonstrates the causal relationship or nature between pairs of such variables dutta maity 2020 structure learning is necessary to develop and understand this structure which comes in three major categories score based constraint based and hybrid algorithms the score based hill climb search algorithm is one of the most common approaches taken to discover network structures and identify the best predictors scutari 2017 this algorithm starts with a saturated graph and compares the score to the maximum score for each potential addition deletion or reversal before creating a top scoring network for the bn in this study the library causalnex beaumont et al 2017 was utilized for creation of bn models and plotting dag graphs with bayesian information criterion scores being employed to choose the best prediction model leu bui 2016 once established it can be analyzed in order to determine if variables are dependent or conditionally independent from one another based on the relationship to target variable this aids in the selection process of inputs for machine learning models that comprise potential predictors directly related to targets in essence through use of dag s and bns one is able to analyze data sets accordingly this provides insight into cause and effect between variables while identifying key information that can be relayed back into ml models as best predictors possible under given conditions 2 3 2 simulation models five state of the art ml models were used to develop models for the prediction of monthly streamflow in 30 s level basins of iran below is a brief description of each model the selected five ml algorithms mlr ann svr rf and xgboost are widely used for streamflow prediction the first technique employed is the multiple linear regression mlr model possibly the most recognizable data driven forecasting technique mlr creates a linear relationship between a continuous dependent variable y and one or several independent variables xi regression is the most widely used to identify variables xi with a relationship to the output y araghinejad 2014 for the mlr model implementation 70 of the available data is used for training and 30 used for testing of the model another simulation approach applied in this research is the artificial neural network ann with a unidirectional multilayer structure consisting of an input layer multiple hidden layers and an output layer each layer consists of several neurons connected to adjacent layers signals entering through the input layer areunidirectionally directed toward the output layer in ann the following transformation is applied for each neuron during signal flow in network 1 y f i 1 n x i w i b i such that y is the output xi is the input n is the number of inputs wi is the weight bi is the bias and f is the activation function of each input used to approximate any mapping between model inputs and outputs furthermore this function normalizes neuron outputs to prevent extreme output values after several layers torre et al 2020 the number of input layer neurons is based on the number of predictors considered for each basin the number of hidden layer neurons is evaluated by forming networks of 5 to 15 neurons and selecting their optimal number by trial and error the ratio of data used for training validation and testing are 56 14 and 30 respectively the minimum network error for simulation cutoff was 0 000001 with maximum number of training failures of 30 and with levenberg marquardt training functions random forest rf fundamentally incorporates over fitting and local convergence issues into multiple classifier forests through a single classifier decision tree dt belgiu drăgu 2016 the method used for resampling takes multiple samples from the original dataset trains a dt for each bootstrap sample combines these dts and averages the predicted values for all the combined dts the prediction value of the i th dt yi is evaluated using equation 2 with n representing the number of dts x is the inputs and y as the prediction result of the rf model 2 y 1 n i 1 n y i x another model employed in this study is the support vector regression svr this model was created by generalization of the support vector machine svm capabilities from classification to regression problems noble 2006 svr inherits the core concepts of data fitting from svm and has been widely applied in multiple areas recently while svm maximizes the distance to the sample point closest to the hyperplane the svr finds a hyperplane that minimizes the distance to the sample point furthest from the hyperplane by turning the process of finding a hyperplane into a convex quadratic programming problem and solving it svr was able to realize nonlinear data modeling and obtain the hyperplane the final model employed in this study is a new intelligent algorithm for predicting streamflow based on the gradient boosting model recently extreme gradient boosting xgboost has received praise in academia for its efficient performance efficacy and fast speed chen guestrin 2016 in this method a dt is selected by xgboost as a weak learner while training a weak learner the algorithm increases the weight of previously misclassified data marginally identifies the next weak learner and adds another weak learner to correct the residuals of all previous weak learners the result is finally obtained through the weighted sum of learners by training the xgboost based model using the collected data a strong learner can be used to predict the streamflow values obtained rk fold cross validation method is used to validate the developed models 2 3 3 repeated k fold cross validation once produced based on training data the model accuracyis assessed using the test dataset a classic validation method for machine learning methods is cross validation here employed by separating the dataset into its training and testing sets high model accuracy in the training phase does not by default guarantee a similar performance with new data emphasizing the importance of balancing between generalization and overfitting of the model outputs model underfitting impliesthe model lacks sufficient performance in both the training and testing phases most likely this is because the model is not well tuned or trained enough on the training set resulting in high bias and low skills model overfitting meansthe model is too far tuned in the training set as a result the model performs well on the training set but poorly onthe test phase resulting in low bias and high variance berrar 2019 cawley talbot 2010 the most important issue with separating data into training and test sets is that test datasets might not follow the distribution of the whole dataset the k fold cross validation process as illustrated in fig 5 solves this by sampling all data in k rounds k is defined as the number of folds and is typically between 3 and 10 but can also be any positive integer the data is then divided into k equal parts the algorithm in k 1 step selects different groupings of folds for testing and separates the remaining folds for the training dataset with this method it is possible to train the model k 1 times independently and measure performance scores k 1 times based on selected criteria finally the average of all scores is evaluated estimating model performance through k fold cv may result in noisy estimates this is due to the fact that every time the procedure is performed a new division of the data enters the k fold leading to a different average estimate of the model performance one way to reduce model performance noise is to increase the number of folds k this reduces bias in performance of model estimates while also increasing the variance of the outputs an alternative approach is to repeat the k fold cv process several times and report the average performance for all rounds this approach is generally called repeated k fold cv kim 2009 molinaro et al 2005 the important point is that each repetition of k fold cross validation has to be performed on the same dataset but with different k folds repeated k fold cv has the advantage of improving the average model performance through fitting and evaluation of other models the process of such a method similar to what is presented in fig 5 repeats multiple times as needed the common number of repetitions are 3 5 and 10 for example if n repetitions with k number of folds are used to estimate model performance n k different models have to be fitted and evaluated this approach is suitable for small to medium sized dataset and models which are not computationally extensive 2 3 4 evaluation procedure a combination of criteria including kge nse and nrmse are used to evaluate model performance modified kling gupta efficiency kge gupta et al 2009 kling et al 2012 is a unique evaluation criterion used to express similarities between observed and simulated runoff the kge and its three decomposed components correlation bias ratio and variability ratio are all dimensionless and defined by 3 kg e 1 r 1 2 β 1 2 γ 1 2 4 β μ s μ o 5 γ σ s μ s σ o μ o where r is the pearson correlation coefficient between simulation and observations β is the bias ratio of simulated and observed flow γ is the variability ratio between simulation and observed standard deviation and σ is the standard deviation the important point is that according to kge the maximum score of kge is the least score of its components this structure guarantees that the highest kge scores show good similarity between simulation and observation discharge a kge 1 an optimum value demonstrates perfect agreement of simulations and observations kge score is typically 0 41 for the mean flow knoben et al 2019 in order to fully evaluate model performance and its reliability nash sutcliffe efficiency nse nash sutcliffe 1970 and normalized root mean square error nrmse janssen heuberger 1995 analyses are also performed these criteria have the advantage of being dimensionless and are used to compare the variety ofbasins climates flow regimes and flow magnitudes equation 6 and equation 7 present nse and nrmse formulations respectively 6 nse 1 i 1 n s i o i 2 i 1 n o i o 2 7 nrmse 1 n i 1 n r i p r i o 2 o such that r i s is the prediction of month i r s is the average of predictions r i o is observation for month i and r o is the average of observations the relative magnitude of residual variance in comparison to the measured data variance is provided by nse nse varies from to 1 with 1 describing a highly performing model nrmse is a deterministic metric varying between 0 and with a perfect score of 0 3 results and discussion 3 1 selection of optimum predictors the high collinearity or interdependence of climatic variables often leads to redundant information and possibly deceptive results where the data it provides is relevant to the analysis but not necessary given its information is highly similar to that of another predictor utilizing these climatic variables requires careful consideration especially when examining the connections between runoff and the climatic variables li et al 2017 there are frequently connections that are overlooked in systems like hydro meteorological issues with nonlinear and non normal co dependencies however by removing collinear correlated and keeping orthogonal uncorrelated variables a correlation based orthogonalization of datasets will reduce the problem s dimensionality to remove irrelevant and redundant features and in order to make the ml models more accurate pca a linear fsa and bn and rfe two non linear fsas were employed in this study feature selection was implemented for the 36 potential predictors through highly correlated variable exclusion using concurrent application of pca rfe and bn algorithms for each basin 36 potential predictors were calculated fig 6 shows the heatmap of the cross correlation between predictors and predictant runoff for basin number 24 with 1 month lead time as an example the results show that temperature predictors are highly correlated amongst all 36 predictors runoff based predictors followed by those of precipitation are highly correlated with observed runoff fsa is compared with two non linear algorithms i e rfe and bn rows with check marks in tables 2 3 and 4 show the features selected by different fsas and used as input in the modeling stage selected predictors are for all 30 s level basins in iran with 1 month lead time lt1 as shown in table 2 pca optimum predictors demonstrate little dependence on temperature while runoff and precipitation show higher feature importance to the predictant the optimum selected predictors from the rfe method are shown in table 3 it can be seen that except in basins 12 27 43 and 53 the most frequent predictors selected by rfe algorithm are from the pool of runoff variables unlike pca neither temperature nor precipitation played a significant role in outputs of rfe algorithm in all 30 basins for each basin a more specialized subset of the potential predictors is selected using bn as an fsa to identify optimum predictors for machine learning models in order to get a clear picture of optimum predictors a conditional independent structure for the target streamflow and potential predictors is established via bn for each major first level basin in iran a sample basin s conditional independence structure is represented by dag in fig 7 for 1 month lead time the highest dependent potential predictors are those that exhibit direct edges between parent and child with the target and other predictors this feature selection depends on climate lead time data availability and the causal relationship between predictors and the target variable noorbeh et al 2020 for the streamflow of basin17 for instance direct edges with three potential predictors are illustrated where out of 36 potential predictors 3 optimum predictors are connected p2 r10 and t1 additionally it shows that while each potential predictor can be thought of as a candidate predictor some of the information they provide is redundant given the information provided by others leading to a network model with only a few optimum predictors node connections having fewer predictors can reduce overfitting that generally leads to better performance on new data das et al 2022 table 4 shows the optimum predictors selected from bn algorithm for 1 month lead time it is worth noting that while in some basins nearly all predictors are selected for modeling there exist also basins where only a very few predictors are selected unlike in pca and rfe algorithms runoff predictors show no significance in their selection compared to precipitation and temperature using bn method the set of variables potential predictors with strong relationships to the target streamflow were taken into account for each of the different lead times 1 3 months in order to create parsimonious models to predict streamflow all the five ml models are developed using the selected predictors that are chosen by the three fsas i e pca rfe and bn in each lead time for all the basins the best fsa and model combinations are chosen according to the kge criteria in each basin to be used as the final set of ml models given the large number of ml models for all basins 5 ml models 3 fsa 3 lead times and 30 basins only ann model results are shown because of their better performance fig 8 shows the kge evaluation criteria for ann models with 1 to 3 month lead time and for all basins created based on a bn b rfe and c pca feature selection algorithm in training phase for all lead times d is the best fsa of each basin with the highest kge value table 5 shows the frequency of bn rfe and pca algorithms selected for all five ml models totaling to 30 s level basins in iran reflecting fig 8 for ann each of the bn rfe and pca algorithms is respectively selected in 12 16 and 2 basins for 1 month lead time in 14 15 and 1 basins for 2 month lead time and in 11 16 and 3 basins for 3 month lead time table 5 shows that based on xgboost ann and rf models rfe and bn feature selection methods were used most frequently while pca was used least frequently in all lead times in contrast for svr model bn was the least frequently selected predictor while for mlr feature selection methods do not follow any apparent pattern across all lead times 3 2 comparison of several ml prediction models for the purpose of creating concise models for predicting streamflow the set of variables optimal predictors with strong connections to the target streamflow were employed for each lead time 1 3 months monthly streamflow is simulated using mlr svr ann rf and xgboost models in all 30 basins in iran and compared with the average observed runoff over the period of 1981 to 2015 in order to improve prediction accuracy tuning of the ml models hyperparameters was performed the grid search gs algorithm was used to optimize the hyperparameters of xgboost rf and svr models in each model a set of values for each hyperparameter were specified for the algorithm 1000 models were created by the gs algorithm using different combinations of hyperparameters fig 9 compares the simulated hydrograph with observed streamflow for one representative basin within each major first level basin with 1 month lead time all models show acceptable results in simulating stream flow time series of the displayed basins fig 10 shows model evaluation results based onkge nse and nrmse criteria for the 30 studied basins with 1 month lead time the color shading demonstrates model efficiency the darkest color shows highest model efficiency and the lightest shows the least efficiency for all evaluation criteria kge criterion shows ann and xgboosthave the highest prediction performance in all basins followed closely by rf it can be seen that almost all models performed poorly in subbasins of major first level basin 5 which is subject to arid climate located on the eastern border of iran nse criterion shows all models have performed well in the training stage in almost all basins it also shows that similar to the kge criterion the best performance in the testing stage is delivered by ann xgboost and rf occurrence of negative nse means that the average of observed values are more reliable than the simulated model predictions ferreira et al 2021 nse of less than 0 5 implies weak model performance moriasi et al 2015 accordingly the low performance of mlr and svr models in particular in major first level basins 4 5 and 6 is obvious given runoff variations across all basins the dimensionless nrmse criterion is used instead of rmse for better evaluation of various models again all models show acceptable performance in all basins except for major basin 5 overall the monthly results from all five models indicate better predictions in semi humid and humid basins enjoying high flows as compared to the semi arid and arid basins with low flow in central eastern and southern regions similar results were reported by slater et al 2017 in order to better understand predictions of mlr svr ann rf and xgboost models monthly streamflow predictions are shown against their observed values during the study period in different basins fig 11 it can be seen that ann rf and xgboost models have higher prediction performance compared to svr and mlr models in particular in basins of high flow similar to the results reported by ferreira et al 2021 based on these scatter plots fig 11 again it is confirmed that the highest bias occurs in major first level basin 5 which has one of the lowest streamflow among all six major basins svr and mlr models demonstrate a slightly larger spread of values especially in the area of low flows the same conclusion reported by szczepanek 2022 lastly the performance of ann compared to other ml models in reducing the bias of data is evident fig 12 shows the spatial distribution of performance of all ml models based on kge nse and nrmse criteria for training phase over all 30 basins in iran for 1 to 3 month lead times lt1 lt2 and lt3 represent 1 2 and 3 month lead times respectively similar to what was shown in fig 10 xgboost rf and ann offer better performance for 1 month lead time all models show nearly the same performance with increasing lead time over all regions the spatial distribution of performance of all ml models are shown in fig 13 based on kge nse and nrmse criteria for test phase over all 30 basins in iran for 1 to 3 month lead times it was found that xgboost rf and ann offer better performance for 1 month lead time predictions all models show decrease in prediction performance with increasing lead time in all regions this is similar to findings of nobakht et al 2021 and wang et al 2019 for longer lead times ann xgboost and rfshow the best prediction performance on all evaluation criteria compared to other models for 2 to 3 months lead times ann xgboost and rfperform best in the western and northern basins for the kge criteria none of the models delivered suitable prediction performance for central and southeastern basins with higher than 1 month lead time next the kge nse and nrmse evaluation criteria were averaged over the six major first level basins of iran creating a single value for each major basin table 6 shows such averaged values with 1 2 and 3 month lead times lt1 lt3 for all mlmodels the final 30 of the data was used for model testing corresponding to the 2006 2015 period all models demonstrate good performance in runoff prediction in high streamflow basins in particular in major basin 2 with the highest streamflow in iran however in central arid regions with the largest land area and lowest precipitation major basin 4 mlr and svr models offer very poor prediction performance while xgboost performs better in the arid regions of eastern iran e g major basin 5 all models provide weaker streamflow predictions in these regions high rainfall variability is a characteristic of arid regions and nwp models frequently overestimate rainfall amounts over these regions robertson et al 2013 the ecmwf forecasting system will naturally be less vulnerable to more severe aridity conditions due to the stronger physical model structure that makes it less dependent on prior recurrence in moisture content and flows to foresee upcoming dynamics on the other hand with few observations it might be challenging for nwp models to replicate the intricate meteorological processes that cause the high rainfall variability hapuarachchi et al 2022 empirical models built on observations and measurements will be limited in hydro meteorological forecasting in arid regions mostly because the automated learning influencing the model behavior will not have mastered the range of dynamic behaviors underlying such climatic conditions therefore it will continue to be challenging to increase forecasting capacity in arid basins similarly reported by nifa et al 2023 according to model evaluation criteria presented in table 6 for 1 month lead time ann xgboost and rf show better performance and achieved kge criteria values of 0 70 0 87 0 68 0 86 and 0 66 0 80 respectively while yielding nse criteria values of 0 66 0 82 0 66 0 86 and 0 65 0 86 in the prediction phase except for major basin 5 an arid area with very low flows as also observed in the scatterplots of fig 11 predictions over low flows regions such as the three subbasins in major basin 5 are underestimated by all analyzed models the kge evaluation criteria values for ann xgboost and rf are 0 24 0 24 and 0 46 and the nse values are 0 21 0 21 and 0 46 showing better performance of the xgboost model in such a low flow simulation while the greatest deviations from the observations can be noticed with the mlr model prediction models for low flow regions are generally found to exhibit low performance similarly noted by szczepanek 2022 moreover also in basins with high flow regime high flow values are typically underestimated by all models as confirmed by szczepanek 2022 overall the models are compared and ranked based on the kge criteria from table 6 with the order of ann xgboost rf mlr and svr and mean kge values of 0 70 0 68 0 66 0 57 and 0 41 respectively in the prediction phase the predictive ability of all the models decreases for 2 and 3 months lead time compared to 1 month lead time forecasts ann and xgboost outperformed other models in case of 2 months lead time with an average kge value of 0 65 for all basins while resulting in an average kge value of 0 60 for 3 months lead time overall the best results are generated by ann and xgboost models and the worst by svr and mlr in all major basins in the prediction phase ann shows the best performance in major basins 1 2 4 and 6 while xgboost shows the same in major basins 3 and 5 in ml algorithms selection of the appropriate hyperparameters by gs method is a time consuming process ni et al 2020 practice has shown that when there is a model with a large number of hyperparameters like xgboost the results may end up worse than those produced using a small number of parameters this can potentially be explained by xgboost model s optimization problems caused by the extensive list of parameters that need to be optimized szczepanek 2022 the hyper parameters in xgboost have to be carefully tuned to achieve satisfactory forecasts and better generalization capability despite the fact that it outperforms other tree based models in terms of its ability to handle overfitting issues this limitation may be the reason behind ann s superior performance over xgboost in some major basins existing studies have shown that each machine learning algorithm has a certain scope of application and there is currently no ml algorithm that performs best on any given dataset shi shen 2022 however by comparing the result in all basins especially by considering major basin 5 it can be seen that the fluctuation range of kge of xgboost is significantly smaller than that of the other ml algorithms this shows the xgboost algorithm is more robust in capturing a wide set of characteristics across all basins compared to other ml algorithms the superior performance of xgboost over other models in particular svr was likely related to its capacity to handle a larger space of features and non linear relationships between features that can better capture the hydrological characteristics of river basins additionally non parametric models such as xgboost have shown ability to identify complex relationships between different variables in river systems ni et al 2020 the results also revealed that xgboost outperformed rf in terms of accuracy and stability likely due to its ability to account for nonlinear interactions between variables which often go undetected by other methods this allows it to capture more complex relationships in the data which would otherwise be ignored tree based machine learning models and boosting techniques displayed reasonably good results given they classify each variable based on their characteristics in making nodes and leaves this allows these models to gradually improve performance starting with weak learners together these techniques further augment the potential of xgboost as a viable alternative for streamflow forecasting applications the results of this study indicate that ecmwf precipitation runoff and temperature ensembles are suitable although with varied degree of accuracy depending on the region for flow forecasting in iran furthermore it was shown that runoff ensemble values contribute most significantly to basin streamflow forecasts the two non linear feature selections i e rfe and bn behaved similarly in selecting the best feature sets for all ml models over 30 s level basins finally ann and xgboost broadly outperformed other ml models in all 30 basins and for all lead times 4 conclusion this study evaluated runoff forecasting using ensemble products of ecmwf monthly precipitation runoff and temperature forecasts over iran s basins with different climate zones for the period of 1981 to 2015 using different linear and non linear fsa i e recursive feature elimination rfe bayesian networks bn and pearson correlation analysis pca best combination of inputs were selected to derive simulation models the simulations of runoff were conducted through five ml models namely extreme gradient boosting xgboost random forest rf artificial neural networks ann support vector regression svr and multiple linear regression mlr while results were compared to observed runoff in 30 basins in iran the findings of this study can help researchers beyond the geographical implementation in iran this analysis suggests that the modeling skill varied considerably according to climate methodology and lead time this study found that ecmwf forecasts are efficient in prediction of runoff over a generally arid semi arid region such as those found in iran in particular the runoff ensemble followed by precipitation showed the highest importance in prediction of observed runoff in all basins the ann followed by xgboost and rf models had better fitting compared to svr and mlr models in the training set in terms of most evaluation criteria in the majority of basins in particular the ann xgboost and rf models showed excellent performance in all basins in the wet months of the year overall all models performed better over basins with higher runoff values i e basins in the country s western region in contrast approximately all models had lower performance in basins with arid climate characteristics like basins in central iran for the three superior models of xgboost ann and rf rfe and bn fsas were selected most frequently across iran s 30 s level basins while pca was used least frequently in all lead times overall model performance based on the kge criteria yield a best to worst ranking of ann xgboost rf mlr and svr with kge values of 0 70 0 68 0 66 0 57 and 0 41 respectively the predictive performance of all models decreased with lead times beyond 1 month where ann and xgboost outperformed other models with kge of 0 65 for 2 month lead time and 0 60 for 3 month lead time finally an increase in lead time reduced the performance of all models although ann and xgboost performed better than other models for longer lead times based on all performance criteria furthermore ann xgboost and rf demonstrated good performance in 2 to 3 month lead times over western and northern basins of iran with high runoff values in the arid central and southeastern iran however except for xgboost all models showed poor performance especially with more than 1 month lead times credit authorship contribution statement mohammad akbarian conceptualization methodology software writing original draft visualization data curation bahram saghafian conceptualization methodology software supervision writing review editing saeed golian conceptualization methodology validation investigation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
1955,seasonal hydrological forecasts play a critical role in water resources management the copernicus climate change service c3s data store provides open access to monthly hydrological forecasts for up to six months this study aims to evaluate for the first time 1 to 3 month runoff forecasts using the european centre for medium range weather forecasts ecmwf ensembles of precipitation runoff and temperature in 1981 2015 period over a total of 30 s level basins in iran we adopted the 5th 50th and 95th ecmwf ensemble quantiles for each variable that represent low medium and high probability of occurrence respectively pearson correlation analysis pca recursive feature elimination rfe via random forest rf model and bayesian networks bn feature selection algorithms were used in order to reduce input variable dimension and select potential predictors to be fed to the machine learning models multiple linear regression mlr artificial neural networks ann support vector regression svr random forest rf and extreme gradient boosting xgboost machine learning models were used with repeated k fold cross validation rk fold cv while model efficiency was evaluated using modified kling gupta efficiency coefficient kge nash sutcliffe efficiency coefficient nse and normalized root mean square error nrmse results of this study revealed that c3s runoff ensembles have the highest impact on forecast accuracy of streamflow followed by precipitation and temperature overall model performance yield a best to worst ranking of ann xgboost rf mlr and svr with kge values of 0 70 0 68 0 66 0 57 and 0 41 respectively the predictive performance of all models decreased with lead times beyond 1 month where ann and xgboost outperformed other models with kge of 0 65 for 2 month lead time and 0 60 for 3 month lead time the three superior models of xgboost ann and rf were employed with rfe and bn fsas most frequently across iran s 30 s level basins in all lead times almost all models in the arid central region of iran showed the lowest performance while highest skills were achieved in the western regions of iran finally for all models and over all regions the model performance reduced by increase in lead time keywords streamflow forecast c3s data store ecmwf ensemble recursive feature elimination rfe bayesian networks bn machine learning ml data availability data will be made available on request 1 introduction given global water scarcity particularly in the past decade it is crucial to adopt the best water resource management practices to handle or avert consequent water crises greve et al 2018 an important factor in water resource management is the accurate estimation of streamflow in order to plan for available water resources sharma machiwal 2021 long term forecasts include weekly monthly seasonal and even annual predictions and are crucial for operation of reservoirs irrigation management systems and hydropower generation liang et al 2018 improving the accuracy of long term forecasts is significantly dependent on improvements in availability and sustainability of climate datasets and modeling tools wegayehu and muluneh 2022 recent advances in meteorological forecasts open up new opportunities for improving long term hydrological forecasting capabilities kilinc haznedar 2022 long term forecasts are more complex to simulate compared to short term forecasts karimi et al 2016 the practical gap in access to accurate long term runoff predictions pushes research toward reliable hydroclimatological forecasts particularly given the benefits received from even a slight increase in the accuracy of these forecasts to water resource management cheng et al 2020 ensemble hydroclimatological forecasts are used to numerically produce a range of forecasts based on possible future atmospheric conditions unlike single predictions ensemble forecasting provides a more robust prediction by generating the range of possible outcomes and evaluating the confidence in future system states hawcroft et al 2021 a prominent provider of ensemble forecasts is the copernicus climate change service c3s multi system seasonal forecast https cds climate copernicus eu the service which has recently released several european center forecasts is run on behalf of the european union by ecmwf and offers seasonal forecasting protocols publicly available through the climate data store the database combines observations of weather systems and provides holistic past present and future information on atmospheric conditions ingleby 2015 lopez 2013 given the recent launch of the c3s database research on its evaluation is very limited in particular over iran the evaluation of performance of ensemble forecast can provide context to decision makers strategic choices for overcoming climate related risks nobakht et al 2021 crochemore et al 2017 evaluated precipitation and river flow forecasting performance in 16 basins in france the study evaluated and post processed seasonal precipitation forecasts from the open library of ecmwf system 4 with 90 day lead time they used linear scaling ls and different distribution mapping methods for post processing of monthly and annual raw precipitation data the results demonstrated that applying post processing techniques increase precipitation forecasting accuracy manzanas et al 2019 applied simple bias adjustment ba methods such as quantile mapping as well as complicated ensemble recalibration rc methods such as non homogenous gaussian regression to increase the accuracy of c3s forecasted precipitation and temperature in particular they evaluated uk met office ukmo glosea5 maclachlan et al 2015 météo france system5 descamps et al 2015 and ecmwf seas5 johnson et al 2019 seasonal forecasts with one month lead time the results revealed that both ba and rc methods correct large raw model biases effectively with high model skill in confined regions and seasons in these instances rc bias correction outperformed ba methods manzanas et al 2019 in another study gebrechorkos et al 2022 evaluated the performance of precipitation forecasts from five potential climate models namely ecmwf uk met office météo france deutscher wetterdienst kaspar et al 2015 and centro euro mediterraneo sui cambiamenti climatici nicolì et al 2023 multi source weighted ensemble precipitation was used as the reference data for model performance evaluation model performance was evaluated in daily weekly monthly and seasonal timeframes and at specific months and lead times all models showed reliable predictions for 1 month forecasts however they showed rapid decline in performance with increase in lead times in particular in drier regions and seasons it was found that ecmwf followed by uk met were the most accurate models among other c3s products gebrechorkos et al 2022 due to machine learning ml algorithms significant nonlinear modeling capabilities in complex problems they have been widely used for the monthly prediction of streamflow ali shahbaz 2020 the data preprocessing feature preprocessing ml algorithm selection and hyperparameter optimization stages are typically included in the machine learning based prediction process for streamflow prediction one of the more common forms of data driven models multiple linear regression mlr has been shown to perform effectively for long term forecasts krstanovic singh 1991 however due to the nature of the process it makes the assumption that the relationships between input and output data are linear and the data has no multicollinearity whereas streamflow prediction is a highly nonlinear process and depends on a variety of known and unknown factors sudheer et al 2014 when it comes to model selection the artificial neural networks anns kilinc haznedar 2022 deep neural networks dnns apaydin et al 2021 he et al 2022 kao et al 2020 2021 maddu et al 2022 support vector regression svr ni et al 2020 random forest rf tyralis et al 2021 adaptive boosting adaboost liu et al 2014 light gradient boosting method lgbm szczepanek 2022 and extreme gradient boosting xgboost ni et al 2020 are frequently used for streamflow prediction both dt and gradient boosting gb are utilized by xgboost it has many benefits including its predictive algorithms are straightforward but still effective simple to understand and don t need as much data preparation ni et al 2020 artificial neural network ann s ability to process and model complex nonlinear time series has recently enabled its widespread applications in hydroclimatological studies in contrast to application of physically based models kilinc haznedar 2022 the ability to work with large amounts of noisy data from nonlinear and dynamic systems especially when the fundamental physical relationships are unknown is one of the advantages ann has over traditional modeling anusree varghese 2016 ann has been successfully applied in various hydrological simulations known as an efficient and accurate tool in forecasting streamflow precipitation and water quality kilinc haznedar 2022 rf is a decision tree based model that addresses the overfitting issues with single decision trees while maintaining their prediction accuracy the rf approach in contrast to ann and svm offers excellent computing speed while being simple to use schoppa et al 2020 multiple studies have reported complexities associated with flow forecasting driven by the natural complexity nonlinearity and randomness of river systems smith et al 2007 in order to reduce the number of predictors different linear or non linear feature selection algorithms fsas such as pearson s correlation analysis pca djibo et al 2015 recursive feature elimination rfe ferreira et al 2021 and bayesian networks bn das et al 2022 are commonly used in terms of ml algorithm calibration the optimization of hyperparameters has a significant impact on the performance of ml models szczepanek 2022 as a result to optimize hyperparameters for ml algorithms researchers have used different methods such as grid search gs lavalle et al 2016 or metaheuristics malik et al 2020 in iran ml algorithms have also been used in many hydroclimatological studies for example forecasts from ecmwf ukmo and national centers for environmental prediction ncep were evaluated for 13 synoptic stations in eight precipitation zones aminyavari et al 2018 the evaluation was based on precipitation data from 2008 to 2016 with a 1 to 3 day lead times their results showed decrease in model performance with increase in lead time while different models performed differently over various regions i e ecmwf in most regions ukmo in mountainous regions and ncep around the persian gulf kolachian and saghafian 2019 used a number of deterministic and probabilistic criteria to evaluate the performance of precipitation forecast for ecmwf sub seasonal to seasonal s2s model with 1 month lead time for several synoptic stations and precipitation regimes in iran they found acceptable performance in wet months in most regions however absence of reliable raw forecasts in dry months was evident no significant relationship was found between the precipitation regime and prediction skill furthermore their results showed forecasting and post processing capabilities vary significantly in different seasons and locations nobakht et al 2021 evaluated the ensemble precipitation forecasts of ecmwf ukmo and météo france c3s models over 1993 2017 period in iran s eight classified precipitation clusters with 1 to 3 month lead times probabilistic and non probabilistic criteria were used for the evaluation the results indicated all models performed better in the western precipitation regions while in the northern region with humid climate models had poor skill scores all forecasts were better at predicting upper tercile events in dry seasons and lower tercile events in wet seasons moreover with increasing lead time the forecast skills worsened in terms of forecasting in dry and wet years the predictions were generally close to observations albeit they underestimated several severe dry periods and overestimated a few wet periods in another study meydani et al 2022 applied weather forecast downscaling and rainfall runoff modeling for daily reservoir inflow forecasts in the urmia lake basin in iran they utilized large scale weather forecasts from ecmwf and ncep to evaluate various downscaling methods including artificial intelligence ai and bayesian belief network bbn techniques to derive local reservoir inflow forecasts the results showed a hybrid downscaling approach that combined group method of data handling gmdh and support vector regression svr performed better than their non hybrid counterparts in downscaling precipitation furthermore the authors found bbn to outperform hybrid ai in forecasting the dynamics of precipitation in observed datasets in this study for the first time the performance of ecmwf runoff forecasts over iran is comprehensively assessed by evaluating the monthly ecmwf s seasonal forecasting system 5 seas5 and its potential in predicting streamflow over 30 s level basins in iran subject to different climate and hydrological regimes based on a set of initial potential predictors and their association high correlation and statistical significance with the target monthly streamflow a set of potential predictors is established the potential predictors are then put through three feature selection algorithms fsa including bayesian networks bn recursive feature elimination rfe and pearson correlation analysis pca to create a set of optimal predictors for each scenario these optimum predictors served as inputs for different ml algorithms the study is also the first to use ecmwf runoff forecasts to predict monthly streamflow in iran according to the authors knowledge additionally this study investigates the use of five ml algorithms notably xgboost a novel intelligent method based on the gradient boosting algorithm to forecast the streamflow across iran s 6 major and 30 s level basins 2 materials and methods this section describes the study area datasets pre processing and modeling techniques to develop streamflow forecasting with lead times from 1 to 3 months over iran the data used are from the latest generation of ecmwf s seasonal forecasting system 5 seas5 and consist of ensemble forecasts of precipitation runoff and temperature from 1991 to 2015 with 1 to 3 month lead time in order to reduce the dimension of input variables pearson s correlation analysis pca recursive feature elimination rfe and bayesian networks bn are used mlr ann rf svr and xgboost models with rk fold cross validation are adopted to model runoff in 30 iranian basins lastly recommendations are proposed for runoff forecast for different basins 2 1 study area iran with a total area of 1 648 195 km2 is located in the mid latitude northern region between 25 n and 40 n latitude and 44 e and 64 e longitude in southwest asia other than western and northern coastal areas iran s general climate is dominated by a mostly arid and semi arid character precipitation varies mainly with latitude and topographical altitude of the region mansouri daneshvar et al 2019 the whole country may be divided into six major basins that are broken into a total of 30 s level basins saatsaz 2020 fig 1 depicts the basins rivers and location of the hydrometeorological stations used in this study in total 571 hydrometric stations in the country have at least 240 months of data throughout the 1981 2015 study period in this study however we selected the stations with the highest elevation in each basin resulting in a total of 30 selected stations shown in fig 1 table 1 provides characteristics of the basins including basin name basin code area and climate class according to köppen geiger climate classification raziei 2022 2 2 datasets and pre processing 2 2 1 hydro meteorological and ensemble seasonal forecast data the observed station data spans the 1946 to 2015 period monthly averages of precipitation and runoff are shown in fig 2 for all 30 basins in percentage c3s data store launched in 2017 by ecmwf regularly releases seasonal forecast products the data store relies on forecasts frommultiple organizations across europe and updates monthly forecasts with up to six months lead time the data includes forecasts created in real time since 2017 and retrospective forecasts hindcasts initialized at equivalent intervals during the 1981 2016 period this study investigated monthly c3s precipitation runoff and temperature ensembles with 1 to 3 month lead time for the time period of 1981 2015 over the 25 40 n 44 64 e geographical area with a grid size of 0 25 0 25 with approximately 25 km spatial resolution the correlation coefficient between seas5 raw runoff average precipitation and temperature outputs with observed values over each basin is presented in fig 3 for 1 to 3 month lead time it shows runoff ensemble demonstrates the highest correlation withobserved values followed by precipitation 2 2 2 pre processing of raw product in order to work with ecmwf seas5 ensemble outputs the quantile classification methodwas appliedin a way that for each variable thecorresponding 5th 50th and 95th quantiles were extracted to represent low medium and high probability of occurrence values respectively these quantile values form potential predictors to drive machine learning ml models golian et al 2010 2011 used a similar quantile classification to derive rainfall thresholds with different probability of occurrence through a monte carlo framework fig 4 illustrates the end to end data preparation process implemented in this study starting with data retrieval from the ecmwf data store to preparation for runoff simulation ensemble precipitation runoff and temperature hindcasts were retrieved for each cell over the entirety of iran from cds website https cds climate copernicus eu for lead times from 1 to 3 months next data in gridded binary format grib were converted and saved in a comma separated values csv format these converted files include information on precipitation runoff and temperature containing all 25 ensembles with 1 to 3 month lead time then precipitation runoff and temperature were extracted for each cell and 5th 50th and 95th quantiles were extracted along with the average of ensembles for each cell inside a particular basin represented by ens 5 ens 50 ens 95 ens mean finally for each variable and across all cells in a basin the 5th 50th and 95th quantiles and the mean values were calculated for each basin represented by cell 5 cell 50 cell 95 cell mean 2 3 methods 2 3 1 selection of predictors given the large number ofpotential predictors i e 36 for each lead time dimension reduction of the input matrix is needed to simplify the model and reduce calculation time the feature selection algorithms used to select the optimum predictors has some limitations such as the risk of overfitting when the data size is small the high computation time when the variables are many and the trade off between reducing variance and increasing bias by eliminating some relevant features munson caruana 2009 predictor selection is herein implemented through two methods of highly correlated variable exclusion and elimination by importance where via a high correlation filter highly redundant predictors are identified and removed kuhn johnson 2013 in pearson correlation analysis pca method the cross correlation value is quantified between all potential predictor variable pairs simultaneously using pca each predictor pairing with a pearson s correlation of more than 95 is compared with other predictors keeping one predictor among those with the highest correlation ferreira et al 2021 thus for each predictor pair with a high pearson correlation those with the highest correlation with other predictors are dropped from further analyses this eliminates redundant input variable predictors that are already captured by other variables the second method elimination by importance is implemented for predictor selection by eliminating redundant and irrelevant inputs the predictability and robustness of machine learning methods are increased while reducing the computational costs a suitable method to identify the most important feature is the recursive feature elimination rfe algorithm in this approach all possible combinations of predictive variables are used to apply the models whereby the explanatory power of each predictor is identified through rfe the algorithm then repeatedly eliminates variables below an importance criterion admitted by the models in each step of searching guyon et al 2002 kuhn johnson 2013 overall rfe is performed on all variables considering 2 to 36 predictors as possible inputs to the simulation models selection of the optimal predictor set based on the leave one out cross validation loocv method is tested using rmse as a performance criterion for each set the ideal predictor set is selected as one with the least rmse and fewest predictors this study has used rfe algorithm based on random forest rf model directed acyclic graph dag is used to define the bayesian network bn it offers the joint probability distribution for a set of random variables and connected nodes that represent these random variables bn demonstrates the causal relationship or nature between pairs of such variables dutta maity 2020 structure learning is necessary to develop and understand this structure which comes in three major categories score based constraint based and hybrid algorithms the score based hill climb search algorithm is one of the most common approaches taken to discover network structures and identify the best predictors scutari 2017 this algorithm starts with a saturated graph and compares the score to the maximum score for each potential addition deletion or reversal before creating a top scoring network for the bn in this study the library causalnex beaumont et al 2017 was utilized for creation of bn models and plotting dag graphs with bayesian information criterion scores being employed to choose the best prediction model leu bui 2016 once established it can be analyzed in order to determine if variables are dependent or conditionally independent from one another based on the relationship to target variable this aids in the selection process of inputs for machine learning models that comprise potential predictors directly related to targets in essence through use of dag s and bns one is able to analyze data sets accordingly this provides insight into cause and effect between variables while identifying key information that can be relayed back into ml models as best predictors possible under given conditions 2 3 2 simulation models five state of the art ml models were used to develop models for the prediction of monthly streamflow in 30 s level basins of iran below is a brief description of each model the selected five ml algorithms mlr ann svr rf and xgboost are widely used for streamflow prediction the first technique employed is the multiple linear regression mlr model possibly the most recognizable data driven forecasting technique mlr creates a linear relationship between a continuous dependent variable y and one or several independent variables xi regression is the most widely used to identify variables xi with a relationship to the output y araghinejad 2014 for the mlr model implementation 70 of the available data is used for training and 30 used for testing of the model another simulation approach applied in this research is the artificial neural network ann with a unidirectional multilayer structure consisting of an input layer multiple hidden layers and an output layer each layer consists of several neurons connected to adjacent layers signals entering through the input layer areunidirectionally directed toward the output layer in ann the following transformation is applied for each neuron during signal flow in network 1 y f i 1 n x i w i b i such that y is the output xi is the input n is the number of inputs wi is the weight bi is the bias and f is the activation function of each input used to approximate any mapping between model inputs and outputs furthermore this function normalizes neuron outputs to prevent extreme output values after several layers torre et al 2020 the number of input layer neurons is based on the number of predictors considered for each basin the number of hidden layer neurons is evaluated by forming networks of 5 to 15 neurons and selecting their optimal number by trial and error the ratio of data used for training validation and testing are 56 14 and 30 respectively the minimum network error for simulation cutoff was 0 000001 with maximum number of training failures of 30 and with levenberg marquardt training functions random forest rf fundamentally incorporates over fitting and local convergence issues into multiple classifier forests through a single classifier decision tree dt belgiu drăgu 2016 the method used for resampling takes multiple samples from the original dataset trains a dt for each bootstrap sample combines these dts and averages the predicted values for all the combined dts the prediction value of the i th dt yi is evaluated using equation 2 with n representing the number of dts x is the inputs and y as the prediction result of the rf model 2 y 1 n i 1 n y i x another model employed in this study is the support vector regression svr this model was created by generalization of the support vector machine svm capabilities from classification to regression problems noble 2006 svr inherits the core concepts of data fitting from svm and has been widely applied in multiple areas recently while svm maximizes the distance to the sample point closest to the hyperplane the svr finds a hyperplane that minimizes the distance to the sample point furthest from the hyperplane by turning the process of finding a hyperplane into a convex quadratic programming problem and solving it svr was able to realize nonlinear data modeling and obtain the hyperplane the final model employed in this study is a new intelligent algorithm for predicting streamflow based on the gradient boosting model recently extreme gradient boosting xgboost has received praise in academia for its efficient performance efficacy and fast speed chen guestrin 2016 in this method a dt is selected by xgboost as a weak learner while training a weak learner the algorithm increases the weight of previously misclassified data marginally identifies the next weak learner and adds another weak learner to correct the residuals of all previous weak learners the result is finally obtained through the weighted sum of learners by training the xgboost based model using the collected data a strong learner can be used to predict the streamflow values obtained rk fold cross validation method is used to validate the developed models 2 3 3 repeated k fold cross validation once produced based on training data the model accuracyis assessed using the test dataset a classic validation method for machine learning methods is cross validation here employed by separating the dataset into its training and testing sets high model accuracy in the training phase does not by default guarantee a similar performance with new data emphasizing the importance of balancing between generalization and overfitting of the model outputs model underfitting impliesthe model lacks sufficient performance in both the training and testing phases most likely this is because the model is not well tuned or trained enough on the training set resulting in high bias and low skills model overfitting meansthe model is too far tuned in the training set as a result the model performs well on the training set but poorly onthe test phase resulting in low bias and high variance berrar 2019 cawley talbot 2010 the most important issue with separating data into training and test sets is that test datasets might not follow the distribution of the whole dataset the k fold cross validation process as illustrated in fig 5 solves this by sampling all data in k rounds k is defined as the number of folds and is typically between 3 and 10 but can also be any positive integer the data is then divided into k equal parts the algorithm in k 1 step selects different groupings of folds for testing and separates the remaining folds for the training dataset with this method it is possible to train the model k 1 times independently and measure performance scores k 1 times based on selected criteria finally the average of all scores is evaluated estimating model performance through k fold cv may result in noisy estimates this is due to the fact that every time the procedure is performed a new division of the data enters the k fold leading to a different average estimate of the model performance one way to reduce model performance noise is to increase the number of folds k this reduces bias in performance of model estimates while also increasing the variance of the outputs an alternative approach is to repeat the k fold cv process several times and report the average performance for all rounds this approach is generally called repeated k fold cv kim 2009 molinaro et al 2005 the important point is that each repetition of k fold cross validation has to be performed on the same dataset but with different k folds repeated k fold cv has the advantage of improving the average model performance through fitting and evaluation of other models the process of such a method similar to what is presented in fig 5 repeats multiple times as needed the common number of repetitions are 3 5 and 10 for example if n repetitions with k number of folds are used to estimate model performance n k different models have to be fitted and evaluated this approach is suitable for small to medium sized dataset and models which are not computationally extensive 2 3 4 evaluation procedure a combination of criteria including kge nse and nrmse are used to evaluate model performance modified kling gupta efficiency kge gupta et al 2009 kling et al 2012 is a unique evaluation criterion used to express similarities between observed and simulated runoff the kge and its three decomposed components correlation bias ratio and variability ratio are all dimensionless and defined by 3 kg e 1 r 1 2 β 1 2 γ 1 2 4 β μ s μ o 5 γ σ s μ s σ o μ o where r is the pearson correlation coefficient between simulation and observations β is the bias ratio of simulated and observed flow γ is the variability ratio between simulation and observed standard deviation and σ is the standard deviation the important point is that according to kge the maximum score of kge is the least score of its components this structure guarantees that the highest kge scores show good similarity between simulation and observation discharge a kge 1 an optimum value demonstrates perfect agreement of simulations and observations kge score is typically 0 41 for the mean flow knoben et al 2019 in order to fully evaluate model performance and its reliability nash sutcliffe efficiency nse nash sutcliffe 1970 and normalized root mean square error nrmse janssen heuberger 1995 analyses are also performed these criteria have the advantage of being dimensionless and are used to compare the variety ofbasins climates flow regimes and flow magnitudes equation 6 and equation 7 present nse and nrmse formulations respectively 6 nse 1 i 1 n s i o i 2 i 1 n o i o 2 7 nrmse 1 n i 1 n r i p r i o 2 o such that r i s is the prediction of month i r s is the average of predictions r i o is observation for month i and r o is the average of observations the relative magnitude of residual variance in comparison to the measured data variance is provided by nse nse varies from to 1 with 1 describing a highly performing model nrmse is a deterministic metric varying between 0 and with a perfect score of 0 3 results and discussion 3 1 selection of optimum predictors the high collinearity or interdependence of climatic variables often leads to redundant information and possibly deceptive results where the data it provides is relevant to the analysis but not necessary given its information is highly similar to that of another predictor utilizing these climatic variables requires careful consideration especially when examining the connections between runoff and the climatic variables li et al 2017 there are frequently connections that are overlooked in systems like hydro meteorological issues with nonlinear and non normal co dependencies however by removing collinear correlated and keeping orthogonal uncorrelated variables a correlation based orthogonalization of datasets will reduce the problem s dimensionality to remove irrelevant and redundant features and in order to make the ml models more accurate pca a linear fsa and bn and rfe two non linear fsas were employed in this study feature selection was implemented for the 36 potential predictors through highly correlated variable exclusion using concurrent application of pca rfe and bn algorithms for each basin 36 potential predictors were calculated fig 6 shows the heatmap of the cross correlation between predictors and predictant runoff for basin number 24 with 1 month lead time as an example the results show that temperature predictors are highly correlated amongst all 36 predictors runoff based predictors followed by those of precipitation are highly correlated with observed runoff fsa is compared with two non linear algorithms i e rfe and bn rows with check marks in tables 2 3 and 4 show the features selected by different fsas and used as input in the modeling stage selected predictors are for all 30 s level basins in iran with 1 month lead time lt1 as shown in table 2 pca optimum predictors demonstrate little dependence on temperature while runoff and precipitation show higher feature importance to the predictant the optimum selected predictors from the rfe method are shown in table 3 it can be seen that except in basins 12 27 43 and 53 the most frequent predictors selected by rfe algorithm are from the pool of runoff variables unlike pca neither temperature nor precipitation played a significant role in outputs of rfe algorithm in all 30 basins for each basin a more specialized subset of the potential predictors is selected using bn as an fsa to identify optimum predictors for machine learning models in order to get a clear picture of optimum predictors a conditional independent structure for the target streamflow and potential predictors is established via bn for each major first level basin in iran a sample basin s conditional independence structure is represented by dag in fig 7 for 1 month lead time the highest dependent potential predictors are those that exhibit direct edges between parent and child with the target and other predictors this feature selection depends on climate lead time data availability and the causal relationship between predictors and the target variable noorbeh et al 2020 for the streamflow of basin17 for instance direct edges with three potential predictors are illustrated where out of 36 potential predictors 3 optimum predictors are connected p2 r10 and t1 additionally it shows that while each potential predictor can be thought of as a candidate predictor some of the information they provide is redundant given the information provided by others leading to a network model with only a few optimum predictors node connections having fewer predictors can reduce overfitting that generally leads to better performance on new data das et al 2022 table 4 shows the optimum predictors selected from bn algorithm for 1 month lead time it is worth noting that while in some basins nearly all predictors are selected for modeling there exist also basins where only a very few predictors are selected unlike in pca and rfe algorithms runoff predictors show no significance in their selection compared to precipitation and temperature using bn method the set of variables potential predictors with strong relationships to the target streamflow were taken into account for each of the different lead times 1 3 months in order to create parsimonious models to predict streamflow all the five ml models are developed using the selected predictors that are chosen by the three fsas i e pca rfe and bn in each lead time for all the basins the best fsa and model combinations are chosen according to the kge criteria in each basin to be used as the final set of ml models given the large number of ml models for all basins 5 ml models 3 fsa 3 lead times and 30 basins only ann model results are shown because of their better performance fig 8 shows the kge evaluation criteria for ann models with 1 to 3 month lead time and for all basins created based on a bn b rfe and c pca feature selection algorithm in training phase for all lead times d is the best fsa of each basin with the highest kge value table 5 shows the frequency of bn rfe and pca algorithms selected for all five ml models totaling to 30 s level basins in iran reflecting fig 8 for ann each of the bn rfe and pca algorithms is respectively selected in 12 16 and 2 basins for 1 month lead time in 14 15 and 1 basins for 2 month lead time and in 11 16 and 3 basins for 3 month lead time table 5 shows that based on xgboost ann and rf models rfe and bn feature selection methods were used most frequently while pca was used least frequently in all lead times in contrast for svr model bn was the least frequently selected predictor while for mlr feature selection methods do not follow any apparent pattern across all lead times 3 2 comparison of several ml prediction models for the purpose of creating concise models for predicting streamflow the set of variables optimal predictors with strong connections to the target streamflow were employed for each lead time 1 3 months monthly streamflow is simulated using mlr svr ann rf and xgboost models in all 30 basins in iran and compared with the average observed runoff over the period of 1981 to 2015 in order to improve prediction accuracy tuning of the ml models hyperparameters was performed the grid search gs algorithm was used to optimize the hyperparameters of xgboost rf and svr models in each model a set of values for each hyperparameter were specified for the algorithm 1000 models were created by the gs algorithm using different combinations of hyperparameters fig 9 compares the simulated hydrograph with observed streamflow for one representative basin within each major first level basin with 1 month lead time all models show acceptable results in simulating stream flow time series of the displayed basins fig 10 shows model evaluation results based onkge nse and nrmse criteria for the 30 studied basins with 1 month lead time the color shading demonstrates model efficiency the darkest color shows highest model efficiency and the lightest shows the least efficiency for all evaluation criteria kge criterion shows ann and xgboosthave the highest prediction performance in all basins followed closely by rf it can be seen that almost all models performed poorly in subbasins of major first level basin 5 which is subject to arid climate located on the eastern border of iran nse criterion shows all models have performed well in the training stage in almost all basins it also shows that similar to the kge criterion the best performance in the testing stage is delivered by ann xgboost and rf occurrence of negative nse means that the average of observed values are more reliable than the simulated model predictions ferreira et al 2021 nse of less than 0 5 implies weak model performance moriasi et al 2015 accordingly the low performance of mlr and svr models in particular in major first level basins 4 5 and 6 is obvious given runoff variations across all basins the dimensionless nrmse criterion is used instead of rmse for better evaluation of various models again all models show acceptable performance in all basins except for major basin 5 overall the monthly results from all five models indicate better predictions in semi humid and humid basins enjoying high flows as compared to the semi arid and arid basins with low flow in central eastern and southern regions similar results were reported by slater et al 2017 in order to better understand predictions of mlr svr ann rf and xgboost models monthly streamflow predictions are shown against their observed values during the study period in different basins fig 11 it can be seen that ann rf and xgboost models have higher prediction performance compared to svr and mlr models in particular in basins of high flow similar to the results reported by ferreira et al 2021 based on these scatter plots fig 11 again it is confirmed that the highest bias occurs in major first level basin 5 which has one of the lowest streamflow among all six major basins svr and mlr models demonstrate a slightly larger spread of values especially in the area of low flows the same conclusion reported by szczepanek 2022 lastly the performance of ann compared to other ml models in reducing the bias of data is evident fig 12 shows the spatial distribution of performance of all ml models based on kge nse and nrmse criteria for training phase over all 30 basins in iran for 1 to 3 month lead times lt1 lt2 and lt3 represent 1 2 and 3 month lead times respectively similar to what was shown in fig 10 xgboost rf and ann offer better performance for 1 month lead time all models show nearly the same performance with increasing lead time over all regions the spatial distribution of performance of all ml models are shown in fig 13 based on kge nse and nrmse criteria for test phase over all 30 basins in iran for 1 to 3 month lead times it was found that xgboost rf and ann offer better performance for 1 month lead time predictions all models show decrease in prediction performance with increasing lead time in all regions this is similar to findings of nobakht et al 2021 and wang et al 2019 for longer lead times ann xgboost and rfshow the best prediction performance on all evaluation criteria compared to other models for 2 to 3 months lead times ann xgboost and rfperform best in the western and northern basins for the kge criteria none of the models delivered suitable prediction performance for central and southeastern basins with higher than 1 month lead time next the kge nse and nrmse evaluation criteria were averaged over the six major first level basins of iran creating a single value for each major basin table 6 shows such averaged values with 1 2 and 3 month lead times lt1 lt3 for all mlmodels the final 30 of the data was used for model testing corresponding to the 2006 2015 period all models demonstrate good performance in runoff prediction in high streamflow basins in particular in major basin 2 with the highest streamflow in iran however in central arid regions with the largest land area and lowest precipitation major basin 4 mlr and svr models offer very poor prediction performance while xgboost performs better in the arid regions of eastern iran e g major basin 5 all models provide weaker streamflow predictions in these regions high rainfall variability is a characteristic of arid regions and nwp models frequently overestimate rainfall amounts over these regions robertson et al 2013 the ecmwf forecasting system will naturally be less vulnerable to more severe aridity conditions due to the stronger physical model structure that makes it less dependent on prior recurrence in moisture content and flows to foresee upcoming dynamics on the other hand with few observations it might be challenging for nwp models to replicate the intricate meteorological processes that cause the high rainfall variability hapuarachchi et al 2022 empirical models built on observations and measurements will be limited in hydro meteorological forecasting in arid regions mostly because the automated learning influencing the model behavior will not have mastered the range of dynamic behaviors underlying such climatic conditions therefore it will continue to be challenging to increase forecasting capacity in arid basins similarly reported by nifa et al 2023 according to model evaluation criteria presented in table 6 for 1 month lead time ann xgboost and rf show better performance and achieved kge criteria values of 0 70 0 87 0 68 0 86 and 0 66 0 80 respectively while yielding nse criteria values of 0 66 0 82 0 66 0 86 and 0 65 0 86 in the prediction phase except for major basin 5 an arid area with very low flows as also observed in the scatterplots of fig 11 predictions over low flows regions such as the three subbasins in major basin 5 are underestimated by all analyzed models the kge evaluation criteria values for ann xgboost and rf are 0 24 0 24 and 0 46 and the nse values are 0 21 0 21 and 0 46 showing better performance of the xgboost model in such a low flow simulation while the greatest deviations from the observations can be noticed with the mlr model prediction models for low flow regions are generally found to exhibit low performance similarly noted by szczepanek 2022 moreover also in basins with high flow regime high flow values are typically underestimated by all models as confirmed by szczepanek 2022 overall the models are compared and ranked based on the kge criteria from table 6 with the order of ann xgboost rf mlr and svr and mean kge values of 0 70 0 68 0 66 0 57 and 0 41 respectively in the prediction phase the predictive ability of all the models decreases for 2 and 3 months lead time compared to 1 month lead time forecasts ann and xgboost outperformed other models in case of 2 months lead time with an average kge value of 0 65 for all basins while resulting in an average kge value of 0 60 for 3 months lead time overall the best results are generated by ann and xgboost models and the worst by svr and mlr in all major basins in the prediction phase ann shows the best performance in major basins 1 2 4 and 6 while xgboost shows the same in major basins 3 and 5 in ml algorithms selection of the appropriate hyperparameters by gs method is a time consuming process ni et al 2020 practice has shown that when there is a model with a large number of hyperparameters like xgboost the results may end up worse than those produced using a small number of parameters this can potentially be explained by xgboost model s optimization problems caused by the extensive list of parameters that need to be optimized szczepanek 2022 the hyper parameters in xgboost have to be carefully tuned to achieve satisfactory forecasts and better generalization capability despite the fact that it outperforms other tree based models in terms of its ability to handle overfitting issues this limitation may be the reason behind ann s superior performance over xgboost in some major basins existing studies have shown that each machine learning algorithm has a certain scope of application and there is currently no ml algorithm that performs best on any given dataset shi shen 2022 however by comparing the result in all basins especially by considering major basin 5 it can be seen that the fluctuation range of kge of xgboost is significantly smaller than that of the other ml algorithms this shows the xgboost algorithm is more robust in capturing a wide set of characteristics across all basins compared to other ml algorithms the superior performance of xgboost over other models in particular svr was likely related to its capacity to handle a larger space of features and non linear relationships between features that can better capture the hydrological characteristics of river basins additionally non parametric models such as xgboost have shown ability to identify complex relationships between different variables in river systems ni et al 2020 the results also revealed that xgboost outperformed rf in terms of accuracy and stability likely due to its ability to account for nonlinear interactions between variables which often go undetected by other methods this allows it to capture more complex relationships in the data which would otherwise be ignored tree based machine learning models and boosting techniques displayed reasonably good results given they classify each variable based on their characteristics in making nodes and leaves this allows these models to gradually improve performance starting with weak learners together these techniques further augment the potential of xgboost as a viable alternative for streamflow forecasting applications the results of this study indicate that ecmwf precipitation runoff and temperature ensembles are suitable although with varied degree of accuracy depending on the region for flow forecasting in iran furthermore it was shown that runoff ensemble values contribute most significantly to basin streamflow forecasts the two non linear feature selections i e rfe and bn behaved similarly in selecting the best feature sets for all ml models over 30 s level basins finally ann and xgboost broadly outperformed other ml models in all 30 basins and for all lead times 4 conclusion this study evaluated runoff forecasting using ensemble products of ecmwf monthly precipitation runoff and temperature forecasts over iran s basins with different climate zones for the period of 1981 to 2015 using different linear and non linear fsa i e recursive feature elimination rfe bayesian networks bn and pearson correlation analysis pca best combination of inputs were selected to derive simulation models the simulations of runoff were conducted through five ml models namely extreme gradient boosting xgboost random forest rf artificial neural networks ann support vector regression svr and multiple linear regression mlr while results were compared to observed runoff in 30 basins in iran the findings of this study can help researchers beyond the geographical implementation in iran this analysis suggests that the modeling skill varied considerably according to climate methodology and lead time this study found that ecmwf forecasts are efficient in prediction of runoff over a generally arid semi arid region such as those found in iran in particular the runoff ensemble followed by precipitation showed the highest importance in prediction of observed runoff in all basins the ann followed by xgboost and rf models had better fitting compared to svr and mlr models in the training set in terms of most evaluation criteria in the majority of basins in particular the ann xgboost and rf models showed excellent performance in all basins in the wet months of the year overall all models performed better over basins with higher runoff values i e basins in the country s western region in contrast approximately all models had lower performance in basins with arid climate characteristics like basins in central iran for the three superior models of xgboost ann and rf rfe and bn fsas were selected most frequently across iran s 30 s level basins while pca was used least frequently in all lead times overall model performance based on the kge criteria yield a best to worst ranking of ann xgboost rf mlr and svr with kge values of 0 70 0 68 0 66 0 57 and 0 41 respectively the predictive performance of all models decreased with lead times beyond 1 month where ann and xgboost outperformed other models with kge of 0 65 for 2 month lead time and 0 60 for 3 month lead time finally an increase in lead time reduced the performance of all models although ann and xgboost performed better than other models for longer lead times based on all performance criteria furthermore ann xgboost and rf demonstrated good performance in 2 to 3 month lead times over western and northern basins of iran with high runoff values in the arid central and southeastern iran however except for xgboost all models showed poor performance especially with more than 1 month lead times credit authorship contribution statement mohammad akbarian conceptualization methodology software writing original draft visualization data curation bahram saghafian conceptualization methodology software supervision writing review editing saeed golian conceptualization methodology validation investigation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
1956,hydrological modeling plays an indispensable role in many practical applications such as hydrology water resources assessment and environmental survey downscaling approach is able to obtain high resolution hydrological models according to a prior reference model in this work we propose a downscaling hydrological modeling approach based on convolutional conditional neural processes convcnp and a novel geostatistical bias correction named convcnp gbc a two stage downscaling modeling workflow is presented in the model building stage to reduce the training consumption of the convcnp low resolution conditioning data are used for training in the model generating stage the convcnp is used to reconstruct coarse scale realizations based on upscaling sampled conditioning data and to generate high resolution realizations by downscaling besides a geostatistical bias correction strategy is presented to refine large scale realizations with high resolution conditioning data constraints the experimental results confirm that convcnp gbc can reproduce heterogeneous patterns by using coarse scale conditioning data and characterize hydrological structures the downscaled realizations also have a high spatial correspondence with the coarse scale realizations which provide fine subsurface heterogeneous structures the proposed method can be easily extended to reservoir modeling geophysical inversion etc keywords hydrological modeling convolutional conditional neural process geostatistical bias correction deep learning data availability data will be made available on request 1 introduction the environmental concerns and the exploration of geological resources are pushing geoscientific research toward high resolution hydrological subsurface investigations al halbouni et al 2021 hoffmann et al 2022 liu et al 2018 large scale hydrological models primarily aim at accurate prediction of subsurface water resource dembélé et al 2020 and related applications of energy benz et al 2022 when characterizing large scale and high resolution subsurface structures the reconstruction from sparse observation data leads to significant uncertainty scheidt et al 2018 the uncertainty has a great negative impact on the analysis of subsurface solute transport and numerical simulation of geological processes arnold et al 2019 renard and allard 2013 therefore it is still a crucial issue how to effectively obtain a more realistic and high resolution view of large scale subsurface structures in practice from known conditioning data boreholes sections etc hydrological modeling is one of the most effective methods to characterize subsurface heterogeneous phenomena dagasan et al 2020 tahmasebi 2018 hydrological modeling methods can be divided into two categories deterministic approaches and geostatistical stochastic approaches as a commonly used modeling method in practice deterministic approaches imply that there is a definite relationship between all observations and hydrological phenomena vilhelmsen et al 2019 this is possible when the observations conditioning data are sufficient and the relationship is well established however it is very difficult to obtain a wealth of observation data for the characterization of large scale and high resolution subsurface structures geostatistical stochastic simulation methods make it possible to characterize the uncertainty of hydrological phenomena hoffimann et al 2021 linde et al 2015 many cases confirm that geostatistical simulation techniques can represent complex subsurface structures and extract high order statistics of hydrogeological variables chen et al 2019 pinheiro et al 2016 zakeri and mariethoz 2021 kriging based geostatistical methods mainly use a variogram model to describe the expected spatial behavior of the hydrological variables lima et al 2021 ochsner et al 2019 these methods are usually not good enough for characterizing heterogeneous and non stationary subsurface structures such as tortuous channels multiple point geostatistical mps methods can extract heterogeneous patterns from training images to characterize complex subsurface structures well mariethoz et al 2010 chen et al 2018 cui et al 2021a zuo et al 2020 nonetheless huge computational consumption and complex parameter configuration are often considered the main obstacles to characterize large scale subsurface structures by using mps methods mariethoz 2010 cui et al 2021b deep learning based generative models have been proposed to build non linear parameterization models to describe the subsurface structures from sparse observation data among them the autoregressive model generative adversarial networks gans and variational autoencoder vae are the most commonly used generative models in geosciences bao et al 2022 zhang et al 2022 these generative models have been applied to many fields such as mineral exploration zhang and zuo 2021 geological facies modeling chen et al 2022 feng et al 2022 yang et al 2022 zhan et al 2022 subsurface flow and transport zhong et al 2019 etc jiang et al 2021 combined the autoregressive model and deep residual u net architecture to predict fluid flows in large scale geosystems song et al 2022 proposed a gansim 3d framework and applied it to the 3 d reconstruction of cave reservoirs lopez alvis et al 2021 presented a constraining gradient based inversion method based on the vae architecture which was proved that vae can be used to reconstruct hydrological structures however these deep learning based approaches often suffer from various limitations autoregressive models are computationally expensive to generate high resolution hydrological structures vae based approaches often generate sub optimal realization quality gans based approaches require a large amount of training data and carefully designed optimization tricks to tame optimization instability besides limited by the requirement for high resolution training data it is difficult to characterize large scale subsurface structures without high resolution training data in contrast to other neural networks neural processes nps have a strong ability to predict probabilistic uncertainty with limited training data garnelo et al 2018 nps can predict a generalized stochastic process by extracting the context representation from observations gorden et al 2019 applied convolutional neural networks cnns in nps and presented the convolutional conditional neural process convcnp convcnp is able to learn context features from limited training data and presents strong generalization ability in similar application scenarios convcnp has been applied in weather prediction quinting and grams 2022 climate modeling vaughan et al 2022 etc cui et al 2022 presented a novel hydrogeological modeling approach based on convcnp named gm convcnp gm convcnp can characterize various multiple scale hydrogeological structures on limited training data however it is still difficult to characterize large scale hydrological structures on extremely sparse conditioning data by using gm convcnp the above mentioned limitations hydrogeological modeling methods make them difficult to characterize large scale hydrological structures with high resolution according to the problem of large scale subsurface characterization we envision that it would be possible to characterize large scale hydrological structures by combining downscaling with deep learning methods downscaling is a critical technique to bridge the gap between large scale spatial structures and coarse scale reference models wang et al 2021 downscaling technique can introduce small scale spatial characteristics into a coarse model with low resolution in the last decades downscaling methods have been widely used in atmospheric modeling castro et al 2005 biogeochemical cycles howard et al 2020 remote sensing oriani et al 2020 peng et al 2017 etc both statistical and deep learning based downscaling models can reconstruct large scale scenarios from local observation data abatzoglou and brown 2012 baño medina et al 2020 compared with statistical models deep learning models have been regarded as promising methods since they can extract characteristics from a large amount of training data by combining downscaling techniques and hydrological modeling it may take easier to characterize hydrological structures with high resolution at larger scales deep learning based downscaling techniques typically apply convolution architectures wang et al 2021 presented a novel super resolution deep residual network for downscaling daily precipitation and temperature serifi et al 2021 investigated the reconstruction of low resolution data in both space and time by using residual predicting network and deconvolutional network gonzalez 2023 summarized deep learning based downscaling of gridded data and integrates an open source downscaling framework named dl4ds the above mentioned downscaling methods can obtain high resolution data based on low resolution priori data and observations a completely different point between subsurface characterization approaches from these downscaling approaches is that there is no low resolution priori data for subsurface characterization thus the downscaling hydrological modeling needs the processes of both hydrological modeling and downscaling it requires not only reconstructing the subsurface realizations from sparse observations but also downscaling the low resolution realizations to high resolution realizations in this work we combine convolutional conditional neural process with downscaling technique and propose a two stage downscaling hydrological modeling workflow convcnp gbc the convcnp and a novel multiple point geostatistical bias correction strategy are embedded in the workflow to achieve multiple scale conditional simulation the presented conditioning constraint loss function is expected to perform hard conditioning on convcnp gbc to alleviate the requirement for high resolution training data only limited training data with low resolution will be used in the optimization process of the neural model besides to avoid the complexity of training multiple neural models at the same time we will use the same trained model in both stages in the workflow a computationally cheap geostatistical bias correction strategy is designed to correct bias in downscaling modeling both 2 d and 3 d experiments will be conducted to assess the effectiveness of the proposed convcnp gbc to sum up the proposed convcnp gbc makes four main contributions 1 to rapidly reconstruct large scale subsurface structures with heterogeneity according to limited observations a two stage downscaling hydrological modeling workflow based on convcnp is proposed the improved convcnp is applied in both the reconstruction of low resolution structures from sparse observations and the downscaling process 2 to correct the bias introduced by the loss of accuracy in the coordinate transformation during upscaling and downscaling a novel multiple point geostatistical bias correction strategy is embedded in the workflow 3 to achieve multiple scale conditional simulation a conditioning constraint loss function is embedded and performed hard conditioning on convcnp gbc 4 to alleviate the requirement for high resolution subsurface training data limited amount of training data are used in the optimization of convcnp gbc the remainder of this paper is organized as follows section 2 formulates the downscaling modeling problem and illustrates the proposed workflow then we demonstrate the architecture of convcnp gbc loss functions and the geostatistical bias correction strategy the parameter configuration and network training are illustrated in section 3 section 4 shows the experimental results the final section concludes the remarks and the limitations of the proposed method 2 methodology 2 1 problem formulation we are interested in predictive downscaling modeling of hydrological structures with hard conditioning which can be described by a function between spatial positions and hydrological variables it is difficult to directly characterize a fine scale description of subsurface hydrological structures through high resolution conditioning data and a coarse scale understanding variograms coarse scale training images etc of spatial characteristics thus two stage downscaling modeling is an appropriate option to reconstruct coarse scale realizations first then downscale the coarse scale realizations with bias correction suppose spatial positions are denoted as x and a set of known high resolution conditioning data are denoted as c h to construct coarse scale realizations the high resolution conditioning data is upscaling to c sampling bias σ s may be introduced during the upscaling according to the definition of hydrological modeling problem in cui et al 2022 the function f that infers properties of unknown spatial locations y from conditioning data is illustrated in formula 1 1 y f x c σ s the set of conditioning data c is composed of the known spatial locations and the corresponding properties for each known spatial location s i j k there is a corresponding variable v i j k v 1 v m which can be also represented as a function f c formula 2 2 v f c s then we can generalize the mathematical expression of the hydrological modeling process according to the formula 2 for each spatial location x we can get a corresponding y based on the conditioning data the first stage of downscaling modeling is to get this function through deep learning sections 2 3 and 2 4 the downscaling modeling can be illustrated by formula 3 3 y f x s v σ s according to formula 3 coarse scale realizations of hydrological structures m raw can be obtained as illustrated in formula 4 the coarse scale hydrological structures are regarded as the conditioning data using in the simulation process of fine scale grid m fine the bias of the conditioning data introduced during the upscaling process is also corrected by geostatistical methods section 2 5 4 m fine g b c f x m raw σ s 2 2 workflow of downscaling modeling via convcnp gbc this section details the workflow of the downscaling modeling process when there is no available high resolution training data a diagram of the downscaling modeling workflow is shown in fig 1 the whole process is divided into two stages model building fig 1a and model generating fig 1b in the model building stage depicted in fig 1a the conditioning data is randomly extracted from the low resolution training data for the same training data the number and distribution of conditioning data are random this random sampling strategy is the same as the strategy of gm convcnp in our previous work cui et al 2022 under the situation of limited amount of training data valid training data instances is very rare which is common in subsurface characterization when training a deep learning model fixing the spatial locations and the amount of conditioning data tends to over fit a deep learning model on limited training data by randomly selecting the number and the distribution of observations for each training the generalization ability of neural models can be greatly extended it also helps the neural models learn the sub task of the reconstruction of subsurface structures i e the nonlinear mapping between spatial locations and properties formula 1 subsequently the spatial locations of the conditioning data and the corresponding attributes are mapped to two cartesian grids with the same resolution as the training data in these grids the spatial locations of the conditioning points are set to 1 other locations are set to 0 as illustrated by formula 3 the spatial locations and the corresponding attributes of conditioning data are used to calculate the nonlinear relationship between spatial locations x and the attributes y by separating the spatial locations and attributes of conditioning data implicit characteristics of the spatial locations and attributes can be extracted separately and merged the strategy can also improve the learning performance of the neural network for 2 d hydrological structures the randomly selected conditioning data are a set of randomly distributed spatial points for 3 d structures the spatial distribution of conditioning data is random in the xy plane and continuous in the z direction then the grid cells of conditioning data and spatial locations are concatenated and input into convcnp gbc the architecture and optimization process of convcnp gbc will be described in detail in sections 2 3 and 2 4 after training convcnp gbc is able to fit a multiple variables normal mvn distribution function for the hydrological variables the mvn distribution function will be applied to the reconstruction of both coarse scale realizations and fine scale realizations in the model generating stage fig 1b demonstrates the entire process of the downscaling modeling first the low resolution conditioning data c is obtained by upscaling high resolution conditioning data c h suppose variables in c h and c are denoted as x h i j k and x i j k respectively the ratio of upscaling in the x y and z directions is m n and o a variable in c can be calculated by using formula 5 5 x i j k 1 mno i i m 1 i j j n 1 j k k o 1 k x h i j k the trained convcnp gbc is used to reconstruct coarse scale realizations according to the low resolution conditioning data c as the coordinates of high resolution conditioning data are not fully divisible by the ratio of upscaling there may be a loss of coordinate accuracy during the upscaling process according to the formula 4 bias is introduced due to the lack of coordinate accuracy and it will be corrected by a geostatistical simulation method section 2 5 the coarse scale models can be denoted as m raw with a resolution of h w d the target high resolution grid is m fine with a resolution of h w d the transform function of spatial locations between coarse scale models and high resolution grid can be illustrated as formula 6 6 x i j k x i h h j w w k d d the coarse scale realizations m raw are treated as conditioning data and input again into the trained convcnp gbc for the reconstruction of high resolution realizations after high resolution models are reconstructed we will correct the deviations according to the high resolution conditioning data and generated high resolution realizations by using geostatistical simulation 2 3 architecture of convcnp gbc as an advanced neural processes model convcnp has the ability to predict complex functions by giving only a few training data vaughan et al 2021 detailed descriptions for convcnp are illustrated in the appendix a given the limited amount of subsurface hydrological data and the problem formulation defined in section 2 1 convcnp is used as the framework of the proposed convcnp gbc fig 2 depicts the architecture of convcnp gbc for 3 d conditional simulation as shown in fig 2 low resolution 3 d hydrological conditioning data are first mapped to two cartesian grids with the resolution of h w d where h w and d are height width and depth respectively if d 1 the input data is a 2 d hydrological image the two grids are used to represent the spatial distribution of the conditioning data and the corresponding attributes the attributes of the conditioning data are preprocessed to the interval 1 to 1 the number of initial feature maps of the input conditioning data are set to 1 then convolution con v θ is used to extract the characteristics of spatial positions and the corresponding attributes of the input data the number of feature maps obtained by con v θ is c subsequently the features of spatial distribution and attribute distribution are concatenated according to the channel dimension the c dimension the concatenated features of spatial distribution and attribute distribution are passed through a cnn containing n convolutional layers in this process the implied correspondence characteristics between spatial and attribute distributions are extracted the relu activation function is used in the hidden layer of cnn besides to ensure that the extracted features have no null values we use the softplus activation function in the output layer of cnn dugas et al 2001 the formulation of the relu and the softplus activation functions are illustrated in formulas 7 and 8 7 r x x max 0 x 8 r x l o g 1 e x the number of feature maps obtained by cnn is 2c these deep features are divided into two parts which will be used as the mean and scale of the mvn distribution function after the training of convcnp gbc for the conditioning data with different spatial distribution the mvn function of the conditioning data in the spatial random field can be obtained which leads to the final realizations 2 4 optimization of convcnp gbc and loss functions the optimization process of deep learning neural networks is to maximize minimize the loss functions by training process to maximize the training efficiency we use the advanced adam optimizer in order to improve the reconstruction performance and the hard data conditioning during the downscaling process we apply a combination of two loss functions i e spatial reconstruction loss and conditioning constraint loss they are optimized to achieve expert level hydrological modeling performance and perfect hard conditioning to measure the difference between the generated probability distributions mvn distribution and the target probability distribution the likelihood estimation method is used to predict the parameters of the target probability distribution tibshirani and hastie 1987 the likelihood estimation method can reversely predict the unknown parameters according to the generated realizations thus we use the average negative log likelihood to establish the spatial reconstruction loss function the spatial reconstruction loss function is formulated in formula 9 9 loss nll 1 n i 1 n log m v n y i μ i x i c σ i x i c as presented in formula 9 the generated realizations are denoted as y i μ i and σ i are the mean and standard deviation extracted from the cnn in convcnp gbc the parameter θ x i c μ i x i c σ i x i c of the mvn distribution is estimated by convcnp gbc according to the conditioning data c besides to perform convcnp gbc in hard conditioning the conditioning constraint loss function is also applied in the optimization of convcnp gbc formula 10 presents the expression of the conditioning constraint loss function suppose the spatial locations of conditioning data can be denoted as s s is a 3 d space matrix consisting of 0 and 1 the number of 1 in s is the same as the number of the conditioning points c also the spatial distribution of 1 in s is exactly the same as the distribution of the conditioning data as shown in formula 10 we exploit the l1 distance to measure the difference between the conditioning data and the generated realizations 10 l 1 1 n i 1 n s y c the combination of the above loss functions are used to optimize convcnp gbc the final loss function is illustrated in formula 11 each loss function has a hyperparameter λ that can be adjusted 11 loss 1 n i 1 n λ 1 log m v n y i μ i x i c σ i x i c λ 2 s y c 2 5 geostatistical bias correction with high resolution conditioning data in the process of upscaling the high resolution conditioning data and downscaling the coarse scale realizations there is an inevitable bias between the generated realizations and the original high resolution conditioning data we propose a novel mps bias correction strategy to correct the deviation during the downscaling modeling since the generated realizations by downscaling modeling usually include a large number of grid cells it is expensive to correct the bias of the entire realizations to alleviate the high computational consumption we use a new adjustable parameter r b to control the operating range of bias correction fig 3 shows a brief case of geostatistical bias correction with high resolution conditioning data the input data are low resolution realizations high resolution realizations and high resolution conditioning data a low resolution realization will be linearly interpolated to get a reference with the same size as high resolution realizations then we divide the data points in generated high resolution realizations into three categories high resolution conditioning data high bias data and low bias data high resolution conditioning data are the initial conditioning data and are also considered as reference data for bias correction for each high resolution conditioning point a range is plotted with a radius of r b the grid cells in this range are considered as high bias data and the grid cells outside this range are considered as low bias data high bias data are the data for which the bias correction must be performed low bias data are not necessarily corrected due to the smaller distance with the reference data all high bias data will be added into the simulation path for re simulation according to the high resolution conditioning data and the interpolation reference the step by step workflow of the geostatistical bias correction strategy are listed in appendix b 3 network training both 2 d and 3 d convcnp gbc are trained and optimized by using the joint loss function and the adam optimizer kingma and ba 2015 they are able to process the input data from a single channel feature map the structured parameters of 2 d and 3 d convcnp gbc are illustrated in tables 1 and 2 respectively the hyperparameters of the adam optimizer are shown in table 3 the hyperparameters optimization process of the cnn is a very interesting problem to investigate many studies adjusted the hyperparameters based on experience and expertise which is not only unfounded but also time consuming during testing yeh et al 2023 in our work the configuration of hyperparameters mainly comes from gm convcnp and is achieved by fine tuning the configuration of gm convcnp cui et al 2022 for 2 d convcnp gbc the size of input training data is expressed as h w for each training we randomly select the number of conditioning data n c within the preset interval at the same time we randomly extract the points obeying bernoulli distribution in 2 d space as the spatial locations of conditioning data the variables corresponding to the spatial locations are used as conditioning data then the conditioning data and the spatial locations are fed into con v θ with a convolution kernel of 9 9 128 feature maps of the conditioning data and the spatial locations are extracted by con v θ and concatenated another convolution with a 1 1 convolution kernel is used to transform its channel to 128 after that a cnn network with a 5 5 convolution kernel is applied to obtain the implicit inline relationship between the spatial locations and the conditioning data as shown in fig 2 8 convolutional layers are integrated into cnn and each convolutional layer adopts the relu activation function the number of convolution layers can be modified according to the complexity of the data distribution the output layer of 2 d convcnp gbc is a convolution operation with a 1 1 convolution kernel by using the softplus activation function the mean and scale of the mvn distribution can be separated from the output according to the last dimension channel similar to 2 d convcnp gbc all 2 d convolutions are replaced by 3 d convolutions in 3 d convcnp gbc the size of input 3 d training data is denoted as h w d one thing worth noting is that the 3 d input data are completely different from the 3 d training data we first obtain data points randomly on the xy plane and then extract all data in the z direction corresponding to each point as the conditioning data another difference from 2 d convcnp gbc is the number of convolutional layers in cnn compared with 2 d hydrological structures 3 d hydrological structures contain more spatial characteristics therefore 3 d convcnp gbc contains more convolutional layers 4 experiments and validation all experiments were conducted to verify the applicability and performance of convcnp gbc presented in this work convcnp gbc was implemented by using python 3 7 and pytorch 1 8 0 the experimental equipment is a workstation with an intel r xeon r gold 5117 cpu 2 00 ghz 64 gb ram and three nivida geforce rtx 3090 ti gpus to measure the performance of the proposed method both statistical and non statistical indicators were used for validation the average root mean square error rmse peak signal to noise ratio psnr and structural similarity ssim are used to statistically present the error in the experimental results the data distribution of facies proportion cumulative distribution function of facies proportion multiple dimension scaling map variograms and connectivity function are selected as non statistical indicators to reveal the performance of the proposed approach 4 1 the nevada mountains example the first experiment refers to a hydrological environment with non stationary spatial morphology the digital elevation models dem of the nevada mountains mariethoz and caers 2014 are shown in fig 4 we used the low resolution walker lake dem images to validate the performance of convcnp gbc in characterizing 2 d hydrological structures as depicted in fig 4a both categorical attributes and continuous attributes dem images were segmented to local scale structures with a resolution of 128 128 pixels via a sliding window strategy a total of 780 images of categorical and continuous attributes were obtained respectively among them 624 images are used for the training of convcnp gbc and 156 images are used for testing for categorical dem images convcnp gbc was trained for 20 epochs which took 1 h and 20 min for continuous attributes dem images 1 h and 40 min were used for 25 epochs training of convcnp gbc to validate the ability of characterizing hydrological structures with categorical attributes we randomly picked 50 categorical test data and obtained 50 different realizations for each test data fig 5 shows five tests of high resolution conditioning data upscaled conditioning data low resolution realizations high resolution realizations and their corresponding low resolution references we exploited the high resolution simulation grid of 1024 1024 pixels and conditioning data spread across the simulation grid to validate our method 1000 high resolution conditioning data were randomly extracted from high resolution realizations generated from low resolution references via bicubic downscaling nuño maganda and arias estrada 2005 from the high resolution simulation grid shown in fig 5 it can be observed that it is difficult to directly reconstruct entire realizations from the sparse conditioning data we upscaled these 1000 discretized conditioning data to a low resolution simulation grid of 128 128 pixels according to formula 5 then low resolution realizations were generated by using the trained convcnp gbc from the visual appearance the spatial structures in low resolution realizations are successfully reconstructed it can be seen that convcnp gbc can reproduce the global spatial characteristics of hydrological structures these low resolution realizations were considered as conditioning data and input again into the convcnp gbc for the reconstruction of high resolution realizations the high resolution realizations significantly increase the number of grid cells and insert more details based on these generated low resolution realizations this further illustrates the effectiveness of the proposed downscaling modeling workflow we also calculated the rmse psnr and ssim to quantitatively describe the performance of the proposed convcnp gbc rmse psnr and ssim of the generated realizations are 0 091 13 13 0 589 respectively from these quantitative statistics it can be derived that convcnp gbc has the ability of characterizing large scale subsurface structures from sparse observations to further reveal the performance of the proposed method we statistically analyze the differences between these generated high resolution realizations and the corresponding references since the dimensions of low resolution references are much smaller than the generated high resolution realizations we compared the statistics according to the following strategy 1 for each pixel voxel in a reference calculate the corresponding patch in a high resolution realization 2 for categorical variables the mode index is selected to represent the patch for continuous variables the patch is represented by the mean value 3 after computing all the pixels in a reference model a representative realization that represent a high resolution realization can be obtained then the dimensions of the representative realization are the same as the dimensions of a reference for subsequent verification experiments we adopted the same comparison strategy cumulative distribution functions are often used to describe the probability distribution of random variables fig 6 a depicts the cumulative distribution functions between the references and the corresponding realizations the red curve of the realizations is close to the blue curve of the corresponding references it confirms that the proposed method is able to reproduce the probability distribution of hydrological variables to fully reflect the diversity of the generated heterogeneous patterns multiple dimension scaling mds map is plotted by calculating the euclidean distances between the reference and 50 generated representative realizations fig 6b it can be observed that the red dots of the realizations are concentrated around the blue triangle of the corresponding reference it shows that the distances between the generated realizations and the reference are very close in 2 d space as shown in fig 6c we also plotted the variograms both in the x direction and the y direction the blue curves of the generated realizations closely fit the black curve of the corresponding reference according to the connectivity functions demonstrated in fig 6d it can be observed that the blue curves of the realizations and the black curve of the corresponding reference are very similar these statistics indicate that the proposed method can accurately represent heterogeneous patterns and reconstruct hydrological structures the dataset presented in fig 4b is used to validate the performance of convcnp gbc for characterizing hydrological structures with continuous attributes similar to the experiment above 50 test data were randomly selected from the test dataset for each test data we also generated 50 different realizations fig 7 shows five of these tests which consist of conditioning data low resolution realization high resolution realization and corresponding low resolution reference compared with categorical attributes it is even more difficult to reconstruct continuous attribute high resolution 1024 1024 pixels realizations directly from 1000 conditioning data these conditioning data were upscaled to the low resolution simulation grid with a resolution of 128 128 pixels according to these generated low resolution realizations the spatial distribution characteristics of hydrological variables are perfectly reproduced it indicates that convcnp gbc owns the ability to characterize hydrological structures with continuous attributes by downscaling with convcnp gbc these obtained high resolution realizations further prove the effectiveness of our downscaling modeling workflow we also used rmse psnr and ssim to compare the generated realizations with the corresponding references the values of rmse psnr and ssim are 0 026 29 71 0 941 respectively it can be inferred that convcnp gbc is more advantageous in characterizing subsurface structures with continuous attributes it confirms that convcnp gbc can reproduce the statistics of large scale subsurface structures with continuous attributes fig 8 shows the comparison statistics between the reference and the generated realizations we also plotted the cumulative distribution function for continuous attribute variables note that the cumulative distribution function we used here is actually the complementary cumulative distribution function ccdf fig 8a shows the ccdf of the realizations and the corresponding references when the variable is greater than 0 5 the blue curve of the references and the red curve of the realizations fit closely it proves that the cumulative distributions between low resolution references and the high resolution realizations of convcnp gbc are similar to show the diversity of the realizations generated by convcnp gbc the mds map of the reference and the corresponding realizations were drawn fig 8b the red dots of realizations are concentrated around the blue triangle of the corresponding reference it also demonstrates that the high resolution realizations generated by convcnp gbc are close to the reference in 2 d space besides we also plotted the variograms of the generated realizations and the reference in x and y directions in fig 8c regardless of the variograms in x or y direction the blue curves of the realizations are highly resembling the black curve of the reference the connectivity functions of the reference and the realizations fig 8d also demonstrate that connectivity areas in the realizations are very similar to those in the reference the above statistical features between the realizations and the corresponding reference are all similar it further confirms the availability of convcnp gbc in the 2 d reconstruction of hydrological structures 4 2 the berea sandstone example the berea sandstone is a typical kind of oil and gas reservoir the high resolution characterization of sandstone is very important for the exploration of oil gas and water resources fig 9 shows the sandstone models from okabe and blunt 2007 used in this 3 d test similar to the 2 d training dataset we also used the sliding window strategy to segment the original sandstone model with a resolution of 200 200 200 voxels 6470 small scale data set with a resolution of 40 40 40 voxels were obtained the small scale data set was divided into 5176 training data and 1294 test data according to the 80 20 rule the 3 d convcnp gbc was trained for 17 epochs using the parameters illustrated in section 3 the entire training process took a total of 4 h and 40 min in this test we randomly extracted 50 different high resolution test data 80 80 80 voxels and 50 different realizations were generated for each test data we evenly sampled 40 boreholes from each test data as the conditioning data for each test conditioning data we upscaled the high resolution conditioning data from the resolution of 80 80 80 voxels to the resolution of 40 40 40 voxels then low resolution realizations were obtained by using the trained 3 d convcnp gbc according to the corresponding upscaled conditioning data the high resolution realizations 80 80 80 voxels were generated based on the downscaling modeling from the low resolution realizations fig 10 depicts ten of the generated low resolution realizations high resolution realizations and corresponding low resolution references by comparing the generated low resolution realizations with the corresponding references the heterogenous spatial patterns of the sandstone were successfully reconstructed it proves that the proposed 3 d convcnp gbc is available to reconstruct 3 d subsurface hydrological structures from limited conditioning data it can be seen from the high resolution realizations that the generated realizations not only preserve the spatial structures of low resolution realizations but also insert many details it once again proves that 3 d convcnp gbc can reproduce high resolution heterogeneous subsurface structures same as the above experiments we also compared the quantitative statistics of the reproduced sandstone realizations with the corresponding references the average rmse psnr and ssim of the sandstone realizations are 0 098 10 06 0 573 respectively it can be observed that the 3 d task is significantly more difficult than 2 d task however convcnp gbc is still able to reconstruct high resolution sandstone realizations from sparse observations fig 11 shows the comparative analysis of the statistics in the generated realizations and the corresponding references the scatter plots of the facies proportion in these generated realizations and the corresponding references are depicted in fig 11a each spot represents a generated realization or a reference the scatter plots demonstrate that the facies proportion of the 50 realizations and the corresponding references are very close the box plots further illustrate that the maximum minimum and median of the facies proportions in the generated realizations are consistent with the facies proportions in the references from the comparison scatter plots we observed that there is an error of 0 02 attribute proportion between some realizations and the corresponding references the box plots also illustrate this error by the maximum and median values from our speculation the reason for this phenomenon can be broadly attributed to the fact that the distribution of 3 d conditioning data is known to be sparser than the distribution of 2 d conditioning data the insufficient amount of conditioning data leads to allowable errors of attribute proportion in a small number of realizations as a result the presence of certain extreme values of attribute proportion affects the performance of the maximum and median values of overall realizations besides we drew the cumulative distribution function to compare the heterogeneous patterns between the generated realizations and the corresponding references as depicted in fig 11b the blue curve of the references is resembling with the red curve of the realizations besides to confirm the diversity of the generated realizations we calculated the euclidean distance between a reference and the corresponding 50 realizations the distances were projected to 2 d space and their spatial distribution is shown in fig 11c these red dots of the realizations are distributed around the blue triangle of the reference it can be seen that the distances between them are all close in fig 11d and 11e the variograms and connectivity functions are reasonably well reproduced these above statistical characteristics further confirm that convcnp gbc can reproduce heterogeneous 3 d patterns and reconstruct subsurface hydrological structures 4 3 comparison with convcnp we also investigated the simulation performance between the proposed convcnp gbc and convcnp by using the proposed downscaling modeling workflow a set of comparative experiments were conducted based on the dataset illustrated in section 4 1 it should be noted that we used low resolution training data for the optimization of both convcnp gbc and convcnp convcnp gbc has the same hyperparameters and number of hidden layers as convcnp for categorical attributes validation fig 12 800 samples were regarded as conditioning data the locations of these samples in a high resolution simulation grid 1024 1024 pixels are shown in fig 12a note that the point size of conditioning data is enlarged by a factor of 20 to visualize them from the high resolution simulation grid figs 12b and 12c present the high resolution realizations obtained by convcnp and convcnp gbc respectively the parameters of geostatistical bias correction were set as r 50 rb 30 t 0 01 the search range r and the threshold t are the most commonly used parameters in mps simulations in this validation the parameters were assigned according to the guide of performing mps simulations which discussed the relationship between r t and final realizations in detail meerschman et al 2013 from the local area circled in red rectangles it can be seen that convcnp gbc can achieve more accurate realizations compared with convcnp it confirms that the proposed convcnp gbc has the ability to fine tune the generated realizations based on high resolution conditioning data to statistically illustrate the effectiveness difference between convcnp and convcnp gbc the variograms of reference and generated realizations are drawn in fig 12d no matter in the x or y direction we observe that the blue curve of convcnp gbc is closer to the curve of the reference than the yellow curve of convcnp this further proves that the proposed convcnp gbc can significantly improve the quality of generated realizations besides we also compare the difference in the facies proportion of the generated realizations fig 12e it can be recognized that the facies proportion generated by convcnp gbc is closer to the facies proportion in the reference the quantitative statistics of the generated categorial attribute realizations are illustrated in table 4 these above results all indicate that the statistics of the realizations generated by convcnp gbc are more consistent with the statistics of the reference we also tried the 2 d hydrological structures with continuous attributes to compare the performance between convcnp gbc and convcnp fig 13 1200 randomly selected samples were regarded as the conditioning data and their spatial distribution in a high resolution grid is drawn in fig 13a figs 13b and 13c show the comparative realizations of convcnp and convcnp gbc respectively the parameters of geostatistical bias correction were set as follows r 50 rb 40 t 0 01 it can be seen that the heterogenous patterns and property distribution are successfully reconstructed in both realizations the area circled by red rectangles confirm that the proposed convcnp gbc can indeed adjust the spatial structures of the generated realizations based on the conditioning data the variogram plots of realization generated by convcnp gbc realization generated by convcnp and corresponding reference are utilized to illustrate the statistical characteristics fig 13d the curve of convcnp gbc is closer to the curve of reference than the curve of convcnp it convinces that the statistical characteristics of the realizations generated by convcnp gbc are more consistent with the statistical characteristics of the reference besides we applied the k nearest neighbor knn classification algorithm peterson 2009 to classify the property in realizations into three categories from fig 13e it is convenient to observe that the property distribution in realizations of convcnp gbc is closer to the property distribution in the reference the quantitative statistics of the generated continuous attribute realizations are illustrated in table 5 these comparative results indicate that convcnp gbc proposed in this work has more advantages in characterizing large scale and high resolution subsurface structures 5 discussions 5 1 convcnp gbc in subsurface characterization the scarcity of resources in terms of fossil fuels groundwater minerals and fluids will put considerable stress on subsurface resource supplies dolan et al 2021 rosa et al 2020 large scale and high resolution subsurface characterization can provide a credible framework for the natural resource evaluation in our work we present a two stage downscaling hydrological modeling method convcnp gbc for rapidly and precisely reconstructing large scale subsurface structures with high resolution for large scale characterization problems almost all existing hydrological modeling methods are limited by the huge computational consumption by embedding downscaling technique into the subsurface characterization the computational complexity and computational consumption are significantly reduced in our work in addition convcnp gbc is able to add local scale detail features while taking into account the global characteristics of subsurface structures several experiments presented in this paper demonstrate the availability of the downscaling modeling framework for large scale subsurface characterization the proposed convcnp gbc can integrate the advantages of hydrological modeling and downscaling it can be extended and applied to characterize large scale heterogeneous subsurface structures and phenomena and further support human activities such as the exploration of geological resources and the protection of the environment the main advantage of our work is that a two stage general workflow for downscaling modeling is proposed we divided the workflow into model building and model generating processes in the stage of model building the convcnp gbc is trained with low resolution data and applied into the reconstructing and downscaling processes of hydrological structures in the stage of model generating high resolution realizations can be generated rapidly according to the trained model high resolution conditioning data and the gbc strategy it is worth noting that the applicability of the proposed workflow is not specific to a particular neural model most procedures where convcnp gbc is used in the workflow can be replaced with other neural networks besides an improved convcnp model is used not only for reconstructing coarse scale realizations but also for downscaling as a highly generalizable meta learning approach convcnp is capable of predicting all spatial positions in the entire space directly from sparse observations and can be applied in subsurface characterization cui et al 2022 on the basis of convcnp the convcnp gbc model is trained by using limited training data with low resolution which ensures it can be applied to the both reconstruction and downscaling processes of hydrological structures compared to other deep learning based downscaling methods that require high resolution training data our approach can be more adapted to high resolution subsurface characterization in addition the complicated training process of multiple neural models can be avoided by using the same model when reconstructing coarse scale structures and the downscaling modeling the experimental results and analysis illustrated in sections 4 1 and 4 2 confirm the performance of convcnp gbc though the 3 d reconstruction study uses an example of binary facies for the verification convcnp gbc can be easily extended to the reconstruction in the cases involving multiple facies or continuous attributes another important advantage of the convcnp gbc is that a joint loss function is used to improve the optimization process the improved loss function ensures that the entire subsurface structures can be reconstructed while obeying the hard conditioning constraints the spatial reconstruction loss is used to optimize the prediction process for the variables in random fields based on the negative log likelihood estimation the conditioning constraint loss is implemented by calculating the l1 distance between conditioning data and realizations the hyperparameters of each term of the joint loss function can be fine tuned to adapt for reconstructing different hydrological structures the presented gbc strategy is able to quickly correct the deviations introduced from low resolution coarse realizations to high resolution realizations the operating range of bias correction increases the computational efficiency the local bias correction strategy can both ensure the precision of the generated realizations and significantly improve the efficiency of the entire downscaling modeling workflow besides different metrics are used to calculate the similarity of data events for categorical and continuous variables respectively the validation in section 4 3 proves the improvement of the proposed convcnp gbc 5 2 limitations and future work the limitation is that the distribution of high resolution conditioning data affects the performance of convcnp gbc if the high resolution conditioning data are distributed unevenly then the upscaled conditioning data are not sufficient to reconstruct credible coarse scale realizations the high resolution realizations based on unreliable coarse scale realizations are difficult to correct the bias via gbc which may result in even greater computational overhead than direct simulation using the same neural network model to perform approximate but different tasks is an issue worthy of further study for example the use of a unified convcnp for both the hydrological modeling and downscaling in convcnp gbc in future work we plan to insert an identification option in convcnp gbc the option is used to help the trained model to determine whether the current task is reconstructing coarse scale realizations or downscaling realizations it holds great promise to improve the performance of convcnp gbc for characterizing large scale subsurface structures with heterogeneous patterns 6 conclusions in this paper we propose a two stage downscaling hydrological modeling approach convcnp gbc to characterize large scale and high resolution subsurface structures and heterogeneous phenomena the approach is based on the convcnp and includes the process of coarse scale realization reconstruction and the process of high resolution realization downscaling we used an improved convcnp embedding for both processes the same trained convcnp model enables two different stages of downscaling hydrological modeling it greatly reduces the training consumption the mps based bias correction method is also applied in the approach we tested the proposed method on both 2 d and 3 d hydrogeological structures and compared it with the original convcnp the numerical results confirm that the convcnp gbc can integrate the advantages of convcnp and have the capability of characterizing large scale and high resolution subsurface structures the proposed convcnp gbc can be widely used to the 3 d characterization of subsurface structures such as hydrological modeling and geophysical inversion credit authorship contribution statement zhesi cui conceptualization methodology software writing original draft validation formal analysis qiyu chen methodology writing original draft project administration supervision gang liu writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is supported in part by the national natural science foundation of china 42172333 41902304 u1711267 and by the knowledge innovation program of wuhan shuguang project 2022010801020206 appendix a convolutional conditional neural processes convolutional conditional neural processes convcnp can rapidly adapt to new conditioning data and estimate the uncertainty in their predictions it allows making correct predictions by giving only a few training data the forward pass of the convcnp is illustrated in algorithm 1 in step 3 the first channel of the functional representation h is the density channel it is helpful to divide the remaining channels when there is a large variation in the density of input locations algorithm 1 forward pass of the convcnp require a image i b context m c c target mask m t forward pass steps begin 1 calculate extract context set z c by using i and m c 2 obtain the functional representation h with c channels by performing a convolution of m c z c t 3 normalize the convolution by dividing the remaining channels by h 1 c h 1 c h 0 4 find the function f with 2c channels by applying a cnn of h and m c 5 obtain the first c outputs are the means of a mvn distribution and the second c the standard deviation end appendix b workflow of geostatistical bias correction strategy the steps of the geostatistical bias correction strategy are listed in algorithm 2 note that the steps marked with are important in the algorithm in step 2 uncorrected nodes are initialized to be empty so as not to affect the bias correction when comparing the data events these data events must keep on a uniform scale thus in steps 8 and 9 the corresponding areas in the low resolution data are projected to a high resolution grid by linear interpolation in step 12 euclidean distance is often used to measure the difference between two data events with categorical attributes for continuous variables the mean absolute error is used to present the distance between two data events algorithm 2 the workflow of the geostatistical bias correction strategy inputs a high resolution conditioning data ch b high resolution realization rh c low resolution realization r d maximal search radius r distance threshold t and operating range of bias correction rb algorithm steps 1 assign the high resolution conditioning data ch to the high resolution grid cells in rh 2 initialize a path list consisting of all high bias data according to rb and ch 3 while path list size 0 do 4 get the current cell x to be corrected from path list 5 identify the data event dev x consisting of informed cells in radius r around x 6 initialize d dev x dev y 7 while d dev x dev y t do 8 find the corresponding area a of the current data event in r 9 project a into the high resolution grid by linear interpolation 10 randomly sample a location y in a 11 identify dev y centered on y with the same geometry of nx 12 compute the distance d dev x dev y 13 end while 14 assign the closest value to the current node x 15 end while output high resolution realization rh appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129498 appendix c supplementary data the following are the supplementary data to this article supplementary data 1 
1956,hydrological modeling plays an indispensable role in many practical applications such as hydrology water resources assessment and environmental survey downscaling approach is able to obtain high resolution hydrological models according to a prior reference model in this work we propose a downscaling hydrological modeling approach based on convolutional conditional neural processes convcnp and a novel geostatistical bias correction named convcnp gbc a two stage downscaling modeling workflow is presented in the model building stage to reduce the training consumption of the convcnp low resolution conditioning data are used for training in the model generating stage the convcnp is used to reconstruct coarse scale realizations based on upscaling sampled conditioning data and to generate high resolution realizations by downscaling besides a geostatistical bias correction strategy is presented to refine large scale realizations with high resolution conditioning data constraints the experimental results confirm that convcnp gbc can reproduce heterogeneous patterns by using coarse scale conditioning data and characterize hydrological structures the downscaled realizations also have a high spatial correspondence with the coarse scale realizations which provide fine subsurface heterogeneous structures the proposed method can be easily extended to reservoir modeling geophysical inversion etc keywords hydrological modeling convolutional conditional neural process geostatistical bias correction deep learning data availability data will be made available on request 1 introduction the environmental concerns and the exploration of geological resources are pushing geoscientific research toward high resolution hydrological subsurface investigations al halbouni et al 2021 hoffmann et al 2022 liu et al 2018 large scale hydrological models primarily aim at accurate prediction of subsurface water resource dembélé et al 2020 and related applications of energy benz et al 2022 when characterizing large scale and high resolution subsurface structures the reconstruction from sparse observation data leads to significant uncertainty scheidt et al 2018 the uncertainty has a great negative impact on the analysis of subsurface solute transport and numerical simulation of geological processes arnold et al 2019 renard and allard 2013 therefore it is still a crucial issue how to effectively obtain a more realistic and high resolution view of large scale subsurface structures in practice from known conditioning data boreholes sections etc hydrological modeling is one of the most effective methods to characterize subsurface heterogeneous phenomena dagasan et al 2020 tahmasebi 2018 hydrological modeling methods can be divided into two categories deterministic approaches and geostatistical stochastic approaches as a commonly used modeling method in practice deterministic approaches imply that there is a definite relationship between all observations and hydrological phenomena vilhelmsen et al 2019 this is possible when the observations conditioning data are sufficient and the relationship is well established however it is very difficult to obtain a wealth of observation data for the characterization of large scale and high resolution subsurface structures geostatistical stochastic simulation methods make it possible to characterize the uncertainty of hydrological phenomena hoffimann et al 2021 linde et al 2015 many cases confirm that geostatistical simulation techniques can represent complex subsurface structures and extract high order statistics of hydrogeological variables chen et al 2019 pinheiro et al 2016 zakeri and mariethoz 2021 kriging based geostatistical methods mainly use a variogram model to describe the expected spatial behavior of the hydrological variables lima et al 2021 ochsner et al 2019 these methods are usually not good enough for characterizing heterogeneous and non stationary subsurface structures such as tortuous channels multiple point geostatistical mps methods can extract heterogeneous patterns from training images to characterize complex subsurface structures well mariethoz et al 2010 chen et al 2018 cui et al 2021a zuo et al 2020 nonetheless huge computational consumption and complex parameter configuration are often considered the main obstacles to characterize large scale subsurface structures by using mps methods mariethoz 2010 cui et al 2021b deep learning based generative models have been proposed to build non linear parameterization models to describe the subsurface structures from sparse observation data among them the autoregressive model generative adversarial networks gans and variational autoencoder vae are the most commonly used generative models in geosciences bao et al 2022 zhang et al 2022 these generative models have been applied to many fields such as mineral exploration zhang and zuo 2021 geological facies modeling chen et al 2022 feng et al 2022 yang et al 2022 zhan et al 2022 subsurface flow and transport zhong et al 2019 etc jiang et al 2021 combined the autoregressive model and deep residual u net architecture to predict fluid flows in large scale geosystems song et al 2022 proposed a gansim 3d framework and applied it to the 3 d reconstruction of cave reservoirs lopez alvis et al 2021 presented a constraining gradient based inversion method based on the vae architecture which was proved that vae can be used to reconstruct hydrological structures however these deep learning based approaches often suffer from various limitations autoregressive models are computationally expensive to generate high resolution hydrological structures vae based approaches often generate sub optimal realization quality gans based approaches require a large amount of training data and carefully designed optimization tricks to tame optimization instability besides limited by the requirement for high resolution training data it is difficult to characterize large scale subsurface structures without high resolution training data in contrast to other neural networks neural processes nps have a strong ability to predict probabilistic uncertainty with limited training data garnelo et al 2018 nps can predict a generalized stochastic process by extracting the context representation from observations gorden et al 2019 applied convolutional neural networks cnns in nps and presented the convolutional conditional neural process convcnp convcnp is able to learn context features from limited training data and presents strong generalization ability in similar application scenarios convcnp has been applied in weather prediction quinting and grams 2022 climate modeling vaughan et al 2022 etc cui et al 2022 presented a novel hydrogeological modeling approach based on convcnp named gm convcnp gm convcnp can characterize various multiple scale hydrogeological structures on limited training data however it is still difficult to characterize large scale hydrological structures on extremely sparse conditioning data by using gm convcnp the above mentioned limitations hydrogeological modeling methods make them difficult to characterize large scale hydrological structures with high resolution according to the problem of large scale subsurface characterization we envision that it would be possible to characterize large scale hydrological structures by combining downscaling with deep learning methods downscaling is a critical technique to bridge the gap between large scale spatial structures and coarse scale reference models wang et al 2021 downscaling technique can introduce small scale spatial characteristics into a coarse model with low resolution in the last decades downscaling methods have been widely used in atmospheric modeling castro et al 2005 biogeochemical cycles howard et al 2020 remote sensing oriani et al 2020 peng et al 2017 etc both statistical and deep learning based downscaling models can reconstruct large scale scenarios from local observation data abatzoglou and brown 2012 baño medina et al 2020 compared with statistical models deep learning models have been regarded as promising methods since they can extract characteristics from a large amount of training data by combining downscaling techniques and hydrological modeling it may take easier to characterize hydrological structures with high resolution at larger scales deep learning based downscaling techniques typically apply convolution architectures wang et al 2021 presented a novel super resolution deep residual network for downscaling daily precipitation and temperature serifi et al 2021 investigated the reconstruction of low resolution data in both space and time by using residual predicting network and deconvolutional network gonzalez 2023 summarized deep learning based downscaling of gridded data and integrates an open source downscaling framework named dl4ds the above mentioned downscaling methods can obtain high resolution data based on low resolution priori data and observations a completely different point between subsurface characterization approaches from these downscaling approaches is that there is no low resolution priori data for subsurface characterization thus the downscaling hydrological modeling needs the processes of both hydrological modeling and downscaling it requires not only reconstructing the subsurface realizations from sparse observations but also downscaling the low resolution realizations to high resolution realizations in this work we combine convolutional conditional neural process with downscaling technique and propose a two stage downscaling hydrological modeling workflow convcnp gbc the convcnp and a novel multiple point geostatistical bias correction strategy are embedded in the workflow to achieve multiple scale conditional simulation the presented conditioning constraint loss function is expected to perform hard conditioning on convcnp gbc to alleviate the requirement for high resolution training data only limited training data with low resolution will be used in the optimization process of the neural model besides to avoid the complexity of training multiple neural models at the same time we will use the same trained model in both stages in the workflow a computationally cheap geostatistical bias correction strategy is designed to correct bias in downscaling modeling both 2 d and 3 d experiments will be conducted to assess the effectiveness of the proposed convcnp gbc to sum up the proposed convcnp gbc makes four main contributions 1 to rapidly reconstruct large scale subsurface structures with heterogeneity according to limited observations a two stage downscaling hydrological modeling workflow based on convcnp is proposed the improved convcnp is applied in both the reconstruction of low resolution structures from sparse observations and the downscaling process 2 to correct the bias introduced by the loss of accuracy in the coordinate transformation during upscaling and downscaling a novel multiple point geostatistical bias correction strategy is embedded in the workflow 3 to achieve multiple scale conditional simulation a conditioning constraint loss function is embedded and performed hard conditioning on convcnp gbc 4 to alleviate the requirement for high resolution subsurface training data limited amount of training data are used in the optimization of convcnp gbc the remainder of this paper is organized as follows section 2 formulates the downscaling modeling problem and illustrates the proposed workflow then we demonstrate the architecture of convcnp gbc loss functions and the geostatistical bias correction strategy the parameter configuration and network training are illustrated in section 3 section 4 shows the experimental results the final section concludes the remarks and the limitations of the proposed method 2 methodology 2 1 problem formulation we are interested in predictive downscaling modeling of hydrological structures with hard conditioning which can be described by a function between spatial positions and hydrological variables it is difficult to directly characterize a fine scale description of subsurface hydrological structures through high resolution conditioning data and a coarse scale understanding variograms coarse scale training images etc of spatial characteristics thus two stage downscaling modeling is an appropriate option to reconstruct coarse scale realizations first then downscale the coarse scale realizations with bias correction suppose spatial positions are denoted as x and a set of known high resolution conditioning data are denoted as c h to construct coarse scale realizations the high resolution conditioning data is upscaling to c sampling bias σ s may be introduced during the upscaling according to the definition of hydrological modeling problem in cui et al 2022 the function f that infers properties of unknown spatial locations y from conditioning data is illustrated in formula 1 1 y f x c σ s the set of conditioning data c is composed of the known spatial locations and the corresponding properties for each known spatial location s i j k there is a corresponding variable v i j k v 1 v m which can be also represented as a function f c formula 2 2 v f c s then we can generalize the mathematical expression of the hydrological modeling process according to the formula 2 for each spatial location x we can get a corresponding y based on the conditioning data the first stage of downscaling modeling is to get this function through deep learning sections 2 3 and 2 4 the downscaling modeling can be illustrated by formula 3 3 y f x s v σ s according to formula 3 coarse scale realizations of hydrological structures m raw can be obtained as illustrated in formula 4 the coarse scale hydrological structures are regarded as the conditioning data using in the simulation process of fine scale grid m fine the bias of the conditioning data introduced during the upscaling process is also corrected by geostatistical methods section 2 5 4 m fine g b c f x m raw σ s 2 2 workflow of downscaling modeling via convcnp gbc this section details the workflow of the downscaling modeling process when there is no available high resolution training data a diagram of the downscaling modeling workflow is shown in fig 1 the whole process is divided into two stages model building fig 1a and model generating fig 1b in the model building stage depicted in fig 1a the conditioning data is randomly extracted from the low resolution training data for the same training data the number and distribution of conditioning data are random this random sampling strategy is the same as the strategy of gm convcnp in our previous work cui et al 2022 under the situation of limited amount of training data valid training data instances is very rare which is common in subsurface characterization when training a deep learning model fixing the spatial locations and the amount of conditioning data tends to over fit a deep learning model on limited training data by randomly selecting the number and the distribution of observations for each training the generalization ability of neural models can be greatly extended it also helps the neural models learn the sub task of the reconstruction of subsurface structures i e the nonlinear mapping between spatial locations and properties formula 1 subsequently the spatial locations of the conditioning data and the corresponding attributes are mapped to two cartesian grids with the same resolution as the training data in these grids the spatial locations of the conditioning points are set to 1 other locations are set to 0 as illustrated by formula 3 the spatial locations and the corresponding attributes of conditioning data are used to calculate the nonlinear relationship between spatial locations x and the attributes y by separating the spatial locations and attributes of conditioning data implicit characteristics of the spatial locations and attributes can be extracted separately and merged the strategy can also improve the learning performance of the neural network for 2 d hydrological structures the randomly selected conditioning data are a set of randomly distributed spatial points for 3 d structures the spatial distribution of conditioning data is random in the xy plane and continuous in the z direction then the grid cells of conditioning data and spatial locations are concatenated and input into convcnp gbc the architecture and optimization process of convcnp gbc will be described in detail in sections 2 3 and 2 4 after training convcnp gbc is able to fit a multiple variables normal mvn distribution function for the hydrological variables the mvn distribution function will be applied to the reconstruction of both coarse scale realizations and fine scale realizations in the model generating stage fig 1b demonstrates the entire process of the downscaling modeling first the low resolution conditioning data c is obtained by upscaling high resolution conditioning data c h suppose variables in c h and c are denoted as x h i j k and x i j k respectively the ratio of upscaling in the x y and z directions is m n and o a variable in c can be calculated by using formula 5 5 x i j k 1 mno i i m 1 i j j n 1 j k k o 1 k x h i j k the trained convcnp gbc is used to reconstruct coarse scale realizations according to the low resolution conditioning data c as the coordinates of high resolution conditioning data are not fully divisible by the ratio of upscaling there may be a loss of coordinate accuracy during the upscaling process according to the formula 4 bias is introduced due to the lack of coordinate accuracy and it will be corrected by a geostatistical simulation method section 2 5 the coarse scale models can be denoted as m raw with a resolution of h w d the target high resolution grid is m fine with a resolution of h w d the transform function of spatial locations between coarse scale models and high resolution grid can be illustrated as formula 6 6 x i j k x i h h j w w k d d the coarse scale realizations m raw are treated as conditioning data and input again into the trained convcnp gbc for the reconstruction of high resolution realizations after high resolution models are reconstructed we will correct the deviations according to the high resolution conditioning data and generated high resolution realizations by using geostatistical simulation 2 3 architecture of convcnp gbc as an advanced neural processes model convcnp has the ability to predict complex functions by giving only a few training data vaughan et al 2021 detailed descriptions for convcnp are illustrated in the appendix a given the limited amount of subsurface hydrological data and the problem formulation defined in section 2 1 convcnp is used as the framework of the proposed convcnp gbc fig 2 depicts the architecture of convcnp gbc for 3 d conditional simulation as shown in fig 2 low resolution 3 d hydrological conditioning data are first mapped to two cartesian grids with the resolution of h w d where h w and d are height width and depth respectively if d 1 the input data is a 2 d hydrological image the two grids are used to represent the spatial distribution of the conditioning data and the corresponding attributes the attributes of the conditioning data are preprocessed to the interval 1 to 1 the number of initial feature maps of the input conditioning data are set to 1 then convolution con v θ is used to extract the characteristics of spatial positions and the corresponding attributes of the input data the number of feature maps obtained by con v θ is c subsequently the features of spatial distribution and attribute distribution are concatenated according to the channel dimension the c dimension the concatenated features of spatial distribution and attribute distribution are passed through a cnn containing n convolutional layers in this process the implied correspondence characteristics between spatial and attribute distributions are extracted the relu activation function is used in the hidden layer of cnn besides to ensure that the extracted features have no null values we use the softplus activation function in the output layer of cnn dugas et al 2001 the formulation of the relu and the softplus activation functions are illustrated in formulas 7 and 8 7 r x x max 0 x 8 r x l o g 1 e x the number of feature maps obtained by cnn is 2c these deep features are divided into two parts which will be used as the mean and scale of the mvn distribution function after the training of convcnp gbc for the conditioning data with different spatial distribution the mvn function of the conditioning data in the spatial random field can be obtained which leads to the final realizations 2 4 optimization of convcnp gbc and loss functions the optimization process of deep learning neural networks is to maximize minimize the loss functions by training process to maximize the training efficiency we use the advanced adam optimizer in order to improve the reconstruction performance and the hard data conditioning during the downscaling process we apply a combination of two loss functions i e spatial reconstruction loss and conditioning constraint loss they are optimized to achieve expert level hydrological modeling performance and perfect hard conditioning to measure the difference between the generated probability distributions mvn distribution and the target probability distribution the likelihood estimation method is used to predict the parameters of the target probability distribution tibshirani and hastie 1987 the likelihood estimation method can reversely predict the unknown parameters according to the generated realizations thus we use the average negative log likelihood to establish the spatial reconstruction loss function the spatial reconstruction loss function is formulated in formula 9 9 loss nll 1 n i 1 n log m v n y i μ i x i c σ i x i c as presented in formula 9 the generated realizations are denoted as y i μ i and σ i are the mean and standard deviation extracted from the cnn in convcnp gbc the parameter θ x i c μ i x i c σ i x i c of the mvn distribution is estimated by convcnp gbc according to the conditioning data c besides to perform convcnp gbc in hard conditioning the conditioning constraint loss function is also applied in the optimization of convcnp gbc formula 10 presents the expression of the conditioning constraint loss function suppose the spatial locations of conditioning data can be denoted as s s is a 3 d space matrix consisting of 0 and 1 the number of 1 in s is the same as the number of the conditioning points c also the spatial distribution of 1 in s is exactly the same as the distribution of the conditioning data as shown in formula 10 we exploit the l1 distance to measure the difference between the conditioning data and the generated realizations 10 l 1 1 n i 1 n s y c the combination of the above loss functions are used to optimize convcnp gbc the final loss function is illustrated in formula 11 each loss function has a hyperparameter λ that can be adjusted 11 loss 1 n i 1 n λ 1 log m v n y i μ i x i c σ i x i c λ 2 s y c 2 5 geostatistical bias correction with high resolution conditioning data in the process of upscaling the high resolution conditioning data and downscaling the coarse scale realizations there is an inevitable bias between the generated realizations and the original high resolution conditioning data we propose a novel mps bias correction strategy to correct the deviation during the downscaling modeling since the generated realizations by downscaling modeling usually include a large number of grid cells it is expensive to correct the bias of the entire realizations to alleviate the high computational consumption we use a new adjustable parameter r b to control the operating range of bias correction fig 3 shows a brief case of geostatistical bias correction with high resolution conditioning data the input data are low resolution realizations high resolution realizations and high resolution conditioning data a low resolution realization will be linearly interpolated to get a reference with the same size as high resolution realizations then we divide the data points in generated high resolution realizations into three categories high resolution conditioning data high bias data and low bias data high resolution conditioning data are the initial conditioning data and are also considered as reference data for bias correction for each high resolution conditioning point a range is plotted with a radius of r b the grid cells in this range are considered as high bias data and the grid cells outside this range are considered as low bias data high bias data are the data for which the bias correction must be performed low bias data are not necessarily corrected due to the smaller distance with the reference data all high bias data will be added into the simulation path for re simulation according to the high resolution conditioning data and the interpolation reference the step by step workflow of the geostatistical bias correction strategy are listed in appendix b 3 network training both 2 d and 3 d convcnp gbc are trained and optimized by using the joint loss function and the adam optimizer kingma and ba 2015 they are able to process the input data from a single channel feature map the structured parameters of 2 d and 3 d convcnp gbc are illustrated in tables 1 and 2 respectively the hyperparameters of the adam optimizer are shown in table 3 the hyperparameters optimization process of the cnn is a very interesting problem to investigate many studies adjusted the hyperparameters based on experience and expertise which is not only unfounded but also time consuming during testing yeh et al 2023 in our work the configuration of hyperparameters mainly comes from gm convcnp and is achieved by fine tuning the configuration of gm convcnp cui et al 2022 for 2 d convcnp gbc the size of input training data is expressed as h w for each training we randomly select the number of conditioning data n c within the preset interval at the same time we randomly extract the points obeying bernoulli distribution in 2 d space as the spatial locations of conditioning data the variables corresponding to the spatial locations are used as conditioning data then the conditioning data and the spatial locations are fed into con v θ with a convolution kernel of 9 9 128 feature maps of the conditioning data and the spatial locations are extracted by con v θ and concatenated another convolution with a 1 1 convolution kernel is used to transform its channel to 128 after that a cnn network with a 5 5 convolution kernel is applied to obtain the implicit inline relationship between the spatial locations and the conditioning data as shown in fig 2 8 convolutional layers are integrated into cnn and each convolutional layer adopts the relu activation function the number of convolution layers can be modified according to the complexity of the data distribution the output layer of 2 d convcnp gbc is a convolution operation with a 1 1 convolution kernel by using the softplus activation function the mean and scale of the mvn distribution can be separated from the output according to the last dimension channel similar to 2 d convcnp gbc all 2 d convolutions are replaced by 3 d convolutions in 3 d convcnp gbc the size of input 3 d training data is denoted as h w d one thing worth noting is that the 3 d input data are completely different from the 3 d training data we first obtain data points randomly on the xy plane and then extract all data in the z direction corresponding to each point as the conditioning data another difference from 2 d convcnp gbc is the number of convolutional layers in cnn compared with 2 d hydrological structures 3 d hydrological structures contain more spatial characteristics therefore 3 d convcnp gbc contains more convolutional layers 4 experiments and validation all experiments were conducted to verify the applicability and performance of convcnp gbc presented in this work convcnp gbc was implemented by using python 3 7 and pytorch 1 8 0 the experimental equipment is a workstation with an intel r xeon r gold 5117 cpu 2 00 ghz 64 gb ram and three nivida geforce rtx 3090 ti gpus to measure the performance of the proposed method both statistical and non statistical indicators were used for validation the average root mean square error rmse peak signal to noise ratio psnr and structural similarity ssim are used to statistically present the error in the experimental results the data distribution of facies proportion cumulative distribution function of facies proportion multiple dimension scaling map variograms and connectivity function are selected as non statistical indicators to reveal the performance of the proposed approach 4 1 the nevada mountains example the first experiment refers to a hydrological environment with non stationary spatial morphology the digital elevation models dem of the nevada mountains mariethoz and caers 2014 are shown in fig 4 we used the low resolution walker lake dem images to validate the performance of convcnp gbc in characterizing 2 d hydrological structures as depicted in fig 4a both categorical attributes and continuous attributes dem images were segmented to local scale structures with a resolution of 128 128 pixels via a sliding window strategy a total of 780 images of categorical and continuous attributes were obtained respectively among them 624 images are used for the training of convcnp gbc and 156 images are used for testing for categorical dem images convcnp gbc was trained for 20 epochs which took 1 h and 20 min for continuous attributes dem images 1 h and 40 min were used for 25 epochs training of convcnp gbc to validate the ability of characterizing hydrological structures with categorical attributes we randomly picked 50 categorical test data and obtained 50 different realizations for each test data fig 5 shows five tests of high resolution conditioning data upscaled conditioning data low resolution realizations high resolution realizations and their corresponding low resolution references we exploited the high resolution simulation grid of 1024 1024 pixels and conditioning data spread across the simulation grid to validate our method 1000 high resolution conditioning data were randomly extracted from high resolution realizations generated from low resolution references via bicubic downscaling nuño maganda and arias estrada 2005 from the high resolution simulation grid shown in fig 5 it can be observed that it is difficult to directly reconstruct entire realizations from the sparse conditioning data we upscaled these 1000 discretized conditioning data to a low resolution simulation grid of 128 128 pixels according to formula 5 then low resolution realizations were generated by using the trained convcnp gbc from the visual appearance the spatial structures in low resolution realizations are successfully reconstructed it can be seen that convcnp gbc can reproduce the global spatial characteristics of hydrological structures these low resolution realizations were considered as conditioning data and input again into the convcnp gbc for the reconstruction of high resolution realizations the high resolution realizations significantly increase the number of grid cells and insert more details based on these generated low resolution realizations this further illustrates the effectiveness of the proposed downscaling modeling workflow we also calculated the rmse psnr and ssim to quantitatively describe the performance of the proposed convcnp gbc rmse psnr and ssim of the generated realizations are 0 091 13 13 0 589 respectively from these quantitative statistics it can be derived that convcnp gbc has the ability of characterizing large scale subsurface structures from sparse observations to further reveal the performance of the proposed method we statistically analyze the differences between these generated high resolution realizations and the corresponding references since the dimensions of low resolution references are much smaller than the generated high resolution realizations we compared the statistics according to the following strategy 1 for each pixel voxel in a reference calculate the corresponding patch in a high resolution realization 2 for categorical variables the mode index is selected to represent the patch for continuous variables the patch is represented by the mean value 3 after computing all the pixels in a reference model a representative realization that represent a high resolution realization can be obtained then the dimensions of the representative realization are the same as the dimensions of a reference for subsequent verification experiments we adopted the same comparison strategy cumulative distribution functions are often used to describe the probability distribution of random variables fig 6 a depicts the cumulative distribution functions between the references and the corresponding realizations the red curve of the realizations is close to the blue curve of the corresponding references it confirms that the proposed method is able to reproduce the probability distribution of hydrological variables to fully reflect the diversity of the generated heterogeneous patterns multiple dimension scaling mds map is plotted by calculating the euclidean distances between the reference and 50 generated representative realizations fig 6b it can be observed that the red dots of the realizations are concentrated around the blue triangle of the corresponding reference it shows that the distances between the generated realizations and the reference are very close in 2 d space as shown in fig 6c we also plotted the variograms both in the x direction and the y direction the blue curves of the generated realizations closely fit the black curve of the corresponding reference according to the connectivity functions demonstrated in fig 6d it can be observed that the blue curves of the realizations and the black curve of the corresponding reference are very similar these statistics indicate that the proposed method can accurately represent heterogeneous patterns and reconstruct hydrological structures the dataset presented in fig 4b is used to validate the performance of convcnp gbc for characterizing hydrological structures with continuous attributes similar to the experiment above 50 test data were randomly selected from the test dataset for each test data we also generated 50 different realizations fig 7 shows five of these tests which consist of conditioning data low resolution realization high resolution realization and corresponding low resolution reference compared with categorical attributes it is even more difficult to reconstruct continuous attribute high resolution 1024 1024 pixels realizations directly from 1000 conditioning data these conditioning data were upscaled to the low resolution simulation grid with a resolution of 128 128 pixels according to these generated low resolution realizations the spatial distribution characteristics of hydrological variables are perfectly reproduced it indicates that convcnp gbc owns the ability to characterize hydrological structures with continuous attributes by downscaling with convcnp gbc these obtained high resolution realizations further prove the effectiveness of our downscaling modeling workflow we also used rmse psnr and ssim to compare the generated realizations with the corresponding references the values of rmse psnr and ssim are 0 026 29 71 0 941 respectively it can be inferred that convcnp gbc is more advantageous in characterizing subsurface structures with continuous attributes it confirms that convcnp gbc can reproduce the statistics of large scale subsurface structures with continuous attributes fig 8 shows the comparison statistics between the reference and the generated realizations we also plotted the cumulative distribution function for continuous attribute variables note that the cumulative distribution function we used here is actually the complementary cumulative distribution function ccdf fig 8a shows the ccdf of the realizations and the corresponding references when the variable is greater than 0 5 the blue curve of the references and the red curve of the realizations fit closely it proves that the cumulative distributions between low resolution references and the high resolution realizations of convcnp gbc are similar to show the diversity of the realizations generated by convcnp gbc the mds map of the reference and the corresponding realizations were drawn fig 8b the red dots of realizations are concentrated around the blue triangle of the corresponding reference it also demonstrates that the high resolution realizations generated by convcnp gbc are close to the reference in 2 d space besides we also plotted the variograms of the generated realizations and the reference in x and y directions in fig 8c regardless of the variograms in x or y direction the blue curves of the realizations are highly resembling the black curve of the reference the connectivity functions of the reference and the realizations fig 8d also demonstrate that connectivity areas in the realizations are very similar to those in the reference the above statistical features between the realizations and the corresponding reference are all similar it further confirms the availability of convcnp gbc in the 2 d reconstruction of hydrological structures 4 2 the berea sandstone example the berea sandstone is a typical kind of oil and gas reservoir the high resolution characterization of sandstone is very important for the exploration of oil gas and water resources fig 9 shows the sandstone models from okabe and blunt 2007 used in this 3 d test similar to the 2 d training dataset we also used the sliding window strategy to segment the original sandstone model with a resolution of 200 200 200 voxels 6470 small scale data set with a resolution of 40 40 40 voxels were obtained the small scale data set was divided into 5176 training data and 1294 test data according to the 80 20 rule the 3 d convcnp gbc was trained for 17 epochs using the parameters illustrated in section 3 the entire training process took a total of 4 h and 40 min in this test we randomly extracted 50 different high resolution test data 80 80 80 voxels and 50 different realizations were generated for each test data we evenly sampled 40 boreholes from each test data as the conditioning data for each test conditioning data we upscaled the high resolution conditioning data from the resolution of 80 80 80 voxels to the resolution of 40 40 40 voxels then low resolution realizations were obtained by using the trained 3 d convcnp gbc according to the corresponding upscaled conditioning data the high resolution realizations 80 80 80 voxels were generated based on the downscaling modeling from the low resolution realizations fig 10 depicts ten of the generated low resolution realizations high resolution realizations and corresponding low resolution references by comparing the generated low resolution realizations with the corresponding references the heterogenous spatial patterns of the sandstone were successfully reconstructed it proves that the proposed 3 d convcnp gbc is available to reconstruct 3 d subsurface hydrological structures from limited conditioning data it can be seen from the high resolution realizations that the generated realizations not only preserve the spatial structures of low resolution realizations but also insert many details it once again proves that 3 d convcnp gbc can reproduce high resolution heterogeneous subsurface structures same as the above experiments we also compared the quantitative statistics of the reproduced sandstone realizations with the corresponding references the average rmse psnr and ssim of the sandstone realizations are 0 098 10 06 0 573 respectively it can be observed that the 3 d task is significantly more difficult than 2 d task however convcnp gbc is still able to reconstruct high resolution sandstone realizations from sparse observations fig 11 shows the comparative analysis of the statistics in the generated realizations and the corresponding references the scatter plots of the facies proportion in these generated realizations and the corresponding references are depicted in fig 11a each spot represents a generated realization or a reference the scatter plots demonstrate that the facies proportion of the 50 realizations and the corresponding references are very close the box plots further illustrate that the maximum minimum and median of the facies proportions in the generated realizations are consistent with the facies proportions in the references from the comparison scatter plots we observed that there is an error of 0 02 attribute proportion between some realizations and the corresponding references the box plots also illustrate this error by the maximum and median values from our speculation the reason for this phenomenon can be broadly attributed to the fact that the distribution of 3 d conditioning data is known to be sparser than the distribution of 2 d conditioning data the insufficient amount of conditioning data leads to allowable errors of attribute proportion in a small number of realizations as a result the presence of certain extreme values of attribute proportion affects the performance of the maximum and median values of overall realizations besides we drew the cumulative distribution function to compare the heterogeneous patterns between the generated realizations and the corresponding references as depicted in fig 11b the blue curve of the references is resembling with the red curve of the realizations besides to confirm the diversity of the generated realizations we calculated the euclidean distance between a reference and the corresponding 50 realizations the distances were projected to 2 d space and their spatial distribution is shown in fig 11c these red dots of the realizations are distributed around the blue triangle of the reference it can be seen that the distances between them are all close in fig 11d and 11e the variograms and connectivity functions are reasonably well reproduced these above statistical characteristics further confirm that convcnp gbc can reproduce heterogeneous 3 d patterns and reconstruct subsurface hydrological structures 4 3 comparison with convcnp we also investigated the simulation performance between the proposed convcnp gbc and convcnp by using the proposed downscaling modeling workflow a set of comparative experiments were conducted based on the dataset illustrated in section 4 1 it should be noted that we used low resolution training data for the optimization of both convcnp gbc and convcnp convcnp gbc has the same hyperparameters and number of hidden layers as convcnp for categorical attributes validation fig 12 800 samples were regarded as conditioning data the locations of these samples in a high resolution simulation grid 1024 1024 pixels are shown in fig 12a note that the point size of conditioning data is enlarged by a factor of 20 to visualize them from the high resolution simulation grid figs 12b and 12c present the high resolution realizations obtained by convcnp and convcnp gbc respectively the parameters of geostatistical bias correction were set as r 50 rb 30 t 0 01 the search range r and the threshold t are the most commonly used parameters in mps simulations in this validation the parameters were assigned according to the guide of performing mps simulations which discussed the relationship between r t and final realizations in detail meerschman et al 2013 from the local area circled in red rectangles it can be seen that convcnp gbc can achieve more accurate realizations compared with convcnp it confirms that the proposed convcnp gbc has the ability to fine tune the generated realizations based on high resolution conditioning data to statistically illustrate the effectiveness difference between convcnp and convcnp gbc the variograms of reference and generated realizations are drawn in fig 12d no matter in the x or y direction we observe that the blue curve of convcnp gbc is closer to the curve of the reference than the yellow curve of convcnp this further proves that the proposed convcnp gbc can significantly improve the quality of generated realizations besides we also compare the difference in the facies proportion of the generated realizations fig 12e it can be recognized that the facies proportion generated by convcnp gbc is closer to the facies proportion in the reference the quantitative statistics of the generated categorial attribute realizations are illustrated in table 4 these above results all indicate that the statistics of the realizations generated by convcnp gbc are more consistent with the statistics of the reference we also tried the 2 d hydrological structures with continuous attributes to compare the performance between convcnp gbc and convcnp fig 13 1200 randomly selected samples were regarded as the conditioning data and their spatial distribution in a high resolution grid is drawn in fig 13a figs 13b and 13c show the comparative realizations of convcnp and convcnp gbc respectively the parameters of geostatistical bias correction were set as follows r 50 rb 40 t 0 01 it can be seen that the heterogenous patterns and property distribution are successfully reconstructed in both realizations the area circled by red rectangles confirm that the proposed convcnp gbc can indeed adjust the spatial structures of the generated realizations based on the conditioning data the variogram plots of realization generated by convcnp gbc realization generated by convcnp and corresponding reference are utilized to illustrate the statistical characteristics fig 13d the curve of convcnp gbc is closer to the curve of reference than the curve of convcnp it convinces that the statistical characteristics of the realizations generated by convcnp gbc are more consistent with the statistical characteristics of the reference besides we applied the k nearest neighbor knn classification algorithm peterson 2009 to classify the property in realizations into three categories from fig 13e it is convenient to observe that the property distribution in realizations of convcnp gbc is closer to the property distribution in the reference the quantitative statistics of the generated continuous attribute realizations are illustrated in table 5 these comparative results indicate that convcnp gbc proposed in this work has more advantages in characterizing large scale and high resolution subsurface structures 5 discussions 5 1 convcnp gbc in subsurface characterization the scarcity of resources in terms of fossil fuels groundwater minerals and fluids will put considerable stress on subsurface resource supplies dolan et al 2021 rosa et al 2020 large scale and high resolution subsurface characterization can provide a credible framework for the natural resource evaluation in our work we present a two stage downscaling hydrological modeling method convcnp gbc for rapidly and precisely reconstructing large scale subsurface structures with high resolution for large scale characterization problems almost all existing hydrological modeling methods are limited by the huge computational consumption by embedding downscaling technique into the subsurface characterization the computational complexity and computational consumption are significantly reduced in our work in addition convcnp gbc is able to add local scale detail features while taking into account the global characteristics of subsurface structures several experiments presented in this paper demonstrate the availability of the downscaling modeling framework for large scale subsurface characterization the proposed convcnp gbc can integrate the advantages of hydrological modeling and downscaling it can be extended and applied to characterize large scale heterogeneous subsurface structures and phenomena and further support human activities such as the exploration of geological resources and the protection of the environment the main advantage of our work is that a two stage general workflow for downscaling modeling is proposed we divided the workflow into model building and model generating processes in the stage of model building the convcnp gbc is trained with low resolution data and applied into the reconstructing and downscaling processes of hydrological structures in the stage of model generating high resolution realizations can be generated rapidly according to the trained model high resolution conditioning data and the gbc strategy it is worth noting that the applicability of the proposed workflow is not specific to a particular neural model most procedures where convcnp gbc is used in the workflow can be replaced with other neural networks besides an improved convcnp model is used not only for reconstructing coarse scale realizations but also for downscaling as a highly generalizable meta learning approach convcnp is capable of predicting all spatial positions in the entire space directly from sparse observations and can be applied in subsurface characterization cui et al 2022 on the basis of convcnp the convcnp gbc model is trained by using limited training data with low resolution which ensures it can be applied to the both reconstruction and downscaling processes of hydrological structures compared to other deep learning based downscaling methods that require high resolution training data our approach can be more adapted to high resolution subsurface characterization in addition the complicated training process of multiple neural models can be avoided by using the same model when reconstructing coarse scale structures and the downscaling modeling the experimental results and analysis illustrated in sections 4 1 and 4 2 confirm the performance of convcnp gbc though the 3 d reconstruction study uses an example of binary facies for the verification convcnp gbc can be easily extended to the reconstruction in the cases involving multiple facies or continuous attributes another important advantage of the convcnp gbc is that a joint loss function is used to improve the optimization process the improved loss function ensures that the entire subsurface structures can be reconstructed while obeying the hard conditioning constraints the spatial reconstruction loss is used to optimize the prediction process for the variables in random fields based on the negative log likelihood estimation the conditioning constraint loss is implemented by calculating the l1 distance between conditioning data and realizations the hyperparameters of each term of the joint loss function can be fine tuned to adapt for reconstructing different hydrological structures the presented gbc strategy is able to quickly correct the deviations introduced from low resolution coarse realizations to high resolution realizations the operating range of bias correction increases the computational efficiency the local bias correction strategy can both ensure the precision of the generated realizations and significantly improve the efficiency of the entire downscaling modeling workflow besides different metrics are used to calculate the similarity of data events for categorical and continuous variables respectively the validation in section 4 3 proves the improvement of the proposed convcnp gbc 5 2 limitations and future work the limitation is that the distribution of high resolution conditioning data affects the performance of convcnp gbc if the high resolution conditioning data are distributed unevenly then the upscaled conditioning data are not sufficient to reconstruct credible coarse scale realizations the high resolution realizations based on unreliable coarse scale realizations are difficult to correct the bias via gbc which may result in even greater computational overhead than direct simulation using the same neural network model to perform approximate but different tasks is an issue worthy of further study for example the use of a unified convcnp for both the hydrological modeling and downscaling in convcnp gbc in future work we plan to insert an identification option in convcnp gbc the option is used to help the trained model to determine whether the current task is reconstructing coarse scale realizations or downscaling realizations it holds great promise to improve the performance of convcnp gbc for characterizing large scale subsurface structures with heterogeneous patterns 6 conclusions in this paper we propose a two stage downscaling hydrological modeling approach convcnp gbc to characterize large scale and high resolution subsurface structures and heterogeneous phenomena the approach is based on the convcnp and includes the process of coarse scale realization reconstruction and the process of high resolution realization downscaling we used an improved convcnp embedding for both processes the same trained convcnp model enables two different stages of downscaling hydrological modeling it greatly reduces the training consumption the mps based bias correction method is also applied in the approach we tested the proposed method on both 2 d and 3 d hydrogeological structures and compared it with the original convcnp the numerical results confirm that the convcnp gbc can integrate the advantages of convcnp and have the capability of characterizing large scale and high resolution subsurface structures the proposed convcnp gbc can be widely used to the 3 d characterization of subsurface structures such as hydrological modeling and geophysical inversion credit authorship contribution statement zhesi cui conceptualization methodology software writing original draft validation formal analysis qiyu chen methodology writing original draft project administration supervision gang liu writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is supported in part by the national natural science foundation of china 42172333 41902304 u1711267 and by the knowledge innovation program of wuhan shuguang project 2022010801020206 appendix a convolutional conditional neural processes convolutional conditional neural processes convcnp can rapidly adapt to new conditioning data and estimate the uncertainty in their predictions it allows making correct predictions by giving only a few training data the forward pass of the convcnp is illustrated in algorithm 1 in step 3 the first channel of the functional representation h is the density channel it is helpful to divide the remaining channels when there is a large variation in the density of input locations algorithm 1 forward pass of the convcnp require a image i b context m c c target mask m t forward pass steps begin 1 calculate extract context set z c by using i and m c 2 obtain the functional representation h with c channels by performing a convolution of m c z c t 3 normalize the convolution by dividing the remaining channels by h 1 c h 1 c h 0 4 find the function f with 2c channels by applying a cnn of h and m c 5 obtain the first c outputs are the means of a mvn distribution and the second c the standard deviation end appendix b workflow of geostatistical bias correction strategy the steps of the geostatistical bias correction strategy are listed in algorithm 2 note that the steps marked with are important in the algorithm in step 2 uncorrected nodes are initialized to be empty so as not to affect the bias correction when comparing the data events these data events must keep on a uniform scale thus in steps 8 and 9 the corresponding areas in the low resolution data are projected to a high resolution grid by linear interpolation in step 12 euclidean distance is often used to measure the difference between two data events with categorical attributes for continuous variables the mean absolute error is used to present the distance between two data events algorithm 2 the workflow of the geostatistical bias correction strategy inputs a high resolution conditioning data ch b high resolution realization rh c low resolution realization r d maximal search radius r distance threshold t and operating range of bias correction rb algorithm steps 1 assign the high resolution conditioning data ch to the high resolution grid cells in rh 2 initialize a path list consisting of all high bias data according to rb and ch 3 while path list size 0 do 4 get the current cell x to be corrected from path list 5 identify the data event dev x consisting of informed cells in radius r around x 6 initialize d dev x dev y 7 while d dev x dev y t do 8 find the corresponding area a of the current data event in r 9 project a into the high resolution grid by linear interpolation 10 randomly sample a location y in a 11 identify dev y centered on y with the same geometry of nx 12 compute the distance d dev x dev y 13 end while 14 assign the closest value to the current node x 15 end while output high resolution realization rh appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129498 appendix c supplementary data the following are the supplementary data to this article supplementary data 1 
1957,in this work we have proposed an adaptive correction iterative ensemble smoother acies to adaptively adjust the range of unknown variables for improving the estimation accuracy while solving the groundwater contaminated source estimation gcse a prior range for the unknown variables must be provided according to the field investigation or manual auxiliary information however the prior range might not include the actual values thus causing misleading estimation results moreover when using acies to solve gcse it requires massive realizations of the simulation model causing huge calculation cost here a surrogate model has been utilized to alleviate the high calculation cost furthermore the accurate estimation of gcse relies on the high fidelity surrogate models the well tuning of the hyper parameters of the surrogate models can guarantee high fitting accuracy fidelity for the origin simulation model although the existing tuning methods such as the grid search method can satisfactorily tune the surrogate model the long tuning time which has been brought by the ergodic running with every potential combination of hyper parameters must not be neglected thus an auto light gradient boosting machine lightgbm tuned with swarm evolutionary algorithm has been utilized as a high fidelity surrogate model with a swift tuning time both the coal gangue and high dimensional estimation scenarios have been designed to evaluate the performance of the proposed method the results indicated the following 1 while encountered with the vague prior ranges the adaptive ensemble smoother could adjust the potential search ranges to provide an accurate estimation 2 when compared with the typical surrogate models an auto lightgbm surrogate could achieve the promising generalization accuracy with fast a tuning time this guaranteed the high fidelity of the surrogate models which further improved the accuracy of the estimated results keywords auto lightgbm iterative ensemble smoother adaptive range correction groundwater contaminated source estimation data availability data will be made available on request 1 introduction groundwater contaminated source estimation gcse is the perquisite for the risk assessment and contaminant remediation generally gsce involves the matching of the simulated outputs at the monitoring points with the true observed values the methods to perform gsce can be categorized as the simulation optimization simulation statistics and simulation data assimilation methods recently the data assimilation methods have drawn increasing attention in the gcse domain ensemble kalman filter enkf is one of the typical data assimilation methods which is a probabilistic approach developed to address the issue of nonlinear problem katzfuss et al 2016 its variant viz iterative ensemble smoother ies is more efficient to implement gcse xu et al 2021 this efficiency owes to the assimilation by es on all data of all time steps to update the state variables when compared with enkf white 2018 employed ies to estimate high dimensional parameters of a real world scale environmental models lam et al 2020 utilized ies to calibrate the model parameters of a synthetic 2d groundwater model improving the performance of parameter estimation jiang et al 2022 utilized an iterative ensemble smoother and sequential gaussian simulation to simultaneously estimate the information of contaminant sources and hydraulic conductivities in the present study we utilized the ies as the basic estimation framework for gcse however while performing gsce with ies the matching process that involves the calculation of the ensemble of unknown variables inevitably requires massive realizations of simulation model which usually incurs a huge computational burden a surrogate strategy has been commonly employed to alleviate this burden zhao et al 2015 have proposed a time efficient surrogate model to identify the characteristics of the groundwater pollutant source mo et al 2017 have introduced a surrogate using the taylor expansion based adaptive design strategy to reduce the massive calculation cost luo et al 2020 have adopted a constrained trust region ctr based adaptive dynamic surrogate model to substitute the high time cost simulation model for dnapl remediation design scenario wu et al 2022 have proposed a multi objective ensemble surrogate model with a low computational cost for handling the groundwater optimization designs the existing surrogate models can be divided into three categories asher et al 2015 viz data driven projection based and multi fidelity based surrogates among those surrogates the data driven surrogate models have been extensively employed in gcse which involves the support vector regression machine svr bashi azghadi et al 2010 xing et al 2019 extreme learning machine elm hou et al 2021 zhao et al 2020 gaussian process regressor gpr wang et al 2021a zhang et al 2016 artificial neural network ann mo et al 2019 pan et al 2021 singh and datta 2007 srivastava and singh 2015 and decision tress dt baudron et al 2013 buras and donado 2021 gaur et al 2018 li et al 2022 xue et al 2015 ouyang et al 2017 used svr gpr and multi gene genetic programming to construct a surrogate models of multi phase flow model for groundwater remediation design at dnapls contaminated sites efficiently saving the calculation time hou and lu 2018 utilized svr and elm to establish accurate surrogate models of a simulation model to solve gcse problems with dense nonaqueous phase liquids dnapls contaminant elzain et al 2022 employed ann svr and dt to establish data driven surrogate models for the evaluation of groundwater contamination vulnerability recently owing to the powerful generalization capacity of the complex system and fast training process the gradient boosting algorithms gba such as light gradient boosting machine lightgbm guo et al 2021 liang et al 2022 extreme gradient boosting xgboost bhagat et al 2021 parsa 2021 and the adaptive boosting adaboost wen et al 2015 wu et al 2020 have drawn intensive attention in various domains among the above mentioned algorithms lightgbm has been proven to possess high efficiency fast training speed and less memory usage qi 2017 the lightgbm is a novel ensemble learning method based on the decision tree algorithm sun et al 2020 wen et al 2021 the light in lightgbm refers to the fact that it is designed to be lightweight and efficient while still maintaining high accuracy it achieves this by using a number of innovative algorithms that are specifically designed for speed and memory efficiency which involves two techniques viz the gradient based one side sampling and the exclusive feature bundling moreover the lightgbm is customizable that can provide a range of hyperparameters that can be tuned to achieve the best results for a particular problem there have been hardly any reports of lightgbm hitherto for constructing a surrogate model in gcse studies here we explored a novel lightgbm to construct a surrogate model of simulation model in gcse while constructing a surrogate model a critical process is the selection of hyperparameter which determines the fidelity of a surrogate model to its origin simulation model generally the hyper parameters are given according to a manual trial strategy or the grid search strategy although these two strategies can provide proper hyper parameters of a surrogate model the high time cost incurred by the exhaustive search for the combination of hyper parameters cannot be neglected the selection of hyper parameters can be described as an integer programming ip problem whose objective function is to maximize the generalization fidelity measurement there has been significant research exploring the relationship between hyperparameter optimization hpo and ip as they are closely related some recent studies have employed ip models to solve hpo problems more efficiently to achieve better optimization results peng et al 2022 used an integer programming approach to solve the hyperparameter optimization hpo problem for deep neural networks achieving promising performance on real world precipitation prediction problem guo et al 2019 used an integer programming approach to solve the hpo problem for deep neural networks the authors formulated the problem as an ip model and proposed an automatic hyperparameter optimization ahpo method that combines a genetic algorithm ga with a differential evolution de algorithm to find the optimal hyperparameters for the dnn model efficiently improving the generalization accuracy in the present study we have introduced swarm evolutional algorithm as the optimizer to automatically select the optimal hyper parameters of a lightgbm surrogate which aims to efficiently promote the generalization performance gcse can be described as a search process where the trial solutions of the unknown variables are repeatedly updated within the search ranges until the corresponding simulated outputs can match with the observed values at the monitoring points generally when performing gcse prior ranges of unknown variables must be preliminarily given based on the field investigation there have been some researches on the impact of prior range or prior distribution on groundwater pollution source estimation lee and kitanidis 2013 proposes a new approach for identifying discrete geologic structures for contaminant cleanup purpose using bayesian inversion with a total variation prior ranges jiang et al 2021 proposed the self organizing maps algorithm for identifying the contaminant sources and hydraulic conductivity field and investigated the effect of different prior ranges of unknown variables on the accuracy of gcse wang et al 2021b proposed an iterative updating heuristic search strategy for groundwater contamination source identification which updated the prior distributions ranges of unknown variables using final iterations obtained from posterior distributions ranges the researches mention above are based on the assumption that the prior ranges must contain the true values and the posterior ranges are the subset of the prior ranges however in some real site world cases the preliminary prior ranges might not include the true values of the unknown variables which can result in the misleading estimation of unknown variables therefore we have proposed a range correction strategy to adaptively adjust the ranges of unknown variables this can improve both the fault tolerant capability of gsce and the estimation accuracy particularly the ensemble results of the estimation in each epoch require to be examined as follows 1 if the majority of the ensemble samples fall into a boundary range neighborhood the boundary alert will be triggered which is accompanied by the adjustment of ranges 2 otherwise the search ranges remain as the prior ranges we have combined the range correction strategy with es to guarantee the reliability of gcse we have proposed a novel adaptive correction iterative ensemble smoother associated with an auto lightgbm surrogate to perform gcse the proposed method has been evaluated with the coal gangue and high dimensional estimation scenarios the main contributions of the present study are given as follows the novel lightgbm with the auto tuning hyper parameters can be a high fidelity and easy to perform surrogate and save the huge computational burden of the simulation model which efficiently improve the estimation results of gcse while encountering with the situation of the misleading prior ranges of unknown variables a range correction strategy has been combined with es to adaptively adjust the range of unknown variables which can guarantee the reliability of gcse 2 methodology 2 1 numerical simulation model the transportation of the groundwater contaminant can be described by a groundwater flow model and solute transport model the basic differential formula that governs the steady flow can be expressed as 1 x i k h z 0 x i x j k h z 0 x j w x i x j t 0 where x i and x j denote the location distances along the respective cartesian coordinate axis k represents the hydraulic conductivity w denotes the volumetric flux per unit area and h represents the water level above the sea level here z 0 represents the elevation of the aquifer bottom above the sea level t represents the simulation time the basic partial differential governing formula of the solute transport model can be expressed as 2 c t x i d ij c x j x i u i c r θ 3 u i k ij θ h x i where c represents the solute concentration d ij denotes the hydrodynamic dispersion tensor u i represents the average linear velocity that satisfies darcy s law r represents the source or sink term and θ denotes the effective porosity the numerical simulation model has been calculated using modflow and mt3d module of the groundwater modeling system gms 2 2 light gradient boosting machine lightgbm the light gradient boosting machine lightgbm is a novel tree based ensemble learning method which has been proposed by the scholars from microsoft and peking university qi 2017 which intends to solve the efficiency and scalability issue while using xgboost for the scenarios of the high dimensional input features and large dataset size the lightgbm method involves two key techniques viz gradient based one side sampling goss and exclusive feature bundling efb assume a dataset x x i y i i 1 n and the goal of lightgbm is to find a function f x to minimize the expected values of the loss function l between the predicted values of f x and y which can be expressed as 4 f a r g m i n e y x l y f x the predicted values of lightgbm consist of the outputs of a series of basic decision trees models h t x which can be expressed as 5 f x t 1 t h t x where t represents the number of basic decision trees the objective function of lightgbm can be simplified with netwon s method as 6 l t i 1 n g i f x i 1 2 h i f 2 x i to solve the problem of gcse the lightgbm was utilized to establish the regression relationship between the unknown variables and observation data at monitoring wells in particular x represents the unknown variables whereas f x denotes the corresponding simulation outputs at monitoring wells once trained the lightgbm can be a low time cost surrogate model to substitute the long time consuming simulation model when using ies to solve gcse 2 3 iterative ensemble smoother the iterative ensemble smoother ies is a variant of ensemble kalman filter to tackle the nonlinear issue being different from enkf the ies has utilized the observation data of all time to update the unknown parameters variables the simulated concentrations at the monitoring wells for the iteration step t c t can be calculated with the numerical simulation model as 7 c t f v ε 8 v v 1 1 v 1 n v v n s 1 v n s n v where f denotes the numerical simulation model v represents the ensemble matrix of unknown variables n s denotes the ensemble size n v represents the dimension of unknown variables and ε represents the observed error which satisfies a standard normal distribution the unknown variables can be updated as xu et al 2021 9 v j v j 1 g j y obs f v j 1 10 g j c o v v j 1 f v j 1 c o v f v j 1 f v j 1 r 1 11 r ξ 2 d i a g o n e s where y obs represents the observed concentrations of the monitoring wells cov denotes the covariance matrix diag indicates the diagonal matrix and j represents the iteration step 2 4 adaptive range correction strategy while performing gcse with ies in a real world case the prior ranges of unknown variables must be given according to the auxiliary information however the prior ranges are commonly not unbiased possibly causing that the true values of the unknown variables are not fully included in the prior ranges henceforth we have introduced an adaptive range correction strategy to adjust the prior ranges of ies which can guarantee the reliability of estimation results given q l b u b and lb represents the lower boundary of the unknown variables of gcse whereas ub represents the upper boundary we have introduced the q 0 25 0 75 as the measurement of the boundary detection which represents the interval of 25 75 sample points of unknown variables if the boundary falls into the interval q 0 25 0 75 the boundary alert is triggered then the boundary range q requires to be adaptively adjusted to involve a larger range particularly the flowchart of the adaptive correction iterative ensemble smoother acies has been described in table 1 in particular the unknown variables of the estimated results are evaluated at each step until the estimated results of these variables are no longer concentrated at the boundary this approach helps to improve the accuracy of the estimation by refining the estimates of the unknown variables and reducing the uncertainty in the results 3 application 3 1 case overview and assumption 3 1 1 coal gangue scenario the study area is a coal gangue pile located in fushun china with a section of 9 700 m 6 100 m fig 1 the average annual precipitation and evaporation in the study area are 700 and 1 500 mm respectively the northwest boundary γ1 is the hun river which is conceptualized as the specific head boundary the northern boundary γ2 and the southern boundary γ4 are parallel to the groundwater flow line in the area thus they are generalized as the no flow boundaries the eastern boundary γ3 and southwest boundary γ5 have been generalized as the specific flow boundaries according to the medium of aquifer the hydraulic conductivity has been divided into two zones viz k 1 and k 2 with the ranges of 80 md 1 110 md 1 respectively the adsorption of the contaminant is also considered which has an absorption distribution coefficient of 6 5 10 2 cm3g 1 the details of the aquifer are shown in table 2 fig 2 presents the contaminant concentration sulfate ion in the leaching experiment initially the concentration of the contaminant in the leachate has been evidently high the leachate fluctuated during 1 15 h but showed a downward trend thereafter the concentration tended to be stable at 15th hour according to the leaching experiment it has been assumed that the contaminant was continuously released from the coal gangue to the aquifer generally the unknown variables to estimate have been determined as the specific head h s fluxes of specific flows f 1 and f 2 hydraulic conductivities k 1 and k 2 and release intensity s respectively for the calculation of numerical simulation model the domain has been discretized by the grids with the size of 50 m 50 m in the following sections a hypothesis scenario has been first designed to evaluate the performance of the proposed method sequentially the proposed method is applied to a real site scenario in the hypothetical scenario nine observation wells obs1 obs9 have been designed to monitor the contaminant concentration the observation concentrations fig 3 have been synthetized through running the simulation model with the reference true values of the unknown variables to estimate being different from the hypothesis scenario in the real world scenario the observed data of obs4 obs6 obs7 and obs9 are real world measured contaminant concentrations and there are no reference values of the unknown variables the observation data for the real site scenario are scarce when compared with those in the hypothetical scenario 3 1 2 high dimensional estimation scenario to further evaluate the performance of the proposed acies framework a high dimensional estimation scenario was introduced which has observation data with stronger nonlinearity than the coal gangue scenario it must be noted that it is a well designed case derived from pan et al 2021 where release histories of three potential contaminated sources and zone hydraulic conductivities total 19 dimensions required to be simultaneously estimated fig 4 shows the details of potential contaminated source boundaries aquifer parameters and observation wells in particular nine observation wells were set to monitor the contaminant concentration fig 5 presents the observation data 45 dimensions at monitoring wells in high dimensional estimation scenario basic description of known information of the aquifer and contaminated source were shown in table 3 it must be noted that the reference values of unknown variables table 4 were allowed to evaluate the estimation accuracy of acies 3 2 surrogate models of simulation model 3 2 1 coal gangue scenario in this section we have compared the proposed auto lightgbm with the four alternative typical surrogates viz xgboost support vector regression machine svm gaussian process regressor gp and deep fully connected neural network fcnn the surrogates have been established on a python environment with regards to the dataset we have introduced the latin hypercube sampling lhs method to generate 500 patterns of unknown variables inputs sequentially the corresponding concentrations at the monitoring wells outputs can be obtained by running the simulation model with the inputs the inputs and outputs have been integrated to a dataset which has been divided into two parts viz the training dataset 80 and validation dataset 20 the performance of the surrogates can be evaluated from two aspects viz the running time cost and the generalization accuracy the experiments of the surrogates have been computed on a pc with intel core i7 12700h cpu 2 30 ghz processor and 16 0 gb ram the coefficient of determination r has been introduced to evaluate the generalization accuracy 12 r 1 i 1 n y sim i f v sim i 2 y sim i m sim 2 where y sim represents the outputs of the simulation model f x sim i is the predicted values of the surrogate model and m sim denotes the mean values of the outputs y sim the auto lightgbm can be described as an optimization problem given as 13 opt m a x i m u m r 14 s t r γ θ 1 θ 2 θ 3 15 100 θ 1 1200 1 θ 2 10 1 θ 3 50 where θ 1 represents the number of basic estimators θ 2 denotes max depth and θ 3 indicates min child weight the optimal value of θ can be solved by using sea which has been performed with geatpy module https www geatpy com in the python environment the parameters of encoding nind maxgen trapped value and maxtrappedcount have been set to ri 10 30 1e 2 and 100 respectively 3 2 2 high dimensional estimation scenario in high dimensional estimation scenario 400 patterns of samples were generated using lhs 80 for training and 20 validation if the boundary alert of adaptive correction is triggered another 400 patterns of samples will be generated in the updated prior ranges to retrain the auto lightgbm the fixed parameters of auto lightgbm keep the same as those in the coal gangue scenario 3 3 adaptive correction iterative ensemble smoother acies 3 3 1 coal gangue scenario for acies the average bias b between the observation data y obs and the correspond outputs y est of the estimated results v est have been introduced to evaluate the estimated accuracy 16 b i 1 n o y obs i y est i 2 17 y est f v est 18 v est 1 n s i 1 n s v i to consider the ergodicity of the unknown variables latin hypercube sampling has been utilized to generate the initial ensemble of acies with ensemble size n s of 1000 particularly the ensemble size has been selected from the experiment with the size values of 200 600 1 000 and 1 200 where the estimation results of different sizes were close enough between 1 000 and 1 200 the max iteration and the terminated tolerance have been set to 100 and 1e 3 respectively moreover to reduce the huge calculation cost the corresponding outputs y est have been calculated by the surrogate model table 5 presents the prior ranges of the unknown variables in the hypothetical and real site scenarios therefore the initial boundaries lb and ub of the unknown variables have been set according to table 5 when the ranges of the unknown variables have been adjusted the trained surrogate model needs to be re trained with the additional samples of the adjusted ranges the detailed sizes of the process variables in the hypothetical and real site scenarios have been shown in tables 6 and 7 respectively sequentially the proposed acies can be utilized to estimate the unknown variables using the observation data 3 4 high dimensional estimation scenario in this section the fixed parameters of acies keep the same as those in the coal gangue scenario prior ranges of unknown variables were shown in table 8 4 results and discussion 4 1 coal gangue scenario 4 1 1 hypothesis scenario 4 1 1 1 performance of auto lightgbm fig 6 presents the trace plot of r score of the auto lightgbm a and regression plot of auto lightgbm b xgboost c svr d gp e and fcnn f fig 6 a reveals that the auto lightgbm has achieved a steady and promising generalization accuracy with the auto optimal tuning pattern of the hyper parameters when compared with the typical machine learning methods such as xgboost svr and gp the auto lightgbm has achieved better generalization ability with r of 0 99654 furthermore auto lightgbm can even approach similar performance of fcnn with r of 0 99627 which is a 6 256 128 64 54 fully connected structure those comparisons have proved that the auto lightgbm has been suitable to be a high fidelity surrogate of the high calculation cost simulation model with regard to the running cost the numerical simulation model requires nearly 300 000 min for 200 000 realizations while using acies whereas the auto lightgbm requires about 5 5 min for the same number of realizations this owes to the natural high running speed advantage of the data driven model the auto lightgbm can substitute the numerical simulation model with the swift running speed which can save a huge computational burden 4 1 1 2 estimation results figs 7 and 8 visualizes the joint distribution and boxplot of the estimation results at 1st a and 2nd b range correction in the hypothetical scenario in the 1st range correction fig 7 a specific head h s fluxes of specific flow f 1 hydraulic conductivities k 1 and k 2 and release intensity s approximately satisfies the normal distribution whereas f 2 has concentrated at a vertical line the majority of the ensemble of f 2 has been accumulated at the upper boundary which has triggered the boundary alert fig 8 a therefore according to the adaptive boundary adjustment step in table 1 the range of f 2 is to be adjusted from 3 000 3 700 to 3 000 4 400 and the ranges of other variables are constant sequentially the second correction has been performed with the updated boundaries ranges of the unknown variables fig 7 b indicates that the ensembles of the unknown variables satisfy the normal distribution fig 8 b reveals that the ensemble has not triggered the boundary alert thus the estimation loop can be terminated finally we have chosen the individual with the least value of b as the optimal point of the estimated result of gcse the comparison between the estimated results at the first and second corrections has been shown in tables 9 and 10 after the first range correction the estimated results have been evidently closer to the reference values with a lower b which provide a strong proof that the adaptive range correction strategy is valid 4 1 2 real site scenario figs 9 and 10 presents the joint distribution and boxplot of the estimation results at first a and second b range correction in the real site scenario for the first range correction f 1 k 2 and s nearly satisfy the normal distribution whereas h s f 2 and k 1 satisfy the skewed distribution fig 9 a whereas the estimated results of h s f 2 and k 1 have triggered the boundary alert thus the further range corrections for those variables are required to be performed particularly the ranges of those variables have been adjusted from 60 63 3000 3700 and 80 110 to 57 63 3000 4400 and 80 140 respectively for the second range correction fig 9 b all of the unknown variables basically conform to the normal distribution the estimated results are listed in table 11 accordingly the b of the second range correction is apparently below the first range correction which indicates that the adaptive range correction strategy efficiently improves the accuracy of the gcse fig 11 presents the comparison between the simulated and observed concentrations indicating that the corresponding outputs of the estimated results satisfactorily fit with the true measurement data furthermore the spatial and temporal distributions of the contamination plume can be predicted through forward running the simulation model with the estimated results of the unknown variables 4 2 high dimensional scenario 4 2 1 performance of auto lightgbm fig 12 presents the trace plot of auto lightgbm the optimal values of the number of basic estimators max depth and min child weight were 978 3 20 respectively it can be observed that the auto hyperparameters tuning strategy evidently improved the r score of lightgbm from 0 943 to 0 989 which further promoted the accuracy of gcse 4 2 2 estimation results fig 13 presents the joint distribution of estimation results at 1st a and 2nd b range correction in high dimensional estimation scenario it can be observed that at the first range correction the unknown variable k ii concentrated at the value of 40 fig 13 a which triggered the boundary alert at the second range correction the prior ranges of k ii have been corrected to 30 50 and all of the unknown variables satisfied a normal distribution table 12 shows the estimated results in high dimensional scenario it can be clearly observed that the adaptive boundary adjustment step corrected the vague prior of k ii and the b of the second range correction 59 3 is lower than that of the first range correction 1 2e 6 which reveals that the estimation accuracy have been efficiently improved furthermore the mean relative errors of the unknown variables were mostly below 5 which indicated the acies can provide a precise and reliable estimation results for gcse in general the proposed acies was evaluated using real site coal gangue and high dimensional estimation scenarios and demonstrated promising accuracy and reliability in scenarios with stronger nonlinearity data furthermore the auto lightgbm can serve as a low computational cost surrogate model for performing numerous realizations when using acies to solve gcse significantly improving the estimation efficiency 5 conclusion we have proposed an adaptive range correction ies with a novel auto lightgbm surrogate as the framework of gcse the accuracy and reliability of the proposed framework have been evaluated with coal gangue real site and high dimensional estimation scenarios the results indicated that the proposed method can provide an accurate and reliable simultaneous estimation of the specific head specific flows hydraulic conductivities and release intensity and the following conclusions can be drawn 1 the proposed auto lightgbm can be a high fidelity and easy to perform surrogate of the high calculation cost simulation model which substantially promotes the efficiency of gsce with respect to the generalization accuracy we only consider the guarantee with a large quantity of samples the refined quality sampling technique will be an interesting issue to consider in a further study furthermore the auto hyperparameter tuning technique combined with the other powerful surrogates can also be explored for performing various tasks in the hydrogeology domain 2 while handling with the potential situation of misleading the prior ranges of unknown variables the acies can be adaptively adjusted for the prior ranges of the unknown variables to guarantee the reliability of gsce which further improves the estimation accuracy of gcse in our future investigations the potential of combining the adaptive range correction with other estimation techniques for performing gcse will be investigated funding see the acknowledgments section 8 availability of data and materials available from the corresponding author upon reasonable request 9 code availability available upon reasonable request 10 ethics approval not applicable 11 consent to participate not applicable 12 consent for publication not applicable credit authorship contribution statement zidong pan conceptualization writing original draft software methodology wenxi lu writing review editing methodology software yukun bai supervision validation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the national natural science foundation of china no 42272283 the national key research and development program of china no 2018yfc1800405 and the graduate innovation fund of jilin university no 2022060 
1957,in this work we have proposed an adaptive correction iterative ensemble smoother acies to adaptively adjust the range of unknown variables for improving the estimation accuracy while solving the groundwater contaminated source estimation gcse a prior range for the unknown variables must be provided according to the field investigation or manual auxiliary information however the prior range might not include the actual values thus causing misleading estimation results moreover when using acies to solve gcse it requires massive realizations of the simulation model causing huge calculation cost here a surrogate model has been utilized to alleviate the high calculation cost furthermore the accurate estimation of gcse relies on the high fidelity surrogate models the well tuning of the hyper parameters of the surrogate models can guarantee high fitting accuracy fidelity for the origin simulation model although the existing tuning methods such as the grid search method can satisfactorily tune the surrogate model the long tuning time which has been brought by the ergodic running with every potential combination of hyper parameters must not be neglected thus an auto light gradient boosting machine lightgbm tuned with swarm evolutionary algorithm has been utilized as a high fidelity surrogate model with a swift tuning time both the coal gangue and high dimensional estimation scenarios have been designed to evaluate the performance of the proposed method the results indicated the following 1 while encountered with the vague prior ranges the adaptive ensemble smoother could adjust the potential search ranges to provide an accurate estimation 2 when compared with the typical surrogate models an auto lightgbm surrogate could achieve the promising generalization accuracy with fast a tuning time this guaranteed the high fidelity of the surrogate models which further improved the accuracy of the estimated results keywords auto lightgbm iterative ensemble smoother adaptive range correction groundwater contaminated source estimation data availability data will be made available on request 1 introduction groundwater contaminated source estimation gcse is the perquisite for the risk assessment and contaminant remediation generally gsce involves the matching of the simulated outputs at the monitoring points with the true observed values the methods to perform gsce can be categorized as the simulation optimization simulation statistics and simulation data assimilation methods recently the data assimilation methods have drawn increasing attention in the gcse domain ensemble kalman filter enkf is one of the typical data assimilation methods which is a probabilistic approach developed to address the issue of nonlinear problem katzfuss et al 2016 its variant viz iterative ensemble smoother ies is more efficient to implement gcse xu et al 2021 this efficiency owes to the assimilation by es on all data of all time steps to update the state variables when compared with enkf white 2018 employed ies to estimate high dimensional parameters of a real world scale environmental models lam et al 2020 utilized ies to calibrate the model parameters of a synthetic 2d groundwater model improving the performance of parameter estimation jiang et al 2022 utilized an iterative ensemble smoother and sequential gaussian simulation to simultaneously estimate the information of contaminant sources and hydraulic conductivities in the present study we utilized the ies as the basic estimation framework for gcse however while performing gsce with ies the matching process that involves the calculation of the ensemble of unknown variables inevitably requires massive realizations of simulation model which usually incurs a huge computational burden a surrogate strategy has been commonly employed to alleviate this burden zhao et al 2015 have proposed a time efficient surrogate model to identify the characteristics of the groundwater pollutant source mo et al 2017 have introduced a surrogate using the taylor expansion based adaptive design strategy to reduce the massive calculation cost luo et al 2020 have adopted a constrained trust region ctr based adaptive dynamic surrogate model to substitute the high time cost simulation model for dnapl remediation design scenario wu et al 2022 have proposed a multi objective ensemble surrogate model with a low computational cost for handling the groundwater optimization designs the existing surrogate models can be divided into three categories asher et al 2015 viz data driven projection based and multi fidelity based surrogates among those surrogates the data driven surrogate models have been extensively employed in gcse which involves the support vector regression machine svr bashi azghadi et al 2010 xing et al 2019 extreme learning machine elm hou et al 2021 zhao et al 2020 gaussian process regressor gpr wang et al 2021a zhang et al 2016 artificial neural network ann mo et al 2019 pan et al 2021 singh and datta 2007 srivastava and singh 2015 and decision tress dt baudron et al 2013 buras and donado 2021 gaur et al 2018 li et al 2022 xue et al 2015 ouyang et al 2017 used svr gpr and multi gene genetic programming to construct a surrogate models of multi phase flow model for groundwater remediation design at dnapls contaminated sites efficiently saving the calculation time hou and lu 2018 utilized svr and elm to establish accurate surrogate models of a simulation model to solve gcse problems with dense nonaqueous phase liquids dnapls contaminant elzain et al 2022 employed ann svr and dt to establish data driven surrogate models for the evaluation of groundwater contamination vulnerability recently owing to the powerful generalization capacity of the complex system and fast training process the gradient boosting algorithms gba such as light gradient boosting machine lightgbm guo et al 2021 liang et al 2022 extreme gradient boosting xgboost bhagat et al 2021 parsa 2021 and the adaptive boosting adaboost wen et al 2015 wu et al 2020 have drawn intensive attention in various domains among the above mentioned algorithms lightgbm has been proven to possess high efficiency fast training speed and less memory usage qi 2017 the lightgbm is a novel ensemble learning method based on the decision tree algorithm sun et al 2020 wen et al 2021 the light in lightgbm refers to the fact that it is designed to be lightweight and efficient while still maintaining high accuracy it achieves this by using a number of innovative algorithms that are specifically designed for speed and memory efficiency which involves two techniques viz the gradient based one side sampling and the exclusive feature bundling moreover the lightgbm is customizable that can provide a range of hyperparameters that can be tuned to achieve the best results for a particular problem there have been hardly any reports of lightgbm hitherto for constructing a surrogate model in gcse studies here we explored a novel lightgbm to construct a surrogate model of simulation model in gcse while constructing a surrogate model a critical process is the selection of hyperparameter which determines the fidelity of a surrogate model to its origin simulation model generally the hyper parameters are given according to a manual trial strategy or the grid search strategy although these two strategies can provide proper hyper parameters of a surrogate model the high time cost incurred by the exhaustive search for the combination of hyper parameters cannot be neglected the selection of hyper parameters can be described as an integer programming ip problem whose objective function is to maximize the generalization fidelity measurement there has been significant research exploring the relationship between hyperparameter optimization hpo and ip as they are closely related some recent studies have employed ip models to solve hpo problems more efficiently to achieve better optimization results peng et al 2022 used an integer programming approach to solve the hyperparameter optimization hpo problem for deep neural networks achieving promising performance on real world precipitation prediction problem guo et al 2019 used an integer programming approach to solve the hpo problem for deep neural networks the authors formulated the problem as an ip model and proposed an automatic hyperparameter optimization ahpo method that combines a genetic algorithm ga with a differential evolution de algorithm to find the optimal hyperparameters for the dnn model efficiently improving the generalization accuracy in the present study we have introduced swarm evolutional algorithm as the optimizer to automatically select the optimal hyper parameters of a lightgbm surrogate which aims to efficiently promote the generalization performance gcse can be described as a search process where the trial solutions of the unknown variables are repeatedly updated within the search ranges until the corresponding simulated outputs can match with the observed values at the monitoring points generally when performing gcse prior ranges of unknown variables must be preliminarily given based on the field investigation there have been some researches on the impact of prior range or prior distribution on groundwater pollution source estimation lee and kitanidis 2013 proposes a new approach for identifying discrete geologic structures for contaminant cleanup purpose using bayesian inversion with a total variation prior ranges jiang et al 2021 proposed the self organizing maps algorithm for identifying the contaminant sources and hydraulic conductivity field and investigated the effect of different prior ranges of unknown variables on the accuracy of gcse wang et al 2021b proposed an iterative updating heuristic search strategy for groundwater contamination source identification which updated the prior distributions ranges of unknown variables using final iterations obtained from posterior distributions ranges the researches mention above are based on the assumption that the prior ranges must contain the true values and the posterior ranges are the subset of the prior ranges however in some real site world cases the preliminary prior ranges might not include the true values of the unknown variables which can result in the misleading estimation of unknown variables therefore we have proposed a range correction strategy to adaptively adjust the ranges of unknown variables this can improve both the fault tolerant capability of gsce and the estimation accuracy particularly the ensemble results of the estimation in each epoch require to be examined as follows 1 if the majority of the ensemble samples fall into a boundary range neighborhood the boundary alert will be triggered which is accompanied by the adjustment of ranges 2 otherwise the search ranges remain as the prior ranges we have combined the range correction strategy with es to guarantee the reliability of gcse we have proposed a novel adaptive correction iterative ensemble smoother associated with an auto lightgbm surrogate to perform gcse the proposed method has been evaluated with the coal gangue and high dimensional estimation scenarios the main contributions of the present study are given as follows the novel lightgbm with the auto tuning hyper parameters can be a high fidelity and easy to perform surrogate and save the huge computational burden of the simulation model which efficiently improve the estimation results of gcse while encountering with the situation of the misleading prior ranges of unknown variables a range correction strategy has been combined with es to adaptively adjust the range of unknown variables which can guarantee the reliability of gcse 2 methodology 2 1 numerical simulation model the transportation of the groundwater contaminant can be described by a groundwater flow model and solute transport model the basic differential formula that governs the steady flow can be expressed as 1 x i k h z 0 x i x j k h z 0 x j w x i x j t 0 where x i and x j denote the location distances along the respective cartesian coordinate axis k represents the hydraulic conductivity w denotes the volumetric flux per unit area and h represents the water level above the sea level here z 0 represents the elevation of the aquifer bottom above the sea level t represents the simulation time the basic partial differential governing formula of the solute transport model can be expressed as 2 c t x i d ij c x j x i u i c r θ 3 u i k ij θ h x i where c represents the solute concentration d ij denotes the hydrodynamic dispersion tensor u i represents the average linear velocity that satisfies darcy s law r represents the source or sink term and θ denotes the effective porosity the numerical simulation model has been calculated using modflow and mt3d module of the groundwater modeling system gms 2 2 light gradient boosting machine lightgbm the light gradient boosting machine lightgbm is a novel tree based ensemble learning method which has been proposed by the scholars from microsoft and peking university qi 2017 which intends to solve the efficiency and scalability issue while using xgboost for the scenarios of the high dimensional input features and large dataset size the lightgbm method involves two key techniques viz gradient based one side sampling goss and exclusive feature bundling efb assume a dataset x x i y i i 1 n and the goal of lightgbm is to find a function f x to minimize the expected values of the loss function l between the predicted values of f x and y which can be expressed as 4 f a r g m i n e y x l y f x the predicted values of lightgbm consist of the outputs of a series of basic decision trees models h t x which can be expressed as 5 f x t 1 t h t x where t represents the number of basic decision trees the objective function of lightgbm can be simplified with netwon s method as 6 l t i 1 n g i f x i 1 2 h i f 2 x i to solve the problem of gcse the lightgbm was utilized to establish the regression relationship between the unknown variables and observation data at monitoring wells in particular x represents the unknown variables whereas f x denotes the corresponding simulation outputs at monitoring wells once trained the lightgbm can be a low time cost surrogate model to substitute the long time consuming simulation model when using ies to solve gcse 2 3 iterative ensemble smoother the iterative ensemble smoother ies is a variant of ensemble kalman filter to tackle the nonlinear issue being different from enkf the ies has utilized the observation data of all time to update the unknown parameters variables the simulated concentrations at the monitoring wells for the iteration step t c t can be calculated with the numerical simulation model as 7 c t f v ε 8 v v 1 1 v 1 n v v n s 1 v n s n v where f denotes the numerical simulation model v represents the ensemble matrix of unknown variables n s denotes the ensemble size n v represents the dimension of unknown variables and ε represents the observed error which satisfies a standard normal distribution the unknown variables can be updated as xu et al 2021 9 v j v j 1 g j y obs f v j 1 10 g j c o v v j 1 f v j 1 c o v f v j 1 f v j 1 r 1 11 r ξ 2 d i a g o n e s where y obs represents the observed concentrations of the monitoring wells cov denotes the covariance matrix diag indicates the diagonal matrix and j represents the iteration step 2 4 adaptive range correction strategy while performing gcse with ies in a real world case the prior ranges of unknown variables must be given according to the auxiliary information however the prior ranges are commonly not unbiased possibly causing that the true values of the unknown variables are not fully included in the prior ranges henceforth we have introduced an adaptive range correction strategy to adjust the prior ranges of ies which can guarantee the reliability of estimation results given q l b u b and lb represents the lower boundary of the unknown variables of gcse whereas ub represents the upper boundary we have introduced the q 0 25 0 75 as the measurement of the boundary detection which represents the interval of 25 75 sample points of unknown variables if the boundary falls into the interval q 0 25 0 75 the boundary alert is triggered then the boundary range q requires to be adaptively adjusted to involve a larger range particularly the flowchart of the adaptive correction iterative ensemble smoother acies has been described in table 1 in particular the unknown variables of the estimated results are evaluated at each step until the estimated results of these variables are no longer concentrated at the boundary this approach helps to improve the accuracy of the estimation by refining the estimates of the unknown variables and reducing the uncertainty in the results 3 application 3 1 case overview and assumption 3 1 1 coal gangue scenario the study area is a coal gangue pile located in fushun china with a section of 9 700 m 6 100 m fig 1 the average annual precipitation and evaporation in the study area are 700 and 1 500 mm respectively the northwest boundary γ1 is the hun river which is conceptualized as the specific head boundary the northern boundary γ2 and the southern boundary γ4 are parallel to the groundwater flow line in the area thus they are generalized as the no flow boundaries the eastern boundary γ3 and southwest boundary γ5 have been generalized as the specific flow boundaries according to the medium of aquifer the hydraulic conductivity has been divided into two zones viz k 1 and k 2 with the ranges of 80 md 1 110 md 1 respectively the adsorption of the contaminant is also considered which has an absorption distribution coefficient of 6 5 10 2 cm3g 1 the details of the aquifer are shown in table 2 fig 2 presents the contaminant concentration sulfate ion in the leaching experiment initially the concentration of the contaminant in the leachate has been evidently high the leachate fluctuated during 1 15 h but showed a downward trend thereafter the concentration tended to be stable at 15th hour according to the leaching experiment it has been assumed that the contaminant was continuously released from the coal gangue to the aquifer generally the unknown variables to estimate have been determined as the specific head h s fluxes of specific flows f 1 and f 2 hydraulic conductivities k 1 and k 2 and release intensity s respectively for the calculation of numerical simulation model the domain has been discretized by the grids with the size of 50 m 50 m in the following sections a hypothesis scenario has been first designed to evaluate the performance of the proposed method sequentially the proposed method is applied to a real site scenario in the hypothetical scenario nine observation wells obs1 obs9 have been designed to monitor the contaminant concentration the observation concentrations fig 3 have been synthetized through running the simulation model with the reference true values of the unknown variables to estimate being different from the hypothesis scenario in the real world scenario the observed data of obs4 obs6 obs7 and obs9 are real world measured contaminant concentrations and there are no reference values of the unknown variables the observation data for the real site scenario are scarce when compared with those in the hypothetical scenario 3 1 2 high dimensional estimation scenario to further evaluate the performance of the proposed acies framework a high dimensional estimation scenario was introduced which has observation data with stronger nonlinearity than the coal gangue scenario it must be noted that it is a well designed case derived from pan et al 2021 where release histories of three potential contaminated sources and zone hydraulic conductivities total 19 dimensions required to be simultaneously estimated fig 4 shows the details of potential contaminated source boundaries aquifer parameters and observation wells in particular nine observation wells were set to monitor the contaminant concentration fig 5 presents the observation data 45 dimensions at monitoring wells in high dimensional estimation scenario basic description of known information of the aquifer and contaminated source were shown in table 3 it must be noted that the reference values of unknown variables table 4 were allowed to evaluate the estimation accuracy of acies 3 2 surrogate models of simulation model 3 2 1 coal gangue scenario in this section we have compared the proposed auto lightgbm with the four alternative typical surrogates viz xgboost support vector regression machine svm gaussian process regressor gp and deep fully connected neural network fcnn the surrogates have been established on a python environment with regards to the dataset we have introduced the latin hypercube sampling lhs method to generate 500 patterns of unknown variables inputs sequentially the corresponding concentrations at the monitoring wells outputs can be obtained by running the simulation model with the inputs the inputs and outputs have been integrated to a dataset which has been divided into two parts viz the training dataset 80 and validation dataset 20 the performance of the surrogates can be evaluated from two aspects viz the running time cost and the generalization accuracy the experiments of the surrogates have been computed on a pc with intel core i7 12700h cpu 2 30 ghz processor and 16 0 gb ram the coefficient of determination r has been introduced to evaluate the generalization accuracy 12 r 1 i 1 n y sim i f v sim i 2 y sim i m sim 2 where y sim represents the outputs of the simulation model f x sim i is the predicted values of the surrogate model and m sim denotes the mean values of the outputs y sim the auto lightgbm can be described as an optimization problem given as 13 opt m a x i m u m r 14 s t r γ θ 1 θ 2 θ 3 15 100 θ 1 1200 1 θ 2 10 1 θ 3 50 where θ 1 represents the number of basic estimators θ 2 denotes max depth and θ 3 indicates min child weight the optimal value of θ can be solved by using sea which has been performed with geatpy module https www geatpy com in the python environment the parameters of encoding nind maxgen trapped value and maxtrappedcount have been set to ri 10 30 1e 2 and 100 respectively 3 2 2 high dimensional estimation scenario in high dimensional estimation scenario 400 patterns of samples were generated using lhs 80 for training and 20 validation if the boundary alert of adaptive correction is triggered another 400 patterns of samples will be generated in the updated prior ranges to retrain the auto lightgbm the fixed parameters of auto lightgbm keep the same as those in the coal gangue scenario 3 3 adaptive correction iterative ensemble smoother acies 3 3 1 coal gangue scenario for acies the average bias b between the observation data y obs and the correspond outputs y est of the estimated results v est have been introduced to evaluate the estimated accuracy 16 b i 1 n o y obs i y est i 2 17 y est f v est 18 v est 1 n s i 1 n s v i to consider the ergodicity of the unknown variables latin hypercube sampling has been utilized to generate the initial ensemble of acies with ensemble size n s of 1000 particularly the ensemble size has been selected from the experiment with the size values of 200 600 1 000 and 1 200 where the estimation results of different sizes were close enough between 1 000 and 1 200 the max iteration and the terminated tolerance have been set to 100 and 1e 3 respectively moreover to reduce the huge calculation cost the corresponding outputs y est have been calculated by the surrogate model table 5 presents the prior ranges of the unknown variables in the hypothetical and real site scenarios therefore the initial boundaries lb and ub of the unknown variables have been set according to table 5 when the ranges of the unknown variables have been adjusted the trained surrogate model needs to be re trained with the additional samples of the adjusted ranges the detailed sizes of the process variables in the hypothetical and real site scenarios have been shown in tables 6 and 7 respectively sequentially the proposed acies can be utilized to estimate the unknown variables using the observation data 3 4 high dimensional estimation scenario in this section the fixed parameters of acies keep the same as those in the coal gangue scenario prior ranges of unknown variables were shown in table 8 4 results and discussion 4 1 coal gangue scenario 4 1 1 hypothesis scenario 4 1 1 1 performance of auto lightgbm fig 6 presents the trace plot of r score of the auto lightgbm a and regression plot of auto lightgbm b xgboost c svr d gp e and fcnn f fig 6 a reveals that the auto lightgbm has achieved a steady and promising generalization accuracy with the auto optimal tuning pattern of the hyper parameters when compared with the typical machine learning methods such as xgboost svr and gp the auto lightgbm has achieved better generalization ability with r of 0 99654 furthermore auto lightgbm can even approach similar performance of fcnn with r of 0 99627 which is a 6 256 128 64 54 fully connected structure those comparisons have proved that the auto lightgbm has been suitable to be a high fidelity surrogate of the high calculation cost simulation model with regard to the running cost the numerical simulation model requires nearly 300 000 min for 200 000 realizations while using acies whereas the auto lightgbm requires about 5 5 min for the same number of realizations this owes to the natural high running speed advantage of the data driven model the auto lightgbm can substitute the numerical simulation model with the swift running speed which can save a huge computational burden 4 1 1 2 estimation results figs 7 and 8 visualizes the joint distribution and boxplot of the estimation results at 1st a and 2nd b range correction in the hypothetical scenario in the 1st range correction fig 7 a specific head h s fluxes of specific flow f 1 hydraulic conductivities k 1 and k 2 and release intensity s approximately satisfies the normal distribution whereas f 2 has concentrated at a vertical line the majority of the ensemble of f 2 has been accumulated at the upper boundary which has triggered the boundary alert fig 8 a therefore according to the adaptive boundary adjustment step in table 1 the range of f 2 is to be adjusted from 3 000 3 700 to 3 000 4 400 and the ranges of other variables are constant sequentially the second correction has been performed with the updated boundaries ranges of the unknown variables fig 7 b indicates that the ensembles of the unknown variables satisfy the normal distribution fig 8 b reveals that the ensemble has not triggered the boundary alert thus the estimation loop can be terminated finally we have chosen the individual with the least value of b as the optimal point of the estimated result of gcse the comparison between the estimated results at the first and second corrections has been shown in tables 9 and 10 after the first range correction the estimated results have been evidently closer to the reference values with a lower b which provide a strong proof that the adaptive range correction strategy is valid 4 1 2 real site scenario figs 9 and 10 presents the joint distribution and boxplot of the estimation results at first a and second b range correction in the real site scenario for the first range correction f 1 k 2 and s nearly satisfy the normal distribution whereas h s f 2 and k 1 satisfy the skewed distribution fig 9 a whereas the estimated results of h s f 2 and k 1 have triggered the boundary alert thus the further range corrections for those variables are required to be performed particularly the ranges of those variables have been adjusted from 60 63 3000 3700 and 80 110 to 57 63 3000 4400 and 80 140 respectively for the second range correction fig 9 b all of the unknown variables basically conform to the normal distribution the estimated results are listed in table 11 accordingly the b of the second range correction is apparently below the first range correction which indicates that the adaptive range correction strategy efficiently improves the accuracy of the gcse fig 11 presents the comparison between the simulated and observed concentrations indicating that the corresponding outputs of the estimated results satisfactorily fit with the true measurement data furthermore the spatial and temporal distributions of the contamination plume can be predicted through forward running the simulation model with the estimated results of the unknown variables 4 2 high dimensional scenario 4 2 1 performance of auto lightgbm fig 12 presents the trace plot of auto lightgbm the optimal values of the number of basic estimators max depth and min child weight were 978 3 20 respectively it can be observed that the auto hyperparameters tuning strategy evidently improved the r score of lightgbm from 0 943 to 0 989 which further promoted the accuracy of gcse 4 2 2 estimation results fig 13 presents the joint distribution of estimation results at 1st a and 2nd b range correction in high dimensional estimation scenario it can be observed that at the first range correction the unknown variable k ii concentrated at the value of 40 fig 13 a which triggered the boundary alert at the second range correction the prior ranges of k ii have been corrected to 30 50 and all of the unknown variables satisfied a normal distribution table 12 shows the estimated results in high dimensional scenario it can be clearly observed that the adaptive boundary adjustment step corrected the vague prior of k ii and the b of the second range correction 59 3 is lower than that of the first range correction 1 2e 6 which reveals that the estimation accuracy have been efficiently improved furthermore the mean relative errors of the unknown variables were mostly below 5 which indicated the acies can provide a precise and reliable estimation results for gcse in general the proposed acies was evaluated using real site coal gangue and high dimensional estimation scenarios and demonstrated promising accuracy and reliability in scenarios with stronger nonlinearity data furthermore the auto lightgbm can serve as a low computational cost surrogate model for performing numerous realizations when using acies to solve gcse significantly improving the estimation efficiency 5 conclusion we have proposed an adaptive range correction ies with a novel auto lightgbm surrogate as the framework of gcse the accuracy and reliability of the proposed framework have been evaluated with coal gangue real site and high dimensional estimation scenarios the results indicated that the proposed method can provide an accurate and reliable simultaneous estimation of the specific head specific flows hydraulic conductivities and release intensity and the following conclusions can be drawn 1 the proposed auto lightgbm can be a high fidelity and easy to perform surrogate of the high calculation cost simulation model which substantially promotes the efficiency of gsce with respect to the generalization accuracy we only consider the guarantee with a large quantity of samples the refined quality sampling technique will be an interesting issue to consider in a further study furthermore the auto hyperparameter tuning technique combined with the other powerful surrogates can also be explored for performing various tasks in the hydrogeology domain 2 while handling with the potential situation of misleading the prior ranges of unknown variables the acies can be adaptively adjusted for the prior ranges of the unknown variables to guarantee the reliability of gsce which further improves the estimation accuracy of gcse in our future investigations the potential of combining the adaptive range correction with other estimation techniques for performing gcse will be investigated funding see the acknowledgments section 8 availability of data and materials available from the corresponding author upon reasonable request 9 code availability available upon reasonable request 10 ethics approval not applicable 11 consent to participate not applicable 12 consent for publication not applicable credit authorship contribution statement zidong pan conceptualization writing original draft software methodology wenxi lu writing review editing methodology software yukun bai supervision validation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the national natural science foundation of china no 42272283 the national key research and development program of china no 2018yfc1800405 and the graduate innovation fund of jilin university no 2022060 
1958,drought is a major natural disaster worldwide understanding the correlation between meteorological drought md and agricultural drought ad is essential for relevant policymaking in this paper standardized precipitation evapotranspiration index and standardized soil moisture index were used to estimate the md and ad in the north china plain ncp to identify the correlation between md and ad during the growth period of winter wheat in addition we investigated the contributions of climate change cc and human activity ha to ad and the factors influencing the loss of winter wheat net primary production npp drought propagation time pt increased spatially from the southern to northern ncp from 3 to 11 months pt first increased and then decreased during the phenological period of winter wheat and the decreasing trend was delayed with an increasing latitude in general the relative contribution of cc to ad was higher than that of ha the correlation between md and ad exhibited a weakening trend particularly during the middle and late phenological stages of winter wheat precipitation was the main driver of the effects of ha on ad the effects were stronger in areas with less precipitation however because of the improved irrigation conditions and scarce rainfall during the growth period of winter wheat in the study area the effects of precipitation on ad were nonsignificant instead temperature wind and total solar radiation which are highly correlated with evapotranspiration were identified as the primary drivers of ad spatiotemporal variations were noted in these correlations prolonged drought pt reduced npp the sensitivity of winter wheat npp to ad was higher in humid areas than in semiarid or semihumid areas npp loss occurred primarily due to ha our findings revealed a correlation between md and ad in agroecosystems and may facilitate policymaking related to drought mitigation and food security keywords agricultural drought meteorological drought net primary production propagation time winter wheat data availability data will be made available on request 1 introduction drought a recurrent natural disaster resulting from water scarcity due to the negative difference between long term precipitation and evapotranspiration ezzine et al 2014 huang et al 2015b huang et al 2015a vicente serrano et al 2013 has devastating effects on agriculture and other ecosystems similar to the threats of global warming increased drought frequency is expected to pose a considerable burden on the agricultural sector daryanto et al 2017 dai et al 2022 in china 20 million hectares of crops are affected by drought every year and the annual loss of grain due to drought has been exceeding 16 3 billion kilograms since 1950 according to the data published by the chinese ministry of water resources li et al 2022 potopová et al 2016 agricultural drought ad is regarded as a threat to regional food security and human life because cereals are a key source of energy required for sustaining normal human activities christensen et al 2013 liu et al 2021 hence the correlation between climate change cc and ad and its impact on agroecosystems must be investigated drought can be classified as meteorological drought md ad hydrological drought and socioeconomic drought bae et al 2019 long term precipitation below the normal levels often leads to md li et al 2022 nonreplenishment of soil moisture for a long time may result in water stress which may lead to ad cao et al 2022 xu et al 2021a propagation time pt refers to the time required for md to ad transition and pt is associated with soil moisture and local climate conditions dai et al 2022 liang et al 2014 in the wei yangtze and yellow river basins of china pt is reportedly shorter in summer and autumn but longer in winter and spring temperature and precipitation are regarded as the primary drivers of pt dai et al 2022 huang et al 2015b li et al 2020 zhang et al 2017 demonstrated no time lag in md to ad transition in india where the transition from md to ad may require 1 month li et al 2014 reported a similar result in their study conducted in hailar they demonstrated that md could easily be transformed into ad therefore the pt of different types of drought varies considerably across regions parsons et al 2019 the effects of human activity ha are stronger on agroecosystems than on other ecosystems cao et al 2022 in the present study human activity was defined as encompassing strategies implemented for the management of agroecosystems such as sowing dates variety renewal irrigation fertilizer application and pest control li et al 2016 irrigation increases soil water levels qiu et al 2016 thus hindering md to ad transition in agroecosystems cao et al 2022 however few studies have focused on the effects of ha and cc on ad or the correlation between md and ad under various climatic conditions during different phenological stages of crop growth net primary production npp is a key indicator of carbon cycling in terrestrial ecosystems and is strongly correlated with crop yield wang et al 2019 because the north china plain ncp is the largest producer of winter wheat in china it represents a suitable region for investigating the effects of drought ha and cc on winter wheat the findings may help ensure regional food security and clarify the regional carbon cycle in the present study we divided the growth period of winter wheat into four phenological stages and the ncp into three regions according to precipitation during the growth period of winter wheat to investigate the correlation between md and ad and the spatiotemporal patterns of pt in addition we evaluated the effects of ha and cc on ad as well as the direct and indirect factors influencing the npp of winter wheat 2 materials and methods 2 1 study area the total area of the ncp is approximately 3 105 km2 winter wheat is produced in 57 of the total area which makes the ncp the largest winter wheat producer in china fig 1 the ncp has a temperate continental monsoon climate the annual temperature and precipitation levels in this area decrease with increasing latitude precipitation occurs mainly in june october crop rotation winter wheat summer maize is the main cropping system in the ncp in general winter wheat is planted in early october and harvested between late may and early june of the next year 2 2 data set daily meteorological data 1981 2013 including maximum temperature minimum temperature relative humidity wind speed precipitation and sunshine hours were obtained from a total of 56 meteorological stations fig 1a under a meteorological data sharing network in china https data cma cn daily solar radiation was calculated using a method described by liu et al 2019 remote sensing data 1981 2013 were obtained using google earth engine https developers google com earth engine datasets and included the normalized difference vegetation index ndvi data from the global inventory monitoring and modeling system version 3 g v1 spatial resolution 0 083 0 083 data set of national oceanic and atmospheric administration liu et al 2022 information regarding soil moisture was obtained from the global land data assimilation system version 2 0 which integrates satellite and ground observational data to generate the best representative states and flux fields by using advanced surface simulation and data assimilation systems spennemann et al 2015 2 3 methods 2 3 1 data preprocessing after reviewing a study conducted by jin et al 2016 we used the ndvi threshold method in the google earth engine to assess the distribution of winter wheat between 1981 and 2013 to simplify calculation the main planting area of winter wheat in the ncp was divided into several grids resolution 10 km 10 km fig 1b although the pattern of winter wheat pixels changed slightly every year due to climate and relevant policies the grids remained constant the kriging method was used to interpolate the meteorological data uniform meteorological and planting conditions were assumed for each grid thus the meteorological and remote sensing data for each grid were the mean values of all winter wheat pixels in that grid to ensure consistency with the spatial resolution of ndvi the spatial resolution of each type of data was resampled to 0 083 0 083 the growth period of winter wheat was divided into four phenological stages phenological stage 1 p1 october and november phenological stage 2 p2 december and january phenological stage 3 p3 february and march and phenological stage 4 p4 april and may p1 p2 p3 and p4 represented the seedling overwintering green up and jointing and heading and maturity stages of winter wheat respectively to evaluate the correlation between md and ad under different humid conditions we divided the ncp into three regions according to precipitation during the growth period of winter wheat yang et al 2020 region 1 r1 precipitation 250 mm region 2 r2 150 mm precipitation 250 mm and region 3 r3 precipitation 150 mm 2 3 2 npp of winter wheat the carnegie ames stanford approach casa model was used to estimate the npp of winter wheat in the ncp this model is based on the remote sensing model and is mainly determined on the basis of absorbed photosynthetically active radiation and light energy utilization cramer et al 1999 the casa model has been widely used because the required parameter data can be easily obtained regionally 1 npp a p a r ε 2 par s o l f p a r 0 5 3 fpar n d v i n d v i min f p a r max f p a r min n d v i max n d v i min f p a r min 4 ε t 1 t 2 w ε max where fpar is the fraction of photosynthetically active radiation which is a function of ndvi ndvimin and ndvimax are the minimum and maximum values of ndvi respectively during the growth period of winter wheat and the values exhibit spatiotemporal variations the distribution of ndvimin and ndvimax from 1981 to 2013 is presented as supplementary information fparmin and fparmax are 0 001 and 0 95 respectively ε represents the actual utilization of light energy which is influenced by soil moisture w and temperature stress t1 and t2 a detailed description is available in a study conducted by sun et al 2022 ε max represents the maximum utilization rate of apar under ideal conditions which was set to 1 95 in the present study after referring to a study conducted by fang et al 2021 simulated npp comprises trend climate and random values we used the quadratic polynomial to fit the trend value and then subtracted the trend value from the observed value to obtain a nontrend residual series of npp yi which was then standardized to obtain a standardized residual series of npp srsnpp eq 5 srsnpp was used as an indicator of the loss of winter wheat npp table 1 presents winter wheat srsnpp values classified according to the criteria used by guo et al 2017 5 sr s npp y i μ σ where yi is the nontrend residual series of npp and μ and σ represent the mean and standard deviation of yi respectively 2 3 3 md and ad the standardized precipitation evapotranspiration index spei was developed by vicente serrano et al 2010 by using precipitation and evapotranspiration data the spei includes the multitime scale characteristics of the standardized precipitation index the spei indicates drought by the degree to which the difference between precipitation and evapotranspiration deviates from the average state this index helps evaluate the spatiotemporal characteristics of drought therefore the spei was used to evaluate md in our study eqs 6 8 6 d i p i p e t i 7 spei w c 0 c 1 w c 2 w 2 1 d 1 w d 2 w 2 d 3 w 3 8 w 2 ln p where pi peti and di represent precipitation evapotranspiration and their difference in month i respectively c0 2 515517 c1 0 800853 c2 0 010328 d1 1 432788 d2 0 189269 and d3 0 001308 p represents the probability of exceeding the d value ad primarily results from water stress and several indicators help monitor ad such as the drought severity index palmer drought severity index vegetation health index and standardized soil moisture index ssi some of the parameters required for ad indices are similar to those required for the casa model therefore to avoid collinearity we used ssi to evaluate ad the calculation of ssi is similar to the calculation principle of the spei another effective index used for evaluating ad hao and aghakouchak 2013 table 2 presents spei and ssi values classified according to the criteria used by danandeh mehr et al 2020 and xu et al 2021b and the corresponding drought types 9 ssi sm μ σ where sm represents monthly soil moisture μ and σ are the mean and standard deviation values of long term sm respectively 2 3 4 pt from md to ad pearson correlation coefficients eq 10 were used to investigate correlations between independent and dependent variables to evaluate the pt from md to ad we investigated the correlations between the ssi and spei at different time scales 1 12 months and compared them the spei time scale corresponding to the largest correlation coefficient was regarded as the pt from md to ad 10 r cov x y σ x σ y x i μ x y i μ y x i μ x 2 y i μ y 2 where r is pearson correlation coefficient x and y are independent and dependent variables respectively cov x y is covariance σ is standard deviation μ x and μ y represent the mean values of x i and y i respectively 2 3 5 relative contribution to investigate the correlation between md and ad we first selected the following climatic factors that influence ad spei temperature precipitation wind and total solar radiation dai et al 2022 li et al 2020 multiple regression correlations were identified between ad and independent variables by using the residual trend method eq 11 the response of drought to various climatic factors may be nonlinear however the performance of each variable under different conditions must be thoroughly analyzed to construct nonlinear models which makes the interpretation of results difficult seddon et al 2016 however multiple linear regression can also offer satisfactory results roy 2021 cao et al 2022 hence we performed multiple linear regressions because our purpose was to evaluate empirical rules the primary factors influencing ad were assumed to be cc and ha therefore the effect of ha on ad could be attributed to the portion of effects that could not be explained by multiple regression models ss i cc a s p e i b t e m c p r e d w i n d e s o l 11 where ssicc indicates ad due to only climatic factors a b c and d are the regression coefficients of the spei temperature precipitation wind and total solar radiation respectively pearson correlation analysis was performed to evaluate the pt from md to ad therefore in eq 11 the time scale calculated using temperature precipitation wind and total solar radiation was the same as that of spei which is the time scale when ssi and spei reached their maximum correlation coefficients ad due to ha can be expressed as follows ss i ha s s i s s i cc 12 where ssiha indicates ad due to ha ssi is the observed ad the relative contributions rcs of cc and ha to ad can be expressed as follows r c i slop e i slop e ss i ha slop e ss i cc 13 where rci is the rc of factor i cc or ha to ad and slopei is the trend of factor i the rcs of climatic factors to ad were calculated using the multiple regression method restrend eqs 14 and 15 14 slop e ssi c tem c pr e c wind c sol δ s s i t e m t e m n s s i pr e pr e n s s i w i n d w i n d n s s i s o l s o l n δ 15 r c i c i c tem c pr e c wind c sol where slopessi represents the trend of ssi ctem cpre cwind and csol are the contributions of temperature precipitation wind and total solar radiation to ssi respectively s s i t e m s s i pr e s s i w i n d and s s i s o l are the slopes of the linear regression curves of temperature precipitation wind and total solar radiation with ssi respectively t e m n pr e n w i n d n and s o l n are the slopes of linear regression curves of temperature precipitation wind and total solar radiation with n respectively n is the number of study years rci and ci are the rc and contribution of the climatic factor i to ssi respectively δ is the residual of slopessi and the four aforementioned climatic factors 2 3 6 structural equation modeling a structural equation model sem was used to evaluate the effects of climatic factors ha and drought on the npp of winter wheat sem is a type of covariance matrix based on observed variables which helps analyze causal relationships among multiple variables in a system this method has been widely used in the domains of social economics bayard and jolly 2007 psychology weston et al 2008 and ecology grace et al 2010 in the present study the sem was constructed using the semopy version 2 3 9 package of python https pypi org project semopy 3 results 3 1 pt from md to ad 3 1 1 spatiotemporal patterns of pt in the ncp md to ad transition occurred at different time scales figs 2 and 3 pt increased with increasing latitude the average pt in regions 1 2 and 3 were 3 5 4 7 and 5 8 months during p1 3 8 7 and 7 8 months during p2 3 7 8 9 and 10 3 months during p3 and 3 2 6 1 and 10 4 months during p4 this indicated that the spatial difference in pt increased with the development of winter wheat the average difference between the maximum and minimum pts in regions 1 2 and 3 were 2 3 4 6 6 and 7 2 months during p1 p2 p3 and p4 respectively in regions 1 and 2 pt first increased but then decreased in region 1 the fluctuation range was the smallest 3 or 4 months pt started decreasing during p3 in region 2 the fluctuation range was 4 9 months pt started decreasing during p4 in region 3 the fluctuation range was the largest 5 11 months pt continued to increase but the average increase time was only 0 1 months during p4 3 1 2 trend of the correlation between md and ad with the construction and improvement of irrigation canals in china and considering global warming correlations among different types of drought are evolving thus we calculated the pearson correlation coefficients between the ssi and spei for a total of 13 suborders by using a 20 year time window to determine the trend of correlation between md and ad between 1981 and 2013 a total of 20 samples were generally considered to be the representative of the total sample ma et al 2022 and vinnarasi and dhanya 2019 also investigated the dynamic changes in climate by using a 20 year time window in the present study the correlation between the spei and ssi exhibited a downward trend except during p2 in regions 1 and 2 table 3 fig 4 this suggested that the correlation between md and ad during the growth period of winter wheat in the ncp was weakening the average maximum correlation coefficients between the ssi and spei increased with increasing latitude the corresponding values were 0 71 0 75 and 0 77 in regions 1 2 and 3 respectively the average maximum correlation coefficients between the ssi and spei decreased with the development of winter wheat the corresponding values were 0 78 0 74 0 73 and 0 72 during p1 p2 p3 and p4 respectively during p3 and p4 the decreasing trend of correlation was the largest and reached significance p 0 01 3 2 rcs of factors influencing md 3 2 1 cc and ha significant spatial differences were noted in the rcs of cc and ha to ad during the growth period of winter wheat fig 5 fig s1 cc was the main driver of ad with negative and positive rc in the southern and northern ncp respectively during p3 and p4 in region 3 the rc of ha to ad was higher than that of cc accounting for 61 6 and 51 9 of the total area respectively table 4 with the development of winter wheat the area affected by ha decreased in regions 1 and 2 but increased in region 3 during p2 p3 and p4 the area affected by ha was larger in region 3 than in regions 1 and 2 3 2 2 climatic factors during the growth period of winter wheat temperature precipitation and wind were the main factors affecting ad in region 1 table 5 their rcs were 23 6 19 2 and 32 5 respectively with increasing latitude the rcs of temperature and wind changed from negative to positive and the rc of total solar radiation changed from positive to negative in region 3 the primary climatic factors affecting ad were temperature wind and total solar radiation their rcs were 30 8 19 7 and 22 1 respectively the rc of each climatic factor was small in region 2 which was the main transition region fig 6 the absolute rc value of a climatic factor was considered to be 25 which was regarded as the main factor affecting ad the rcs of climatic factors to ad varied across seasons fig 7 in region 1 precipitation 40 1 and wind 27 2 wind 53 6 and total solar radiation 42 1 temperature 37 9 and precipitation 34 9 and temperature 64 3 were the main drivers of p1 p2 p3 and p4 respectively in region 2 precipitation 29 1 and total solar radiation 52 2 wind 50 5 temperature 60 8 and temperature 48 1 and total solar radiation 32 1 were the main drivers of p1 p2 p3 and p4 respectively in region 3 temperature 37 4 and total solar radiation 47 5 total solar radiation 61 9 temperature 52 3 and solar radiation 33 9 and precipitation 26 1 and wind 47 1 were the main drivers of p1 p2 p3 and p4 respectively 3 3 factors influencing the npp of winter wheat 3 3 1 ad as shown in figs 8 and 9 srsnpp increased with increasing ssi r 0 57 p 0 01 and decreased with increasing pt during p1 r 0 52 p 0 01 p2 r 0 51 p 0 01 p3 r 0 53 p 0 01 and p4 r 0 59 p 0 01 indicating that winter wheat npp increased as pt decreased or soil water availability increased an ssi value of 0 5 indicated no drought stress whereas a value of 0 5 indicated drought stress in univariate regression the slope of variance between the ssi and srsnpp was 0 31 when ssi was 0 5 and 0 64 when ssi was 0 5 this implied that the sensitivity of winter wheat npp to drought is higher in humid areas than in arid areas 3 3 2 sem the main drivers of npp loss in regions 1 and 2 were ha fig 10 followed by pt and ad md exerted a relatively weak effect the path coefficients pc were 0 188 and 0 161 for ha 0 147 and 0 131 for pt 0 105 and 0 102 for ssi and 0 03 and 0 001 for md in regions 1 and 2 respectively in region 3 ha was the main driver of npp loss pc 0 159 followed by md pc 0 149 ad pc 0 144 and pt pc 0 123 this suggested that increased soil water availability and ha increased the npp of winter wheat npp whereas pt decreased it with decreasing precipitation the sensitivity of winter wheat npp to pt ha and ad decreased wind exerted the strongest effect on md and pt and ad was primarily dependent on md while precipitation was the main driver of ha 4 discussion 4 1 spatiotemporal factors influencing pt we noted spatial and seasonal differences in pt which might be associated with the geographical environment and ecological structure drought pt in the warmer southern ncp was shorter than that in the northern ncp this finding corroborates that reported by dai et al 2022 who indicated temperature to be the main factor influencing pt however in previous studies longer pts in winter and spring and shorter pts in summer and autumn were interpreted to have resulted from the accumulation of precipitation in the soil during the previous period dai et al 2022 huang et al 2015a this finding is inconsistent with that of our study because the differences among ecological structures were not considered the rotation between winter wheat and summer maize is the main cropping system in the ncp although the growth period of summer maize coincides with monsoon the loss of soil water due to high temperature leads to water stress chen et al 2019 thus the soil retains low levels of water during the growth period of winter wheat we found that pt first increased and then decreased during the phenological period of winter wheat and the decrease trend was delayed with an increasing latitude because winter wheat is an overwintering crop its growth stagnates and it requires a relatively low amount of water during the overwintering period fang et al 2015 the crop resumes growth during the green up stage because of the increasing temperature and reaches its peak growth during the heading stage the water requirement of winter wheat during this period increases sharply which may easily lead to ad in the absence of additional soil moisture in the warmer southern ncp the overwintering stage ends early and the growth stage begins however low levels of precipitation occur during this period and soil evaporation and crop transpiration accelerate the consumption of soil water the temperature is lower in the northern ncp than in the southern ncp therefore the loss of soil water in the northern ncp is less than that in the southern ncp which leads to a considerable difference in pt between the two regions thus the pt in agroecosystems may be associated with crop phenology and local climate conditions with increasing global warming the advance of phenology may shorten pt saddique et al 2020 thus in the future effective irrigation strategies must be devised to prevent yield loss due to md to ad transition 4 2 effects of cc and ha on ad snowfall is a key source of soil moisture in the study area during cold weather conditions liquid precipitation is converted into snow which covers the soil surface and may prevent the dissipation of ground heat and the entry of cold air because of its low thermal conductivity dutta et al 2018 park et al 2015 zeeman et al 2017 therefore snow cover can keep soil warm and protect crops from frost further when the temperature increases snow serves as a source of moisture for crops we noted spatial variations in the effects of climatic factors on ad in the southern ncp the area of snow covered soil is small and snow melts rapidly because of the warm climate therefore a high amount of snow melt water is absorbed by the agroecosystem in this area before winter wheat enters its vigorous growth period this explains the negative contributions of the major climatic factors e g temperature and wind speed to ad in the southern ncp table 5 although the temperature increase in the northern ncp also shortened the time for winter wheat to enter its vigorous growth period the temperature was considerably lower in the north than in the south further the effects of snow insulation and water supply persisted longer in the north than in the south precipitation was identified as the primary driver of the effects of ha on ad fig 10 however the amount of precipitation during the growth period of winter wheat in study area does not satisfy the water requirement of winter wheat hence precipitation may misguide the development of irrigation strategies resulting in ad in this study cc had a higher rc to ad than did ha and precipitation was not the main driver of ad these findings suggest that the strategies used for addressing ad problems in the ncp during the growth period of winter wheat were inadequate therefore the prediction of ad based only on precipitation is not a sound practice because evapotranspiration is also a driver of ad and is strongly correlated with temperature wind and total solar radiation huo et al 2013 quantifying the rcs of these climatic factors is difficult without the use of modern measurement tools considering the lag of crops to climate wu et al 2015 the growing environment of crops may be unfavorable if ad is identified solely on the basis of precipitation hence we recommend that the local agricultural and meteorological department should predict ad on the basis of the previous meteorological conditions of the region to provide effective irrigation related guidance to farmers the irrigation of farmland is guaranteed because of the construction of irrigation and water conservancy facilities which resulted in a decreasing trend in the correlation between the spei and ssi in our study however the aforementioned approach may not be suitable for the ncp ecosystem irrigation will increase pt our findings revealed that the rc of ha to ad was lower than that of cc which indicates an unreasonable in the irrigation time and a possibility of groundwater waste cao et al 2013 reported substantial overdrawing of groundwater in the ncp 4 3 regional differences in npp loss the npp of winter wheat decreased with increasing pt in the casa model evapotranspiration is a key parameter peng et al 2020 revealed a strong correlation between evapotranspiration and npp furthermore scott et al 2021 reported that higher levels of evapotranspiration from vegetation indicate higher levels of photosynthesis therefore considering the worsening effects of global warming as well as insufficient resources of water and heat in the northern ncp we recommend introducing plant varieties with high heat demand and late maturity to improve the ability of winter wheat to endure the negative effects of low precipitation during the middle and late phenological stages this approach may help reduce the dependence on groundwater and also increase crop transpiration we found that the sensitivity of winter wheat npp to ad was higher in humid areas than in semiarid or semihumid areas fig 9 this might be because crops in semiarid areas have adapted to periodic water deficits measures such as improving crop varieties and implementing advanced irrigation strategies have ensured suitable physiological and management conditions for these crops to cope with drought stress chai et al 2016 vicente serrano et al 2013 thus in the southern ncp the use of inadequate irrigation instead of flood irrigation may be a suitable approach to increasing the adaptation of winter wheat to water deficits 4 4 limitations and future work our study has some limitations considering water deficit as the main driver of drought the ssi was used to monitor ad however pests diseases soil types and nutrition may also influence ad moreover deficit irrigation exerts negligible effects on yield azad et al 2018 introducing uncertainty in the effects of the ssi on crops therefore in the future the changes in the physiological indicators of crops must be considered while developing indicators of ad owing to limited data availability in the present study we could not clarify the irrigation strategies used for winter wheat during its growth period in the ncp thus future studies must focus on the inversion of irrigation amount and frequency by satellite to evaluate the effectiveness of irrigation strategies 5 conclusions we evaluated the correlation between md and ad during different phenological stages of winter wheat growth in the ncp between 1981 and 2013 we further quantified the rcs of cc and ha to ad and identified the factors influencing the loss of winter wheat npp our primary conclusions are as follows 1 during the growth period of winter wheat in the ncp we noted spatiotemporal variations in the pt from md to ad ranging from 3 to 11 months and increasing from the southern to northern ncp temporally pt first increased and then decreased during the phenological period of winter wheat in the study area spatially the decreasing trend was delayed with an increasing latitude 2 farmers identify ad primarily on the basis of precipitation amount ha exerted stronger effects on ad in areas with less precipitation however the correlation between md and ad exhibited a weakening trend particularly in the middle and late growth stages of winter wheat because of the establishment of irrigation and water conservation facilities however in general the effect of cc on ad was stronger than that of ha which indicated that the current irrigation strategies for winter wheat in the ncp require improvement 3 precipitation was not the main driver of ad due to the lack of precipitation during the growth period of winter wheat in the ncp and the improvement of irrigation strategies in the study area the main driving factors were temperature wind speed and solar radiation which are highly correlated with evapotranspiration and the climatic requirements of winter wheat vary across its phenological stages thus the effects of climatic factors on ad differ between the southern and northern ncp therefore when predicting ad in winter wheat agroecosystems the specific phenological stages of winter wheat and its response to local climatic conditions must be considered 4 pt was primarily correlated with wind and negatively correlated with the npp of winter wheat however npp was more sensitive to ad in humid areas than in semiarid areas the sensitivity of npp to pt and ha decreased with decreasing precipitation however ha remained the primary factor influencing the loss of winter wheat npp credit authorship contribution statement jiujiang wu conceptualization methodology formal analysis writing original draft yuhui gu validation data curation kexin sun writing review editing nan wang data curation hongzheng shen visualization yongqiang wang methodology xiaoyi ma writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the national natural science foundation of china grant numbers 52179048 the national key r d program of china grant numbers 2021yfd1900600 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129504 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 supplementary data 2 
1958,drought is a major natural disaster worldwide understanding the correlation between meteorological drought md and agricultural drought ad is essential for relevant policymaking in this paper standardized precipitation evapotranspiration index and standardized soil moisture index were used to estimate the md and ad in the north china plain ncp to identify the correlation between md and ad during the growth period of winter wheat in addition we investigated the contributions of climate change cc and human activity ha to ad and the factors influencing the loss of winter wheat net primary production npp drought propagation time pt increased spatially from the southern to northern ncp from 3 to 11 months pt first increased and then decreased during the phenological period of winter wheat and the decreasing trend was delayed with an increasing latitude in general the relative contribution of cc to ad was higher than that of ha the correlation between md and ad exhibited a weakening trend particularly during the middle and late phenological stages of winter wheat precipitation was the main driver of the effects of ha on ad the effects were stronger in areas with less precipitation however because of the improved irrigation conditions and scarce rainfall during the growth period of winter wheat in the study area the effects of precipitation on ad were nonsignificant instead temperature wind and total solar radiation which are highly correlated with evapotranspiration were identified as the primary drivers of ad spatiotemporal variations were noted in these correlations prolonged drought pt reduced npp the sensitivity of winter wheat npp to ad was higher in humid areas than in semiarid or semihumid areas npp loss occurred primarily due to ha our findings revealed a correlation between md and ad in agroecosystems and may facilitate policymaking related to drought mitigation and food security keywords agricultural drought meteorological drought net primary production propagation time winter wheat data availability data will be made available on request 1 introduction drought a recurrent natural disaster resulting from water scarcity due to the negative difference between long term precipitation and evapotranspiration ezzine et al 2014 huang et al 2015b huang et al 2015a vicente serrano et al 2013 has devastating effects on agriculture and other ecosystems similar to the threats of global warming increased drought frequency is expected to pose a considerable burden on the agricultural sector daryanto et al 2017 dai et al 2022 in china 20 million hectares of crops are affected by drought every year and the annual loss of grain due to drought has been exceeding 16 3 billion kilograms since 1950 according to the data published by the chinese ministry of water resources li et al 2022 potopová et al 2016 agricultural drought ad is regarded as a threat to regional food security and human life because cereals are a key source of energy required for sustaining normal human activities christensen et al 2013 liu et al 2021 hence the correlation between climate change cc and ad and its impact on agroecosystems must be investigated drought can be classified as meteorological drought md ad hydrological drought and socioeconomic drought bae et al 2019 long term precipitation below the normal levels often leads to md li et al 2022 nonreplenishment of soil moisture for a long time may result in water stress which may lead to ad cao et al 2022 xu et al 2021a propagation time pt refers to the time required for md to ad transition and pt is associated with soil moisture and local climate conditions dai et al 2022 liang et al 2014 in the wei yangtze and yellow river basins of china pt is reportedly shorter in summer and autumn but longer in winter and spring temperature and precipitation are regarded as the primary drivers of pt dai et al 2022 huang et al 2015b li et al 2020 zhang et al 2017 demonstrated no time lag in md to ad transition in india where the transition from md to ad may require 1 month li et al 2014 reported a similar result in their study conducted in hailar they demonstrated that md could easily be transformed into ad therefore the pt of different types of drought varies considerably across regions parsons et al 2019 the effects of human activity ha are stronger on agroecosystems than on other ecosystems cao et al 2022 in the present study human activity was defined as encompassing strategies implemented for the management of agroecosystems such as sowing dates variety renewal irrigation fertilizer application and pest control li et al 2016 irrigation increases soil water levels qiu et al 2016 thus hindering md to ad transition in agroecosystems cao et al 2022 however few studies have focused on the effects of ha and cc on ad or the correlation between md and ad under various climatic conditions during different phenological stages of crop growth net primary production npp is a key indicator of carbon cycling in terrestrial ecosystems and is strongly correlated with crop yield wang et al 2019 because the north china plain ncp is the largest producer of winter wheat in china it represents a suitable region for investigating the effects of drought ha and cc on winter wheat the findings may help ensure regional food security and clarify the regional carbon cycle in the present study we divided the growth period of winter wheat into four phenological stages and the ncp into three regions according to precipitation during the growth period of winter wheat to investigate the correlation between md and ad and the spatiotemporal patterns of pt in addition we evaluated the effects of ha and cc on ad as well as the direct and indirect factors influencing the npp of winter wheat 2 materials and methods 2 1 study area the total area of the ncp is approximately 3 105 km2 winter wheat is produced in 57 of the total area which makes the ncp the largest winter wheat producer in china fig 1 the ncp has a temperate continental monsoon climate the annual temperature and precipitation levels in this area decrease with increasing latitude precipitation occurs mainly in june october crop rotation winter wheat summer maize is the main cropping system in the ncp in general winter wheat is planted in early october and harvested between late may and early june of the next year 2 2 data set daily meteorological data 1981 2013 including maximum temperature minimum temperature relative humidity wind speed precipitation and sunshine hours were obtained from a total of 56 meteorological stations fig 1a under a meteorological data sharing network in china https data cma cn daily solar radiation was calculated using a method described by liu et al 2019 remote sensing data 1981 2013 were obtained using google earth engine https developers google com earth engine datasets and included the normalized difference vegetation index ndvi data from the global inventory monitoring and modeling system version 3 g v1 spatial resolution 0 083 0 083 data set of national oceanic and atmospheric administration liu et al 2022 information regarding soil moisture was obtained from the global land data assimilation system version 2 0 which integrates satellite and ground observational data to generate the best representative states and flux fields by using advanced surface simulation and data assimilation systems spennemann et al 2015 2 3 methods 2 3 1 data preprocessing after reviewing a study conducted by jin et al 2016 we used the ndvi threshold method in the google earth engine to assess the distribution of winter wheat between 1981 and 2013 to simplify calculation the main planting area of winter wheat in the ncp was divided into several grids resolution 10 km 10 km fig 1b although the pattern of winter wheat pixels changed slightly every year due to climate and relevant policies the grids remained constant the kriging method was used to interpolate the meteorological data uniform meteorological and planting conditions were assumed for each grid thus the meteorological and remote sensing data for each grid were the mean values of all winter wheat pixels in that grid to ensure consistency with the spatial resolution of ndvi the spatial resolution of each type of data was resampled to 0 083 0 083 the growth period of winter wheat was divided into four phenological stages phenological stage 1 p1 october and november phenological stage 2 p2 december and january phenological stage 3 p3 february and march and phenological stage 4 p4 april and may p1 p2 p3 and p4 represented the seedling overwintering green up and jointing and heading and maturity stages of winter wheat respectively to evaluate the correlation between md and ad under different humid conditions we divided the ncp into three regions according to precipitation during the growth period of winter wheat yang et al 2020 region 1 r1 precipitation 250 mm region 2 r2 150 mm precipitation 250 mm and region 3 r3 precipitation 150 mm 2 3 2 npp of winter wheat the carnegie ames stanford approach casa model was used to estimate the npp of winter wheat in the ncp this model is based on the remote sensing model and is mainly determined on the basis of absorbed photosynthetically active radiation and light energy utilization cramer et al 1999 the casa model has been widely used because the required parameter data can be easily obtained regionally 1 npp a p a r ε 2 par s o l f p a r 0 5 3 fpar n d v i n d v i min f p a r max f p a r min n d v i max n d v i min f p a r min 4 ε t 1 t 2 w ε max where fpar is the fraction of photosynthetically active radiation which is a function of ndvi ndvimin and ndvimax are the minimum and maximum values of ndvi respectively during the growth period of winter wheat and the values exhibit spatiotemporal variations the distribution of ndvimin and ndvimax from 1981 to 2013 is presented as supplementary information fparmin and fparmax are 0 001 and 0 95 respectively ε represents the actual utilization of light energy which is influenced by soil moisture w and temperature stress t1 and t2 a detailed description is available in a study conducted by sun et al 2022 ε max represents the maximum utilization rate of apar under ideal conditions which was set to 1 95 in the present study after referring to a study conducted by fang et al 2021 simulated npp comprises trend climate and random values we used the quadratic polynomial to fit the trend value and then subtracted the trend value from the observed value to obtain a nontrend residual series of npp yi which was then standardized to obtain a standardized residual series of npp srsnpp eq 5 srsnpp was used as an indicator of the loss of winter wheat npp table 1 presents winter wheat srsnpp values classified according to the criteria used by guo et al 2017 5 sr s npp y i μ σ where yi is the nontrend residual series of npp and μ and σ represent the mean and standard deviation of yi respectively 2 3 3 md and ad the standardized precipitation evapotranspiration index spei was developed by vicente serrano et al 2010 by using precipitation and evapotranspiration data the spei includes the multitime scale characteristics of the standardized precipitation index the spei indicates drought by the degree to which the difference between precipitation and evapotranspiration deviates from the average state this index helps evaluate the spatiotemporal characteristics of drought therefore the spei was used to evaluate md in our study eqs 6 8 6 d i p i p e t i 7 spei w c 0 c 1 w c 2 w 2 1 d 1 w d 2 w 2 d 3 w 3 8 w 2 ln p where pi peti and di represent precipitation evapotranspiration and their difference in month i respectively c0 2 515517 c1 0 800853 c2 0 010328 d1 1 432788 d2 0 189269 and d3 0 001308 p represents the probability of exceeding the d value ad primarily results from water stress and several indicators help monitor ad such as the drought severity index palmer drought severity index vegetation health index and standardized soil moisture index ssi some of the parameters required for ad indices are similar to those required for the casa model therefore to avoid collinearity we used ssi to evaluate ad the calculation of ssi is similar to the calculation principle of the spei another effective index used for evaluating ad hao and aghakouchak 2013 table 2 presents spei and ssi values classified according to the criteria used by danandeh mehr et al 2020 and xu et al 2021b and the corresponding drought types 9 ssi sm μ σ where sm represents monthly soil moisture μ and σ are the mean and standard deviation values of long term sm respectively 2 3 4 pt from md to ad pearson correlation coefficients eq 10 were used to investigate correlations between independent and dependent variables to evaluate the pt from md to ad we investigated the correlations between the ssi and spei at different time scales 1 12 months and compared them the spei time scale corresponding to the largest correlation coefficient was regarded as the pt from md to ad 10 r cov x y σ x σ y x i μ x y i μ y x i μ x 2 y i μ y 2 where r is pearson correlation coefficient x and y are independent and dependent variables respectively cov x y is covariance σ is standard deviation μ x and μ y represent the mean values of x i and y i respectively 2 3 5 relative contribution to investigate the correlation between md and ad we first selected the following climatic factors that influence ad spei temperature precipitation wind and total solar radiation dai et al 2022 li et al 2020 multiple regression correlations were identified between ad and independent variables by using the residual trend method eq 11 the response of drought to various climatic factors may be nonlinear however the performance of each variable under different conditions must be thoroughly analyzed to construct nonlinear models which makes the interpretation of results difficult seddon et al 2016 however multiple linear regression can also offer satisfactory results roy 2021 cao et al 2022 hence we performed multiple linear regressions because our purpose was to evaluate empirical rules the primary factors influencing ad were assumed to be cc and ha therefore the effect of ha on ad could be attributed to the portion of effects that could not be explained by multiple regression models ss i cc a s p e i b t e m c p r e d w i n d e s o l 11 where ssicc indicates ad due to only climatic factors a b c and d are the regression coefficients of the spei temperature precipitation wind and total solar radiation respectively pearson correlation analysis was performed to evaluate the pt from md to ad therefore in eq 11 the time scale calculated using temperature precipitation wind and total solar radiation was the same as that of spei which is the time scale when ssi and spei reached their maximum correlation coefficients ad due to ha can be expressed as follows ss i ha s s i s s i cc 12 where ssiha indicates ad due to ha ssi is the observed ad the relative contributions rcs of cc and ha to ad can be expressed as follows r c i slop e i slop e ss i ha slop e ss i cc 13 where rci is the rc of factor i cc or ha to ad and slopei is the trend of factor i the rcs of climatic factors to ad were calculated using the multiple regression method restrend eqs 14 and 15 14 slop e ssi c tem c pr e c wind c sol δ s s i t e m t e m n s s i pr e pr e n s s i w i n d w i n d n s s i s o l s o l n δ 15 r c i c i c tem c pr e c wind c sol where slopessi represents the trend of ssi ctem cpre cwind and csol are the contributions of temperature precipitation wind and total solar radiation to ssi respectively s s i t e m s s i pr e s s i w i n d and s s i s o l are the slopes of the linear regression curves of temperature precipitation wind and total solar radiation with ssi respectively t e m n pr e n w i n d n and s o l n are the slopes of linear regression curves of temperature precipitation wind and total solar radiation with n respectively n is the number of study years rci and ci are the rc and contribution of the climatic factor i to ssi respectively δ is the residual of slopessi and the four aforementioned climatic factors 2 3 6 structural equation modeling a structural equation model sem was used to evaluate the effects of climatic factors ha and drought on the npp of winter wheat sem is a type of covariance matrix based on observed variables which helps analyze causal relationships among multiple variables in a system this method has been widely used in the domains of social economics bayard and jolly 2007 psychology weston et al 2008 and ecology grace et al 2010 in the present study the sem was constructed using the semopy version 2 3 9 package of python https pypi org project semopy 3 results 3 1 pt from md to ad 3 1 1 spatiotemporal patterns of pt in the ncp md to ad transition occurred at different time scales figs 2 and 3 pt increased with increasing latitude the average pt in regions 1 2 and 3 were 3 5 4 7 and 5 8 months during p1 3 8 7 and 7 8 months during p2 3 7 8 9 and 10 3 months during p3 and 3 2 6 1 and 10 4 months during p4 this indicated that the spatial difference in pt increased with the development of winter wheat the average difference between the maximum and minimum pts in regions 1 2 and 3 were 2 3 4 6 6 and 7 2 months during p1 p2 p3 and p4 respectively in regions 1 and 2 pt first increased but then decreased in region 1 the fluctuation range was the smallest 3 or 4 months pt started decreasing during p3 in region 2 the fluctuation range was 4 9 months pt started decreasing during p4 in region 3 the fluctuation range was the largest 5 11 months pt continued to increase but the average increase time was only 0 1 months during p4 3 1 2 trend of the correlation between md and ad with the construction and improvement of irrigation canals in china and considering global warming correlations among different types of drought are evolving thus we calculated the pearson correlation coefficients between the ssi and spei for a total of 13 suborders by using a 20 year time window to determine the trend of correlation between md and ad between 1981 and 2013 a total of 20 samples were generally considered to be the representative of the total sample ma et al 2022 and vinnarasi and dhanya 2019 also investigated the dynamic changes in climate by using a 20 year time window in the present study the correlation between the spei and ssi exhibited a downward trend except during p2 in regions 1 and 2 table 3 fig 4 this suggested that the correlation between md and ad during the growth period of winter wheat in the ncp was weakening the average maximum correlation coefficients between the ssi and spei increased with increasing latitude the corresponding values were 0 71 0 75 and 0 77 in regions 1 2 and 3 respectively the average maximum correlation coefficients between the ssi and spei decreased with the development of winter wheat the corresponding values were 0 78 0 74 0 73 and 0 72 during p1 p2 p3 and p4 respectively during p3 and p4 the decreasing trend of correlation was the largest and reached significance p 0 01 3 2 rcs of factors influencing md 3 2 1 cc and ha significant spatial differences were noted in the rcs of cc and ha to ad during the growth period of winter wheat fig 5 fig s1 cc was the main driver of ad with negative and positive rc in the southern and northern ncp respectively during p3 and p4 in region 3 the rc of ha to ad was higher than that of cc accounting for 61 6 and 51 9 of the total area respectively table 4 with the development of winter wheat the area affected by ha decreased in regions 1 and 2 but increased in region 3 during p2 p3 and p4 the area affected by ha was larger in region 3 than in regions 1 and 2 3 2 2 climatic factors during the growth period of winter wheat temperature precipitation and wind were the main factors affecting ad in region 1 table 5 their rcs were 23 6 19 2 and 32 5 respectively with increasing latitude the rcs of temperature and wind changed from negative to positive and the rc of total solar radiation changed from positive to negative in region 3 the primary climatic factors affecting ad were temperature wind and total solar radiation their rcs were 30 8 19 7 and 22 1 respectively the rc of each climatic factor was small in region 2 which was the main transition region fig 6 the absolute rc value of a climatic factor was considered to be 25 which was regarded as the main factor affecting ad the rcs of climatic factors to ad varied across seasons fig 7 in region 1 precipitation 40 1 and wind 27 2 wind 53 6 and total solar radiation 42 1 temperature 37 9 and precipitation 34 9 and temperature 64 3 were the main drivers of p1 p2 p3 and p4 respectively in region 2 precipitation 29 1 and total solar radiation 52 2 wind 50 5 temperature 60 8 and temperature 48 1 and total solar radiation 32 1 were the main drivers of p1 p2 p3 and p4 respectively in region 3 temperature 37 4 and total solar radiation 47 5 total solar radiation 61 9 temperature 52 3 and solar radiation 33 9 and precipitation 26 1 and wind 47 1 were the main drivers of p1 p2 p3 and p4 respectively 3 3 factors influencing the npp of winter wheat 3 3 1 ad as shown in figs 8 and 9 srsnpp increased with increasing ssi r 0 57 p 0 01 and decreased with increasing pt during p1 r 0 52 p 0 01 p2 r 0 51 p 0 01 p3 r 0 53 p 0 01 and p4 r 0 59 p 0 01 indicating that winter wheat npp increased as pt decreased or soil water availability increased an ssi value of 0 5 indicated no drought stress whereas a value of 0 5 indicated drought stress in univariate regression the slope of variance between the ssi and srsnpp was 0 31 when ssi was 0 5 and 0 64 when ssi was 0 5 this implied that the sensitivity of winter wheat npp to drought is higher in humid areas than in arid areas 3 3 2 sem the main drivers of npp loss in regions 1 and 2 were ha fig 10 followed by pt and ad md exerted a relatively weak effect the path coefficients pc were 0 188 and 0 161 for ha 0 147 and 0 131 for pt 0 105 and 0 102 for ssi and 0 03 and 0 001 for md in regions 1 and 2 respectively in region 3 ha was the main driver of npp loss pc 0 159 followed by md pc 0 149 ad pc 0 144 and pt pc 0 123 this suggested that increased soil water availability and ha increased the npp of winter wheat npp whereas pt decreased it with decreasing precipitation the sensitivity of winter wheat npp to pt ha and ad decreased wind exerted the strongest effect on md and pt and ad was primarily dependent on md while precipitation was the main driver of ha 4 discussion 4 1 spatiotemporal factors influencing pt we noted spatial and seasonal differences in pt which might be associated with the geographical environment and ecological structure drought pt in the warmer southern ncp was shorter than that in the northern ncp this finding corroborates that reported by dai et al 2022 who indicated temperature to be the main factor influencing pt however in previous studies longer pts in winter and spring and shorter pts in summer and autumn were interpreted to have resulted from the accumulation of precipitation in the soil during the previous period dai et al 2022 huang et al 2015a this finding is inconsistent with that of our study because the differences among ecological structures were not considered the rotation between winter wheat and summer maize is the main cropping system in the ncp although the growth period of summer maize coincides with monsoon the loss of soil water due to high temperature leads to water stress chen et al 2019 thus the soil retains low levels of water during the growth period of winter wheat we found that pt first increased and then decreased during the phenological period of winter wheat and the decrease trend was delayed with an increasing latitude because winter wheat is an overwintering crop its growth stagnates and it requires a relatively low amount of water during the overwintering period fang et al 2015 the crop resumes growth during the green up stage because of the increasing temperature and reaches its peak growth during the heading stage the water requirement of winter wheat during this period increases sharply which may easily lead to ad in the absence of additional soil moisture in the warmer southern ncp the overwintering stage ends early and the growth stage begins however low levels of precipitation occur during this period and soil evaporation and crop transpiration accelerate the consumption of soil water the temperature is lower in the northern ncp than in the southern ncp therefore the loss of soil water in the northern ncp is less than that in the southern ncp which leads to a considerable difference in pt between the two regions thus the pt in agroecosystems may be associated with crop phenology and local climate conditions with increasing global warming the advance of phenology may shorten pt saddique et al 2020 thus in the future effective irrigation strategies must be devised to prevent yield loss due to md to ad transition 4 2 effects of cc and ha on ad snowfall is a key source of soil moisture in the study area during cold weather conditions liquid precipitation is converted into snow which covers the soil surface and may prevent the dissipation of ground heat and the entry of cold air because of its low thermal conductivity dutta et al 2018 park et al 2015 zeeman et al 2017 therefore snow cover can keep soil warm and protect crops from frost further when the temperature increases snow serves as a source of moisture for crops we noted spatial variations in the effects of climatic factors on ad in the southern ncp the area of snow covered soil is small and snow melts rapidly because of the warm climate therefore a high amount of snow melt water is absorbed by the agroecosystem in this area before winter wheat enters its vigorous growth period this explains the negative contributions of the major climatic factors e g temperature and wind speed to ad in the southern ncp table 5 although the temperature increase in the northern ncp also shortened the time for winter wheat to enter its vigorous growth period the temperature was considerably lower in the north than in the south further the effects of snow insulation and water supply persisted longer in the north than in the south precipitation was identified as the primary driver of the effects of ha on ad fig 10 however the amount of precipitation during the growth period of winter wheat in study area does not satisfy the water requirement of winter wheat hence precipitation may misguide the development of irrigation strategies resulting in ad in this study cc had a higher rc to ad than did ha and precipitation was not the main driver of ad these findings suggest that the strategies used for addressing ad problems in the ncp during the growth period of winter wheat were inadequate therefore the prediction of ad based only on precipitation is not a sound practice because evapotranspiration is also a driver of ad and is strongly correlated with temperature wind and total solar radiation huo et al 2013 quantifying the rcs of these climatic factors is difficult without the use of modern measurement tools considering the lag of crops to climate wu et al 2015 the growing environment of crops may be unfavorable if ad is identified solely on the basis of precipitation hence we recommend that the local agricultural and meteorological department should predict ad on the basis of the previous meteorological conditions of the region to provide effective irrigation related guidance to farmers the irrigation of farmland is guaranteed because of the construction of irrigation and water conservancy facilities which resulted in a decreasing trend in the correlation between the spei and ssi in our study however the aforementioned approach may not be suitable for the ncp ecosystem irrigation will increase pt our findings revealed that the rc of ha to ad was lower than that of cc which indicates an unreasonable in the irrigation time and a possibility of groundwater waste cao et al 2013 reported substantial overdrawing of groundwater in the ncp 4 3 regional differences in npp loss the npp of winter wheat decreased with increasing pt in the casa model evapotranspiration is a key parameter peng et al 2020 revealed a strong correlation between evapotranspiration and npp furthermore scott et al 2021 reported that higher levels of evapotranspiration from vegetation indicate higher levels of photosynthesis therefore considering the worsening effects of global warming as well as insufficient resources of water and heat in the northern ncp we recommend introducing plant varieties with high heat demand and late maturity to improve the ability of winter wheat to endure the negative effects of low precipitation during the middle and late phenological stages this approach may help reduce the dependence on groundwater and also increase crop transpiration we found that the sensitivity of winter wheat npp to ad was higher in humid areas than in semiarid or semihumid areas fig 9 this might be because crops in semiarid areas have adapted to periodic water deficits measures such as improving crop varieties and implementing advanced irrigation strategies have ensured suitable physiological and management conditions for these crops to cope with drought stress chai et al 2016 vicente serrano et al 2013 thus in the southern ncp the use of inadequate irrigation instead of flood irrigation may be a suitable approach to increasing the adaptation of winter wheat to water deficits 4 4 limitations and future work our study has some limitations considering water deficit as the main driver of drought the ssi was used to monitor ad however pests diseases soil types and nutrition may also influence ad moreover deficit irrigation exerts negligible effects on yield azad et al 2018 introducing uncertainty in the effects of the ssi on crops therefore in the future the changes in the physiological indicators of crops must be considered while developing indicators of ad owing to limited data availability in the present study we could not clarify the irrigation strategies used for winter wheat during its growth period in the ncp thus future studies must focus on the inversion of irrigation amount and frequency by satellite to evaluate the effectiveness of irrigation strategies 5 conclusions we evaluated the correlation between md and ad during different phenological stages of winter wheat growth in the ncp between 1981 and 2013 we further quantified the rcs of cc and ha to ad and identified the factors influencing the loss of winter wheat npp our primary conclusions are as follows 1 during the growth period of winter wheat in the ncp we noted spatiotemporal variations in the pt from md to ad ranging from 3 to 11 months and increasing from the southern to northern ncp temporally pt first increased and then decreased during the phenological period of winter wheat in the study area spatially the decreasing trend was delayed with an increasing latitude 2 farmers identify ad primarily on the basis of precipitation amount ha exerted stronger effects on ad in areas with less precipitation however the correlation between md and ad exhibited a weakening trend particularly in the middle and late growth stages of winter wheat because of the establishment of irrigation and water conservation facilities however in general the effect of cc on ad was stronger than that of ha which indicated that the current irrigation strategies for winter wheat in the ncp require improvement 3 precipitation was not the main driver of ad due to the lack of precipitation during the growth period of winter wheat in the ncp and the improvement of irrigation strategies in the study area the main driving factors were temperature wind speed and solar radiation which are highly correlated with evapotranspiration and the climatic requirements of winter wheat vary across its phenological stages thus the effects of climatic factors on ad differ between the southern and northern ncp therefore when predicting ad in winter wheat agroecosystems the specific phenological stages of winter wheat and its response to local climatic conditions must be considered 4 pt was primarily correlated with wind and negatively correlated with the npp of winter wheat however npp was more sensitive to ad in humid areas than in semiarid areas the sensitivity of npp to pt and ha decreased with decreasing precipitation however ha remained the primary factor influencing the loss of winter wheat npp credit authorship contribution statement jiujiang wu conceptualization methodology formal analysis writing original draft yuhui gu validation data curation kexin sun writing review editing nan wang data curation hongzheng shen visualization yongqiang wang methodology xiaoyi ma writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the national natural science foundation of china grant numbers 52179048 the national key r d program of china grant numbers 2021yfd1900600 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129504 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 supplementary data 2 
1959,the merging of multi source precipitation retrievals prs and gauge based observations go provides a new opportunity for precipitation field estimation however the uncertainties associated with prs may become relatively more evident when the gauge network better captures the spatial distribution of the rainfall fields to dynamically balance the utilisation of prs we propose a merging framework based on a novel concept namely the virtual gauge where the grid cells utilising prs are regarded as virtual gauges and the framework is henceforth referred to as vg the main steps were as follows i determine the locations of virtual gauges from multi source prs ii estimate rainfall at virtual gauges using a basic merging method and iii spatially interpolate using both actual and virtual gauges accordingly the case study employed random forest and inverse distance weight as the basic merging and interpolation methods of vg evaluation using real world data over a region where nearly each 0 1 grid cell contains a ground gauge indicates that vg improves around 7 11 over its basic methods and improves around 32 240 over its inputting prs vg performed better and was more stable than the basic methods under various gauge densities rainfall intensities and rainfall distributions the results showed that vg could supplement the spatial information of rainfall fields missed by the gauge network and reduce interference caused by the uncertainties of prs overall the framework integrates the advantages of existing merging and spatial interpolation methods by adjusting the number locations and rainfall estimations of virtual gauges keywords precipitation merging gauge density rainfall field distribution virtual gauges multi source precipitation products data availability data will be made available on request 1 introduction precipitation is essential in hydrology dou et al 2021a agriculture zhang et al 2020 climatology ye et al 2018 and other scientific fields however accurate monitoring of precipitation fields remains challenging owing to their large spatial heterogeneity in general there are two approaches to estimating precipitation one is via gauge based observations go which is known as the direct measurement approach chacon hurtado et al 2017 mishra and coulibaly 2009 the other is via grid precipitation retrieval prs based on radar hou et al 2015 satellites dou et al 2021b numerical weather models cai et al 2019 and crowd sourced models yang and ng 2017 known as indirect estimation approaches go has been widely used because of its accurate observations at the point scale however practical limitations arise from inadequate spatial coverage which makes it difficult to capture the spatial distribution of rainfall fields for example data on the maximum or minimum rainfall can be missed wei et al 2018 however owing to the current rapid proliferation of telemetry techniques indirect estimation approaches make it possible to obtain continuous rainfall maps which can reflect the spatial heterogeneity of rainfall fields at satisfactory spatial scales sun et al 2018 especially for the locations of maximum and minimum rainfall however there are still multiple errors and uncertainties in the rainfall estimations of prs tang et al 2020 particularly in prs which can be obtained in real time or near real time massari et al 2020 hence merging the two mainstream approaches of precipitation estimation is significant based on the advantages and disadvantages of go and prs two types of merging principles can be considered they are i to correct rainfall estimations of prs using go as a reference and ii to supplement the spatial distribution information of rainfall fields interpolated by go using prs in recent years efforts have been made to derive grid rainfall estimations by merging go and prs following the first type of merging principle these include i bias correction or residual based methods e g multiplicative shift ines and hansen 2006 kernel smoothing li and shao 2010 and standardised reconstruction acharya et al 2013 ii merging prs with weights e g the geographically weighted regression chao et al 2018 bayesian model averaging scheme ma et al 2021 rahman and shang 2020 and three cornered hat method xu et al 2020 and iii machine learning methods e g random forest baez villanueva et al 2020 zhang et al 2022 and support vector machine zhang et al 2021 the procedures of these merging methods can be summarised as follows i at the grid cells with ground gauges train or calibrate a transfer function between merged precipitation and multi source inputting prs using go as criteria and then ii at the grid cells without ground gauges calculate the merged precipitation using the calibrated transfer function hengl et al 2018 therefore these techniques can be regarded as grid by gird methods and they can consider spatial information as predictors of the transfer function baez villanueva et al 2020 compared with spatial interpolation of rainfall using only go the above merging techniques provide outstanding performance in regions where rain gauges are very sparsely distributed such as 127 gauges per 106 km2 and 76 gauges per 106 km2 in chen et al 2020 and bai et al 2019 respectively we found that existing grid by grid merging methods are employed in regions where ground gauges are too sparse to capture rainfall fields therefore no studies have attempted to follow the aforementioned second merging principle however as the gauge density increased the grid by grid merging methods slightly improved over spatial interpolation using go zhang et al 2021 furthermore when the gauge density exceeds a threshold spatial interpolation can perform better than the merging method for example the threshold counted for around 60 gauges per 106 km2 in the study by wu et al 2018 although the threshold seems to be satisfactory at the global scale it is still too sparse to capture rainfall field distribution accurately and steadily compared to the gauge density recommended by the world meteorological organization wmo rodda 2011 that is no less than 1000 gauges per 106 km2 moreover the number of ground gauges is increasing with the development of economy and technology it would be uneconomical for prs to be used only in areas with very sparse gauge networks therefore this study aims to answer how prs can improve estimation of rainfall fields under various gauge densities furthermore it explores the various rainfall field capture abilities of go go can capture rainfall fields relatively better as the gauge density increases sreeparvathy and srinivas 2022 and utilizing prs at each grid may cause interferences due to the relatively more evident uncertainties associated with prs in the merging process thus there is a gauge density threshold above which merging methods cannot produce positive effects the core problem is that each grid cell utilises prs for existing grid by grid merging methods whereas no grid cell utilises prs for spatial interpolation by go therefore it is necessary to pre select grid cells utilising prs before forcing the existing grid by grid merging methods grid cells utilising prs were regarded as virtual gauges in this study which were used to supplement the spatial distribution information for the go interpolated rainfall fields the number and locations of virtual gauges should be dynamically adjusted according to the ability of the go to capture the heterogeneity of rainfall fields as a result the second merging principle mentioned above is naturally taken into consideration here we propose a framework based on virtual gauges used for merging multiple sources of prs and go hereinafter referred to as vg the main steps of vg are to 1 determine the number and locations of virtual gauges from the spatial comparison between multi source prs and go 2 estimate the merged rainfall at virtual gauges and 3 perform spatial interpolation using both virtual gauges and actual gauges the proposed vg was presented with the aim of improving the estimation of the spatial distribution of rainfall fields under varying rainfall distributions and gauge densities additionally the influence of pr combinations on the merging performance of vg was analysed 2 methodology as shown in fig 1 b when estimating rainfall fields by spatial interpolation using ground gauges the theoretical range of rainfall estimation located at the grid cell x without gauges can be approximated as p x p min p max where p min m i n p g 1 p g 2 p max m a x p g 1 p g 2 p x indicates the estimated rainfall located in grid cell x p g i indicates the rainfall measured by g i and g i indicates the ground gauge within a certain distance from the grid cell x however if p x p max or p x p min where p x indicates the true precipitation at x then the ground gauges miss the maximal or minimal rainfall respectively therefore it is necessary to find a point for the maximal or minimal rainfall added to the ground gauges before spatial interpolation to supplement the spatial information and extend the theoretical range of p x as shown in fig 1 rainfall in a number of grid cells is heavier than p max or lighter than p min resulting in the loss of spatial information of the rainfall field by spatial interpolation using only actual gauges after adding the virtual gauges spatial information was supplemented the main steps of the proposed vg are i dividing the study area into multiple sub areas ii for each sub area determine the locations of virtual gauges from the spatial comparison between the rainfall fields estimated by multiple prs and go iii estimating rainfall located at virtual gauges by grid by grid merging methods and iv spatial interpolation using both actual gauges and virtual gauges the algorithm flow chart of vg is shown in fig 2 2 1 virtual gauge merging framework 1 divide area p x should be estimated by go within a control distance commonly considered to be 10 50 km because observations of a ground gauge can reflect rainfall within the distance in other words the gauge controls the area within a given distance grid cells controlled by the same actual gauges were divided into sub areas owing to their same theoretical ranges of rainfall estimations when estimating rainfall fields using pure actual gauges taking step 1 in fig 2 as an example the subareas in yellow and orange are controlled by g 1 and g 2 respectively and the sub area in green is controlled by both g 1 and g 2 2 determine the locations of virtual gauges for the most evident improvement and for less interference caused by pr uncertainties not all p x exceeding the theoretical range of p x need to be considered the maximum and minimum of the p x exceeding the range can be selected as the virtual gauges to supplement the most important spatial information of the rainfall field and to extend the theoretical range of p x as much as possible conditions in a sub area include i some p x is less than p min and some p x is larger than p max therefore in this sub area two virtual gauges need to be added at the two grid cells where the maximum and minimum rainfall are located ii no p x is less than p min and some p x are larger than p max therefore in this sub area one maximal virtual gauge needs to be added at the grid cell where the maximum rainfall is located iii some p x is less than p min and no p x is larger than p max therefore in this sub area one minimal virtual gauge needs to be added at the grid cell where the minimum rainfall is located iv all p x are within the p min p max and no virtual gauge needs to be added in this sub area therefore in each sub area there were up to two virtual gauges for the maximum and minimum rainfall exceeding the theoretical range since the true rainfall fields are unknown a comparison between p x and the theoretical range requires the help of multi source prs in this framework we assumed that the location of a virtual gauge is dependable when it is recommended by more than half sources the locations of the maximal and minimal virtual gauges are recommended based on the spatial comparison between rainfall fields estimated by prs and go to focus on the spatial information of prs min max normalisation was conducted for each source of prs each day the normalised rainfall field estimated by a source of prs namely f s was obtained as shown in eq 1 at a grid cell x 1 f x s p x s m i n p s max p s m i n p s where s indicates a source of prs p s indicates the rainfall field estimated by s in a time step p x s indicates p s at x therefore f s ranges from 0 to 1 when the maximum of f s in a sub area is larger than the maximum of f s located at the gauges which control the sub area the location of the former is recommended as the location of the maximal virtual gauge by s the mathematical expression is for a sub area if 2 max f x s max f g x s a max then the location of max f x s is the location of the maximal virtual gauge recommended by s where f x s f x 1 s f x 2 s f x m s indicates f s located in a sub area x x 1 x 2 x m indicates grid cells in a sub area f g x s f g 1 s f g 2 s indicates f s located at the gauges controlling the sub area g x indicates actual gauges controlling a sub area and is a subset of the ensemble of the total actual gauges the additional a max is a parameter to enhance the significance of the inequality relationship for recommending the location of the maximal virtual gauge and 0 1 0 2 is recommended to avoid the misunderstanding produced by a single source of prs a grid cell can be regarded as the location of the maximal virtual gauge on the condition that the grid cell or its adjacency is recommended as the maximal virtual gauge by more than half sources of prs similarly if 3 m i n f x s min f g x s a min then the location of m i n f x s is the location of the minimal virtual gauge recommended by s where a min is a parameter that enhances the significance of the inequality relationship for recommending the location of the minimal virtual gauge and 0 0 1 is recommended to avoid the misunderstanding produced by a single source of prs a grid cell can be regarded as the location of the minimal virtual gauge on the condition that the grid cell or its adjacency is recommended as the minimal virtual gauge by more than half sources of prs additionally a max and a min are also set to consider the spatial interpolation methods which may estimate rainfall fields that slightly exceed the theoretical range 3 estimate rainfall at virtual gauges the grid by grid merging methods proposed in previous studies can be used in this framework to estimate rainfall at virtual gauges the principle of merging methods is to build a transfer function between the predictors and the prediction using various regression analyses first the transfer function was trained or calibrated at the grid cells with actual gauges as shown in eq 4 4 p g f p g s 1 p g s 2 p g s 3 a g ε where f is the transfer function p g indicates the rainfall measured by the actual gauge g p g s i indicates the rainfall estimation of s i in the grid cell with g a g indicates auxiliary variables related to g such as geographic and topographic information and ε indicates the model error then the estimated rainfall at the virtual gauges p v was calculated using the transfer function as shown in eq 5 5 p v f p v s 1 p v s 2 p v s 3 a v where p v s i and a v are predictors the former indicates the rainfall estimation of s i in the grid cell with virtual gauge v and the latter indicates auxiliary variables related to v it is worth noting that only the grid cells selected as virtual gauges used the values of prs 4 spatial interpolation using both actual and virtual gauges the ensemble of all gauges can be updated as 6 g g 1 g 2 g h v 1 v 2 where v i indicates virtual gauges when the gauges are too sparse to control all grid cells another round of virtual gauge inference should be conducted based on the updated g i divide the area based on the updated g ii determine the location of virtual gauges for each new sub area and iii estimate the rainfall at the new virtual gauges the rainfall at any grid cell x was estimated using spatial interpolation as shown in eq 7 7 p x φ p g ϑ where φ indicates a spatial interpolation method p g p g 1 p g 2 p v 1 p v 2 indicates rainfall at g g 1 g 2 v 1 v 2 g is a subset of g and obeys the equation that d x g d c ϑ indicates auxiliary variables related to g 2 2 experimental design random forest rf prasad et al 2006 was employed as the basic merging method for estimating rainfall at virtual gauges in the experiment the predictors included rainfall estimations of multi source prs located in the target grid cell the measured rainfall of the two actual gauges nearest to the target grid cell and the distances between the nearest actual gauges and the target grid cell the target grid cell indicated the cell containing the gauges actual gauges during training and virtual gauges during estimation the prediction is the measured rainfall located in the cell containing actual gauges during training whereas the prediction is the merged rainfall located in the cell containing virtual gauges when estimating the inverse distance weighted idw method was selected as the basic spatial interpolation method for the experiment in other words vg was based on rf and idw in the experimental case therefore we evaluated and comprised the performances of vg rf and idw in section 4 2 3 evaluation metrics 1 performance of estimated rainfall fields the statistical metrics used to quantify the similarity between the estimated and true rainfall fields during each time step included the pearson correlation coefficient eq 8 and root mean square error eq 9 8 c c t x 1 n p x o p o p x p x 1 n p x o p o 2 x 1 n p x p 2 9 rms e t x 1 n p x p x o 2 n where n indicates the number of grid cells of the rainfall field needed to be evaluated p o indicates the measured rainfall field p x o indicates p o at grid cell x p indicates the estimated rainfall field 10 cc t 1 t c c t t 11 rmse t 1 t r m s e t t where t indicates the number of time steps needed to be evaluated 2 ability of actual gauges to capture the heterogeneity of rainfall fields ground gauges can roughly capture the spatial distribution of rainfall fields roughly and capture ability varied with different rainfall fields the ability of actual gauges to capture the heterogeneity of rainfall fields can be quantified by the correlation between true rainfall fields and rainfall fields estimated by spatial interpolation using actual gauges namely idwcc t this can be expressed by eq 8 where p x is the estimated rainfall calculated by spatial interpolation using the actual gauges idwcc t varied at each time step as the true rainfall field varied a larger idwcc t indicates the better capture ability of actual gauges in contrast a smaller idwcc t indicates that the gauges miss important spatial information about rainfall fields such as rainfall centres 3 contribution of each source of prs in vg varying performances and characteristics associated with prs result in different numbers and locations of virtual gauges therefore the selection of pr sources may influence the merging performance of the vg the shapley value shorrocks 2013 is used to quantify the marginal contribution of each source of prs in vg which can be mathematically expressed as eq 12 12 c i e k k i ω k e k e k i where c i indicates the contribution of the i th source of the prs e indicates the metric for evaluating the merging performance k i indicates the ensemble formed by the pr combinations containing the i th source of the prs k indicates the number of pr sources in the k th combination ω k indicates the weighting factor shown in eq 13 13 ω k k 1 l k l 3 case study 3 1 study area the study area is located in northeast china with a longitude extending from 123 48 e to 128 32 e and latitude extending from 40 11 n to 42 31 n fig 3 it was selected because of the abundance of precipitation gauges which can provide the true daily rainfall fields as references to evaluate the performance of the proposed vg the area of the region is 31 352 km2 and the elevation ranges from 3 to 2400 m benefiting from its location on the eastern coast of eurasia this area has a typical temperate monsoon climate with dry and cold winters rainy and hot summers and short springs and autumns the annual precipitation is approximately 1100 mm and is seriously inhomogeneous where precipitation is concentrated between may and september 3 2 datasets and pre processing 1 gauge based precipitation in this study 624 rainfall gauges were installed and maintained by the ministry of water resources mwr the rainfall gauges were tipping bucket recorders with a minimum detectability of 0 1 mm none of them were included in the global precipitation climatology centre gpcc gauge based precipitation data were collected daily during the wet season from may to september from 2013 to 2020 some gauges sometimes miss the value therefore the working gauges may not be consistent daily missing and extreme values were excluded from quality control gauge based precipitation data were pre processed by allocating to 0 1 grid cells to obtain reliable references that could fairly evaluate the merged rainfall estimations the study area was covered by 332 0 1 grid cells indicating that almost all grid cells contained an actual gauge under the condition of 624 rainfall gauges when there was more than one gauge in a grid cell the rainfall measured by the gauges was averaged as a reference when there was no gauge in the grid cell the rainfall measured by the gauges within 10 km was averaged as the reference sometimes there is also no gauge within 10 km therefore the grid cell is marked as a missing value ten rainfall gauges with less than 2 missing values were selected as the ground actual gauges from the 624 gauges and the gauge density was 308 gauges per 106 km2 the remaining gauges were only used to evaluate the rainfall estimations 2 multi source precipitation retrievals five satellite based precipitation products and three control forecast precipitation products based on numerical weather prediction nwp were selected for this case study as summarised in table 1 they were collected daily to maintain consistency with gauge based rainfall data the main reasons why these sources of prs were selected are as follows i the selected prs are available in near real time which is vital for disaster monitoring and early warning ii they perform worse than post real time gauge bias adjusted prs and are more likely to cause interference during the merging process iii estimation records were available for the wet seasons from 2013 to 2020 the selected prs used in the case study were two near real time nrt versions of the global satellite mapping of precipitation gsmap kubota et al 2020 kubota et al 2007 developed by the japan aerospace exploration agency jaxa i e gsmap v6 nrt gsmap n gsmap v6 gauge nrt gsmap gn two nrt versions of the integrated multi satellite retrievals for the global precipitation measurement imerg huffman et al 2019 huffman et al 2018 developed by the national aeronautics and space administration nasa i e merg v6 early imerg e imerg v6 late imerg l a nrt version of precipitation estimation from remotely sensed information using artificial neural networks persiann hong et al 2004 sorooshian et al 2000 developed by university of california at irvine uci i e persiann ccs three nwp based products respectively provided by european centre for medium range weather forecast ecmwf berner et al 2009 buizza et al 2008 japan meteorological agency jma yamaguchi and majumdar 2010 and national centers for environmental prediction ncep wei et al 2008 the nwp based products were selected on the base time of 00 00 utc with a one day lead time the spatial resolution of nwp based products was 0 5 when they were selected in tigge thorpex interactive grand global ensemble bougeault et al 2010 to obtain a consistent spatial resolution of prs before merging procedure prs were pre processed by resampling to 0 1 that is the three nwp based products were downscaled using the nearest neighbour method while the persiann ccs upscaled using bilinear interpolation the pre processing method aimed to fairly compare merged precipitation to individual pr and avoid any improvements in the products performance prior to the merging procedure baez villanueva et al 2020 4 results 4 1 overall performance comparison between vg rf and idw the vg and rf were forced by ten actual gauges and eight sources of prs while the corresponding ten actual gauges were employed to force the idw fig 4 shows the overall performances cc and rmse of vg rf idw and the eight individual prs the performances of rainfall estimations are better as cc approaches 1 and rmse approaches 0 in other words the blank between the blue and orange bars under the same label approaches the top of fig 4 therefore rainfall estimations can be divided into three echelons according to their overall performance vg rf and idw were the first echelons with significantly better performance than pure prs benefiting from the consideration of go gsmap n gsmap gn imerg e and imerg l are the second echelon with a cc from 0 39 to 0 41 and an rmse from 8 64 mm to 9 19 mm the performances of the timeliest prs namely persiann ccs ecmwf jma and ncep are the worst with a cc from 0 18 to 0 26 and an rmse from 9 24 mm to 10 67 mm among the first echelon rf and idw perform similarly with a cc of 0 56 and 0 54 respectively and an rmse of 6 40 mm and 6 38 mm respectively vg is better than rf and idw in terms of both cc and rmse the cc of vg was 0 60 which improved by 7 1 and 11 1 over that of rf and idw respectively the rmse of vg was 5 89 mm which improved by 8 0 and 7 7 compared to rf and idw respectively virtual gauges were added daily during the study period the number of virtual gauges varied from 1 to 27 averaging 10 82 virtual gauges for each day which is approximately equal to the number of actual gauges 4 2 performances under various rainfall fields fig 5 shows the daily performance comparisons between vg rf and idw using scatter diagrams where each scatter indicates the estimation performance of a day in fig 5 scatters above the 1 1 line indicate that vg a and rf b perform better than idw on those days in other words prs can provide helpful information on those days in contrast scatters below the 1 1 line indicate that prs disturb the estimation performance of rainfall fields on those days 19 of the scatters in fig 5 a and 41 of the scatters in fig 5 b are below the 1 1 line among the scatters below the 1 1 line the scatters in fig 5 b are farther away from the 1 1 line than those in fig 5 a this means that vg reduces the interference caused by pr uncertainties in addition using idwcc t as the horizontal axis can not only compare the daily performance of idw with the other two methods but also see how the performances of vg and rf vary with the ability of actual gauges to capture the heterogeneity of rainfall fields the trend line of the scatters in fig 5 a is significantly higher than the 1 1 line when idwcc t is less than 0 4 gradually approaching the 1 1 line as idwcc t grows the trend line in fig 5 b is above the 1 1 line at first however it is below the 1 1 line after idwcc t becomes larger than 0 5 this indicates that both vg and rf can provide helpful information on rainfall distribution on days when actual gauges miss important spatial information of rainfall fields however when actual gauges capture the spatial distribution of the rainfall fields well vg tends to be consistent with idw whereas rf performs much worse than idw therefore the proposed vg can supplement the important spatial information missed by actual gauges and reduce the interference caused by the uncertainties of prs fig 6 compares the true rainfall fields with those estimated by vg idw and rf on four typical days to demonstrate the effect of virtual gauges the days were selected due to their representativeness and various characteristics regarding rainfall intensity the days shown in fig 6 from a to d rained 196 0 mm 13 9 mm 28 8 mm and 39 7 mm at rainfall centres respectively vg worked well on the condition of each rainfall intensity on the day shown in fig 6 a the rainfall field estimated by vg was the most similar to the true rainfall field followed by idw rf performed the worst the better performance of idw compared with rf shows that the ten actual gauges can roughly capture the spatial distribution of rainfall fields on that day while utilising prs at each grid cell causes significant interference however actual gauges working alone cannot capture all the spatial information of rainfall fields the comparison between the idw and true rainfall fields show an overestimation of idw in the southern part of the study area by comparing vg and idw in fig 6 a it can be observed that the addition of minimal virtual gauges in the southern part of the study area effectively corrected the range of heavy rain on the day shown in fig 6 b the rainfall field estimated by vg was the most similar to the true rainfall field whereas idw underestimated the rainfall because the actual gauges missed the rainfall centre and rf overestimated the rainfall the comparison between vg and idw in fig 6 b shows that adding maximal virtual gauges in the southern part of the study area effectively supplemented the rainfall centre it is worth noting that vg does not always perform significantly better than idw and rf as shown by the scatter near the 1 1 line in fig 5 a taking the day shown in fig 6 c as an example when the rainfall field estimated by idw is already similar to the true one that is when actual gauges can capture the heterogeneity of rainfall fields well vg can perform similarly to idw by adding a few virtual gauges taking the day shown in fig 6 d as an example when the rainfall field estimated by rf is more similar to the true field vg can perform similarly to rf by adding more virtual gauges in summary vg can perform similarly to the better one in idw and rf the vg can balance the utilisation degree of prs by adjusting the number and locations of the virtual gauges 4 3 performances under various gauge densities the rainfall fields estimated by idw rf and vg were forced by gauges of various densities where vg and rf were additionally forced by eight sources of prs to investigate the influence of the actual gauge density we computed vg rf and idw using 2 3 4 6 10 14 22 and 30 actual gauges accounting for 62 92 123 185 308 431 677 and 923 gauges per 106 km2 respectively fig 7 demonstrates that the overall performances cc and rmse of idw rf and vg varied with gauge density in which the darker colour indicates better performance among idw rf and vg for a certain gauge density in terms of both cc and rmse the rainfall fields estimated by vg performed best for each gauge density rf performs better than idw when forced by sparse gauges and worse than idw when the gauge density exceeds the marginal threshold of approximately 300 gauges per 106 km2 in the case study the marginal gauge density varies with the region and period zhang et al 2021 therefore vg is more dependable than the two basic methods regardless of gauge density fig 8 demonstrates that idw rf and vg performance differences varied with gauge densities as the blue scatters and dotted curves in fig 8 shown vg improves slightly over rf when forced by sparse gauges while performing gradually better than rf as the gauge density increases when gauge density approaches zero the performance improvements of vg over rf approach zero too in other words vg tends to perform equally to rf as gauge density tends to be zero as the orange scatters and dotted curves in fig 8 show vg clearly improves over idw when forced by sparse gauges while it increasingly improves slightly than idw as the gauge density increases vg tends to perform equally to idw when forced by very dense gauges the grey scatters and dotted curves are below the orange ones in fig 8 a and above them in fig 8 b regardless of the gauge density 4 4 influences of precipitation retrievals selection each source of prs shows varying performance and has their advantages and disadvantages therefore the number and quality of the selected prs may influence the merging performance of vg we forced vg using various pr combinations and ten actual gauges consistent with section 4 1 to investigate the influence of pr selection because the vg requires at least three sources of prs to recommend virtual gauges there are 219 schemes of pr combinations to force the vg s 3 8 c 8 s 219 using up to eight sources of prs where s indicates the number of input prs fig 9 demonstrates the overall performances cc and rmse of the 219 input schemes of pr combinations using a scatter diagram where each scatter indicates the performance of an input scheme the estimation performance improves as the scatter approaches the bottom right corner of fig 9 the colours distinguish the number of prs in the input scheme under the condition that the numbers of dark blue and grey scatters in fig 9 are equal c 8 3 c 8 5 56 dark blue scatters are more scattered than grey scatters therefore the uncertainty of the results can be reduced by using more sources of prs when using a certain number of prs to force vg three sources of prs for example cc and rmse of the worst input scheme are 0 54 and 6 17 respectively while cc and rmse of the best input scheme are 0 60 and 5 83 respectively therefore these large differences are caused by the quality of the different prs it is noting that vg forced by the worst input scheme performs similarly with idw cc 0 54 rmse 6 38 so the detailed differences between vg forced by the worst input scheme and it forced by a better input scheme can be seen from the comparison between idw and vg in section 4 2 fig 10 demonstrates pr contribution to the performance of vg varying with pr quality where the pr contribution is quantified by shapley value and the dotted lines indicate the trendlines of scatters the shapley values increase as the cc of prs increase and decrease as the rmse of prs increase which indicates the larger contribution of the more accurate prs to the vg performances the scatters in fig 10 a are more correlated than those in fig 10 b indicating that the correlation between pr contribution and cc is more significant than that between pr contribution and rmse therefore cc of prs is more noteworthy than rmse of prs when forcing vg 5 discussion apart from the grid by grid merging methods another type of merging method considers the spatial distribution of prs as a whole by inputting a source of rainfall field like an image in a time step pan et al 2019 and using convolution calculation examples are the long short term memory wu et al 2020 convolutional neural network xue et al 2021 and generative adversarial network wang et al 2021 spatial techniques mine the spatial distribution features of prs using multilayer convolution calculation and find the correlations automatically as a result they are black box models a larger size of samples with fewer uncertainties can reduce the difficulties associated with deep learning in general the sample size of spatial techniques is approximately 20 times larger than that of grid by grid techniques biau and scornet 2016 the proposed vg is a white box model and can consider the spatial distribution of prs therefore it can estimate rainfall fields using smaller samples with relatively larger uncertainties after the comparison between prs and go in section 1 we obtained two merging principles i correct rainfall estimations of prs using go as references and ii supplement the spatial distribution information of rainfall fields interpolated by go using prs the two crucial procedures of the proposed vg correspond to the two merging principles i to determine the locations of virtual gauges from the spatial comparison between the rainfall fields estimated by multiple prs and go and ii to estimate rainfall located at virtual gauges by grid by grid merging methods the aim of the former procedure is to investigate whether the spatial distributions of the rainfall fields interpolated by go need to be supplemented the aim of the latter procedure is to correct the rainfall estimations of the prs using go as a reference therefore vg follows both principles of the merging procedures mentioned in section 1 grid by grid merging techniques perform worse than spatial interpolation using only go when ground gauges capture the spatial distribution of rainfall fields better in contrast they perform better when ground gauges capture the spatial distribution of rainfall fields worse to solve this problem the proposed vg can supplement the important spatial information missed by actual gauges and reduce the interference caused by the uncertainties of prs the following are the efforts we made in vg to reduce the interference caused by the uncertainties of prs 1 prs are utilised only where the important spatial information needs to be supplemented 2 only the locations of pr extreme values in a sub area are considered because prs are skilled in reflecting the locations of maximum and minimum rainfall and 3 a grid cell can be regarded as a virtual gauge only when it is recommended by more than half of the sources of prs after adding virtual gauges to the actual gauges a spatial interpolation was conducted therefore vg integrates spatial interpolation using only go and the methods that merge prs and go 6 conclusions this paper proposes a framework vg for merging rainfall estimation of multi source prs and go based on a novel concept namely the virtual gauge in the proposed vg prs can perform positively under various densities of gauge networks and various the ability of gauge networks to capture the heterogeneity of rainfall fields to illustrate this vg was assessed over a region where almost all 0 1 grid cells contained a ground gauge the primary conclusions are summarised as follows 1 in terms of overall performance during the study period vg performed better than its basic methods namely idw and rf and inputting prs namely gsmap n gsmap gn imerg e imerg l persiann ccs ecmwf jma and ncep when ten gauges were selected as the inputs counting for 308 gauges per 106 km2 2 the daily performance comparison showed that the proposed vg could supplement the important spatial information of rainfall fields when the gauge network cannot capture the rainfall field well however it can reduce the interference caused by the uncertainties of prs when the gauge network captures the rainfall fields well under the condition of various rainfall intensities and rainfall distributions 3 a comparison of idw rf and vg forced by gauges of various densities showed that vg always performed the best regardless of the gauge density vg performed much better than rf under the condition of dense gauges whereas it performed much better than idw under the condition of sparse gauges vg tended to perform equally with rf when the gauge density tended to be zero while it tended to perform equally with idw when forced by very dense gauges 4 vg forced by various pr combinations showed the influence of the number and quality of prs on the merging performance of vg on the one hand the uncertainty of performance can be reduced by using more sources of prs however there is a larger contribution to the vg performance of the more accurate prs especially for those with accurate spatial distributions it is recommended that vg be forced using more sources of prs with better spatial performance overall vg performed better and was more stable than the basic methods vg is a framework that can integrate the advantages of existing grid by grid merging and spatial interpolation methods by adjusting the number and location of virtual gauges to balance the degree of pr utilisation as always we invite discussions and collaborations on these and related aspects credit authorship contribution statement yanhong dou conceptualization methodology validation formal analysis writing original draft visualization lei ye funding acquisition resources supervision writing review editing jiaqi ai software data curation chi zhang funding acquisition resources project administration huicheng zhou funding acquisition resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china nos 52179006 51925902 u2240204 and the fundamental research funds for the central universities no dut22jc28 the precipitation data can be obtained from https disc gsfc nasa gov https www jaxa jp https chrsdata eng uci edu and https apps ecmwf int datasets data tigge 
1959,the merging of multi source precipitation retrievals prs and gauge based observations go provides a new opportunity for precipitation field estimation however the uncertainties associated with prs may become relatively more evident when the gauge network better captures the spatial distribution of the rainfall fields to dynamically balance the utilisation of prs we propose a merging framework based on a novel concept namely the virtual gauge where the grid cells utilising prs are regarded as virtual gauges and the framework is henceforth referred to as vg the main steps were as follows i determine the locations of virtual gauges from multi source prs ii estimate rainfall at virtual gauges using a basic merging method and iii spatially interpolate using both actual and virtual gauges accordingly the case study employed random forest and inverse distance weight as the basic merging and interpolation methods of vg evaluation using real world data over a region where nearly each 0 1 grid cell contains a ground gauge indicates that vg improves around 7 11 over its basic methods and improves around 32 240 over its inputting prs vg performed better and was more stable than the basic methods under various gauge densities rainfall intensities and rainfall distributions the results showed that vg could supplement the spatial information of rainfall fields missed by the gauge network and reduce interference caused by the uncertainties of prs overall the framework integrates the advantages of existing merging and spatial interpolation methods by adjusting the number locations and rainfall estimations of virtual gauges keywords precipitation merging gauge density rainfall field distribution virtual gauges multi source precipitation products data availability data will be made available on request 1 introduction precipitation is essential in hydrology dou et al 2021a agriculture zhang et al 2020 climatology ye et al 2018 and other scientific fields however accurate monitoring of precipitation fields remains challenging owing to their large spatial heterogeneity in general there are two approaches to estimating precipitation one is via gauge based observations go which is known as the direct measurement approach chacon hurtado et al 2017 mishra and coulibaly 2009 the other is via grid precipitation retrieval prs based on radar hou et al 2015 satellites dou et al 2021b numerical weather models cai et al 2019 and crowd sourced models yang and ng 2017 known as indirect estimation approaches go has been widely used because of its accurate observations at the point scale however practical limitations arise from inadequate spatial coverage which makes it difficult to capture the spatial distribution of rainfall fields for example data on the maximum or minimum rainfall can be missed wei et al 2018 however owing to the current rapid proliferation of telemetry techniques indirect estimation approaches make it possible to obtain continuous rainfall maps which can reflect the spatial heterogeneity of rainfall fields at satisfactory spatial scales sun et al 2018 especially for the locations of maximum and minimum rainfall however there are still multiple errors and uncertainties in the rainfall estimations of prs tang et al 2020 particularly in prs which can be obtained in real time or near real time massari et al 2020 hence merging the two mainstream approaches of precipitation estimation is significant based on the advantages and disadvantages of go and prs two types of merging principles can be considered they are i to correct rainfall estimations of prs using go as a reference and ii to supplement the spatial distribution information of rainfall fields interpolated by go using prs in recent years efforts have been made to derive grid rainfall estimations by merging go and prs following the first type of merging principle these include i bias correction or residual based methods e g multiplicative shift ines and hansen 2006 kernel smoothing li and shao 2010 and standardised reconstruction acharya et al 2013 ii merging prs with weights e g the geographically weighted regression chao et al 2018 bayesian model averaging scheme ma et al 2021 rahman and shang 2020 and three cornered hat method xu et al 2020 and iii machine learning methods e g random forest baez villanueva et al 2020 zhang et al 2022 and support vector machine zhang et al 2021 the procedures of these merging methods can be summarised as follows i at the grid cells with ground gauges train or calibrate a transfer function between merged precipitation and multi source inputting prs using go as criteria and then ii at the grid cells without ground gauges calculate the merged precipitation using the calibrated transfer function hengl et al 2018 therefore these techniques can be regarded as grid by gird methods and they can consider spatial information as predictors of the transfer function baez villanueva et al 2020 compared with spatial interpolation of rainfall using only go the above merging techniques provide outstanding performance in regions where rain gauges are very sparsely distributed such as 127 gauges per 106 km2 and 76 gauges per 106 km2 in chen et al 2020 and bai et al 2019 respectively we found that existing grid by grid merging methods are employed in regions where ground gauges are too sparse to capture rainfall fields therefore no studies have attempted to follow the aforementioned second merging principle however as the gauge density increased the grid by grid merging methods slightly improved over spatial interpolation using go zhang et al 2021 furthermore when the gauge density exceeds a threshold spatial interpolation can perform better than the merging method for example the threshold counted for around 60 gauges per 106 km2 in the study by wu et al 2018 although the threshold seems to be satisfactory at the global scale it is still too sparse to capture rainfall field distribution accurately and steadily compared to the gauge density recommended by the world meteorological organization wmo rodda 2011 that is no less than 1000 gauges per 106 km2 moreover the number of ground gauges is increasing with the development of economy and technology it would be uneconomical for prs to be used only in areas with very sparse gauge networks therefore this study aims to answer how prs can improve estimation of rainfall fields under various gauge densities furthermore it explores the various rainfall field capture abilities of go go can capture rainfall fields relatively better as the gauge density increases sreeparvathy and srinivas 2022 and utilizing prs at each grid may cause interferences due to the relatively more evident uncertainties associated with prs in the merging process thus there is a gauge density threshold above which merging methods cannot produce positive effects the core problem is that each grid cell utilises prs for existing grid by grid merging methods whereas no grid cell utilises prs for spatial interpolation by go therefore it is necessary to pre select grid cells utilising prs before forcing the existing grid by grid merging methods grid cells utilising prs were regarded as virtual gauges in this study which were used to supplement the spatial distribution information for the go interpolated rainfall fields the number and locations of virtual gauges should be dynamically adjusted according to the ability of the go to capture the heterogeneity of rainfall fields as a result the second merging principle mentioned above is naturally taken into consideration here we propose a framework based on virtual gauges used for merging multiple sources of prs and go hereinafter referred to as vg the main steps of vg are to 1 determine the number and locations of virtual gauges from the spatial comparison between multi source prs and go 2 estimate the merged rainfall at virtual gauges and 3 perform spatial interpolation using both virtual gauges and actual gauges the proposed vg was presented with the aim of improving the estimation of the spatial distribution of rainfall fields under varying rainfall distributions and gauge densities additionally the influence of pr combinations on the merging performance of vg was analysed 2 methodology as shown in fig 1 b when estimating rainfall fields by spatial interpolation using ground gauges the theoretical range of rainfall estimation located at the grid cell x without gauges can be approximated as p x p min p max where p min m i n p g 1 p g 2 p max m a x p g 1 p g 2 p x indicates the estimated rainfall located in grid cell x p g i indicates the rainfall measured by g i and g i indicates the ground gauge within a certain distance from the grid cell x however if p x p max or p x p min where p x indicates the true precipitation at x then the ground gauges miss the maximal or minimal rainfall respectively therefore it is necessary to find a point for the maximal or minimal rainfall added to the ground gauges before spatial interpolation to supplement the spatial information and extend the theoretical range of p x as shown in fig 1 rainfall in a number of grid cells is heavier than p max or lighter than p min resulting in the loss of spatial information of the rainfall field by spatial interpolation using only actual gauges after adding the virtual gauges spatial information was supplemented the main steps of the proposed vg are i dividing the study area into multiple sub areas ii for each sub area determine the locations of virtual gauges from the spatial comparison between the rainfall fields estimated by multiple prs and go iii estimating rainfall located at virtual gauges by grid by grid merging methods and iv spatial interpolation using both actual gauges and virtual gauges the algorithm flow chart of vg is shown in fig 2 2 1 virtual gauge merging framework 1 divide area p x should be estimated by go within a control distance commonly considered to be 10 50 km because observations of a ground gauge can reflect rainfall within the distance in other words the gauge controls the area within a given distance grid cells controlled by the same actual gauges were divided into sub areas owing to their same theoretical ranges of rainfall estimations when estimating rainfall fields using pure actual gauges taking step 1 in fig 2 as an example the subareas in yellow and orange are controlled by g 1 and g 2 respectively and the sub area in green is controlled by both g 1 and g 2 2 determine the locations of virtual gauges for the most evident improvement and for less interference caused by pr uncertainties not all p x exceeding the theoretical range of p x need to be considered the maximum and minimum of the p x exceeding the range can be selected as the virtual gauges to supplement the most important spatial information of the rainfall field and to extend the theoretical range of p x as much as possible conditions in a sub area include i some p x is less than p min and some p x is larger than p max therefore in this sub area two virtual gauges need to be added at the two grid cells where the maximum and minimum rainfall are located ii no p x is less than p min and some p x are larger than p max therefore in this sub area one maximal virtual gauge needs to be added at the grid cell where the maximum rainfall is located iii some p x is less than p min and no p x is larger than p max therefore in this sub area one minimal virtual gauge needs to be added at the grid cell where the minimum rainfall is located iv all p x are within the p min p max and no virtual gauge needs to be added in this sub area therefore in each sub area there were up to two virtual gauges for the maximum and minimum rainfall exceeding the theoretical range since the true rainfall fields are unknown a comparison between p x and the theoretical range requires the help of multi source prs in this framework we assumed that the location of a virtual gauge is dependable when it is recommended by more than half sources the locations of the maximal and minimal virtual gauges are recommended based on the spatial comparison between rainfall fields estimated by prs and go to focus on the spatial information of prs min max normalisation was conducted for each source of prs each day the normalised rainfall field estimated by a source of prs namely f s was obtained as shown in eq 1 at a grid cell x 1 f x s p x s m i n p s max p s m i n p s where s indicates a source of prs p s indicates the rainfall field estimated by s in a time step p x s indicates p s at x therefore f s ranges from 0 to 1 when the maximum of f s in a sub area is larger than the maximum of f s located at the gauges which control the sub area the location of the former is recommended as the location of the maximal virtual gauge by s the mathematical expression is for a sub area if 2 max f x s max f g x s a max then the location of max f x s is the location of the maximal virtual gauge recommended by s where f x s f x 1 s f x 2 s f x m s indicates f s located in a sub area x x 1 x 2 x m indicates grid cells in a sub area f g x s f g 1 s f g 2 s indicates f s located at the gauges controlling the sub area g x indicates actual gauges controlling a sub area and is a subset of the ensemble of the total actual gauges the additional a max is a parameter to enhance the significance of the inequality relationship for recommending the location of the maximal virtual gauge and 0 1 0 2 is recommended to avoid the misunderstanding produced by a single source of prs a grid cell can be regarded as the location of the maximal virtual gauge on the condition that the grid cell or its adjacency is recommended as the maximal virtual gauge by more than half sources of prs similarly if 3 m i n f x s min f g x s a min then the location of m i n f x s is the location of the minimal virtual gauge recommended by s where a min is a parameter that enhances the significance of the inequality relationship for recommending the location of the minimal virtual gauge and 0 0 1 is recommended to avoid the misunderstanding produced by a single source of prs a grid cell can be regarded as the location of the minimal virtual gauge on the condition that the grid cell or its adjacency is recommended as the minimal virtual gauge by more than half sources of prs additionally a max and a min are also set to consider the spatial interpolation methods which may estimate rainfall fields that slightly exceed the theoretical range 3 estimate rainfall at virtual gauges the grid by grid merging methods proposed in previous studies can be used in this framework to estimate rainfall at virtual gauges the principle of merging methods is to build a transfer function between the predictors and the prediction using various regression analyses first the transfer function was trained or calibrated at the grid cells with actual gauges as shown in eq 4 4 p g f p g s 1 p g s 2 p g s 3 a g ε where f is the transfer function p g indicates the rainfall measured by the actual gauge g p g s i indicates the rainfall estimation of s i in the grid cell with g a g indicates auxiliary variables related to g such as geographic and topographic information and ε indicates the model error then the estimated rainfall at the virtual gauges p v was calculated using the transfer function as shown in eq 5 5 p v f p v s 1 p v s 2 p v s 3 a v where p v s i and a v are predictors the former indicates the rainfall estimation of s i in the grid cell with virtual gauge v and the latter indicates auxiliary variables related to v it is worth noting that only the grid cells selected as virtual gauges used the values of prs 4 spatial interpolation using both actual and virtual gauges the ensemble of all gauges can be updated as 6 g g 1 g 2 g h v 1 v 2 where v i indicates virtual gauges when the gauges are too sparse to control all grid cells another round of virtual gauge inference should be conducted based on the updated g i divide the area based on the updated g ii determine the location of virtual gauges for each new sub area and iii estimate the rainfall at the new virtual gauges the rainfall at any grid cell x was estimated using spatial interpolation as shown in eq 7 7 p x φ p g ϑ where φ indicates a spatial interpolation method p g p g 1 p g 2 p v 1 p v 2 indicates rainfall at g g 1 g 2 v 1 v 2 g is a subset of g and obeys the equation that d x g d c ϑ indicates auxiliary variables related to g 2 2 experimental design random forest rf prasad et al 2006 was employed as the basic merging method for estimating rainfall at virtual gauges in the experiment the predictors included rainfall estimations of multi source prs located in the target grid cell the measured rainfall of the two actual gauges nearest to the target grid cell and the distances between the nearest actual gauges and the target grid cell the target grid cell indicated the cell containing the gauges actual gauges during training and virtual gauges during estimation the prediction is the measured rainfall located in the cell containing actual gauges during training whereas the prediction is the merged rainfall located in the cell containing virtual gauges when estimating the inverse distance weighted idw method was selected as the basic spatial interpolation method for the experiment in other words vg was based on rf and idw in the experimental case therefore we evaluated and comprised the performances of vg rf and idw in section 4 2 3 evaluation metrics 1 performance of estimated rainfall fields the statistical metrics used to quantify the similarity between the estimated and true rainfall fields during each time step included the pearson correlation coefficient eq 8 and root mean square error eq 9 8 c c t x 1 n p x o p o p x p x 1 n p x o p o 2 x 1 n p x p 2 9 rms e t x 1 n p x p x o 2 n where n indicates the number of grid cells of the rainfall field needed to be evaluated p o indicates the measured rainfall field p x o indicates p o at grid cell x p indicates the estimated rainfall field 10 cc t 1 t c c t t 11 rmse t 1 t r m s e t t where t indicates the number of time steps needed to be evaluated 2 ability of actual gauges to capture the heterogeneity of rainfall fields ground gauges can roughly capture the spatial distribution of rainfall fields roughly and capture ability varied with different rainfall fields the ability of actual gauges to capture the heterogeneity of rainfall fields can be quantified by the correlation between true rainfall fields and rainfall fields estimated by spatial interpolation using actual gauges namely idwcc t this can be expressed by eq 8 where p x is the estimated rainfall calculated by spatial interpolation using the actual gauges idwcc t varied at each time step as the true rainfall field varied a larger idwcc t indicates the better capture ability of actual gauges in contrast a smaller idwcc t indicates that the gauges miss important spatial information about rainfall fields such as rainfall centres 3 contribution of each source of prs in vg varying performances and characteristics associated with prs result in different numbers and locations of virtual gauges therefore the selection of pr sources may influence the merging performance of the vg the shapley value shorrocks 2013 is used to quantify the marginal contribution of each source of prs in vg which can be mathematically expressed as eq 12 12 c i e k k i ω k e k e k i where c i indicates the contribution of the i th source of the prs e indicates the metric for evaluating the merging performance k i indicates the ensemble formed by the pr combinations containing the i th source of the prs k indicates the number of pr sources in the k th combination ω k indicates the weighting factor shown in eq 13 13 ω k k 1 l k l 3 case study 3 1 study area the study area is located in northeast china with a longitude extending from 123 48 e to 128 32 e and latitude extending from 40 11 n to 42 31 n fig 3 it was selected because of the abundance of precipitation gauges which can provide the true daily rainfall fields as references to evaluate the performance of the proposed vg the area of the region is 31 352 km2 and the elevation ranges from 3 to 2400 m benefiting from its location on the eastern coast of eurasia this area has a typical temperate monsoon climate with dry and cold winters rainy and hot summers and short springs and autumns the annual precipitation is approximately 1100 mm and is seriously inhomogeneous where precipitation is concentrated between may and september 3 2 datasets and pre processing 1 gauge based precipitation in this study 624 rainfall gauges were installed and maintained by the ministry of water resources mwr the rainfall gauges were tipping bucket recorders with a minimum detectability of 0 1 mm none of them were included in the global precipitation climatology centre gpcc gauge based precipitation data were collected daily during the wet season from may to september from 2013 to 2020 some gauges sometimes miss the value therefore the working gauges may not be consistent daily missing and extreme values were excluded from quality control gauge based precipitation data were pre processed by allocating to 0 1 grid cells to obtain reliable references that could fairly evaluate the merged rainfall estimations the study area was covered by 332 0 1 grid cells indicating that almost all grid cells contained an actual gauge under the condition of 624 rainfall gauges when there was more than one gauge in a grid cell the rainfall measured by the gauges was averaged as a reference when there was no gauge in the grid cell the rainfall measured by the gauges within 10 km was averaged as the reference sometimes there is also no gauge within 10 km therefore the grid cell is marked as a missing value ten rainfall gauges with less than 2 missing values were selected as the ground actual gauges from the 624 gauges and the gauge density was 308 gauges per 106 km2 the remaining gauges were only used to evaluate the rainfall estimations 2 multi source precipitation retrievals five satellite based precipitation products and three control forecast precipitation products based on numerical weather prediction nwp were selected for this case study as summarised in table 1 they were collected daily to maintain consistency with gauge based rainfall data the main reasons why these sources of prs were selected are as follows i the selected prs are available in near real time which is vital for disaster monitoring and early warning ii they perform worse than post real time gauge bias adjusted prs and are more likely to cause interference during the merging process iii estimation records were available for the wet seasons from 2013 to 2020 the selected prs used in the case study were two near real time nrt versions of the global satellite mapping of precipitation gsmap kubota et al 2020 kubota et al 2007 developed by the japan aerospace exploration agency jaxa i e gsmap v6 nrt gsmap n gsmap v6 gauge nrt gsmap gn two nrt versions of the integrated multi satellite retrievals for the global precipitation measurement imerg huffman et al 2019 huffman et al 2018 developed by the national aeronautics and space administration nasa i e merg v6 early imerg e imerg v6 late imerg l a nrt version of precipitation estimation from remotely sensed information using artificial neural networks persiann hong et al 2004 sorooshian et al 2000 developed by university of california at irvine uci i e persiann ccs three nwp based products respectively provided by european centre for medium range weather forecast ecmwf berner et al 2009 buizza et al 2008 japan meteorological agency jma yamaguchi and majumdar 2010 and national centers for environmental prediction ncep wei et al 2008 the nwp based products were selected on the base time of 00 00 utc with a one day lead time the spatial resolution of nwp based products was 0 5 when they were selected in tigge thorpex interactive grand global ensemble bougeault et al 2010 to obtain a consistent spatial resolution of prs before merging procedure prs were pre processed by resampling to 0 1 that is the three nwp based products were downscaled using the nearest neighbour method while the persiann ccs upscaled using bilinear interpolation the pre processing method aimed to fairly compare merged precipitation to individual pr and avoid any improvements in the products performance prior to the merging procedure baez villanueva et al 2020 4 results 4 1 overall performance comparison between vg rf and idw the vg and rf were forced by ten actual gauges and eight sources of prs while the corresponding ten actual gauges were employed to force the idw fig 4 shows the overall performances cc and rmse of vg rf idw and the eight individual prs the performances of rainfall estimations are better as cc approaches 1 and rmse approaches 0 in other words the blank between the blue and orange bars under the same label approaches the top of fig 4 therefore rainfall estimations can be divided into three echelons according to their overall performance vg rf and idw were the first echelons with significantly better performance than pure prs benefiting from the consideration of go gsmap n gsmap gn imerg e and imerg l are the second echelon with a cc from 0 39 to 0 41 and an rmse from 8 64 mm to 9 19 mm the performances of the timeliest prs namely persiann ccs ecmwf jma and ncep are the worst with a cc from 0 18 to 0 26 and an rmse from 9 24 mm to 10 67 mm among the first echelon rf and idw perform similarly with a cc of 0 56 and 0 54 respectively and an rmse of 6 40 mm and 6 38 mm respectively vg is better than rf and idw in terms of both cc and rmse the cc of vg was 0 60 which improved by 7 1 and 11 1 over that of rf and idw respectively the rmse of vg was 5 89 mm which improved by 8 0 and 7 7 compared to rf and idw respectively virtual gauges were added daily during the study period the number of virtual gauges varied from 1 to 27 averaging 10 82 virtual gauges for each day which is approximately equal to the number of actual gauges 4 2 performances under various rainfall fields fig 5 shows the daily performance comparisons between vg rf and idw using scatter diagrams where each scatter indicates the estimation performance of a day in fig 5 scatters above the 1 1 line indicate that vg a and rf b perform better than idw on those days in other words prs can provide helpful information on those days in contrast scatters below the 1 1 line indicate that prs disturb the estimation performance of rainfall fields on those days 19 of the scatters in fig 5 a and 41 of the scatters in fig 5 b are below the 1 1 line among the scatters below the 1 1 line the scatters in fig 5 b are farther away from the 1 1 line than those in fig 5 a this means that vg reduces the interference caused by pr uncertainties in addition using idwcc t as the horizontal axis can not only compare the daily performance of idw with the other two methods but also see how the performances of vg and rf vary with the ability of actual gauges to capture the heterogeneity of rainfall fields the trend line of the scatters in fig 5 a is significantly higher than the 1 1 line when idwcc t is less than 0 4 gradually approaching the 1 1 line as idwcc t grows the trend line in fig 5 b is above the 1 1 line at first however it is below the 1 1 line after idwcc t becomes larger than 0 5 this indicates that both vg and rf can provide helpful information on rainfall distribution on days when actual gauges miss important spatial information of rainfall fields however when actual gauges capture the spatial distribution of the rainfall fields well vg tends to be consistent with idw whereas rf performs much worse than idw therefore the proposed vg can supplement the important spatial information missed by actual gauges and reduce the interference caused by the uncertainties of prs fig 6 compares the true rainfall fields with those estimated by vg idw and rf on four typical days to demonstrate the effect of virtual gauges the days were selected due to their representativeness and various characteristics regarding rainfall intensity the days shown in fig 6 from a to d rained 196 0 mm 13 9 mm 28 8 mm and 39 7 mm at rainfall centres respectively vg worked well on the condition of each rainfall intensity on the day shown in fig 6 a the rainfall field estimated by vg was the most similar to the true rainfall field followed by idw rf performed the worst the better performance of idw compared with rf shows that the ten actual gauges can roughly capture the spatial distribution of rainfall fields on that day while utilising prs at each grid cell causes significant interference however actual gauges working alone cannot capture all the spatial information of rainfall fields the comparison between the idw and true rainfall fields show an overestimation of idw in the southern part of the study area by comparing vg and idw in fig 6 a it can be observed that the addition of minimal virtual gauges in the southern part of the study area effectively corrected the range of heavy rain on the day shown in fig 6 b the rainfall field estimated by vg was the most similar to the true rainfall field whereas idw underestimated the rainfall because the actual gauges missed the rainfall centre and rf overestimated the rainfall the comparison between vg and idw in fig 6 b shows that adding maximal virtual gauges in the southern part of the study area effectively supplemented the rainfall centre it is worth noting that vg does not always perform significantly better than idw and rf as shown by the scatter near the 1 1 line in fig 5 a taking the day shown in fig 6 c as an example when the rainfall field estimated by idw is already similar to the true one that is when actual gauges can capture the heterogeneity of rainfall fields well vg can perform similarly to idw by adding a few virtual gauges taking the day shown in fig 6 d as an example when the rainfall field estimated by rf is more similar to the true field vg can perform similarly to rf by adding more virtual gauges in summary vg can perform similarly to the better one in idw and rf the vg can balance the utilisation degree of prs by adjusting the number and locations of the virtual gauges 4 3 performances under various gauge densities the rainfall fields estimated by idw rf and vg were forced by gauges of various densities where vg and rf were additionally forced by eight sources of prs to investigate the influence of the actual gauge density we computed vg rf and idw using 2 3 4 6 10 14 22 and 30 actual gauges accounting for 62 92 123 185 308 431 677 and 923 gauges per 106 km2 respectively fig 7 demonstrates that the overall performances cc and rmse of idw rf and vg varied with gauge density in which the darker colour indicates better performance among idw rf and vg for a certain gauge density in terms of both cc and rmse the rainfall fields estimated by vg performed best for each gauge density rf performs better than idw when forced by sparse gauges and worse than idw when the gauge density exceeds the marginal threshold of approximately 300 gauges per 106 km2 in the case study the marginal gauge density varies with the region and period zhang et al 2021 therefore vg is more dependable than the two basic methods regardless of gauge density fig 8 demonstrates that idw rf and vg performance differences varied with gauge densities as the blue scatters and dotted curves in fig 8 shown vg improves slightly over rf when forced by sparse gauges while performing gradually better than rf as the gauge density increases when gauge density approaches zero the performance improvements of vg over rf approach zero too in other words vg tends to perform equally to rf as gauge density tends to be zero as the orange scatters and dotted curves in fig 8 show vg clearly improves over idw when forced by sparse gauges while it increasingly improves slightly than idw as the gauge density increases vg tends to perform equally to idw when forced by very dense gauges the grey scatters and dotted curves are below the orange ones in fig 8 a and above them in fig 8 b regardless of the gauge density 4 4 influences of precipitation retrievals selection each source of prs shows varying performance and has their advantages and disadvantages therefore the number and quality of the selected prs may influence the merging performance of vg we forced vg using various pr combinations and ten actual gauges consistent with section 4 1 to investigate the influence of pr selection because the vg requires at least three sources of prs to recommend virtual gauges there are 219 schemes of pr combinations to force the vg s 3 8 c 8 s 219 using up to eight sources of prs where s indicates the number of input prs fig 9 demonstrates the overall performances cc and rmse of the 219 input schemes of pr combinations using a scatter diagram where each scatter indicates the performance of an input scheme the estimation performance improves as the scatter approaches the bottom right corner of fig 9 the colours distinguish the number of prs in the input scheme under the condition that the numbers of dark blue and grey scatters in fig 9 are equal c 8 3 c 8 5 56 dark blue scatters are more scattered than grey scatters therefore the uncertainty of the results can be reduced by using more sources of prs when using a certain number of prs to force vg three sources of prs for example cc and rmse of the worst input scheme are 0 54 and 6 17 respectively while cc and rmse of the best input scheme are 0 60 and 5 83 respectively therefore these large differences are caused by the quality of the different prs it is noting that vg forced by the worst input scheme performs similarly with idw cc 0 54 rmse 6 38 so the detailed differences between vg forced by the worst input scheme and it forced by a better input scheme can be seen from the comparison between idw and vg in section 4 2 fig 10 demonstrates pr contribution to the performance of vg varying with pr quality where the pr contribution is quantified by shapley value and the dotted lines indicate the trendlines of scatters the shapley values increase as the cc of prs increase and decrease as the rmse of prs increase which indicates the larger contribution of the more accurate prs to the vg performances the scatters in fig 10 a are more correlated than those in fig 10 b indicating that the correlation between pr contribution and cc is more significant than that between pr contribution and rmse therefore cc of prs is more noteworthy than rmse of prs when forcing vg 5 discussion apart from the grid by grid merging methods another type of merging method considers the spatial distribution of prs as a whole by inputting a source of rainfall field like an image in a time step pan et al 2019 and using convolution calculation examples are the long short term memory wu et al 2020 convolutional neural network xue et al 2021 and generative adversarial network wang et al 2021 spatial techniques mine the spatial distribution features of prs using multilayer convolution calculation and find the correlations automatically as a result they are black box models a larger size of samples with fewer uncertainties can reduce the difficulties associated with deep learning in general the sample size of spatial techniques is approximately 20 times larger than that of grid by grid techniques biau and scornet 2016 the proposed vg is a white box model and can consider the spatial distribution of prs therefore it can estimate rainfall fields using smaller samples with relatively larger uncertainties after the comparison between prs and go in section 1 we obtained two merging principles i correct rainfall estimations of prs using go as references and ii supplement the spatial distribution information of rainfall fields interpolated by go using prs the two crucial procedures of the proposed vg correspond to the two merging principles i to determine the locations of virtual gauges from the spatial comparison between the rainfall fields estimated by multiple prs and go and ii to estimate rainfall located at virtual gauges by grid by grid merging methods the aim of the former procedure is to investigate whether the spatial distributions of the rainfall fields interpolated by go need to be supplemented the aim of the latter procedure is to correct the rainfall estimations of the prs using go as a reference therefore vg follows both principles of the merging procedures mentioned in section 1 grid by grid merging techniques perform worse than spatial interpolation using only go when ground gauges capture the spatial distribution of rainfall fields better in contrast they perform better when ground gauges capture the spatial distribution of rainfall fields worse to solve this problem the proposed vg can supplement the important spatial information missed by actual gauges and reduce the interference caused by the uncertainties of prs the following are the efforts we made in vg to reduce the interference caused by the uncertainties of prs 1 prs are utilised only where the important spatial information needs to be supplemented 2 only the locations of pr extreme values in a sub area are considered because prs are skilled in reflecting the locations of maximum and minimum rainfall and 3 a grid cell can be regarded as a virtual gauge only when it is recommended by more than half of the sources of prs after adding virtual gauges to the actual gauges a spatial interpolation was conducted therefore vg integrates spatial interpolation using only go and the methods that merge prs and go 6 conclusions this paper proposes a framework vg for merging rainfall estimation of multi source prs and go based on a novel concept namely the virtual gauge in the proposed vg prs can perform positively under various densities of gauge networks and various the ability of gauge networks to capture the heterogeneity of rainfall fields to illustrate this vg was assessed over a region where almost all 0 1 grid cells contained a ground gauge the primary conclusions are summarised as follows 1 in terms of overall performance during the study period vg performed better than its basic methods namely idw and rf and inputting prs namely gsmap n gsmap gn imerg e imerg l persiann ccs ecmwf jma and ncep when ten gauges were selected as the inputs counting for 308 gauges per 106 km2 2 the daily performance comparison showed that the proposed vg could supplement the important spatial information of rainfall fields when the gauge network cannot capture the rainfall field well however it can reduce the interference caused by the uncertainties of prs when the gauge network captures the rainfall fields well under the condition of various rainfall intensities and rainfall distributions 3 a comparison of idw rf and vg forced by gauges of various densities showed that vg always performed the best regardless of the gauge density vg performed much better than rf under the condition of dense gauges whereas it performed much better than idw under the condition of sparse gauges vg tended to perform equally with rf when the gauge density tended to be zero while it tended to perform equally with idw when forced by very dense gauges 4 vg forced by various pr combinations showed the influence of the number and quality of prs on the merging performance of vg on the one hand the uncertainty of performance can be reduced by using more sources of prs however there is a larger contribution to the vg performance of the more accurate prs especially for those with accurate spatial distributions it is recommended that vg be forced using more sources of prs with better spatial performance overall vg performed better and was more stable than the basic methods vg is a framework that can integrate the advantages of existing grid by grid merging and spatial interpolation methods by adjusting the number and location of virtual gauges to balance the degree of pr utilisation as always we invite discussions and collaborations on these and related aspects credit authorship contribution statement yanhong dou conceptualization methodology validation formal analysis writing original draft visualization lei ye funding acquisition resources supervision writing review editing jiaqi ai software data curation chi zhang funding acquisition resources project administration huicheng zhou funding acquisition resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china nos 52179006 51925902 u2240204 and the fundamental research funds for the central universities no dut22jc28 the precipitation data can be obtained from https disc gsfc nasa gov https www jaxa jp https chrsdata eng uci edu and https apps ecmwf int datasets data tigge 
