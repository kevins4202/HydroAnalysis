index,text
25465,the foundation of a healthy marine environment is central to the ecosystem based management approach and is recommended for achieving sustainable development outcomes in marine spatial planning algoa bay south africa features a metropolitan area protected natural area and hosts a range of uses that are closely interconnected with the health of the marine system future trajectories of marine uses and related marine sustainability goals are expected to develop according to shifting needs of various sectors operating in the bay we therefore leverage the application of system dynamics modelling to explore these trends and the underlying feedback effects between marine uses and the marine environment by developing the algoa marine systems analysis tool the model is used to investigate sustainability outcomes under alternative scenarios over time and demonstrates how multiple cross sectoral management interventions can result in system wide change and can be used to achieve desirable marine sustainability goals in the long term graphical abstract image 1 keywords blue economy integrated ocean management systems analysis system dynamics modelling social ecological system abbreviations iom integrated ocean management sdm system dynamics modelling ses social ecological system sd system dynamics data availability data used in this study can be found in indicated references and supplementary materials 1 introduction humans are closely connected with the ocean 1 1 the terms ocean marine and sea are often used interchangeably in this study the term ocean may be used in a larger or global context whereas marine will be used at a more regional scale both directly and indirectly and the value obtained in terms of natural social and economic benefits is immense allison et al 2020 in response to the growing needs from an increasing human population marine uses and activities are projected to undergo a significant change in coming decades dnv 2021 jouffray et al 2020 oecd 2016 though these trends present socio economic opportunity as recognised in worldwide oceans economy directives utilisation of the ocean space does not come without limitations owing to the increasing demands on ocean space and resources humans are further encroaching on its natural limits through observed trends of biodiversity loss marine pollution ocean warming and acidification halpern et al 2017 hameed et al 2017 mccauley et al 2015 these escalating pressures are partly a result of inadequacies or failure in ocean governance and the lack of successful implementation of ecosystem based eb management approaches lombard et al 2019 in response integrated ocean management approaches particularly an ecosystem based eb marine spatial planning msp process has been endorsed globally borja et al 2016 ehler 2020 msp is a process that aims to bring together multiple users of the ocean space to make informed and coordinated decisions about the management of marine uses and resources ehler and douvere 2009 jones et al 2016 msp has therefore become increasingly popular among nations including in south africa to support eb management domínguez tejo et al 2016 douvere 2008 and to facilitate sustainable development of the oceans economy ioc unesco 2017 dea 2017a a large array of decision support tools have been developed to support msp gee et al 2019 janβen et al 2019 pınarbaşı et al 2017 existing tools have been developed for different functions with some explicitly developed to assist in spatial mapping e g sea sketch mcclintock and gordon 2015 spatial optimisation e g marxan ball et al 2009 simfish bartelings et al 2015 cumulative impact assessments e g halpern et al 2008 symphony hammar et al 2020 invest guerry et al 2012 or to investigate user interactions e g seanergy bonnevie et al 2020 however there remains a niche for tools to evaluate the temporal dynamics of human activities and to evaluate the underlying cause and effect relationships within complex social ecological marine systems aswani et al 2017 gissi et al 2019 the relationship between human dependence and interference with the marine environment can be non linear affected by time delays and dominated by feedback loops across social economic and environmental dimensions pongsiri et al 2017 there is therefore a need to understand the dynamics and feedback behaviour driving the change between current and increasing human uses in the marine space to support msp moreover key characteristics of an eb management approach to msp involves describing parts systems environments and their interactions and hence recognising system dynamics principles ehler and douvere 2007 agardy et al 2011 in this regard the applicability of systems dynamics sd modelling is leveraged as an appropriate tool to support msp sd modelling involves studying and managing complex systems over time forrester 1961 and is widely applied in social ecological research e g kelly et al 2013 elsawah et al 2017 drechsler 2020 with existing applications in the field of marine science e g hopkins et al 2012 boumans et al 2015 to date the multiscale integrated model for ecosystem services framework is well known for its application in the massachusetts marine model which was developed as a collaborative msp tool altman et al 2014 boumans et al 2015 other studies have also proved the application and strengths of system dynamics to msp some through successful demonstrations of participatory modelling and decision making processes e g videira et al 2012 vugteveen et al 2015 this study reports on the development of a system dynamics model the algoa marine systems analysis tool algoamsat and demonstrates the procedure of how sd can be applied to support msp in a densely utilised and local marine context such as in algoa bay south africa the algoamsat specifically aims to explore the temporal change of marine activities and trade offs between marine sustainability goals under different scenarios in algoa bay as a supporting msp framework algoamsat provides a holistic cross sectoral overview of the causal interactions between marine uses and the marine environment as a simulation model it provides a platform for scenario and policy analyses in relation to sustainable management of the bay together through the use of the interactive model interface it can help to facilitate collaborative stakeholder engagement in a msp process section two provides a description of the model boundary section three describes the method and model development process and section four describes the model structure section five presents the model results with the discussion and conclusions provided in section six 2 model boundary the algoamsat was developed as a management framework and simulation model that applies system dynamics sd modelling to facilitate and support msp sd models incorporate temporal dynamics and thereby can support msp by evaluating changes in marine uses as well as interconnections between marine uses and between marine uses and the health of the marine environment over time the model boundaries have therefore been delineated in line with this research goal the broad model boundary of algoamsat is comprised of seven sub models fig 1 five of these represent the selected marine uses namely shipping mariculture fishing tourism and recreation and land discharge activities the other two sub models integrate the outputs from each marine use in terms of their ecological and socio economic marine sustainability outputs respectively measured in terms of marine health and marine wealth and labour fig 1 these marine uses were particularly selected owing to their socio economic value and the level of impact or reliance on marine health among other criteria including political relevance public concern and recent media coverage the algoamsat was developed for the marine space delineated by the algoa bay msp project study boundary ranging from the shoreline to the 12 nautical miles territorial limit as defined in the maritime zones act rsa 1994 and from the western limit of the sardinia bay marine protected area mpa to the eastern edge of cannon rocks dorrington et al 2018 within the land boundaries of the nelson mandela bay nmb municipality fig 2 3 methods 3 1 model development process the modelling process adopted in this study evolved through a series of iterative steps during the first three steps of the modelling process marine uses were selected in relation to the model boundary management problems and marine use conflicts were identified and indicative behaviour over time graphs for key problem variables were defined these steps were carried out through extensive literature research and through consultation with experts and stakeholders through a sub project hosted in parallel to the study to support model development see section 3 1 2 on the algoa bay collaborative dynamic modelling process thereafter cause and effect dynamics for different marine uses were mapped according to the information gathered on marine use interactions in the bay which entailed adopting a circular thinking and structuring approach such as adopted in the driver pressure state impact response dpsir atkins et al 2011 or the adapted dapsi w r m framework 2 2 the dapsi w r m framework shows that drivers of basic human needs require activities which lead to pressures these pressures result in change in the state of the system which leads to impacts on human welfare those in turn require responses as management measures this is merely an extension of the original dpsir driver pressure state impact response framework atkins et al 2011 elliott et al 2017 elliott et al 2017 next the quantitative modelling stage entailed iteratively parameterising simulating and revising the model structure until the simulation captured the dynamic problem behaviour this was followed by model verification and validation section 3 1 3 3 1 1 model setup system dynamics is the primary modelling method adopted in this study model development was conducted using stella architect software isee systems version 2 1 1 richmond and peterson 2001 on a windows system the software enables the configuration of models using a range of functions including stocks flows and converter variables stock variables s capture the accumulation of information through mathematical integration over time t eq 1 whereas flow variables capture the rate of change or simply inputs and outputs into the stock variable through a differential function eq 2 sterman 2010 1 s t o c k t t 0 t i n f l o w s o u t f l o w s d s s t o c k t 0 2 d s t o c k d t i n f l o w t o u t f l o w t the selected time horizon and simulation period for the model is 40 years from 2010 to 2050 thus allowing model behaviour during the first 10 years to be compared with reference behaviour from 2010 to 2020 and to allow for scenario planning and policy testing from 2020 onwards i e the next 30 years an integration timestep i e delta time 1 12 was deemed appropriate for the simulation resulting in a consistent level of numerical accuracy with a total of 480 time steps 40 years 12 time steps year falling well within the recommended range of less than 1000 time steps ford 2009 the euler first order integration method was selected over runge kutta two or four euler was a suitable method given the combination of functions in the model and the model content whereas a higher order integration method may be suitable for models with many timesteps and higher turnover dynamics ford 2009 the model was initialised with base case parameters and assumptions table a2 and supplementary materials 3 1 2 stakeholder engagement the involvement of stakeholders has been recognised to be key to the success of modelling complex environmental problems and has resulted in the emergence of powerful participatory modelling techniques krueger et al 2012 voinov and bousquet 2010 in this study stakeholder engagement during the early stages of model development primarily assisted in integrating knowledge on different sectors activities impacts and planning visions into the model framework the algoa bay collaborative dynamic modelling ab codym process adapted from clifford holmes et al 2016 progressed through two phases phase one was initiated through stakeholder meetings n 34 with individuals from a range of institutions representing the marine uses in the model fig 3 to capture expert opinion as well as a range of perspectives in the model stakeholder representatives from academia n 13 businesses n 10 civil society n 5 as well as the essential end users including municipal and government officials n 6 were involved in the process in phase two of the process stakeholder engagement involved a pilot multi sector workshop fig 3 the workshop hosted 13 participants consisting of members of the algoa bay msp project and external researchers during the pilot workshop the model structure was demonstrated and discussed thereafter workshop participants played a scenario planning game using the model interface wherein participants were categorised into the different marine sectors and required to represent or role play a stakeholder or manager from one of the five marine sectors the pilot workshop was mainly intended to test and receive feedback on the preliminary model structure and to get an idea of how the model once completed can be applied in a multi sectoral stakeholder setting to support collaborative engagement and planning in msp 3 1 3 model validation the stakeholder engagement process served as a form of verification of the qualitative model structure but to further verify the quantitative model structure and behaviour a series of validation tests were performed these included direct structure tests ensuring dimensional consistency among the units and ensuring that model behaviour remained physically realistic during univariate sensitivity analysis where data could be sourced from literature or from stakeholders it was plotted against the simulated results in this instance validation was intended to measure reproducibility of model behaviour against observed data with emphasis on pattern or trend prediction rather than point prediction barlas 1996 a multivariate sensitivity analysis was also used to evaluate the model behaviour under a combination of values of multiple uncertain parameters razavi et al 2021 saltelli et al 2019 variables included in the sensitivity analyses included the exogenous variables that were estimated to a degree of uncertainty and were hypothesised to have an influence on the model dynamics ford and flynn 2005 turner 2020 parameter values known to have a higher degree of certainty i e values identified in literature or values that were substantiated by data were varied by 50 of the baseline value ford 2009 whereas parameter values known to have a lower degree of certainty i e values for which physical data do not exist or estimates that were provided by stakeholders were allocated a sampling range twice as large as the baseline value or a sampling range between maximum bounds of likelihood e g 0 1 ford 2009 sterman 2010 the latin hypercube sampling method which aims to obtain an even spread in the sample values across the sampling interval was chosen and the analyses ran for 50 model simulations as recommended in ford 2009 lastly to distinguish the degree of influence of the input exogenous parameters on the key variables of interest over the course of the simulation the statistical screening approach developed in ford and flynn 2005 and applied in taylor et al 2010 was conducted this involved exporting the model data to microsoft excel and calculating the spearman correlation coefficient between the input and output variable of interest at every model timestep under the condition of a monotonic and non normal data distribution this assisted in identifying which input variables held more leverage in the system and hence to apply in policy and scenario planning as well as include in the model interface 4 model structure fig 4 shows the condensed holistic overview of the model structure representing the interconnections among the key variables in each sub model that capture the temporal trends in each marine use from a marine sustainability perspective i e trends that are important in order to plan for marine use and marine environment threats and conflicts as well as opportunities and benefits for the marine economy detailed diagrams for each sub model structure are presented in appendix a figure a1 figure a7 a comprehensive overview of the model parameters and sources of information is provided in tables a1 and a2 with information on model equations provided in supplementary materials note that assumptions have been made in order to generalise and simplify relevant dynamics in each marine use and that dynamics represented are not for individual businesses or companies but have been aggregated to represent sectors as a whole in the ports and shipping sub model the key trends are captured in terms of the annual number of vessel visits captured as a stock variable fig 4 in land discharge the main trends captured are the volumes of wastewater and household litter discharged directly and indirectly into the marine environment both primarily driven by the coastal population stock and tourists in the bay fig 4 trends in mariculture include annual harvest yields of bivalves consisting of oysters magallana gigas and mussels mytilus galloprovincialis or perna perna and future finfish stocks consisting of yellowtail seriola lalandi or dusky kob argyrosomus japonicas that change through planting and farm investment fig 4 note that these are the species that are currently or are proposed to be farmed in the bay massie et al 2019 the fishing sub model specifically the sardine sardinops sagax fishery investigates trends in the fish biomass stock in relation to changes in catch effort fig 4 lastly trends in tourism and recreation focus on the annual number of tourists visiting the bay particularly tourists attracted to the bay for coastal and marine activities fig 4 in terms of marine sustainability outputs in terms of marine health wealth and labour are measured fig 4 the model additionally investigates interconnections between marine uses and how changing trends in one may impact on another links between the marine uses have been made explicit where there is enough scientific evidence and valid assumptions from research and stakeholders there is a direct link between the land discharge and mariculture sub model as well as to the tourism and recreation sub model fig 4 this link is connected through water quality where changes in volumes of wastewater affects mariculture harvest yields and the number of tourists engaging in coastal and marine activities other direct links in the model exist between shipping and tourism and between fishing and tourism where more vessels can decrease the abundance of iconic marine species due to increased noise and disturbance and similarly more fishing can decrease food availability although different marine species have been shown to react differently to noise pollution erbe et al 2019 the general assumption is that a higher level of cumulative impacts is likely to negatively influence the abundance of marine species that are valuable to marine tourism in the bay iconic marine species in the model include the african penguin spheniscus demersus the white shark carcharodon carcharias and humpback megaptera novaeangliae and southern right whales eubalaena australis furthermore there is a link between fishing and mariculture where a lower sardine biomass increases mariculture through a higher investment and vice versa fig 4 though other conflicts may exist between the marine uses these have been omitted owing to insufficient evidence and the need for ongoing research to verify relationships between them however it is important to note that indirect links between marine uses may still be captured through changes in the marine health of the bay which is measured following the cumulative impacts formulation from halpern et al 2008 fig 4 the main causal dynamics are formulated as follows the more vessels in the bay the higher the associated environmental risks and the lower the marine health at the same time the more vessels in the bay the higher the socio economic potential in terms of marine wealth and labour in terms of land discharge higher wastewater effluent and litter loads and decrease marine health but increase labour requirements to manage increasing loads and associated environmental impacts fig 4 note that no outputs from the land discharge sub model are connected to marine wealth as the sector is not driven for the goal of profit but rather to provide a public service in the mariculture sub model higher harvest yields can increase socio economic value but decrease marine health through increased environmental risks conversely lower marine health as a result of cumulative impacts from marine uses can decrease harvest yields and hence the socio economic value of mariculture b1 productive mariculture loop in the fishing sub model a higher biomass corresponds to a higher socio economic value in terms of profits labour and marine health with the opposite effects for a low biomass comparably a lower marine health can decrease fish biomass and hence its socio economic value and health status b2 sustainable fishing loop fig 4 lastly in the tourism and recreation sub model more tourists in the bay translates into higher socio economic value with a larger influence on marine health in terms of the contribution of tourists to land discharge fig 4 the growth of tourism is however also affected by marine health through changes in coastal and marine tourism attractiveness b3 tourism aesthetics loop tourism dynamics can be formulated through different structures with varying assumptions but should match the problem description in the respective case study and local context in algoa bay there is a need to increase coastal and marine tourism as opposed to in other regions of the world where unsustainable tourism with respect to overcrowding is more problematic kapmeier and gonçalves 2018 pizzitutti et al 2017 the three main balancing loops incorporated into the holistic structure of the model are therefore the mariculture b1 fishing b2 and tourism b2 loops fig 4 from an eb perspective these uses are dependent on a healthy marine system to sustain their marine activities in the bay the shipping and land discharge sub models however do not entail feedback to marine health because these uses are not directly dependent on a healthy marine environment to operate in the bay potential feedback effects that can create a balancing effect on the shipping and land discharge sectors exist if it becomes more costly to operate owing to stricter environmental regulations because of deteriorating marine health forcing stricter environmental policies in those sectors though these feedbacks are not built into the model structure they can be considered in management interventions to further endogenise model dynamics indirect links exist between the economic value of the uses in terms of marine wealth and the growth of marine activities through reinforcing feedback effects that are induced through changes in profitability r1 shipping growth loop r2 tourism growth loop r3 fishing growth loop r4 mariculture growth loop fig 4 additional details on the feedback processes in each marine use are shown in figure a1 figure a7 5 model results 5 1 baseline results the baseline simulation shows the behaviour over time of the key model variables under initial conditions owing to the extent of the model boundary and the number of model variables only the results of the primary variables of interest in each marine use are presented table 1 results for the secondary model variables are presented in appendix a figure a8 overall the model results show increasing growth in select marine activities with varying rates of change in response to varying driving dynamics and feedback effects in each marine use fig 5 and a1 a7 5 1 1 marine sustainability outputs fig 5 a c presents the baseline results of the outputs from the marine uses in terms of marine health marine wealth a measurement of the cumulative economic value gained directly from marine uses and marine labour as expected marine wealth is increasing over the course of the simulation starting at r3 7 billion in 2010 and growing to approximately r112 billion in 2050 fig 5b as a result of the accumulation of annual profits obtained from the marine activities altogether the contribution of the marine economy towards the local economy in terms of the gross value added increases from approximately 5 4 in 2010 to 9 4 in 2030 closing on 7 2 in 2050 similarly marine labour shows an increasing trend with an initial model value of 70 000 persons in 2010 and reaching 140 000 persons in 2050 fig 5c as a result of the cumulative pressures from the different marine activities the level of marine health in reference to the natural state of the marine system is decreasing over time starting at an initial value of 0 80 and reaching a low of 0 20 in 2050 fig 5a model outputs such as the variable marine health that are highly subjective to stakeholder opinion and hence contain a higher degree of uncertainty are further subject to sensitivity analysis see section 5 2 5 1 2 ports and shipping shipping activity in the bay is projected to increase in terms of annual recordings of vessel visits from approximately 700 vessels in 2010 7700 vessel visits in 2050 fig 5d the increasing trend is a result of the complex interplay between a historical growth fraction and interactions between the attractiveness of port services port efficiency and profitability of the sector profitability in turn determines the rate of change of port capacity which subject to time delays essentially limits the growth in vessel arrivals through marine traffic effects in conjunction it is projected that profitability and labour of the sector is expected to increase over the 40 year period in relation to the number of vessel visits profits from shipping show an increasing trend from approximately r860 million in 2010 to r8 billion in 2050 with total labour direct and indirect increasing from 700 persons in 2010 to 6000 persons in 2050 consequently trends in marine environmental risk subject to the level of traffic in the bay are projected to increase thus causing marine health to decrease from the reference point of 0 8 dmnl in 2010 to approximately 0 3 in 2050 5 1 3 land discharge annual trends in land discharge loads specifically wastewater effluent constituent nutrient loads and litter loads show an increasing rate of change over the course of the 40 year model period annual nutrient loads are projected to increase from 1 1 million kg year for dissolved inorganic nitrate din and 242 000 kg year for dissolved inorganic phosphate dip to 4 6 million kg years and 1 4 million kg years in 2050 respectively fig 5e similarly trends in household waste production are projected to increase from approximately 280 000 tons year in 2010 to 1 4 million tons per year in 2050 fig 5f this increasing trend is predominantly driven through an increase in the coastal population with a small contribution 2 5 from tourists in the bay though wastewater treatment infrastructure is expected to increase in response to the growing sanitation demand unexpected delays through planning financing maintenance or construction can result in periods where demand exceeds municipal capacity furthermore irrespective of the assimilation capacity of the receiving marine environment the continuous and increasing discharge of excess wastewater nutrients and litter can result in cumulative impacts on marine water quality and ultimately decrease marine health from approximately 0 85 in 2010 to 0 36 in 2050 costs incurred for wastewater treatment associated with the need for upgrades and treatment of wastewater are equated to r 280 million year in 2010 steeply increasing to r1 billion year towards 2012 and continuing to r1 5 billion year in 2050 for waste or refuse that equates to roughly r255 million year in 2010 and doubling to r466 million year in 2050 in terms of labour potential the growth in the land discharge sector assuming no technological innovation can double labour requirements to approximately 5000 workers by 2050 5 1 4 mariculture at a local scale mariculture trends are projected to increase fig 5g though the annual production and harvests can be determined only through observations of past trends and estimates of future investment the model shows that oyster production and harvests are to increase subject to zoning approvals from 100 tons year in 2020 to approximately 4800 tons year in 2050 on 600 hectares ha fig 5g this is however highly dependent on the fraction of profits obtained from annual harvests that are reinvested into the farm among other variables including the health of the marine environment and the seafood demand similar dynamics apply to mussel and finfish farming mussel production and harvesting is projected to increase from 0 tons year during 2010 2020 to approximately 129 tons year on 20 ha by 2050 fig 5g in finfish farming assuming the initial planting investment is 70 ha finfish harvests are projected to reach 730 tons year by 2025 fig 5g the projected estimates of mariculture production in the bay contain a high degree of uncertainty owing to the uncertainty in initial farm investments which are further explored in scenario analysis nevertheless consistent with these trends total mariculture profits are projected to increase from r26 million in 2010 obtained only through oyster farming to 340 million in 2050 across all farming types similarly total direct labour in the sea based mariculture sector is projected to increase from approximately 30 persons in 2010 to approximately 429 persons in 2050 although farm production is increasing problems associated to increasing land discharge are congruently expected to increase which can lead to additional losses through harvest sales and ultimately profitability and farm reinvestment concurrently marine health shows a decreasing trend from the level of 0 98 in 2010 to approximately 0 65 in 2050 due to pressures incurred through the expansion of farming induced through estimated losses in the benthic habitat structure and nutrient loading from finfish production positive effects from bivalve mariculture through water purification was not included in this study as these effects are highly dependent on the density of farming and surrounding hydrographic conditions but even so these effects are suggested to be minor turner et al 2019 5 1 5 fishing the baseline simulation shows a decreasing trend in sardine biomass from an initial value of 100 000 tons in 2010 and ending on 24 000 tons in 2050 fig 5h the declining sardine biomass is attributed to a combination of changes in catch and the health of the marine environment a decrease in sardine biomass further effects the catch per boat with additional effects on the annual catch catch value and hence revenues and profits as a result fishing profits are declining from around r20 million in 2010 2020 to approximately r9 million 2050 in terms of labour potential total labour including direct and indirect estimates changes in relation to the number of boats fishing with eight boats fishing at the start of the simulation and ending on 15 boats this translates into approximately a total of 480 persons employed in 2010 with the largest contribution provided through indirect i e secondary and tertiary employment and 891 persons in 2050 marine health in the fishing sub model is measured through the ratio of the sardine biomass normalised against the sustainable biomass formulated as half of the carrying capacity mahata et al 2020 therefore according to this calculation and under baseline model values marine health from the perspective of the fishing sector is decreasing from the initial starting point of 0 95 and levelling off at around 0 47 in 2050 however as the sardine population continues to decline fishing becomes more expensive such that the fishing fleet decreases hence decreasing the level of catch which in turn causes the sardine biomass to oscillate around a very low biomass of 20 000 tons the declining sardine biomass has knock on effects on other uses in the bay such as on the tourism and recreation sector whereby the abundance of marine iconic species is sensitive to the trend in sardine biomass 5 1 6 tourism and recreation trends in tourism are increasing in algoa bay with foreign tourism increasing at a higher rate than domestic tourism fig 5i fig 5i shows how the number of foreign domestic tourists increased from 180 000 3 million visitors in 2010 to approximately 1 1 million 5 3 million in 2050 growth in the number of tourists is driven through the effects from a combination of variables with the dominating effect emanating from the normal annual growth fraction which is subject to changes in coastal and marine attractiveness under baseline values the trend in coastal and marine attractiveness is declining over the course of the simulation starting at an initial value of 1 in 2010 and ending on 0 25 in 2050 due to changes in the level of beach recreation and marine wildlife tours which in turn are decreasing due to knock on effects from other marine uses and a decline in marine health moreover the decrease in coastal and marine tourism attractiveness does not only influence the annual growth fraction of tourists but similarly influences the duration of time spent in the bay this consequently affects tourism revenues obtained from accommodation and coastal and marine activities despite the increase in the number of tourists the trend in total profits steadily increases from approximately r3 billion year in 2010 until r4 5 billion year in 2031 after which it declines to r3 6 billion year in 2050 owing to changes in the duration of time spent in the bay lastly the changing trend in the number of tourists has a positive effect on employment where tourism labour is projected to double towards the end of the simulation from 60 000 persons in 2010 to 120 000 persons in 2050 5 2 validation results while the baseline simulation captures a conservative perspective of the model results subject to model assumptions and initial values a sensitivity analysis provides a broader perspective on the possible growth trajectories under alternative model parameter values and combinations thereof pianosi et al 2016 saltelli et al 2019 the model parameters included in the multivariate sensitivity analyses and respective sampling ranges are shown in table a3 fig 6 shows the results obtained from the multivariate sensitivity analyses for the primary model outputs obtained from 50 model simulations with the spread of uncertainty depicted by the confidence intervals overall the results trace out a pattern consistent to the baseline simulation and in line with the mean result across 50 runs which show increasing trends in marine uses apart from a decreasing trend observed for the variables of sardine biomass and marine health fig 6a and h the spread of the results for variables also highlight that more uncertainty is evident in the results of marine health mariculture harvesting and the sardine biomass owing to underlying uncertainties in the initial input model values fig 6a g and h variations in the parameter values of the model inputs do not appear to alter the fundamental pattern of behaviour but rather the rate of change at which marine activities are projected to grow with observed knock on effects on the marine sustainability outputs table a4 presents the statistical screening results measured in terms of the correlation coefficient and polarity of the model input against the primary variables of interest ranked according to the five highest correlated model inputs the correlation coefficient presented reports the correlation coefficients between the input and output variables for the beginning of the projected model simulation 2020 and at the end of the projected model simulation 2050 the statistical screening results therefore show which input variables are the most influential on model behaviour during those time periods table a4 the results show that the primary variables of interest in the model are not only influenced through direct effects within each sector but are additionally influenced through indirect cross sectoral feedback effects with some effects being more influential than others for the most part the input variables that have a positive effect on the growth of marine activities and towards marine wealth have a negative effect on marine health and vice versa table a4 5 3 scenario analysis and model interface scenario planning was used in this study to explore the temporal dynamics and underlying uncertainty through asking what if questions surrounding marine uses in the bay scenario planning has been shown to be a particularly useful tool in integrated management processes claassen et al 2013 wwf sa and bcg 2017 and is hence suitable to support msp scenarios are not forecasts or predictions but rather projections to explore how current situations in this case the expansion and growth of marine uses in the bay might evolve in the next couple of decades to reflect alternative visions for the bay a scenario matrix was developed by the research team to show how varying levels of ocean governance ranging from poor to good governance strategies in msp can result in different scenarios with trade offs between ecological and socio economic marine sustainability goals fig 7 though it would have been ideal to define and test the scenarios with stakeholders limited time and project resources in addition to the pandemic and broader project concerns around stakeholder fatigue prevented additional stakeholder engagement during this stage of the analysis the marine sustainability matrix is comprised of three axes of uncertainty the x axis to represent the level of marine health and the y axis to captures the level of marine wealth and labour fig 7 trade offs among these axes are further dependent on the level of ocean governance represented by the z axis or diagonal axis fig 7 the four most extreme exploratory scenarios are defined by four narratives namely the sea of plenty eerie bay dead man s chest and deserted paradise and compared against the baseline or business as usual bau scenario here the level of ocean governance can be interpreted in relation to ocean governance characteristics that have been identified and recommended by ehler and douvere 2009 flannery et al 2018 jones et al 2016 and kelly et al 2019 and the level of desirability is measured against the outcome of best practice msp where both socio economic and ecological objectives are met across sectors more specifically the scenarios are defined as follows 5 3 1 base case or business as usual scenario the bau scenario assumes a continuation of past trends and no changes to current ocean governance strategies but simply a continuation of the growth in marine activities under standard management practices 5 3 2 sea of plenty scenario the sea of plenty scenario fig 7 the most desirable scenario reflects a sustainable ocean governance strategy that is built upon an integrated and eb approach moreover precautionary governance ensures that the growth of marine activities is managed within marine environmental limits to sustain the provision of socio economic benefits obtained from the marine economy in the long term 5 3 3 dead man s chest scenario the dead man s chest scenario fig 7 the least desirable scenario reflects a poor marine economy with low employment and investment potential debilitated by an unhealthy marine environment this is assumed to be because of unsustainable governance characterised by weak enforcement and uncoordinated and siloed planning 5 3 4 eerie bay scenario the eerie bay scenario fig 7 reflects a sub desirable future where marine health is deteriorating at the expense of expedient sectoral growth that is driven through short term political goals thus through exclusive and fragmented ocean governance only the dominant uses benefit resulting in unintended consequences on the marine environment and on development objectives of other uses in the bay 5 3 5 deserted paradise scenario the deserted paradise scenario fig 7 similarly represents a sub desirable future assuming that low levels of innovation ambition or capacity development in ocean governance may result in a thriving marine environment that is under valued in its potential towards growing the marine economy and providing socio economic benefits 5 4 scenario results table a2 presents the model input variables and parameter values selected for each scenario model inputs that have been applied in scenario planning include the input variables that were identified to be most influential through the statistical screening approach section 5 2 2 which may be adapted through changes in management and policy interventions note that for the purpose of the scenario analysis in the model the changes in the model input parameters are effective only from 2020 onwards results of the primary variables of interest are shown in fig 8 as defined the sea of plenty scenario is hypothesised to result in a prosperous marine economy built upon a healthy marine environment according to fig 8 a this scenario shows that marine health is slowly increasing above trends in marine health over the previous decade this result is owing to the reduction in the marine health recovery time owing to the implementation of the marine protected area which subsequently increases the resilience of the marine health stock in the model the high trends in marine health are additionally as a result of the sustainable management measures that are implemented in each marine sector in the model table a2 marine wealth and labour similarly show higher trends for this scenario owing to higher observed trends in vessel visits mariculture yields fish biomass and tourist numbers with lower trends in pollutants from land discharge fig 8 under the adjusted model parameter values the marine uses are observed to be functioning above normal i e the base line run which altogether results in a lowering rate of marine health deterioration and an increase in the value of marine wealth and labour fig 8a c in contrast trends in the dead man s chest scenario show a rapid deterioration in marine health due to the cumulative effects from less sustainable trends in every sector fig 8a higher cumulative effects are as a result of higher pollutant loads in terms of wastewater and marine litter and a lower sardine biomass at the same time ports are not operating efficiently and therefore have a low annual throughout but with a higher level of marine traffic in the bay higher pressures from marine uses in the absence of marine health and sustainable sectoral management interventions therefore results in a lower and faster deteriorating marine health furthermore constraints within sectors and knock on effects between sectors decrease the overall value of marine wealth and labour trends shown for the eerie bay scenario are more closely related to the projections of the base line scenario fig 8 in this instance the dominant sectors that are less dependent on a healthy marine environment are performing better than the sectors that are more dependent on a healthy marine system for example the trends in shipping are higher just below the sea of plenty scenario and are therefore contributing largely towards marine wealth and labour fig 8 at the same time trends in land discharge are higher thereby exerting a high pressure on marine health with no consequences on the functioning of the use itself therefore through ineffective and unsustainable governance more dominant sectors may continue to function within their silos regardless of the knock on effects on the marine environment and thereby on other marine uses in the bay results for the deserted paradise scenario show an increasing marine health with a slower rate of change in health deterioration and a lower value in marine wealth and labour as a result of changing trends in the marine activities fig 8 a higher marine health is observed owing to lower growth in shipping and land discharge but similarly because of more sustainable trends in the level of sardine biomass trends in mariculture harvest and in tourism numbers while trends in marine labour are more consistent to those in the other scenarios the observation of lower marine wealth may be attributed to the lower growth in ports and shipping which contribute the biggest fraction to the marine economy over the course of the simulation followed by tourism and recreation fig 8 altogether these results highlight trade offs that exist between marine uses and between marine uses and the environment a high growth in the dominant or wealthier sectors may have a positive impact on marine wealth and labour in the short term with negative impacts on the marine environment in the longer term these changes may consequently impact on the growth in less dominant sectors who do not contribute as much value to marine wealth but are more vulnerable to changes in marine health in the bay therefore to achieve the most desirable msp scenario each sector should function sustainably with adequate management interventions in order to improve marine health and grow the marine economy although scenario analysis in this study is limited to four governance scenarios an additional scenario not shown but reported in vermeulen miltz et al 2022 included exploring model behaviour and associated disruptions particularly in the coastal tourism sector during the covid 19 pandemic furthermore additional scenarios can be explored using the visual user interface fig 9 which was specifically developed to provide a user friendly portal for stakeholders and decision makers to engage with the model the interface can additionally be used in a multi sectoral stakeholder setting whereby stakeholders representing different marine uses can implement alternative management interventions and thereby compare scenarios similar to what was demonstrated during the pilot multi sectoral stakeholder meeting during the ab codym process fig 3 the algoamsat interface can be run on any internet browser and is published online on the isee systems model exchange platform accessible via the following link https exchange iseesystems com public esteevermeulen the algoa marine systems analysis tool algoamsat user interface 6 discussion and conclusions msp has been adopted globally as the most suitable planning process towards facilitating a transition to a sustainable oceans economy whilst conserving the health of the marine environment ehler 2020 it has emerged among other integrated ocean management planning tools as a sensible approach to balance social ecological objectives among multiple uses of the marine space winther et al 2020 this has been seen to be particularly challenging considering the multiple objectives and perspectives arising from multiple interconnections between humans and the marine environment several decision support tools have been developed to support the msp process coleman et al 2011 pınarbaşı et al 2017 however a niche remains open for the development of tools to evaluate the growth of marine activities and associated user conflicts in order to plan and identify areas for management intervention a holistic systems analysis approach using system dynamics sd modelling was thus recognised to be well suited to addressing the complexity in social ecological marine systems over time this led to the development of the algoa marine systems analysis tool algoamsat to demonstrate how sd modelling can be applied to support msp by specifically informing planners of the projected trends of marine activities including associated marine use and marine environmental conflicts the baseline model results show that increasing trends in marine activities may result in a continual decrease in marine health whist increasing marine wealth and labour a common trend observed in marine sustainability outputs in other regions halpern et al 2019 oecd 2016 sink et al 2019 to alter these growth trajectories will require interventions that will have the greatest potential to shift marine sustainability towards a desirable trajectory where ecological and socio economic goals are met such as is shown in the sea of plenty scenario this may include focussing management interventions on the variables that hold deeper leverage and that govern the problem dynamics in the bigger system abson et al 2017 meadows 2009 particularly the drivers of change in conjunction it requires a combination of sectoral and cross sectoral management interventions that are directed to control growth within manageable limits set by both infrastructure capacity and marine environmental thresholds this may require sufficient investment in cleaner operations across marine sectors e g greener shipping wastewater recycling non invasive mariculture sustainable fishing and an overall increase in marine health awareness in the bay e g marine education campaigns mpa awareness programmes ecolabel support such as the blue flag beach and sustainable seafood initiatives moreover additional research should aim to quantify the marine environmental carrying capacity of the bay through the measure of essential marine health indicators furthermore to support the implementation of an eb msp process requires stricter enforcement of existing policy and legislation as well as the potential development of an overarching marine management policy qu et al 2016 such as a marine environmental tax that can restrict or incentivise marine users based on their sustainable growth trajectories and or on the sensitivity of a marine area management attention should additionally be directed to the dominant marine uses especially those that are exacerbating marine use conflicts such as to prevent them from growing unsustainably lastly interventions with even greater potential to achieve the goal of marine sustainability may include a transformation in ocean governance from one that is exclusive and fragmented to one that is coherent inclusive adaptive and acknowledges marine health as the basis for long term sustainability of marine uses in agreement with suggestions in haas et al 2021 and rudolph et al 2020 to support msp algoamsat provides a holistic management framework that integrates the complex dynamics of multiple marine uses and the marine environment through feedback loops secondly it serves as a quantitative model to simulate policy and management interventions under alternative scenarios to meet the overarching goal of an eb msp process which is to conserve a healthy marine environment that can sustain marine uses and support their growth though the outputs from algoamsat are not spatially explicit it supports the temporal planning component through temporally explicit trade off analyses scenario planning and policy design although the study was not explicitly designed to be a participatory modelling exercise as recognised as a common limitation in existing social ecological models drechsler 2020 it facilitated stakeholder engagement during the initial stages of model development this encouraged stakeholders to act as systems thinkers and gain a shared understanding of the pathways needed to achieve sustainable management within the sectors in the bay the model can therefore provide strategic guidance to msp and serve as a communication and scenario planning tool to use in a stakeholder or decision support setting while the study is by no means comprehensive and is limited to the extent of the model boundary it demonstrates the importance of understanding the complexities of feedbacks and changing dynamics over time and hence provides a proof of concept of how sd modelling can be applied to support msp to assist in planning processes in other regions or at larger scales the sectors in the model can be adapted or changed with context specific data relative to the dynamic marine planning area the model can additionally be used to supplement existing planning tools to collectively reach the goals of an ecosystem based msp process further application of the model in algoa bay beyond this study includes the incorporation of an ecosystem services valuation sub model blignaut et al in prep and may also include soft coupling the temporal results into a spatially explicit framework to further inform spatial mapping studies in the bay e g bassi et al 2016 lemahieu et al in prep such analytical assessments and tools are critical to progress towards the ambitious national goals of a sustainable blue economy and global development goals such as united nations sustainable development goal 14 conserve and sustainably use the oceans seas and marine resources united nations 2018 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank the national research foundation of south africa for research funding for the algoa bay project through the sarchl msp 98574 grant in addition to the funding support for the ab codym process from the one ocean hub gcrf ukri grant ne s008950 1 ev m was the lead modeller and author on this paper atl assisted in conceptualisation of the study contributed to the paper through writing and editing and provided the financial support jc h assisted in model development writing and lead the ab codym process us assisted with writing and model analyses the authors acknowledge the contributions from mr teun sluijs for his role in the ab codym process and contributions to an earlier version of the model lastly the authors would like to acknowledge members of the algoa bay project team for project support prof bernadette snow dr victoria goodall dr nina rivers dr kelly ortega cisneros and ms hannah truter as well as members from the institute for coastal and marine research the south african environmental observation network seaon and the sustainable seas trust sst for their participation in the pilot multi sector workshop photos credits are attributed as follows oiled penguin by lloyd edwards from raggy charters breaching whale by brigitte melly and dr stephanie plön algoa bay red tide from massie et al 2019 courtesy of dr pitcher deserted beach photo from google images source unknown supplementary material the following is the supplementary data to this article multimedia component 1 multimedia component 1 supplementary material supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105601 appendix a table a 1 data sources for the key variables represented in each sub model in the algoamsat parameter values for variables were obtained from a range of data sources including scientific literature government and industry reports newspaper articles and through stakeholder meetings shm more information on data sources can be found in the supplementary material table a 1 sub model variables data source 1 marine health marine health stakeholder holder meeting shm 2 marine wealth and labour marine wealth modeller s estimate value calculated for separate marine sectors and guided by dea 2018 2017b nmb gdp ecsecc 2017 nmb population ecsecc 2017 nmb labour force ecsecc 2017 3 ports and shipping vessel visits tnpa 2016 port capacity ports regulator of south africa 2015 rappetti et al 2020 shm shipping economics tnpa 2020 tnpa 2019a tnpa 2016 shipping labour tnpa 2012 4 land discharge wastewater production nmbm 2009 plastic production award 2019 barnardo and ribbink 2020 wastewater treatment plant wwtp capacity nmbm 2009 pollutant concentrations lemley et al 2019 wastewater economics roux et al 2010 land discharge labour modeller s estimate 5 mariculture farm area massie et al 2019 shm yield per hectare massie et al 2019 probyn et al 2015 nutrient pollution wright et al 2019 mariculture economics britz et al 2016 shm mariculture labour britz et al 2016 shm 6 fishing sardine biomass potter 2013 weigum 2019 shm sardine fishing fleet chalmers et al 2014 fishing profits hutchings et al 2015 fishing labour cochrane et al 2020 hutchings et al 2015 7 tourism and recreation tourists myles and louw 2011 nmbt 2018 rand international capital 2018 shm beach recreation fraction thornton 2013 nmbt 2018 beach trips ballance et al 2000 penguins nmbt 2019 shm accommodation myles and louw 2011 2014 shm tourist economics myles and louw 2014 2011 nmbt 2018 shm tourism labour myles and louw 2014 2011 nmbt 2018 table a 2 parameter values for selected model inputs applied in the baseline simulation and in the scenario analysis table a 2 model parameter and unit suggested policy or management interventions base value sea of plenty dead man s chest eerie bay deserted paradise marine sustainability marine health recovery time years marine protected area 5 3 10 5 1 nmb population growth fraction 1 year 0 015 0 010 0 030 0 015 0 010 nmb gdp growth fraction 1 year 0 016 0 030 0 010 0 016 0 020 ports and shipping shipping growth fraction 1 year 0 08 0 04 0 10 0 08 0 02 vessel turnaround time days port efficiency 2 1 5 5 2 2 port capacity expansion vessels 0 4 6 8 0 fraction of profits invested on port capacity dmnl government subsides 0 50 0 50 0 10 0 60 0 020 oil spill response capacity dmnl environmental risk preparedness 1 3 0 5 1 1 land discharge nutrient concentration mg l treatment efficiency dmnl 1 2 5 0 5 0 8 1 5 wastewater production l person day ww reuse recycling 150 100 250 200 100 household waste production kg person day recycling litter awareness and education 2 5 1 3 2 5 1 wwtp durability year maintenance 5 10 2 5 5 mariculture fraction of profits reinvested into farm dmnl government subsides 0 50 0 60 0 20 0 50 0 20 quality purification innovation dmnl quality control innovation and funding 1 2 1 1 1 finfish nutrient inputs kg ton year feed innovation 75 50 100 75 75 fishing sardine carrying capacity tons marine health interventions 1e5 2e5 5e4 1e5 2e5 catch per boat ton boat day catch innovation 6 6 10 8 6 fishing days days year closed open season 100 150 200 150 50 fraction of profits invested on fleet dmnl government subsides 0 50 0 50 0 20 0 60 0 30 tourism and recreation domestic tourism growth 1 year marketing 0 005 0 05 0 05 0 005 0 005 foreign tourism growth 1 year marketing 0 085 0 085 0 02 0 02 0 02 beach recreation fraction dmnl beach management 0 55 0 80 0 20 0 40 0 60 tourist time spent in the bay days year tourism activities and infrastructure 2 8 4 1 2 2 table a 3 input model parameters their base values and bounds on the range of uncertainty applied during the multivariate sensitivity analysis and statistical screening procedure parameter values were selected from a uniform distribution values in bold highlight the values that contain a higher degree of uncertainty and hence have a sampling range twice as large as the baseline parameter value or between the maximum bounds of certainty e g 0 1 table a 3 sub model model parameter and units base value sampling range upper lower bound marine sustainability health recovery time years 5 0 5 10 nmb net population growth fraction 1 year 0 015 0 0075 0 0225 nmb gva growth fraction 1 year 0 02 0 01 0 03 ports and shipping normal shipping growth fraction 1 year 0 06 0 03 0 09 normal vessel turnaround time days 2 1 3 normal anchorage capacity vessels 16 8 24 port revenue per vessel visit rand vessel 1 5 e6 5e5 2e6 indirect shipping labour 5 1 10 fraction of profits invested on port capacity dmnl 0 50 0 1 land discharge wastewater production l person day 150 75 225 wwtp durability 5 2 5 7 5 wwtp upgrade construction delay years 2 1 3 nutrient concentration for treated wastewater discharge din fwf mg l 16 5 8 25 24 75 household waste generated kg person day 2 5 1 25 3 75 fraction of marine litter washing out on the beach dmnl years 0 50 0 05 1 wwtp employees persons ml day 5 2 5 7 5 refuse employees persons ton 0 001 0 0005 0 0015 mariculture morality fraction 1 year 0 15 0 075 0 225 fraction of profits reinvested into farm dmnl 0 20 0 1 farm investment costs oysters rand hectare 5e5 1e5 1e6 unit benthic biodiversity loss dmnl hectare 0 05 0 1 nutrient inputs per ton finfish kg ton year 75 37 5 112 5 fishing normal recruitment fraction dmnl year 1 0 2 mortality fraction dmnl year 0 50 0 25 0 75 migration fraction dmnl year 0 26 0 13 0 39 carrying capacity tons 250 000 50 000 500 000 catch per boat ton boat day 6 3 9 normal fishing days days year 100 50 150 fraction of profits invested on fleet dmnl 0 50 0 1 operating costs per boat rand boat day 5000 2500 7500 indirect fishing multiplier dmnl 5 2 5 7 5 tourism and recreation foreign tourism growth dmnl year 0 085 0 04 0 12 domestic tourism growth dmnl year 0 005 0 0025 0 0075 time spent in the bay days year 2 8 1 4 4 2 tourism budget costs rand year 100e6 50e6 150e6 normal beach recreation fraction dmnl 0 55 0 275 0 825 tourism employee multiplier persons visitor 0 020 0 010 0 030 tourist spend per beach trip rand visitor trip 15 0 30 fraction of tourists going on marine tours dmnl year 0 05 0 01 0 10 fraction of locals that are regular beach visitors 0 30 0 15 0 45 note that the minimum bound for the recovery time and deterioration time is not zero because zero is an unrealistic measure of time and therefore causes a division error in the model note for the land discharge sub model sensitivity analyses were carried out only for the fish water flats fwf wastewater treatment plant wwtp since this is the biggest wwtp and to avoid an extensive sensitivity analysis for every element of the arrayed structure results of the sensitivity analysis in the land discharge sub model are similarly only evaluated in terms of dissolved inorganic nitrates din array element as the inputs from din are much higher than dissolved inorganic phosphates dip similarly the sensitivity analysis was performed only for the oyster farm element in the arrayed mariculture structure being the only currently existing farm type in the bay and being the most influential element in the mariculture sub model table a 4 summary of statistical screening results indicating the highest correlations of input variables against the primary output variables of interest the results of the correlation coefficient r and polarity of influence are shown in parentheses for the beginning and end period of interest i e 2020 2050 the correlations in bold font indicate significance p 0 05 during the simulation highlighted variables are those that can be influenced through policy and management interventions and have been applied in the scenario and policy analysis table a 4 rank marine health marine wealth marine labour vessel visits nutrient loads din oyster harvests sardine biomass tourists 1 marine health recovery time 0 60 0 95 tourist time spent in the bay 0 89 0 50 tourism employee multiplier 0 97 0 88 shipping growth fraction 0 95 0 05 wastewater production 0 80 0 53 oyster mortality fraction 1 0 63 sardine recruitment fraction 0 70 0 65 beach recreation fraction 0 80 0 77 2 waste production 0 46 0 25 vessel turnaround time 0 20 0 50 beach recreation fraction 0 29 0 46 vessel turnaround time 0 04 0 91 nutrient concentration of treated wastewater 0 53 0 74 fraction of profits reinvested into oyster farm 0 07 0 31 sardine carrying capacity 0 58 0 53 marine health recovery time 0 19 0 45 3 wwtp durability 0 37 0 27 tourism employee multiplier 0 44 0 30 marine health recovery time 0 18 0 34 shipping anchorage capacity 0 11 0 50 wwtp durability 0 19 0 38 marine health recovery time 0 02 0 21 sardine mortality fraction 0 44 0 26 tourism employee multiplier 0 21 0 31 4 vessel turnaround time 0 31 0 26 fishing days 0 33 0 41 sardine recruitment fraction 0 27 0 33 indirect fishing labour multiplier 0 05 0 32 construction delay 0 30 0 16 normal catch per boat 0 02 0 22 marine health recovery time 0 16 0 41 sardine recruitment fraction 0 28 0 25 5 sardine recruitment fraction 0 68 0 24 port revenue per vessel visit 0 04 0 48 shipping growth fraction 0 22 0 20 tourism employee multiplier 0 26 0 13 beach recreation fraction 0 29 0 22 beach recreation fraction 0 03 0 27 waste production 0 31 0 31 gva growth fraction 0 45 0 08 fig a 1 marine health sub model structure coloured variables are used to represent the inputs from and outputs to different marine use sub models mh refers to marine health fig a 1 fig a 2 marine wealth and labour sub model structure coloured variables are used to represent the inputs from and outputs to different marine use sub models fig a 2 fig a 3 shipping sub model structure endogenous shipping variables are in red and exogenous variables are in black links to other marine use sub models are in their respective colours fig a 3 fig a 4 land discharge sub model structure endogenous variables are in brown and exogenous variables are in black note the arrayed wwtp stock structure fig a 4 fig a 5 mariculture sub model structure endogenous variables are in green and exogenous variables are in black fig a 5 fig a 6 fishing sub model structure endogenous variables are in blue and exogenous variables are in black fig a 6 fig a 7 tourism and recreation sub model structure endogenous variables are in pink and exogenous variables are in black fig a 7 fig a 8 baseline model results of the secondary variables of interest in each sub model model results are shown in solid blue from top left to bottom right marine labour blue and marine employment fraction dotted red a marine contribution to nmb economy b shipping profits c shipping labour d shipping marine health e wastewater loads f land discharge sector costs g land discharge labour h land discharge marine health i mariculture farm area j mariculture profits k mariculture labour l figure a8 continued mariculture marine health m sardine catch dashed blue and sardine biomass red n fishing profits o fishing labour p fishing marine health q tourism profits r tourism labour s and coastal and marine tourism attractiveness t fig a 8 
25465,the foundation of a healthy marine environment is central to the ecosystem based management approach and is recommended for achieving sustainable development outcomes in marine spatial planning algoa bay south africa features a metropolitan area protected natural area and hosts a range of uses that are closely interconnected with the health of the marine system future trajectories of marine uses and related marine sustainability goals are expected to develop according to shifting needs of various sectors operating in the bay we therefore leverage the application of system dynamics modelling to explore these trends and the underlying feedback effects between marine uses and the marine environment by developing the algoa marine systems analysis tool the model is used to investigate sustainability outcomes under alternative scenarios over time and demonstrates how multiple cross sectoral management interventions can result in system wide change and can be used to achieve desirable marine sustainability goals in the long term graphical abstract image 1 keywords blue economy integrated ocean management systems analysis system dynamics modelling social ecological system abbreviations iom integrated ocean management sdm system dynamics modelling ses social ecological system sd system dynamics data availability data used in this study can be found in indicated references and supplementary materials 1 introduction humans are closely connected with the ocean 1 1 the terms ocean marine and sea are often used interchangeably in this study the term ocean may be used in a larger or global context whereas marine will be used at a more regional scale both directly and indirectly and the value obtained in terms of natural social and economic benefits is immense allison et al 2020 in response to the growing needs from an increasing human population marine uses and activities are projected to undergo a significant change in coming decades dnv 2021 jouffray et al 2020 oecd 2016 though these trends present socio economic opportunity as recognised in worldwide oceans economy directives utilisation of the ocean space does not come without limitations owing to the increasing demands on ocean space and resources humans are further encroaching on its natural limits through observed trends of biodiversity loss marine pollution ocean warming and acidification halpern et al 2017 hameed et al 2017 mccauley et al 2015 these escalating pressures are partly a result of inadequacies or failure in ocean governance and the lack of successful implementation of ecosystem based eb management approaches lombard et al 2019 in response integrated ocean management approaches particularly an ecosystem based eb marine spatial planning msp process has been endorsed globally borja et al 2016 ehler 2020 msp is a process that aims to bring together multiple users of the ocean space to make informed and coordinated decisions about the management of marine uses and resources ehler and douvere 2009 jones et al 2016 msp has therefore become increasingly popular among nations including in south africa to support eb management domínguez tejo et al 2016 douvere 2008 and to facilitate sustainable development of the oceans economy ioc unesco 2017 dea 2017a a large array of decision support tools have been developed to support msp gee et al 2019 janβen et al 2019 pınarbaşı et al 2017 existing tools have been developed for different functions with some explicitly developed to assist in spatial mapping e g sea sketch mcclintock and gordon 2015 spatial optimisation e g marxan ball et al 2009 simfish bartelings et al 2015 cumulative impact assessments e g halpern et al 2008 symphony hammar et al 2020 invest guerry et al 2012 or to investigate user interactions e g seanergy bonnevie et al 2020 however there remains a niche for tools to evaluate the temporal dynamics of human activities and to evaluate the underlying cause and effect relationships within complex social ecological marine systems aswani et al 2017 gissi et al 2019 the relationship between human dependence and interference with the marine environment can be non linear affected by time delays and dominated by feedback loops across social economic and environmental dimensions pongsiri et al 2017 there is therefore a need to understand the dynamics and feedback behaviour driving the change between current and increasing human uses in the marine space to support msp moreover key characteristics of an eb management approach to msp involves describing parts systems environments and their interactions and hence recognising system dynamics principles ehler and douvere 2007 agardy et al 2011 in this regard the applicability of systems dynamics sd modelling is leveraged as an appropriate tool to support msp sd modelling involves studying and managing complex systems over time forrester 1961 and is widely applied in social ecological research e g kelly et al 2013 elsawah et al 2017 drechsler 2020 with existing applications in the field of marine science e g hopkins et al 2012 boumans et al 2015 to date the multiscale integrated model for ecosystem services framework is well known for its application in the massachusetts marine model which was developed as a collaborative msp tool altman et al 2014 boumans et al 2015 other studies have also proved the application and strengths of system dynamics to msp some through successful demonstrations of participatory modelling and decision making processes e g videira et al 2012 vugteveen et al 2015 this study reports on the development of a system dynamics model the algoa marine systems analysis tool algoamsat and demonstrates the procedure of how sd can be applied to support msp in a densely utilised and local marine context such as in algoa bay south africa the algoamsat specifically aims to explore the temporal change of marine activities and trade offs between marine sustainability goals under different scenarios in algoa bay as a supporting msp framework algoamsat provides a holistic cross sectoral overview of the causal interactions between marine uses and the marine environment as a simulation model it provides a platform for scenario and policy analyses in relation to sustainable management of the bay together through the use of the interactive model interface it can help to facilitate collaborative stakeholder engagement in a msp process section two provides a description of the model boundary section three describes the method and model development process and section four describes the model structure section five presents the model results with the discussion and conclusions provided in section six 2 model boundary the algoamsat was developed as a management framework and simulation model that applies system dynamics sd modelling to facilitate and support msp sd models incorporate temporal dynamics and thereby can support msp by evaluating changes in marine uses as well as interconnections between marine uses and between marine uses and the health of the marine environment over time the model boundaries have therefore been delineated in line with this research goal the broad model boundary of algoamsat is comprised of seven sub models fig 1 five of these represent the selected marine uses namely shipping mariculture fishing tourism and recreation and land discharge activities the other two sub models integrate the outputs from each marine use in terms of their ecological and socio economic marine sustainability outputs respectively measured in terms of marine health and marine wealth and labour fig 1 these marine uses were particularly selected owing to their socio economic value and the level of impact or reliance on marine health among other criteria including political relevance public concern and recent media coverage the algoamsat was developed for the marine space delineated by the algoa bay msp project study boundary ranging from the shoreline to the 12 nautical miles territorial limit as defined in the maritime zones act rsa 1994 and from the western limit of the sardinia bay marine protected area mpa to the eastern edge of cannon rocks dorrington et al 2018 within the land boundaries of the nelson mandela bay nmb municipality fig 2 3 methods 3 1 model development process the modelling process adopted in this study evolved through a series of iterative steps during the first three steps of the modelling process marine uses were selected in relation to the model boundary management problems and marine use conflicts were identified and indicative behaviour over time graphs for key problem variables were defined these steps were carried out through extensive literature research and through consultation with experts and stakeholders through a sub project hosted in parallel to the study to support model development see section 3 1 2 on the algoa bay collaborative dynamic modelling process thereafter cause and effect dynamics for different marine uses were mapped according to the information gathered on marine use interactions in the bay which entailed adopting a circular thinking and structuring approach such as adopted in the driver pressure state impact response dpsir atkins et al 2011 or the adapted dapsi w r m framework 2 2 the dapsi w r m framework shows that drivers of basic human needs require activities which lead to pressures these pressures result in change in the state of the system which leads to impacts on human welfare those in turn require responses as management measures this is merely an extension of the original dpsir driver pressure state impact response framework atkins et al 2011 elliott et al 2017 elliott et al 2017 next the quantitative modelling stage entailed iteratively parameterising simulating and revising the model structure until the simulation captured the dynamic problem behaviour this was followed by model verification and validation section 3 1 3 3 1 1 model setup system dynamics is the primary modelling method adopted in this study model development was conducted using stella architect software isee systems version 2 1 1 richmond and peterson 2001 on a windows system the software enables the configuration of models using a range of functions including stocks flows and converter variables stock variables s capture the accumulation of information through mathematical integration over time t eq 1 whereas flow variables capture the rate of change or simply inputs and outputs into the stock variable through a differential function eq 2 sterman 2010 1 s t o c k t t 0 t i n f l o w s o u t f l o w s d s s t o c k t 0 2 d s t o c k d t i n f l o w t o u t f l o w t the selected time horizon and simulation period for the model is 40 years from 2010 to 2050 thus allowing model behaviour during the first 10 years to be compared with reference behaviour from 2010 to 2020 and to allow for scenario planning and policy testing from 2020 onwards i e the next 30 years an integration timestep i e delta time 1 12 was deemed appropriate for the simulation resulting in a consistent level of numerical accuracy with a total of 480 time steps 40 years 12 time steps year falling well within the recommended range of less than 1000 time steps ford 2009 the euler first order integration method was selected over runge kutta two or four euler was a suitable method given the combination of functions in the model and the model content whereas a higher order integration method may be suitable for models with many timesteps and higher turnover dynamics ford 2009 the model was initialised with base case parameters and assumptions table a2 and supplementary materials 3 1 2 stakeholder engagement the involvement of stakeholders has been recognised to be key to the success of modelling complex environmental problems and has resulted in the emergence of powerful participatory modelling techniques krueger et al 2012 voinov and bousquet 2010 in this study stakeholder engagement during the early stages of model development primarily assisted in integrating knowledge on different sectors activities impacts and planning visions into the model framework the algoa bay collaborative dynamic modelling ab codym process adapted from clifford holmes et al 2016 progressed through two phases phase one was initiated through stakeholder meetings n 34 with individuals from a range of institutions representing the marine uses in the model fig 3 to capture expert opinion as well as a range of perspectives in the model stakeholder representatives from academia n 13 businesses n 10 civil society n 5 as well as the essential end users including municipal and government officials n 6 were involved in the process in phase two of the process stakeholder engagement involved a pilot multi sector workshop fig 3 the workshop hosted 13 participants consisting of members of the algoa bay msp project and external researchers during the pilot workshop the model structure was demonstrated and discussed thereafter workshop participants played a scenario planning game using the model interface wherein participants were categorised into the different marine sectors and required to represent or role play a stakeholder or manager from one of the five marine sectors the pilot workshop was mainly intended to test and receive feedback on the preliminary model structure and to get an idea of how the model once completed can be applied in a multi sectoral stakeholder setting to support collaborative engagement and planning in msp 3 1 3 model validation the stakeholder engagement process served as a form of verification of the qualitative model structure but to further verify the quantitative model structure and behaviour a series of validation tests were performed these included direct structure tests ensuring dimensional consistency among the units and ensuring that model behaviour remained physically realistic during univariate sensitivity analysis where data could be sourced from literature or from stakeholders it was plotted against the simulated results in this instance validation was intended to measure reproducibility of model behaviour against observed data with emphasis on pattern or trend prediction rather than point prediction barlas 1996 a multivariate sensitivity analysis was also used to evaluate the model behaviour under a combination of values of multiple uncertain parameters razavi et al 2021 saltelli et al 2019 variables included in the sensitivity analyses included the exogenous variables that were estimated to a degree of uncertainty and were hypothesised to have an influence on the model dynamics ford and flynn 2005 turner 2020 parameter values known to have a higher degree of certainty i e values identified in literature or values that were substantiated by data were varied by 50 of the baseline value ford 2009 whereas parameter values known to have a lower degree of certainty i e values for which physical data do not exist or estimates that were provided by stakeholders were allocated a sampling range twice as large as the baseline value or a sampling range between maximum bounds of likelihood e g 0 1 ford 2009 sterman 2010 the latin hypercube sampling method which aims to obtain an even spread in the sample values across the sampling interval was chosen and the analyses ran for 50 model simulations as recommended in ford 2009 lastly to distinguish the degree of influence of the input exogenous parameters on the key variables of interest over the course of the simulation the statistical screening approach developed in ford and flynn 2005 and applied in taylor et al 2010 was conducted this involved exporting the model data to microsoft excel and calculating the spearman correlation coefficient between the input and output variable of interest at every model timestep under the condition of a monotonic and non normal data distribution this assisted in identifying which input variables held more leverage in the system and hence to apply in policy and scenario planning as well as include in the model interface 4 model structure fig 4 shows the condensed holistic overview of the model structure representing the interconnections among the key variables in each sub model that capture the temporal trends in each marine use from a marine sustainability perspective i e trends that are important in order to plan for marine use and marine environment threats and conflicts as well as opportunities and benefits for the marine economy detailed diagrams for each sub model structure are presented in appendix a figure a1 figure a7 a comprehensive overview of the model parameters and sources of information is provided in tables a1 and a2 with information on model equations provided in supplementary materials note that assumptions have been made in order to generalise and simplify relevant dynamics in each marine use and that dynamics represented are not for individual businesses or companies but have been aggregated to represent sectors as a whole in the ports and shipping sub model the key trends are captured in terms of the annual number of vessel visits captured as a stock variable fig 4 in land discharge the main trends captured are the volumes of wastewater and household litter discharged directly and indirectly into the marine environment both primarily driven by the coastal population stock and tourists in the bay fig 4 trends in mariculture include annual harvest yields of bivalves consisting of oysters magallana gigas and mussels mytilus galloprovincialis or perna perna and future finfish stocks consisting of yellowtail seriola lalandi or dusky kob argyrosomus japonicas that change through planting and farm investment fig 4 note that these are the species that are currently or are proposed to be farmed in the bay massie et al 2019 the fishing sub model specifically the sardine sardinops sagax fishery investigates trends in the fish biomass stock in relation to changes in catch effort fig 4 lastly trends in tourism and recreation focus on the annual number of tourists visiting the bay particularly tourists attracted to the bay for coastal and marine activities fig 4 in terms of marine sustainability outputs in terms of marine health wealth and labour are measured fig 4 the model additionally investigates interconnections between marine uses and how changing trends in one may impact on another links between the marine uses have been made explicit where there is enough scientific evidence and valid assumptions from research and stakeholders there is a direct link between the land discharge and mariculture sub model as well as to the tourism and recreation sub model fig 4 this link is connected through water quality where changes in volumes of wastewater affects mariculture harvest yields and the number of tourists engaging in coastal and marine activities other direct links in the model exist between shipping and tourism and between fishing and tourism where more vessels can decrease the abundance of iconic marine species due to increased noise and disturbance and similarly more fishing can decrease food availability although different marine species have been shown to react differently to noise pollution erbe et al 2019 the general assumption is that a higher level of cumulative impacts is likely to negatively influence the abundance of marine species that are valuable to marine tourism in the bay iconic marine species in the model include the african penguin spheniscus demersus the white shark carcharodon carcharias and humpback megaptera novaeangliae and southern right whales eubalaena australis furthermore there is a link between fishing and mariculture where a lower sardine biomass increases mariculture through a higher investment and vice versa fig 4 though other conflicts may exist between the marine uses these have been omitted owing to insufficient evidence and the need for ongoing research to verify relationships between them however it is important to note that indirect links between marine uses may still be captured through changes in the marine health of the bay which is measured following the cumulative impacts formulation from halpern et al 2008 fig 4 the main causal dynamics are formulated as follows the more vessels in the bay the higher the associated environmental risks and the lower the marine health at the same time the more vessels in the bay the higher the socio economic potential in terms of marine wealth and labour in terms of land discharge higher wastewater effluent and litter loads and decrease marine health but increase labour requirements to manage increasing loads and associated environmental impacts fig 4 note that no outputs from the land discharge sub model are connected to marine wealth as the sector is not driven for the goal of profit but rather to provide a public service in the mariculture sub model higher harvest yields can increase socio economic value but decrease marine health through increased environmental risks conversely lower marine health as a result of cumulative impacts from marine uses can decrease harvest yields and hence the socio economic value of mariculture b1 productive mariculture loop in the fishing sub model a higher biomass corresponds to a higher socio economic value in terms of profits labour and marine health with the opposite effects for a low biomass comparably a lower marine health can decrease fish biomass and hence its socio economic value and health status b2 sustainable fishing loop fig 4 lastly in the tourism and recreation sub model more tourists in the bay translates into higher socio economic value with a larger influence on marine health in terms of the contribution of tourists to land discharge fig 4 the growth of tourism is however also affected by marine health through changes in coastal and marine tourism attractiveness b3 tourism aesthetics loop tourism dynamics can be formulated through different structures with varying assumptions but should match the problem description in the respective case study and local context in algoa bay there is a need to increase coastal and marine tourism as opposed to in other regions of the world where unsustainable tourism with respect to overcrowding is more problematic kapmeier and gonçalves 2018 pizzitutti et al 2017 the three main balancing loops incorporated into the holistic structure of the model are therefore the mariculture b1 fishing b2 and tourism b2 loops fig 4 from an eb perspective these uses are dependent on a healthy marine system to sustain their marine activities in the bay the shipping and land discharge sub models however do not entail feedback to marine health because these uses are not directly dependent on a healthy marine environment to operate in the bay potential feedback effects that can create a balancing effect on the shipping and land discharge sectors exist if it becomes more costly to operate owing to stricter environmental regulations because of deteriorating marine health forcing stricter environmental policies in those sectors though these feedbacks are not built into the model structure they can be considered in management interventions to further endogenise model dynamics indirect links exist between the economic value of the uses in terms of marine wealth and the growth of marine activities through reinforcing feedback effects that are induced through changes in profitability r1 shipping growth loop r2 tourism growth loop r3 fishing growth loop r4 mariculture growth loop fig 4 additional details on the feedback processes in each marine use are shown in figure a1 figure a7 5 model results 5 1 baseline results the baseline simulation shows the behaviour over time of the key model variables under initial conditions owing to the extent of the model boundary and the number of model variables only the results of the primary variables of interest in each marine use are presented table 1 results for the secondary model variables are presented in appendix a figure a8 overall the model results show increasing growth in select marine activities with varying rates of change in response to varying driving dynamics and feedback effects in each marine use fig 5 and a1 a7 5 1 1 marine sustainability outputs fig 5 a c presents the baseline results of the outputs from the marine uses in terms of marine health marine wealth a measurement of the cumulative economic value gained directly from marine uses and marine labour as expected marine wealth is increasing over the course of the simulation starting at r3 7 billion in 2010 and growing to approximately r112 billion in 2050 fig 5b as a result of the accumulation of annual profits obtained from the marine activities altogether the contribution of the marine economy towards the local economy in terms of the gross value added increases from approximately 5 4 in 2010 to 9 4 in 2030 closing on 7 2 in 2050 similarly marine labour shows an increasing trend with an initial model value of 70 000 persons in 2010 and reaching 140 000 persons in 2050 fig 5c as a result of the cumulative pressures from the different marine activities the level of marine health in reference to the natural state of the marine system is decreasing over time starting at an initial value of 0 80 and reaching a low of 0 20 in 2050 fig 5a model outputs such as the variable marine health that are highly subjective to stakeholder opinion and hence contain a higher degree of uncertainty are further subject to sensitivity analysis see section 5 2 5 1 2 ports and shipping shipping activity in the bay is projected to increase in terms of annual recordings of vessel visits from approximately 700 vessels in 2010 7700 vessel visits in 2050 fig 5d the increasing trend is a result of the complex interplay between a historical growth fraction and interactions between the attractiveness of port services port efficiency and profitability of the sector profitability in turn determines the rate of change of port capacity which subject to time delays essentially limits the growth in vessel arrivals through marine traffic effects in conjunction it is projected that profitability and labour of the sector is expected to increase over the 40 year period in relation to the number of vessel visits profits from shipping show an increasing trend from approximately r860 million in 2010 to r8 billion in 2050 with total labour direct and indirect increasing from 700 persons in 2010 to 6000 persons in 2050 consequently trends in marine environmental risk subject to the level of traffic in the bay are projected to increase thus causing marine health to decrease from the reference point of 0 8 dmnl in 2010 to approximately 0 3 in 2050 5 1 3 land discharge annual trends in land discharge loads specifically wastewater effluent constituent nutrient loads and litter loads show an increasing rate of change over the course of the 40 year model period annual nutrient loads are projected to increase from 1 1 million kg year for dissolved inorganic nitrate din and 242 000 kg year for dissolved inorganic phosphate dip to 4 6 million kg years and 1 4 million kg years in 2050 respectively fig 5e similarly trends in household waste production are projected to increase from approximately 280 000 tons year in 2010 to 1 4 million tons per year in 2050 fig 5f this increasing trend is predominantly driven through an increase in the coastal population with a small contribution 2 5 from tourists in the bay though wastewater treatment infrastructure is expected to increase in response to the growing sanitation demand unexpected delays through planning financing maintenance or construction can result in periods where demand exceeds municipal capacity furthermore irrespective of the assimilation capacity of the receiving marine environment the continuous and increasing discharge of excess wastewater nutrients and litter can result in cumulative impacts on marine water quality and ultimately decrease marine health from approximately 0 85 in 2010 to 0 36 in 2050 costs incurred for wastewater treatment associated with the need for upgrades and treatment of wastewater are equated to r 280 million year in 2010 steeply increasing to r1 billion year towards 2012 and continuing to r1 5 billion year in 2050 for waste or refuse that equates to roughly r255 million year in 2010 and doubling to r466 million year in 2050 in terms of labour potential the growth in the land discharge sector assuming no technological innovation can double labour requirements to approximately 5000 workers by 2050 5 1 4 mariculture at a local scale mariculture trends are projected to increase fig 5g though the annual production and harvests can be determined only through observations of past trends and estimates of future investment the model shows that oyster production and harvests are to increase subject to zoning approvals from 100 tons year in 2020 to approximately 4800 tons year in 2050 on 600 hectares ha fig 5g this is however highly dependent on the fraction of profits obtained from annual harvests that are reinvested into the farm among other variables including the health of the marine environment and the seafood demand similar dynamics apply to mussel and finfish farming mussel production and harvesting is projected to increase from 0 tons year during 2010 2020 to approximately 129 tons year on 20 ha by 2050 fig 5g in finfish farming assuming the initial planting investment is 70 ha finfish harvests are projected to reach 730 tons year by 2025 fig 5g the projected estimates of mariculture production in the bay contain a high degree of uncertainty owing to the uncertainty in initial farm investments which are further explored in scenario analysis nevertheless consistent with these trends total mariculture profits are projected to increase from r26 million in 2010 obtained only through oyster farming to 340 million in 2050 across all farming types similarly total direct labour in the sea based mariculture sector is projected to increase from approximately 30 persons in 2010 to approximately 429 persons in 2050 although farm production is increasing problems associated to increasing land discharge are congruently expected to increase which can lead to additional losses through harvest sales and ultimately profitability and farm reinvestment concurrently marine health shows a decreasing trend from the level of 0 98 in 2010 to approximately 0 65 in 2050 due to pressures incurred through the expansion of farming induced through estimated losses in the benthic habitat structure and nutrient loading from finfish production positive effects from bivalve mariculture through water purification was not included in this study as these effects are highly dependent on the density of farming and surrounding hydrographic conditions but even so these effects are suggested to be minor turner et al 2019 5 1 5 fishing the baseline simulation shows a decreasing trend in sardine biomass from an initial value of 100 000 tons in 2010 and ending on 24 000 tons in 2050 fig 5h the declining sardine biomass is attributed to a combination of changes in catch and the health of the marine environment a decrease in sardine biomass further effects the catch per boat with additional effects on the annual catch catch value and hence revenues and profits as a result fishing profits are declining from around r20 million in 2010 2020 to approximately r9 million 2050 in terms of labour potential total labour including direct and indirect estimates changes in relation to the number of boats fishing with eight boats fishing at the start of the simulation and ending on 15 boats this translates into approximately a total of 480 persons employed in 2010 with the largest contribution provided through indirect i e secondary and tertiary employment and 891 persons in 2050 marine health in the fishing sub model is measured through the ratio of the sardine biomass normalised against the sustainable biomass formulated as half of the carrying capacity mahata et al 2020 therefore according to this calculation and under baseline model values marine health from the perspective of the fishing sector is decreasing from the initial starting point of 0 95 and levelling off at around 0 47 in 2050 however as the sardine population continues to decline fishing becomes more expensive such that the fishing fleet decreases hence decreasing the level of catch which in turn causes the sardine biomass to oscillate around a very low biomass of 20 000 tons the declining sardine biomass has knock on effects on other uses in the bay such as on the tourism and recreation sector whereby the abundance of marine iconic species is sensitive to the trend in sardine biomass 5 1 6 tourism and recreation trends in tourism are increasing in algoa bay with foreign tourism increasing at a higher rate than domestic tourism fig 5i fig 5i shows how the number of foreign domestic tourists increased from 180 000 3 million visitors in 2010 to approximately 1 1 million 5 3 million in 2050 growth in the number of tourists is driven through the effects from a combination of variables with the dominating effect emanating from the normal annual growth fraction which is subject to changes in coastal and marine attractiveness under baseline values the trend in coastal and marine attractiveness is declining over the course of the simulation starting at an initial value of 1 in 2010 and ending on 0 25 in 2050 due to changes in the level of beach recreation and marine wildlife tours which in turn are decreasing due to knock on effects from other marine uses and a decline in marine health moreover the decrease in coastal and marine tourism attractiveness does not only influence the annual growth fraction of tourists but similarly influences the duration of time spent in the bay this consequently affects tourism revenues obtained from accommodation and coastal and marine activities despite the increase in the number of tourists the trend in total profits steadily increases from approximately r3 billion year in 2010 until r4 5 billion year in 2031 after which it declines to r3 6 billion year in 2050 owing to changes in the duration of time spent in the bay lastly the changing trend in the number of tourists has a positive effect on employment where tourism labour is projected to double towards the end of the simulation from 60 000 persons in 2010 to 120 000 persons in 2050 5 2 validation results while the baseline simulation captures a conservative perspective of the model results subject to model assumptions and initial values a sensitivity analysis provides a broader perspective on the possible growth trajectories under alternative model parameter values and combinations thereof pianosi et al 2016 saltelli et al 2019 the model parameters included in the multivariate sensitivity analyses and respective sampling ranges are shown in table a3 fig 6 shows the results obtained from the multivariate sensitivity analyses for the primary model outputs obtained from 50 model simulations with the spread of uncertainty depicted by the confidence intervals overall the results trace out a pattern consistent to the baseline simulation and in line with the mean result across 50 runs which show increasing trends in marine uses apart from a decreasing trend observed for the variables of sardine biomass and marine health fig 6a and h the spread of the results for variables also highlight that more uncertainty is evident in the results of marine health mariculture harvesting and the sardine biomass owing to underlying uncertainties in the initial input model values fig 6a g and h variations in the parameter values of the model inputs do not appear to alter the fundamental pattern of behaviour but rather the rate of change at which marine activities are projected to grow with observed knock on effects on the marine sustainability outputs table a4 presents the statistical screening results measured in terms of the correlation coefficient and polarity of the model input against the primary variables of interest ranked according to the five highest correlated model inputs the correlation coefficient presented reports the correlation coefficients between the input and output variables for the beginning of the projected model simulation 2020 and at the end of the projected model simulation 2050 the statistical screening results therefore show which input variables are the most influential on model behaviour during those time periods table a4 the results show that the primary variables of interest in the model are not only influenced through direct effects within each sector but are additionally influenced through indirect cross sectoral feedback effects with some effects being more influential than others for the most part the input variables that have a positive effect on the growth of marine activities and towards marine wealth have a negative effect on marine health and vice versa table a4 5 3 scenario analysis and model interface scenario planning was used in this study to explore the temporal dynamics and underlying uncertainty through asking what if questions surrounding marine uses in the bay scenario planning has been shown to be a particularly useful tool in integrated management processes claassen et al 2013 wwf sa and bcg 2017 and is hence suitable to support msp scenarios are not forecasts or predictions but rather projections to explore how current situations in this case the expansion and growth of marine uses in the bay might evolve in the next couple of decades to reflect alternative visions for the bay a scenario matrix was developed by the research team to show how varying levels of ocean governance ranging from poor to good governance strategies in msp can result in different scenarios with trade offs between ecological and socio economic marine sustainability goals fig 7 though it would have been ideal to define and test the scenarios with stakeholders limited time and project resources in addition to the pandemic and broader project concerns around stakeholder fatigue prevented additional stakeholder engagement during this stage of the analysis the marine sustainability matrix is comprised of three axes of uncertainty the x axis to represent the level of marine health and the y axis to captures the level of marine wealth and labour fig 7 trade offs among these axes are further dependent on the level of ocean governance represented by the z axis or diagonal axis fig 7 the four most extreme exploratory scenarios are defined by four narratives namely the sea of plenty eerie bay dead man s chest and deserted paradise and compared against the baseline or business as usual bau scenario here the level of ocean governance can be interpreted in relation to ocean governance characteristics that have been identified and recommended by ehler and douvere 2009 flannery et al 2018 jones et al 2016 and kelly et al 2019 and the level of desirability is measured against the outcome of best practice msp where both socio economic and ecological objectives are met across sectors more specifically the scenarios are defined as follows 5 3 1 base case or business as usual scenario the bau scenario assumes a continuation of past trends and no changes to current ocean governance strategies but simply a continuation of the growth in marine activities under standard management practices 5 3 2 sea of plenty scenario the sea of plenty scenario fig 7 the most desirable scenario reflects a sustainable ocean governance strategy that is built upon an integrated and eb approach moreover precautionary governance ensures that the growth of marine activities is managed within marine environmental limits to sustain the provision of socio economic benefits obtained from the marine economy in the long term 5 3 3 dead man s chest scenario the dead man s chest scenario fig 7 the least desirable scenario reflects a poor marine economy with low employment and investment potential debilitated by an unhealthy marine environment this is assumed to be because of unsustainable governance characterised by weak enforcement and uncoordinated and siloed planning 5 3 4 eerie bay scenario the eerie bay scenario fig 7 reflects a sub desirable future where marine health is deteriorating at the expense of expedient sectoral growth that is driven through short term political goals thus through exclusive and fragmented ocean governance only the dominant uses benefit resulting in unintended consequences on the marine environment and on development objectives of other uses in the bay 5 3 5 deserted paradise scenario the deserted paradise scenario fig 7 similarly represents a sub desirable future assuming that low levels of innovation ambition or capacity development in ocean governance may result in a thriving marine environment that is under valued in its potential towards growing the marine economy and providing socio economic benefits 5 4 scenario results table a2 presents the model input variables and parameter values selected for each scenario model inputs that have been applied in scenario planning include the input variables that were identified to be most influential through the statistical screening approach section 5 2 2 which may be adapted through changes in management and policy interventions note that for the purpose of the scenario analysis in the model the changes in the model input parameters are effective only from 2020 onwards results of the primary variables of interest are shown in fig 8 as defined the sea of plenty scenario is hypothesised to result in a prosperous marine economy built upon a healthy marine environment according to fig 8 a this scenario shows that marine health is slowly increasing above trends in marine health over the previous decade this result is owing to the reduction in the marine health recovery time owing to the implementation of the marine protected area which subsequently increases the resilience of the marine health stock in the model the high trends in marine health are additionally as a result of the sustainable management measures that are implemented in each marine sector in the model table a2 marine wealth and labour similarly show higher trends for this scenario owing to higher observed trends in vessel visits mariculture yields fish biomass and tourist numbers with lower trends in pollutants from land discharge fig 8 under the adjusted model parameter values the marine uses are observed to be functioning above normal i e the base line run which altogether results in a lowering rate of marine health deterioration and an increase in the value of marine wealth and labour fig 8a c in contrast trends in the dead man s chest scenario show a rapid deterioration in marine health due to the cumulative effects from less sustainable trends in every sector fig 8a higher cumulative effects are as a result of higher pollutant loads in terms of wastewater and marine litter and a lower sardine biomass at the same time ports are not operating efficiently and therefore have a low annual throughout but with a higher level of marine traffic in the bay higher pressures from marine uses in the absence of marine health and sustainable sectoral management interventions therefore results in a lower and faster deteriorating marine health furthermore constraints within sectors and knock on effects between sectors decrease the overall value of marine wealth and labour trends shown for the eerie bay scenario are more closely related to the projections of the base line scenario fig 8 in this instance the dominant sectors that are less dependent on a healthy marine environment are performing better than the sectors that are more dependent on a healthy marine system for example the trends in shipping are higher just below the sea of plenty scenario and are therefore contributing largely towards marine wealth and labour fig 8 at the same time trends in land discharge are higher thereby exerting a high pressure on marine health with no consequences on the functioning of the use itself therefore through ineffective and unsustainable governance more dominant sectors may continue to function within their silos regardless of the knock on effects on the marine environment and thereby on other marine uses in the bay results for the deserted paradise scenario show an increasing marine health with a slower rate of change in health deterioration and a lower value in marine wealth and labour as a result of changing trends in the marine activities fig 8 a higher marine health is observed owing to lower growth in shipping and land discharge but similarly because of more sustainable trends in the level of sardine biomass trends in mariculture harvest and in tourism numbers while trends in marine labour are more consistent to those in the other scenarios the observation of lower marine wealth may be attributed to the lower growth in ports and shipping which contribute the biggest fraction to the marine economy over the course of the simulation followed by tourism and recreation fig 8 altogether these results highlight trade offs that exist between marine uses and between marine uses and the environment a high growth in the dominant or wealthier sectors may have a positive impact on marine wealth and labour in the short term with negative impacts on the marine environment in the longer term these changes may consequently impact on the growth in less dominant sectors who do not contribute as much value to marine wealth but are more vulnerable to changes in marine health in the bay therefore to achieve the most desirable msp scenario each sector should function sustainably with adequate management interventions in order to improve marine health and grow the marine economy although scenario analysis in this study is limited to four governance scenarios an additional scenario not shown but reported in vermeulen miltz et al 2022 included exploring model behaviour and associated disruptions particularly in the coastal tourism sector during the covid 19 pandemic furthermore additional scenarios can be explored using the visual user interface fig 9 which was specifically developed to provide a user friendly portal for stakeholders and decision makers to engage with the model the interface can additionally be used in a multi sectoral stakeholder setting whereby stakeholders representing different marine uses can implement alternative management interventions and thereby compare scenarios similar to what was demonstrated during the pilot multi sectoral stakeholder meeting during the ab codym process fig 3 the algoamsat interface can be run on any internet browser and is published online on the isee systems model exchange platform accessible via the following link https exchange iseesystems com public esteevermeulen the algoa marine systems analysis tool algoamsat user interface 6 discussion and conclusions msp has been adopted globally as the most suitable planning process towards facilitating a transition to a sustainable oceans economy whilst conserving the health of the marine environment ehler 2020 it has emerged among other integrated ocean management planning tools as a sensible approach to balance social ecological objectives among multiple uses of the marine space winther et al 2020 this has been seen to be particularly challenging considering the multiple objectives and perspectives arising from multiple interconnections between humans and the marine environment several decision support tools have been developed to support the msp process coleman et al 2011 pınarbaşı et al 2017 however a niche remains open for the development of tools to evaluate the growth of marine activities and associated user conflicts in order to plan and identify areas for management intervention a holistic systems analysis approach using system dynamics sd modelling was thus recognised to be well suited to addressing the complexity in social ecological marine systems over time this led to the development of the algoa marine systems analysis tool algoamsat to demonstrate how sd modelling can be applied to support msp by specifically informing planners of the projected trends of marine activities including associated marine use and marine environmental conflicts the baseline model results show that increasing trends in marine activities may result in a continual decrease in marine health whist increasing marine wealth and labour a common trend observed in marine sustainability outputs in other regions halpern et al 2019 oecd 2016 sink et al 2019 to alter these growth trajectories will require interventions that will have the greatest potential to shift marine sustainability towards a desirable trajectory where ecological and socio economic goals are met such as is shown in the sea of plenty scenario this may include focussing management interventions on the variables that hold deeper leverage and that govern the problem dynamics in the bigger system abson et al 2017 meadows 2009 particularly the drivers of change in conjunction it requires a combination of sectoral and cross sectoral management interventions that are directed to control growth within manageable limits set by both infrastructure capacity and marine environmental thresholds this may require sufficient investment in cleaner operations across marine sectors e g greener shipping wastewater recycling non invasive mariculture sustainable fishing and an overall increase in marine health awareness in the bay e g marine education campaigns mpa awareness programmes ecolabel support such as the blue flag beach and sustainable seafood initiatives moreover additional research should aim to quantify the marine environmental carrying capacity of the bay through the measure of essential marine health indicators furthermore to support the implementation of an eb msp process requires stricter enforcement of existing policy and legislation as well as the potential development of an overarching marine management policy qu et al 2016 such as a marine environmental tax that can restrict or incentivise marine users based on their sustainable growth trajectories and or on the sensitivity of a marine area management attention should additionally be directed to the dominant marine uses especially those that are exacerbating marine use conflicts such as to prevent them from growing unsustainably lastly interventions with even greater potential to achieve the goal of marine sustainability may include a transformation in ocean governance from one that is exclusive and fragmented to one that is coherent inclusive adaptive and acknowledges marine health as the basis for long term sustainability of marine uses in agreement with suggestions in haas et al 2021 and rudolph et al 2020 to support msp algoamsat provides a holistic management framework that integrates the complex dynamics of multiple marine uses and the marine environment through feedback loops secondly it serves as a quantitative model to simulate policy and management interventions under alternative scenarios to meet the overarching goal of an eb msp process which is to conserve a healthy marine environment that can sustain marine uses and support their growth though the outputs from algoamsat are not spatially explicit it supports the temporal planning component through temporally explicit trade off analyses scenario planning and policy design although the study was not explicitly designed to be a participatory modelling exercise as recognised as a common limitation in existing social ecological models drechsler 2020 it facilitated stakeholder engagement during the initial stages of model development this encouraged stakeholders to act as systems thinkers and gain a shared understanding of the pathways needed to achieve sustainable management within the sectors in the bay the model can therefore provide strategic guidance to msp and serve as a communication and scenario planning tool to use in a stakeholder or decision support setting while the study is by no means comprehensive and is limited to the extent of the model boundary it demonstrates the importance of understanding the complexities of feedbacks and changing dynamics over time and hence provides a proof of concept of how sd modelling can be applied to support msp to assist in planning processes in other regions or at larger scales the sectors in the model can be adapted or changed with context specific data relative to the dynamic marine planning area the model can additionally be used to supplement existing planning tools to collectively reach the goals of an ecosystem based msp process further application of the model in algoa bay beyond this study includes the incorporation of an ecosystem services valuation sub model blignaut et al in prep and may also include soft coupling the temporal results into a spatially explicit framework to further inform spatial mapping studies in the bay e g bassi et al 2016 lemahieu et al in prep such analytical assessments and tools are critical to progress towards the ambitious national goals of a sustainable blue economy and global development goals such as united nations sustainable development goal 14 conserve and sustainably use the oceans seas and marine resources united nations 2018 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank the national research foundation of south africa for research funding for the algoa bay project through the sarchl msp 98574 grant in addition to the funding support for the ab codym process from the one ocean hub gcrf ukri grant ne s008950 1 ev m was the lead modeller and author on this paper atl assisted in conceptualisation of the study contributed to the paper through writing and editing and provided the financial support jc h assisted in model development writing and lead the ab codym process us assisted with writing and model analyses the authors acknowledge the contributions from mr teun sluijs for his role in the ab codym process and contributions to an earlier version of the model lastly the authors would like to acknowledge members of the algoa bay project team for project support prof bernadette snow dr victoria goodall dr nina rivers dr kelly ortega cisneros and ms hannah truter as well as members from the institute for coastal and marine research the south african environmental observation network seaon and the sustainable seas trust sst for their participation in the pilot multi sector workshop photos credits are attributed as follows oiled penguin by lloyd edwards from raggy charters breaching whale by brigitte melly and dr stephanie plön algoa bay red tide from massie et al 2019 courtesy of dr pitcher deserted beach photo from google images source unknown supplementary material the following is the supplementary data to this article multimedia component 1 multimedia component 1 supplementary material supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105601 appendix a table a 1 data sources for the key variables represented in each sub model in the algoamsat parameter values for variables were obtained from a range of data sources including scientific literature government and industry reports newspaper articles and through stakeholder meetings shm more information on data sources can be found in the supplementary material table a 1 sub model variables data source 1 marine health marine health stakeholder holder meeting shm 2 marine wealth and labour marine wealth modeller s estimate value calculated for separate marine sectors and guided by dea 2018 2017b nmb gdp ecsecc 2017 nmb population ecsecc 2017 nmb labour force ecsecc 2017 3 ports and shipping vessel visits tnpa 2016 port capacity ports regulator of south africa 2015 rappetti et al 2020 shm shipping economics tnpa 2020 tnpa 2019a tnpa 2016 shipping labour tnpa 2012 4 land discharge wastewater production nmbm 2009 plastic production award 2019 barnardo and ribbink 2020 wastewater treatment plant wwtp capacity nmbm 2009 pollutant concentrations lemley et al 2019 wastewater economics roux et al 2010 land discharge labour modeller s estimate 5 mariculture farm area massie et al 2019 shm yield per hectare massie et al 2019 probyn et al 2015 nutrient pollution wright et al 2019 mariculture economics britz et al 2016 shm mariculture labour britz et al 2016 shm 6 fishing sardine biomass potter 2013 weigum 2019 shm sardine fishing fleet chalmers et al 2014 fishing profits hutchings et al 2015 fishing labour cochrane et al 2020 hutchings et al 2015 7 tourism and recreation tourists myles and louw 2011 nmbt 2018 rand international capital 2018 shm beach recreation fraction thornton 2013 nmbt 2018 beach trips ballance et al 2000 penguins nmbt 2019 shm accommodation myles and louw 2011 2014 shm tourist economics myles and louw 2014 2011 nmbt 2018 shm tourism labour myles and louw 2014 2011 nmbt 2018 table a 2 parameter values for selected model inputs applied in the baseline simulation and in the scenario analysis table a 2 model parameter and unit suggested policy or management interventions base value sea of plenty dead man s chest eerie bay deserted paradise marine sustainability marine health recovery time years marine protected area 5 3 10 5 1 nmb population growth fraction 1 year 0 015 0 010 0 030 0 015 0 010 nmb gdp growth fraction 1 year 0 016 0 030 0 010 0 016 0 020 ports and shipping shipping growth fraction 1 year 0 08 0 04 0 10 0 08 0 02 vessel turnaround time days port efficiency 2 1 5 5 2 2 port capacity expansion vessels 0 4 6 8 0 fraction of profits invested on port capacity dmnl government subsides 0 50 0 50 0 10 0 60 0 020 oil spill response capacity dmnl environmental risk preparedness 1 3 0 5 1 1 land discharge nutrient concentration mg l treatment efficiency dmnl 1 2 5 0 5 0 8 1 5 wastewater production l person day ww reuse recycling 150 100 250 200 100 household waste production kg person day recycling litter awareness and education 2 5 1 3 2 5 1 wwtp durability year maintenance 5 10 2 5 5 mariculture fraction of profits reinvested into farm dmnl government subsides 0 50 0 60 0 20 0 50 0 20 quality purification innovation dmnl quality control innovation and funding 1 2 1 1 1 finfish nutrient inputs kg ton year feed innovation 75 50 100 75 75 fishing sardine carrying capacity tons marine health interventions 1e5 2e5 5e4 1e5 2e5 catch per boat ton boat day catch innovation 6 6 10 8 6 fishing days days year closed open season 100 150 200 150 50 fraction of profits invested on fleet dmnl government subsides 0 50 0 50 0 20 0 60 0 30 tourism and recreation domestic tourism growth 1 year marketing 0 005 0 05 0 05 0 005 0 005 foreign tourism growth 1 year marketing 0 085 0 085 0 02 0 02 0 02 beach recreation fraction dmnl beach management 0 55 0 80 0 20 0 40 0 60 tourist time spent in the bay days year tourism activities and infrastructure 2 8 4 1 2 2 table a 3 input model parameters their base values and bounds on the range of uncertainty applied during the multivariate sensitivity analysis and statistical screening procedure parameter values were selected from a uniform distribution values in bold highlight the values that contain a higher degree of uncertainty and hence have a sampling range twice as large as the baseline parameter value or between the maximum bounds of certainty e g 0 1 table a 3 sub model model parameter and units base value sampling range upper lower bound marine sustainability health recovery time years 5 0 5 10 nmb net population growth fraction 1 year 0 015 0 0075 0 0225 nmb gva growth fraction 1 year 0 02 0 01 0 03 ports and shipping normal shipping growth fraction 1 year 0 06 0 03 0 09 normal vessel turnaround time days 2 1 3 normal anchorage capacity vessels 16 8 24 port revenue per vessel visit rand vessel 1 5 e6 5e5 2e6 indirect shipping labour 5 1 10 fraction of profits invested on port capacity dmnl 0 50 0 1 land discharge wastewater production l person day 150 75 225 wwtp durability 5 2 5 7 5 wwtp upgrade construction delay years 2 1 3 nutrient concentration for treated wastewater discharge din fwf mg l 16 5 8 25 24 75 household waste generated kg person day 2 5 1 25 3 75 fraction of marine litter washing out on the beach dmnl years 0 50 0 05 1 wwtp employees persons ml day 5 2 5 7 5 refuse employees persons ton 0 001 0 0005 0 0015 mariculture morality fraction 1 year 0 15 0 075 0 225 fraction of profits reinvested into farm dmnl 0 20 0 1 farm investment costs oysters rand hectare 5e5 1e5 1e6 unit benthic biodiversity loss dmnl hectare 0 05 0 1 nutrient inputs per ton finfish kg ton year 75 37 5 112 5 fishing normal recruitment fraction dmnl year 1 0 2 mortality fraction dmnl year 0 50 0 25 0 75 migration fraction dmnl year 0 26 0 13 0 39 carrying capacity tons 250 000 50 000 500 000 catch per boat ton boat day 6 3 9 normal fishing days days year 100 50 150 fraction of profits invested on fleet dmnl 0 50 0 1 operating costs per boat rand boat day 5000 2500 7500 indirect fishing multiplier dmnl 5 2 5 7 5 tourism and recreation foreign tourism growth dmnl year 0 085 0 04 0 12 domestic tourism growth dmnl year 0 005 0 0025 0 0075 time spent in the bay days year 2 8 1 4 4 2 tourism budget costs rand year 100e6 50e6 150e6 normal beach recreation fraction dmnl 0 55 0 275 0 825 tourism employee multiplier persons visitor 0 020 0 010 0 030 tourist spend per beach trip rand visitor trip 15 0 30 fraction of tourists going on marine tours dmnl year 0 05 0 01 0 10 fraction of locals that are regular beach visitors 0 30 0 15 0 45 note that the minimum bound for the recovery time and deterioration time is not zero because zero is an unrealistic measure of time and therefore causes a division error in the model note for the land discharge sub model sensitivity analyses were carried out only for the fish water flats fwf wastewater treatment plant wwtp since this is the biggest wwtp and to avoid an extensive sensitivity analysis for every element of the arrayed structure results of the sensitivity analysis in the land discharge sub model are similarly only evaluated in terms of dissolved inorganic nitrates din array element as the inputs from din are much higher than dissolved inorganic phosphates dip similarly the sensitivity analysis was performed only for the oyster farm element in the arrayed mariculture structure being the only currently existing farm type in the bay and being the most influential element in the mariculture sub model table a 4 summary of statistical screening results indicating the highest correlations of input variables against the primary output variables of interest the results of the correlation coefficient r and polarity of influence are shown in parentheses for the beginning and end period of interest i e 2020 2050 the correlations in bold font indicate significance p 0 05 during the simulation highlighted variables are those that can be influenced through policy and management interventions and have been applied in the scenario and policy analysis table a 4 rank marine health marine wealth marine labour vessel visits nutrient loads din oyster harvests sardine biomass tourists 1 marine health recovery time 0 60 0 95 tourist time spent in the bay 0 89 0 50 tourism employee multiplier 0 97 0 88 shipping growth fraction 0 95 0 05 wastewater production 0 80 0 53 oyster mortality fraction 1 0 63 sardine recruitment fraction 0 70 0 65 beach recreation fraction 0 80 0 77 2 waste production 0 46 0 25 vessel turnaround time 0 20 0 50 beach recreation fraction 0 29 0 46 vessel turnaround time 0 04 0 91 nutrient concentration of treated wastewater 0 53 0 74 fraction of profits reinvested into oyster farm 0 07 0 31 sardine carrying capacity 0 58 0 53 marine health recovery time 0 19 0 45 3 wwtp durability 0 37 0 27 tourism employee multiplier 0 44 0 30 marine health recovery time 0 18 0 34 shipping anchorage capacity 0 11 0 50 wwtp durability 0 19 0 38 marine health recovery time 0 02 0 21 sardine mortality fraction 0 44 0 26 tourism employee multiplier 0 21 0 31 4 vessel turnaround time 0 31 0 26 fishing days 0 33 0 41 sardine recruitment fraction 0 27 0 33 indirect fishing labour multiplier 0 05 0 32 construction delay 0 30 0 16 normal catch per boat 0 02 0 22 marine health recovery time 0 16 0 41 sardine recruitment fraction 0 28 0 25 5 sardine recruitment fraction 0 68 0 24 port revenue per vessel visit 0 04 0 48 shipping growth fraction 0 22 0 20 tourism employee multiplier 0 26 0 13 beach recreation fraction 0 29 0 22 beach recreation fraction 0 03 0 27 waste production 0 31 0 31 gva growth fraction 0 45 0 08 fig a 1 marine health sub model structure coloured variables are used to represent the inputs from and outputs to different marine use sub models mh refers to marine health fig a 1 fig a 2 marine wealth and labour sub model structure coloured variables are used to represent the inputs from and outputs to different marine use sub models fig a 2 fig a 3 shipping sub model structure endogenous shipping variables are in red and exogenous variables are in black links to other marine use sub models are in their respective colours fig a 3 fig a 4 land discharge sub model structure endogenous variables are in brown and exogenous variables are in black note the arrayed wwtp stock structure fig a 4 fig a 5 mariculture sub model structure endogenous variables are in green and exogenous variables are in black fig a 5 fig a 6 fishing sub model structure endogenous variables are in blue and exogenous variables are in black fig a 6 fig a 7 tourism and recreation sub model structure endogenous variables are in pink and exogenous variables are in black fig a 7 fig a 8 baseline model results of the secondary variables of interest in each sub model model results are shown in solid blue from top left to bottom right marine labour blue and marine employment fraction dotted red a marine contribution to nmb economy b shipping profits c shipping labour d shipping marine health e wastewater loads f land discharge sector costs g land discharge labour h land discharge marine health i mariculture farm area j mariculture profits k mariculture labour l figure a8 continued mariculture marine health m sardine catch dashed blue and sardine biomass red n fishing profits o fishing labour p fishing marine health q tourism profits r tourism labour s and coastal and marine tourism attractiveness t fig a 8 
25466,this study aims at developing an empirical multi variable tsunami damage model for buildings based on machine learning algorithms which leverage about 250 000 ex post data surveyed by the japanese ministry of land infrastructure and transportation after the 2011 great east japan event in the tōhoku region by implementing simple geospatial tools the dataset is integrated with additional explanatory variables including among others factors accounting for the mutual interaction between the inundated structures tests on models sensitivity to the number and type of input features used for model development reveal the importance on the predictive performance of considering usually neglected mechanisms like the shielding effect and the debris impact generation the analysis for the potential spatial transferability indicates a reduction in the accuracy thus suggesting a better suitability of empirical models for descriptive purposes limiting their predictive ability only to region specific cases keywords building damage tsunami machine learning feature importance spatial transferability japan data availability please refer to the software and data availability section of the paper 1 introduction the development of damage models enabling the estimation of the impacts generated by the occurrence of extreme natural events plays a key role in supporting decision making for the adoption of effective and efficient disaster management strategies the conventional tool used in tsunami risk modelling is represented by fragility curves which describe the probability of reaching or exceeding a certain damage state as a function of one or more event intensity parameters usually inundation depth and vulnerability characteristics of the exposed object these functions can be derived empirically based on post event observations or using synthetic expert based approaches tarbotton et al 2015 charvet et al 2017 the ability of a building to withstand the impact of a tsunami depends on several factors including the structural features and shape of the building its construction material the foundation type as well as the characteristics of the surrounding environment reese et al 2007 2011 kappes et al 2012 suppasri et al 2013 2015 charvet et al 2014 2015 2017 leelawat et al 2014 tarbotton et al 2015 dall osso et al 2016 regarding the latter factors coastal topography is expected to have a significant influence on tsunami damage due to possible amplifying effects occurring in ria coasts which would induce for a same value of the inundation depth higher damage levels than in plain areas suppasri et al 2013 2015 leelawat et al 2014 de risi et al 2017 in addition as observed in past tsunami events the mutual interaction between neighboring buildings can result in a shielding effect i e robust buildings could act as a shield for other ones leone et al 2011 reese et al 2011 tomiczek et al 2016 winter et al 2020 moris et al 2021 or conversely in a negative source of debris in the case of collapsed or washed away buildings charvet et al 2015 nistor et al 2017 in the complexity and heterogeneity of urban environments these factors can combine in different ways thus making the modelling of building vulnerability to tsunamis a rather arduous exercise exacerbated by the general limited availability of empirical data which are the basis for model development and or validation therefore complexity of involved damage phenomena and data availability are the main reasons behind the limited ability of most existing models in describing tsunami damage comprehensively with obvious potential repercussions on the results of risk analyses machine learning based methods although adopted in many fields of the scientific research gibert et al 2018 razavi 2021 have still found to date limited application for the development of empirical tsunami damage models indeed even where large empirical damage datasets are available these have been traditionally employed in simple regression models e g koshimura et al 2009 and suppasri et al 2011 for the 2004 indian ocean tsunami or suppasri et al 2013 2015 and leelawat et al 2014 for the 2011 great east japan tsunami that however as also pointed out by charvet et al 2015 may bring considerable uncertainty in damage estimation while data driven approaches for flood damage modelling have gained increasing traction in the last decade merz et al 2013 wagenaar et al 2017 schröter et al 2018 amadio et al 2019 only recently they started to receive attention for the case of tsunamis as shown by saengtabtim et al 2021 who analyzed damage data for 18 000 buildings hit by the 2011 tsunami in ishinomaki city by considering only inundation depth and flow velocity as the main variables for developing decision tree related algorithms in this context the present study aims at further exploiting data driven approaches for developing an empirical multi variable tsunami damage model for civil buildings based on the full dataset prepared by the japanese ministry of land infrastructure and transportation mlit 2012 after the 2011 great east japan tsunami containing micro scale information on the damage level and other explanatory parameters for about 250 000 affected buildings in the tōhoku area leelawat et al 2014 suppasri et al 2013 2015 the considered machine learning techniques characterized by the ability to capture nonlinear interactions among the descriptive features of the phenomenon will allow to analyze the relative importance of the different parameters at stake on modelling accuracy in particular the main novelty of this study is to evaluate besides traditional building vulnerability parameters the effect of other explanatory factors often described in the literature as potentially important but actually neglected in the development of existing damage models to this aim the original mlit database will be enhanced by implementing geospatial algorithms that will enable to calculate other geometric building related features i e shape factors or site related parameters as a function of building position and orientation with respect to the coastline as minimum distance direction of wave origin which will in turn be classified according to the coastal morphology ria or plain coast in addition an approach will be proposed to account for the effect of the reciprocal interaction between buildings and or singular structures with reference to the following aspects i the potential shielding effect exerted between interacting buildings which may reduce the impact of water ii the presence of collapsed buildings that may themselves be a source of debris and iii the presence of protective barriers as seawalls criteria for the selection of potentially interacting elements will be developed and analyzed by evaluating their influence on models performance finally this study will compare the predictive power of several models trained by considering different combinations of the input parameters classified in terms of their complexity for data retrieval or pre processing thus providing useful insights to address an efficient characterization of buildings vulnerability for tsunami damage modelling indeed since the characterization of the various input data may not always be possible in all contexts or cases e g due to the lack or difficulties in retrieving or pre processing specific local information from a practical perspective it is key to find an efficient trade off between predictive accuracy and complexity of input data collection i e input data cost 2 materials and methods 2 1 extended mlit dataset the main data source for this study was the database prepared by the japanese ministry of land infrastructure and transportation mlit 2012 after conducting fields surveys in the aftermath of the 2011 tsunami in the tōhoku region the dataset consists of a polygon shapefile containing micro scale i e building scale information on the degree of observed damage distinguished in 7 classes 1 no damage 2 minor damage 3 moderate damage 4 major damage 5 complete damage 6 collapse and 7 washed away and other explanatory variables for about 250 000 affected buildings the original data for each building include among the descriptive variables information on both hazard and vulnerability characteristics table 1 the former is represented by the inundation depth at the building location h and any other concurrent hazard factors othhaz e g earthquake fire liquefaction hokugo et al 2011 yamaguchi et al 2012 nishino et al 2015 while the latter include the structural type bs i e construction material and number of floors nf of the building its intended use use and the year of construction year detailed descriptions of each damage class and explanatory features of the mlit database can be found in previously published literature e g suppasri et al 2013 2015 charvet et al 2014 leelawat et al 2014 the original mlit database was then enriched by including other possible damage explanatory variables table 1 as described hereinafter geometric features of the buildings were determined based on straightforward geospatial operations on building polygons contained in the mlit database these included the attribution of the footprint area fa and the calculation of the following shape factors degree of compactness degcomp 4 π a p 2 where a and p are respectively the area and perimeter of the building polygon building orientation borient defined as the rotation of the reference system which minimizes the ratio between the projections of the building on a cartesian coordinate system computed with a 5 resolution fig 1 and a proxy for the shape elongation bsideratio dy dx coast related parameters were calculated by combining the mlit dataset with the polyline of the japanese shoreline the simplest one involved the assessment of the minimum distance between the centroid of each building and the coastline distance then the direction of the line of minimum distance was also considered as a proxy for the probable direction of tsunami wake attack wdir furthermore a coast type binary label was associated to each building based on its location and on the morphological characteristics of the coast 0 for plain and 1 for ria coast in particular the tōhoku region is characterized to have a ria coast in sanriku area in iwate prefecture and north of miyagi prefecture and a plain coast in the south of miyagi prefecture and fukushima prefecture suppasri et al 2013 2015 the influence of the mutual interaction between the individual buildings and the neighboring structures in terms of shielding effect and debris impact was quantitatively taken into account by defining a criterium for the selection of the potentially interacting elements based on the location of dispersed vessels naito et al 2014 assumed that debris propagates from its origin towards a direction perpendicular to the coast with a 45 spread angle which can also be larger in the areas close to the coast due to drawdown moreover it seems reasonable to assume that the area spanning from the considered building to the shoreline is the more interesting for both mechanisms as a consequence we defined and tested two different buffer geometries denoted as narrow n and large l hereinafter for the selection of the interacting elements with the extreme points represented by the centroid of the examined building and the nearest point on the shoreline fig 2 shows an example of the shapes of the different buffer types generated for a target building while analytical details on the construction of the two geometries is provided in the supplementary material the intersections between each building s buffer polygon n or l and the mlit shapefile was then used to compute the following synthetic parameters describing the mentioned mechanisms in particular coherently with charvet et al 2015 only washed away buildings in the intersections were considered as a potential source of debris to compute the following indices 1 l d i a r e a n d i a r e a i 1 n w a a i a b 2 l d i v o l n d i v o l i 1 n w a a i n f i a b where nwa ai and nfi are respectively the number of washed away buildings in the buffer the footprint area and the number of floors of the i th building in the buffer while ab is the area of the buffer polygon diarea calculated on l or n buffers is then an indicator of the areal density of washed away elements in the surrounding of the building of interest while eq 2 integrates this formulation by considering also the height of the buildings by means of nfi as a proxy for their volumetric density a similar approach was used also for the shielding effect with the only difference that in this case the total number of buildings in the buffer ntot was considered 3 l s h a r e a n s h a r e a i 1 n t o t a i a b 4 l s h v o l n s h v o l i 1 n t o t a i n f i a b when accounting for the shielding effect a question could arise whether all buildings in the buffers should be considered or the damaged ones of any damage level should be neglected from a physical perspective it is reasonable to assume that all buildings subtract momentum to the incoming floodwater but it is also true that collapsed ones may exert less resistance however since diarea is also accounted for from a modelling point of view there would be no difference among the two options because the footprint area of the less damaged buildings is exactly the difference between sharea and diarea finally the shielding action produced by seawalls was also taken into account by calculating the following index 5 l s w n s w i 1 n s w l i h i a b where nsw is the total number of seawall elements within the buffer while li and hi are respectively the length and the height of the i th element in the buffer to this aim li and hi were estimated by creating a digital file with the linear extension and corresponding height of seawalls and barriers based on information from ranghieri and ishiwatari 2014 integrated with interpretation of satellite images and virtual surveys by means of google street view 2 2 data handling about 1 of the data were kept out from the training procedure in order to test the spatial transferability of the developed models to this aim a spatial transferability test set was generated by extracting the data from a few municipalities characterized to be heterogeneously distributed across the impacted region and by having different hazard and vulnerability features the resulting subset for a total of 2 762 entries included the data for the municipalities of hashikami iwaizumi matsushima rifu and shinchi from the remaining 237 563 entries the 5 was holdout as a validation set for hyperparameter tuning section 2 3 and then again the remaining entries were split in training 95 and test set 5 in total about 11 of the data were excluded from the training set for validation and testing purposes all splittings were performed by stratifying the dataset according to the damage level to ensure similar distributions of the target feature furthermore as a final step the accuracy of trained models was tested on the spatial transferability test set in order to obtain more insights on their generalization ability which is recognized to be one of the main problems that empirical damage models face in their practical application amadio et al 2019 cerri et al 2021 lüdtke et al 2019 wagenaar et al 2021 2 3 implemented machine learning algorithms four different machine learning algorithms were selected to analyze the data with the first being a multi layer perceptron mlp and the others being based on decision trees with ensemble methods random forest rf extra trees xt also known as extremely randomized trees and extreme gradient boosting xgboost xgb the multi layer perceptron rumelhart et al 1986 here implemented by using the python library tensorflow abadi et al 2016 is an evolution of the classic linear perceptron which can tackle non linear problems by virtue of the presence of additional intermediate hidden layers all tree based algorithms were instead developed through the python libraries scikit learn pedregosa et al 2011 and xgboost chen and guestrin 2016 both random forest ho 1995 and extra trees geurts et al 2006 apply a bagging algorithm as ensemble method obtaining the classification result by taking into account the predictions of every tree by majority vote with slight differences in the growing tree procedure random forest selects the instances for each splitting using bootstrap replicas of the dataset and it defines the cut points in order to obtain the optimal split for each feature extra trees conversely uses the original entries of the dataset for each splitting node and randomly selects the cut points with lower computational costs under the same conditions after selecting the split points for the whole subset of features both algorithms select the optimal one xgboost chen and guestrin 2016 applies a gradient boosting algorithm as ensemble method when selecting the dataset instances for the splitting nodes for a new tree instead of a random selection only a subset from the residuals i e the misclassified entries of the previous tree is considered for the optimization also the prediction of each tree has its own weight and the optimal ones are determined with a gradient optimization process to minimize a loss function for each model a preliminary random search was performed to properly tune the hyperparameters and assess the variability in the validation accuracy for the mlp the tuning mainly focused on the number of layers and neurons per layer and the best results were achieved by using 3 hidden layers of 80 nodes with tanh as activation function and a last layer with 7 softmax nodes the maximum number of epochs for the training process was set at 300 with an early stopping after 10 epochs without improvements similarly an early stopping procedure was established to determine the number of trees for the xgboost model with the growth arrested after adding 10 trees without improvements with the maximum set at 300 and the depth of each tree limited to 9 the random forest was built with 150 fully grown trees using the shannon entropy as splitting evaluation criterium and 90 of the training entries to build the bootstrap replicas at each splitting for the extra trees classifier the number of trees was fixed at 130 and the splitting process was performed by evaluating the decrease in gini impurity on random subsets with 60 of the training entries the adoption of methods to contain the tree growth e g maximum depth minimum number of samples per leaf minimum required impurity decrease for splitting affected the validation accuracy for the last two models the metric chosen for the evaluation of models performance was the relative hit rate hr i e the ratio of the number of correct predictions to the total number of predictions moreover a normalized confusion matrix was computed for each model to have a more comprehensive picture of the error patterns i e to understand whether some damage classes are more commonly misclassified 2 4 assessment of the feature importance different approaches are available in the literature to obtain insights on the feature importance which consists in measuring the drop in the performance of the model when that feature is not considered in the training the more complete approach would be to train the model with all the available n features and then train it again with all possible combinations of n 1 features this can be time consuming when the number of features is large but examples exist with a limited number of input variables azmathullah et al 2005 azimi et al 2018 di bacco and scorzini 2019 for tree based ensemble methods a first simple variable importance measure is the number of times each feature is selected by the trees a more elaborate measure is obtained by computing a weighted average improvement caused by each variable in the splitting criterion of the trees friedman 2001 for classification ensembles the improvement is evaluated as gini impurity or entropy reduction while the variance reduction is considered for regression problems however besides being suitable only for tree based ensembles these methods tend to overestimate the importance of the features with a higher number of classes or higher number of different values when feature selection is based on impurity reduction strobl et al 2007 in this study we instead selected a more robust estimation of the feature importance based on the relative mean decrease accuracy mda which was computed by randomly shuffling each feature in turn breiman 2001 and then measuring the consequent variation in the accuracy accshuff with respect to the original one acc0 7 m d a a c c 0 a c c s h u f f a c c 0 in addition to the analysis of the feature importance it would be also interesting to investigate the sensitivity of models accuracy to the comprehensiveness and complexity of the input data considered in model development indeed in practical applications information for the characterization of the damage explanatory variables may not be always available in any regional context due to possible lack of appropriate data or considerable retrieval and or pre processing costs to this aim the performances for each of the four machine learning algorithms were tested by considering for the training process different combinations of the input parameters with increasing degree of data type complexity in detail we analyzed a total of 29 possible combinations which were classified into the following three macro groups fig 3 group a indicated in green in fig 3 in these runs only original mlit data and simple geometrical building features were used as input for the algorithms group b indicated in yellow in fig 3 these runs included other geometrical building features and simple coast related indicators group c indicated in red in fig 3 these combinations included also buffer related indices calculated on n or l geometries accounting for shielding mechanisms and debris impact 3 results and discussion 3 1 models accuracy and feature importance fig 4 summarizes the achieved accuracies in terms of relative hit rate hr for the four tested models trained with all the different feature combinations listed in fig 3 lower accuracies are obviously evident for the group a green runs with mlp and xgb performing slightly better hr 0 60 than rf and xt hr 0 51 for the simplest combination considering water depth and building footprint area as the only input parameters for all the tested algorithms the accuracy is observed to increase with models complexity i e with the number of considered variables although at different rates with xgb providing the best performances for all the group a combinations when coast related parameters in particular distance coasttype and wdir group b runs are included in the models the hr increases up to 0 70 for mlp which systematically reports the worst performances and to 0 74 for xt rf and xgb a further significant improvement hr 0 80 is achieved when considering more complex input feature combinations and especially those including buffer related parameters accounting for the debris impact and shielding effect thus supporting empirical observations on the importance of these mechanisms on tsunami damage leone et al 2011 reese et al 2011 charvet et al 2015 moris et al 2021 in addition to overall results confusion matrices by depicting the hit and misclassification rates among the different damage classes can provide a more detailed description of models accuracy however before discussing these results in detail it would be useful to analyze the correlation matrix of the features considered in the extended mlit dataset fig 5 in case of buffer related indicators calculated on l left or n right geometries given that any strong correlation among the explanatory variables should be reminded when interpreting accuracy and feature importance results as expected the damage class has a strong positive correlation with the inundation depth 0 66 and a negative correlation with the distance from the shoreline 0 51 it also shows a positive correlation 0 39 with coasttype coherently with the discussed amplifying effects occurring in ria coasts suppasri et al 2013 2015 leelawat et al 2014 de risi et al 2017 debris impact parameters also appear to have a strong positive correlation 0 45 with the damage revealing a possible relevant impact of this mechanism on damage occurrence however it should be reminded that these indicators were computed based on the presence of interacting washed away buildings resulting in a stronger correlation with h and damage differently from shielding effect indicators which instead exhibited very weak linear correlation with the two mentioned parameters the weak negative correlation between othhaz and damage is explained by the fact that occurrences in the dataset with othhaz 1 were mainly associated to buildings which experienced relatively shallow water depths and then minor damages fig 6 reports an example of the obtained confusion matrices for feature combinations c 19 and c 16 which are two of the most complete calculated on l buffer geometries with the four selected algorithms interestingly fig 6 indicates that damage classes 3 4 and 7 are the more easily recognized while class 5 is not only the one with the higher misclassification rates but also the less common guess for all models probably because it shares the lowest number of occurrences in the dataset along with class 1 as visible in fig 7 fig 6 also highlights that most of the misclassified buildings are still assigned to the two nearest classes thereby indicating reasonable accuracy even when the prediction is not exact the only exception is represented by buildings in class 5 which are observed to be more frequently assigned to class 7 rather than 4 or 6 in general the error distributions have been found to be model independent being qualitatively similar for all the considered algorithms this may suggest that identified patterns could have be driven also by uncertainties in the attribution of individual buildings to the specific damage classes in the post event field surveying phase with central classes being prone to larger uncertainties lin et al 2018 lorenzo and reuland 2019 the confusion matrices for feature combinations c 18 and c 15 similar to c 19 and c 16 but with buffer related indices calculated on n geometries are reported in the supplement fig s1 the comparison of fig 6 and fig s1 reveals very similar performances indicating that while the consideration of debris impact and shielding mechanisms are essential for model accuracy the choice of the buffer shape for calculating the related indicators between the two proposed in this study has only a slight influence an example of the results regarding the feature importance analysis is shown in fig 8 which describes the mean relative decrease in hit rate caused by the shuffling of each input feature for the most complete run c 16 averaged on 10 different shufflings the corresponding plots for the feature combination c 19 fig s2 together with those calculated for n geometries runs c 18 and c 15 are instead reported in the supplement figs s3 s4 the indications provided by fig 8 roughly confirm the overall results displayed in fig 4 as expected according to the relative mean decrease accuracy inundation depth is always by far the most significant explanatory variable together with other related parameters such as the distance from the shoreline and the debris impact indicators this highlights one more time the importance of relying on accurate representations of inundation scenarios when performing ex ante or ex post damage assessments scorzini et al 2022 interestingly although with almost no linear correlation with building damage or water depth fig 5 the proxies accounting for the shielding effect are identified among the key features thus confirming the importance of considering this mechanism when modelling tsunami damage other critical features are the proxy variable for the wave direction wdir the building structure bs and the indicator for the presence of seawalls lsw coasttype results to be one of the variables with higher mean decrease accuracy for mlp and extra trees but it appears to be less influent for rf and xgb this different behavior could be ascribed to the selection of the cut points during the splitting procedure which is completely random for xt and optimized in the other models with no difference for binary variables as a consequence xt should be relatively more prone to select binary features among the typical building vulnerability parameters use and year although sometimes indicated in the literature as descriptors for building resistance suppasri et al 2015 are here observed to be among the less important ones together with other shape parameters of the buildings othhaz also appears to be ranked in the last positions but probably due to the characteristics of the considered tsunami event where the inundation depth was the main driver for damage occurrence as highlighted in the correlation matrix in fig 5 it can be noted that the results for the most complete runs c 16 and c 19 with the latter including only the volumetric version of the buffer related indices are practically identical in terms of accuracy fig 6 given that due to the collinearity of ldiarea lsharea and ldivol lshvol the exclusion of one of them has little effect on the models performance because they can get the same information from the correlated feature this is also the reason explaining why the two added areal variables are still among the most important ones for combination c 16 even though they share their importance with the volumetric versions of the indicators fig 8 and s2 moreover the comparable feature importance depicted by fig 8 and s2 and by the corresponding figs s3 and s4 respectively for large and narrow buffer related indices confirms the limited influence of buffer geometry on the overall results 3 2 spatial transferability test table 2 summarizes the results of the spatial transferability analysis which consisted in testing the most complete and accurate rf xgb and xt models i e under feature combination c 16 on the data from the municipalities of hashikami iwaizumi matsushima rifu and shinchi that were excluded from the training procedure table 2 indicates a significant worsening of models performances on the five cities with the exception of shinchi where the hit rate was still reasonably high 0 73 0 76 the main reason for these heterogeneous results may be probably attributed to the different distributions of some key features of the datasets as shown in fig 7 for damage h and distance in particular the very low accuracies found for rifu and matsushima 0 3 0 5 can be clearly explained by the reported larger shares of low damages and practically no washed away buildings resulting from shallower inundation depths registered at these locations compared to those depicted in the whole dataset low accuracies are also evident for iwaizumi although being characterized by empirical cumulative distribution functions for h and distance similar to those of the main dataset however in this case the reason is to be found in the less dense urban fabric of the city as highlighted in fig 7d for lsharea and similarly in fig s5 for nsharea which is obviously a function of building density therefore this analysis confirms that consistency in input data is a critical requirement for the spatial transfer of damage models among regions amadio et al 2019 lüdtke et al 2019 cerri et al 2021 wagenaar et al 2021 which otherwise could negatively affect the predictive performance such results then suggest that data driven models to be applied in a more effective way should leverage on very numerous ex post building data capable of resembling the individual local hazard and vulnerability features of the different sites of model implementation otherwise they should be considered more suitable as descriptive models rather than for the predictive purposes however for homogeneous areas characterized by the availability of a sufficient number of empirical event data to avoid overfitting a feasible approach could be to train the models on subsets of these local data in order to have a faster calibration and higher similarity between the feature distributions of the training and test sets the suitability of this operation is clearly confirmed by the results reported in table 3 which shows as an example the performances of rf xgb and xt models under feature combination c 16 trained and tested using the same splitting procedure and tuning as performed with the whole dataset on the subsets of the extended mlit dataset based only on the city of ishinomaki for a total of 68 596 data the values of the global hit rate 0 89 as well as of its distribution among the damage classes confusion matrices in fig s6 are indeed found to be comparable or even higher than the ones of the main models but with lower computational time and no significant difference between validation and test accuracy 4 conclusions the present study focused on the implementation of machine learning approaches for tsunami damage modelling based on an enhanced version of building damage data from the 2011 great east japan event which involved the assessment and integration of usually neglected predictors accounting for building and site related features as well as mechanisms of building interaction like the shielding effect or debris impact from collapsed structures here modelled by proposing a new buffer based method compared to simpler and commonly used regression methods machine learning approaches can capture complex input output relationships without being constrained by a functional shape or by any a priori assumption on the statistical distribution of the data these features make them suitable tools for improving knowledge on physical mechanisms involved in tsunami damage and then for shedding light on the importance of the different variables at stake on the modelled phenomenon four algorithms have been tested and compared starting from simple multi layer perceptron neural networks mlp to decision tree based algorithms random forest rf and extra trees xt and boosting methods xgboost xgb in particular models accuracy in damage classification has been analyzed for different combinations of the explanatory variables which were distinguished based on their complexity for retrieval and or computation input data cost thus providing useful insights for the identification of key and ancillary variables to be used in tsunami damage modelling the results revealed that independently of the models and or input feature combinations especially for rf xt and xgb a critical role even more than typical vulnerability parameters included in standard fragility functions is played by the proposed indicators describing the mutual interaction between the inundated buildings which then should not be neglected when modelling tsunami damage furthermore while providing such new insights the developed models can be also considered as predictive tools to be used for damage assessment under potential simulated tsunami scenarios finally in order to assess the potential spatial transferability i e generalizability of the developed models their predictive ability on data kept out from the training dataset has been analyzed spatial transferability test set the results in line with similar studies carried out on flood damage modelling amadio et al 2019 lüdtke et al 2019 cerri et al 2021 wagenaar et al 2021 confirmed that a model transfer from one region to another often comes with a reduced predictive performance thus suggesting the need to rely on more specific local models as presented for the ishinomaki case able to give an adequate representation of the typical characteristics and damage processes occurring in the different areas software and data availability model development relies on the python programming language version 3 8 5 and open source libraries including numpy harris et al 2020 pandas mckinney 2010 scikit learn pedregosa et al 2011 tensorflow abadi et al 2016 and xgboost chen and guestrin 2016 the geospatial computations are performed with the open source software qgis 3 16 with the modules saga 7 8 2 and grass gis 7 8 6 the original mlit dataset is publicly available from the website of the ministry of land infrastructure and transportation of japan http www mlit go jp toshi toshi hukkou arkaibu html and related webgis https doi org 10 5638 thagis 21 87 http fukkou csis u tokyo ac jp for registered users the additional explanatory variables computed in this study are available on mendeley data doi 10 17632 8mf9rtvkhk 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105604 
25466,this study aims at developing an empirical multi variable tsunami damage model for buildings based on machine learning algorithms which leverage about 250 000 ex post data surveyed by the japanese ministry of land infrastructure and transportation after the 2011 great east japan event in the tōhoku region by implementing simple geospatial tools the dataset is integrated with additional explanatory variables including among others factors accounting for the mutual interaction between the inundated structures tests on models sensitivity to the number and type of input features used for model development reveal the importance on the predictive performance of considering usually neglected mechanisms like the shielding effect and the debris impact generation the analysis for the potential spatial transferability indicates a reduction in the accuracy thus suggesting a better suitability of empirical models for descriptive purposes limiting their predictive ability only to region specific cases keywords building damage tsunami machine learning feature importance spatial transferability japan data availability please refer to the software and data availability section of the paper 1 introduction the development of damage models enabling the estimation of the impacts generated by the occurrence of extreme natural events plays a key role in supporting decision making for the adoption of effective and efficient disaster management strategies the conventional tool used in tsunami risk modelling is represented by fragility curves which describe the probability of reaching or exceeding a certain damage state as a function of one or more event intensity parameters usually inundation depth and vulnerability characteristics of the exposed object these functions can be derived empirically based on post event observations or using synthetic expert based approaches tarbotton et al 2015 charvet et al 2017 the ability of a building to withstand the impact of a tsunami depends on several factors including the structural features and shape of the building its construction material the foundation type as well as the characteristics of the surrounding environment reese et al 2007 2011 kappes et al 2012 suppasri et al 2013 2015 charvet et al 2014 2015 2017 leelawat et al 2014 tarbotton et al 2015 dall osso et al 2016 regarding the latter factors coastal topography is expected to have a significant influence on tsunami damage due to possible amplifying effects occurring in ria coasts which would induce for a same value of the inundation depth higher damage levels than in plain areas suppasri et al 2013 2015 leelawat et al 2014 de risi et al 2017 in addition as observed in past tsunami events the mutual interaction between neighboring buildings can result in a shielding effect i e robust buildings could act as a shield for other ones leone et al 2011 reese et al 2011 tomiczek et al 2016 winter et al 2020 moris et al 2021 or conversely in a negative source of debris in the case of collapsed or washed away buildings charvet et al 2015 nistor et al 2017 in the complexity and heterogeneity of urban environments these factors can combine in different ways thus making the modelling of building vulnerability to tsunamis a rather arduous exercise exacerbated by the general limited availability of empirical data which are the basis for model development and or validation therefore complexity of involved damage phenomena and data availability are the main reasons behind the limited ability of most existing models in describing tsunami damage comprehensively with obvious potential repercussions on the results of risk analyses machine learning based methods although adopted in many fields of the scientific research gibert et al 2018 razavi 2021 have still found to date limited application for the development of empirical tsunami damage models indeed even where large empirical damage datasets are available these have been traditionally employed in simple regression models e g koshimura et al 2009 and suppasri et al 2011 for the 2004 indian ocean tsunami or suppasri et al 2013 2015 and leelawat et al 2014 for the 2011 great east japan tsunami that however as also pointed out by charvet et al 2015 may bring considerable uncertainty in damage estimation while data driven approaches for flood damage modelling have gained increasing traction in the last decade merz et al 2013 wagenaar et al 2017 schröter et al 2018 amadio et al 2019 only recently they started to receive attention for the case of tsunamis as shown by saengtabtim et al 2021 who analyzed damage data for 18 000 buildings hit by the 2011 tsunami in ishinomaki city by considering only inundation depth and flow velocity as the main variables for developing decision tree related algorithms in this context the present study aims at further exploiting data driven approaches for developing an empirical multi variable tsunami damage model for civil buildings based on the full dataset prepared by the japanese ministry of land infrastructure and transportation mlit 2012 after the 2011 great east japan tsunami containing micro scale information on the damage level and other explanatory parameters for about 250 000 affected buildings in the tōhoku area leelawat et al 2014 suppasri et al 2013 2015 the considered machine learning techniques characterized by the ability to capture nonlinear interactions among the descriptive features of the phenomenon will allow to analyze the relative importance of the different parameters at stake on modelling accuracy in particular the main novelty of this study is to evaluate besides traditional building vulnerability parameters the effect of other explanatory factors often described in the literature as potentially important but actually neglected in the development of existing damage models to this aim the original mlit database will be enhanced by implementing geospatial algorithms that will enable to calculate other geometric building related features i e shape factors or site related parameters as a function of building position and orientation with respect to the coastline as minimum distance direction of wave origin which will in turn be classified according to the coastal morphology ria or plain coast in addition an approach will be proposed to account for the effect of the reciprocal interaction between buildings and or singular structures with reference to the following aspects i the potential shielding effect exerted between interacting buildings which may reduce the impact of water ii the presence of collapsed buildings that may themselves be a source of debris and iii the presence of protective barriers as seawalls criteria for the selection of potentially interacting elements will be developed and analyzed by evaluating their influence on models performance finally this study will compare the predictive power of several models trained by considering different combinations of the input parameters classified in terms of their complexity for data retrieval or pre processing thus providing useful insights to address an efficient characterization of buildings vulnerability for tsunami damage modelling indeed since the characterization of the various input data may not always be possible in all contexts or cases e g due to the lack or difficulties in retrieving or pre processing specific local information from a practical perspective it is key to find an efficient trade off between predictive accuracy and complexity of input data collection i e input data cost 2 materials and methods 2 1 extended mlit dataset the main data source for this study was the database prepared by the japanese ministry of land infrastructure and transportation mlit 2012 after conducting fields surveys in the aftermath of the 2011 tsunami in the tōhoku region the dataset consists of a polygon shapefile containing micro scale i e building scale information on the degree of observed damage distinguished in 7 classes 1 no damage 2 minor damage 3 moderate damage 4 major damage 5 complete damage 6 collapse and 7 washed away and other explanatory variables for about 250 000 affected buildings the original data for each building include among the descriptive variables information on both hazard and vulnerability characteristics table 1 the former is represented by the inundation depth at the building location h and any other concurrent hazard factors othhaz e g earthquake fire liquefaction hokugo et al 2011 yamaguchi et al 2012 nishino et al 2015 while the latter include the structural type bs i e construction material and number of floors nf of the building its intended use use and the year of construction year detailed descriptions of each damage class and explanatory features of the mlit database can be found in previously published literature e g suppasri et al 2013 2015 charvet et al 2014 leelawat et al 2014 the original mlit database was then enriched by including other possible damage explanatory variables table 1 as described hereinafter geometric features of the buildings were determined based on straightforward geospatial operations on building polygons contained in the mlit database these included the attribution of the footprint area fa and the calculation of the following shape factors degree of compactness degcomp 4 π a p 2 where a and p are respectively the area and perimeter of the building polygon building orientation borient defined as the rotation of the reference system which minimizes the ratio between the projections of the building on a cartesian coordinate system computed with a 5 resolution fig 1 and a proxy for the shape elongation bsideratio dy dx coast related parameters were calculated by combining the mlit dataset with the polyline of the japanese shoreline the simplest one involved the assessment of the minimum distance between the centroid of each building and the coastline distance then the direction of the line of minimum distance was also considered as a proxy for the probable direction of tsunami wake attack wdir furthermore a coast type binary label was associated to each building based on its location and on the morphological characteristics of the coast 0 for plain and 1 for ria coast in particular the tōhoku region is characterized to have a ria coast in sanriku area in iwate prefecture and north of miyagi prefecture and a plain coast in the south of miyagi prefecture and fukushima prefecture suppasri et al 2013 2015 the influence of the mutual interaction between the individual buildings and the neighboring structures in terms of shielding effect and debris impact was quantitatively taken into account by defining a criterium for the selection of the potentially interacting elements based on the location of dispersed vessels naito et al 2014 assumed that debris propagates from its origin towards a direction perpendicular to the coast with a 45 spread angle which can also be larger in the areas close to the coast due to drawdown moreover it seems reasonable to assume that the area spanning from the considered building to the shoreline is the more interesting for both mechanisms as a consequence we defined and tested two different buffer geometries denoted as narrow n and large l hereinafter for the selection of the interacting elements with the extreme points represented by the centroid of the examined building and the nearest point on the shoreline fig 2 shows an example of the shapes of the different buffer types generated for a target building while analytical details on the construction of the two geometries is provided in the supplementary material the intersections between each building s buffer polygon n or l and the mlit shapefile was then used to compute the following synthetic parameters describing the mentioned mechanisms in particular coherently with charvet et al 2015 only washed away buildings in the intersections were considered as a potential source of debris to compute the following indices 1 l d i a r e a n d i a r e a i 1 n w a a i a b 2 l d i v o l n d i v o l i 1 n w a a i n f i a b where nwa ai and nfi are respectively the number of washed away buildings in the buffer the footprint area and the number of floors of the i th building in the buffer while ab is the area of the buffer polygon diarea calculated on l or n buffers is then an indicator of the areal density of washed away elements in the surrounding of the building of interest while eq 2 integrates this formulation by considering also the height of the buildings by means of nfi as a proxy for their volumetric density a similar approach was used also for the shielding effect with the only difference that in this case the total number of buildings in the buffer ntot was considered 3 l s h a r e a n s h a r e a i 1 n t o t a i a b 4 l s h v o l n s h v o l i 1 n t o t a i n f i a b when accounting for the shielding effect a question could arise whether all buildings in the buffers should be considered or the damaged ones of any damage level should be neglected from a physical perspective it is reasonable to assume that all buildings subtract momentum to the incoming floodwater but it is also true that collapsed ones may exert less resistance however since diarea is also accounted for from a modelling point of view there would be no difference among the two options because the footprint area of the less damaged buildings is exactly the difference between sharea and diarea finally the shielding action produced by seawalls was also taken into account by calculating the following index 5 l s w n s w i 1 n s w l i h i a b where nsw is the total number of seawall elements within the buffer while li and hi are respectively the length and the height of the i th element in the buffer to this aim li and hi were estimated by creating a digital file with the linear extension and corresponding height of seawalls and barriers based on information from ranghieri and ishiwatari 2014 integrated with interpretation of satellite images and virtual surveys by means of google street view 2 2 data handling about 1 of the data were kept out from the training procedure in order to test the spatial transferability of the developed models to this aim a spatial transferability test set was generated by extracting the data from a few municipalities characterized to be heterogeneously distributed across the impacted region and by having different hazard and vulnerability features the resulting subset for a total of 2 762 entries included the data for the municipalities of hashikami iwaizumi matsushima rifu and shinchi from the remaining 237 563 entries the 5 was holdout as a validation set for hyperparameter tuning section 2 3 and then again the remaining entries were split in training 95 and test set 5 in total about 11 of the data were excluded from the training set for validation and testing purposes all splittings were performed by stratifying the dataset according to the damage level to ensure similar distributions of the target feature furthermore as a final step the accuracy of trained models was tested on the spatial transferability test set in order to obtain more insights on their generalization ability which is recognized to be one of the main problems that empirical damage models face in their practical application amadio et al 2019 cerri et al 2021 lüdtke et al 2019 wagenaar et al 2021 2 3 implemented machine learning algorithms four different machine learning algorithms were selected to analyze the data with the first being a multi layer perceptron mlp and the others being based on decision trees with ensemble methods random forest rf extra trees xt also known as extremely randomized trees and extreme gradient boosting xgboost xgb the multi layer perceptron rumelhart et al 1986 here implemented by using the python library tensorflow abadi et al 2016 is an evolution of the classic linear perceptron which can tackle non linear problems by virtue of the presence of additional intermediate hidden layers all tree based algorithms were instead developed through the python libraries scikit learn pedregosa et al 2011 and xgboost chen and guestrin 2016 both random forest ho 1995 and extra trees geurts et al 2006 apply a bagging algorithm as ensemble method obtaining the classification result by taking into account the predictions of every tree by majority vote with slight differences in the growing tree procedure random forest selects the instances for each splitting using bootstrap replicas of the dataset and it defines the cut points in order to obtain the optimal split for each feature extra trees conversely uses the original entries of the dataset for each splitting node and randomly selects the cut points with lower computational costs under the same conditions after selecting the split points for the whole subset of features both algorithms select the optimal one xgboost chen and guestrin 2016 applies a gradient boosting algorithm as ensemble method when selecting the dataset instances for the splitting nodes for a new tree instead of a random selection only a subset from the residuals i e the misclassified entries of the previous tree is considered for the optimization also the prediction of each tree has its own weight and the optimal ones are determined with a gradient optimization process to minimize a loss function for each model a preliminary random search was performed to properly tune the hyperparameters and assess the variability in the validation accuracy for the mlp the tuning mainly focused on the number of layers and neurons per layer and the best results were achieved by using 3 hidden layers of 80 nodes with tanh as activation function and a last layer with 7 softmax nodes the maximum number of epochs for the training process was set at 300 with an early stopping after 10 epochs without improvements similarly an early stopping procedure was established to determine the number of trees for the xgboost model with the growth arrested after adding 10 trees without improvements with the maximum set at 300 and the depth of each tree limited to 9 the random forest was built with 150 fully grown trees using the shannon entropy as splitting evaluation criterium and 90 of the training entries to build the bootstrap replicas at each splitting for the extra trees classifier the number of trees was fixed at 130 and the splitting process was performed by evaluating the decrease in gini impurity on random subsets with 60 of the training entries the adoption of methods to contain the tree growth e g maximum depth minimum number of samples per leaf minimum required impurity decrease for splitting affected the validation accuracy for the last two models the metric chosen for the evaluation of models performance was the relative hit rate hr i e the ratio of the number of correct predictions to the total number of predictions moreover a normalized confusion matrix was computed for each model to have a more comprehensive picture of the error patterns i e to understand whether some damage classes are more commonly misclassified 2 4 assessment of the feature importance different approaches are available in the literature to obtain insights on the feature importance which consists in measuring the drop in the performance of the model when that feature is not considered in the training the more complete approach would be to train the model with all the available n features and then train it again with all possible combinations of n 1 features this can be time consuming when the number of features is large but examples exist with a limited number of input variables azmathullah et al 2005 azimi et al 2018 di bacco and scorzini 2019 for tree based ensemble methods a first simple variable importance measure is the number of times each feature is selected by the trees a more elaborate measure is obtained by computing a weighted average improvement caused by each variable in the splitting criterion of the trees friedman 2001 for classification ensembles the improvement is evaluated as gini impurity or entropy reduction while the variance reduction is considered for regression problems however besides being suitable only for tree based ensembles these methods tend to overestimate the importance of the features with a higher number of classes or higher number of different values when feature selection is based on impurity reduction strobl et al 2007 in this study we instead selected a more robust estimation of the feature importance based on the relative mean decrease accuracy mda which was computed by randomly shuffling each feature in turn breiman 2001 and then measuring the consequent variation in the accuracy accshuff with respect to the original one acc0 7 m d a a c c 0 a c c s h u f f a c c 0 in addition to the analysis of the feature importance it would be also interesting to investigate the sensitivity of models accuracy to the comprehensiveness and complexity of the input data considered in model development indeed in practical applications information for the characterization of the damage explanatory variables may not be always available in any regional context due to possible lack of appropriate data or considerable retrieval and or pre processing costs to this aim the performances for each of the four machine learning algorithms were tested by considering for the training process different combinations of the input parameters with increasing degree of data type complexity in detail we analyzed a total of 29 possible combinations which were classified into the following three macro groups fig 3 group a indicated in green in fig 3 in these runs only original mlit data and simple geometrical building features were used as input for the algorithms group b indicated in yellow in fig 3 these runs included other geometrical building features and simple coast related indicators group c indicated in red in fig 3 these combinations included also buffer related indices calculated on n or l geometries accounting for shielding mechanisms and debris impact 3 results and discussion 3 1 models accuracy and feature importance fig 4 summarizes the achieved accuracies in terms of relative hit rate hr for the four tested models trained with all the different feature combinations listed in fig 3 lower accuracies are obviously evident for the group a green runs with mlp and xgb performing slightly better hr 0 60 than rf and xt hr 0 51 for the simplest combination considering water depth and building footprint area as the only input parameters for all the tested algorithms the accuracy is observed to increase with models complexity i e with the number of considered variables although at different rates with xgb providing the best performances for all the group a combinations when coast related parameters in particular distance coasttype and wdir group b runs are included in the models the hr increases up to 0 70 for mlp which systematically reports the worst performances and to 0 74 for xt rf and xgb a further significant improvement hr 0 80 is achieved when considering more complex input feature combinations and especially those including buffer related parameters accounting for the debris impact and shielding effect thus supporting empirical observations on the importance of these mechanisms on tsunami damage leone et al 2011 reese et al 2011 charvet et al 2015 moris et al 2021 in addition to overall results confusion matrices by depicting the hit and misclassification rates among the different damage classes can provide a more detailed description of models accuracy however before discussing these results in detail it would be useful to analyze the correlation matrix of the features considered in the extended mlit dataset fig 5 in case of buffer related indicators calculated on l left or n right geometries given that any strong correlation among the explanatory variables should be reminded when interpreting accuracy and feature importance results as expected the damage class has a strong positive correlation with the inundation depth 0 66 and a negative correlation with the distance from the shoreline 0 51 it also shows a positive correlation 0 39 with coasttype coherently with the discussed amplifying effects occurring in ria coasts suppasri et al 2013 2015 leelawat et al 2014 de risi et al 2017 debris impact parameters also appear to have a strong positive correlation 0 45 with the damage revealing a possible relevant impact of this mechanism on damage occurrence however it should be reminded that these indicators were computed based on the presence of interacting washed away buildings resulting in a stronger correlation with h and damage differently from shielding effect indicators which instead exhibited very weak linear correlation with the two mentioned parameters the weak negative correlation between othhaz and damage is explained by the fact that occurrences in the dataset with othhaz 1 were mainly associated to buildings which experienced relatively shallow water depths and then minor damages fig 6 reports an example of the obtained confusion matrices for feature combinations c 19 and c 16 which are two of the most complete calculated on l buffer geometries with the four selected algorithms interestingly fig 6 indicates that damage classes 3 4 and 7 are the more easily recognized while class 5 is not only the one with the higher misclassification rates but also the less common guess for all models probably because it shares the lowest number of occurrences in the dataset along with class 1 as visible in fig 7 fig 6 also highlights that most of the misclassified buildings are still assigned to the two nearest classes thereby indicating reasonable accuracy even when the prediction is not exact the only exception is represented by buildings in class 5 which are observed to be more frequently assigned to class 7 rather than 4 or 6 in general the error distributions have been found to be model independent being qualitatively similar for all the considered algorithms this may suggest that identified patterns could have be driven also by uncertainties in the attribution of individual buildings to the specific damage classes in the post event field surveying phase with central classes being prone to larger uncertainties lin et al 2018 lorenzo and reuland 2019 the confusion matrices for feature combinations c 18 and c 15 similar to c 19 and c 16 but with buffer related indices calculated on n geometries are reported in the supplement fig s1 the comparison of fig 6 and fig s1 reveals very similar performances indicating that while the consideration of debris impact and shielding mechanisms are essential for model accuracy the choice of the buffer shape for calculating the related indicators between the two proposed in this study has only a slight influence an example of the results regarding the feature importance analysis is shown in fig 8 which describes the mean relative decrease in hit rate caused by the shuffling of each input feature for the most complete run c 16 averaged on 10 different shufflings the corresponding plots for the feature combination c 19 fig s2 together with those calculated for n geometries runs c 18 and c 15 are instead reported in the supplement figs s3 s4 the indications provided by fig 8 roughly confirm the overall results displayed in fig 4 as expected according to the relative mean decrease accuracy inundation depth is always by far the most significant explanatory variable together with other related parameters such as the distance from the shoreline and the debris impact indicators this highlights one more time the importance of relying on accurate representations of inundation scenarios when performing ex ante or ex post damage assessments scorzini et al 2022 interestingly although with almost no linear correlation with building damage or water depth fig 5 the proxies accounting for the shielding effect are identified among the key features thus confirming the importance of considering this mechanism when modelling tsunami damage other critical features are the proxy variable for the wave direction wdir the building structure bs and the indicator for the presence of seawalls lsw coasttype results to be one of the variables with higher mean decrease accuracy for mlp and extra trees but it appears to be less influent for rf and xgb this different behavior could be ascribed to the selection of the cut points during the splitting procedure which is completely random for xt and optimized in the other models with no difference for binary variables as a consequence xt should be relatively more prone to select binary features among the typical building vulnerability parameters use and year although sometimes indicated in the literature as descriptors for building resistance suppasri et al 2015 are here observed to be among the less important ones together with other shape parameters of the buildings othhaz also appears to be ranked in the last positions but probably due to the characteristics of the considered tsunami event where the inundation depth was the main driver for damage occurrence as highlighted in the correlation matrix in fig 5 it can be noted that the results for the most complete runs c 16 and c 19 with the latter including only the volumetric version of the buffer related indices are practically identical in terms of accuracy fig 6 given that due to the collinearity of ldiarea lsharea and ldivol lshvol the exclusion of one of them has little effect on the models performance because they can get the same information from the correlated feature this is also the reason explaining why the two added areal variables are still among the most important ones for combination c 16 even though they share their importance with the volumetric versions of the indicators fig 8 and s2 moreover the comparable feature importance depicted by fig 8 and s2 and by the corresponding figs s3 and s4 respectively for large and narrow buffer related indices confirms the limited influence of buffer geometry on the overall results 3 2 spatial transferability test table 2 summarizes the results of the spatial transferability analysis which consisted in testing the most complete and accurate rf xgb and xt models i e under feature combination c 16 on the data from the municipalities of hashikami iwaizumi matsushima rifu and shinchi that were excluded from the training procedure table 2 indicates a significant worsening of models performances on the five cities with the exception of shinchi where the hit rate was still reasonably high 0 73 0 76 the main reason for these heterogeneous results may be probably attributed to the different distributions of some key features of the datasets as shown in fig 7 for damage h and distance in particular the very low accuracies found for rifu and matsushima 0 3 0 5 can be clearly explained by the reported larger shares of low damages and practically no washed away buildings resulting from shallower inundation depths registered at these locations compared to those depicted in the whole dataset low accuracies are also evident for iwaizumi although being characterized by empirical cumulative distribution functions for h and distance similar to those of the main dataset however in this case the reason is to be found in the less dense urban fabric of the city as highlighted in fig 7d for lsharea and similarly in fig s5 for nsharea which is obviously a function of building density therefore this analysis confirms that consistency in input data is a critical requirement for the spatial transfer of damage models among regions amadio et al 2019 lüdtke et al 2019 cerri et al 2021 wagenaar et al 2021 which otherwise could negatively affect the predictive performance such results then suggest that data driven models to be applied in a more effective way should leverage on very numerous ex post building data capable of resembling the individual local hazard and vulnerability features of the different sites of model implementation otherwise they should be considered more suitable as descriptive models rather than for the predictive purposes however for homogeneous areas characterized by the availability of a sufficient number of empirical event data to avoid overfitting a feasible approach could be to train the models on subsets of these local data in order to have a faster calibration and higher similarity between the feature distributions of the training and test sets the suitability of this operation is clearly confirmed by the results reported in table 3 which shows as an example the performances of rf xgb and xt models under feature combination c 16 trained and tested using the same splitting procedure and tuning as performed with the whole dataset on the subsets of the extended mlit dataset based only on the city of ishinomaki for a total of 68 596 data the values of the global hit rate 0 89 as well as of its distribution among the damage classes confusion matrices in fig s6 are indeed found to be comparable or even higher than the ones of the main models but with lower computational time and no significant difference between validation and test accuracy 4 conclusions the present study focused on the implementation of machine learning approaches for tsunami damage modelling based on an enhanced version of building damage data from the 2011 great east japan event which involved the assessment and integration of usually neglected predictors accounting for building and site related features as well as mechanisms of building interaction like the shielding effect or debris impact from collapsed structures here modelled by proposing a new buffer based method compared to simpler and commonly used regression methods machine learning approaches can capture complex input output relationships without being constrained by a functional shape or by any a priori assumption on the statistical distribution of the data these features make them suitable tools for improving knowledge on physical mechanisms involved in tsunami damage and then for shedding light on the importance of the different variables at stake on the modelled phenomenon four algorithms have been tested and compared starting from simple multi layer perceptron neural networks mlp to decision tree based algorithms random forest rf and extra trees xt and boosting methods xgboost xgb in particular models accuracy in damage classification has been analyzed for different combinations of the explanatory variables which were distinguished based on their complexity for retrieval and or computation input data cost thus providing useful insights for the identification of key and ancillary variables to be used in tsunami damage modelling the results revealed that independently of the models and or input feature combinations especially for rf xt and xgb a critical role even more than typical vulnerability parameters included in standard fragility functions is played by the proposed indicators describing the mutual interaction between the inundated buildings which then should not be neglected when modelling tsunami damage furthermore while providing such new insights the developed models can be also considered as predictive tools to be used for damage assessment under potential simulated tsunami scenarios finally in order to assess the potential spatial transferability i e generalizability of the developed models their predictive ability on data kept out from the training dataset has been analyzed spatial transferability test set the results in line with similar studies carried out on flood damage modelling amadio et al 2019 lüdtke et al 2019 cerri et al 2021 wagenaar et al 2021 confirmed that a model transfer from one region to another often comes with a reduced predictive performance thus suggesting the need to rely on more specific local models as presented for the ishinomaki case able to give an adequate representation of the typical characteristics and damage processes occurring in the different areas software and data availability model development relies on the python programming language version 3 8 5 and open source libraries including numpy harris et al 2020 pandas mckinney 2010 scikit learn pedregosa et al 2011 tensorflow abadi et al 2016 and xgboost chen and guestrin 2016 the geospatial computations are performed with the open source software qgis 3 16 with the modules saga 7 8 2 and grass gis 7 8 6 the original mlit dataset is publicly available from the website of the ministry of land infrastructure and transportation of japan http www mlit go jp toshi toshi hukkou arkaibu html and related webgis https doi org 10 5638 thagis 21 87 http fukkou csis u tokyo ac jp for registered users the additional explanatory variables computed in this study are available on mendeley data doi 10 17632 8mf9rtvkhk 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105604 
25467,a new modeling platform was developed to simulate spatially distributed sediment production and transport in agricultural landscapes the sediment production at the grid scale was computed using the stand alone water erosion prediction project hillslope erosion code while an advection dispersion equation represented sediment transport the model s performance was tested in the water resources and erosion watersheds at the oklahoma and central plains agricultural research center el reno ok usa results showed that this modeling approach could capture the complex behavior of sediment under different management practices in the watersheds the modeling framework provides mechanistic processes to simulate the fate and transport of sediment across the watershed at spatio temporal scales that are useful in assessing the environmental impacts of management schemes therefore this study is expected to help advance the current approaches to estimating soil erosion by bridging scale differences to capture the large scale effects of small scale soil erosion processes keywords soil erosion sediment transport wepp advection dispersion equation wre watersheds data availability i included software availabilitly in the manuscript 1 introduction excessive soil erosion can result in land degradation in agricultural fields and hence a major threat to food supply and water quality luetzenburg et al 2020 montanarella et al 2016 parsons 2019 model simulations of climate change coupled with expected intensive land cultivation suggest that over the next 50 years soil erosion by water could significantly increase by 30 66 worldwide borrelli et al 2020 such increases in soil erosion are expected to result in high economic and societal costs therefore reliable information on soil erosion should be developed to assist in land use and policy decisions at individual field and catchment scales fu et al 2019 momm et al 2019 parsons 2019 the number of computer models for sediment predictions has rapidly increased in recent decades due to the increasing use of computer applications and computing power measurements of sediment in agricultural landscapes are scarce and hard to obtain highlighting the importance of using computer models to overcome this limitation however due to simplifying assumptions and parameter values often based on local conditions many models still have a high degree of uncertainty hajigholizadeh et al 2018 this is especially true regarding the spatiotemporal dynamics of soil erosion by water which are often misrepresented and not well quantified when the erosion processes are conceptualized in transport models at a watershed scale wainwright et al 2015 for example the soil and water assessment tool swat arnold et al 1998 2012 and agricultural policy environmental extender model apex williams et al 2000 models have been widely used in recent national conservation effects assessments by the us department of agriculture usda natural resources conservation service nrcs and agricultural research service ars flanagan et al 2020 however these models have their limitations in terms of sediment transport simulations for instance the swat model divides sediment transport calculations into overland and river flow and the sediment that reaches the stream network minus a lag is typically estimated by the modified universal soil loss equation musle williams and berndt 1977 and only considers temporary retention of sediment similarly the apex model estimates soil erosion using the universal soil loss equation usle and its derivatives wischmeier and smith 1978 and it has a limited ability to simulate the effects of changing management bhandari et al 2017 even though soil erosion can occur at various scales most of the models available currently evaluate soil loss as a lumped system e g sub catchment levels assuming average values of the watershed characteristics meinen and robinson 2021 dabney et al 2015 introduced a spatially distributed adaptation of the revised universal soil loss equation rusle however the adaptation relied on the empirical foundation and thus the model cannot explain the physical phenomena of soil erosion to overcome these issues process based models have been developed that combine the governing equations of conservation of mass and momentum to describe water and sediment fluxes examples of such models include the areal nonpoint source watershed environment response simulation answers model beasley et al 1980 the chemicals runoff and erosion from agricultural management systems creams model knisel 1980 and the water erosion prediction project wepp model flanagan and nearing 1995 however a common limitation of these models is that they were conceptualized to represent soil loss at the hillslope or small watershed scale baffaut et al 1997 miller et al 2011 this limitation is especially crucial since the quantification of soil erosion is sensitive to the spatial scale of process representation lee et al 2021a and sediment transport for assessment of conservation strategies is typically conducted at the regional scale therefore a watershed scale model that can capture the spatiotemporal dynamics of soil erosion and that has the capacity to integrate process representation at a higher spatial resolution for resource and environmental management is needed understanding the physical processes of suspended sediment in agricultural landscapes is important for the soil and water conservation quantification needed to evaluate and develop sustainable agricultural production systems inaccurate sediment prediction can arise from the failure to capture the temporal and spatial heterogeneities of input data and parameters used in the model or the failure to represent the physical phenomena of detachment transport and deposition of soil particles fischer et al 2018 to minimize the impact of these issues temporal and spatial information on soil erosion processes is required hajigholizadeh et al 2018 kuznetsov et al 1998 in this regard a distributed model e g grid based is an effective approach for resolving spatial distributions of environmental data and physical processes of soil erosion lee et al 2021a in light of the need to provide a holistic view of sediment processes in watershed management planning the main objective of this study was to develop an integrated modeling framework that can evaluate distributed sediment production and its propagation in a fully distributed setting herein we describe a model framework that was developed to seamlessly integrate field scale erosion processes and a sediment transport concept based on the advection dispersion equation in a fully distributed setting the model was evaluated using measured runoff depth and sediment load from the water resources and erosion wre experimental watersheds managed by the usda ars oklahoma and central plains agricultural research center el reno ok usa 2 model development 2 1 overview of sediment transport model the main aim of the developed sediment transport model is to provide reliable distributed sediment production i e sediment source and to simulate the fate and transport of the eroded soils in agricultural landscapes the model domain was discretized into rectangular grid cells and the distributed sediment source was estimated based on a stand alone water erosion prediction project hillslope erosion wepp he code the wepp he code was extracted from the full wepp v2004 7 model flanagan et al 2005 containing only erosion calculations at a hillslope scale the wepp he code was seamlessly integrated into the developed model to provide daily soil loss for each grid cell across the model domain and partition sediment predictions into five different sediment classes table 1 and fig 1 several input parameters such as sub daily precipitation and runoff characteristics should be provided fig 1 each grid cell is conceptualized as a single hillslope component having different input data such as soil properties and hydrologic variables to execute the wepp he code to simulate the fate and transport of the eroded soil between grid cells a two dimensional advection dispersion equation with sink and source terms was numerically solved for each sediment class in the sediment transport model fig 1 the sediment transport model stm requires inputs from a hydrologic model in terms of runoff depth and velocity fields in the x and y directions fig 1 once the wepp he computes the distributed sediment source for each sediment class the stm separately simulates the transport of the eroded sediment for each sediment class across the model domain using the provided hydrologic variables fig 1 2 2 estimation of sediment production the wepp model simulates hillslopes and small watersheds flanagan and nearing 1995 in this study instead of using the complete wepp hillslope model the stand alone wepp he code was used together with the sediment transport model the wepp he code includes only the erosion computations at hillslope scales and excludes the rest of the components used in the full wepp model at the hillslope scale it estimates interrill and rill erosion and sediment delivery spatially down a profile interrill erosion is defined as soil movement that occurs due to the impact of raindrops and sheet flow while rill erosion occurs when runoff forms small channels as it concentrates down a hillslope ranges tens of meters this model provides reliable estimates for soil loss in a broad range of climate and soil conditions laflen et al 2004 tiwari et al 2000 zhang et al 1996 the wepp he code can be executed with the hydrologic values of runoff depth sub daily precipitation and runoff characteristics as the main inputs the runoff characteristics are peak runoff rate and effective runoff duration the runoff depth divided by peak runoff rate in a given day other input variables include soil texture information canopy height adjusted interrill and rill erodibilities critical shear stress friction factor values and slope fig 1 for sub daily hydrologic variables a continuous single rainfall and runoff event was assumed to occur on a given day in addition each grid cell is conceptualized as a unique combination of topography soil properties land use and management practice sediment predictions for each grid cell were partitioned into five sediment classes table 1 and these values were separately added to a source term of the advection dispersion equations across the model domain in the sediment transport model fig 1 the fractions of each sediment class were estimated from the distribution of primary particles e g clay silt and sand in the topsoil layer based on the derivations of foster et al 1985 the wepp he was verified against the full wepp model in predicting soil loss flanagan et al 2005 and has shown its functionality when coupled with a large scale hydrologic model mao et al 2010 in the wepp he model the movement of sediment at hillslope scales is described by 1 d e d x d i n t e r r i l l d r i l l where x is the distance down the slope m e is sediment load per unit width of the hillslope kg s 1 m 1 d i n t e r r i l l is interrill erosion rate kg s 1 m 2 and d r i l l is rill erosion rate kg s 1 m 2 foster et al 1995 the interrill erosion rate is calculated by 2 d i n t e r r i l l k i a d j i e o i r s d r r r f n o z z l e r s w where k i a d j is the adjusted interrill erodibility factor kg s m 4 i e is the effective rainfall intensity m s 1 o i r is the interrill runoff rate m s 1 s d r r r is a sediment delivery ratio f n o z z l e is an adjustment factor to account for irrigation water drop impact energy and the default value of 1 was assigned in the model r s is the rill spacing m and w is the rill width m the net soil detachment in rills is then estimated by 3 d r i l l k r a d j τ f τ c a d j 1 g t c where k r a d j is the adjusted rill erodibility parameter s m 1 τ f is the flow shear stress acting on the soil particles pa τ c a d j is the adjusted critical shear stress of the soil pa and t c is the sediment transport capacity in the rill kg s 1 m 1 note that the net soil detachment only occurs when t c and τ f are greater than g and τ c a d j respectively otherwise net deposition occurs as a function of the settling velocity of the soil particles to calculate the deposition rate foster et al 1995 to provide the sediment source for each grid cell a single grid cell was conceptualized as a single hillslope across the model domain and daily values of the estimated sediment yield at the outlet of the hillslope were added as a source term in the advection dispersion equation 2 3 simulation of sediment transport sediment transport in overland flow was described using an explicit finite difference solution to the advection dispersion equations when ponded water exists in overland flow i e soil profile is inundated when the water table is above the soil surface the two dimensional 2d multiclass sediment transport advection dispersion equations with source and sink terms were applied for five sediment classes eq 4 the suspended sediment was assumed to be completely mixed over the depth inflow and the five sediment classes behaved independently 4 c i t x c i u y c i v x ε s c i x y ε s c i y e i d i where c i is the depth averaged sediment concentration for sediment class i t is the time x and y are the spatial coordinates u and v are the velocities in the x and y directions ε s is the longitudinal diffusion coefficient and e i and d i are the erosion and deposition rates respectively the governing equation was discretized using the explicit finite difference method with central differencing for diffusion terms and the lax and wendroff 1960 scheme for advection terms the sediment source estimated from the wepp he eq 1 was added to the erosion term e i for each grid cell while the multiclass deposition term d i was estimated by ockenden 1993 teeter 2000a 2000b 5 d i w i c i h 1 τ b τ c r i f o r τ b τ c r i where w i is the settling velocity for each sediment class h is the water depth τ b is the bed shear stress and τ c r i is the critical shear stress deposition only occurs when the bed shear stress is less than the critical shear stress and if the water depth is less than detention storage of the surface in eq 5 the settling velocity w i and critical shear stress τ c i were estimated considering each particle size table 1 dietrich 1982 derived the dimensionless relationships between the settling velocity and particle reynolds number r e p for natural particles as 6 w i g d ρ s ρ w ρ w exp b 1 b 2 ln r e p b 3 ln r e p 2 b 4 ln r e p 3 b 5 ln r e p 4 where r e p ρ s ρ w ρ w g d d ν ρ s is the particle density and ρ w is the water density ν is the kinematic viscosity d is the particle diameter b 1 is 2 891394 b 2 is 0 95296 b 3 is 0 056835 b 4 is 0 002892 and b 5 is 0 000245 brownlie s 1981 fitted curve which is based on the experimental data of the shields curve is widely used to determine the dimensionless critical shear stress τ c r i furthermore parker et al 2003 amended brownlie s expression to adjust τ c r i to 0 03 for a large reynolds number and this relation was used to estimate τ c r i as follows 7 τ c r i 0 5 0 22 r e p 0 6 0 06 exp 17 77 r e p 0 6 based on eq 7 critical shear stress τ c r i can be estimated by 8 τ c r i ρ s ρ w ρ w g d τ c r i the relation of the longitudinal diffusion coefficients for the depth averaged model ε s can be derived assuming the balance between diffusivity and eddy viscosity prandtl analogy 9 ε s κ 6 u h 1 15 u h where κ is the von karman s constant 0 41 and u is the shear velocity this relation is known to provide a good approximation to estimate the longitudinal dispersion of fine grained sediment garcia 2008 huang and garcia 2000 the needed hydrologic variables to simulate the sediment transport models are water depth and velocity fields in the x and y directions in overland flow and these variables were extracted from the simulation results from the mike she model abbott et al 1986 a fully distributed and integrated surface and subsurface model refsgaard and storm 1995 this model can simulate the components of overland flow evapotranspiration unsaturated flow and groundwater flow at watershed scales note that any other distributed hydrologic model can be coupled to this transport model as long as the model can provide the needed hydrologic variables 3 model evaluation 3 1 study area the stm model was evaluated in two of the eight water resources and erosion watersheds wre6 and wre7 fig 2 located at the usda ars oklahoma and central plains agricultural research center usda ars ocparc in el reno oklahoma these watersheds were established in 1976 by the usda ars ocparc to study the long term impacts of managing practices in water quality and sediment in agricultural landscapes nelson et al 2020 the climate of the study area is semiarid to sub humid climate conditions receiving annual precipitation of 860 mm wre6 and wre7 are 80 m wide and 200 m long with an average slope profile of 2 9 for both watersheds surrounded by berms to define the watersheds domain the direction of the overland flow for both fields is from the east to the west fig 2b and c precipitation was measured using belford weighing rain gauges between 1976 and 1998 and the amount of daily runoff and sediment leaving the watersheds was measured using h flumes from 1977 to 1999 fig 2 the wre6 and wre7 were selected as study areas because the two rain gauges were implemented at the top and foot of the hills near the corners of the two watersheds fig 2 and the soil catena and aspects were the same in both watersheds in addition both wre6 and wre7 were planted with winter wheat wre6 was maximally disturbed using a moldboard plow while wre7 was minimally disturbed with no till practices for the period between 1993 and 1995 the contrast in management between wre6 and wre7 was used to evaluate the model s capability to account for differences in management considering that tillage is a significant driver of sediment production the different watershed management systems resulted in significant differences in sediment rates based on the measurements between 1993 and 1995 fig 3 during the simulation period between 1993 and 1995 the measured runoff depths showed relatively small differences between wre6 and wre7 with slightly greater runoff depth for wre6 compared to wre7 table 2 and fig 3a however wre6 produced more than three times the sediment yield compared to wre7 mainly caused by the intensive tillage practices implemented in wre6 during this period the measurement comparison showed a 71 reduction in sediment yield using the no till implementation in wre7 this reduction in sediment production is in agreement with a different study on a watershed in illinois lee et al 2021b 3 2 hydrologic model set up the hydrologic processes were simulated using the mike she model in the mike she model the unsaturated flow was simulated by the one dimensional implicit finite difference richards equation while the saturated flow was simulatedby the implicit finite difference solution of the three dimensional darcy equation a two dimensional diffusive wave approximation for overland flow was solved using a finite difference scheme the kristensen and jensen 1975 method and modified makkinks equation de bruin and lablans 1998 were used to estimate actual and reference evapotranspiration each watershed was discretized into 20 x 20 m grid cells determined based on the size of the watershed and distance between soil survey measurements and the simulation period was from 1993 to 1995 with a three year warm up period to obtain the model s initial conditions daily air temperatures were obtained from the national weather service nws cooperative observer program coop station 2818 located approximately 6 km east of the wre watersheds and daily precipitation was taken from the average of the two rain gauges near the watersheds fig 2 the watersheds were not irrigated and thus the model does not consider irrigation the needed soil properties were extracted from either the soil survey geographic database ssurgo usda 2022 or field soil survey data conducted in 2018 note that soil hydraulic parameters such as hydraulic conductivity were extracted from the ssurgo database while the fraction of clay silt and sand was extracted from the field survey data to distinguish between conventional tillage and no till practice in the hydrologic model different values of manning s roughness detention storage leaf area index lai and root depth were assigned for wre6 and wre7 the first two parameters were assigned based on the work of mohamoud 1992 while the latter two for winter wheat were extracted from the wepp database the observed runoff data at the flumes were used to calibrate and validate the performance of the hydrologic model the depth of the topsoil layer and the crop coefficients for actual evapotranspiration were adjusted as calibration parameters to improve model performance the model outputs of water depth and velocity fields in the x and y directions provided the needed hydrologic inputs for the sediment transport model 3 2 1 sediment transport model set up the stand alone wepp he code was implemented to generate the sediment source in the sediment transport model since the wepp he needed sub daily properties of precipitation and runoff characteristics the cligen model nicks and gander 1994 a stochastic weather generator was used to generate sub daily rainfall intensity and duration based on the actual daily temperature and precipitation data the runoff characteristics such as water depth were obtained from the mike she simulation results the soil survey data including soil texture information were assigned to each grid and management records including planting harvesting and tillage practices in the fields were referenced to assign time series of canopy height of the plant residue cover fraction and soil random roughness the time series datasets were directly extracted from the wepp database while interrill and rill erodibility factors were used as calibration parameters in the wepp he code the parameterized wepp he code was used to compute daily sediment sources for each grid cell considering the five sediment classes table 1 which were added as a source term in the advection dispersion equations the advection dispersion equations and deposition sink terms were solved considering particle diameters for these different classes as mentioned earlier the detention storage concept was used to set the threshold value to deposit all the sediment in each grid the threshold values were assigned as 0 2 mm and 1 0 mm for conventional tillage wre6 and no till practice wre7 respectively based on the work of mohamoud 1992 the simulated daily sediment leaving from the watershed was compared with the measured sediment and these comparisons were used as assessment endpoints to verify the integrated modeling for sediment predictions 4 results and discussion 4 1 hydrologic model performance the mike she simulations from 1993 to 1996 were used to evaluate the model performance by comparing the runoff depth against measured data at the flume for both watersheds the simulatedrunoff discharge computed at the bottom of each watershed was divided by the watershed area i e depth unit to correspond to the unit of runoff measurement since the mike she model is a physically based model with measurable parameters in the study sites minimal calibrations were required however some parameters such as crop coefficients and depth of the topsoil layer were adjusted to improve the model performance since these values were taken from existing technical reports and literature and not directly from the measurements to assess the model performance the nash sutcliffe efficiency nse nash and sutcliffe 1970 and percent bias pbias were used the nse coefficient measures the general agreement between measurements and simulation results while pbias represents the water balance discrepancy at the end of the simulation period the simulation results showed good agreement with the measured runoff for both watersheds nse values of 0 77 and 0 61 and pbias of 3 and 7 for wre6 and wre7 respectively fig 4 it can be recalled that the difference between wre6 and wre7 was the management practice this difference was conceptualized in the model using manning s n detention storage lai and root depth table 3 the reduction in the runoff in the no till management practice was aligned with the meta data analysis of runoff production from no till sun et al 2015 due in part to crop residue retention which can increase surface roughness and prevent surface crusting and sealing these phenomena can result in improved pore continuity and thus increasing infiltration these changes affected nse and pbias as the peak runoff rate and cumulative runoff were reduced in no till management practice and the model simulations were able to capture these changes in runoff fig 4 4 2 sediment model performance to reflect the management practices for each watershed several parameters were considered to conceptualize the different management practices in the stm that can significantly affect the rate of soil erosion these parameters were residue cover fraction rcf canopy height soil random roughness srr and adjusted values for interrill and rill erodibility factors the rcf is the fraction of the materials left on the cultivated land after the crop was harvested and the srr is the standard deviation of bed surface elevation around the mean elevation the abrupt changes in these two parameters were mainly induced by tillage where higher values of rcf can yield lower rates of soil erosion the adjusted values were used to reflect the temporal changes in the interrill and rill erodibility factors for freshly tilled soil conditions the higher values of the adjustments represented highly erodible materials the adjustments for these two variables typically ranged from 0 to 1 and the conservation tillage had lower values of adjustments compared to the conventional tillage showing high variations in time and space lee et al 2022 except for the adjusted values the values of the rest of the parameters were obtained from the wepp database considering the recorded dates for planting harvesting and tillage practices for wre6 and wre7 fig 5 in both watersheds winter wheat was planted and harvested in mid to end of october and june respectively several tillage practices such as moldboard plow disk and harrow were implemented between harvesting and planting seasons in wre6 nelson et al 2020 because of the aggressive tillage practices in wre6 the values of rcf were assumed to be as close to zero as possible during the growing seasons while wre7 showed relatively high values of rcf throughout the year fig 5a the canopy height did not significantly differ between the two watersheds fig 5b however the values of srr showed almost constant values for wre7 while wre6 showed higher fluctuation during the implementation of the tillage practices fig 5c causing higher values of srr in wre6 than wre7 to test the performance of the stm two separate periods were selected for both watersheds including several peak events for measured sediment the sediment yield leaving the model domain at the bottom of the watershed was estimated on a daily basis and compared with the measured sediments constant values for adjustment factors for the interrill and rill erodibilities were used as calibration parameters for each period due to the lack of the needed information to estimate these parameters note that for the first period 04 1993 to 07 1993 the interrill and rill erodibility adjustment factors used were 0 2 and 0 003 and for the second period 04 1994 to 05 1994 the adjustment factors used were 0 07 and 0 0025 respectively since the sediment transport was separately simulated for each sediment class the total sediment yield was calculated by adding the values from the five sediment classes and compared with the measured sediment during the first period several sediment events occurred when surface runoff occurred and the sediment simulations successfully captured these events with an nse of 0 58 fig 6 b a total of 1 577 kg of sediment was reported for wre6 based on the measured data while the simulation result estimated a cumulative value of 1 505 kg fig 6c showing a pbias of 5 the sediment delivery ratio sdr defined as the ratio of the sediment yield at the outlet of the watershed to the gross sediment production for the watershed was 0 23 the small aggregate was the dominant sediment component produced among the five sediment classes 40 followed by large aggregate 30 the spatial distribution of transport variables across the wre6 watershed for one of the peak events 5 9 1993 showed that the distribution of sediment source was highly correlated to that of surface water depth fig 7 a and b once the distribution of sediment sources was computed the eroded sediment was transported toward the bottom of the watershed fig 7d and the deposition rates were estimated using eq 5 for each sediment class fig 7c the deposition displayed in fig 7c appeared one day later compared to the other three variables because the deposition rates were almost zero across the model domain as the bed shear stress values were greater than the critical shear stress on 5 9 1993 for the second period there were four main peaks for runoff and sediment in wre6 fig 8 the results showed that the runoff highly influenced the timing and magnitude of sediment losses as the shapes of the runoff and sediment graphs corresponded to each other for both observation and simulation results the sediment predictions indicated good agreement with nse and pbias of 0 72 and 6 respectively the sdr was 0 22 similar to that in the first period with the largest fraction of sediment yield being the small aggregate 40 in general the sediment model captured the timing and magnitudes of sediment events based on the comparisons against the observation data in the wre6 watershed the same two periods were selected for the wre7 watershed to test the model performance of the stm since wre7 implemented no till practices in the field a significant reduction in sediment was observed based on the observations the sediment yield was reduced from 1 577 kg 161 kg for the first period by implementing the no till practice in wre7 fig 9 compared to wre6 the total sediment yield of wre7 for the first and second periods were reduced by 90 and 83 respectively based on the measurements and the simulations were able to capture these reductions showing 90 and 82 reductions for the two periods respectively similar to wre6 the dominant sediment class was the small aggregate for both periods followed by the large aggregate the sediment predictions for the two periods showed good agreement with the measured sediment data figs 9 and 10 the values of nse were 0 61 and 0 78 and pbias were 7 and 12 for the first and second periods respectively based on comparisons between simulated outputs with the observed data the stm was able to represent the behavior of sediment for these watersheds all the cases for wre6 and wre7 showed good agreement with the observations on a daily basis however it should be noted that reasonable sediment prediction was obtained only when the required hydrologic variables were well represented especially the timing and magnitude of runoff and sediment were highly correlated and thus a reasonable representation of the hydrologic processes should be provided in order to obtain acceptable sediment predictions in addition further improvements in sediment predictions can be expected with additional information on the model first sub daily properties for precipitation and runoff characteristics relied on statistical methods such as generation with the cligen model if these properties were obtainable sediment prediction could be further improved second if the time varying adjustments for interrill and rill erodibility factors for winter wheat were provided temporal variations of the soil s susceptibility to erosion could be added to the model lastly complete mixing was the primary assumption used in deriving the transport model eq 4 which means that every time sediment comes into the grid it will be mixed with what is in the grid however when solved numerically the mixing was not considered due to the discretization of the time step and the decoupled simulations of the hydrologic model if the hydrologic model can be fully coupled with stm allowing feedback mechanisms the deposited sediment can be updated for each time step in both hydrologic and sediment simulations temporal elevation and sediment mixing changes can then be updated with additional numerical schemes such as the exner equation exner 1920 overall however the simulation results showed that the stm was capable of simulating sediment fate and transport processes in agricultural watersheds capturing the different characteristics of agricultural management practices the stm is expected to advance the modeling capabilities of simulating soil erosion at watershed scales as the model can simulate sediment processes in a fully distributed setting with physically based equations and measurable input parameters and thus manage agricultural landscapes more sustainably first sediment production was evaluated at a grid scale where small scale soil erosion processes can be captured the model s ability to provide spatially distributed sediment production across the watershed allows stakeholders to identify the most vulnerable areas in terms of soil erosion for more strategic implementation of management practices detailed spatial information can significantly reduce erosion risk by implementing site specific erosion reduction measures second the stm transports the suspended sediment by considering the physical processes of soil particles such as deposition between grid cells many existing models rely on statistical observations or the use of a time lag to simplify the transport processes in overland flow ignoring the heterogeneity of driving forces such as rainfall and runoff these simplifications may lead to unrealistic outcomes as they ignore the inherent non linear relations in the catchment system hajigholizadeh et al 2018 the transport concept used in the stm makes it possible to capture the dynamics of flow and sediment particles in the model lastly one of the features of this model was to provide estimates of sediment production and transport considering five sediment classes this feature can be expanded to nonpoint source pollution models to account for the transport of sediment associated contaminants from agricultural lands as soil adsorbed contaminants are mainly routed on finer sediment particles 5 conclusions estimating soil erosion in agricultural watersheds is imperative for management and conservation endeavors however the complexity of the transport processes due to the influence of both natural and anthropogenic factors makes this task challenging the biggest challenge is how to account for the small scale processes that occur at the field scale less than tens of meters in large scale phenomena at the watershed scale that are used as a basis for policy and decision making we can capture the small scale changes representing farming operations by treating the domain as grids instead of hillslopes this feature can capture spatial variations of environmental stressors and watershed characteristics related to erosion processes with the support of high resolution datasets as more detailed observations become available at a finer scale in large areas the spatially distributed representations of erosion processes can be further improved with the fully distributed settings in addition to estimating sediment production at the grid level simulating the transport of the eroded soils should not be overlooked many erosion models simplify the sediment transport processes however a large portion of the eroded sediment is deposited along the way and the concept of the advection dispersion equation with a deposition term for five sediment classes makes it possible to assess these processes accounting for the physical process of the soil particle transport capturing the small scale phenomena and transport processes at catchment scales can provide a holistic view to the stakeholders help them understand the potential erosion risks and support strategic decisions and policies the developed and presented modeling framework provides mechanistic processes in simulating the fate and transport of sediment across the watershed instead of statistical inferences at spatio temporal scales that are useful in assessing the environmental impacts of land management schemes the ability of stm to simulate sediment production and transport considering five sediment classes could provide additional information for water quality assessment such as the transport of sediment borne contaminants in addition this tool can provide spatially distributed sediment sources to the sediment transport modeling for the river network for large river basins therefore this study is expected to help advance the current approaches to estimating soil erosion in agricultural landscapes by bridging scale differences to capture the large scale effects of small scale soil erosion processes software availability the sediment transport model stm was developed by sanghyun lee slee589 illinois edu using python and the model can be executed on a regular pc the stm and user manual readme docx are freely accessible at https github com sanghlee589 stm note that the manual contains detailed descriptions of the model such as the needed input file model set up and outputs from the model declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests maria l chu reports financial support was provided by national institute of food and agriculture acknowledgments this research contributed to the conservation effects assessment project and is a contribution of the usda agricultural research service ars southern plains long term agroecosystem research site and the usda ars national soil erosion research laboratory the us department of agriculture provided financial assistance for this research national institute for food and agriculture nifa project illu 741 617 
25467,a new modeling platform was developed to simulate spatially distributed sediment production and transport in agricultural landscapes the sediment production at the grid scale was computed using the stand alone water erosion prediction project hillslope erosion code while an advection dispersion equation represented sediment transport the model s performance was tested in the water resources and erosion watersheds at the oklahoma and central plains agricultural research center el reno ok usa results showed that this modeling approach could capture the complex behavior of sediment under different management practices in the watersheds the modeling framework provides mechanistic processes to simulate the fate and transport of sediment across the watershed at spatio temporal scales that are useful in assessing the environmental impacts of management schemes therefore this study is expected to help advance the current approaches to estimating soil erosion by bridging scale differences to capture the large scale effects of small scale soil erosion processes keywords soil erosion sediment transport wepp advection dispersion equation wre watersheds data availability i included software availabilitly in the manuscript 1 introduction excessive soil erosion can result in land degradation in agricultural fields and hence a major threat to food supply and water quality luetzenburg et al 2020 montanarella et al 2016 parsons 2019 model simulations of climate change coupled with expected intensive land cultivation suggest that over the next 50 years soil erosion by water could significantly increase by 30 66 worldwide borrelli et al 2020 such increases in soil erosion are expected to result in high economic and societal costs therefore reliable information on soil erosion should be developed to assist in land use and policy decisions at individual field and catchment scales fu et al 2019 momm et al 2019 parsons 2019 the number of computer models for sediment predictions has rapidly increased in recent decades due to the increasing use of computer applications and computing power measurements of sediment in agricultural landscapes are scarce and hard to obtain highlighting the importance of using computer models to overcome this limitation however due to simplifying assumptions and parameter values often based on local conditions many models still have a high degree of uncertainty hajigholizadeh et al 2018 this is especially true regarding the spatiotemporal dynamics of soil erosion by water which are often misrepresented and not well quantified when the erosion processes are conceptualized in transport models at a watershed scale wainwright et al 2015 for example the soil and water assessment tool swat arnold et al 1998 2012 and agricultural policy environmental extender model apex williams et al 2000 models have been widely used in recent national conservation effects assessments by the us department of agriculture usda natural resources conservation service nrcs and agricultural research service ars flanagan et al 2020 however these models have their limitations in terms of sediment transport simulations for instance the swat model divides sediment transport calculations into overland and river flow and the sediment that reaches the stream network minus a lag is typically estimated by the modified universal soil loss equation musle williams and berndt 1977 and only considers temporary retention of sediment similarly the apex model estimates soil erosion using the universal soil loss equation usle and its derivatives wischmeier and smith 1978 and it has a limited ability to simulate the effects of changing management bhandari et al 2017 even though soil erosion can occur at various scales most of the models available currently evaluate soil loss as a lumped system e g sub catchment levels assuming average values of the watershed characteristics meinen and robinson 2021 dabney et al 2015 introduced a spatially distributed adaptation of the revised universal soil loss equation rusle however the adaptation relied on the empirical foundation and thus the model cannot explain the physical phenomena of soil erosion to overcome these issues process based models have been developed that combine the governing equations of conservation of mass and momentum to describe water and sediment fluxes examples of such models include the areal nonpoint source watershed environment response simulation answers model beasley et al 1980 the chemicals runoff and erosion from agricultural management systems creams model knisel 1980 and the water erosion prediction project wepp model flanagan and nearing 1995 however a common limitation of these models is that they were conceptualized to represent soil loss at the hillslope or small watershed scale baffaut et al 1997 miller et al 2011 this limitation is especially crucial since the quantification of soil erosion is sensitive to the spatial scale of process representation lee et al 2021a and sediment transport for assessment of conservation strategies is typically conducted at the regional scale therefore a watershed scale model that can capture the spatiotemporal dynamics of soil erosion and that has the capacity to integrate process representation at a higher spatial resolution for resource and environmental management is needed understanding the physical processes of suspended sediment in agricultural landscapes is important for the soil and water conservation quantification needed to evaluate and develop sustainable agricultural production systems inaccurate sediment prediction can arise from the failure to capture the temporal and spatial heterogeneities of input data and parameters used in the model or the failure to represent the physical phenomena of detachment transport and deposition of soil particles fischer et al 2018 to minimize the impact of these issues temporal and spatial information on soil erosion processes is required hajigholizadeh et al 2018 kuznetsov et al 1998 in this regard a distributed model e g grid based is an effective approach for resolving spatial distributions of environmental data and physical processes of soil erosion lee et al 2021a in light of the need to provide a holistic view of sediment processes in watershed management planning the main objective of this study was to develop an integrated modeling framework that can evaluate distributed sediment production and its propagation in a fully distributed setting herein we describe a model framework that was developed to seamlessly integrate field scale erosion processes and a sediment transport concept based on the advection dispersion equation in a fully distributed setting the model was evaluated using measured runoff depth and sediment load from the water resources and erosion wre experimental watersheds managed by the usda ars oklahoma and central plains agricultural research center el reno ok usa 2 model development 2 1 overview of sediment transport model the main aim of the developed sediment transport model is to provide reliable distributed sediment production i e sediment source and to simulate the fate and transport of the eroded soils in agricultural landscapes the model domain was discretized into rectangular grid cells and the distributed sediment source was estimated based on a stand alone water erosion prediction project hillslope erosion wepp he code the wepp he code was extracted from the full wepp v2004 7 model flanagan et al 2005 containing only erosion calculations at a hillslope scale the wepp he code was seamlessly integrated into the developed model to provide daily soil loss for each grid cell across the model domain and partition sediment predictions into five different sediment classes table 1 and fig 1 several input parameters such as sub daily precipitation and runoff characteristics should be provided fig 1 each grid cell is conceptualized as a single hillslope component having different input data such as soil properties and hydrologic variables to execute the wepp he code to simulate the fate and transport of the eroded soil between grid cells a two dimensional advection dispersion equation with sink and source terms was numerically solved for each sediment class in the sediment transport model fig 1 the sediment transport model stm requires inputs from a hydrologic model in terms of runoff depth and velocity fields in the x and y directions fig 1 once the wepp he computes the distributed sediment source for each sediment class the stm separately simulates the transport of the eroded sediment for each sediment class across the model domain using the provided hydrologic variables fig 1 2 2 estimation of sediment production the wepp model simulates hillslopes and small watersheds flanagan and nearing 1995 in this study instead of using the complete wepp hillslope model the stand alone wepp he code was used together with the sediment transport model the wepp he code includes only the erosion computations at hillslope scales and excludes the rest of the components used in the full wepp model at the hillslope scale it estimates interrill and rill erosion and sediment delivery spatially down a profile interrill erosion is defined as soil movement that occurs due to the impact of raindrops and sheet flow while rill erosion occurs when runoff forms small channels as it concentrates down a hillslope ranges tens of meters this model provides reliable estimates for soil loss in a broad range of climate and soil conditions laflen et al 2004 tiwari et al 2000 zhang et al 1996 the wepp he code can be executed with the hydrologic values of runoff depth sub daily precipitation and runoff characteristics as the main inputs the runoff characteristics are peak runoff rate and effective runoff duration the runoff depth divided by peak runoff rate in a given day other input variables include soil texture information canopy height adjusted interrill and rill erodibilities critical shear stress friction factor values and slope fig 1 for sub daily hydrologic variables a continuous single rainfall and runoff event was assumed to occur on a given day in addition each grid cell is conceptualized as a unique combination of topography soil properties land use and management practice sediment predictions for each grid cell were partitioned into five sediment classes table 1 and these values were separately added to a source term of the advection dispersion equations across the model domain in the sediment transport model fig 1 the fractions of each sediment class were estimated from the distribution of primary particles e g clay silt and sand in the topsoil layer based on the derivations of foster et al 1985 the wepp he was verified against the full wepp model in predicting soil loss flanagan et al 2005 and has shown its functionality when coupled with a large scale hydrologic model mao et al 2010 in the wepp he model the movement of sediment at hillslope scales is described by 1 d e d x d i n t e r r i l l d r i l l where x is the distance down the slope m e is sediment load per unit width of the hillslope kg s 1 m 1 d i n t e r r i l l is interrill erosion rate kg s 1 m 2 and d r i l l is rill erosion rate kg s 1 m 2 foster et al 1995 the interrill erosion rate is calculated by 2 d i n t e r r i l l k i a d j i e o i r s d r r r f n o z z l e r s w where k i a d j is the adjusted interrill erodibility factor kg s m 4 i e is the effective rainfall intensity m s 1 o i r is the interrill runoff rate m s 1 s d r r r is a sediment delivery ratio f n o z z l e is an adjustment factor to account for irrigation water drop impact energy and the default value of 1 was assigned in the model r s is the rill spacing m and w is the rill width m the net soil detachment in rills is then estimated by 3 d r i l l k r a d j τ f τ c a d j 1 g t c where k r a d j is the adjusted rill erodibility parameter s m 1 τ f is the flow shear stress acting on the soil particles pa τ c a d j is the adjusted critical shear stress of the soil pa and t c is the sediment transport capacity in the rill kg s 1 m 1 note that the net soil detachment only occurs when t c and τ f are greater than g and τ c a d j respectively otherwise net deposition occurs as a function of the settling velocity of the soil particles to calculate the deposition rate foster et al 1995 to provide the sediment source for each grid cell a single grid cell was conceptualized as a single hillslope across the model domain and daily values of the estimated sediment yield at the outlet of the hillslope were added as a source term in the advection dispersion equation 2 3 simulation of sediment transport sediment transport in overland flow was described using an explicit finite difference solution to the advection dispersion equations when ponded water exists in overland flow i e soil profile is inundated when the water table is above the soil surface the two dimensional 2d multiclass sediment transport advection dispersion equations with source and sink terms were applied for five sediment classes eq 4 the suspended sediment was assumed to be completely mixed over the depth inflow and the five sediment classes behaved independently 4 c i t x c i u y c i v x ε s c i x y ε s c i y e i d i where c i is the depth averaged sediment concentration for sediment class i t is the time x and y are the spatial coordinates u and v are the velocities in the x and y directions ε s is the longitudinal diffusion coefficient and e i and d i are the erosion and deposition rates respectively the governing equation was discretized using the explicit finite difference method with central differencing for diffusion terms and the lax and wendroff 1960 scheme for advection terms the sediment source estimated from the wepp he eq 1 was added to the erosion term e i for each grid cell while the multiclass deposition term d i was estimated by ockenden 1993 teeter 2000a 2000b 5 d i w i c i h 1 τ b τ c r i f o r τ b τ c r i where w i is the settling velocity for each sediment class h is the water depth τ b is the bed shear stress and τ c r i is the critical shear stress deposition only occurs when the bed shear stress is less than the critical shear stress and if the water depth is less than detention storage of the surface in eq 5 the settling velocity w i and critical shear stress τ c i were estimated considering each particle size table 1 dietrich 1982 derived the dimensionless relationships between the settling velocity and particle reynolds number r e p for natural particles as 6 w i g d ρ s ρ w ρ w exp b 1 b 2 ln r e p b 3 ln r e p 2 b 4 ln r e p 3 b 5 ln r e p 4 where r e p ρ s ρ w ρ w g d d ν ρ s is the particle density and ρ w is the water density ν is the kinematic viscosity d is the particle diameter b 1 is 2 891394 b 2 is 0 95296 b 3 is 0 056835 b 4 is 0 002892 and b 5 is 0 000245 brownlie s 1981 fitted curve which is based on the experimental data of the shields curve is widely used to determine the dimensionless critical shear stress τ c r i furthermore parker et al 2003 amended brownlie s expression to adjust τ c r i to 0 03 for a large reynolds number and this relation was used to estimate τ c r i as follows 7 τ c r i 0 5 0 22 r e p 0 6 0 06 exp 17 77 r e p 0 6 based on eq 7 critical shear stress τ c r i can be estimated by 8 τ c r i ρ s ρ w ρ w g d τ c r i the relation of the longitudinal diffusion coefficients for the depth averaged model ε s can be derived assuming the balance between diffusivity and eddy viscosity prandtl analogy 9 ε s κ 6 u h 1 15 u h where κ is the von karman s constant 0 41 and u is the shear velocity this relation is known to provide a good approximation to estimate the longitudinal dispersion of fine grained sediment garcia 2008 huang and garcia 2000 the needed hydrologic variables to simulate the sediment transport models are water depth and velocity fields in the x and y directions in overland flow and these variables were extracted from the simulation results from the mike she model abbott et al 1986 a fully distributed and integrated surface and subsurface model refsgaard and storm 1995 this model can simulate the components of overland flow evapotranspiration unsaturated flow and groundwater flow at watershed scales note that any other distributed hydrologic model can be coupled to this transport model as long as the model can provide the needed hydrologic variables 3 model evaluation 3 1 study area the stm model was evaluated in two of the eight water resources and erosion watersheds wre6 and wre7 fig 2 located at the usda ars oklahoma and central plains agricultural research center usda ars ocparc in el reno oklahoma these watersheds were established in 1976 by the usda ars ocparc to study the long term impacts of managing practices in water quality and sediment in agricultural landscapes nelson et al 2020 the climate of the study area is semiarid to sub humid climate conditions receiving annual precipitation of 860 mm wre6 and wre7 are 80 m wide and 200 m long with an average slope profile of 2 9 for both watersheds surrounded by berms to define the watersheds domain the direction of the overland flow for both fields is from the east to the west fig 2b and c precipitation was measured using belford weighing rain gauges between 1976 and 1998 and the amount of daily runoff and sediment leaving the watersheds was measured using h flumes from 1977 to 1999 fig 2 the wre6 and wre7 were selected as study areas because the two rain gauges were implemented at the top and foot of the hills near the corners of the two watersheds fig 2 and the soil catena and aspects were the same in both watersheds in addition both wre6 and wre7 were planted with winter wheat wre6 was maximally disturbed using a moldboard plow while wre7 was minimally disturbed with no till practices for the period between 1993 and 1995 the contrast in management between wre6 and wre7 was used to evaluate the model s capability to account for differences in management considering that tillage is a significant driver of sediment production the different watershed management systems resulted in significant differences in sediment rates based on the measurements between 1993 and 1995 fig 3 during the simulation period between 1993 and 1995 the measured runoff depths showed relatively small differences between wre6 and wre7 with slightly greater runoff depth for wre6 compared to wre7 table 2 and fig 3a however wre6 produced more than three times the sediment yield compared to wre7 mainly caused by the intensive tillage practices implemented in wre6 during this period the measurement comparison showed a 71 reduction in sediment yield using the no till implementation in wre7 this reduction in sediment production is in agreement with a different study on a watershed in illinois lee et al 2021b 3 2 hydrologic model set up the hydrologic processes were simulated using the mike she model in the mike she model the unsaturated flow was simulated by the one dimensional implicit finite difference richards equation while the saturated flow was simulatedby the implicit finite difference solution of the three dimensional darcy equation a two dimensional diffusive wave approximation for overland flow was solved using a finite difference scheme the kristensen and jensen 1975 method and modified makkinks equation de bruin and lablans 1998 were used to estimate actual and reference evapotranspiration each watershed was discretized into 20 x 20 m grid cells determined based on the size of the watershed and distance between soil survey measurements and the simulation period was from 1993 to 1995 with a three year warm up period to obtain the model s initial conditions daily air temperatures were obtained from the national weather service nws cooperative observer program coop station 2818 located approximately 6 km east of the wre watersheds and daily precipitation was taken from the average of the two rain gauges near the watersheds fig 2 the watersheds were not irrigated and thus the model does not consider irrigation the needed soil properties were extracted from either the soil survey geographic database ssurgo usda 2022 or field soil survey data conducted in 2018 note that soil hydraulic parameters such as hydraulic conductivity were extracted from the ssurgo database while the fraction of clay silt and sand was extracted from the field survey data to distinguish between conventional tillage and no till practice in the hydrologic model different values of manning s roughness detention storage leaf area index lai and root depth were assigned for wre6 and wre7 the first two parameters were assigned based on the work of mohamoud 1992 while the latter two for winter wheat were extracted from the wepp database the observed runoff data at the flumes were used to calibrate and validate the performance of the hydrologic model the depth of the topsoil layer and the crop coefficients for actual evapotranspiration were adjusted as calibration parameters to improve model performance the model outputs of water depth and velocity fields in the x and y directions provided the needed hydrologic inputs for the sediment transport model 3 2 1 sediment transport model set up the stand alone wepp he code was implemented to generate the sediment source in the sediment transport model since the wepp he needed sub daily properties of precipitation and runoff characteristics the cligen model nicks and gander 1994 a stochastic weather generator was used to generate sub daily rainfall intensity and duration based on the actual daily temperature and precipitation data the runoff characteristics such as water depth were obtained from the mike she simulation results the soil survey data including soil texture information were assigned to each grid and management records including planting harvesting and tillage practices in the fields were referenced to assign time series of canopy height of the plant residue cover fraction and soil random roughness the time series datasets were directly extracted from the wepp database while interrill and rill erodibility factors were used as calibration parameters in the wepp he code the parameterized wepp he code was used to compute daily sediment sources for each grid cell considering the five sediment classes table 1 which were added as a source term in the advection dispersion equations the advection dispersion equations and deposition sink terms were solved considering particle diameters for these different classes as mentioned earlier the detention storage concept was used to set the threshold value to deposit all the sediment in each grid the threshold values were assigned as 0 2 mm and 1 0 mm for conventional tillage wre6 and no till practice wre7 respectively based on the work of mohamoud 1992 the simulated daily sediment leaving from the watershed was compared with the measured sediment and these comparisons were used as assessment endpoints to verify the integrated modeling for sediment predictions 4 results and discussion 4 1 hydrologic model performance the mike she simulations from 1993 to 1996 were used to evaluate the model performance by comparing the runoff depth against measured data at the flume for both watersheds the simulatedrunoff discharge computed at the bottom of each watershed was divided by the watershed area i e depth unit to correspond to the unit of runoff measurement since the mike she model is a physically based model with measurable parameters in the study sites minimal calibrations were required however some parameters such as crop coefficients and depth of the topsoil layer were adjusted to improve the model performance since these values were taken from existing technical reports and literature and not directly from the measurements to assess the model performance the nash sutcliffe efficiency nse nash and sutcliffe 1970 and percent bias pbias were used the nse coefficient measures the general agreement between measurements and simulation results while pbias represents the water balance discrepancy at the end of the simulation period the simulation results showed good agreement with the measured runoff for both watersheds nse values of 0 77 and 0 61 and pbias of 3 and 7 for wre6 and wre7 respectively fig 4 it can be recalled that the difference between wre6 and wre7 was the management practice this difference was conceptualized in the model using manning s n detention storage lai and root depth table 3 the reduction in the runoff in the no till management practice was aligned with the meta data analysis of runoff production from no till sun et al 2015 due in part to crop residue retention which can increase surface roughness and prevent surface crusting and sealing these phenomena can result in improved pore continuity and thus increasing infiltration these changes affected nse and pbias as the peak runoff rate and cumulative runoff were reduced in no till management practice and the model simulations were able to capture these changes in runoff fig 4 4 2 sediment model performance to reflect the management practices for each watershed several parameters were considered to conceptualize the different management practices in the stm that can significantly affect the rate of soil erosion these parameters were residue cover fraction rcf canopy height soil random roughness srr and adjusted values for interrill and rill erodibility factors the rcf is the fraction of the materials left on the cultivated land after the crop was harvested and the srr is the standard deviation of bed surface elevation around the mean elevation the abrupt changes in these two parameters were mainly induced by tillage where higher values of rcf can yield lower rates of soil erosion the adjusted values were used to reflect the temporal changes in the interrill and rill erodibility factors for freshly tilled soil conditions the higher values of the adjustments represented highly erodible materials the adjustments for these two variables typically ranged from 0 to 1 and the conservation tillage had lower values of adjustments compared to the conventional tillage showing high variations in time and space lee et al 2022 except for the adjusted values the values of the rest of the parameters were obtained from the wepp database considering the recorded dates for planting harvesting and tillage practices for wre6 and wre7 fig 5 in both watersheds winter wheat was planted and harvested in mid to end of october and june respectively several tillage practices such as moldboard plow disk and harrow were implemented between harvesting and planting seasons in wre6 nelson et al 2020 because of the aggressive tillage practices in wre6 the values of rcf were assumed to be as close to zero as possible during the growing seasons while wre7 showed relatively high values of rcf throughout the year fig 5a the canopy height did not significantly differ between the two watersheds fig 5b however the values of srr showed almost constant values for wre7 while wre6 showed higher fluctuation during the implementation of the tillage practices fig 5c causing higher values of srr in wre6 than wre7 to test the performance of the stm two separate periods were selected for both watersheds including several peak events for measured sediment the sediment yield leaving the model domain at the bottom of the watershed was estimated on a daily basis and compared with the measured sediments constant values for adjustment factors for the interrill and rill erodibilities were used as calibration parameters for each period due to the lack of the needed information to estimate these parameters note that for the first period 04 1993 to 07 1993 the interrill and rill erodibility adjustment factors used were 0 2 and 0 003 and for the second period 04 1994 to 05 1994 the adjustment factors used were 0 07 and 0 0025 respectively since the sediment transport was separately simulated for each sediment class the total sediment yield was calculated by adding the values from the five sediment classes and compared with the measured sediment during the first period several sediment events occurred when surface runoff occurred and the sediment simulations successfully captured these events with an nse of 0 58 fig 6 b a total of 1 577 kg of sediment was reported for wre6 based on the measured data while the simulation result estimated a cumulative value of 1 505 kg fig 6c showing a pbias of 5 the sediment delivery ratio sdr defined as the ratio of the sediment yield at the outlet of the watershed to the gross sediment production for the watershed was 0 23 the small aggregate was the dominant sediment component produced among the five sediment classes 40 followed by large aggregate 30 the spatial distribution of transport variables across the wre6 watershed for one of the peak events 5 9 1993 showed that the distribution of sediment source was highly correlated to that of surface water depth fig 7 a and b once the distribution of sediment sources was computed the eroded sediment was transported toward the bottom of the watershed fig 7d and the deposition rates were estimated using eq 5 for each sediment class fig 7c the deposition displayed in fig 7c appeared one day later compared to the other three variables because the deposition rates were almost zero across the model domain as the bed shear stress values were greater than the critical shear stress on 5 9 1993 for the second period there were four main peaks for runoff and sediment in wre6 fig 8 the results showed that the runoff highly influenced the timing and magnitude of sediment losses as the shapes of the runoff and sediment graphs corresponded to each other for both observation and simulation results the sediment predictions indicated good agreement with nse and pbias of 0 72 and 6 respectively the sdr was 0 22 similar to that in the first period with the largest fraction of sediment yield being the small aggregate 40 in general the sediment model captured the timing and magnitudes of sediment events based on the comparisons against the observation data in the wre6 watershed the same two periods were selected for the wre7 watershed to test the model performance of the stm since wre7 implemented no till practices in the field a significant reduction in sediment was observed based on the observations the sediment yield was reduced from 1 577 kg 161 kg for the first period by implementing the no till practice in wre7 fig 9 compared to wre6 the total sediment yield of wre7 for the first and second periods were reduced by 90 and 83 respectively based on the measurements and the simulations were able to capture these reductions showing 90 and 82 reductions for the two periods respectively similar to wre6 the dominant sediment class was the small aggregate for both periods followed by the large aggregate the sediment predictions for the two periods showed good agreement with the measured sediment data figs 9 and 10 the values of nse were 0 61 and 0 78 and pbias were 7 and 12 for the first and second periods respectively based on comparisons between simulated outputs with the observed data the stm was able to represent the behavior of sediment for these watersheds all the cases for wre6 and wre7 showed good agreement with the observations on a daily basis however it should be noted that reasonable sediment prediction was obtained only when the required hydrologic variables were well represented especially the timing and magnitude of runoff and sediment were highly correlated and thus a reasonable representation of the hydrologic processes should be provided in order to obtain acceptable sediment predictions in addition further improvements in sediment predictions can be expected with additional information on the model first sub daily properties for precipitation and runoff characteristics relied on statistical methods such as generation with the cligen model if these properties were obtainable sediment prediction could be further improved second if the time varying adjustments for interrill and rill erodibility factors for winter wheat were provided temporal variations of the soil s susceptibility to erosion could be added to the model lastly complete mixing was the primary assumption used in deriving the transport model eq 4 which means that every time sediment comes into the grid it will be mixed with what is in the grid however when solved numerically the mixing was not considered due to the discretization of the time step and the decoupled simulations of the hydrologic model if the hydrologic model can be fully coupled with stm allowing feedback mechanisms the deposited sediment can be updated for each time step in both hydrologic and sediment simulations temporal elevation and sediment mixing changes can then be updated with additional numerical schemes such as the exner equation exner 1920 overall however the simulation results showed that the stm was capable of simulating sediment fate and transport processes in agricultural watersheds capturing the different characteristics of agricultural management practices the stm is expected to advance the modeling capabilities of simulating soil erosion at watershed scales as the model can simulate sediment processes in a fully distributed setting with physically based equations and measurable input parameters and thus manage agricultural landscapes more sustainably first sediment production was evaluated at a grid scale where small scale soil erosion processes can be captured the model s ability to provide spatially distributed sediment production across the watershed allows stakeholders to identify the most vulnerable areas in terms of soil erosion for more strategic implementation of management practices detailed spatial information can significantly reduce erosion risk by implementing site specific erosion reduction measures second the stm transports the suspended sediment by considering the physical processes of soil particles such as deposition between grid cells many existing models rely on statistical observations or the use of a time lag to simplify the transport processes in overland flow ignoring the heterogeneity of driving forces such as rainfall and runoff these simplifications may lead to unrealistic outcomes as they ignore the inherent non linear relations in the catchment system hajigholizadeh et al 2018 the transport concept used in the stm makes it possible to capture the dynamics of flow and sediment particles in the model lastly one of the features of this model was to provide estimates of sediment production and transport considering five sediment classes this feature can be expanded to nonpoint source pollution models to account for the transport of sediment associated contaminants from agricultural lands as soil adsorbed contaminants are mainly routed on finer sediment particles 5 conclusions estimating soil erosion in agricultural watersheds is imperative for management and conservation endeavors however the complexity of the transport processes due to the influence of both natural and anthropogenic factors makes this task challenging the biggest challenge is how to account for the small scale processes that occur at the field scale less than tens of meters in large scale phenomena at the watershed scale that are used as a basis for policy and decision making we can capture the small scale changes representing farming operations by treating the domain as grids instead of hillslopes this feature can capture spatial variations of environmental stressors and watershed characteristics related to erosion processes with the support of high resolution datasets as more detailed observations become available at a finer scale in large areas the spatially distributed representations of erosion processes can be further improved with the fully distributed settings in addition to estimating sediment production at the grid level simulating the transport of the eroded soils should not be overlooked many erosion models simplify the sediment transport processes however a large portion of the eroded sediment is deposited along the way and the concept of the advection dispersion equation with a deposition term for five sediment classes makes it possible to assess these processes accounting for the physical process of the soil particle transport capturing the small scale phenomena and transport processes at catchment scales can provide a holistic view to the stakeholders help them understand the potential erosion risks and support strategic decisions and policies the developed and presented modeling framework provides mechanistic processes in simulating the fate and transport of sediment across the watershed instead of statistical inferences at spatio temporal scales that are useful in assessing the environmental impacts of land management schemes the ability of stm to simulate sediment production and transport considering five sediment classes could provide additional information for water quality assessment such as the transport of sediment borne contaminants in addition this tool can provide spatially distributed sediment sources to the sediment transport modeling for the river network for large river basins therefore this study is expected to help advance the current approaches to estimating soil erosion in agricultural landscapes by bridging scale differences to capture the large scale effects of small scale soil erosion processes software availability the sediment transport model stm was developed by sanghyun lee slee589 illinois edu using python and the model can be executed on a regular pc the stm and user manual readme docx are freely accessible at https github com sanghlee589 stm note that the manual contains detailed descriptions of the model such as the needed input file model set up and outputs from the model declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests maria l chu reports financial support was provided by national institute of food and agriculture acknowledgments this research contributed to the conservation effects assessment project and is a contribution of the usda agricultural research service ars southern plains long term agroecosystem research site and the usda ars national soil erosion research laboratory the us department of agriculture provided financial assistance for this research national institute for food and agriculture nifa project illu 741 617 
25468,accurately estimating soil moisture at fine resolutions scales approaching a few meters has been a target of a broad community involved in soil moisture research and application development from agriculture support improving land atmosphere interaction in weather and climate prediction and hydrologic modeling to a variety of military applications the us army interest in high spatial resolution soil moisture products is based on the need to better understand soil impacts on broad range of applications including but not limited to movement and maneuver and logistics the geospatial weather affected terrain conditions and hazards geowatch application was developed to support army needs for high resolution soil moisture products this paper describes the geowatch application development of the high resolution soil moisture analysis and prediction capability and linkages to downstream army applications that deliver weather informed content to support decision making this paper also describes product evaluation and validation efforts to understand application performance keywords soil moisture downscaling gis soil modeling model validation soil strength decision support data availability data will be made available on request 1 introduction background the us army requires an advanced knowledge of high resolution weather and terrain conditions in order to support global operations szymber 2006 understanding the variability of the dynamic environment including weather and climate effects on terrain conditions is critical to effectively characterizing the risks within the operating environment the broad range of weather impacts require the us army corp of engineers together with service partner air force weather to develop and improve algorithms models and applications that better integrate weather and climate products with army geospatial information system gis based software systems eylander et al 2020 the gis enabled display of weather and terrain information supply geographic context to enable informed decision making for military planning and preparation department of the army 2019 engineering support to partner countries usaid 2022 department of the army 2009a 2009b and provide awareness for global water and food security applications verdin et al 2005 funk and verdin 2010 arsenault et al 2020 peters lidard et al 2021 funk et al 2019 such as the us agency for international development famine early warning systems network fews net www fews net and us department of agriculture crop explorer https ipad fas usda gov cropexplorer default aspx while global land data assimilation and soil moisture modeling systems have been developed kumar et al 2006 peters lidard and coauthors 2007 rodell et al 2004 to deliver global estimates of soil moisture at weather scale on the order of hundreds of meters to tens of kilometers most army applications need environmental products delivered at resolutions approaching tens of meters or finer to support tactical decision making the us air force 557th weather wing 557ww operates coupled land atmosphere weather and climate models to compute soil moisture temperature and surface energy budget products that blend the strengths of remote sensing with land surface models via assimilation lawrence et al 2019 niu et al 2011 yang et al 2011 best and coauthors 2011 clark and coauthors 2011 chen and coauthors 1996 ek et al 2003 koster et al 2000 kumar et al 2008 reichle et al 2002 reichle et al 2010 the 557ww computes these global models at 3 hourly intervals daily at 10 km spatial resolutions 17 km resolution forecasts eylander et al 2022 kemp et al 2022 yoon et al 2022 which is coarse in spatial resolution compared to the needs of the army movement and maneuvering which can occur both on and off road need a complete accounting for terrain conditions that impact mobility at resolutions on the order of meters or finer to deliver risk criteria for go no go decision making or routing decision to account for and avoid weather impacted terrain obstacles this paper explores geowatch a tool which generates soil moisture analyses and forecasts at finer scale resolutions spatially less than 100 m with the ability to rapidly compute high resolution soil moisture for any domain size globally this capability enables the army to directly link real time weather and climate data to decision support applications and better account for weather and climate impacts on operational environments 1 1 introduction to geowatch the geospatial weather affected terrain conditions and hazards geowatch system was designed to deliver high resolution global estimates of soil moisture based on gridded weather analyses and predictions the system delivers high resolution gridded soil moisture analysis and forecast using the 557ww weather terrain analyses and forecasts as an initial baseline then downscales those products to a 30 90 m spatial resolution on demand for any global region further the geowatch soil moisture products are dynamically linked with army developed algorithms to compute soil strength this supports army maneuver applications to deliver a first of a kind prediction of weather informed maneuver impacts computed for any region globally within minutes of computational time this paper describes the geowatch system including the input weather and terrain data downscaling algorithm link to the army applications and provides an assessment of our evaluation of the geowatch downscaled soil moisture results geowatch ingests and dynamically links gridded soil moisture analyses and forecasts from the 557ww as the low resolution basis for its downscaled soil moisture and soil strength products the dependence on weather data provided by the 557ww is based on joint us army and us air force policies and agreements department of the army 2021 detailing the usaf operational weather support to the us army since the usaf operates the lis land data assimilation system to compute global soil moisture temperature and surface energy budget analyses and the global air land weather exploitation model galwem stoffler 2017 a usaf specific version of the united kingdom meteorology office unified model ukmet model geowatch relies on those input data sources to initialize the downscaling model the 557ww implementation of lis provides 10 km global 3 hourly analyses of soil moisture and temperature for four soil layers 0 10 cm 10 40 cm 40 100 cm and 100 200 cm depths along with surface and subsurface runoff sensible latent and ground heat fluxes surface evaporation and evapotranspiration 1 2 previous efforts downscaling is the process of computing a variable finer spatial resolution from some coarser level inputs there are many published methods to estimate higher resolution soil moisture and or downscale soil moisture from coarser resolution inputs some methods include downscaling coarse resolution passive microwave remote sensing observations using methods that blend products from multiple sensors piles et al 2016 peng et al 2016 choi and hur 2012 using complex statistical and or machine learning methods to downscale srivastava et al 2013 loew and mauser 2008 verhoest et al 2015 chai et al 2011 conducting multi satellite downscaling combined with model products fang et al 2013 2018 using the active and passive measurements from the nasa soil moisture active passive satellite entekhabi et al 2010 das et al 2011 2014 and downscaling algorithms that are driven by either coarse resolution input model estimates remotely sensed analyses or both coleman and niemann 2013 ranney et al 2015 vergopolan et al 2021 peng et al 2017 and fang 2020 each provide a good overview of soil moisture downscaling algorithms further there are several downscaling efforts focused on global land data assimilation using soil moisture observations from passive microwave and or other and computing products at higher resolution kumar et al 2009 2012 2014 santanello et al 2016 xue et al 2021 lievens et al 2015 draper and reichle 2015 reichle et al 2017 2019 sahoo et al 2013 many remote sensing based methods compute products at resolutions finer than 1 km spatially the smap hydroblocks product vergopolan et al 2021 is a more complicated blend of the model and remote sensing downscaling approach and provides some of the highest resolution output producing 30 m soil moisture results over the continental unites states hydroblocks uses a land surface model radiative transfer model high resolution digital terrain data blended with smap remote sensing observations to downscale the soil moisture results non downscaling approaches that focus on using high resolution thermal remote sensing imagery to estimate soil moisture and surface energy balance result in soil moisture products on the order of a 1 km but are still limited to the resolution of the finest spatial scale satellite data available or having a cloud free scene of which to obtain the imagery bastiaanssen et al 1998 allen et al 2007 allen 2011 hong 2008 anderson et al 1997 2007 2011 these approaches partition the surface energy sensible and latent energy flux estimate using remotely sensed vegetation parameters e g ndvi and then estimate the soil moisture based on evapotranspiration models the surface energy balance algorithm for land sebal bastiaanssen et al 1998 and atmosphere land exchange inverse alexi anderson et al 1997 2007 2011 methods both have demonstrated using higher resolution optical imagery to characterize land energy and or mass values the sebal approach supports finer scale soil moisture using infrared satellite measurements of vegetation supplying both surface and root zone soil moisture at the resolution of the infrared satellite imagery the alexi approach resolves the surface temperature from the infrared results partitions the temperatures into a bare ground and canopy surface values and then estimates the vegetation evapotranspiration and uses a priestly taylor priestley and taylor 1972 approximation to estimate the soil moisture values while these methods deliver high resolution soil water mass and energy parameters the resulting products are computed at the resolution of the satellite data available and are reliant on cloud free scenes to generate products the challenge with relying on high resolution imagery data is that a real time global estimate is still limited by scanning methodologies of the imaging satellites and or weather affects cloud cover which can impact large amounts of land at any given time topmodel is one of the longest used methods for computing basin level rainfall runoff relationships from topography an approach initially designed by beven and kirkby 1979 and beven et al 1984 topmodel was designed to capture the runoff from terrain contributing areas and was the first hydrology model to incorporate topography beven et al 2020 topmodel assumes that the hydraulic gradient can be approximated using the slope angle the local transmissivity and storage in rainfall equivalent depth units an intermediate step in the topmodel generates the topographic index the upslope contributing area divided by the hydraulic gradient where the hydraulic gradient is a function of the terrain slope beven et al 2021 this topographic index can be computed once for any basin and stored since the dynamic components of the algorithm are separated out as local soil moisture deficit terms the topmodel local soil moisture deficit parameter reduces the rainfall contribution to runoff for those grid points in the domain below the saturation point walter et al 2002 modified topmodel using a hydraulic conductivity function exponential in soil moisture and reformulating the equations to use soil moisture deficit in place of water table depth the modified approach named stopmodel models shallow soils and flow within them coleman and niemann 2013 devised a similar model based on the contributing area method focusing on the soil moisture results rather than on the runoff resulting in the equilibrium moisture from topography emt model the emt model includes additional terrain characteristics in the downscaling of soil moisture including parameters of deep drainage lateral flow radiative evapotranspiration and aerodynamic evapotranspiration further ranney et al 2015 updated the emt model to include vegetation intercept of precipitation and impact on infiltration as well as vegetative effects on evapotranspiration labeled as emt vs one challenge to their approach is the use of in situ observations to help calibrate the results which can be difficult when considering domains or regions with limited or no in situ observations the geowatch downscaling algorithm was designed using the topmodel stopmodel approach to downscale coarse resolution soil moisture products from weather analysis and prediction systems these models provide terrain and soil based downscaling combining high resolution static data computed from topography with coarse resolution dynamic soil moisture estimates this combination of fine scale static data and coarse scale dynamic data enables excellent computational efficiency and the simplicity of these models reflect the availability of data buytaert et al 2008 geowatch integrates pre computed topographic wetness index for all land areas the computation of downscaled soil moisture is based on the stopmodel approach modified to downscale volumetric soil moisture and includes additional water budget terms to allow for non percolative water transport effects a more complete description of this algorithm is in section 2 1 3 soil strength and soil moisture soil moisture contributes to the overall strength of soil in most instances higher water content reduces soil strength and reduces the pressure required to displace soil frankenstein et al 2006 wagner 2013 therefore soil moisture is important to remotely assessments and predictions of soil strength thus understanding and predicting soil water states is important for evaluation and prediction of vehicle mobility on unpaved roads or in cross country off road situations understanding the freeze thaw state of the ground is also important as roads and frozen ground strengthen when the surface and soil water is frozen but cause issues in situations during melt phase where sub surface frozen layers can trap water near the surface and decrease soil strength leading to increases in rutting slipperiness and increasing the potential for immobility frankenstein and koenig 2004 for soil moisture strength and freeze state predictions to be useful for ground operations the scale of predictions needs to capture impacts due to terrain elevation drainage routes vegetation condition impacts and other surface hydrology effects 2 geowatch architecture data and algorithms in this section we describe geowatch architecture algorithms techniques and the data on which the application relies we describe the soil moisture downscaling algorithm the links between the soil moisture and soil strength vehicle mobility algorithms terrain characterization datasets e g elevation vegetation soils and finally the weather model data ingested from the 557ww 2 1 geowatch architecture overview geowatch is a python based software application which combines global weather and terrain data layers from multiple sources to generate results viewable in a geospatially relevant web user interface or other gis viewer the geowatch input data layers consist of a variety of static and dynamic datasets the dynamic datasets are regularly updated land surface model products from land data assimilation systems or coupled land atmosphere prediction systems geowatch processes regional terrain data characterizing elevation soil texture vegetation maps harmonizing the datasets to provide consistent coordinate systems coverage and resolutions and inputs these harmonized datasets to its downscaling algorithms requests to geowatch result in fetching and harmonizing the necessary static data described in section 2 4 dynamic weather and land surface moisture temperature and energy balance analyses and forecasts section 2 5 these datasets are then inputs to geowatch algorithms for calculating downscaled soil moisture described in section 2 1 soil strength section 2 2 and vehicle mobility section 2 3 depending on the request requests are served through an ogc compliant wms 1 3 0 and wcs 1 0 0 open geospatial consortium 2005 interface as well as through custom geowatch web application program interface api endpoints to allow time series requests as well as detailed accounting of inputs for any requested layer described in section 3 0 the system architecture is represented diagrammatically in fig 1 2 2 soil moisture downscaling algorithm the soil moisture downscaling algorithm uses a two stage approach combining the stopmodel based approach walter et al 2002 with a flux based computation that accounts for the effects of vegetation to generate the higher resolution products the stopmodel approach includes topographic corrections based on topographic wetness index twi beven and kirkby 1979 buytaert et al 2008 aspect and slope to obtain those values we used pydem ueckermann et al 2018 a python package developed specifically for this project which was released as an open source package via github https github com creare com pydem we use stopmodel instead of topmodel because it explicitly routes shallow subsurface flow and we are interested in the soil moisture near the surface 2 2 1 topographic corrections the geowatch downscaling model uses slope and soil texture as a basis for redistributing soil moisture via a twi however the geowatch calculation of the twi was modified to use volumetric soil moisture instead of relative soil moisture to enable computation of flux values the terrain based downscaled volumetric soil moisture value θ is computed using the weather scale coarse resolution 10 km 10 km soil moisture as follows 1 θ θ w s 1 k λ λ 1 k ln k s ln k s where θ w s is the weather scale volumetric soil moisture the 1 k λ λ term is accounts for twi corrections and the 1 k ln k s ln k s term accounts for soil based corrections using differences in hydraulic conductivity between the weather scale and finer scales k 13 is a calibration parameter λ is the weather scale twi λ is the high resolution dem based twi k s is the high resolution hydraulic conductivity of the soil and ln k s is the natural log of the hydraulic conductivity spatially averaged over the weather scale block 2 2 2 flux corrections the second stage of the downscaling algorithm accounts water flux due to vegetation based factors e t and direct soil evaporation edir via the following equation 2 θ θ δ t f θ f θ w s where δ t is the time required for the soil moisture to drop from field capacity θ r e f to the weather scale soil moisture value θ w s and 3 f θ e t θ e d i r θ where e t represents the rate of soil moisture change due to vegetation and e d i r represents rate of soil moisture change for bare soil following chen and coauthors 1996 e t is given by 4 e t θ σ f e p θ θ w θ r e f θ w where σ f is the vegetation greenness fraction e p is the evapotranspiration rate θ r e f is the soil moisture at field capacity and θ w is the soil moisture value at wilting point for the reference soil texture the direct soil evaporation rate e d i r describes the direct soil evaporation following ek et al 2003 5 e d i r θ r d 1 r d ı e p 1 σ f θ θ r e f θ s θ r e f where θ s is the local saturated soil moisture and r d is the fraction of diffusive light i e related to cloudiness and not subject to the solar view factor ι with a default value of 0 15 the solar view factor ι is described by 6 ι α 1 α 2 n ˆ s u n n ˆ s u r f d α where α 1 and α 2 are the sun s azimuth at sunrise and sunset respectively while n ˆ s u n and n ˆ s u r f are unit vectors pointing at the sun and normal to the terrain respectively note that the weather scale flux terms use soil properties averaged to weather scale resolution soil the timestep δ t in equation 2 is determined by estimating how long it would take to reach the weather scale volumetric soil moisture starting from the soil moisture at field capacity using the flux at saturation 7 δ t δ t s θ w s θ r e f f θ s where δ t s c t s θ θ s θ θ s e θ θ s θ s and c t s 0 1 is a calibration coefficient finally the value of δ t is clipped between 0 and a maximum allowed value e g 30 days to ensure numerical stability 2 3 soil strength soil strength is of interest to the us army as an important input to logistical considerations for where vehicles can drive cross country as such the us army has its own soil strength algorithms which have been imported to run in geowatch taking the geowatch downscaled soil moisture as an input along with soils texture information geowatch includes components from the us army fast all season soil strength model frankenstein and koenig 2004 and previously included the soil moisture strength prediction model version ii smsp ii sullivan et al 1997 the geowatch computation of soil strength is based on the rating cone index rci the cone index ci is a gauge of the strength of the undisturbed soil and is measured by a cone penetrometer the rci is a gauge of soil strength after a vehicle passed over the location and is given by the ci value multiplied by the remolding index soil moisture s influence on the soil strength is described by the following equation 8 rci exp c 1 c 2 ln mc where c 1 and c 2 are coefficients based on the uscs soil texture classes and mc is the fractional soil moisture content by weight within geowatch the soil moisture is converted from a volumetric value into a mass weight value when passed to equation 8 note that this paper does not attempt to validate the soil strength algorithms as the us army has its own internal validations 2 4 cross country mobility algorithm the soil strength calculations outlined above can be fed into another us army algorithm to estimate vehicle mobility speeds based on vehicle properties and soil strength the mobility model within geowatch is an updated version of the us army standard mobility application interface stndmob ported to python stndmob calculates a cross country vehicle speed estimate for off road mobility for a suite a vehicle classes the stndmob application was initially developed in java by baylot jr et al 2005 and is best described as a set of mobility parameter tables that are computed using the nato reference mobility model nrmm ahlvin and haley 1992 the python version of stndmob was developed to enable the application to be linked either as a plug in into geographic information system software or integrated within a geospatial python application like geowatch the stndmob algorithm requires as input the dem elevation and slope land use and vegetation cover soil texture information soil strength given as a rating cone index rci value and climate zone information the application computes a cross country mobility vehicle speed for certain broad vehicle classifications though not for specific vehicles the user of the mobility output must understand the vehicle classifications to apply the information to relevant mission planning scenarios we describe the geowatch cross country mobility capabilities here as a demonstration of the utility of the downscaled soil moisture product and this paper does not attempt to validate the stndmob application which is developed owned and used by the us army 2 5 static data the foundational data in geowatch consists of terrain datasets that are not updated frequently including raster data layers of soil texture combined land use and land cover maps and elevation and slope data from digital elevation models dem many static data layers within geowatch represent aggregated datasets composited from multiple sources before feeding into soil moisture downscaling algorithms the terrain elevation composite layer is based on global 30 m resolution generated from usgs national elevation data at 30 m resolution ned us geological survey 2019 over the continental united states along with nasa shuttle radar topography mission srtm 30 m data for global locations between 60 latitude in regions were the ned or srtm data is not available the viewfinder panorama dem data is used albeit at a coarse 90 m resolution de ferranti 2014 the resolution of the elevation data sets the finest scale at which downscaled soil moisture products can be computed soil texture composite layers include both agricultural soil texture classes usda soil texture and engineering classes under the unified soil classification system soil texture layers uscs us army corp of engineers 1953 astm international 2006 stevens 1982 the uscs soil texture classification sorts soils according to their textural and plasticity qualities and by grouping with respect to soil performance as engineering construction materials the usda and uscs composite maps were generated by combining usda soil survey geographic database ssurgo and state soil geographic database statsgo from the usda web soil survey usda nrcs soil survey staff with the national geospatial intelligence agency soilscape 30 m soil texture database available for non us locations additionally soil texture maps for sweden and finland were obtained from european partners and are included for high resolution soils data in the composited data sources map layers of land use and vegetation information are a composite of the us geological survey 2016 national land cover database nlcd homer and coauthors 2020 yang and coauthors 2018 jin and coauthors 2019 within the u s army geospatial center geocover and nga visual navigation visnav land use and land cover maps outside the u s and nasa terra and aqua satellites moderate resolution spectroradiometer modis based land cover type mcd12q1 products friedl and sulla menashe 2019 any gaps in coverage from for these datasets are filled by using the land use land cover maps from the land information system output which is a modis based international geosphere biosphere programme igbp 14 category vegetation and land use map modified by noaa national centers for environmental prediction to include 3 classes of tundra lakes cropland natural vegetation and permanent wetland 2 6 dynamic input data the geowatch system incorporates gridded weather models and forecasts from the us air force usaf weather service 557th weather wing or 557ww obtained using an automated machine to machine open geospatial consortium ogc compliant web coverage service open geospatial consortium 2005 data download and ingestion service which retrieves model data from 557ww every 3 h the primary sources of geowatch weather and soil moisture data are gridded model runs generated by the 557ww versions of the lis eylander et al 2022 kemp et al 2022 yoon et al 2022 peters lidard and coauthors 2007 kumar et al 2008 which computes soil water mass and energy products and a 557ww specific implementation of the uk met model weather prediction system operated in a global configuration with output distributed as products of the galwem stoffler 2017 the lis model data are aggregated over time so that geowatch can generate downscaled soil moisture over its full range of stored lis model data from 2012 to the present the lis system is a global land data assimilation software system used to generate land surface soil moisture temperature and energy budget assessments using aggregated weather data from in situ observations satellite measurements and model estimates the current usaf operational version of lis generates products at a global 10 km spatial resolution on a latitude and longitude based raster grid the 557ww executes the lis system every 3 h for the 00 06 12 and 18 utc model cycles every day generating eight grided soil moisture analyses daily further the 557ww operational lis includes assimilated smap soil moisture observations a multi sensor and multi satellite precipitation analysis kemp et al 2022 and snow analysis yoon et al 2022 the galwem model is currently operated with a coupled land atmosphere configuration using the jules land surface model as the primary configuration galwem data is retrieved every 6 h pulling forecast soil moisture and surface weather products from the 557ww for the forecast cycles at 00 06 12 18 utc where each model run includes a 3 h forecast increment to 24 h in the future and 24 h forecast increments out to 144 h table 1 includes a list of 557ww lis products used within the geowatch system while table 2 includes a list of galwem parameters used by geowatch combining the static data layers with lis allows geowatch to produce on demand downscaled soil moisture at 30 m resolution globally across decades of historical data and including the galwem forecast model data allows forecasting soil moisture at up to 30 m resolution globally for several days these downscaled soil moisture estimates can then feed into high resolution soil strength and mobility estimates for us army analysis the difference between the lis and galwem soil moisture estimations causes a temporal discontinuity in downscaled soil moisture estimates from geowatch the 557ww operational lis model uses the noah lsm chen and coauthors 1996 ek and holtslag 2004 mahrt and ek 1984 with four soil layers of varying depths 0 10 cm 10 40 cm 40 100 cm 100 200 cm while the galwem model uses the joint uk land environment simulator best and coauthors 2011 clark and coauthors 2011 which computes soil moisture for 0 10 cm 10 35 cm 35 100 cm and 100 300 cm depths additionally the noah and jules land surface models differ in the computation of soil moisture a bias correction approach within geowatch or configuration change within the 557ww land surface models should be investigated in the future to avoid the discontinuity in soil moisture estimates between past and future additionally there are numerous other factors which could influence soil moisture which are not immediately considered by the lis or galwem models such as flood wildfire overland flow etc site studies examining the effects of these phenomena and developing models are needed in order to consider some of these effects 2 7 additional computing considerations calculating downscaled soil moisture over the globe is computationally expensive and would require vast resources to match the rate of lis and galwem model updates storing the outputs similarly would require enormous amounts of storage thus geowatch is built to generate downscaled soil moisture soil strength and vehicle mobility products entirely on demand for a given region and time speed was thus an important development constraint as geowatch must produce outputs fast enough for human users to browse in gis tools and to generate outputs across large regions as before a new model run makes them obsolete this speed and on demand access makes geowatch a useful tool for decision makers who can generate the downscaled geowatch outputs on demand as an example for a 20 km 20 km region of interest geowatch s map interface generates 256 pixel 256 pixel images requiring 9 images to cover the region at its maximum 30 m resolution generating one such 256 256 image for downscaled soil moisture takes roughly 5 s on our baseline demonstration server which is an 8 core machine with 64 gb of ram and a mixture of network and mounted ssd data storage thus geowatch would take 45 s to generate downscaled soil moisture for this region of interest when generated sequentially or 10 s in parallel on our demonstration machine 3 geowatch application user interface the geowatch application runs on a server that includes a web interface enabling users to interact with the geowatch data and algorithms using a slippy map i e similar to google maps based on the leaflet javascript library agafonkin v 2013 a login page allows non registered public users to see the map or access simple geowatch application programming interface api examples for machine to machine requests to geowatch fig 2 the map interface includes a layer menu fig 3 where users can select a specific geowatch product layer for display the application computes and serves the layer on demand and displays the results on the user s map interface fig 4 aside from static data which can be viewed none of the results are cached or precomputed the downscaling algorithm only operates on the region in the user s view and the resulting soil moisture resolution depends on the user s map zoom level additional products computed from the downscaled soil moisture including soil strength fig 5 and mobility maps available to registered users only are also only computed on demand based on the resolution of the user s map interface because of the on demand computing complex data products like soil moisture which must first calculate downscaled soil moisture may take several seconds and up to a minute to load depending on resolution and limited by the demonstration server s capacity the geowatch viewer can display 557ww lis and galwem model data terrain elevation soil texture vegetation land use all served from our custom ogc compliant wms 1 3 0 open geospatial consortium 2006 server as well as open streets maps and a satellite view additional advanced settings and layers are available to registered users including intermediate calculated layers from the calculation of downscaled soil moisture or soil strength geowatch enables users to request timeseries data for specific layers through the web user interface fig 6 or by using a custom restful http api timeseries data is returned in a comma separated file which can be read into most spreadsheet and data analysis applications additionally geowatch supports wcs 1 0 0 open geospatial consortium 2005 calls which return geotiff files that can be merged using gdal or other programs for use in a gis system 4 downscaled soil moisture validation results 4 1 validation using scan soil moisture sensors gambill et al 2020 evaluated geowatch against usda soil climate analysis network scan in situ soil moisture measurements at 127 sites to develop a bias correction algorithm for the downscaled volumetric soil moisture results their evaluation grouped the geowatch results by general soil texture class clay silty sandy loamy and presented an overall root mean square error rmse for each soil texture class they found the highest rmse for sandy soils 0 11 and the lowest rmse for clayey soils 0 08 gambill also computed a nash sutcliffe efficiency score nash and sutcliffe 1970 for each primary soil texture class and found that sandy soils had the lowest goodness of fit and clayey soils had the best these findings did not include a comparison of the goodness of geowatch to that of lower resolution alternatives like the lis soil moisture model while the accuracy of geowatch output is important comparison to alternative methods for soil moisture estimation are lacking here we compare geowatch soil moisture estimation accuracy with its weather scale input lis soil moisture to determine whether the downscaling adds value compared to the raw input as a ground truth baseline we use in situ observations from the international soil moisture network ismn dorigo and coauthors 2013 2011 for the period between january 1 and december 31 2018 specifically we compare the geowatch and lis 0 10 cm layer volumetric soil moisture results to usda scan schaefer et al 2007 5 cm soil moisture observations pulled from the ismn soil moisture probe network archive we use the ismn database to access the scan data because the ismn archival data include computed data quality flags for each observation dorigo and coauthors 2013 this evaluation specifically measured performance of geowatch and lis against a collection of 28 individual scan sites within the ismn data for the evaluation period we selected these sites to represent the separate soil textural categories within the usda agricultural soil texture classes aiming for at least three sites per soil texture category we also selected for the completeness of scan records for the 2018 observation period based on the number of good observations marked by the ismn quality flags and limited number of missing measurements the final site selection criterion was availability of lis data as locations near coastlines water bodies or other terrain features are not predicted by the lis model along with the soil moisture observations the soil temperature observation date and time latitude and longitude of each site and data quality flags are included in the data record we developed a set of analysis scripts to retrieve and analyze scan data from the ismn archive and geowatch and lis soil moisture estimates the scan measurements are datetime matched rather than temporally averaged to the 3 hourly lis and geowatch data and comparisons are limited to datetimes when the scan and geowatch soil moisture are both available each observation location model grid pair were evaluated independently and grouped according to usda soil texture class for group comparisons for most of the 10 soil texture classes at least three sites were used in the analysis however there were a limited number of clay and clay loam sites that met the data completeness criteria for each site we compute the temporal root mean square error rmse bias computed as model minus observation linear correlation coefficient r2 and normalized nash sutcliffe efficiency goodness of fit value nash and sutcliffe 1970 this comparison of the geowatch and lis output against scan in situ measurements focuses on a subset of the more than 203 scan observation sites in the ismn archive a few sites were omitted simply because the soil moisture record in the observations were reported as exceedingly wet relatively speaking or dry in the absence or presence of precipitation in the observations as reported by the scan network though those observations were not flagged in the ismn dataset some of those observation values may be accurate and represent ponding excessive drought or other natural processes but those processes are not necessarily represented by the land surface models within lis and geowatch which are limited in the range of soil moisture observations by both the wilting point minimum soil moisture value and soil porosity maximum soil moisture value additionally we did not consider sites characterized by organic soils wetlands or urban environments for each of the sites evaluated the pedon reports for each location was compared to the soil texture and vegetation information in the geowatch layers and the latitude and longitude information from the pedon reports was also cross referenced with the ismn data files google earth imagery for the location and site images to make sure the site was properly co located with geowatch grid cells this resulted in a few latitude and longitude updates to the ismn data files specifically including both adams ranch nm and sandy ridge ms used in this evaluation where the values in the ismn data file did not match the soil pedon reports from the usda website and our analysis indicated the pedon reports more accurately matched the site images and google earth hosted satellite imagery than the ismn location information after evaluating geowatch and lis at the 28 sites within this study both models effectively represent the wetting and drying as captured by the in situ measurements a brief evaluation of the precipitation gauge information obtained from the usda scan web tool compared to the lis precipitation analysis indicates the model captures precipitation events at the appropriate times though this study did not produce any specific evaluation of the precipitation skill in lis generally large increases in soil moisture in the observations were captured by the models fig 7 is a soil moisture graph from walnut gulch arizona and illustrates the above point well as the observed soil moisture includes numerous spikes in the observed data coinciding with similar jumps in the soil moisture from both lis and geowatch table 3 contains the 28 sites used in this study along with the resulting statistics for both the geowatch and lis comparisons to the observations many of the sites are adjacent to farms or are in locations characterized as grasslands or shrubland with very little if any vegetation the table includes the geowatch composite land cover classification for each site along with the usgs nlcd canopy coverage value of the 28 sites six sites had a vegetated canopy coverage value of 1 or greater of those sites three locations kyle canyon sellers lake and goodwin creek timer had vegetation canopy at greater than 40 and were listed as being within a forest mixed forest or broadleaf forest the sites were grouped into the following soil categories based on their location to within the geowatch composite soil texture map sand loamy sand silty loam sandy loam silty clay loam loam and finally clay and clay loam soils overall the statistics for geowatch scan comparisons were either equivalent to or better than the statistical lis scan comparisons at 15 locations with an additional 7 sites reported mixed statistical results where one or more metrics were better for geowatch than lis with a simultaneous decrease in another statistical measure only four sites reported worse statistical results for geowatch scan compared to the lis scan results the computed bias was reduced by at least 0 01 m3 m3 in the geowatch statistics at 17 locations nnse increased by at least the same increment at 16 locations and both r2 and rmse improved at 15 of the locations of those 11 sites reported more significant statistical improvements either with a bias reduction of 0 05 nnse or r2 value increasing by 0 10 or an rmse reduction of 0 05 the sites with the most significant difference in geowatch skill versus lis were those estimating soil moisture in sandy locations the geowatch downscaling generally produced a reduced model bias at the four sites characterized as purely sand fig 8 with at least one case the sandy hollow location where the model bias was reduced significantly with respect to the lis scan model bias the bias for the sandy hollow site was 0 14 in the lis scan comparisons while the geowatch scan model bias is 0 00 m3 m3 however the r2 values did not significantly change at that location between the models there was some minor improvement in the nnse values but a large reduction in the rmse for the geowatch scan results for the remaining sandy observation sites evaluated the geowatch downscaling had a smaller model bias as compared to the lis scan comparison reducing the bias from 0 12 to 0 07 at the hals canyon observation site 0 11 to 0 05 for spooky and a slight increase in geowatch bias from 0 16 to 0 17 at sellers canyon but with no change in the correlation for these locations the sellers canyon site was the only one with a slope less than 1 the sandy hollow site was the steepest terrain at 18 grade the vegetation for the three most improved sites was listed as zero percent with sellers canyon being 85 covered in canopy there were four locations with a sandy loam soil texture fig 9 with land use classification of either grassland or shrubland in the geowatch the composite land use map evaluated each of the sites were on slopes ranging from less than 1 at the morris farms location slope is 0 07 to nearly 20 for the lovell site in most of the locations the geowatch soil moisture results were less biased than the lis results with the bias reduction amount depending on the location the model bias was lower in the geowatch scan results than the lis scan results by a range of 0 02 0 07 with some large increases in nnse and r2 and reductions in rmse the only site that didn t follow that trend was lovell summit which recorded an increase in bias large decrease in nnse and a slight decrease in r2 and 0 04 m3 m3 increase in rmse the lovell summit site is a classified as shrubland in the composite land use data with an nlcd vegetation canopy coverage of 39 and has the highest composite terrain slope at nearly 20 for this soil texture category in our evaluation of the five sites the lovell site was the only site with nlcd canopy coverage above 0 the sites with clay and clay loam fig 10 and silty loam fig 11 soils overall bias nnse and rmse results were slightly lower for geowatch scan than they were for lis scan comparisons four of the sites within the silty loam category recorded higher bias reduced nnse and lower rmse for the geowatch scan results with higher bias at the goodwin creek timer rock springs and mt vernon locations and lower nnse and higher rmse at those sites plus at the eros data center location for the clay and clay loam soil grouping two of the sites north piedmont and onward both had slightly higher bias in the geowatch scan comparison while the vernon site had better statistical skill with nnse scores higher by 0 26 in the geowatch scan results lower bias and lower rms the land use classification ranged from either farmland pastureland or grassland to forest with slopes ranging from 0 4 mt vernon to 15 north piedmont overall the statistics point to an improvement in the downscaled soil moisture compared with the weather scale input a majority of sites had either a statistically significant beneficial change in statistical results or at the worst had nearly no change in statistical skill with results that were very similar to the lis skill however in some locations geowatch downscaling increased the bias significantly the contributing factors for that need further study 4 2 validation using single catchment studies to further validate geowatch s downscaling algorithms its high resolution soil moisture predictions were compared to in situ soil moisture data from two published high resolution single catchment studies these published catchment studies were performed in tarrawarra australia and shale hills pa respectively 4 2 1 tarrawarra catchment comparisons the first catchment comparisons used the tarrawarra data set western and grayson 1998 western et al 2004 which focused on a single catchment in southeastern australia fig 12 the authors of the tarrawara study describe the site as 1 1 text condensed from http people eng unimelb edu au aww tarrawarra catchdesc html the tarrawarra catchment is located in southern victoria australia 37 39 south 145 26 east the climate is temperate with a mean annual rainfall of 820 mm and a potential evapotranspiration of 830 mm the local terrain is undulating and has an elevation of approximately 100 mahd australian height datum the catchment area is 10 8 ha and the land is used for dryland grazing of dairy cows we believe that the tarrawarra catchment is representative of landscapes with relatively shallow soils of low to moderately high lateral permeability with an impeding layer at depth for which topography plays a significant role in routing water through the landscape and for climates ranging from temperate to subhumid these conditions apply over large parts of south eastern australia and in many other parts of the world the tarrawarra data set fig 13 includes two different sets of in situ soil moisture measurements a 500 point grid of time domain reflectometry tdr values 10m 20m spacing and a 20 point collection of neutron moisture measurement nmm values the tdr measurements were taken on different 13 dates while the nmm measurements were taken on 59 different test dates in addition to the soil moisture data numerous other site specific quantities are provided including digital elevation model survey data 5 m spatial resolution and sieve based particle size distribution measurements from 11 locations within the site while the tarrawara catchment site does not show much variability in land cover vegetation or soil type it does present some modest topographic changes therefore it provided an opportunity to validate geowatch s model for downscaling soil moisture based on topographic effects for these purposes geowatch soil moisture downscaling algorithms were applied to the measurement locations and dates utilized in the tarrawarra study in addition available site specific geospatial data such as site average soil moisture 5 m dem and soil texture data provided by the study authors were also used as inputs to geowatch in lieu of its default global geospatial inputs example comparisons are shown in fig 14 for one date 27 sep 1995 on the left is the weather scale soil moisture input provided to geowatch which is the mean value of the tdr soil moisture measurements for this date in the middle is a map of the downscaled soil moisture estimated by geowatch on the right is a map of the in situ tdr measurements as can be seen geowatch successfully predicts the spatially distributed structure of the reported tdr soil moisture data fig 15 compares the geowatch predictions to the in situ tdr measurements for several other dates it is evident from the tdr measurements that topography has a prominent influence on the spatial distribution of soil moisture and that geowatch s downscaling model reproduces this spatial distribution moreover the only times that topography does not strongly influence the soil moisture pattern is when the site is extremely dry see 1996 02 23 top left or extremely wet see 1996 09 02 bottom right presumably the reduced topographic influence under these conditions is due to inadequate water to redistribute dry dates or overall saturation with water wet dates and geowatch likewise predicts a dampened topographic effect under these conditions to further assess the performance of geowatch s downscaling algorithms using the tarrawarra dataset the rms error between geowatch s output downscaled soil moisture and the reported tdr values were computed for each of the 13 dates where tdr values are available we calculate an rms error between the tdr and geowatch downscaled soil moisture values these values are plotted on the vertical axis of fig 16 we compare against the difference between geowatch s downscaled soil moisture and the site wide mean soil moisture from the tdr measurements these values are plotted on the horizontal axis of fig 16 values below the dashed 1 1 line in fig 16 indicate reduced error in geowatch estimates vs the mean as can be seen in fig 16 geowatch provides more accurate soil moisture maps for 9 of the 13 dates 69 compared with simply using the site wide mean tdr value for that date looking across all dates the site mean performs well the rms error is only 0 0352 m3 m3 absolute volumetric soil moisture geowatch slightly outperformed this metric by reducing the rms error to 0 0321 m3 m3 in addition to the tdr values the tarrawarra data set also includes soil moisture measurements performed by nmm the nmm data has only 20 locations per date versus 508 locations per date for tdr so the nmm data is not good at conveying spatial structure however it is available for 59 dates versus 13 for tdr so we use it for quantifying error over a denser temporal range using the same methodology as presented above for tdr we computed the geowatch error relative to the mean nmm measurements fig 17 shows that geowatch downscaled soil moisture provides an improvement over the soil moisture measurements for 56 of the 59 dates 95 the overall error across all dates is reduced from 0 040 m3 m3 using the site mean value to 0 030 m3 m3 using the geowatch values 4 2 2 shale hills catchment comparisons shale hills is another detailed catchment site with publicly available data naithani and baldwin 2015 enabling detailed comparison geowatch s downscaled soil moisture predictions the shale hills watershed is a small scale 500m 300m forested and v shaped first order watershed with an area of 0 08 km2 8 ha in central pennsylvania usa fig 18 this watershed is part of the susquehanna shale hills critical zone observatory sshczo the mean annual temperature is 10 c and the mean annual precipitation is 107 cm shi et al 2015 the topography spans about 70m fig 19 the publicly available data for this site includes dem data at 0 5m 0 5m resolution fig 20 there is a weather station and flux tower as well as an outlet gauge for the stream that drains the catchment in situ soil moisture measurements are available from time domain reflectometry tdr for up to 106 locations see yellow and green markers on fig 20 though typically data is only available for approximately 61 of the sites the data set includes soil moisture measurements for 76 dates that span 2007 2010 most of the tdr measurements were taken manually over the course of a day though 16 of the locations were measured using automated equipment as with the tarrawarra analysis we used the site specific dem data and the site wide mean soil moisture for each day as inputs to geowatch after using geowatch to estimate the local soil moisture at each of the tdr sensor locations for each of the 74 available dates we compared performance of geowatch soil moisture with the spatial mean of tdr measurements for each date which represents a larger scale measurement of soil moisture in the area though higher resolution than weather scale as can be seen in fig 21 geowatch provides more accurate soil moisture maps for 55 of the 74 dates 74 compared with the site wide mean tdr value for that date looking across all dates the site mean tdr measurement has an rms error of 0 060 m3 m3 while geowatch reduces that error to 0 054 m3 m3 5 summary and conclusions the geowatch system was developed to bridge the gap between coarse resolution gridded weather products and fine scale needs of military and other communities the system ingests gridded lis and galwem weather products from the usaf combined with a number of high resolution terrain characterization layers terrain elevation vegetation and soils masks to compute higher resolution maps of soil moisture products based on the resolution of the terrain data the geowatch system also includes an interface that enables users to interact with the data generate maps for any region of the globe evaluate time series of products and interrogate the data additionally the geowatch software has been linked successfully to army soil strength and the army standard mobility application interface tool to generate maps of vehicle cross country vehicle speed estimates which provides a unique machine to machine capability for delivering weather informed impacts to downstream users the geowatch system has been evaluated against in situ observations from the usda scan network via the ismn and determined to provide results that are statistically similar or better than the input weather model results from the usaf in most cases some of the statistical evaluations point to significant improvements while there are also some areas of further study and improvement needed these scan comparisons offer a picture of the temporal predictive value of geowatch downscaled soil moisture and its lis inputs but give little insight into the spatial structure offered by the downscaling technique further evaluation of the geowatch products against higher resolution soil moisture networks to assess the downscaling accuracy are needed as well as a global set of soil moisture observations to assess how variations in terrain vegetation slope and soils affect geowatch downscaled soil moisture and to understand areas for improvement we compared geowatch s downscaled soil moisture to data from the published tarrawarra and shale hills high resolution catchment studies at tarrawarra we found that the geowatch models accurately captured the topographically influence spatial distribution of soil moisture quantitatively using tarrawarra tdr data geowatch downscaling resulted in lower rms error for 9 of 13 dates 69 using tarrawarra nmm data geowatch resulted in lower rms error for 56 of 59 dates 95 repeating the analysis for shale hills tdr data geowatch resulted in lower rms error for 55 of 74 dates 74 these small catchment studies offer a better view of the value of downscaled soil moisture but are still limited to small areas smaller than a lis grid tile and thus do not offer evaluation of the effects of soil type vegetation slope and terrain variations a larger study area and more diverse study areas around the globe are needed to understand the weaknesses and strengths of geowatch the lis system was designed to provide added functionality and utility to the global weather products generated by complex land data assimilation systems and coupled land atmosphere weather prediction systems the value of the geowatch system is very much coupled to the accuracy of those complex global analyses and forecasts any improvements made to the lis including the use of remotely sensed soil moisture observations and or galwem systems to provide more accurate and relevant analyses will result in better inputs to geowatch leading to more accurate results software and data availability the geowatch application includes an open access website that allows public viewing of downscaled soil moisture results at https mobility crearecomputing com further an example python based script is available that enables users to reproduce the downscaling algorithm is available through github at https github com creare com podpac examples blob main notebooks 5 datalib smap smap downscaling example application ipynb access to the army standard mobility application and results are restricted declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the development of the geowatch application by creare llc was supported by a us army corp of engineers small business innovation research award numbers w913e5 13 c 0005 and w913e5 14 c 0001 usace monitoring evaluation and technical transition support was provided by multiple us army supported applied research and development programs including the science and technical objective research program army terrestrial environmental modeling and intelligence system artemis the simulation tools for combat vehicle robotics geospatial tools for mobility work unit and intelligent environmental battlespace awareness integration task finally the authors would like to thank the four anonymous reviewers for their careful and constructive review comments and suggestions that helped to strengthen this manuscript 
25468,accurately estimating soil moisture at fine resolutions scales approaching a few meters has been a target of a broad community involved in soil moisture research and application development from agriculture support improving land atmosphere interaction in weather and climate prediction and hydrologic modeling to a variety of military applications the us army interest in high spatial resolution soil moisture products is based on the need to better understand soil impacts on broad range of applications including but not limited to movement and maneuver and logistics the geospatial weather affected terrain conditions and hazards geowatch application was developed to support army needs for high resolution soil moisture products this paper describes the geowatch application development of the high resolution soil moisture analysis and prediction capability and linkages to downstream army applications that deliver weather informed content to support decision making this paper also describes product evaluation and validation efforts to understand application performance keywords soil moisture downscaling gis soil modeling model validation soil strength decision support data availability data will be made available on request 1 introduction background the us army requires an advanced knowledge of high resolution weather and terrain conditions in order to support global operations szymber 2006 understanding the variability of the dynamic environment including weather and climate effects on terrain conditions is critical to effectively characterizing the risks within the operating environment the broad range of weather impacts require the us army corp of engineers together with service partner air force weather to develop and improve algorithms models and applications that better integrate weather and climate products with army geospatial information system gis based software systems eylander et al 2020 the gis enabled display of weather and terrain information supply geographic context to enable informed decision making for military planning and preparation department of the army 2019 engineering support to partner countries usaid 2022 department of the army 2009a 2009b and provide awareness for global water and food security applications verdin et al 2005 funk and verdin 2010 arsenault et al 2020 peters lidard et al 2021 funk et al 2019 such as the us agency for international development famine early warning systems network fews net www fews net and us department of agriculture crop explorer https ipad fas usda gov cropexplorer default aspx while global land data assimilation and soil moisture modeling systems have been developed kumar et al 2006 peters lidard and coauthors 2007 rodell et al 2004 to deliver global estimates of soil moisture at weather scale on the order of hundreds of meters to tens of kilometers most army applications need environmental products delivered at resolutions approaching tens of meters or finer to support tactical decision making the us air force 557th weather wing 557ww operates coupled land atmosphere weather and climate models to compute soil moisture temperature and surface energy budget products that blend the strengths of remote sensing with land surface models via assimilation lawrence et al 2019 niu et al 2011 yang et al 2011 best and coauthors 2011 clark and coauthors 2011 chen and coauthors 1996 ek et al 2003 koster et al 2000 kumar et al 2008 reichle et al 2002 reichle et al 2010 the 557ww computes these global models at 3 hourly intervals daily at 10 km spatial resolutions 17 km resolution forecasts eylander et al 2022 kemp et al 2022 yoon et al 2022 which is coarse in spatial resolution compared to the needs of the army movement and maneuvering which can occur both on and off road need a complete accounting for terrain conditions that impact mobility at resolutions on the order of meters or finer to deliver risk criteria for go no go decision making or routing decision to account for and avoid weather impacted terrain obstacles this paper explores geowatch a tool which generates soil moisture analyses and forecasts at finer scale resolutions spatially less than 100 m with the ability to rapidly compute high resolution soil moisture for any domain size globally this capability enables the army to directly link real time weather and climate data to decision support applications and better account for weather and climate impacts on operational environments 1 1 introduction to geowatch the geospatial weather affected terrain conditions and hazards geowatch system was designed to deliver high resolution global estimates of soil moisture based on gridded weather analyses and predictions the system delivers high resolution gridded soil moisture analysis and forecast using the 557ww weather terrain analyses and forecasts as an initial baseline then downscales those products to a 30 90 m spatial resolution on demand for any global region further the geowatch soil moisture products are dynamically linked with army developed algorithms to compute soil strength this supports army maneuver applications to deliver a first of a kind prediction of weather informed maneuver impacts computed for any region globally within minutes of computational time this paper describes the geowatch system including the input weather and terrain data downscaling algorithm link to the army applications and provides an assessment of our evaluation of the geowatch downscaled soil moisture results geowatch ingests and dynamically links gridded soil moisture analyses and forecasts from the 557ww as the low resolution basis for its downscaled soil moisture and soil strength products the dependence on weather data provided by the 557ww is based on joint us army and us air force policies and agreements department of the army 2021 detailing the usaf operational weather support to the us army since the usaf operates the lis land data assimilation system to compute global soil moisture temperature and surface energy budget analyses and the global air land weather exploitation model galwem stoffler 2017 a usaf specific version of the united kingdom meteorology office unified model ukmet model geowatch relies on those input data sources to initialize the downscaling model the 557ww implementation of lis provides 10 km global 3 hourly analyses of soil moisture and temperature for four soil layers 0 10 cm 10 40 cm 40 100 cm and 100 200 cm depths along with surface and subsurface runoff sensible latent and ground heat fluxes surface evaporation and evapotranspiration 1 2 previous efforts downscaling is the process of computing a variable finer spatial resolution from some coarser level inputs there are many published methods to estimate higher resolution soil moisture and or downscale soil moisture from coarser resolution inputs some methods include downscaling coarse resolution passive microwave remote sensing observations using methods that blend products from multiple sensors piles et al 2016 peng et al 2016 choi and hur 2012 using complex statistical and or machine learning methods to downscale srivastava et al 2013 loew and mauser 2008 verhoest et al 2015 chai et al 2011 conducting multi satellite downscaling combined with model products fang et al 2013 2018 using the active and passive measurements from the nasa soil moisture active passive satellite entekhabi et al 2010 das et al 2011 2014 and downscaling algorithms that are driven by either coarse resolution input model estimates remotely sensed analyses or both coleman and niemann 2013 ranney et al 2015 vergopolan et al 2021 peng et al 2017 and fang 2020 each provide a good overview of soil moisture downscaling algorithms further there are several downscaling efforts focused on global land data assimilation using soil moisture observations from passive microwave and or other and computing products at higher resolution kumar et al 2009 2012 2014 santanello et al 2016 xue et al 2021 lievens et al 2015 draper and reichle 2015 reichle et al 2017 2019 sahoo et al 2013 many remote sensing based methods compute products at resolutions finer than 1 km spatially the smap hydroblocks product vergopolan et al 2021 is a more complicated blend of the model and remote sensing downscaling approach and provides some of the highest resolution output producing 30 m soil moisture results over the continental unites states hydroblocks uses a land surface model radiative transfer model high resolution digital terrain data blended with smap remote sensing observations to downscale the soil moisture results non downscaling approaches that focus on using high resolution thermal remote sensing imagery to estimate soil moisture and surface energy balance result in soil moisture products on the order of a 1 km but are still limited to the resolution of the finest spatial scale satellite data available or having a cloud free scene of which to obtain the imagery bastiaanssen et al 1998 allen et al 2007 allen 2011 hong 2008 anderson et al 1997 2007 2011 these approaches partition the surface energy sensible and latent energy flux estimate using remotely sensed vegetation parameters e g ndvi and then estimate the soil moisture based on evapotranspiration models the surface energy balance algorithm for land sebal bastiaanssen et al 1998 and atmosphere land exchange inverse alexi anderson et al 1997 2007 2011 methods both have demonstrated using higher resolution optical imagery to characterize land energy and or mass values the sebal approach supports finer scale soil moisture using infrared satellite measurements of vegetation supplying both surface and root zone soil moisture at the resolution of the infrared satellite imagery the alexi approach resolves the surface temperature from the infrared results partitions the temperatures into a bare ground and canopy surface values and then estimates the vegetation evapotranspiration and uses a priestly taylor priestley and taylor 1972 approximation to estimate the soil moisture values while these methods deliver high resolution soil water mass and energy parameters the resulting products are computed at the resolution of the satellite data available and are reliant on cloud free scenes to generate products the challenge with relying on high resolution imagery data is that a real time global estimate is still limited by scanning methodologies of the imaging satellites and or weather affects cloud cover which can impact large amounts of land at any given time topmodel is one of the longest used methods for computing basin level rainfall runoff relationships from topography an approach initially designed by beven and kirkby 1979 and beven et al 1984 topmodel was designed to capture the runoff from terrain contributing areas and was the first hydrology model to incorporate topography beven et al 2020 topmodel assumes that the hydraulic gradient can be approximated using the slope angle the local transmissivity and storage in rainfall equivalent depth units an intermediate step in the topmodel generates the topographic index the upslope contributing area divided by the hydraulic gradient where the hydraulic gradient is a function of the terrain slope beven et al 2021 this topographic index can be computed once for any basin and stored since the dynamic components of the algorithm are separated out as local soil moisture deficit terms the topmodel local soil moisture deficit parameter reduces the rainfall contribution to runoff for those grid points in the domain below the saturation point walter et al 2002 modified topmodel using a hydraulic conductivity function exponential in soil moisture and reformulating the equations to use soil moisture deficit in place of water table depth the modified approach named stopmodel models shallow soils and flow within them coleman and niemann 2013 devised a similar model based on the contributing area method focusing on the soil moisture results rather than on the runoff resulting in the equilibrium moisture from topography emt model the emt model includes additional terrain characteristics in the downscaling of soil moisture including parameters of deep drainage lateral flow radiative evapotranspiration and aerodynamic evapotranspiration further ranney et al 2015 updated the emt model to include vegetation intercept of precipitation and impact on infiltration as well as vegetative effects on evapotranspiration labeled as emt vs one challenge to their approach is the use of in situ observations to help calibrate the results which can be difficult when considering domains or regions with limited or no in situ observations the geowatch downscaling algorithm was designed using the topmodel stopmodel approach to downscale coarse resolution soil moisture products from weather analysis and prediction systems these models provide terrain and soil based downscaling combining high resolution static data computed from topography with coarse resolution dynamic soil moisture estimates this combination of fine scale static data and coarse scale dynamic data enables excellent computational efficiency and the simplicity of these models reflect the availability of data buytaert et al 2008 geowatch integrates pre computed topographic wetness index for all land areas the computation of downscaled soil moisture is based on the stopmodel approach modified to downscale volumetric soil moisture and includes additional water budget terms to allow for non percolative water transport effects a more complete description of this algorithm is in section 2 1 3 soil strength and soil moisture soil moisture contributes to the overall strength of soil in most instances higher water content reduces soil strength and reduces the pressure required to displace soil frankenstein et al 2006 wagner 2013 therefore soil moisture is important to remotely assessments and predictions of soil strength thus understanding and predicting soil water states is important for evaluation and prediction of vehicle mobility on unpaved roads or in cross country off road situations understanding the freeze thaw state of the ground is also important as roads and frozen ground strengthen when the surface and soil water is frozen but cause issues in situations during melt phase where sub surface frozen layers can trap water near the surface and decrease soil strength leading to increases in rutting slipperiness and increasing the potential for immobility frankenstein and koenig 2004 for soil moisture strength and freeze state predictions to be useful for ground operations the scale of predictions needs to capture impacts due to terrain elevation drainage routes vegetation condition impacts and other surface hydrology effects 2 geowatch architecture data and algorithms in this section we describe geowatch architecture algorithms techniques and the data on which the application relies we describe the soil moisture downscaling algorithm the links between the soil moisture and soil strength vehicle mobility algorithms terrain characterization datasets e g elevation vegetation soils and finally the weather model data ingested from the 557ww 2 1 geowatch architecture overview geowatch is a python based software application which combines global weather and terrain data layers from multiple sources to generate results viewable in a geospatially relevant web user interface or other gis viewer the geowatch input data layers consist of a variety of static and dynamic datasets the dynamic datasets are regularly updated land surface model products from land data assimilation systems or coupled land atmosphere prediction systems geowatch processes regional terrain data characterizing elevation soil texture vegetation maps harmonizing the datasets to provide consistent coordinate systems coverage and resolutions and inputs these harmonized datasets to its downscaling algorithms requests to geowatch result in fetching and harmonizing the necessary static data described in section 2 4 dynamic weather and land surface moisture temperature and energy balance analyses and forecasts section 2 5 these datasets are then inputs to geowatch algorithms for calculating downscaled soil moisture described in section 2 1 soil strength section 2 2 and vehicle mobility section 2 3 depending on the request requests are served through an ogc compliant wms 1 3 0 and wcs 1 0 0 open geospatial consortium 2005 interface as well as through custom geowatch web application program interface api endpoints to allow time series requests as well as detailed accounting of inputs for any requested layer described in section 3 0 the system architecture is represented diagrammatically in fig 1 2 2 soil moisture downscaling algorithm the soil moisture downscaling algorithm uses a two stage approach combining the stopmodel based approach walter et al 2002 with a flux based computation that accounts for the effects of vegetation to generate the higher resolution products the stopmodel approach includes topographic corrections based on topographic wetness index twi beven and kirkby 1979 buytaert et al 2008 aspect and slope to obtain those values we used pydem ueckermann et al 2018 a python package developed specifically for this project which was released as an open source package via github https github com creare com pydem we use stopmodel instead of topmodel because it explicitly routes shallow subsurface flow and we are interested in the soil moisture near the surface 2 2 1 topographic corrections the geowatch downscaling model uses slope and soil texture as a basis for redistributing soil moisture via a twi however the geowatch calculation of the twi was modified to use volumetric soil moisture instead of relative soil moisture to enable computation of flux values the terrain based downscaled volumetric soil moisture value θ is computed using the weather scale coarse resolution 10 km 10 km soil moisture as follows 1 θ θ w s 1 k λ λ 1 k ln k s ln k s where θ w s is the weather scale volumetric soil moisture the 1 k λ λ term is accounts for twi corrections and the 1 k ln k s ln k s term accounts for soil based corrections using differences in hydraulic conductivity between the weather scale and finer scales k 13 is a calibration parameter λ is the weather scale twi λ is the high resolution dem based twi k s is the high resolution hydraulic conductivity of the soil and ln k s is the natural log of the hydraulic conductivity spatially averaged over the weather scale block 2 2 2 flux corrections the second stage of the downscaling algorithm accounts water flux due to vegetation based factors e t and direct soil evaporation edir via the following equation 2 θ θ δ t f θ f θ w s where δ t is the time required for the soil moisture to drop from field capacity θ r e f to the weather scale soil moisture value θ w s and 3 f θ e t θ e d i r θ where e t represents the rate of soil moisture change due to vegetation and e d i r represents rate of soil moisture change for bare soil following chen and coauthors 1996 e t is given by 4 e t θ σ f e p θ θ w θ r e f θ w where σ f is the vegetation greenness fraction e p is the evapotranspiration rate θ r e f is the soil moisture at field capacity and θ w is the soil moisture value at wilting point for the reference soil texture the direct soil evaporation rate e d i r describes the direct soil evaporation following ek et al 2003 5 e d i r θ r d 1 r d ı e p 1 σ f θ θ r e f θ s θ r e f where θ s is the local saturated soil moisture and r d is the fraction of diffusive light i e related to cloudiness and not subject to the solar view factor ι with a default value of 0 15 the solar view factor ι is described by 6 ι α 1 α 2 n ˆ s u n n ˆ s u r f d α where α 1 and α 2 are the sun s azimuth at sunrise and sunset respectively while n ˆ s u n and n ˆ s u r f are unit vectors pointing at the sun and normal to the terrain respectively note that the weather scale flux terms use soil properties averaged to weather scale resolution soil the timestep δ t in equation 2 is determined by estimating how long it would take to reach the weather scale volumetric soil moisture starting from the soil moisture at field capacity using the flux at saturation 7 δ t δ t s θ w s θ r e f f θ s where δ t s c t s θ θ s θ θ s e θ θ s θ s and c t s 0 1 is a calibration coefficient finally the value of δ t is clipped between 0 and a maximum allowed value e g 30 days to ensure numerical stability 2 3 soil strength soil strength is of interest to the us army as an important input to logistical considerations for where vehicles can drive cross country as such the us army has its own soil strength algorithms which have been imported to run in geowatch taking the geowatch downscaled soil moisture as an input along with soils texture information geowatch includes components from the us army fast all season soil strength model frankenstein and koenig 2004 and previously included the soil moisture strength prediction model version ii smsp ii sullivan et al 1997 the geowatch computation of soil strength is based on the rating cone index rci the cone index ci is a gauge of the strength of the undisturbed soil and is measured by a cone penetrometer the rci is a gauge of soil strength after a vehicle passed over the location and is given by the ci value multiplied by the remolding index soil moisture s influence on the soil strength is described by the following equation 8 rci exp c 1 c 2 ln mc where c 1 and c 2 are coefficients based on the uscs soil texture classes and mc is the fractional soil moisture content by weight within geowatch the soil moisture is converted from a volumetric value into a mass weight value when passed to equation 8 note that this paper does not attempt to validate the soil strength algorithms as the us army has its own internal validations 2 4 cross country mobility algorithm the soil strength calculations outlined above can be fed into another us army algorithm to estimate vehicle mobility speeds based on vehicle properties and soil strength the mobility model within geowatch is an updated version of the us army standard mobility application interface stndmob ported to python stndmob calculates a cross country vehicle speed estimate for off road mobility for a suite a vehicle classes the stndmob application was initially developed in java by baylot jr et al 2005 and is best described as a set of mobility parameter tables that are computed using the nato reference mobility model nrmm ahlvin and haley 1992 the python version of stndmob was developed to enable the application to be linked either as a plug in into geographic information system software or integrated within a geospatial python application like geowatch the stndmob algorithm requires as input the dem elevation and slope land use and vegetation cover soil texture information soil strength given as a rating cone index rci value and climate zone information the application computes a cross country mobility vehicle speed for certain broad vehicle classifications though not for specific vehicles the user of the mobility output must understand the vehicle classifications to apply the information to relevant mission planning scenarios we describe the geowatch cross country mobility capabilities here as a demonstration of the utility of the downscaled soil moisture product and this paper does not attempt to validate the stndmob application which is developed owned and used by the us army 2 5 static data the foundational data in geowatch consists of terrain datasets that are not updated frequently including raster data layers of soil texture combined land use and land cover maps and elevation and slope data from digital elevation models dem many static data layers within geowatch represent aggregated datasets composited from multiple sources before feeding into soil moisture downscaling algorithms the terrain elevation composite layer is based on global 30 m resolution generated from usgs national elevation data at 30 m resolution ned us geological survey 2019 over the continental united states along with nasa shuttle radar topography mission srtm 30 m data for global locations between 60 latitude in regions were the ned or srtm data is not available the viewfinder panorama dem data is used albeit at a coarse 90 m resolution de ferranti 2014 the resolution of the elevation data sets the finest scale at which downscaled soil moisture products can be computed soil texture composite layers include both agricultural soil texture classes usda soil texture and engineering classes under the unified soil classification system soil texture layers uscs us army corp of engineers 1953 astm international 2006 stevens 1982 the uscs soil texture classification sorts soils according to their textural and plasticity qualities and by grouping with respect to soil performance as engineering construction materials the usda and uscs composite maps were generated by combining usda soil survey geographic database ssurgo and state soil geographic database statsgo from the usda web soil survey usda nrcs soil survey staff with the national geospatial intelligence agency soilscape 30 m soil texture database available for non us locations additionally soil texture maps for sweden and finland were obtained from european partners and are included for high resolution soils data in the composited data sources map layers of land use and vegetation information are a composite of the us geological survey 2016 national land cover database nlcd homer and coauthors 2020 yang and coauthors 2018 jin and coauthors 2019 within the u s army geospatial center geocover and nga visual navigation visnav land use and land cover maps outside the u s and nasa terra and aqua satellites moderate resolution spectroradiometer modis based land cover type mcd12q1 products friedl and sulla menashe 2019 any gaps in coverage from for these datasets are filled by using the land use land cover maps from the land information system output which is a modis based international geosphere biosphere programme igbp 14 category vegetation and land use map modified by noaa national centers for environmental prediction to include 3 classes of tundra lakes cropland natural vegetation and permanent wetland 2 6 dynamic input data the geowatch system incorporates gridded weather models and forecasts from the us air force usaf weather service 557th weather wing or 557ww obtained using an automated machine to machine open geospatial consortium ogc compliant web coverage service open geospatial consortium 2005 data download and ingestion service which retrieves model data from 557ww every 3 h the primary sources of geowatch weather and soil moisture data are gridded model runs generated by the 557ww versions of the lis eylander et al 2022 kemp et al 2022 yoon et al 2022 peters lidard and coauthors 2007 kumar et al 2008 which computes soil water mass and energy products and a 557ww specific implementation of the uk met model weather prediction system operated in a global configuration with output distributed as products of the galwem stoffler 2017 the lis model data are aggregated over time so that geowatch can generate downscaled soil moisture over its full range of stored lis model data from 2012 to the present the lis system is a global land data assimilation software system used to generate land surface soil moisture temperature and energy budget assessments using aggregated weather data from in situ observations satellite measurements and model estimates the current usaf operational version of lis generates products at a global 10 km spatial resolution on a latitude and longitude based raster grid the 557ww executes the lis system every 3 h for the 00 06 12 and 18 utc model cycles every day generating eight grided soil moisture analyses daily further the 557ww operational lis includes assimilated smap soil moisture observations a multi sensor and multi satellite precipitation analysis kemp et al 2022 and snow analysis yoon et al 2022 the galwem model is currently operated with a coupled land atmosphere configuration using the jules land surface model as the primary configuration galwem data is retrieved every 6 h pulling forecast soil moisture and surface weather products from the 557ww for the forecast cycles at 00 06 12 18 utc where each model run includes a 3 h forecast increment to 24 h in the future and 24 h forecast increments out to 144 h table 1 includes a list of 557ww lis products used within the geowatch system while table 2 includes a list of galwem parameters used by geowatch combining the static data layers with lis allows geowatch to produce on demand downscaled soil moisture at 30 m resolution globally across decades of historical data and including the galwem forecast model data allows forecasting soil moisture at up to 30 m resolution globally for several days these downscaled soil moisture estimates can then feed into high resolution soil strength and mobility estimates for us army analysis the difference between the lis and galwem soil moisture estimations causes a temporal discontinuity in downscaled soil moisture estimates from geowatch the 557ww operational lis model uses the noah lsm chen and coauthors 1996 ek and holtslag 2004 mahrt and ek 1984 with four soil layers of varying depths 0 10 cm 10 40 cm 40 100 cm 100 200 cm while the galwem model uses the joint uk land environment simulator best and coauthors 2011 clark and coauthors 2011 which computes soil moisture for 0 10 cm 10 35 cm 35 100 cm and 100 300 cm depths additionally the noah and jules land surface models differ in the computation of soil moisture a bias correction approach within geowatch or configuration change within the 557ww land surface models should be investigated in the future to avoid the discontinuity in soil moisture estimates between past and future additionally there are numerous other factors which could influence soil moisture which are not immediately considered by the lis or galwem models such as flood wildfire overland flow etc site studies examining the effects of these phenomena and developing models are needed in order to consider some of these effects 2 7 additional computing considerations calculating downscaled soil moisture over the globe is computationally expensive and would require vast resources to match the rate of lis and galwem model updates storing the outputs similarly would require enormous amounts of storage thus geowatch is built to generate downscaled soil moisture soil strength and vehicle mobility products entirely on demand for a given region and time speed was thus an important development constraint as geowatch must produce outputs fast enough for human users to browse in gis tools and to generate outputs across large regions as before a new model run makes them obsolete this speed and on demand access makes geowatch a useful tool for decision makers who can generate the downscaled geowatch outputs on demand as an example for a 20 km 20 km region of interest geowatch s map interface generates 256 pixel 256 pixel images requiring 9 images to cover the region at its maximum 30 m resolution generating one such 256 256 image for downscaled soil moisture takes roughly 5 s on our baseline demonstration server which is an 8 core machine with 64 gb of ram and a mixture of network and mounted ssd data storage thus geowatch would take 45 s to generate downscaled soil moisture for this region of interest when generated sequentially or 10 s in parallel on our demonstration machine 3 geowatch application user interface the geowatch application runs on a server that includes a web interface enabling users to interact with the geowatch data and algorithms using a slippy map i e similar to google maps based on the leaflet javascript library agafonkin v 2013 a login page allows non registered public users to see the map or access simple geowatch application programming interface api examples for machine to machine requests to geowatch fig 2 the map interface includes a layer menu fig 3 where users can select a specific geowatch product layer for display the application computes and serves the layer on demand and displays the results on the user s map interface fig 4 aside from static data which can be viewed none of the results are cached or precomputed the downscaling algorithm only operates on the region in the user s view and the resulting soil moisture resolution depends on the user s map zoom level additional products computed from the downscaled soil moisture including soil strength fig 5 and mobility maps available to registered users only are also only computed on demand based on the resolution of the user s map interface because of the on demand computing complex data products like soil moisture which must first calculate downscaled soil moisture may take several seconds and up to a minute to load depending on resolution and limited by the demonstration server s capacity the geowatch viewer can display 557ww lis and galwem model data terrain elevation soil texture vegetation land use all served from our custom ogc compliant wms 1 3 0 open geospatial consortium 2006 server as well as open streets maps and a satellite view additional advanced settings and layers are available to registered users including intermediate calculated layers from the calculation of downscaled soil moisture or soil strength geowatch enables users to request timeseries data for specific layers through the web user interface fig 6 or by using a custom restful http api timeseries data is returned in a comma separated file which can be read into most spreadsheet and data analysis applications additionally geowatch supports wcs 1 0 0 open geospatial consortium 2005 calls which return geotiff files that can be merged using gdal or other programs for use in a gis system 4 downscaled soil moisture validation results 4 1 validation using scan soil moisture sensors gambill et al 2020 evaluated geowatch against usda soil climate analysis network scan in situ soil moisture measurements at 127 sites to develop a bias correction algorithm for the downscaled volumetric soil moisture results their evaluation grouped the geowatch results by general soil texture class clay silty sandy loamy and presented an overall root mean square error rmse for each soil texture class they found the highest rmse for sandy soils 0 11 and the lowest rmse for clayey soils 0 08 gambill also computed a nash sutcliffe efficiency score nash and sutcliffe 1970 for each primary soil texture class and found that sandy soils had the lowest goodness of fit and clayey soils had the best these findings did not include a comparison of the goodness of geowatch to that of lower resolution alternatives like the lis soil moisture model while the accuracy of geowatch output is important comparison to alternative methods for soil moisture estimation are lacking here we compare geowatch soil moisture estimation accuracy with its weather scale input lis soil moisture to determine whether the downscaling adds value compared to the raw input as a ground truth baseline we use in situ observations from the international soil moisture network ismn dorigo and coauthors 2013 2011 for the period between january 1 and december 31 2018 specifically we compare the geowatch and lis 0 10 cm layer volumetric soil moisture results to usda scan schaefer et al 2007 5 cm soil moisture observations pulled from the ismn soil moisture probe network archive we use the ismn database to access the scan data because the ismn archival data include computed data quality flags for each observation dorigo and coauthors 2013 this evaluation specifically measured performance of geowatch and lis against a collection of 28 individual scan sites within the ismn data for the evaluation period we selected these sites to represent the separate soil textural categories within the usda agricultural soil texture classes aiming for at least three sites per soil texture category we also selected for the completeness of scan records for the 2018 observation period based on the number of good observations marked by the ismn quality flags and limited number of missing measurements the final site selection criterion was availability of lis data as locations near coastlines water bodies or other terrain features are not predicted by the lis model along with the soil moisture observations the soil temperature observation date and time latitude and longitude of each site and data quality flags are included in the data record we developed a set of analysis scripts to retrieve and analyze scan data from the ismn archive and geowatch and lis soil moisture estimates the scan measurements are datetime matched rather than temporally averaged to the 3 hourly lis and geowatch data and comparisons are limited to datetimes when the scan and geowatch soil moisture are both available each observation location model grid pair were evaluated independently and grouped according to usda soil texture class for group comparisons for most of the 10 soil texture classes at least three sites were used in the analysis however there were a limited number of clay and clay loam sites that met the data completeness criteria for each site we compute the temporal root mean square error rmse bias computed as model minus observation linear correlation coefficient r2 and normalized nash sutcliffe efficiency goodness of fit value nash and sutcliffe 1970 this comparison of the geowatch and lis output against scan in situ measurements focuses on a subset of the more than 203 scan observation sites in the ismn archive a few sites were omitted simply because the soil moisture record in the observations were reported as exceedingly wet relatively speaking or dry in the absence or presence of precipitation in the observations as reported by the scan network though those observations were not flagged in the ismn dataset some of those observation values may be accurate and represent ponding excessive drought or other natural processes but those processes are not necessarily represented by the land surface models within lis and geowatch which are limited in the range of soil moisture observations by both the wilting point minimum soil moisture value and soil porosity maximum soil moisture value additionally we did not consider sites characterized by organic soils wetlands or urban environments for each of the sites evaluated the pedon reports for each location was compared to the soil texture and vegetation information in the geowatch layers and the latitude and longitude information from the pedon reports was also cross referenced with the ismn data files google earth imagery for the location and site images to make sure the site was properly co located with geowatch grid cells this resulted in a few latitude and longitude updates to the ismn data files specifically including both adams ranch nm and sandy ridge ms used in this evaluation where the values in the ismn data file did not match the soil pedon reports from the usda website and our analysis indicated the pedon reports more accurately matched the site images and google earth hosted satellite imagery than the ismn location information after evaluating geowatch and lis at the 28 sites within this study both models effectively represent the wetting and drying as captured by the in situ measurements a brief evaluation of the precipitation gauge information obtained from the usda scan web tool compared to the lis precipitation analysis indicates the model captures precipitation events at the appropriate times though this study did not produce any specific evaluation of the precipitation skill in lis generally large increases in soil moisture in the observations were captured by the models fig 7 is a soil moisture graph from walnut gulch arizona and illustrates the above point well as the observed soil moisture includes numerous spikes in the observed data coinciding with similar jumps in the soil moisture from both lis and geowatch table 3 contains the 28 sites used in this study along with the resulting statistics for both the geowatch and lis comparisons to the observations many of the sites are adjacent to farms or are in locations characterized as grasslands or shrubland with very little if any vegetation the table includes the geowatch composite land cover classification for each site along with the usgs nlcd canopy coverage value of the 28 sites six sites had a vegetated canopy coverage value of 1 or greater of those sites three locations kyle canyon sellers lake and goodwin creek timer had vegetation canopy at greater than 40 and were listed as being within a forest mixed forest or broadleaf forest the sites were grouped into the following soil categories based on their location to within the geowatch composite soil texture map sand loamy sand silty loam sandy loam silty clay loam loam and finally clay and clay loam soils overall the statistics for geowatch scan comparisons were either equivalent to or better than the statistical lis scan comparisons at 15 locations with an additional 7 sites reported mixed statistical results where one or more metrics were better for geowatch than lis with a simultaneous decrease in another statistical measure only four sites reported worse statistical results for geowatch scan compared to the lis scan results the computed bias was reduced by at least 0 01 m3 m3 in the geowatch statistics at 17 locations nnse increased by at least the same increment at 16 locations and both r2 and rmse improved at 15 of the locations of those 11 sites reported more significant statistical improvements either with a bias reduction of 0 05 nnse or r2 value increasing by 0 10 or an rmse reduction of 0 05 the sites with the most significant difference in geowatch skill versus lis were those estimating soil moisture in sandy locations the geowatch downscaling generally produced a reduced model bias at the four sites characterized as purely sand fig 8 with at least one case the sandy hollow location where the model bias was reduced significantly with respect to the lis scan model bias the bias for the sandy hollow site was 0 14 in the lis scan comparisons while the geowatch scan model bias is 0 00 m3 m3 however the r2 values did not significantly change at that location between the models there was some minor improvement in the nnse values but a large reduction in the rmse for the geowatch scan results for the remaining sandy observation sites evaluated the geowatch downscaling had a smaller model bias as compared to the lis scan comparison reducing the bias from 0 12 to 0 07 at the hals canyon observation site 0 11 to 0 05 for spooky and a slight increase in geowatch bias from 0 16 to 0 17 at sellers canyon but with no change in the correlation for these locations the sellers canyon site was the only one with a slope less than 1 the sandy hollow site was the steepest terrain at 18 grade the vegetation for the three most improved sites was listed as zero percent with sellers canyon being 85 covered in canopy there were four locations with a sandy loam soil texture fig 9 with land use classification of either grassland or shrubland in the geowatch the composite land use map evaluated each of the sites were on slopes ranging from less than 1 at the morris farms location slope is 0 07 to nearly 20 for the lovell site in most of the locations the geowatch soil moisture results were less biased than the lis results with the bias reduction amount depending on the location the model bias was lower in the geowatch scan results than the lis scan results by a range of 0 02 0 07 with some large increases in nnse and r2 and reductions in rmse the only site that didn t follow that trend was lovell summit which recorded an increase in bias large decrease in nnse and a slight decrease in r2 and 0 04 m3 m3 increase in rmse the lovell summit site is a classified as shrubland in the composite land use data with an nlcd vegetation canopy coverage of 39 and has the highest composite terrain slope at nearly 20 for this soil texture category in our evaluation of the five sites the lovell site was the only site with nlcd canopy coverage above 0 the sites with clay and clay loam fig 10 and silty loam fig 11 soils overall bias nnse and rmse results were slightly lower for geowatch scan than they were for lis scan comparisons four of the sites within the silty loam category recorded higher bias reduced nnse and lower rmse for the geowatch scan results with higher bias at the goodwin creek timer rock springs and mt vernon locations and lower nnse and higher rmse at those sites plus at the eros data center location for the clay and clay loam soil grouping two of the sites north piedmont and onward both had slightly higher bias in the geowatch scan comparison while the vernon site had better statistical skill with nnse scores higher by 0 26 in the geowatch scan results lower bias and lower rms the land use classification ranged from either farmland pastureland or grassland to forest with slopes ranging from 0 4 mt vernon to 15 north piedmont overall the statistics point to an improvement in the downscaled soil moisture compared with the weather scale input a majority of sites had either a statistically significant beneficial change in statistical results or at the worst had nearly no change in statistical skill with results that were very similar to the lis skill however in some locations geowatch downscaling increased the bias significantly the contributing factors for that need further study 4 2 validation using single catchment studies to further validate geowatch s downscaling algorithms its high resolution soil moisture predictions were compared to in situ soil moisture data from two published high resolution single catchment studies these published catchment studies were performed in tarrawarra australia and shale hills pa respectively 4 2 1 tarrawarra catchment comparisons the first catchment comparisons used the tarrawarra data set western and grayson 1998 western et al 2004 which focused on a single catchment in southeastern australia fig 12 the authors of the tarrawara study describe the site as 1 1 text condensed from http people eng unimelb edu au aww tarrawarra catchdesc html the tarrawarra catchment is located in southern victoria australia 37 39 south 145 26 east the climate is temperate with a mean annual rainfall of 820 mm and a potential evapotranspiration of 830 mm the local terrain is undulating and has an elevation of approximately 100 mahd australian height datum the catchment area is 10 8 ha and the land is used for dryland grazing of dairy cows we believe that the tarrawarra catchment is representative of landscapes with relatively shallow soils of low to moderately high lateral permeability with an impeding layer at depth for which topography plays a significant role in routing water through the landscape and for climates ranging from temperate to subhumid these conditions apply over large parts of south eastern australia and in many other parts of the world the tarrawarra data set fig 13 includes two different sets of in situ soil moisture measurements a 500 point grid of time domain reflectometry tdr values 10m 20m spacing and a 20 point collection of neutron moisture measurement nmm values the tdr measurements were taken on different 13 dates while the nmm measurements were taken on 59 different test dates in addition to the soil moisture data numerous other site specific quantities are provided including digital elevation model survey data 5 m spatial resolution and sieve based particle size distribution measurements from 11 locations within the site while the tarrawara catchment site does not show much variability in land cover vegetation or soil type it does present some modest topographic changes therefore it provided an opportunity to validate geowatch s model for downscaling soil moisture based on topographic effects for these purposes geowatch soil moisture downscaling algorithms were applied to the measurement locations and dates utilized in the tarrawarra study in addition available site specific geospatial data such as site average soil moisture 5 m dem and soil texture data provided by the study authors were also used as inputs to geowatch in lieu of its default global geospatial inputs example comparisons are shown in fig 14 for one date 27 sep 1995 on the left is the weather scale soil moisture input provided to geowatch which is the mean value of the tdr soil moisture measurements for this date in the middle is a map of the downscaled soil moisture estimated by geowatch on the right is a map of the in situ tdr measurements as can be seen geowatch successfully predicts the spatially distributed structure of the reported tdr soil moisture data fig 15 compares the geowatch predictions to the in situ tdr measurements for several other dates it is evident from the tdr measurements that topography has a prominent influence on the spatial distribution of soil moisture and that geowatch s downscaling model reproduces this spatial distribution moreover the only times that topography does not strongly influence the soil moisture pattern is when the site is extremely dry see 1996 02 23 top left or extremely wet see 1996 09 02 bottom right presumably the reduced topographic influence under these conditions is due to inadequate water to redistribute dry dates or overall saturation with water wet dates and geowatch likewise predicts a dampened topographic effect under these conditions to further assess the performance of geowatch s downscaling algorithms using the tarrawarra dataset the rms error between geowatch s output downscaled soil moisture and the reported tdr values were computed for each of the 13 dates where tdr values are available we calculate an rms error between the tdr and geowatch downscaled soil moisture values these values are plotted on the vertical axis of fig 16 we compare against the difference between geowatch s downscaled soil moisture and the site wide mean soil moisture from the tdr measurements these values are plotted on the horizontal axis of fig 16 values below the dashed 1 1 line in fig 16 indicate reduced error in geowatch estimates vs the mean as can be seen in fig 16 geowatch provides more accurate soil moisture maps for 9 of the 13 dates 69 compared with simply using the site wide mean tdr value for that date looking across all dates the site mean performs well the rms error is only 0 0352 m3 m3 absolute volumetric soil moisture geowatch slightly outperformed this metric by reducing the rms error to 0 0321 m3 m3 in addition to the tdr values the tarrawarra data set also includes soil moisture measurements performed by nmm the nmm data has only 20 locations per date versus 508 locations per date for tdr so the nmm data is not good at conveying spatial structure however it is available for 59 dates versus 13 for tdr so we use it for quantifying error over a denser temporal range using the same methodology as presented above for tdr we computed the geowatch error relative to the mean nmm measurements fig 17 shows that geowatch downscaled soil moisture provides an improvement over the soil moisture measurements for 56 of the 59 dates 95 the overall error across all dates is reduced from 0 040 m3 m3 using the site mean value to 0 030 m3 m3 using the geowatch values 4 2 2 shale hills catchment comparisons shale hills is another detailed catchment site with publicly available data naithani and baldwin 2015 enabling detailed comparison geowatch s downscaled soil moisture predictions the shale hills watershed is a small scale 500m 300m forested and v shaped first order watershed with an area of 0 08 km2 8 ha in central pennsylvania usa fig 18 this watershed is part of the susquehanna shale hills critical zone observatory sshczo the mean annual temperature is 10 c and the mean annual precipitation is 107 cm shi et al 2015 the topography spans about 70m fig 19 the publicly available data for this site includes dem data at 0 5m 0 5m resolution fig 20 there is a weather station and flux tower as well as an outlet gauge for the stream that drains the catchment in situ soil moisture measurements are available from time domain reflectometry tdr for up to 106 locations see yellow and green markers on fig 20 though typically data is only available for approximately 61 of the sites the data set includes soil moisture measurements for 76 dates that span 2007 2010 most of the tdr measurements were taken manually over the course of a day though 16 of the locations were measured using automated equipment as with the tarrawarra analysis we used the site specific dem data and the site wide mean soil moisture for each day as inputs to geowatch after using geowatch to estimate the local soil moisture at each of the tdr sensor locations for each of the 74 available dates we compared performance of geowatch soil moisture with the spatial mean of tdr measurements for each date which represents a larger scale measurement of soil moisture in the area though higher resolution than weather scale as can be seen in fig 21 geowatch provides more accurate soil moisture maps for 55 of the 74 dates 74 compared with the site wide mean tdr value for that date looking across all dates the site mean tdr measurement has an rms error of 0 060 m3 m3 while geowatch reduces that error to 0 054 m3 m3 5 summary and conclusions the geowatch system was developed to bridge the gap between coarse resolution gridded weather products and fine scale needs of military and other communities the system ingests gridded lis and galwem weather products from the usaf combined with a number of high resolution terrain characterization layers terrain elevation vegetation and soils masks to compute higher resolution maps of soil moisture products based on the resolution of the terrain data the geowatch system also includes an interface that enables users to interact with the data generate maps for any region of the globe evaluate time series of products and interrogate the data additionally the geowatch software has been linked successfully to army soil strength and the army standard mobility application interface tool to generate maps of vehicle cross country vehicle speed estimates which provides a unique machine to machine capability for delivering weather informed impacts to downstream users the geowatch system has been evaluated against in situ observations from the usda scan network via the ismn and determined to provide results that are statistically similar or better than the input weather model results from the usaf in most cases some of the statistical evaluations point to significant improvements while there are also some areas of further study and improvement needed these scan comparisons offer a picture of the temporal predictive value of geowatch downscaled soil moisture and its lis inputs but give little insight into the spatial structure offered by the downscaling technique further evaluation of the geowatch products against higher resolution soil moisture networks to assess the downscaling accuracy are needed as well as a global set of soil moisture observations to assess how variations in terrain vegetation slope and soils affect geowatch downscaled soil moisture and to understand areas for improvement we compared geowatch s downscaled soil moisture to data from the published tarrawarra and shale hills high resolution catchment studies at tarrawarra we found that the geowatch models accurately captured the topographically influence spatial distribution of soil moisture quantitatively using tarrawarra tdr data geowatch downscaling resulted in lower rms error for 9 of 13 dates 69 using tarrawarra nmm data geowatch resulted in lower rms error for 56 of 59 dates 95 repeating the analysis for shale hills tdr data geowatch resulted in lower rms error for 55 of 74 dates 74 these small catchment studies offer a better view of the value of downscaled soil moisture but are still limited to small areas smaller than a lis grid tile and thus do not offer evaluation of the effects of soil type vegetation slope and terrain variations a larger study area and more diverse study areas around the globe are needed to understand the weaknesses and strengths of geowatch the lis system was designed to provide added functionality and utility to the global weather products generated by complex land data assimilation systems and coupled land atmosphere weather prediction systems the value of the geowatch system is very much coupled to the accuracy of those complex global analyses and forecasts any improvements made to the lis including the use of remotely sensed soil moisture observations and or galwem systems to provide more accurate and relevant analyses will result in better inputs to geowatch leading to more accurate results software and data availability the geowatch application includes an open access website that allows public viewing of downscaled soil moisture results at https mobility crearecomputing com further an example python based script is available that enables users to reproduce the downscaling algorithm is available through github at https github com creare com podpac examples blob main notebooks 5 datalib smap smap downscaling example application ipynb access to the army standard mobility application and results are restricted declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the development of the geowatch application by creare llc was supported by a us army corp of engineers small business innovation research award numbers w913e5 13 c 0005 and w913e5 14 c 0001 usace monitoring evaluation and technical transition support was provided by multiple us army supported applied research and development programs including the science and technical objective research program army terrestrial environmental modeling and intelligence system artemis the simulation tools for combat vehicle robotics geospatial tools for mobility work unit and intelligent environmental battlespace awareness integration task finally the authors would like to thank the four anonymous reviewers for their careful and constructive review comments and suggestions that helped to strengthen this manuscript 
25469,rising global temperatures and the urban heat island effect can amplify heat related health risks to urban residents cities are considering various heat adaptation actions to improve public health enhance social equity and cope with future conditions beyond past experience we present the city heat equity adaptation tool city heat which suggests optimal investments for mitigating urban heat and reducing health impacts through modifications of built cool roofs pavements and natural urban afforestation environments and reductions of people s heat exposure cooling centers the optimization considers multiple public health and social objectives under a wide range of future scenarios an application to baltimore md usa demonstrates how city heat can generate pareto efficient multi year heat adaptation plans we quantify effectiveness efficiency equity tradeoffs among alternative plans and show the advantages of flexible decision making city heat can be adapted to the natural built and social environments of other cities to support their urban heat adaptation planning recognizing local objectives and uncertainty keywords heat waves urban heat island urban heat adaptation deep uncertainty equity multi objective robust decision making data availability i have shared my code data in the software availability section 1 introduction 1 1 health impacts of heat waves a heat wave or an extreme heat event is often defined as a consecutive period of hot days with temperatures above a threshold mazdiyasni et al 2017 although there is no universal agreement on the threshold values of duration and intensity chen et al 2015 faye et al 2021 it is widely recognized that heat waves are one of the deadliest climate hazards around the world mazdiyasni et al 2017 for instance the 1995 chicago heat wave caused over 700 excess deaths in one week whitman et al 1997 the 2003 heat wave in europe was responsible for over 70 000 excess deaths robine et al 2008 and in the past three decades the number of deaths caused by heat waves has quadrupled in china cai et al 2021 rising global temperatures are likely to further increase the intensity frequency and duration of heat waves around the world ipcc et al 2021 even with strict and effective greenhouse gas ghgs mitigation policies the increasing trend in global temperatures is likely to continue for at least several decades peters et al 2013 without precautionary and well designed adaptation plans humans will suffer more from heat waves in the future 1 2 urban heat island effects amplify health impacts the urban heat island uhi effect makes cities hotter than their rural surroundings oke 1982 li et al 2019 the synergy between heat waves and the uhi can further amplify heat health risks to city residents cao et al 2021 he et al 2021 recent studies in measuring intra urban temperatures have shown that the uhi of a city however is neither uniform nor constant scott et al 2016 shandas et al 2019 it is a time varying urban heat archipelago uha ziter et al 2019 in which thermal contrasts vary with neighborhoods and weather types scott et al 2016 and the highest heat conditions are often found in poor minority neighborhoods with histories of discriminatory housing policies popovich and flavelle 2019 hoffman et al 2020 wilson 2020 as a result heat adaptation in cities should aim not just to mitigate climate risks and lessen health impacts but also to restore social equity which historically has been undermined fortunately several u s cities emphasize the importance of heat adaptation in their hazard mitigation plans or other official documents ccap 2008 los angeles 2018 los angeles and s green 2019 baltimore 2019 philadelphia 2015 philadelphia 2019 phoenix 2019 nycem 2019 an important feature of these plans is their emphasis on multiple policy objectives including protecting human life and infrastructure enhancing social equity promoting the local economy maximizing cost effectiveness increasing adaptive capacity to future conditions strengthening community resilience and valuing other co benefits however systematic approaches to designing and evaluating the effectiveness equity and other social impacts of alternative heat adaptation plans are lacking keith et al 2019 1 3 actions for mitigating heat wave impacts this paper addresses the optimal selection of actions that cities can take to lessen the temperature and human impacts of heat waves there are two main categories of actions for mitigating heat wave impacts that are recommended by the usepa n d studied in the literature and considered by cities 1 temperature reduction and 2 exposure reduction cities must choose a mix of such actions accounting for many objectives kasprzyk et al 2013 as well as deep uncertainties marchau et al 2019 about future climate and the effectiveness of possible actions temperature reduction can be achieved by afforestation increasing vegetated areas and covering impervious surfaces such as roofs and roads with light colored materials loughner et al 2012 li et al 2014 santamouris et al 2017 ziter et al 2019 actions using trees or vegetation hereafter referred to as green actions reduce temperature through evapotranspiration higher albedo and shade that reduce direct sunlight and incoming heat li et al 2019 light colored surfaces have higher albedo and store less heat than dark surfaces li et al 2014 santamouris et al 2017 which can reduce the uhi and uha effects green actions are generally more effective in cooling cities than light colored surfaces and can yield co benefits that are valuable for cities such as air purification stormwater runoff control and amenity values santamouris et al 2017 however the costs of green actions are often considerably higher which may be a financial burden for cities it also takes time for some green actions to achieve their maximum benefits e g trees need time to grow to full canopy the costs and benefits of investing in green actions thus need to be carefully weighed investing in temperature reduction actions is important for promoting environmental equity for low income neighborhoods where temperatures are often higher than in other neighborhoods due to discriminatory housing policies huang et al 2011 hoffman et al 2020 wilson 2020 the second category is exposure reduction which reduces or eliminates people s exposure to heat waves common such actions are cooling centers and financial support for low income populations to buy air conditioners acs for example new york city opened over 500 cooling centers in 2018 and offered assistance for purchasing acs through its home energy assistance program nycem 2019 operating cooling centers requires less investment than temperature reduction actions especially when cities can use public facilities e g public libraries and recreation centers as cooling shelters however it is difficult for some vulnerable people e g the elderly and people with disabilities to access cooling centers and others may be reluctant to classify themselves as vulnerable and go to such shelters therefore utilization rate of cooling centers can be low despite the need for them 1 4 previous studies on urban heat adaptation and remaining gaps while numerous studies have analyzed alternative urban heat adaptation strategies keith et al 2019 recently concluded that few studies delve into extreme heat planning and governance processes they call for the creation of integrated strategies that combine extreme heat risk management e g cooling centers and design of the built environment e g afforestation and also point out the need for new multidisciplinary tools and expanded databases to support urban heat adaptation furthermore even fewer studies have used formal numerical optimization to choose from alternative strategies while explicitly considering public health and other human impact indicators one strand of papers uses numerical methods to identify optimal urban heat adaptation strategies but those studies do not explicitly include social impact indicators such as public health and social equity in their objectives for example optimization has been used to inform green space or land use planning with the objective of reducing urban rural temperature differences zhang et al 2017 other studies address optimization of siting of cooling centers for instance bradford et al 2015 solve for optimal center locations in pittsburgh pa however rather than minimizing direct health impacts of heat their objective is to site cooling centers to maximize heat risk population within a specified distance of the centers where individuals are weighted by a heat vulnerability index another group of papers simulates how alternative heat mitigation strategies reduce heat health impacts however those studies do not use numerical optimization to search for the best adaptation strategies from a wide range of alternatives instead they pre define a small set of strategies and then evaluate those strategies using urban scale meteorological models and health response models for example stone et al 2014 use the weather research forecasting wrf and benmap models to evaluate seven heat reduction strategies with different combinations of increased tree and vegetation covers green and cool roofs and cool pavements they find that 40 99 of projected 2050 heat related mortality could be avoided in three u s metropolitan areas vargo et al 2016 study the same three areas but they further investigate the health benefits for subpopulations of different ages races and income levels they conclude that vulnerable populations benefit most from heat adaptation so plans should prioritize actions in regions where those populations live as a final example boumans et al 2014 develop a model to assess the effect of various levels of urban vegetation on reducing heat related mortality in austin tx but they only find a modest reduction in heat related deaths these studies have greatly improved our understanding of the possible costs and benefits of efforts to mitigate heat impacts however capitalizing on these findings to develop a useful and insightful decision support tool for urban heat adaptation requires several additional advancements these include learning how to 1 fully address multiple and often conflicting adaptation objectives such as health cost and especially inequities in heat health impacts hoffman et al 2020 wilson 2020 2 quantify and account for long term costs and benefits of heat adaptation actions over a time horizon rather than in a single future year 3 take advantage of synergies in temperature reduction and exposure reduction actions considering the full range of possible measures and 4 plan considering uncertainties about the rate and nature of climate change changes of social factors and the cost and performance of possible actions recognizing that plans can be adapted in response to observations to ensure the desired goals can be met 1 5 city heat an innovative decision support tool for urban heat adaptation in this paper we propose the city heat equity adaptation tool city heat to support urban heat adaptation planning unlike other planning tools in the literature city heat can optimize multiple adaptation objectives consider numerous future scenarios and adaptation actions and generate adaptive plans at fine temporal annual and spatial sub city resolutions in addition to the mathematical model described in this paper we also provide open source code https github com srayum city heat that other researchers or city planners can use to apply city heat to urban heat adaptation problems elsewhere we now describe the contribution of this tool and its application to the literature by describing the following five general characteristics which address the five issues that are presently unresolved in the literature section 1 4 respectively 1 targeting of multiple planning objectives because urban heat adaptation is inherently a multi objective planning problem plans that only focus on one goal may overlook inexpensive plan modifications with large benefits for other objectives thereby leading to inefficient and ineffective decisions previous studies have not used multiple objectives measuring a range of social impacts to optimize heat adaptation notably inequity exacerbated by heat is a great concern of cities and consequently an equity indicator should be included as an objective fortunately in recent years increased computing power has allowed researchers to study and solve complex multi objective problems with many objectives hadka and reed 2013 kasprzyk et al 2013 trindade et al 2017 including so called many objective problems which consider more than four objectives many objective optimization yields a set of approximately pareto optimal or non dominated solutions coello et al 2007 that stakeholders can choose from in a set of such solutions no solution equals or outperforms any other solution on all objectives but every solution is strictly better in at least one objective when compared pairwise with every other solution in the set the term pareto approximate is used to refer to the best known set of non dominated solutions acknowledging the true pareto optimal set may have solutions that pareto dominate these but they have not been discovered this paper s version of city heat includes five objectives but users can modify or add new objectives based on their preferences and city heat will generate a set of pareto approximate urban heat adaptation plans according to those objectives 2 evaluate costs and benefits over long time horizons and by city subareas the costs and benefits of urban heat adaptations are realized over the long term climate change and its variability mean the costs and benefits of heat adaptation can vary significantly over time for example the benefits of heat adaptation in reducing heat health impacts can be much larger in a hot year compared to a year with less stressful conditions in addition some actions e g urban afforestation not only have long life times but also have long lead times this delay in realizing the benefits means that investments have to be made well in advance of the need and so may be politically more difficult to implement and benefits may also be more uncertain than for projects with short lead times choosing a single time point e g a year for analysis lacks a comprehensive perspective on long term costs benefits and uncertainties the result may be pareto inefficient planning decisions and a failure to appreciate how investments and benefits will vary over time furthermore environmental conditions and vulnerable populations vary over space shandas et al 2019 wilson 2020 so that costs and benefits of heat adaptation are likely to be unevenly distributed across a city s subareas by distinguishing vulnerabilities and opportunities among subareas city heat allows adaptation actions to be tailored to each subarea with the resulting impacts aggregated to the city level to compute the plan s overall performance on objectives by considering costs and benefits over time and space city heat provides a more comprehensive comparison of alternative adaptation plans 3 optimize adaptation actions over type location time and magnitude in order to identify the optimal heat adaptation plans the type magnitude location and timing of actions need to be decided rendering this decision problem high dimensional and complex previous studies evaluated the effects of a few pre defined adaptation plans at a given time point likely overlooking superior combinations of actions over longer time horizons by contrast city heat can identify and recommend heat adaptation plans specifying what actions to take at finer temporal annual and spatial subarea resolutions according to the objectives defined and desired by city planners 4 address multiple types of uncertainties and adapt plans in response to information uncertainties especially deep uncertainties are a major challenge to making long term decisions lempert et al 2003 marchau et al 2019 the performance of alternative plans depends on how the future unfolds which cannot be perfectly predicted when decisions are made lempert et al 2003 climate change political and economic developments and population growth are typical examples of deep uncertainties which are difficult to characterize by probability distributions milly et al 2008 trindade et al 2017 these uncertainties are important to assessing urban heat adaptation plans peng et al 2010 cao et al 2021 in a scenario with fast global warming for instance a plan that implements massive urban afforestation might be preferred because it works to avoid many possible heat related deaths however in a scenario with slow global warming the same plan might not be attractive as it requires a significant amount of investment while perhaps not saving many lives in addition previous studies defined heat wave in different ways in part because of uncertainty about the role of peak temperatures night time temperatures and heat wave durations on health effects chen et al 2015 faye et al 2021 therefore it is desirable to develop a tool that can evaluate alternative plans under uncertainty by considering a large ensemble of possible future scenarios city heat can identify pareto optimal adaptation plans and evaluate plan robustness across a wide range of future scenarios in particular city heat enhances the robustness of plans by explicitly considering the flexibility of plans to adapt to changing conditions surprises and learning lempert et al 2003 hung and hobbs 2018 marchau et al 2019 the ability to track system changes and flexibly adjust decisions can reduce the risks of insufficiency and redundancy in decisions holling 1978 de neufville and scholtes 2011 to enable adaptation city heat employs the direct policy search dps method giuliani et al 2016 quinn et al 2017a which uses key system information that has been observed prior to each decision stage e g year to guide that stage s decisions this makes decisions at each stage contingent on information obtained thus far and decisions made at previous time steps dps functions have been employed to model adaptive decisions for many complex dynamic environmental management problems under uncertainty including multi purpose reservoir management giuliani et al 2016 quinn et al 2017b lake pollution control quinn et al 2017a coastal dike heightening garner and keller 2018 fishery harvesting hadjimichael et al 2020 microgrid management gupta et al 2020 and carbon mitigation problems marangoni et al 2021 the above features of city heat address challenges of urban heat adaptation problems that are not fully studied or combined in the current literature city heat is a generalizable tool that can support urban heat adaptation planning through optimizing decisions based on multiple planning objectives considering various future scenarios and generating candidate adaptation plans at fine temporal annual and spatial sub city resolutions to illustrate how city heat works we apply this tool to a hypothetical case study for baltimore md where heat risks to health are a major concern anderson and bell 2010 and where historical redlining has contributed to great inequity in heat health risks plumer et al 2020 wilson 2020 the rest of this paper is structured as follows section 2 introduces the mathematical formulation of city heat the algorithm used to solve the resulting optimization problem and the metrics that characterize the robustness of generated plans then section 3 describes the setup and data sources of the baltimore case study section 4 follows with results of the case study along with discussions of those results section 5 concludes the paper by summarizing the contributions of city heat and potential enhancements of the method 2 method city heat consists of two parts 1 a simulation model that can evaluate the effects and costs of alternative heat adaptation plans and 2 an optimization model that can search for pareto optimal plans based on multiple objectives in this section we first describe the problem setting in city heat section 2 1 then we present equations used in the heat impact and cost simulations section 2 1 1 and optimization section 2 1 2 parts of city heat these are presented as general statements and the particular implementation for the baltimore case study is discussed in section 3 next we introduce the algorithm borg hadka and reed 2013 that we adopt to solve the many objective optimization problem section 2 2 in city heat finally we define and present the metrics to evaluate the robustness of the generated plans section 2 3 a schematic diagram that includes key components of city heat is shown in fig 1 2 1 problem setting in city heat city heat is a simulation based optimization model that solves for non dominated or pareto approximate hadka and reed 2013 urban heat adaptation plans according to multiple objectives under uncertainty in city heat temperature reduction i 1 and exposure reduction i 2 actions are both considered typical temperature reduction actions i 1 include urban afforestation increased vegetation covers green cool roofs and cool pavement those actions are further divided into two subcategories actions in the first set take time to achieve their maximum effect i 1 1 e g urban afforestation while actions in the second set yield immediate benefits i 1 2 e g cool roofs pavements distinguishing actions in i 1 1 and i 1 2 is important because this reflects a tradeoff in choosing between relatively inexpensive actions i 1 2 with lower temperature reduction potential but immediate benefits as opposed to actions i 1 1 that might have higher temperature reduction potential in the long run but at the expense of greater upfront investment and delays in realized benefits on the other hand common exposure reduction actions i 2 include cooling centers and free ac programs which provide cooling to people in need by avoiding exposure to extreme heat those actions are expected to reduce mortality during heat wave events but will not affect the frequency and intensity of heat waves in general city heat allows the user to specify ahead of time which of the considered actions are allowed to be taken in each of r sub city regions and in which of the t years for instance in the application in section 3 we allow actions in i 1 to be implemented anywhere at any time but restrict actions in i 2 to the first period this setting is also to demonstrate that city heat can model different types of decisions in practice how a city is divided into sub city regions depends on the purpose of an application as well as on available data and computational resources for example sub city regions could be aligned with jurisdiction classifications such as planning districts or census tracts or they could instead be based on physical and demographic characteristics e g temperature or population vulnerability city heat addresses uncertainties by considering various plausible future scenarios here the scenarios are combinations of different realizations of uncertain parameters that describe future climate demographics and action effectiveness realizations are drawn using latin hypercubic sampling lhs iman et al 1980 although other scenario definition methods could be applied the case study in section 3 describes the scenario definition process in more detail including which uncertain variables are considered in the model presented below the parameters with a subscript s are uncertain and scenario dependent 2 1 1 city heat simulation model four modules make up the simulation part of city heat 1 temperature modification 2 demographic projections 3 mortality estimation and 4 cost calculation 1 temperature modification the temperature modification module calculates the temperature reductions and resulting decrease in heat wave days caused by various adaptation actions in each sub city region and year this is done in three steps first the initial step calculates for a given year the incremental investment in each temperature reduction action based on the dps functions system state variables and a set of dps parameters that are to be optimized eq 1 the effective cumulative changes of those actions are then calculated based on the annual changes eq 2 the second step estimates the resulting temperature reduction in each year and sub city region using the cumulative changes of actions and their temperature reduction functions eq 3 in the third and final step the number of heat wave days with temperature reduction actions eq 4 and the number of baseline heat wave days assuming no action is taken eq 5 are calculated based on a heat wave count function for each case the details of these steps are described below eq 1 shows the dps function in the first step which depicts the fractional increase of temperature reduction action i in sub city region r and year t under scenario s this function considers the system state variables θ r t 1 s through year t 1 and a set of dps policy parameters μ i r to be optimized details in section 2 1 2 below eq 1 x i r t s d p s θ r t 1 s μ i r i i 1 x i r t s is the output of a dps function and represents the fractional increase of temperature reduction action i in sub city region r and year t under scenario s the dps function uses a vector it can also be a scalar of system state variables θ r t 1 s as inputs making the decisions in year t dependent on the key system state variables and their values observed until year t 1 which is stochastic and dynamic because it considers multiple possibilities scenarios and years users can select which state variables they would like to have considered when deciding on investments for instance the average annual heat wave days of the previous five years or the observed temperature effects of actions can both be used as system state variables to inform decisions these state variables are scenario dependent and change with time the actions taken in previous time steps can also affect the system states later for example if massive urban afforestation is implemented in year 1 heat wave days an example of θ r t s are likely to be reduced afterward compared to no action being taken the set of policy parameters μ i r is customized for each action i and sub city region r but is independent of scenarios and years this means that the policy function controlled by μ i r stays the same under different scenarios and over time but the resulting decisions x i r t s will be scenario and time dependent inasmuch as the inputs θ r t 1 s change with scenarios and time under this design the policy functions could lead to adaptive strategies that unfold over time differently in different scenarios using such policy functions can make decisions flexible adjusting them dynamically for each scenario for example an ideal policy function can lead to aggressive urban afforestation or implementation of light colored materials to reduce health impacts under a scenario with many heat wave days but the same policy function might result in slight or no changes under a scenario with limited heat waves to avoid a waste of resources by optimizing the policy parameters μ i r city heat can find the pareto optimal policy functions across a wide range of scenarios eq 2 calculates the effective cumulative changes e g amount of tree canopy or cool pavements roofs from temperature reduction actions taken in previous years eq 2 x i r t s g x i r 1 s x i r 2 s x i r t s i i 1 the effective cumulative change of a temperature reduction action i 1 represents the changes that are in effect to reduce the temperature for the actions yielding immediate benefits i 1 2 the effective cumulative changes are the sum of the changes taken in previous years such that g x i r 1 s x i r 2 s x i r t s t 1 t x r i t s i i 1 2 while for actions that take time to achieve their maximum effects e g trees need a predetermined number of years t g r o w to mature the effective cumulative changes will be less than the sum of actions in previous years until t g r o w is reached if we assume a linear increase of x and t g r o w years for x to reach its maximum effect then g x i r 1 s x i r 2 s x i r t s t 0 t min t t t g r o w 1 x r i t s i i 1 1 to illustrate if it takes 20 years for a tree to reach its full canopy and just 10 of the potential tree capacity is planted in year 1 then the planted trees will achieve a total canopy coverage of 5 of the potential coverage by year 10 min 10 0 20 1 10 5 and reach 10 by year 20 min 20 0 20 1 10 10 different growth functions could be used for different actions but in the case study we use the linear function for its simplicity eq 3 shows how temperature reduction in sub city region r and year t is calculated the temperature reduction is determined by the effective cumulative changes of each action and their temperature reduction effect the integrated effect of all actions is assumed to be a linear summation of individual actions in addition to considering actions taken in one sub city region to calculate its temperature reduction we also account for the potential spillover effect of actions taken in adjacent sub city regions eq 3 t r t s m i t i i f i x i r t s r a r i β i r r s f i x i r t s a r is the set that contains all sub city regions that are geographically adjacent to the sub city region r the spillover effect is discounted by a factor β i r r s 1 which is scenario dependent representing the uncertainty in the geographical range over which the temperature reduction actions are effective β i r r s is a function of regions r and r and can depend on several factors such as two regions areas and distances apart also β i r r s can vary with action i here for simplicity we assume β i r r s is the same for all actions and is adjusted by the ratio of areas of two adjacent sub city regions r and r with no influence of their distance apart yielding β i r r s β s a r e a r a r e a r i the temperature reduction function f i can be obtained from existing observations or simulation studies loughner et al 2012 li et al 2014 santamouris et al 2017 ziter et al 2019 eqs 4 and 5 calculate the number of heat wave days hwds in sub city region r and year t with adaptation eq 4 and without adaptation eq 5 eq 4 h w d r t s h s t 0 t s δ t r t s t r t s m i t i eq 5 h w d r t s 0 h s t 0 t s δ t r t s h s is a heat wave count function that calculates the number of heat wave days from a daily temperature series e g a 153 day daily temperature series from may 1 sept 30 for each sub city region r and future year t under scenario s the future daily temperature series with temperature reduction actions is assumed to be a linear combination of three parts first t 0 t s is a daily temperature projection for the whole city that is retrieved from climate models in a given year t t 0 t s is scenario dependent because different climate models can be used second δ t r t s is the temperature deviation of sub city region r compared to the city wide mean temperature third and finally t r t s m i t i is the temperature reduction because of adaptation this item is only used in eq 4 while eq 5 serves as a baseline without considering temperature reduction from adaptation actions here t r t s m i t i is determined by decisions eq 3 while t 0 t s and δ t r t s are input parameters since heat wave definitions play a fundamental role in the calculation of heat wave days and their estimated health impacts chen et al 2015 guo et al 2017 faye et al 2021 we adopt multiple definitions that have been widely used in the literature to calculate heat wave days this represents the scientific uncertainty in defining heat waves making h s scenario dependent in each scenario the same h s is applied to all sub city regions but different numbers of heat wave days might be obtained for each sub city region because of the differences in δ t r t s and t r t s m i t i when considering temperature reduction actions h s counts the number of heat waves from t 0 t s δ t r t s and t r t s m i t i based on predefined criteria that can be a highly complex function of our decision variables see examples in section 3 t 0 t s represents an average city wide daily temperature projection in year t and scenario s under a base case set of assumptions concerning land use and cover it can be obtained from climate models such as downscaled general circulation models at the grid cell containing the city of interest peng et al 2010 t 0 t s is scenario dependent because various climate models provide different temperature projections for future periods these models represent the current understanding of climate change and the use of multiple models captures uncertainty about future climate conditions incorporating information from multiple climate models in the urban heat adaptation problem allows us to identify pareto optimal adaptation plans under future climate conditions and to address climate change uncertainty it is also useful to hold out some climate models and scenarios from the optimization to evaluate the robustness of the optimized heat adaptation plans to possible futures on which they were not trained this out of sample evaluation is useful because in practice the future will not look exactly like any of the projections so it is important to find plans that perform robustly under unmodeled but possible future conditions because most climate models do not have a spatial resolution equivalent to the sub city level we treat t 0 t s as a city wide mean temperature projection under base conditions sub city level temperature projections are developed by adjusting t 0 t s by δ t r t s the estimated deviation of sub city region r s temperature from the city wide average a similar method to obtain sub city level temperature projections is adopted by boumans et al 2014 however δ t r t s can vary with several factors such as weather types scott et al 2016 and thus it is difficult to characterize δ t r t s under different background temperature conditions here we simplify δ t r t s to δ t r s t which is the temperature deviation across sub city regions under the high temperature condition of interest the sub city temperature deviations can be estimated from monitoring data for intra urban temperatures scott et al 2016 shandas et al 2019 shi et al 2021 therefore we use t 0 t s δ t r as an approximate daily temperature projection for sub city region r in year t t r t s m i t i is the temperature reduction that results from a chosen set of investments in sub city region r and year t under scenario s relative to a baseline in which no action is taken i e t r t s m i t i is used in eq 4 to calculate h w d r t s but not in eq 5 to calculate h w d r t s 0 because of the temperature reduction effect h w d r t s is no greater than the baseline h w d r t s 0 and the difference between h w d r t s 0 and h w d r t s is the decrease in the number of heat wave days attributed to t r t s m i t i the temperature reduction relative to the base case t r t s m i t i can be affected by many factors such as the surrounding environment temperature and the extent of adaptation in the case study of this paper we assume for simplicity a linear or quadratic function for temperature reductions based on the extent of adaptation efforts as have been observed in the literature li et al 2014 ziter et al 2019 however users can use more elaborate temperature reduction functions if they wish although we assume the modified temperature projection is a linear function of t r t s m i t i eq 4 the number of heat wave days may be non linear with t r t s m i t i given the possible non linearity of h s 2 demographic projections the demographic module projects the demographic structure for different population groups in future years here we classify the population by two factors that are widely recognized to have significant impacts on heat health risks age below or above 65 years old and income below or above the poverty line basu and samet 2002 madrigano et al 2015 tong et al 2021 the two classes for each of these two factors result in four population groups with different relative risks during heat waves other factors could be included to further stratify the population but a linear increase in the number of classifying factors and their classes will exponentially increase the number of population groups making the calculation more complicated here we only include the two most important factors for simplicity and demonstration users can include additional factors if desired the projection of each population group is done through three steps first the population size in sub city region r and year t under scenario s is estimated using a projection function p r o j p o p eq 6 for example for a simple exponential growth model and an average annual population growth rate a p g r s the population size in sub city region r and year t p o p r t s is p o p r t 0 1 a p g r s t t 0 a p g r s is scenario dependent and represents alternative population change scenarios where positive values of a p g r s represent an expanding population while negative values represent a shrinking population more sophisticated population growth assumptions and models kii 2021 can be adopted by the users if desired eq 6 p o p r t s p r o j p o p p o p r t 0 a p g r t s t second eqs 7 10 project the relative size of each of the four age income population groups the fractions of the population above 65 years of age or below the poverty line are projected using projection functions p r o j a g e in eq 7 and p r o j i n c o m e in eq 9 respectively if a linear change function and an average annual rate of aging a a r r s are assumed then eq 7 will be simplified as p a g e 65 r t s min 1 p a g e 65 r t 0 a a r r s t t 0 bounded by 1 because it is a fraction similarly eq 9 will be simplified as p i n c o m e b e l o w p o v e r t y l i n e r t s min 1 p i n c o m e b e l o w p o v e r t y l i n e r t 0 a p r r s t t 0 with the assumed annual rate of poverty a p r r s then the fractions of the population younger than 65 years of age or with an income above the poverty line can be calculated as 1 p a g e 65 r t s and 1 p i n c o m e b e l o w p o v e r t y l i n e r t s as shown in eqs 8 and 10 respectively in the case study later we will use the linear functions for simplicity and demonstration eq 7 p a g e 65 r t s p r o j a g e p a g e 65 r t 0 s a a r r s t eq 8 p a g e 65 r t s 1 p a g e 65 r t s eq 9 p i n c o m e b e l o w p o v e r t y l i n e r t s p r o j i n c o m e p i n c o m e b e l o w p o v e r t y l i n e r t 0 s a p r r s t eq 10 p i n c o m e a b o v e p o v e r t y l i n e r t s 1 p i n c o m e b e l o w p o v e r t y l i n e r t s in the final steps the population size of each of the four age income groups is calculated by using the overall population size multiplied by the proportion of the population in each age and income group assuming here for simplicity that the two factors of age and income are independent that is we assume p a g e i n c o m e r t s p a g e r t s p i n c o m e r t s eq 11 where better data is available alternative assumptions can be made this approximation is also adopted by bradford et al 2015 eq 11 p o p i n c o m e a g e r t s p o p r t s p a g e i n c o m e r t s a g e a b o v e o r b e l o w 65 i n c o m e b e l o w o r a b o v e p o v e r t y l i n e 3 mortality estimation the mortality estimation module estimates the expected mortality on a single heat wave day and the resulting annual heat mortality in each sub city region and year under a given scenario for each population group the population exposed to heat waves and their group specific heat wave mortality risks are used to calculate the expected heat mortality the exposure reduction actions i 2 can protect some populations from heat waves and in this study we only consider a single such action the provision of cooling centers users of cooling centers are assumed to have no heat wave mortality risk berisha et al 2017 and the number of users will be subtracted from each population group exposed to heat wave mortality risks this estimation process is conducted in two steps initially we estimate the number of users of cooling centers in each sub city region r and year t eqs 12 and 13 the number of users of the subpopulations u s e r a g e i n c o m e r t s is calculated based on the number of available cooling centers c c r t the capacity of each center c c c a p r t s the utilization rate c c u s e r t s and the percentage of users of each age and income group p u s e r a g e i n c o m e r t s c c r t is the decision variable representing the number of cooling centers to open in sub city region r and year t the number and locations of cooling centers can stay the same for years e g baltimore code red so we assume c c r t is decided at the beginning of the study horizon and remains the same over time so c c r t c c r t and also the capacity c c c a p r t s is fixed and same so c c c a p r t s c c c a p r t s these assumptions illustrate how city heat can handle different types of decisions differently c c r t in just the first period versus x i r t s in every period but future work could explore the possibility of opening or closing more centers following a state dependent rule the uncertainties in the utilization rate and users c c u s e r t s and the age and income distribution of users population groups p u s e r a g e i n c o m e r t s contribute to uncertainty concerning the number of cooling center users this translates into an uncertain effectiveness level of cooling centers to avoid a large number of parameters in the case study here we assume c c u s e r t s and p u s e r a g e i n c o m e r t s are identical across sub city region and time and therefore c c u s e r t s c c u s e s and p u s e r a g e i n c o m e r t s p u s e r a g e i n c o m e s r t also we assume p u s e r a g e i n c o m e s p u s e r i n c o m e s p u s e r a g e s for the same reason as p a g e i n c o m e r t s eq 11 as described above of course more general assumptions are possible the total number of cooling center users is calculated in eq 13 which is used later to calculate the operating costs of cooling centers eq 12 u s e r a g e i n c o m e r t s min p o p a g e i n c o m e r t s c c r t s c c c a p r t s c c u s e r t s p u s e r a g e i n c o m e r t s eq 13 u s e r r t s a g e i n c o m e u s e r a g e i n c o m e r t s second we calculate the heat wave mortality relative risk of each age income group h w m r a g e i n c o m e s using the average mortality rate m r a g e i n c o m e and multiplied by relative heat wave risk factor for each age income group h w r r a g e i n c o m e s eq 14 to simplify the case study we assume h w r r a g e i n c o m e s h w r r a g e s h w r r i n c o m e s although more sophisticated assumptions reflecting local demographic conditions are possible and desirable in real applications then the expected daily heat wave mortality with cooling centers d e m r t s and without cooling centers d e m r t s 0 are calculated in eqs 15 and 17 respectively in eq 15 the number of cooling center users is subtracted from each population group and so the d e m r t s is no greater than d e m r t s 0 the difference between d e m r t s and d e m r t s 0 is the mortality prevention effect on a heat wave day to be attributed to cooling centers with h w d r t s and d e m r t s eqs 4 and 15 as well as h w d r t s 0 and d e m r t s 0 eqs 5 and 17 the expected heat mortality with and without adaptation in year t and region r under scenario s e m r t s and e m r t s 0 are calculated in eqs 16 and 18 respectively the difference between e m r t s and e m r t s 0 is the mortality reduction contributed by both temperature and exposure reduction actions this is a key indicator that is later used in the objective functions to optimize adaptation decisions eq 14 h w m r a g e i n c o m e s m r a g e i n c o m e h w r r a g e i n c o m e s eq 15 d e m r t s a g e i n c o m e h w m r a g e i n c o m e s p o p a g e i n c o m e r t s u s e r a g e i n c o m e r t s eq 16 e m r t s h w d r t s d e m r t s eq 17 d e m r t s 0 a g e i n c o m e h w m r a g e i n c o m e s p o p a g e i n c o m e r t s eq 18 e m r t s 0 h w d r t s 0 d e m r t s 0 in the current study we only include acute mortality reduction as a health impact of urban heat adaptation actions even though those actions are also useful in mitigating heat related morbidity such as longer term chronic impacts of repeated heat stress those additional health benefits could be considered in future studies to more comprehensively characterize the health benefits of adaptation 4 cost calculation the cost calculation module calculates the costs of each heat adaptation plan in net present value npv δ t s is the real discount rate in yr 1 and is scenario dependent since it may take on different values to represent different borrowing costs and time preferences severens and milne 2004 here we assume δ t s δ s t to avoid a large number of parameters but time series of discount rates that change over time can be used in future work zarekarizi et al 2020 for temperature reduction actions costs include the capital cost of newly implemented actions at each time x i r t s and maintenance cost of cumulative changes x i r t s which are captured by fixed cost f c i and operation and maintenance o m cost o m i coefficients respectively eq 19 eq 19 t r c o s t r t s i 1 i 1 f c i x i r t s o m i x i r t s 1 δ s t the costs of cooling centers are also of two types the first is the upfront cost of each cooling center typically cooling centers are set up in public facilities such as libraries senior centers and religious buildings the number of cooling centers in each sub city region is assumed to be decided ahead of the planning horizon and the cooling centers can be open during each heat wave day upfront costs for cooling centers may not be necessary since cooling centers are mostly public facilities here we include an upfront cost eq 20 to reflect the efforts of city authorities to look for negotiate and contract to make a facility available as a cooling center the actual cost of each cooling center can be obtained from organizations in charge of cooling centers eq 20 u p f r o n t c o s t c c r t 0 s c c r c o s t c c the second type of cooling center cost is the operation cost eq 21 which depends on the number of cooling center users u s e r s r t s eq 13 heat wave days h w d r t s eq 4 and the unit cost per user day served o p e r c o s t c c eq 21 o p e r a t i o n cos t c c r t s o p e r c o s t c c u s e r s r t s h w d r t s 1 δ s t this represents the energy costs for cooling as well as expenditures on water and food that some cooling centers provide for the visitors berisha et al 2017 in summary the total cost of an adaptation plan is the sum of 1 capital and o m costs of temperature reduction actions i 1 t r c o s t r t s as well as 2 the upfront costs and operation costs of cooling centers i 2 in this plan 2 1 2 city heat optimization model the simulation part of city heat is designed to evaluate the mortality reduction and cost of a specified adaptation plan the optimization part is used to generate near pareto optimal coello et al 2007 adaptation plans based on several objectives that are calculated using variables from the simulation part five objectives are included in the current version of city heat and are defined as follows 1 objectives objective 1 mortality reduction maximize the city wide reduction in expected heat wave mortality eq 22 o 1 1 s s 1 s t 1 t r 1 r e m r t s 0 e m r t s o 1 represents the scenario averaged total mortality reduction effect over time and sub city regions this objective is used as an indicator of the overall public health benefits differences between e m r t s 0 and e m r t s measure the overall decrease in mortality that is anticipated from temperature and exposure reduction actions objective 2 cost minimize total net present value npv of all costs eq 23 o 2 1 s s 1 s t 1 t r 1 r o p e r a t i o n c o s t c c r t s t r c o s t r t s r 1 r u p f r o n t c o s t c c r t 0 s o 2 is the scenario averaged total cost of adaptation and is an indicator of the overall financial cost the capital and o m costs of temperature reduction actions t r c o s t r t s eq 19 and includes the operation cost of the cooling center o p e r a t i o n c o s t c c r t s eq 21 are added over years and sub city regions u p f r o n t c o s t c c r t 0 s eq 20 represents the upfront cost needed for cooling centers objective 3 equity minimize the largest disparity in mortality rate across sub city regions eq 24 o 3 1 s s 1 s max t m a x r e m r t s p o p r t s m i n r e m r t s p o p r t s o 3 the scenario averaged maximum difference in heat mortality rate over time is selected as an equity indicator a smaller o 3 is associated with a more equitable plan the term e m r t s p o p r t s is the heat wave mortality rate fraction of population the difference between the maximum and minimum mortality rate over sub city regions is one way to represent the spatial inequity of heat health risks within a city wilson 2020 in that inequity is a sensitive social issue alternatives whose worst largest in this case mortality difference among sub city regions is small may be preferred objective 4 reliability minimize worst year city wide heat wave mortality eq 25 o 4 1 s s 1 s max t r 1 r e m r t s o 4 reflects a desire to minimize the worst case outcome defining a less heat vulnerable adaptation plan as one that avoids any large single year mortality the maximum single year and city wide mortality max t r 1 r e m r t s over years and averaged across scenarios s is chosen as a vulnerability indicator a smaller o 4 indicates a less vulnerable heat adaptation plan objective 5 carbon reduction maximize cumulative co2 mitigation co benefit eq 26 o 5 1 s s 1 s i i c o b e n t 1 t r 1 r γ i x i r t s o 5 represents the scenario averaged co benefit that is yielded by adaptation actions in addition to health benefits some adaptation actions could have other co benefits that are valuable for city planners for example green actions provide co benefits such as co2 reduction stormwater runoff management and cooling and heating energy savings here one type of several possible categories of co benefits is considered in eq 26 where the co benefit is calculated using a coefficient γ i if for instance co2 reductions are considered then γ i is the coefficient tons yr of co2 reduction from a unit installation of action type i more complex formulations that quantify different types of co benefits that also stem from changes in heat wave frequency are also possible multiple types of co benefits can be added to the model based on the users needs in the current model we include the objectives described above eqs 22 26 representing five performance indicators effectiveness cost equity vulnerability and co benefits to compare and optimize alternative adaptation plans users of city heat can modify or add objectives based on the application and priorities of planners and stakeholders in addition we aggregate outcomes across scenarios by simply averaging however other scenario aggregation operators can be chosen based on the users risk attitudes for example if the users are risk averse to the equity objective eq 24 they could use a min max criterion minimizing the scenario maximum disparity in mortality rates among sub city regions instead of minimizing a scenario averaged disparity 2 optimization after defining the objectives in city heat this section introduces the formulation of the optimization problem city heat uses many objective optimization hadka and reed 2013 to search for a set of heat adaptation plans l s that optimize the five objectives described in eqs 22 26 eq 27 for a many objective optimization problem the optimal solutions l s is a set of non dominated solutions i e there is no solution in the set that is equal or better than another in all objectives which is called the pareto set coello et al 2007 in city heat a solution l represents a pareto optimal urban heat adaptation plan and differences in objective values among those plans represent tradeoffs that users will want to consider in heat adaptation planning eq 27 m i n i m i z e f l o 1 o 2 o 3 o 4 o 5 eq 28 l l 1 l 2 l r eq 29 l r μ 1 r μ 2 r μ i r c c r r 1 2 r eq 30 x r i t s min d p s θ r t 1 s μ i r max a n n u a l c h a n g e i r t eq 31 μ i r l μ i r μ i r u c c r l c c r c c r u c c r i s i n t e g e r the decision variables form a vector l which contains the decision variables l r for each sub city region r eq 28 l r is a vector that contains the parameters μ i r of dps functions for action i in the sub city region r as well as the number of cooling centers to open c c r eq 29 the actual change of action i in year t sub city region r and scenario s is the output of a dps function constrained by an upper bound eq 30 which is determined by the policy parameters μ i r and the system state variables observed until year t 1 θ r t 1 s in that θ r t 1 s changes with time and scenarios the same policy function will lead to different actual decisions in different years and scenarios the annual changes are constrained by the maximum possible annual changes max a n n u a l c h a n g e i r t eq 30 for example the annual changes cannot exceed the available land capacity e g available lands for trees or surface areas e g total roof areas for cool roofs to install those changes in which case m a x a n n u a l c h a n g e i r t l u c a p i r s t 1 t 1 x r i t s where l u c a p i r s is the available land capacity to implement action i in sub city region r and t 1 t 1 x r i t s is the changes that have been taken during previous years their difference is the remaining available capacity for action i at year t the policy parameters μ i r are constrained by the corresponding lower and upper bound μ i r l and μ i r u respectively eq 31 the number of cooling centers c c r are integers and they are bounded by the corresponding lower and upper bounds c c r l and c c r u respectively eq 31 a minimum number of cooling centers might for instance be required to open in each sub city region and the maximum number of cooling centers might depend on available facilities to apply the dps method it is important to decide the family of policy functions to use artificial neural networks anns and radial basis functions rbfs are widely used in modeling adaptive decisions in reservoir operation giuliani et al 2016 2017 quinn et al 2017b lake pollution control quinn et al 2017a and microgrid management gupta et al 2020 garner and keller 2018 use quadratic functions to model dike heightening decisions in response to occurrences of uncertain sea level rise and storm surge giuliani et al 2016 find that functions with more parameters such as anns tend to be more flexible but could lead to problems such as overfitting and increased computational burden they conclude that there is no universal recommendation for which policy functions to apply to a specific problem although anns and rbfs functions are highly flexible so that they can represent a wide range of different policy rules those functions might sometimes be difficult to communicate therefore simple functions such as the promotional integral derivative pid function lempert and turner 2021 could be adopted for better interpretability in the setting of city heat the investment decisions are in the form of fractional increment of trees planted on available lands as well as pavements and roofs converted to cool pavements and roofs these factions are expected to monotonically increase with the number of heat waves and are bounded between 0 and 1 with this expectation we choose a three parameter logistic function whose parsimony reduces the number of decision variables speeds up convergence and leads to more intuitive and understandable policies compared to more complicated functions e g anns the three parameter logistic function is shown as in eq 32 eq 32 d p s θ r t 1 s μ i r a i r b i r d i r max 1 d i r 1 e a i r θ r t 1 s b i r d i r 0 l u c a p where max 1 d i r 1 e a i r θ r t 1 s b i r d i r 0 gives a fraction between 0 1 representing a fractional change of the total available land use l u c a p i r s the shape of max 1 d i r 1 e a i r θ r t 1 s b i r d i r 0 is controlled by a i r b i r d i r which are optimized to get the pareto optimal policy function for each temperature reduction action in each sub city region we use the average heat wave days that occurred in the previous five years a scalar rather than a vector here as the system state variable in this study so that θ r t 1 s 1 5 t 5 t 1 h w d r t s because h w d r t s h s t 0 t s δ t r t r t s m i t i the actual decision x i r t s depends on θ r t 1 s h w d r t s and t 0 t s thus decisions are dynamically adjusted to different climate scenarios represented by t 0 t s the logistic function is non decreasing which means that more temperature reduction actions are implemented when more heat wave days are observed fig 2 provides several examples of the shape of a three parameter logistic function from fig 2 a c we can see that a i r controls the steepness of the logistic curve representing the sensitivity of change to system information on θ r t 1 s larger values of a i r represent increased sensitivity and steeper curves fig 2 c here we impose a boundary 0 a i r 10 to limit the decision space while spanning a low to high sensitivity level next b i r is the inflection point of the logistic curve representing the value of θ r t 1 s in our model the average heat wave days during the previous 5 years where the rate of increase in change is largest fig 2 a c here we constrain 0 b i r 200 to allow for policies whose largest rate is achieved when the average heat wave days are anywhere between 0 and 200 days this is longer than the period from may to september 153 days enabling policies that perhaps never implement adaptation actions moreover d i r is a parameter that shifts the curve so that it has a non zero intercept at 0 heat wave days this allows for immediate investment positive intercept if d i r 0 fig 2 g because the fraction of converted land use is bounded below by 0 through max 1 d i r 1 e a i r θ r t 1 s b i r d i r 0 if d i r 0 this simply results in delayed investment fig 2 i the range of 1 d i r 1 allows for a wide range of starting conditions of adaptation actions there are three main advantages of using the dps method for the urban heat adaptation planning problem first the dps method reduces the number of decision variables needed making the search for optimal decisions easier in the traditional intertemporal open loop setting in which action in each region and time is modeled as a decision variable decisions of i 1 types of temperature reduction actions in r regions through t years require optimizing i 1 r t decision variables in the dps closed loop setting it needs i 1 r p decision variables where p is the number of parameters in the dps function which is often much smaller than t in our case p 3 and t 20 quinn et al 2017a show that the optimization of a lake pollution control problem converges much faster using the dps closed loop form than its open loop counterpart second the closed loop dps scheme leverages the most up to date system information to guide future decisions this makes decisions more flexible and responsive to changing conditions and therefore likely to perform better de neufville and scholtes 2011 third the policy functions optimized within a time horizon t can be extended to future years which is not possible with the open loop setting 2 2 borg moea to solve many objective optimization problems after formulating the optimization problem we now introduce the algorithm that we use to solve the many objective optimization problem in city heat complex planning problems often contain many interacting subsystems that introduce non linearities and non separable dependencies reed et al 2013 as the number of objectives increases it becomes more challenging for multi objective evolutionary algorithms moeas to solve such constrained non linear problems with decision spaces that are high dimensional discrete non convex and stochastic coello et al 2007 reed et al 2013 in our problem for instance the reduction in the number of heat wave days caused by temperature reduction actions depends on many factors such as the choice of climate projections heat wave definitions and effectiveness of each action which results in non linear and complex relationships no closed form derivative information can be obtained and thus gradient based optimization algorithms do not work in this case instead we adopt a state of art moea borg which has been shown to find high quality tradeoff solutions for complex many objective and dynamical problems zeff et al 2016 giuliani et al 2017 garner and keller 2018 gupta et al 2020 such as mine zeff et al 2016 use borg to solve for optimal integrated planning pathways that coordinate long term water supply infrastructure development and adaptive short term drought management actions under uncertainty considering six objectives for four water utilities in north carolina giuliani et al 2017 identify the optimal control of four multipurpose water reservoirs in the red river basin in vietnam under stochastic hydrologic conditions with three objectives garner and keller 2018 use borg to optimize the annual dike heightening decisions based on two objectives and under uncertain sea level rise projections and storm surge gupta et al 2020 employ borg to search for optimal microgrid management policies with stochastic samples of load wind and solar while simultaneously optimizing three objectives all these aforementioned studies show borg can find high quality and diverse solutions for complex many objective optimization problems under uncertainty however there is no study using borg to optimize urban heat adaptation plans given the complexity of finding heat adaptation plans optimizing multiple objectives under uncertainty and borg s ability to solve dynamic many objective problems we adopt borg moea to solve the stochastic many objective optimization problem section 1 1 2 in city heat one main advantage of borg is its use of ε dominance ward et al 2015 which speeds up the algorithmic search process while also providing a mathematical proof of convergence and guaranteeing diversity in performance across objectives hadka and reed 2013 for each objective users specify a preferred precision level ε that represents a meaningful improvement in performance hadka and reed 2013 within a multi dimensional ε box defined by these precision levels only the solution closest in terms of the l2 norm to the ideal point within that box is archived where the ideal point combines the best values of the objectives within the box this reduces the number of solutions stored in the archive and reduces the computational burden hadka and reed 2013 the ε values we adopt for each objective in this study can be found in table 1 hadka and reed 2013 provide a detailed explanation of the features and advantages of borg compared to other moeas since the borg moea is a stochastic heuristic search algorithm whose search depends on the random seed used to initialize the population and generate new solutions we solve this problem using 10 different random seeds to start the algorithm we use the parallel implementation of borg with a single master and multiple workers hadka and reed 2015 the hypervolume a multi dimensional measure of the space dominated by the pareto set quinn et al 2017a is adopted to assess the convergence of the search consistent flattening to a common hypervolume across seeds indicates the algorithm has converged 2 3 robustness of adaptation plans to test the robustness of the pareto adaptation plans generated by city heat we further re evaluate their performance on additional out of sample scenarios beyond those that were considered when optimizing the policy functions in our study the satisficing robustness measure quinn et al 2017a is used to evaluate those optimal plans a satisficing measure computes the fraction of scenarios in which a solution achieves target performance levels on given criteria the selection of these targets is obviously an important value judgment that will affect what solutions perform best by this measure alternatively other robustness measures such as regret measures lempert et al 2003 could also be used for the re evaluation purpose the choice of robustness measures depends on users preferences and risk attitudes which may affect which plans are considered most robust we recommend using multiple robustness metrics in real world applications if possible also we only include a single criterion i e mortality rate for simplicity to illustrate the robustness analysis however multiple criteria could be considered to define the robustness of adaptation plans in real world applications 3 a hypothetical study for baltimore md usa baltimore is a mid sized city in the mid atlantic region of the united states the summertime heat in such a warm and humid city is associated with many direct and indirect health impacts bunker et al 2016 heilmann et al 2021 recently there were 134 heat related deaths reported in maryland between 2012 and 2018 marylandreporter 2019 twenty eight percent or thirty seven of those deaths occurred in baltimore city which is home to only ten percent of maryland s residents therefore the city government of baltimore has made prioritized heat stress management in its urban climate adaptation efforts baltimore 2019 furthermore the association of elevated uhi with poverty and race strongly applies in baltimore where the highest heat conditions are often found in poor minority neighborhoods with a history of discriminatory housing policies huang et al 2011 hoffman et al 2020 wilson 2020 disparities in heat exposure reinforce inequities equitable urban planning and health policy and environmentally just climate adaptation actions thus emphasize the need to reduce heat exposure in underserved and vulnerable populations milan and creutzig 2015 therefore we apply city heat to a hypothetical heat adaptation problem in baltimore to demonstrate the usage of this tool and some important results and insights it can generate in the baltimore case we assume adaptation decisions are made for each of the city s 11 planning districts fig 3 as defined by the baltimore city department of planning aligning decision making with planning districts makes the identified adaptation plans useful in the real world context thus the sub city regions described in section 2 are referred to as districts in our baltimore study demographic data including population age structure and poverty in baltimore are retrieved from the american community survey us census bureau 2018 the data are collected at the census tract level and then aggregated to the district level if a census tract crosses district lines which occurs in 10 200 cases it is assigned to the district in which most of its area is located land use data are obtained from the chesapeake phase 6 land use dataset which is a gridded 1 m resolution dataset with 16 different land use categories usgs 2014 the building footprint in each district is retrieved from baltimore open gis data baltimore 2019 for each district we calculate the current area of tree canopies road surfaces roof surfaces and the area available for tree planting in each district the available land for tree planting is the sum of turf land and a portion α r s of impervious surfaces that are not occupied by buildings or roads e g sidewalks and parking lots α r s is treated as an uncertain parameter because different levels of tree conversion might be possible depending on infrastructure conditions for simplicity we treat α r s as the same across all districts in the case study α r s α s r in that the cost of tree planting is higher when trees are planted on impervious surfaces e g cost for making tree wells we model the capital cost of trees as a step function whose higher value is reached when all turf lands have been converted to trees the temperature deviation of each district is estimated with temperature data in baltimore collected by a dense sensor network deployed by johns hopkins university scott et al 2016 shi et al 2021 table 2 provides a summary of the district level demographic and land use data of baltimore we use 32 daily maximum temperature projections generated by different climate models with different downscaling and bias correction methods retrieved from na cordex ncar 2014 and loca datasets pierce et al 2014 the 32 models represent a range of possible climate conditions that baltimore could experience for the next decades representing the climate uncertainty the annual summer mean temperatures of each projection are calculated based on the daily data and are shown in fig 4 in this study we use 10 projections red lines in fig 4 in optimization and use the remaining 22 projections grey lines in fig 4 in robustness analysis mean summer daily maximum temperatures range from 28 3 c to 30 6 c in the climate projections we use compared with the historical value 27 8 c in baltimore these projections represent a wide range of global warming scenarios of an increase from 0 5 c to 2 8 c by 2040 which well characterizes climate uncertainty these temperature projections are further adjusted by δ t r to get the district level temperature projections as described in section 2 1 1 in addition we include three different heat wave definitions in this study anderson and bell 2010 huang et al 2010 peng et al 2010 because previous studies found different heat related mortality impacts from different heat wave definitions the magnitude of heat health impacts is a key factor that determines the desirability of alternative heat adaptation strategies these heat wave definitions represent three different ways to define a heat wave the first from huang et al 2010 defines a heat wave as a period with at least three consecutive days with daily maximum temperatures over a single absolute temperature threshold of 35 c in the second anderson and bell 2010 define a heat wave as a period with at least two consecutive days with daily maximum temperature over a single relative temperature threshold defined as the 95th percentile of local summer temperature from may 1 to september 30 the third way to define a heat wave is from peng et al 2010 who define it using two relative temperature thresholds t 1 as the 97 5th percentile and t 2 as the 81st percentile of daily maximum temperature respectively a heat wave is then defined as the longest period of consecutive days satisfying the following conditions the daily maximum temperature is above t 1 for at least three days the daily maximum temperature is above t 2 for every day of the entire period and the average of daily maximum temperatures over the entire period is above t 1 other heat wave definitions could also be used but these three definitions represent typical defining methods in the literature chen et al 2015 faye et al 2021 in this study we use the moving average of heat wave days that occurred during five preceding years θ r t 1 s 1 5 t 5 t 1 h w d r t s as the input of the dps functions section 2 1 2 to trigger temperature reduction actions in each sub city region district percentiles for the second and third heat wave definitions are estimated empirically from historical data over the period 1949 2019 because temperatures in each district are affected by δ t r and t r t s m i t i the heat wave conditions are different in each district table 2 other variables such as the observed effectiveness of each action could be used as inputs in dps functions and other policy functions section 2 1 2 can be used as the dps functions these possibilities are left for future studies additionally we assume three temperature reduction actions trees cool roofs and cool pavements as well as one exposure reduction action cooling centers can be invested in each district in baltimore within a 20 year period from 2020 to 2039 the number of cooling centers is assumed to be decided ahead of the study period in each district and then these cooling centers can accommodate users during heat waves through the 20 year period the temperature reduction actions can be taken each year the purpose of including two types of decision variables c c r and μ i r i r is to show city heat can flexibly handle different types of decision variables the temperature reduction functions f i in eq 3 of the three temperature reduction actions are shown in figure a 1 in the supplementary materials here we assume that the temperature reduction effects increase linearly with the percentage of cool roofs and pavements li et al 2014 levinson and harvey 2017 for trees previous studies find that the temperature reduction effect has a non linear relationship with the percentage of the tree coverage and the marginal effect increases ziter et al 2019 alonzo et al 2021 therefore we assume the tree canopy coverage has a quartic temperature reduction function the coefficient t r i s is uncertain because the temperature reduction effects of those actions vary largely across studies santamouris et al 2017 the uncertainty about future climate change population growth and actions effectiveness along with other factors are characterized by various scenarios we take a wide range of those uncertain parameters table a 1 and use the lhs method to generate thousands of scenarios for optimization and robustness analysis the values of scenario independent parameters are summarized in table a 2 for a more thorough case study values or ranges of parameters can be obtained from more comprehensive literature reviews or interviews with decision makers and experts in our optimization we generate 1500 scenarios which are the combination of 10 temperature projections red lines in fig 4 3 heat wave definitions and 50 lhs samples of scenario dependent parameters 10 3 50 table a 1 for robustness analysis we generate another 3300 scenarios which are the combination of the remaining 22 temperature projections the grey lines in fig 4 3 heat wave definitions and another 50 lhs samples of scenario dependent parameters generated over the same parameter ranges 22 3 50 here we define the robustness of an adaptation plan by calculating the fraction of 66 000 scenario years 20 years 3300 scenarios satisfying the condition that the annual mortality rate is below the level of the 1995 chicago heat wave whitman et al 1997 we select this robustness indicator for demonstration the definition of robustness and choice of robustness indicators can be determined by users based on their preferences 4 results discussion in this section we demonstrate the insights that city heat can yield by exploring the set of pareto optimal plans and their tradeoffs through its application in the baltimore case city heat found 166 approximately pareto optimal heat adaptation plans for baltimore fig 5 each plan consists of policy functions for each of the three temperature reduction actions that we study and the recommended number of cooling centers to install immediately in each district more details of those plans are presented in section 4 3 the hypervolume analysis across 10 random seeds indicated fast and consistent convergence of the borg algorithm on our problem figure a 2 the parallel coordinate plot in fig 5 a provides an overview of the objective values of 166 pareto approximate adaptation plans which illustrates the tradeoffs that city planners need to consider when choosing among alternative plans each line results from an individual adaptation plan and its intercepts with the five vertical axes correspond to its performance on the five objectives the lower a line lies on an axis representing lower values the better performance the plan has on that objective because the desire is to minimize those objectives eq 27 note that the maximization objectives o 1 and o 5 are multiplied by 1 so that they can be minimized according to the definition of pareto solutions there is no pair of lines such that one lies on or below the other on all axes while being strictly below on at least one axis this means that compared to each of the other plans any one particular plan has at least one objective that is better the color of each line represents the average cost across scenarios per life saved relative to the no action baseline i e the expected unit cost per life saved defined as the discounted cost divided by the undiscounted number of lives saved alternatively the levelized cost defined as the discounted cost relative to the base no action case divided by the discounted number of lives saved could be calculated however it is controversial whether a non monetized benefit such as lives saved should be discounted and if so which discount should be used severens and milne 2004 here we used the undiscounted number of lives saved as the objective o 1 eq 22 and calculated the expected unit cost as defined above we further calculate the discounted number of lives saved and the levelized cost per life saved of each plan using the same discount rate as used for money in each scenario the result shows no significant changes in the relative rank of those 166 plans when lives saved are discounted figure a3 however the levelized costs increased by approximately 50 80 of the 166 generated plans this visualization helps decision makers to understand the tradeoffs involved in choosing among alternative adaptation plans for example among the 166 plans the most expensive plan is plan 6 fig 5 b on average across scenarios this plan costs 2 167m all monetary values are in 2019 dollars over 20 years saves 200 lives absorbs 354 kton co2 limits worst year heat related mortality to 32 deaths and controls the maximum difference in mortality rate to 9 deaths per 100 000 people across these same scenarios another plan plan 133 fig 5 b has about half the average cost 1 051m but saves only 160 lives absorbs 209 kton co2 limits worst year heat related mortality to 37 deaths and keeps the maximum difference in mortality rate at 10 deaths per 100 000 people decision makers can choose between these two plans based on their own judgment as to whether the expected incremental investment of 1 116m 2 167m 1 051m over 20 years is worth the expected improvement in saving an extra 40 lives 200 160 lowering the worst year mortality by 5 deaths 37 32 and eliminating an extra 145 354 209 kton co2 quantifying these tradeoffs can improve the understanding of outcomes resulting from different plans and facilitate negotiation among decision makers 4 1 increasing marginal cost of saving lives across all solutions in the pareto set we see that the expected total cost increases non linearly with the expected mortality reduction indicating a rising marginal cost or decreasing efficiency in saving lives through urban heat adaptation fig 5 b this could be a result of the non linear relationship between heat wave days and average summer temperatures examples in figure a 4 under high temperature conditions a unit degree decrease in temperature results in a greater decrease in the number of heat wave days and consequently heat related mortality than the same unit degree decrease under lower temperature conditions therefore the initial investment used to alleviate the hottest conditions is more efficient in reducing heat wave days and mortality subsequent investments have diminishing efficiency because it is more difficult to reduce heat wave days when the temperatures are already moderate to low in the baltimore case the expected unit cost as defined above of the 166 plans ranges from 0 4m to 11 6m per life saved which provides a wide spectrum of plans with different cost efficiency levels for city planners to choose from the cost of each plan would be lower if co benefits such as carbon reduction are converted to monetary values which further increases the cost efficiency of those plans city planners can select the plans with the most appropriate cost efficiency level for their own cities to note some adaptation plans save fewer lives while also incurring a higher unit cost per life saved this is because they have advantages in terms of other objectives for example plan 64 fig 5 b saves an average of 44 lives across scenarios with an average unit cost of about 11 6m life saved compared to other plans with a similar level of mortality reduction plan 64 relies heavily on trees which significantly increases the unit cost but also reduces more co2 4 2 effectiveness efficiency and equity in the design and application of city heat promoting equity is set as one of the main objectives of heat adaptation from the baltimore case we identify a potential effectiveness efficiency equity tradeoff among the optimized plans as shown in fig 5 c there is an inverted u shape between the expected mortality reduction effectiveness and the expected maximum disparity in district level mortality rates the indicator of inequity when the investment level is low the potential for mortality reduction is low and the adaptation actions are concentrated in a limited set of districts due to the limited investment this exaggerates the disparity in mortality rates between districts with and districts without adaptation exacerbating inequity increasing investment continues to exacerbate inequities up to a point beyond which further investment begins to decrease inequity this happens when investments are sufficient to be more evenly distributed across districts in addition to the tradeoff between effectiveness mortality reduction and equity disparity in mortality rate among districts we also find a tradeoff between efficiency expected unit cost and equity an inefficient plan with a high unit cost often results in more equitable outcomes than a more efficient plan with a lower unit cost that achieves the same mortality reduction effectiveness for example plans 60 and 65 fig 5 c both save an average of 100 lives plan 60 has an expected unit cost of 5 0m per life saved which is lower than the expected unit cost 8 8m of plan 65 however plan 65 leads to a lower expected district level mortality rate disparity 8 per 100 000 people than plan 60 does 11 per 100 000 people this is because plan 60 concentrates investment in a few districts with the lowest marginal cost for saving lives leading to a lower unit cost to achieve a certain total mortality reduction however to balance heat health risks among districts plan 65 spreads investments across more districts including some districts where the cost effectiveness is not as high resulting in a higher expected unit cost for example plan 60 only invests trees in three districts while plan 65 invests trees in five districts however plan 65 to a smaller discrepancy in mortality rate across districts better equity outcomes 4 3 adaptation actions type timing and location the parallel coordinate plot in fig 5 a presents an overview of the performance of the optimized adaptation plans without providing information on the specific actions they take hence we further provide a detailed visualization of each generated plan including its number of cooling centers policy functions of three temperature reduction actions expected fractional changes and expected discounted cost of each action taken in each district and year here we visualize plan 6 as an example fig 6 this plan achieves the greatest expected mortality reduction among all the pareto optimal plans the table at the top of fig 6 summarizes its five objective values the first row of fig 6 shows a the optimal distribution of cooling centers across the city and the optimized dps policy functions for investing in b trees c cool pavements and d cool roofs in each city district the varying shapes of these policy functions reflect that different adaptation rules are optimal in different districts that translate previous heat wave conditions into different temperature reduction actions for example curves with a step change e g the grey line in fig 6 b representing trees in the central district indicate a rapid investment in adaptation once a heat wave threshold or tipping point is reached curves in the form of a horizontal line e g the blue line in fig 6 c representing cool pavements in the south district indicate a constant investment that is independent of heat wave conditions curves with a non zero intercept on the y axis e g the purple line in fig 6 d representing cool roofs in the southwest district indicate that adaptation should be taken even if there are no heat wave days which could be viewed as a preemptive adaptation strategy applying these rules across the 1500 scenarios leads to varying levels of investment in those actions in each district the average levels of these investments across scenarios are displayed in terms of the scenario average cumulative fraction of that district s capacity for each action that is implemented over time in fig 6 e h and the cost of implementing that action over time in fig 6 i l as the costliest plan plan 6 has non zero average investments in almost every action in every district tree planting actions are implemented close to their maximum extent in the near term in all but two districts as trees take time to grow the cumulative fraction of possible land area converted to trees fig 6 e does not look the same as the cumulative fraction of possible tree canopy fig 6 f this illustrates the necessity to invest quickly in actions that take time to fully realize their health benefits if the most mortality reduction is desired investments in actions that have immediate benefits like cool pavements and roofs are more evenly and gradually increased in the first several years to take advantage of their discounted cost as time goes on the average cost of the cooling centers fig 6 i is much lower compared to the other three actions which require large capital and maintenance costs with this visualization tool planners can get a direct sense of the timing magnitude and cost of taking each action in each district further visualizations could investigate how these actions and their costs differ in different scenarios to see the influence of the optimized state dependent rules and uncertainty on investments as an illustration we only visualize plan 6 but the same plot can be generated for any pareto approximate adaptation plan of users interest therefore this tool provides a roadmap illustrating actions and costs in each sub city region and year which helps users better understand each generated plan 4 4 actions and locations requiring preemptive adaptation in fig 6 b d some dps curves have non zero intercepts on the y axis parameter d 0 see fig 2 and section 2 1 2 for explanation indicating that adaptation should be taken even if there are no heat wave days this can be viewed as a anticipatory adaptation strategy and is crucial to preventing heat mortality most effectively it is therefore important to understand in which districts or sub city regions in general and under which conditions those precautionary actions are needed in fig 7 we present the values of parameter d in the dps functions representing the investment in trees of the 166 optimized heat adaptation plans for most of the plans that have a scenario averaged mortality reduction of over 150 lives those on the right side of the red dashed line the values of d are negative below the blue dash line in districts including the north northeast northwest southwest and west the negative d values indicate that urban afforestation should start even before observing any heat wave days in those districts which can ensure an effective mortality reduction under uncertainty these five districts are the areas with the most vulnerable populations low income and elderly in baltimore fig 8 figs 7 and 8 together suggest that it is important to protect vulnerable populations such as the elderly and the low income to achieve the effectiveness and equity in urban heat adaptation practices values of d for cool pavements and roofs can be found in figure a 5 and figure a 6 in the appendix similar to trees precautionary investments in cool pavements are necessary for some districts with the most heat vulnerable populations to achieve high mortality reduction effects however very few plans choose to invest in cool roofs before the occurrence of heat waves this is because cool roofs have similar effects in reducing air temperatures as cool pavements but at higher costs santamouris et al 2017 this is consistent with the finding that cool roofs in general are not favored by city heat the conclusions might change with respect to different cost and effectiveness assumptions those findings suggest that urban heat adaptation should prioritize the areas of greatest population vulnerability to be most protective of human health which is also found in previous studies vargo et al 2016 this can be an important rule for cities to allocate their heat adaptation resources to maximize cost effectiveness 4 5 robustness of optimized adaptation plans robustness is important when comparing alternative adaptation plans under uncertainty and especially deep uncertainty lempert et al 2003 marchau et al 2019 in this study we re evaluate the 166 adaptation plans across 3300 out of sample scenarios see section 3 then we calculate the fraction of scenario years 3300 20 that meet our satisficing metric whether the city wide annual mortality rate in a given year is below that of the 1995 chicago heat wave a larger fraction indicates that a plan is interpreted as more robust fig 9 shows the robustness of the 166 adaptation plans and their total cost as shown in fig 9 the more costly adaptation plans considered here have higher robustness against uncertainties and can lower the possibility of a heat disaster for example the robustness indicators of the adaptation plans with the lowest cost 5m and the highest cost 2 167m are 99 2 and 99 9 respectively which means the possibility of experiencing a heat disaster that is more severe than the 1995 chicago one drops from 0 8 to 0 1 by the increase in investments robustness information for the adaptation plans enables users to quantify the residual risks of a heat disaster associated with each plan and to select an adaptation plan if they are risk averse and concerned about the worst scenarios the selection of metrics measuring robustness can be made by users based on project needs plan robustness could be defined by comparing with major historical hazard events as above or using user selected criteria with a practical interpretation this current study only includes mortality rate as a criterion but other criteria such as cost equity or co benefits could also be considered 4 6 comparisons of plans generated by city heat and plans based on simple rules or optimized with a single scenario shi et al 2019 suggest that it is valuable to consider multiple scenarios and adaptive management rules to address complex climate adaptation problems to demonstrate the value of city heat in identifying better strategies for the high dimensional urban heat adaptation problem with multiple objectives and uncertainty we compare the 166 plans generated by city heat considering the 1500 scenarios set 1 compared to 1 plans optimized with respect to the average scenario set 2 and to 2 plans based on simple decision rules set 3 for a fair comparison the performance of all these plans is compared on the 3300 out of sample scenarios see section 4 5 this ensures all plans are evaluated under scenarios to which they were not optimized analogous to the out of sample validation method that is widely used in comparing the performance of machine learning models for the first comparison we define the single average scenario to which set 2 policies are optimized based on the following criteria 1 the climate projection has the summer mean temperature closest to the average of all projections 2 a single heat wave definition is chosen the definition in anderson and bell 2010 is chosen for demonstration and 3 the middle value of the range of each uncertain parameter is used these criteria make the scenario represent the average of all scenarios we then generate another set of pareto approximate heat adaptation plans set 2 using city heat under this average scenario there are 215 plans generated in set 2 which represents the best trade offs among the five objectives when only considering the average future conditions fig 10 presents the scenario averaged values of the five objectives eq 22 26 for both plans in set 1 green color and in set 2 red color across the 3300 out of sample scenarios from this we see that anchoring on a single scenario leads to the identification of high cost plans red lines in set 2 which do not outperform plans in set 1 in other objectives we find that no plan in set 1 0 of 166 is dominated by any plan in set 2 for all five objectives by contrast 55 plans in set 2 26 of 215 are dominated by at least one plan in set 1 this suggests that optimal plans for a single average scenario may be unfavorable if the future is not as assumed thus it is important to consider a wide range of possible future scenarios for the second comparison we consider two simple rule based heat adaptation plans set 3 this first simple rule based plan or plan 1 year is to make 5 of all possible investment options of trees cool roofs and cool pavements every year through 20 years the second plan or plan 5 year is to make 25 of all possible investment options every 5 years for both plan 1 year and plan 5 year 10 cooling centers the upper bound considered in the optimization are assumed to open in each district the performances on the five objectives of these two plans are evaluated using the city heat simulation model section 2 1 1 under each scenario the two city heat generated plans set 1 with the closest scenario averaged cost o 2 to plan 1 year and plan 5 year respectively are chosen for comparison plan 68 is chosen for plan 1 year and plan 141 for plan 5 year table 3 summarizes the number of scenarios in which the two set 1 solutions plan 68 and plan 141 are better than the two set 3 solutions plan 1 yr and plan 5 yr from this we can see how considering adaptive management can help identify better heat adaptation strategies among the 3300 out of sample scenarios plan 68 has better performances in all 5 objectives than plan 1 yr in 872 26 42 scenarios while in contrast plan 1 yr does not outperform plan 68 in all 5 objectives in any scenario since plan 68 has at least 1 objective that is better than plan 1 yr for all 3300 scenarios the same conclusion can be drawn based on the comparison between plan 141 and plan 5 yr plan 141 has better performances in all 5 objectives than plan 1 yr in 907 27 48 scenarios while in contrast plan 5 yr does not outperform plan 141 in all 5 objectives in any scenario this comparison shows that flexible plans generated by city heat set 1 which leverage adaptive management based on evolving system information outperform plans based on simple open loop rules set 3 this further demonstrates the usefulness and value of sophisticated decision analysis tools like city heat which can consider information learned over time through the baltimore case study we demonstrate that city heat can provide valuable insights such as 1 quantitative trade offs among multiple health and social economic objectives 2 adaptive policy plans for different adaptation actions and 3 policy robustness across a wide range of future scenarios these insights are important to support urban heat adaptation and are not provided in previous studies as discussed in section 1 4 and 1 5 5 conclusion the urban heat adaptation planning problem is a multi objective high dimensional stochastic and dynamic decision making problem we propose a decision support tool city heat to address those challenges and provide a case study for baltimore city to illustrate what insights this tool can generate city heat optimizes the long term performance of alternative urban heat adaptation plans in five dimensions including plan effectiveness in achieving health benefits total financial cost equity in mortality disparities across sub city regions heat vulnerability measured by worst case single year heat mortality and co benefits such as cumulative co2 sequestered to address the impact of uncertainty on adaptation plans city heat searches for optimal adaptive decision rules across thousands of scenarios that represent various possible future situations for climate change population and demographic change and the effectiveness of adaptation actions in this study the heat wave conditions characterized by the average number of heat wave days during the previous five years are used to trigger decisions in which the rules use the most recent information to adjust decisions at the current time step by using optimization the best parameters for the decision rules across the range of uncertainties are identified those seemingly complicated rules fig 6 can be interpreted as simple statements for policy recommendations as in our baltimore example we find several districts need anticipatory urban afforestation and this can be directly formed as a policy recommendation in addition several decision rules fig 6 exhibit pronounced tipping points thresholds for the number of heat waves beyond which an action should be taken in each district where the curve goes above zero therefore city heat results can be interpreted in terms of easily understood policy recommendations advantages of city heat can be summarized as follows first city heat quantifies complex trade offs among multiple planning objectives in our baltimore example city heat explicitly identifies trade offs among effectiveness efficiency and equity of alternative heat adaptation plans fig 5 indicating the challenges of mitigating inequities in urban heat management also city heat reveals a need to prioritize populations with the greatest vulnerabilities fig 7 in urban heat adaptation therefore city heat provides quantitative evidence for policy making in urban heat adaptation second city heat generates rule based adaptation decisions at fine temporal annual and spatial sub city resolutions while considering multiple planning objectives and robustness across a large ensemble of future scenarios it is impossible to generate a rich set of pareto efficient candidate adaptation plans intuitively or based on rules of thumb without the support of tools such as city heat these features distinguish city heat from tools developed or employed in previous studies which have either not explicitly optimized health objectives limited themselves to a small set of pre defined adaptation plans and or disregarded uncertainty of climatic and social factors third city heat is a generalizable and flexible decision support framework and tool what candidate adaptation actions performance objectives and uncertain factors to include can be customized based on users needs many of the needed inputs such as climate projection data and population data can be conveniently retrieved from publicly available datasets such as na cordex loca and the american community survey data used in our study sub city level temperature data is less easily obtained but their availability is likely to increase because many cities have begun to measure their intra urban temperature distributions scott et al 2016 shandas et al 2019 shi et al 2021 finally other assumptions about the range of uncertain parameters and some case specific data such as costs of actions in different cities can be tailored to the application and be informed by interviews with stakeholders and experts fourth city heat has the ability to examine many important questions in urban heat adaptation planning especially about social equity and uncertainty for example concerning equity how do populations of varying ages races genders and income levels benefit from urban heat adaptation how can environmental justice be enhanced through urban heat adaptation regarding uncertainty with the tool we can ask which uncertain parameters contribute the most to uncertainty in the performance of adaptation plans what is the value of improved knowledge about those uncertain parameters in terms of enhanced performance of strategies answering those questions could help city planners better address the challenges of urban heat adaptation by finding more effective efficient and equitable adaptation plans finally compared to more simplified decision processes we show that city heat derives better strategies our comparisons of the heat adaptation plans generated by city heat with plans based on simple rules demonstrate the usefulness and advantages of city heat in identifying better solutions for the urban heat adaptation problem which is high dimensional and complex city heat plans generated under multiple scenarios that can adapt to those scenarios often pareto dominate simple plans based on a constant rate of an investment over the time horizon in each adaptation action or optimized considering just one scenario this further confirms that considering various scenarios optimizing multiple objectives and incorporating the flexibility to adapt decisions in response to information learned over time can be valuable to identify better decisions for complex climate adaptation problems shi et al 2019 future research could pursue several directions although the case study was sufficient to illustrate how city heat can be applied to a real world case and what insights can be generated several assumptions such as the costs of different actions and performance measures taken from the literature might not best reflect the situation and priorities in baltimore future work can refine these assumptions through engagement with local experts managers and stakeholders to best reflect their knowledge and preferences additional objectives could also be defined to reflect the full range of community benefits and concerns such as reduction of air pollution or compensation for historic injustices finally the tool could be expanded in scope to address other urban adaptation problems such as managing possible increases in runoff and pollution from more frequent severe storms this expansion is desirable because some actions addressing urban heat such as green roofs also can help mitigate other climate impacts in summary city heat is an innovative decision tool for identifying both immediate actions and long term adaptations to mitigate the effects of urban heat waves considering multiple objectives under uncertainty tradeoffs among objectives such as effectiveness cost equity vulnerability and co benefits are critical inputs to decisions and are quantified and visualized by city heat software availability software name city heat equity adaptation tool city heat developers rui shi benjamin f hobbs julianne d quinn robert lempert and debra knopman year first official release 2022 hardware requirements pc or hpc system requirements windows with microsoft mpi mac or linux with openmpi program language c program size 1 mb 4 mb including borg software from http borgmoea org availability https github com srayum city heat license gnu general public license v3 0 documentation user guide and examples hosted at https github com srayum city heat the authors do not have any conflict of interest declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research was supported by the mid atlantic regional integrated sciences and assessments marisa consortium through a grant to the rand corporation from the national oceanic and atmospheric administration s climate program office na16oar4310179 the johns hopkins 21st century cities initiative and the johns hopkins alliance of a healthier world we thank reid cagir benjamin zaitchik and darryn waugh of jhu klaus keller of psu jordan fischbach of the water institute of the gulf and other marisa colleagues for their suggestions to improve this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105607 
25469,rising global temperatures and the urban heat island effect can amplify heat related health risks to urban residents cities are considering various heat adaptation actions to improve public health enhance social equity and cope with future conditions beyond past experience we present the city heat equity adaptation tool city heat which suggests optimal investments for mitigating urban heat and reducing health impacts through modifications of built cool roofs pavements and natural urban afforestation environments and reductions of people s heat exposure cooling centers the optimization considers multiple public health and social objectives under a wide range of future scenarios an application to baltimore md usa demonstrates how city heat can generate pareto efficient multi year heat adaptation plans we quantify effectiveness efficiency equity tradeoffs among alternative plans and show the advantages of flexible decision making city heat can be adapted to the natural built and social environments of other cities to support their urban heat adaptation planning recognizing local objectives and uncertainty keywords heat waves urban heat island urban heat adaptation deep uncertainty equity multi objective robust decision making data availability i have shared my code data in the software availability section 1 introduction 1 1 health impacts of heat waves a heat wave or an extreme heat event is often defined as a consecutive period of hot days with temperatures above a threshold mazdiyasni et al 2017 although there is no universal agreement on the threshold values of duration and intensity chen et al 2015 faye et al 2021 it is widely recognized that heat waves are one of the deadliest climate hazards around the world mazdiyasni et al 2017 for instance the 1995 chicago heat wave caused over 700 excess deaths in one week whitman et al 1997 the 2003 heat wave in europe was responsible for over 70 000 excess deaths robine et al 2008 and in the past three decades the number of deaths caused by heat waves has quadrupled in china cai et al 2021 rising global temperatures are likely to further increase the intensity frequency and duration of heat waves around the world ipcc et al 2021 even with strict and effective greenhouse gas ghgs mitigation policies the increasing trend in global temperatures is likely to continue for at least several decades peters et al 2013 without precautionary and well designed adaptation plans humans will suffer more from heat waves in the future 1 2 urban heat island effects amplify health impacts the urban heat island uhi effect makes cities hotter than their rural surroundings oke 1982 li et al 2019 the synergy between heat waves and the uhi can further amplify heat health risks to city residents cao et al 2021 he et al 2021 recent studies in measuring intra urban temperatures have shown that the uhi of a city however is neither uniform nor constant scott et al 2016 shandas et al 2019 it is a time varying urban heat archipelago uha ziter et al 2019 in which thermal contrasts vary with neighborhoods and weather types scott et al 2016 and the highest heat conditions are often found in poor minority neighborhoods with histories of discriminatory housing policies popovich and flavelle 2019 hoffman et al 2020 wilson 2020 as a result heat adaptation in cities should aim not just to mitigate climate risks and lessen health impacts but also to restore social equity which historically has been undermined fortunately several u s cities emphasize the importance of heat adaptation in their hazard mitigation plans or other official documents ccap 2008 los angeles 2018 los angeles and s green 2019 baltimore 2019 philadelphia 2015 philadelphia 2019 phoenix 2019 nycem 2019 an important feature of these plans is their emphasis on multiple policy objectives including protecting human life and infrastructure enhancing social equity promoting the local economy maximizing cost effectiveness increasing adaptive capacity to future conditions strengthening community resilience and valuing other co benefits however systematic approaches to designing and evaluating the effectiveness equity and other social impacts of alternative heat adaptation plans are lacking keith et al 2019 1 3 actions for mitigating heat wave impacts this paper addresses the optimal selection of actions that cities can take to lessen the temperature and human impacts of heat waves there are two main categories of actions for mitigating heat wave impacts that are recommended by the usepa n d studied in the literature and considered by cities 1 temperature reduction and 2 exposure reduction cities must choose a mix of such actions accounting for many objectives kasprzyk et al 2013 as well as deep uncertainties marchau et al 2019 about future climate and the effectiveness of possible actions temperature reduction can be achieved by afforestation increasing vegetated areas and covering impervious surfaces such as roofs and roads with light colored materials loughner et al 2012 li et al 2014 santamouris et al 2017 ziter et al 2019 actions using trees or vegetation hereafter referred to as green actions reduce temperature through evapotranspiration higher albedo and shade that reduce direct sunlight and incoming heat li et al 2019 light colored surfaces have higher albedo and store less heat than dark surfaces li et al 2014 santamouris et al 2017 which can reduce the uhi and uha effects green actions are generally more effective in cooling cities than light colored surfaces and can yield co benefits that are valuable for cities such as air purification stormwater runoff control and amenity values santamouris et al 2017 however the costs of green actions are often considerably higher which may be a financial burden for cities it also takes time for some green actions to achieve their maximum benefits e g trees need time to grow to full canopy the costs and benefits of investing in green actions thus need to be carefully weighed investing in temperature reduction actions is important for promoting environmental equity for low income neighborhoods where temperatures are often higher than in other neighborhoods due to discriminatory housing policies huang et al 2011 hoffman et al 2020 wilson 2020 the second category is exposure reduction which reduces or eliminates people s exposure to heat waves common such actions are cooling centers and financial support for low income populations to buy air conditioners acs for example new york city opened over 500 cooling centers in 2018 and offered assistance for purchasing acs through its home energy assistance program nycem 2019 operating cooling centers requires less investment than temperature reduction actions especially when cities can use public facilities e g public libraries and recreation centers as cooling shelters however it is difficult for some vulnerable people e g the elderly and people with disabilities to access cooling centers and others may be reluctant to classify themselves as vulnerable and go to such shelters therefore utilization rate of cooling centers can be low despite the need for them 1 4 previous studies on urban heat adaptation and remaining gaps while numerous studies have analyzed alternative urban heat adaptation strategies keith et al 2019 recently concluded that few studies delve into extreme heat planning and governance processes they call for the creation of integrated strategies that combine extreme heat risk management e g cooling centers and design of the built environment e g afforestation and also point out the need for new multidisciplinary tools and expanded databases to support urban heat adaptation furthermore even fewer studies have used formal numerical optimization to choose from alternative strategies while explicitly considering public health and other human impact indicators one strand of papers uses numerical methods to identify optimal urban heat adaptation strategies but those studies do not explicitly include social impact indicators such as public health and social equity in their objectives for example optimization has been used to inform green space or land use planning with the objective of reducing urban rural temperature differences zhang et al 2017 other studies address optimization of siting of cooling centers for instance bradford et al 2015 solve for optimal center locations in pittsburgh pa however rather than minimizing direct health impacts of heat their objective is to site cooling centers to maximize heat risk population within a specified distance of the centers where individuals are weighted by a heat vulnerability index another group of papers simulates how alternative heat mitigation strategies reduce heat health impacts however those studies do not use numerical optimization to search for the best adaptation strategies from a wide range of alternatives instead they pre define a small set of strategies and then evaluate those strategies using urban scale meteorological models and health response models for example stone et al 2014 use the weather research forecasting wrf and benmap models to evaluate seven heat reduction strategies with different combinations of increased tree and vegetation covers green and cool roofs and cool pavements they find that 40 99 of projected 2050 heat related mortality could be avoided in three u s metropolitan areas vargo et al 2016 study the same three areas but they further investigate the health benefits for subpopulations of different ages races and income levels they conclude that vulnerable populations benefit most from heat adaptation so plans should prioritize actions in regions where those populations live as a final example boumans et al 2014 develop a model to assess the effect of various levels of urban vegetation on reducing heat related mortality in austin tx but they only find a modest reduction in heat related deaths these studies have greatly improved our understanding of the possible costs and benefits of efforts to mitigate heat impacts however capitalizing on these findings to develop a useful and insightful decision support tool for urban heat adaptation requires several additional advancements these include learning how to 1 fully address multiple and often conflicting adaptation objectives such as health cost and especially inequities in heat health impacts hoffman et al 2020 wilson 2020 2 quantify and account for long term costs and benefits of heat adaptation actions over a time horizon rather than in a single future year 3 take advantage of synergies in temperature reduction and exposure reduction actions considering the full range of possible measures and 4 plan considering uncertainties about the rate and nature of climate change changes of social factors and the cost and performance of possible actions recognizing that plans can be adapted in response to observations to ensure the desired goals can be met 1 5 city heat an innovative decision support tool for urban heat adaptation in this paper we propose the city heat equity adaptation tool city heat to support urban heat adaptation planning unlike other planning tools in the literature city heat can optimize multiple adaptation objectives consider numerous future scenarios and adaptation actions and generate adaptive plans at fine temporal annual and spatial sub city resolutions in addition to the mathematical model described in this paper we also provide open source code https github com srayum city heat that other researchers or city planners can use to apply city heat to urban heat adaptation problems elsewhere we now describe the contribution of this tool and its application to the literature by describing the following five general characteristics which address the five issues that are presently unresolved in the literature section 1 4 respectively 1 targeting of multiple planning objectives because urban heat adaptation is inherently a multi objective planning problem plans that only focus on one goal may overlook inexpensive plan modifications with large benefits for other objectives thereby leading to inefficient and ineffective decisions previous studies have not used multiple objectives measuring a range of social impacts to optimize heat adaptation notably inequity exacerbated by heat is a great concern of cities and consequently an equity indicator should be included as an objective fortunately in recent years increased computing power has allowed researchers to study and solve complex multi objective problems with many objectives hadka and reed 2013 kasprzyk et al 2013 trindade et al 2017 including so called many objective problems which consider more than four objectives many objective optimization yields a set of approximately pareto optimal or non dominated solutions coello et al 2007 that stakeholders can choose from in a set of such solutions no solution equals or outperforms any other solution on all objectives but every solution is strictly better in at least one objective when compared pairwise with every other solution in the set the term pareto approximate is used to refer to the best known set of non dominated solutions acknowledging the true pareto optimal set may have solutions that pareto dominate these but they have not been discovered this paper s version of city heat includes five objectives but users can modify or add new objectives based on their preferences and city heat will generate a set of pareto approximate urban heat adaptation plans according to those objectives 2 evaluate costs and benefits over long time horizons and by city subareas the costs and benefits of urban heat adaptations are realized over the long term climate change and its variability mean the costs and benefits of heat adaptation can vary significantly over time for example the benefits of heat adaptation in reducing heat health impacts can be much larger in a hot year compared to a year with less stressful conditions in addition some actions e g urban afforestation not only have long life times but also have long lead times this delay in realizing the benefits means that investments have to be made well in advance of the need and so may be politically more difficult to implement and benefits may also be more uncertain than for projects with short lead times choosing a single time point e g a year for analysis lacks a comprehensive perspective on long term costs benefits and uncertainties the result may be pareto inefficient planning decisions and a failure to appreciate how investments and benefits will vary over time furthermore environmental conditions and vulnerable populations vary over space shandas et al 2019 wilson 2020 so that costs and benefits of heat adaptation are likely to be unevenly distributed across a city s subareas by distinguishing vulnerabilities and opportunities among subareas city heat allows adaptation actions to be tailored to each subarea with the resulting impacts aggregated to the city level to compute the plan s overall performance on objectives by considering costs and benefits over time and space city heat provides a more comprehensive comparison of alternative adaptation plans 3 optimize adaptation actions over type location time and magnitude in order to identify the optimal heat adaptation plans the type magnitude location and timing of actions need to be decided rendering this decision problem high dimensional and complex previous studies evaluated the effects of a few pre defined adaptation plans at a given time point likely overlooking superior combinations of actions over longer time horizons by contrast city heat can identify and recommend heat adaptation plans specifying what actions to take at finer temporal annual and spatial subarea resolutions according to the objectives defined and desired by city planners 4 address multiple types of uncertainties and adapt plans in response to information uncertainties especially deep uncertainties are a major challenge to making long term decisions lempert et al 2003 marchau et al 2019 the performance of alternative plans depends on how the future unfolds which cannot be perfectly predicted when decisions are made lempert et al 2003 climate change political and economic developments and population growth are typical examples of deep uncertainties which are difficult to characterize by probability distributions milly et al 2008 trindade et al 2017 these uncertainties are important to assessing urban heat adaptation plans peng et al 2010 cao et al 2021 in a scenario with fast global warming for instance a plan that implements massive urban afforestation might be preferred because it works to avoid many possible heat related deaths however in a scenario with slow global warming the same plan might not be attractive as it requires a significant amount of investment while perhaps not saving many lives in addition previous studies defined heat wave in different ways in part because of uncertainty about the role of peak temperatures night time temperatures and heat wave durations on health effects chen et al 2015 faye et al 2021 therefore it is desirable to develop a tool that can evaluate alternative plans under uncertainty by considering a large ensemble of possible future scenarios city heat can identify pareto optimal adaptation plans and evaluate plan robustness across a wide range of future scenarios in particular city heat enhances the robustness of plans by explicitly considering the flexibility of plans to adapt to changing conditions surprises and learning lempert et al 2003 hung and hobbs 2018 marchau et al 2019 the ability to track system changes and flexibly adjust decisions can reduce the risks of insufficiency and redundancy in decisions holling 1978 de neufville and scholtes 2011 to enable adaptation city heat employs the direct policy search dps method giuliani et al 2016 quinn et al 2017a which uses key system information that has been observed prior to each decision stage e g year to guide that stage s decisions this makes decisions at each stage contingent on information obtained thus far and decisions made at previous time steps dps functions have been employed to model adaptive decisions for many complex dynamic environmental management problems under uncertainty including multi purpose reservoir management giuliani et al 2016 quinn et al 2017b lake pollution control quinn et al 2017a coastal dike heightening garner and keller 2018 fishery harvesting hadjimichael et al 2020 microgrid management gupta et al 2020 and carbon mitigation problems marangoni et al 2021 the above features of city heat address challenges of urban heat adaptation problems that are not fully studied or combined in the current literature city heat is a generalizable tool that can support urban heat adaptation planning through optimizing decisions based on multiple planning objectives considering various future scenarios and generating candidate adaptation plans at fine temporal annual and spatial sub city resolutions to illustrate how city heat works we apply this tool to a hypothetical case study for baltimore md where heat risks to health are a major concern anderson and bell 2010 and where historical redlining has contributed to great inequity in heat health risks plumer et al 2020 wilson 2020 the rest of this paper is structured as follows section 2 introduces the mathematical formulation of city heat the algorithm used to solve the resulting optimization problem and the metrics that characterize the robustness of generated plans then section 3 describes the setup and data sources of the baltimore case study section 4 follows with results of the case study along with discussions of those results section 5 concludes the paper by summarizing the contributions of city heat and potential enhancements of the method 2 method city heat consists of two parts 1 a simulation model that can evaluate the effects and costs of alternative heat adaptation plans and 2 an optimization model that can search for pareto optimal plans based on multiple objectives in this section we first describe the problem setting in city heat section 2 1 then we present equations used in the heat impact and cost simulations section 2 1 1 and optimization section 2 1 2 parts of city heat these are presented as general statements and the particular implementation for the baltimore case study is discussed in section 3 next we introduce the algorithm borg hadka and reed 2013 that we adopt to solve the many objective optimization problem section 2 2 in city heat finally we define and present the metrics to evaluate the robustness of the generated plans section 2 3 a schematic diagram that includes key components of city heat is shown in fig 1 2 1 problem setting in city heat city heat is a simulation based optimization model that solves for non dominated or pareto approximate hadka and reed 2013 urban heat adaptation plans according to multiple objectives under uncertainty in city heat temperature reduction i 1 and exposure reduction i 2 actions are both considered typical temperature reduction actions i 1 include urban afforestation increased vegetation covers green cool roofs and cool pavement those actions are further divided into two subcategories actions in the first set take time to achieve their maximum effect i 1 1 e g urban afforestation while actions in the second set yield immediate benefits i 1 2 e g cool roofs pavements distinguishing actions in i 1 1 and i 1 2 is important because this reflects a tradeoff in choosing between relatively inexpensive actions i 1 2 with lower temperature reduction potential but immediate benefits as opposed to actions i 1 1 that might have higher temperature reduction potential in the long run but at the expense of greater upfront investment and delays in realized benefits on the other hand common exposure reduction actions i 2 include cooling centers and free ac programs which provide cooling to people in need by avoiding exposure to extreme heat those actions are expected to reduce mortality during heat wave events but will not affect the frequency and intensity of heat waves in general city heat allows the user to specify ahead of time which of the considered actions are allowed to be taken in each of r sub city regions and in which of the t years for instance in the application in section 3 we allow actions in i 1 to be implemented anywhere at any time but restrict actions in i 2 to the first period this setting is also to demonstrate that city heat can model different types of decisions in practice how a city is divided into sub city regions depends on the purpose of an application as well as on available data and computational resources for example sub city regions could be aligned with jurisdiction classifications such as planning districts or census tracts or they could instead be based on physical and demographic characteristics e g temperature or population vulnerability city heat addresses uncertainties by considering various plausible future scenarios here the scenarios are combinations of different realizations of uncertain parameters that describe future climate demographics and action effectiveness realizations are drawn using latin hypercubic sampling lhs iman et al 1980 although other scenario definition methods could be applied the case study in section 3 describes the scenario definition process in more detail including which uncertain variables are considered in the model presented below the parameters with a subscript s are uncertain and scenario dependent 2 1 1 city heat simulation model four modules make up the simulation part of city heat 1 temperature modification 2 demographic projections 3 mortality estimation and 4 cost calculation 1 temperature modification the temperature modification module calculates the temperature reductions and resulting decrease in heat wave days caused by various adaptation actions in each sub city region and year this is done in three steps first the initial step calculates for a given year the incremental investment in each temperature reduction action based on the dps functions system state variables and a set of dps parameters that are to be optimized eq 1 the effective cumulative changes of those actions are then calculated based on the annual changes eq 2 the second step estimates the resulting temperature reduction in each year and sub city region using the cumulative changes of actions and their temperature reduction functions eq 3 in the third and final step the number of heat wave days with temperature reduction actions eq 4 and the number of baseline heat wave days assuming no action is taken eq 5 are calculated based on a heat wave count function for each case the details of these steps are described below eq 1 shows the dps function in the first step which depicts the fractional increase of temperature reduction action i in sub city region r and year t under scenario s this function considers the system state variables θ r t 1 s through year t 1 and a set of dps policy parameters μ i r to be optimized details in section 2 1 2 below eq 1 x i r t s d p s θ r t 1 s μ i r i i 1 x i r t s is the output of a dps function and represents the fractional increase of temperature reduction action i in sub city region r and year t under scenario s the dps function uses a vector it can also be a scalar of system state variables θ r t 1 s as inputs making the decisions in year t dependent on the key system state variables and their values observed until year t 1 which is stochastic and dynamic because it considers multiple possibilities scenarios and years users can select which state variables they would like to have considered when deciding on investments for instance the average annual heat wave days of the previous five years or the observed temperature effects of actions can both be used as system state variables to inform decisions these state variables are scenario dependent and change with time the actions taken in previous time steps can also affect the system states later for example if massive urban afforestation is implemented in year 1 heat wave days an example of θ r t s are likely to be reduced afterward compared to no action being taken the set of policy parameters μ i r is customized for each action i and sub city region r but is independent of scenarios and years this means that the policy function controlled by μ i r stays the same under different scenarios and over time but the resulting decisions x i r t s will be scenario and time dependent inasmuch as the inputs θ r t 1 s change with scenarios and time under this design the policy functions could lead to adaptive strategies that unfold over time differently in different scenarios using such policy functions can make decisions flexible adjusting them dynamically for each scenario for example an ideal policy function can lead to aggressive urban afforestation or implementation of light colored materials to reduce health impacts under a scenario with many heat wave days but the same policy function might result in slight or no changes under a scenario with limited heat waves to avoid a waste of resources by optimizing the policy parameters μ i r city heat can find the pareto optimal policy functions across a wide range of scenarios eq 2 calculates the effective cumulative changes e g amount of tree canopy or cool pavements roofs from temperature reduction actions taken in previous years eq 2 x i r t s g x i r 1 s x i r 2 s x i r t s i i 1 the effective cumulative change of a temperature reduction action i 1 represents the changes that are in effect to reduce the temperature for the actions yielding immediate benefits i 1 2 the effective cumulative changes are the sum of the changes taken in previous years such that g x i r 1 s x i r 2 s x i r t s t 1 t x r i t s i i 1 2 while for actions that take time to achieve their maximum effects e g trees need a predetermined number of years t g r o w to mature the effective cumulative changes will be less than the sum of actions in previous years until t g r o w is reached if we assume a linear increase of x and t g r o w years for x to reach its maximum effect then g x i r 1 s x i r 2 s x i r t s t 0 t min t t t g r o w 1 x r i t s i i 1 1 to illustrate if it takes 20 years for a tree to reach its full canopy and just 10 of the potential tree capacity is planted in year 1 then the planted trees will achieve a total canopy coverage of 5 of the potential coverage by year 10 min 10 0 20 1 10 5 and reach 10 by year 20 min 20 0 20 1 10 10 different growth functions could be used for different actions but in the case study we use the linear function for its simplicity eq 3 shows how temperature reduction in sub city region r and year t is calculated the temperature reduction is determined by the effective cumulative changes of each action and their temperature reduction effect the integrated effect of all actions is assumed to be a linear summation of individual actions in addition to considering actions taken in one sub city region to calculate its temperature reduction we also account for the potential spillover effect of actions taken in adjacent sub city regions eq 3 t r t s m i t i i f i x i r t s r a r i β i r r s f i x i r t s a r is the set that contains all sub city regions that are geographically adjacent to the sub city region r the spillover effect is discounted by a factor β i r r s 1 which is scenario dependent representing the uncertainty in the geographical range over which the temperature reduction actions are effective β i r r s is a function of regions r and r and can depend on several factors such as two regions areas and distances apart also β i r r s can vary with action i here for simplicity we assume β i r r s is the same for all actions and is adjusted by the ratio of areas of two adjacent sub city regions r and r with no influence of their distance apart yielding β i r r s β s a r e a r a r e a r i the temperature reduction function f i can be obtained from existing observations or simulation studies loughner et al 2012 li et al 2014 santamouris et al 2017 ziter et al 2019 eqs 4 and 5 calculate the number of heat wave days hwds in sub city region r and year t with adaptation eq 4 and without adaptation eq 5 eq 4 h w d r t s h s t 0 t s δ t r t s t r t s m i t i eq 5 h w d r t s 0 h s t 0 t s δ t r t s h s is a heat wave count function that calculates the number of heat wave days from a daily temperature series e g a 153 day daily temperature series from may 1 sept 30 for each sub city region r and future year t under scenario s the future daily temperature series with temperature reduction actions is assumed to be a linear combination of three parts first t 0 t s is a daily temperature projection for the whole city that is retrieved from climate models in a given year t t 0 t s is scenario dependent because different climate models can be used second δ t r t s is the temperature deviation of sub city region r compared to the city wide mean temperature third and finally t r t s m i t i is the temperature reduction because of adaptation this item is only used in eq 4 while eq 5 serves as a baseline without considering temperature reduction from adaptation actions here t r t s m i t i is determined by decisions eq 3 while t 0 t s and δ t r t s are input parameters since heat wave definitions play a fundamental role in the calculation of heat wave days and their estimated health impacts chen et al 2015 guo et al 2017 faye et al 2021 we adopt multiple definitions that have been widely used in the literature to calculate heat wave days this represents the scientific uncertainty in defining heat waves making h s scenario dependent in each scenario the same h s is applied to all sub city regions but different numbers of heat wave days might be obtained for each sub city region because of the differences in δ t r t s and t r t s m i t i when considering temperature reduction actions h s counts the number of heat waves from t 0 t s δ t r t s and t r t s m i t i based on predefined criteria that can be a highly complex function of our decision variables see examples in section 3 t 0 t s represents an average city wide daily temperature projection in year t and scenario s under a base case set of assumptions concerning land use and cover it can be obtained from climate models such as downscaled general circulation models at the grid cell containing the city of interest peng et al 2010 t 0 t s is scenario dependent because various climate models provide different temperature projections for future periods these models represent the current understanding of climate change and the use of multiple models captures uncertainty about future climate conditions incorporating information from multiple climate models in the urban heat adaptation problem allows us to identify pareto optimal adaptation plans under future climate conditions and to address climate change uncertainty it is also useful to hold out some climate models and scenarios from the optimization to evaluate the robustness of the optimized heat adaptation plans to possible futures on which they were not trained this out of sample evaluation is useful because in practice the future will not look exactly like any of the projections so it is important to find plans that perform robustly under unmodeled but possible future conditions because most climate models do not have a spatial resolution equivalent to the sub city level we treat t 0 t s as a city wide mean temperature projection under base conditions sub city level temperature projections are developed by adjusting t 0 t s by δ t r t s the estimated deviation of sub city region r s temperature from the city wide average a similar method to obtain sub city level temperature projections is adopted by boumans et al 2014 however δ t r t s can vary with several factors such as weather types scott et al 2016 and thus it is difficult to characterize δ t r t s under different background temperature conditions here we simplify δ t r t s to δ t r s t which is the temperature deviation across sub city regions under the high temperature condition of interest the sub city temperature deviations can be estimated from monitoring data for intra urban temperatures scott et al 2016 shandas et al 2019 shi et al 2021 therefore we use t 0 t s δ t r as an approximate daily temperature projection for sub city region r in year t t r t s m i t i is the temperature reduction that results from a chosen set of investments in sub city region r and year t under scenario s relative to a baseline in which no action is taken i e t r t s m i t i is used in eq 4 to calculate h w d r t s but not in eq 5 to calculate h w d r t s 0 because of the temperature reduction effect h w d r t s is no greater than the baseline h w d r t s 0 and the difference between h w d r t s 0 and h w d r t s is the decrease in the number of heat wave days attributed to t r t s m i t i the temperature reduction relative to the base case t r t s m i t i can be affected by many factors such as the surrounding environment temperature and the extent of adaptation in the case study of this paper we assume for simplicity a linear or quadratic function for temperature reductions based on the extent of adaptation efforts as have been observed in the literature li et al 2014 ziter et al 2019 however users can use more elaborate temperature reduction functions if they wish although we assume the modified temperature projection is a linear function of t r t s m i t i eq 4 the number of heat wave days may be non linear with t r t s m i t i given the possible non linearity of h s 2 demographic projections the demographic module projects the demographic structure for different population groups in future years here we classify the population by two factors that are widely recognized to have significant impacts on heat health risks age below or above 65 years old and income below or above the poverty line basu and samet 2002 madrigano et al 2015 tong et al 2021 the two classes for each of these two factors result in four population groups with different relative risks during heat waves other factors could be included to further stratify the population but a linear increase in the number of classifying factors and their classes will exponentially increase the number of population groups making the calculation more complicated here we only include the two most important factors for simplicity and demonstration users can include additional factors if desired the projection of each population group is done through three steps first the population size in sub city region r and year t under scenario s is estimated using a projection function p r o j p o p eq 6 for example for a simple exponential growth model and an average annual population growth rate a p g r s the population size in sub city region r and year t p o p r t s is p o p r t 0 1 a p g r s t t 0 a p g r s is scenario dependent and represents alternative population change scenarios where positive values of a p g r s represent an expanding population while negative values represent a shrinking population more sophisticated population growth assumptions and models kii 2021 can be adopted by the users if desired eq 6 p o p r t s p r o j p o p p o p r t 0 a p g r t s t second eqs 7 10 project the relative size of each of the four age income population groups the fractions of the population above 65 years of age or below the poverty line are projected using projection functions p r o j a g e in eq 7 and p r o j i n c o m e in eq 9 respectively if a linear change function and an average annual rate of aging a a r r s are assumed then eq 7 will be simplified as p a g e 65 r t s min 1 p a g e 65 r t 0 a a r r s t t 0 bounded by 1 because it is a fraction similarly eq 9 will be simplified as p i n c o m e b e l o w p o v e r t y l i n e r t s min 1 p i n c o m e b e l o w p o v e r t y l i n e r t 0 a p r r s t t 0 with the assumed annual rate of poverty a p r r s then the fractions of the population younger than 65 years of age or with an income above the poverty line can be calculated as 1 p a g e 65 r t s and 1 p i n c o m e b e l o w p o v e r t y l i n e r t s as shown in eqs 8 and 10 respectively in the case study later we will use the linear functions for simplicity and demonstration eq 7 p a g e 65 r t s p r o j a g e p a g e 65 r t 0 s a a r r s t eq 8 p a g e 65 r t s 1 p a g e 65 r t s eq 9 p i n c o m e b e l o w p o v e r t y l i n e r t s p r o j i n c o m e p i n c o m e b e l o w p o v e r t y l i n e r t 0 s a p r r s t eq 10 p i n c o m e a b o v e p o v e r t y l i n e r t s 1 p i n c o m e b e l o w p o v e r t y l i n e r t s in the final steps the population size of each of the four age income groups is calculated by using the overall population size multiplied by the proportion of the population in each age and income group assuming here for simplicity that the two factors of age and income are independent that is we assume p a g e i n c o m e r t s p a g e r t s p i n c o m e r t s eq 11 where better data is available alternative assumptions can be made this approximation is also adopted by bradford et al 2015 eq 11 p o p i n c o m e a g e r t s p o p r t s p a g e i n c o m e r t s a g e a b o v e o r b e l o w 65 i n c o m e b e l o w o r a b o v e p o v e r t y l i n e 3 mortality estimation the mortality estimation module estimates the expected mortality on a single heat wave day and the resulting annual heat mortality in each sub city region and year under a given scenario for each population group the population exposed to heat waves and their group specific heat wave mortality risks are used to calculate the expected heat mortality the exposure reduction actions i 2 can protect some populations from heat waves and in this study we only consider a single such action the provision of cooling centers users of cooling centers are assumed to have no heat wave mortality risk berisha et al 2017 and the number of users will be subtracted from each population group exposed to heat wave mortality risks this estimation process is conducted in two steps initially we estimate the number of users of cooling centers in each sub city region r and year t eqs 12 and 13 the number of users of the subpopulations u s e r a g e i n c o m e r t s is calculated based on the number of available cooling centers c c r t the capacity of each center c c c a p r t s the utilization rate c c u s e r t s and the percentage of users of each age and income group p u s e r a g e i n c o m e r t s c c r t is the decision variable representing the number of cooling centers to open in sub city region r and year t the number and locations of cooling centers can stay the same for years e g baltimore code red so we assume c c r t is decided at the beginning of the study horizon and remains the same over time so c c r t c c r t and also the capacity c c c a p r t s is fixed and same so c c c a p r t s c c c a p r t s these assumptions illustrate how city heat can handle different types of decisions differently c c r t in just the first period versus x i r t s in every period but future work could explore the possibility of opening or closing more centers following a state dependent rule the uncertainties in the utilization rate and users c c u s e r t s and the age and income distribution of users population groups p u s e r a g e i n c o m e r t s contribute to uncertainty concerning the number of cooling center users this translates into an uncertain effectiveness level of cooling centers to avoid a large number of parameters in the case study here we assume c c u s e r t s and p u s e r a g e i n c o m e r t s are identical across sub city region and time and therefore c c u s e r t s c c u s e s and p u s e r a g e i n c o m e r t s p u s e r a g e i n c o m e s r t also we assume p u s e r a g e i n c o m e s p u s e r i n c o m e s p u s e r a g e s for the same reason as p a g e i n c o m e r t s eq 11 as described above of course more general assumptions are possible the total number of cooling center users is calculated in eq 13 which is used later to calculate the operating costs of cooling centers eq 12 u s e r a g e i n c o m e r t s min p o p a g e i n c o m e r t s c c r t s c c c a p r t s c c u s e r t s p u s e r a g e i n c o m e r t s eq 13 u s e r r t s a g e i n c o m e u s e r a g e i n c o m e r t s second we calculate the heat wave mortality relative risk of each age income group h w m r a g e i n c o m e s using the average mortality rate m r a g e i n c o m e and multiplied by relative heat wave risk factor for each age income group h w r r a g e i n c o m e s eq 14 to simplify the case study we assume h w r r a g e i n c o m e s h w r r a g e s h w r r i n c o m e s although more sophisticated assumptions reflecting local demographic conditions are possible and desirable in real applications then the expected daily heat wave mortality with cooling centers d e m r t s and without cooling centers d e m r t s 0 are calculated in eqs 15 and 17 respectively in eq 15 the number of cooling center users is subtracted from each population group and so the d e m r t s is no greater than d e m r t s 0 the difference between d e m r t s and d e m r t s 0 is the mortality prevention effect on a heat wave day to be attributed to cooling centers with h w d r t s and d e m r t s eqs 4 and 15 as well as h w d r t s 0 and d e m r t s 0 eqs 5 and 17 the expected heat mortality with and without adaptation in year t and region r under scenario s e m r t s and e m r t s 0 are calculated in eqs 16 and 18 respectively the difference between e m r t s and e m r t s 0 is the mortality reduction contributed by both temperature and exposure reduction actions this is a key indicator that is later used in the objective functions to optimize adaptation decisions eq 14 h w m r a g e i n c o m e s m r a g e i n c o m e h w r r a g e i n c o m e s eq 15 d e m r t s a g e i n c o m e h w m r a g e i n c o m e s p o p a g e i n c o m e r t s u s e r a g e i n c o m e r t s eq 16 e m r t s h w d r t s d e m r t s eq 17 d e m r t s 0 a g e i n c o m e h w m r a g e i n c o m e s p o p a g e i n c o m e r t s eq 18 e m r t s 0 h w d r t s 0 d e m r t s 0 in the current study we only include acute mortality reduction as a health impact of urban heat adaptation actions even though those actions are also useful in mitigating heat related morbidity such as longer term chronic impacts of repeated heat stress those additional health benefits could be considered in future studies to more comprehensively characterize the health benefits of adaptation 4 cost calculation the cost calculation module calculates the costs of each heat adaptation plan in net present value npv δ t s is the real discount rate in yr 1 and is scenario dependent since it may take on different values to represent different borrowing costs and time preferences severens and milne 2004 here we assume δ t s δ s t to avoid a large number of parameters but time series of discount rates that change over time can be used in future work zarekarizi et al 2020 for temperature reduction actions costs include the capital cost of newly implemented actions at each time x i r t s and maintenance cost of cumulative changes x i r t s which are captured by fixed cost f c i and operation and maintenance o m cost o m i coefficients respectively eq 19 eq 19 t r c o s t r t s i 1 i 1 f c i x i r t s o m i x i r t s 1 δ s t the costs of cooling centers are also of two types the first is the upfront cost of each cooling center typically cooling centers are set up in public facilities such as libraries senior centers and religious buildings the number of cooling centers in each sub city region is assumed to be decided ahead of the planning horizon and the cooling centers can be open during each heat wave day upfront costs for cooling centers may not be necessary since cooling centers are mostly public facilities here we include an upfront cost eq 20 to reflect the efforts of city authorities to look for negotiate and contract to make a facility available as a cooling center the actual cost of each cooling center can be obtained from organizations in charge of cooling centers eq 20 u p f r o n t c o s t c c r t 0 s c c r c o s t c c the second type of cooling center cost is the operation cost eq 21 which depends on the number of cooling center users u s e r s r t s eq 13 heat wave days h w d r t s eq 4 and the unit cost per user day served o p e r c o s t c c eq 21 o p e r a t i o n cos t c c r t s o p e r c o s t c c u s e r s r t s h w d r t s 1 δ s t this represents the energy costs for cooling as well as expenditures on water and food that some cooling centers provide for the visitors berisha et al 2017 in summary the total cost of an adaptation plan is the sum of 1 capital and o m costs of temperature reduction actions i 1 t r c o s t r t s as well as 2 the upfront costs and operation costs of cooling centers i 2 in this plan 2 1 2 city heat optimization model the simulation part of city heat is designed to evaluate the mortality reduction and cost of a specified adaptation plan the optimization part is used to generate near pareto optimal coello et al 2007 adaptation plans based on several objectives that are calculated using variables from the simulation part five objectives are included in the current version of city heat and are defined as follows 1 objectives objective 1 mortality reduction maximize the city wide reduction in expected heat wave mortality eq 22 o 1 1 s s 1 s t 1 t r 1 r e m r t s 0 e m r t s o 1 represents the scenario averaged total mortality reduction effect over time and sub city regions this objective is used as an indicator of the overall public health benefits differences between e m r t s 0 and e m r t s measure the overall decrease in mortality that is anticipated from temperature and exposure reduction actions objective 2 cost minimize total net present value npv of all costs eq 23 o 2 1 s s 1 s t 1 t r 1 r o p e r a t i o n c o s t c c r t s t r c o s t r t s r 1 r u p f r o n t c o s t c c r t 0 s o 2 is the scenario averaged total cost of adaptation and is an indicator of the overall financial cost the capital and o m costs of temperature reduction actions t r c o s t r t s eq 19 and includes the operation cost of the cooling center o p e r a t i o n c o s t c c r t s eq 21 are added over years and sub city regions u p f r o n t c o s t c c r t 0 s eq 20 represents the upfront cost needed for cooling centers objective 3 equity minimize the largest disparity in mortality rate across sub city regions eq 24 o 3 1 s s 1 s max t m a x r e m r t s p o p r t s m i n r e m r t s p o p r t s o 3 the scenario averaged maximum difference in heat mortality rate over time is selected as an equity indicator a smaller o 3 is associated with a more equitable plan the term e m r t s p o p r t s is the heat wave mortality rate fraction of population the difference between the maximum and minimum mortality rate over sub city regions is one way to represent the spatial inequity of heat health risks within a city wilson 2020 in that inequity is a sensitive social issue alternatives whose worst largest in this case mortality difference among sub city regions is small may be preferred objective 4 reliability minimize worst year city wide heat wave mortality eq 25 o 4 1 s s 1 s max t r 1 r e m r t s o 4 reflects a desire to minimize the worst case outcome defining a less heat vulnerable adaptation plan as one that avoids any large single year mortality the maximum single year and city wide mortality max t r 1 r e m r t s over years and averaged across scenarios s is chosen as a vulnerability indicator a smaller o 4 indicates a less vulnerable heat adaptation plan objective 5 carbon reduction maximize cumulative co2 mitigation co benefit eq 26 o 5 1 s s 1 s i i c o b e n t 1 t r 1 r γ i x i r t s o 5 represents the scenario averaged co benefit that is yielded by adaptation actions in addition to health benefits some adaptation actions could have other co benefits that are valuable for city planners for example green actions provide co benefits such as co2 reduction stormwater runoff management and cooling and heating energy savings here one type of several possible categories of co benefits is considered in eq 26 where the co benefit is calculated using a coefficient γ i if for instance co2 reductions are considered then γ i is the coefficient tons yr of co2 reduction from a unit installation of action type i more complex formulations that quantify different types of co benefits that also stem from changes in heat wave frequency are also possible multiple types of co benefits can be added to the model based on the users needs in the current model we include the objectives described above eqs 22 26 representing five performance indicators effectiveness cost equity vulnerability and co benefits to compare and optimize alternative adaptation plans users of city heat can modify or add objectives based on the application and priorities of planners and stakeholders in addition we aggregate outcomes across scenarios by simply averaging however other scenario aggregation operators can be chosen based on the users risk attitudes for example if the users are risk averse to the equity objective eq 24 they could use a min max criterion minimizing the scenario maximum disparity in mortality rates among sub city regions instead of minimizing a scenario averaged disparity 2 optimization after defining the objectives in city heat this section introduces the formulation of the optimization problem city heat uses many objective optimization hadka and reed 2013 to search for a set of heat adaptation plans l s that optimize the five objectives described in eqs 22 26 eq 27 for a many objective optimization problem the optimal solutions l s is a set of non dominated solutions i e there is no solution in the set that is equal or better than another in all objectives which is called the pareto set coello et al 2007 in city heat a solution l represents a pareto optimal urban heat adaptation plan and differences in objective values among those plans represent tradeoffs that users will want to consider in heat adaptation planning eq 27 m i n i m i z e f l o 1 o 2 o 3 o 4 o 5 eq 28 l l 1 l 2 l r eq 29 l r μ 1 r μ 2 r μ i r c c r r 1 2 r eq 30 x r i t s min d p s θ r t 1 s μ i r max a n n u a l c h a n g e i r t eq 31 μ i r l μ i r μ i r u c c r l c c r c c r u c c r i s i n t e g e r the decision variables form a vector l which contains the decision variables l r for each sub city region r eq 28 l r is a vector that contains the parameters μ i r of dps functions for action i in the sub city region r as well as the number of cooling centers to open c c r eq 29 the actual change of action i in year t sub city region r and scenario s is the output of a dps function constrained by an upper bound eq 30 which is determined by the policy parameters μ i r and the system state variables observed until year t 1 θ r t 1 s in that θ r t 1 s changes with time and scenarios the same policy function will lead to different actual decisions in different years and scenarios the annual changes are constrained by the maximum possible annual changes max a n n u a l c h a n g e i r t eq 30 for example the annual changes cannot exceed the available land capacity e g available lands for trees or surface areas e g total roof areas for cool roofs to install those changes in which case m a x a n n u a l c h a n g e i r t l u c a p i r s t 1 t 1 x r i t s where l u c a p i r s is the available land capacity to implement action i in sub city region r and t 1 t 1 x r i t s is the changes that have been taken during previous years their difference is the remaining available capacity for action i at year t the policy parameters μ i r are constrained by the corresponding lower and upper bound μ i r l and μ i r u respectively eq 31 the number of cooling centers c c r are integers and they are bounded by the corresponding lower and upper bounds c c r l and c c r u respectively eq 31 a minimum number of cooling centers might for instance be required to open in each sub city region and the maximum number of cooling centers might depend on available facilities to apply the dps method it is important to decide the family of policy functions to use artificial neural networks anns and radial basis functions rbfs are widely used in modeling adaptive decisions in reservoir operation giuliani et al 2016 2017 quinn et al 2017b lake pollution control quinn et al 2017a and microgrid management gupta et al 2020 garner and keller 2018 use quadratic functions to model dike heightening decisions in response to occurrences of uncertain sea level rise and storm surge giuliani et al 2016 find that functions with more parameters such as anns tend to be more flexible but could lead to problems such as overfitting and increased computational burden they conclude that there is no universal recommendation for which policy functions to apply to a specific problem although anns and rbfs functions are highly flexible so that they can represent a wide range of different policy rules those functions might sometimes be difficult to communicate therefore simple functions such as the promotional integral derivative pid function lempert and turner 2021 could be adopted for better interpretability in the setting of city heat the investment decisions are in the form of fractional increment of trees planted on available lands as well as pavements and roofs converted to cool pavements and roofs these factions are expected to monotonically increase with the number of heat waves and are bounded between 0 and 1 with this expectation we choose a three parameter logistic function whose parsimony reduces the number of decision variables speeds up convergence and leads to more intuitive and understandable policies compared to more complicated functions e g anns the three parameter logistic function is shown as in eq 32 eq 32 d p s θ r t 1 s μ i r a i r b i r d i r max 1 d i r 1 e a i r θ r t 1 s b i r d i r 0 l u c a p where max 1 d i r 1 e a i r θ r t 1 s b i r d i r 0 gives a fraction between 0 1 representing a fractional change of the total available land use l u c a p i r s the shape of max 1 d i r 1 e a i r θ r t 1 s b i r d i r 0 is controlled by a i r b i r d i r which are optimized to get the pareto optimal policy function for each temperature reduction action in each sub city region we use the average heat wave days that occurred in the previous five years a scalar rather than a vector here as the system state variable in this study so that θ r t 1 s 1 5 t 5 t 1 h w d r t s because h w d r t s h s t 0 t s δ t r t r t s m i t i the actual decision x i r t s depends on θ r t 1 s h w d r t s and t 0 t s thus decisions are dynamically adjusted to different climate scenarios represented by t 0 t s the logistic function is non decreasing which means that more temperature reduction actions are implemented when more heat wave days are observed fig 2 provides several examples of the shape of a three parameter logistic function from fig 2 a c we can see that a i r controls the steepness of the logistic curve representing the sensitivity of change to system information on θ r t 1 s larger values of a i r represent increased sensitivity and steeper curves fig 2 c here we impose a boundary 0 a i r 10 to limit the decision space while spanning a low to high sensitivity level next b i r is the inflection point of the logistic curve representing the value of θ r t 1 s in our model the average heat wave days during the previous 5 years where the rate of increase in change is largest fig 2 a c here we constrain 0 b i r 200 to allow for policies whose largest rate is achieved when the average heat wave days are anywhere between 0 and 200 days this is longer than the period from may to september 153 days enabling policies that perhaps never implement adaptation actions moreover d i r is a parameter that shifts the curve so that it has a non zero intercept at 0 heat wave days this allows for immediate investment positive intercept if d i r 0 fig 2 g because the fraction of converted land use is bounded below by 0 through max 1 d i r 1 e a i r θ r t 1 s b i r d i r 0 if d i r 0 this simply results in delayed investment fig 2 i the range of 1 d i r 1 allows for a wide range of starting conditions of adaptation actions there are three main advantages of using the dps method for the urban heat adaptation planning problem first the dps method reduces the number of decision variables needed making the search for optimal decisions easier in the traditional intertemporal open loop setting in which action in each region and time is modeled as a decision variable decisions of i 1 types of temperature reduction actions in r regions through t years require optimizing i 1 r t decision variables in the dps closed loop setting it needs i 1 r p decision variables where p is the number of parameters in the dps function which is often much smaller than t in our case p 3 and t 20 quinn et al 2017a show that the optimization of a lake pollution control problem converges much faster using the dps closed loop form than its open loop counterpart second the closed loop dps scheme leverages the most up to date system information to guide future decisions this makes decisions more flexible and responsive to changing conditions and therefore likely to perform better de neufville and scholtes 2011 third the policy functions optimized within a time horizon t can be extended to future years which is not possible with the open loop setting 2 2 borg moea to solve many objective optimization problems after formulating the optimization problem we now introduce the algorithm that we use to solve the many objective optimization problem in city heat complex planning problems often contain many interacting subsystems that introduce non linearities and non separable dependencies reed et al 2013 as the number of objectives increases it becomes more challenging for multi objective evolutionary algorithms moeas to solve such constrained non linear problems with decision spaces that are high dimensional discrete non convex and stochastic coello et al 2007 reed et al 2013 in our problem for instance the reduction in the number of heat wave days caused by temperature reduction actions depends on many factors such as the choice of climate projections heat wave definitions and effectiveness of each action which results in non linear and complex relationships no closed form derivative information can be obtained and thus gradient based optimization algorithms do not work in this case instead we adopt a state of art moea borg which has been shown to find high quality tradeoff solutions for complex many objective and dynamical problems zeff et al 2016 giuliani et al 2017 garner and keller 2018 gupta et al 2020 such as mine zeff et al 2016 use borg to solve for optimal integrated planning pathways that coordinate long term water supply infrastructure development and adaptive short term drought management actions under uncertainty considering six objectives for four water utilities in north carolina giuliani et al 2017 identify the optimal control of four multipurpose water reservoirs in the red river basin in vietnam under stochastic hydrologic conditions with three objectives garner and keller 2018 use borg to optimize the annual dike heightening decisions based on two objectives and under uncertain sea level rise projections and storm surge gupta et al 2020 employ borg to search for optimal microgrid management policies with stochastic samples of load wind and solar while simultaneously optimizing three objectives all these aforementioned studies show borg can find high quality and diverse solutions for complex many objective optimization problems under uncertainty however there is no study using borg to optimize urban heat adaptation plans given the complexity of finding heat adaptation plans optimizing multiple objectives under uncertainty and borg s ability to solve dynamic many objective problems we adopt borg moea to solve the stochastic many objective optimization problem section 1 1 2 in city heat one main advantage of borg is its use of ε dominance ward et al 2015 which speeds up the algorithmic search process while also providing a mathematical proof of convergence and guaranteeing diversity in performance across objectives hadka and reed 2013 for each objective users specify a preferred precision level ε that represents a meaningful improvement in performance hadka and reed 2013 within a multi dimensional ε box defined by these precision levels only the solution closest in terms of the l2 norm to the ideal point within that box is archived where the ideal point combines the best values of the objectives within the box this reduces the number of solutions stored in the archive and reduces the computational burden hadka and reed 2013 the ε values we adopt for each objective in this study can be found in table 1 hadka and reed 2013 provide a detailed explanation of the features and advantages of borg compared to other moeas since the borg moea is a stochastic heuristic search algorithm whose search depends on the random seed used to initialize the population and generate new solutions we solve this problem using 10 different random seeds to start the algorithm we use the parallel implementation of borg with a single master and multiple workers hadka and reed 2015 the hypervolume a multi dimensional measure of the space dominated by the pareto set quinn et al 2017a is adopted to assess the convergence of the search consistent flattening to a common hypervolume across seeds indicates the algorithm has converged 2 3 robustness of adaptation plans to test the robustness of the pareto adaptation plans generated by city heat we further re evaluate their performance on additional out of sample scenarios beyond those that were considered when optimizing the policy functions in our study the satisficing robustness measure quinn et al 2017a is used to evaluate those optimal plans a satisficing measure computes the fraction of scenarios in which a solution achieves target performance levels on given criteria the selection of these targets is obviously an important value judgment that will affect what solutions perform best by this measure alternatively other robustness measures such as regret measures lempert et al 2003 could also be used for the re evaluation purpose the choice of robustness measures depends on users preferences and risk attitudes which may affect which plans are considered most robust we recommend using multiple robustness metrics in real world applications if possible also we only include a single criterion i e mortality rate for simplicity to illustrate the robustness analysis however multiple criteria could be considered to define the robustness of adaptation plans in real world applications 3 a hypothetical study for baltimore md usa baltimore is a mid sized city in the mid atlantic region of the united states the summertime heat in such a warm and humid city is associated with many direct and indirect health impacts bunker et al 2016 heilmann et al 2021 recently there were 134 heat related deaths reported in maryland between 2012 and 2018 marylandreporter 2019 twenty eight percent or thirty seven of those deaths occurred in baltimore city which is home to only ten percent of maryland s residents therefore the city government of baltimore has made prioritized heat stress management in its urban climate adaptation efforts baltimore 2019 furthermore the association of elevated uhi with poverty and race strongly applies in baltimore where the highest heat conditions are often found in poor minority neighborhoods with a history of discriminatory housing policies huang et al 2011 hoffman et al 2020 wilson 2020 disparities in heat exposure reinforce inequities equitable urban planning and health policy and environmentally just climate adaptation actions thus emphasize the need to reduce heat exposure in underserved and vulnerable populations milan and creutzig 2015 therefore we apply city heat to a hypothetical heat adaptation problem in baltimore to demonstrate the usage of this tool and some important results and insights it can generate in the baltimore case we assume adaptation decisions are made for each of the city s 11 planning districts fig 3 as defined by the baltimore city department of planning aligning decision making with planning districts makes the identified adaptation plans useful in the real world context thus the sub city regions described in section 2 are referred to as districts in our baltimore study demographic data including population age structure and poverty in baltimore are retrieved from the american community survey us census bureau 2018 the data are collected at the census tract level and then aggregated to the district level if a census tract crosses district lines which occurs in 10 200 cases it is assigned to the district in which most of its area is located land use data are obtained from the chesapeake phase 6 land use dataset which is a gridded 1 m resolution dataset with 16 different land use categories usgs 2014 the building footprint in each district is retrieved from baltimore open gis data baltimore 2019 for each district we calculate the current area of tree canopies road surfaces roof surfaces and the area available for tree planting in each district the available land for tree planting is the sum of turf land and a portion α r s of impervious surfaces that are not occupied by buildings or roads e g sidewalks and parking lots α r s is treated as an uncertain parameter because different levels of tree conversion might be possible depending on infrastructure conditions for simplicity we treat α r s as the same across all districts in the case study α r s α s r in that the cost of tree planting is higher when trees are planted on impervious surfaces e g cost for making tree wells we model the capital cost of trees as a step function whose higher value is reached when all turf lands have been converted to trees the temperature deviation of each district is estimated with temperature data in baltimore collected by a dense sensor network deployed by johns hopkins university scott et al 2016 shi et al 2021 table 2 provides a summary of the district level demographic and land use data of baltimore we use 32 daily maximum temperature projections generated by different climate models with different downscaling and bias correction methods retrieved from na cordex ncar 2014 and loca datasets pierce et al 2014 the 32 models represent a range of possible climate conditions that baltimore could experience for the next decades representing the climate uncertainty the annual summer mean temperatures of each projection are calculated based on the daily data and are shown in fig 4 in this study we use 10 projections red lines in fig 4 in optimization and use the remaining 22 projections grey lines in fig 4 in robustness analysis mean summer daily maximum temperatures range from 28 3 c to 30 6 c in the climate projections we use compared with the historical value 27 8 c in baltimore these projections represent a wide range of global warming scenarios of an increase from 0 5 c to 2 8 c by 2040 which well characterizes climate uncertainty these temperature projections are further adjusted by δ t r to get the district level temperature projections as described in section 2 1 1 in addition we include three different heat wave definitions in this study anderson and bell 2010 huang et al 2010 peng et al 2010 because previous studies found different heat related mortality impacts from different heat wave definitions the magnitude of heat health impacts is a key factor that determines the desirability of alternative heat adaptation strategies these heat wave definitions represent three different ways to define a heat wave the first from huang et al 2010 defines a heat wave as a period with at least three consecutive days with daily maximum temperatures over a single absolute temperature threshold of 35 c in the second anderson and bell 2010 define a heat wave as a period with at least two consecutive days with daily maximum temperature over a single relative temperature threshold defined as the 95th percentile of local summer temperature from may 1 to september 30 the third way to define a heat wave is from peng et al 2010 who define it using two relative temperature thresholds t 1 as the 97 5th percentile and t 2 as the 81st percentile of daily maximum temperature respectively a heat wave is then defined as the longest period of consecutive days satisfying the following conditions the daily maximum temperature is above t 1 for at least three days the daily maximum temperature is above t 2 for every day of the entire period and the average of daily maximum temperatures over the entire period is above t 1 other heat wave definitions could also be used but these three definitions represent typical defining methods in the literature chen et al 2015 faye et al 2021 in this study we use the moving average of heat wave days that occurred during five preceding years θ r t 1 s 1 5 t 5 t 1 h w d r t s as the input of the dps functions section 2 1 2 to trigger temperature reduction actions in each sub city region district percentiles for the second and third heat wave definitions are estimated empirically from historical data over the period 1949 2019 because temperatures in each district are affected by δ t r and t r t s m i t i the heat wave conditions are different in each district table 2 other variables such as the observed effectiveness of each action could be used as inputs in dps functions and other policy functions section 2 1 2 can be used as the dps functions these possibilities are left for future studies additionally we assume three temperature reduction actions trees cool roofs and cool pavements as well as one exposure reduction action cooling centers can be invested in each district in baltimore within a 20 year period from 2020 to 2039 the number of cooling centers is assumed to be decided ahead of the study period in each district and then these cooling centers can accommodate users during heat waves through the 20 year period the temperature reduction actions can be taken each year the purpose of including two types of decision variables c c r and μ i r i r is to show city heat can flexibly handle different types of decision variables the temperature reduction functions f i in eq 3 of the three temperature reduction actions are shown in figure a 1 in the supplementary materials here we assume that the temperature reduction effects increase linearly with the percentage of cool roofs and pavements li et al 2014 levinson and harvey 2017 for trees previous studies find that the temperature reduction effect has a non linear relationship with the percentage of the tree coverage and the marginal effect increases ziter et al 2019 alonzo et al 2021 therefore we assume the tree canopy coverage has a quartic temperature reduction function the coefficient t r i s is uncertain because the temperature reduction effects of those actions vary largely across studies santamouris et al 2017 the uncertainty about future climate change population growth and actions effectiveness along with other factors are characterized by various scenarios we take a wide range of those uncertain parameters table a 1 and use the lhs method to generate thousands of scenarios for optimization and robustness analysis the values of scenario independent parameters are summarized in table a 2 for a more thorough case study values or ranges of parameters can be obtained from more comprehensive literature reviews or interviews with decision makers and experts in our optimization we generate 1500 scenarios which are the combination of 10 temperature projections red lines in fig 4 3 heat wave definitions and 50 lhs samples of scenario dependent parameters 10 3 50 table a 1 for robustness analysis we generate another 3300 scenarios which are the combination of the remaining 22 temperature projections the grey lines in fig 4 3 heat wave definitions and another 50 lhs samples of scenario dependent parameters generated over the same parameter ranges 22 3 50 here we define the robustness of an adaptation plan by calculating the fraction of 66 000 scenario years 20 years 3300 scenarios satisfying the condition that the annual mortality rate is below the level of the 1995 chicago heat wave whitman et al 1997 we select this robustness indicator for demonstration the definition of robustness and choice of robustness indicators can be determined by users based on their preferences 4 results discussion in this section we demonstrate the insights that city heat can yield by exploring the set of pareto optimal plans and their tradeoffs through its application in the baltimore case city heat found 166 approximately pareto optimal heat adaptation plans for baltimore fig 5 each plan consists of policy functions for each of the three temperature reduction actions that we study and the recommended number of cooling centers to install immediately in each district more details of those plans are presented in section 4 3 the hypervolume analysis across 10 random seeds indicated fast and consistent convergence of the borg algorithm on our problem figure a 2 the parallel coordinate plot in fig 5 a provides an overview of the objective values of 166 pareto approximate adaptation plans which illustrates the tradeoffs that city planners need to consider when choosing among alternative plans each line results from an individual adaptation plan and its intercepts with the five vertical axes correspond to its performance on the five objectives the lower a line lies on an axis representing lower values the better performance the plan has on that objective because the desire is to minimize those objectives eq 27 note that the maximization objectives o 1 and o 5 are multiplied by 1 so that they can be minimized according to the definition of pareto solutions there is no pair of lines such that one lies on or below the other on all axes while being strictly below on at least one axis this means that compared to each of the other plans any one particular plan has at least one objective that is better the color of each line represents the average cost across scenarios per life saved relative to the no action baseline i e the expected unit cost per life saved defined as the discounted cost divided by the undiscounted number of lives saved alternatively the levelized cost defined as the discounted cost relative to the base no action case divided by the discounted number of lives saved could be calculated however it is controversial whether a non monetized benefit such as lives saved should be discounted and if so which discount should be used severens and milne 2004 here we used the undiscounted number of lives saved as the objective o 1 eq 22 and calculated the expected unit cost as defined above we further calculate the discounted number of lives saved and the levelized cost per life saved of each plan using the same discount rate as used for money in each scenario the result shows no significant changes in the relative rank of those 166 plans when lives saved are discounted figure a3 however the levelized costs increased by approximately 50 80 of the 166 generated plans this visualization helps decision makers to understand the tradeoffs involved in choosing among alternative adaptation plans for example among the 166 plans the most expensive plan is plan 6 fig 5 b on average across scenarios this plan costs 2 167m all monetary values are in 2019 dollars over 20 years saves 200 lives absorbs 354 kton co2 limits worst year heat related mortality to 32 deaths and controls the maximum difference in mortality rate to 9 deaths per 100 000 people across these same scenarios another plan plan 133 fig 5 b has about half the average cost 1 051m but saves only 160 lives absorbs 209 kton co2 limits worst year heat related mortality to 37 deaths and keeps the maximum difference in mortality rate at 10 deaths per 100 000 people decision makers can choose between these two plans based on their own judgment as to whether the expected incremental investment of 1 116m 2 167m 1 051m over 20 years is worth the expected improvement in saving an extra 40 lives 200 160 lowering the worst year mortality by 5 deaths 37 32 and eliminating an extra 145 354 209 kton co2 quantifying these tradeoffs can improve the understanding of outcomes resulting from different plans and facilitate negotiation among decision makers 4 1 increasing marginal cost of saving lives across all solutions in the pareto set we see that the expected total cost increases non linearly with the expected mortality reduction indicating a rising marginal cost or decreasing efficiency in saving lives through urban heat adaptation fig 5 b this could be a result of the non linear relationship between heat wave days and average summer temperatures examples in figure a 4 under high temperature conditions a unit degree decrease in temperature results in a greater decrease in the number of heat wave days and consequently heat related mortality than the same unit degree decrease under lower temperature conditions therefore the initial investment used to alleviate the hottest conditions is more efficient in reducing heat wave days and mortality subsequent investments have diminishing efficiency because it is more difficult to reduce heat wave days when the temperatures are already moderate to low in the baltimore case the expected unit cost as defined above of the 166 plans ranges from 0 4m to 11 6m per life saved which provides a wide spectrum of plans with different cost efficiency levels for city planners to choose from the cost of each plan would be lower if co benefits such as carbon reduction are converted to monetary values which further increases the cost efficiency of those plans city planners can select the plans with the most appropriate cost efficiency level for their own cities to note some adaptation plans save fewer lives while also incurring a higher unit cost per life saved this is because they have advantages in terms of other objectives for example plan 64 fig 5 b saves an average of 44 lives across scenarios with an average unit cost of about 11 6m life saved compared to other plans with a similar level of mortality reduction plan 64 relies heavily on trees which significantly increases the unit cost but also reduces more co2 4 2 effectiveness efficiency and equity in the design and application of city heat promoting equity is set as one of the main objectives of heat adaptation from the baltimore case we identify a potential effectiveness efficiency equity tradeoff among the optimized plans as shown in fig 5 c there is an inverted u shape between the expected mortality reduction effectiveness and the expected maximum disparity in district level mortality rates the indicator of inequity when the investment level is low the potential for mortality reduction is low and the adaptation actions are concentrated in a limited set of districts due to the limited investment this exaggerates the disparity in mortality rates between districts with and districts without adaptation exacerbating inequity increasing investment continues to exacerbate inequities up to a point beyond which further investment begins to decrease inequity this happens when investments are sufficient to be more evenly distributed across districts in addition to the tradeoff between effectiveness mortality reduction and equity disparity in mortality rate among districts we also find a tradeoff between efficiency expected unit cost and equity an inefficient plan with a high unit cost often results in more equitable outcomes than a more efficient plan with a lower unit cost that achieves the same mortality reduction effectiveness for example plans 60 and 65 fig 5 c both save an average of 100 lives plan 60 has an expected unit cost of 5 0m per life saved which is lower than the expected unit cost 8 8m of plan 65 however plan 65 leads to a lower expected district level mortality rate disparity 8 per 100 000 people than plan 60 does 11 per 100 000 people this is because plan 60 concentrates investment in a few districts with the lowest marginal cost for saving lives leading to a lower unit cost to achieve a certain total mortality reduction however to balance heat health risks among districts plan 65 spreads investments across more districts including some districts where the cost effectiveness is not as high resulting in a higher expected unit cost for example plan 60 only invests trees in three districts while plan 65 invests trees in five districts however plan 65 to a smaller discrepancy in mortality rate across districts better equity outcomes 4 3 adaptation actions type timing and location the parallel coordinate plot in fig 5 a presents an overview of the performance of the optimized adaptation plans without providing information on the specific actions they take hence we further provide a detailed visualization of each generated plan including its number of cooling centers policy functions of three temperature reduction actions expected fractional changes and expected discounted cost of each action taken in each district and year here we visualize plan 6 as an example fig 6 this plan achieves the greatest expected mortality reduction among all the pareto optimal plans the table at the top of fig 6 summarizes its five objective values the first row of fig 6 shows a the optimal distribution of cooling centers across the city and the optimized dps policy functions for investing in b trees c cool pavements and d cool roofs in each city district the varying shapes of these policy functions reflect that different adaptation rules are optimal in different districts that translate previous heat wave conditions into different temperature reduction actions for example curves with a step change e g the grey line in fig 6 b representing trees in the central district indicate a rapid investment in adaptation once a heat wave threshold or tipping point is reached curves in the form of a horizontal line e g the blue line in fig 6 c representing cool pavements in the south district indicate a constant investment that is independent of heat wave conditions curves with a non zero intercept on the y axis e g the purple line in fig 6 d representing cool roofs in the southwest district indicate that adaptation should be taken even if there are no heat wave days which could be viewed as a preemptive adaptation strategy applying these rules across the 1500 scenarios leads to varying levels of investment in those actions in each district the average levels of these investments across scenarios are displayed in terms of the scenario average cumulative fraction of that district s capacity for each action that is implemented over time in fig 6 e h and the cost of implementing that action over time in fig 6 i l as the costliest plan plan 6 has non zero average investments in almost every action in every district tree planting actions are implemented close to their maximum extent in the near term in all but two districts as trees take time to grow the cumulative fraction of possible land area converted to trees fig 6 e does not look the same as the cumulative fraction of possible tree canopy fig 6 f this illustrates the necessity to invest quickly in actions that take time to fully realize their health benefits if the most mortality reduction is desired investments in actions that have immediate benefits like cool pavements and roofs are more evenly and gradually increased in the first several years to take advantage of their discounted cost as time goes on the average cost of the cooling centers fig 6 i is much lower compared to the other three actions which require large capital and maintenance costs with this visualization tool planners can get a direct sense of the timing magnitude and cost of taking each action in each district further visualizations could investigate how these actions and their costs differ in different scenarios to see the influence of the optimized state dependent rules and uncertainty on investments as an illustration we only visualize plan 6 but the same plot can be generated for any pareto approximate adaptation plan of users interest therefore this tool provides a roadmap illustrating actions and costs in each sub city region and year which helps users better understand each generated plan 4 4 actions and locations requiring preemptive adaptation in fig 6 b d some dps curves have non zero intercepts on the y axis parameter d 0 see fig 2 and section 2 1 2 for explanation indicating that adaptation should be taken even if there are no heat wave days this can be viewed as a anticipatory adaptation strategy and is crucial to preventing heat mortality most effectively it is therefore important to understand in which districts or sub city regions in general and under which conditions those precautionary actions are needed in fig 7 we present the values of parameter d in the dps functions representing the investment in trees of the 166 optimized heat adaptation plans for most of the plans that have a scenario averaged mortality reduction of over 150 lives those on the right side of the red dashed line the values of d are negative below the blue dash line in districts including the north northeast northwest southwest and west the negative d values indicate that urban afforestation should start even before observing any heat wave days in those districts which can ensure an effective mortality reduction under uncertainty these five districts are the areas with the most vulnerable populations low income and elderly in baltimore fig 8 figs 7 and 8 together suggest that it is important to protect vulnerable populations such as the elderly and the low income to achieve the effectiveness and equity in urban heat adaptation practices values of d for cool pavements and roofs can be found in figure a 5 and figure a 6 in the appendix similar to trees precautionary investments in cool pavements are necessary for some districts with the most heat vulnerable populations to achieve high mortality reduction effects however very few plans choose to invest in cool roofs before the occurrence of heat waves this is because cool roofs have similar effects in reducing air temperatures as cool pavements but at higher costs santamouris et al 2017 this is consistent with the finding that cool roofs in general are not favored by city heat the conclusions might change with respect to different cost and effectiveness assumptions those findings suggest that urban heat adaptation should prioritize the areas of greatest population vulnerability to be most protective of human health which is also found in previous studies vargo et al 2016 this can be an important rule for cities to allocate their heat adaptation resources to maximize cost effectiveness 4 5 robustness of optimized adaptation plans robustness is important when comparing alternative adaptation plans under uncertainty and especially deep uncertainty lempert et al 2003 marchau et al 2019 in this study we re evaluate the 166 adaptation plans across 3300 out of sample scenarios see section 3 then we calculate the fraction of scenario years 3300 20 that meet our satisficing metric whether the city wide annual mortality rate in a given year is below that of the 1995 chicago heat wave a larger fraction indicates that a plan is interpreted as more robust fig 9 shows the robustness of the 166 adaptation plans and their total cost as shown in fig 9 the more costly adaptation plans considered here have higher robustness against uncertainties and can lower the possibility of a heat disaster for example the robustness indicators of the adaptation plans with the lowest cost 5m and the highest cost 2 167m are 99 2 and 99 9 respectively which means the possibility of experiencing a heat disaster that is more severe than the 1995 chicago one drops from 0 8 to 0 1 by the increase in investments robustness information for the adaptation plans enables users to quantify the residual risks of a heat disaster associated with each plan and to select an adaptation plan if they are risk averse and concerned about the worst scenarios the selection of metrics measuring robustness can be made by users based on project needs plan robustness could be defined by comparing with major historical hazard events as above or using user selected criteria with a practical interpretation this current study only includes mortality rate as a criterion but other criteria such as cost equity or co benefits could also be considered 4 6 comparisons of plans generated by city heat and plans based on simple rules or optimized with a single scenario shi et al 2019 suggest that it is valuable to consider multiple scenarios and adaptive management rules to address complex climate adaptation problems to demonstrate the value of city heat in identifying better strategies for the high dimensional urban heat adaptation problem with multiple objectives and uncertainty we compare the 166 plans generated by city heat considering the 1500 scenarios set 1 compared to 1 plans optimized with respect to the average scenario set 2 and to 2 plans based on simple decision rules set 3 for a fair comparison the performance of all these plans is compared on the 3300 out of sample scenarios see section 4 5 this ensures all plans are evaluated under scenarios to which they were not optimized analogous to the out of sample validation method that is widely used in comparing the performance of machine learning models for the first comparison we define the single average scenario to which set 2 policies are optimized based on the following criteria 1 the climate projection has the summer mean temperature closest to the average of all projections 2 a single heat wave definition is chosen the definition in anderson and bell 2010 is chosen for demonstration and 3 the middle value of the range of each uncertain parameter is used these criteria make the scenario represent the average of all scenarios we then generate another set of pareto approximate heat adaptation plans set 2 using city heat under this average scenario there are 215 plans generated in set 2 which represents the best trade offs among the five objectives when only considering the average future conditions fig 10 presents the scenario averaged values of the five objectives eq 22 26 for both plans in set 1 green color and in set 2 red color across the 3300 out of sample scenarios from this we see that anchoring on a single scenario leads to the identification of high cost plans red lines in set 2 which do not outperform plans in set 1 in other objectives we find that no plan in set 1 0 of 166 is dominated by any plan in set 2 for all five objectives by contrast 55 plans in set 2 26 of 215 are dominated by at least one plan in set 1 this suggests that optimal plans for a single average scenario may be unfavorable if the future is not as assumed thus it is important to consider a wide range of possible future scenarios for the second comparison we consider two simple rule based heat adaptation plans set 3 this first simple rule based plan or plan 1 year is to make 5 of all possible investment options of trees cool roofs and cool pavements every year through 20 years the second plan or plan 5 year is to make 25 of all possible investment options every 5 years for both plan 1 year and plan 5 year 10 cooling centers the upper bound considered in the optimization are assumed to open in each district the performances on the five objectives of these two plans are evaluated using the city heat simulation model section 2 1 1 under each scenario the two city heat generated plans set 1 with the closest scenario averaged cost o 2 to plan 1 year and plan 5 year respectively are chosen for comparison plan 68 is chosen for plan 1 year and plan 141 for plan 5 year table 3 summarizes the number of scenarios in which the two set 1 solutions plan 68 and plan 141 are better than the two set 3 solutions plan 1 yr and plan 5 yr from this we can see how considering adaptive management can help identify better heat adaptation strategies among the 3300 out of sample scenarios plan 68 has better performances in all 5 objectives than plan 1 yr in 872 26 42 scenarios while in contrast plan 1 yr does not outperform plan 68 in all 5 objectives in any scenario since plan 68 has at least 1 objective that is better than plan 1 yr for all 3300 scenarios the same conclusion can be drawn based on the comparison between plan 141 and plan 5 yr plan 141 has better performances in all 5 objectives than plan 1 yr in 907 27 48 scenarios while in contrast plan 5 yr does not outperform plan 141 in all 5 objectives in any scenario this comparison shows that flexible plans generated by city heat set 1 which leverage adaptive management based on evolving system information outperform plans based on simple open loop rules set 3 this further demonstrates the usefulness and value of sophisticated decision analysis tools like city heat which can consider information learned over time through the baltimore case study we demonstrate that city heat can provide valuable insights such as 1 quantitative trade offs among multiple health and social economic objectives 2 adaptive policy plans for different adaptation actions and 3 policy robustness across a wide range of future scenarios these insights are important to support urban heat adaptation and are not provided in previous studies as discussed in section 1 4 and 1 5 5 conclusion the urban heat adaptation planning problem is a multi objective high dimensional stochastic and dynamic decision making problem we propose a decision support tool city heat to address those challenges and provide a case study for baltimore city to illustrate what insights this tool can generate city heat optimizes the long term performance of alternative urban heat adaptation plans in five dimensions including plan effectiveness in achieving health benefits total financial cost equity in mortality disparities across sub city regions heat vulnerability measured by worst case single year heat mortality and co benefits such as cumulative co2 sequestered to address the impact of uncertainty on adaptation plans city heat searches for optimal adaptive decision rules across thousands of scenarios that represent various possible future situations for climate change population and demographic change and the effectiveness of adaptation actions in this study the heat wave conditions characterized by the average number of heat wave days during the previous five years are used to trigger decisions in which the rules use the most recent information to adjust decisions at the current time step by using optimization the best parameters for the decision rules across the range of uncertainties are identified those seemingly complicated rules fig 6 can be interpreted as simple statements for policy recommendations as in our baltimore example we find several districts need anticipatory urban afforestation and this can be directly formed as a policy recommendation in addition several decision rules fig 6 exhibit pronounced tipping points thresholds for the number of heat waves beyond which an action should be taken in each district where the curve goes above zero therefore city heat results can be interpreted in terms of easily understood policy recommendations advantages of city heat can be summarized as follows first city heat quantifies complex trade offs among multiple planning objectives in our baltimore example city heat explicitly identifies trade offs among effectiveness efficiency and equity of alternative heat adaptation plans fig 5 indicating the challenges of mitigating inequities in urban heat management also city heat reveals a need to prioritize populations with the greatest vulnerabilities fig 7 in urban heat adaptation therefore city heat provides quantitative evidence for policy making in urban heat adaptation second city heat generates rule based adaptation decisions at fine temporal annual and spatial sub city resolutions while considering multiple planning objectives and robustness across a large ensemble of future scenarios it is impossible to generate a rich set of pareto efficient candidate adaptation plans intuitively or based on rules of thumb without the support of tools such as city heat these features distinguish city heat from tools developed or employed in previous studies which have either not explicitly optimized health objectives limited themselves to a small set of pre defined adaptation plans and or disregarded uncertainty of climatic and social factors third city heat is a generalizable and flexible decision support framework and tool what candidate adaptation actions performance objectives and uncertain factors to include can be customized based on users needs many of the needed inputs such as climate projection data and population data can be conveniently retrieved from publicly available datasets such as na cordex loca and the american community survey data used in our study sub city level temperature data is less easily obtained but their availability is likely to increase because many cities have begun to measure their intra urban temperature distributions scott et al 2016 shandas et al 2019 shi et al 2021 finally other assumptions about the range of uncertain parameters and some case specific data such as costs of actions in different cities can be tailored to the application and be informed by interviews with stakeholders and experts fourth city heat has the ability to examine many important questions in urban heat adaptation planning especially about social equity and uncertainty for example concerning equity how do populations of varying ages races genders and income levels benefit from urban heat adaptation how can environmental justice be enhanced through urban heat adaptation regarding uncertainty with the tool we can ask which uncertain parameters contribute the most to uncertainty in the performance of adaptation plans what is the value of improved knowledge about those uncertain parameters in terms of enhanced performance of strategies answering those questions could help city planners better address the challenges of urban heat adaptation by finding more effective efficient and equitable adaptation plans finally compared to more simplified decision processes we show that city heat derives better strategies our comparisons of the heat adaptation plans generated by city heat with plans based on simple rules demonstrate the usefulness and advantages of city heat in identifying better solutions for the urban heat adaptation problem which is high dimensional and complex city heat plans generated under multiple scenarios that can adapt to those scenarios often pareto dominate simple plans based on a constant rate of an investment over the time horizon in each adaptation action or optimized considering just one scenario this further confirms that considering various scenarios optimizing multiple objectives and incorporating the flexibility to adapt decisions in response to information learned over time can be valuable to identify better decisions for complex climate adaptation problems shi et al 2019 future research could pursue several directions although the case study was sufficient to illustrate how city heat can be applied to a real world case and what insights can be generated several assumptions such as the costs of different actions and performance measures taken from the literature might not best reflect the situation and priorities in baltimore future work can refine these assumptions through engagement with local experts managers and stakeholders to best reflect their knowledge and preferences additional objectives could also be defined to reflect the full range of community benefits and concerns such as reduction of air pollution or compensation for historic injustices finally the tool could be expanded in scope to address other urban adaptation problems such as managing possible increases in runoff and pollution from more frequent severe storms this expansion is desirable because some actions addressing urban heat such as green roofs also can help mitigate other climate impacts in summary city heat is an innovative decision tool for identifying both immediate actions and long term adaptations to mitigate the effects of urban heat waves considering multiple objectives under uncertainty tradeoffs among objectives such as effectiveness cost equity vulnerability and co benefits are critical inputs to decisions and are quantified and visualized by city heat software availability software name city heat equity adaptation tool city heat developers rui shi benjamin f hobbs julianne d quinn robert lempert and debra knopman year first official release 2022 hardware requirements pc or hpc system requirements windows with microsoft mpi mac or linux with openmpi program language c program size 1 mb 4 mb including borg software from http borgmoea org availability https github com srayum city heat license gnu general public license v3 0 documentation user guide and examples hosted at https github com srayum city heat the authors do not have any conflict of interest declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research was supported by the mid atlantic regional integrated sciences and assessments marisa consortium through a grant to the rand corporation from the national oceanic and atmospheric administration s climate program office na16oar4310179 the johns hopkins 21st century cities initiative and the johns hopkins alliance of a healthier world we thank reid cagir benjamin zaitchik and darryn waugh of jhu klaus keller of psu jordan fischbach of the water institute of the gulf and other marisa colleagues for their suggestions to improve this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105607 
