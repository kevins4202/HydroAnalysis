index,text
25555,accurate water level prediction is the premise of farmland waterlogging prediction a simple water level prediction model fdpre based on four machine learning ml algorithms and weather forecasts were developed the model can not only predict two key driving factors of waterlogging rainfall and node water level but also estimate disaster losses the results showed that the random forest and multiple perception model r2 ranged from 0 7180 to 0 9803 and 0 5717 to 0 9965 performed best in the case of flooding lasting for one day the economic loss of waterlogging under the 100 mm rainfall scenario 23 53 million dollars was much higher than that under the 50 mm rainfall 12 69 million dollars under the two rainfall scenarios the yield reduction rate in the lower reaches of the sihu basin was higher than that in the upper reaches the method of coupling ml and weather forecasts can well predict farmland waterlogging keywords farmland flood disaster forecast machine learning weather forecast fdpre model flood disaster loss 1 introduction in the context of climate change frequent extreme rainstorms and floods have caused huge economic losses zhang et al 2012 as one of the major agricultural disasters in humid and sub humid areas waterlogging affects 20 35 of agricultural soil in china yang et al 2017 the anoxic condition of waterlogging stress limited root respiration shaw and meyer 2015 affected the absorption of water and nutrients sairam et al 2008 and increased plant senescence hossain et al 2011 meanwhile the pesticides and chemical fertilizers that farmland waterlogging may carry also increase the risk of water pollution and soil salinization chen et al 2020 it has been found that waterlogging may lead to the reduction of crop yield by more than 32 9 tian et al 2021 rice feeds 60 of china s population and accounts for more than 20 of china s arable land according to a previous study yang et al 2015 the loss of rice production caused by waterlogging accounts for a quarter of all natural disaster losses in china every year in addition the yangtze river basin is the most serious waterlogging affected area in china chen et al 2018 therefore it is necessary to predict farmland flood disasters in the yangtze river basin anthropogenic climate change increases the risk of heavy storms and waterlogging pall et al 2011 water level forecasting is important for the prediction and mitigation of flood disasters sapitang et al 2020 precipitation is undoubtedly a crucial factor affecting the extent of flood disasters and the effects of human intervention such as pumping stations on drainage should also be considered however previous studies either focused on the impact of urban waterlogging liu et al 2020 real time prediction of the reservoir or river water levels nur adli zakaria et al 2021 or simulation of flood hydrographs kaya et al 2019 there are few reports on the early warning of farmland waterlogging and the prediction of water level on the gate after rainfall in addition the improvement of remote sensing satellite resolution and the development of meteorological science makes it easy to obtain solid and reliable weather forecasts hou et al 2014 kendon et al 2014 this also makes the prediction of farmland flood disasters more feasible given the time consuming and laborious drainage experiments at a regional scale tandy et al 2018 it is necessary to apply the modeling methods to study regional waterlogging many hydraulic models have been used in waterlogging simulation such as soil and water assessment swat wei and bailey 2019 model based and incremental knowledge engineering flood mike flood li et al 2018 and drainmod shaw et al 2013 however although those process based hydraulic models have been widely used there also have some limitations for example complex hydraulic models require high professional skills of users and river network zoning and model operation is time consuming which are difficult to be used for forecasting and early warning machine learning ml based on big data science and artificial intelligence can be used as an alternative and complementary approach to those models ml has been widely used in agriculture wolanin et al 2020 hydrology qiu et al 2017 zhang et al 2018 and other domains among them computational intelligence methods such as fuzzy systems neural networks and evolutionary algorithms have been introduced to flood prediction fotovatikhah et al 2018 and rainfall time series forecasting wu and chau 2013 moreover deep learning dl with a multi layer structure such as convolutional neural network cnn and recurrent neural network rnn has developed rapidly in recent years and has been successfully applied in crop yield prediction kamilaris and prenafeta boldú 2018 streamflow forecasting ha et al 2021 and so on li et al 2021 however the studies on ml application in waterlogging prediction mainly focus on image or text recognition of urban waterlogging liu et al 2021b or flood susceptibility mapping avand et al 2021b while the studies on farmland waterlogging prediction under artificial intervention are still limited for example cnn was used to extract urban waterlogging depth from video images to improve monitoring capability jiang et al 2020 ml is also used to detect and classify cotton leaves under waterlogging stress zhao et al 2021 although ml has been used to predict water level the prediction of the most unfavorable conditions after rainfall is less and it is not combined with the weather forecasts farmland waterlogging is affected by water level rainfall and human intervention hence we assumed that the ml algorithm the initial water level on the sluice and the drainage capacity of pumping stations can be used to predict the farmland waterlogging after rainfall in this study four ml models random forest rf multiple perception mlp cnn and long short term memory lstm were selected compared with the complex inputs of the hydrological model and high requirements for users these four models are relatively simple and have good performance in regression issues among them rf and mlp are two common ml models which are widely used in solving regression problems because they are not easy to overfit and have good approximation ability while as two typical representatives of deep learning dl cnn and lstm perform well in predicting nonlinear problems such as time series barzegar et al 2021 current research on flood disaster prediction is mainly concentrated in cities and there is little study on farmland waterlogging considering that extreme precipitation node water level and other disaster driving factors are the key indicators of flood disaster wu et al 2020 the underlying question is how to build a simple and accurate key node water level prediction method while obtaining rainfall prediction to the best of the authors knowledge this is the first study to combine the ml method and weather forecast in predicting farmland flood disasters in this study a typical rice planting plain sihu basin in the yangtze river basin was selected to study waterlogging since its flat terrain and well preserved historical monitoring data of waterlogging and hydrological it has always been a hotspot of waterlogging research yuan et al 2021 the objectives of this study are 1 to develop a software for predicting water level after rainfall based on ml and weather forecast 2 to verify whether the ml method can be used to predict the most dangerous situation after rainfall i e the highest water level on the sluice and determine the best ml model 3 to predict the spatial distribution of farmland flood disaster and the loss of yield reduction under different heavy rainfall scenarios 2 material and methods 2 1 study area description the study area is in sihu river basin 29 21 30 00 n 112 00 114 05 n jianghan plain which located in the middle reaches of the yangtze river hubei province of china it is adjacent to the yangtze river in the east south and west the hanjiang river and dongjing river in the north and the yizhang mountain area in the northwest there are four lakes namely changhu lake sha lake bailu lake and honghu lake fig 1 which are also the origin of the name of the sihu region the total area of this basin is 11547 5 km2 the study area is in a subtropical monsoon climate zone with an average annual net flow total precipitation evaporation and forest free period of 3 5 billion m3 1200 mm 1300 mm and 260 days respectively the local has an average annual air pressure of 1016 3 hpa a mean sunshine duration of 2000 h and an annual average temperature of 16 c the region has been suffering from flood disasters with about 70 of the rainfall during the flood season the location of the study area and the layout of the 15 main drainage pumping stations in the basin are shown in fig 1 the 15 stations are xijiakou xjk tianguan tg xinglong xl liuling ll gaochang gc xulisi xls pengjiahe pjh zhougou zg futian ft zibeiyuan zby xiaxinhe xxh xiaogang xg zhangdakou zdk xindi xd xintan xt 2 2 data collection 2 2 1 historical data in this study the historical hydrological data of 15 main drainage pumping stations in the sihu basin in the last ten years 2010 2020 were collected from the sihu engineering administration bureau of jingzhou city http www jzshglj com cn and the local hydrological bureau specifically the dataset includes the water level before rainfall wl0 rainfall the number of rain days tr pumping station flow fl and water level after rainfall wl among which wl is the output and other parameters are the inputs these input parameters were chosen because their observed and predicted values were easy to obtain and closely related to water level dynamics which can be easily extended to places with limited data zhu et al 2020 more complex meteorological data especially under future prediction are more difficult to obtain which might increase the uncertainty furthermore historical yield data 2019 of late rice in the sihu basin were obtained from the hubei provincial bureau of statistics http tjj hubei gov cn tjsj sjkscx tjnj qstjnj 2 2 2 weather forecast and cloud platform development openweathermap https openweathermap org is an external weather provider that provides a scientific yet simple approach api interface to forecast the weather including hourly weather maximum and minimum temperature air pressure wind speed precipitation and other meteorological variables every 3 h in the next 5 days colezea et al 2018 in this study a cloud platform that can automatically capture future weather forecasts is built and developed on alibaba could server ip address 39 105 36 150 the schematic diagram of the automatic cloud platform operation is shown in fig a1 after using a crawler to capture real time future weather forecasts the cloud platform automatically processes the data into comma separated value csv format and saves them in the cloud in addition we compared the local measured rainfall from september to october in 2019 with the predicted value fig a1 and found that the results reached a credible level r2 0 99 2 3 machine learning models 2 3 1 random forest rf rf is a bagging algorithm combining tree predictors breiman 2001 which has shown good performance in the previous research without over fitting data multiple classifications and regression trees cart were constructed randomly by taking bootstrap samples and the number of predictors for optimal segmentation was found at each node integrating different cart rf can solve the problems of classification and regression due to the advantages of fewer hyperparameters and parallel training rf has been widely used in ecology hydrology and agriculture prasad et al 2006 rodriguez galiano et al 2014 2 3 2 multiple perception mlp mlp network feedforwarded network is a popular artificial neural network ann that consists of interconnected neurons or nodes its basic form is a three layer structure including an input layer a hidden layer and an output layer hu and weng 2009 mlp has a strong ability to approximate any function park and jo 2016 so it performs well in solving regression problems the performance of mlp regression mlpr is mainly affected by the model structure activation function and connection weight updating mode tien bui et al 2015 mlp has been widely used in flood prediction due to its good generalization ability simple and nonlinear activation mosavi et al 2018 2 3 3 convolutional neural networks cnn cnn has strong feature representation power and has achieved great success in image classification and recognition cheng et al 2016 local connections shared weights pooling and using multiple layers are the basic principles of cnn when the softmax layer is replaced by a linear or sigmoid activated fully connected regression layer namely vanilla deep regression lathuiliere et al 2020 cnn can also be used to solve regression problems 2 3 4 long short term memory lstm lstm is essentially a type of recurrent neural network rnn lstm performs well in sequence learning by building a directed cycle to establish cell connections the memory cell of the hidden layer of lstm adds an input gate forget gate and output gate to adjust the state so it is good at overcoming the gradient disappearance and explosion of rnn fischer and krauss 2018 more details are available from a previous study zhao et al 2017 k fold cross validation method has been widely used in ml validation when comparing the prediction accuracy of two or more methods researchers usually divide the original data into k random subsets to minimize the bias related to the random sampling of training and holdout data samples delen et al 2005 in this study a commonly used 5 fold cross validation was carried out to ensure that the data appeared in training and test datasets as much as possible and avoid overfitting because other methods such as the leave one out were too computationally complex the ml models rf and mlpr were implemented using python 3 6 by sklearn package https scikit learn org the dl models cnn and lstm were implemented with pytorch 1 8 1 deep learning framework https pytorch org all the codes were written in python language on a laptop equipped with intel core i5 8300h cpu 2 30 ghz 8g ram and navid getforce gtx 1050 ti gpu 2 4 model inputs and evaluation criteria model inputs included wl0 rainfall tr and fl while the outputs were the wl after rainfall for each pumping station the inputs of rf and mlp models were a four dimensional wl0 rainfall tr and fl data frame with a total size of 1238 which contains the information on rainfall events in the past decade and the output was a vector wl according to the splitting ratio of 8 1 1 the training validation and test data size were 990 124 and 124 respectively when it comes to the cnn and lstm the input data set was also divided into those three parts and appropriate format conversion was also required before training with pytorch to meet the input requirements of these dl algorithms the filter size of cnn channels and minimum batch size were selected as 2 3 4 32 and 64 thus the input format of cnn is 64 1 4 1 where the first 1 represents the channel signal meeting the input requirements and the last 4 and 1 represent four dimensional variables and one dimensional output respectively through the three dimensional convolution filters 2 3 4 three characteristic matrices were obtained and spliced into the shape of 64 96 similarly the mini batch size and the hidden layer size of the lstm model were selected as 64 and a number in 256 356 respectively and the number of layers was set to 1 or 2 according to the optimization results thus a three dimensional 64 4 1 matrix was obtained the inputs also included a time series the output of the last time step was obtained each time as the result of a rainfall event all the model hyperparameters are adjusted by the grid search method and trial and error method table 1 to avoid reusing the weather conditions the model was trained using the historical rainfall events specifically the historical dataset was randomly subdivided into three subsets 80 for the training 10 for the validating and 10 for the testing when the trained model was applied for prediction the captured weather forecast was used the discussion of rainfall prediction accuracy can be found in section 3 2 3 three criteria were adopted to evaluate the performance of ml models root mean squared error rmse determine coefficient r2 and pearson correlation coefficient pearson r cueff et al 2021 petković et al 2015 these indicators were selected because they are simple and robust and are often used to compare ml models in different fields elzain et al 2022 they were calculated as follows 1 r m s e a i 1 n o i p i 2 n 2 r 2 i 1 n o i o a v g p i p a v g i 1 n o i o a v g 2 i 1 n p i p a v g 2 2 3 p e a r s o n r n i 1 n o i p i i 1 n o i i 1 n p i n i 1 n o i 2 i 1 n o i 2 n i 1 n p i 2 i 1 n p i 2 where o i represents the observed value p i is the predicted value n is the number of samples o avg and p avg represent the average observed and predicted value respectively the value range of r 2 and pearson r are both 0 to 1 previous studies suggested that the smaller rmse the closer r 2 and pearson r to 1 the better the performance of the model jiang et al 2021 the sensitivity index si was also used to evaluate the sensitivity of model inputs as follows 4a s i r p max r p min r p a v g p max p min p a v g where p max p min and p avg are the maximum minimum and average values of input parameters r pmax r pmin and r pavg are the corresponding simulation results respectively 2 5 scenario setting and loss assessment of farmland flood disaster considering that the extreme value of local rainfall in the past decade is about 106 mm two scenarios were set to evaluate the loss of farmland flood disaster s1 conventional rainstorm i e 50 mm rainfall in one day according to the china meteorological administration http www cma gov cn s2 95 of the extreme rainfall value 100 mm rainfall in one day the scenario settings refer to previous studies hu et al 2019 siswanto et al 2016 tr was 1 day wl0 was set as the most frequent water level in the history of each station given the heavy rain in the scenario simulation fl was set as the maximum drainage flow of the pumping station tab a1 to quantify the impact of the assumption a sensitivity analysis was performed and can be found in section 3 4 given that the local area is a flat polder and the drainage capacity of the pumping station has been considered in the predicted wl we assumed that the increased value of the water level of the sluice before and after rainfall is approximately equal to the water depth of the local farmland under the condition of high water level yield reduction rate y is calculated according to a previous study xiong et al 2018 as follows eqs 4 and 5 the reason for using this formula is that it was derived from waterlogging test data rather than model simulation and has the coefficient of hubei province where this study is located in addition its reliability has been demonstrated in a previous research chen et al 2020 4b y 0 h h c a h w h r b t c h h c 5 h r 1 0 0059 d 2 1 4518 d 15 09 100 where hc hw and hr are the critical waterlogging depth m waterlogging depth m and rice height m respectively t is the accumulated waterlogging time d a b and c are empirical parameters d is the day after sowing the jointing and booting stage is one of the most serious periods of rice yield reduction caused by waterlogging when hc a b and c are 0 2m 36 909 2 084 and 0 437 respectively xiong et al 2018 to simplify the study we selected the local late rice with the highest risk of waterlogging assumed that the heavy rain occurred during the jointing and booting period and last for 1 day hr was 0 86 m and calculated the yield losses in different regions of the basin 3 results and discussion 3 1 the development of fdpre model to combine machine learning with the weather forecasts to predict farmland waterlogging in real time the software fdpre was developed the main interface and workflow of the model are shown in fig 2 the fdpre model mainly consists of three modules a water level prediction b crawler acquisition of weather forecast and c interactive chart drawing four ml algorithms namely rf mlpr cnn and lstm were embedded in the fdpre model on the one hand model users can select typical sluice stations or build new ones import historical hydrological data for ml training and then predict the maximum water level on the sluice after rainfall and draw interactive charts on the other hand the model can automatically obtain real time rainfall forecasts through a built in crawler program and import the rainfall into the water level prediction module for the forecast the model can be downloaded from github https github com jzw787 fdpre tree master detailed instruction for the model is provided in the supplementary material 3 2 the performance of fdpre 3 2 1 comparison of the performance of different machine learning models figs 3 and 4 show the comparison of the observed and predicted wl values obtained by fdrpe at 15 stations during the training and test stages overall the wl values predicted by ml are distributed near the 1 1 line it is illustrated that predicting wl by ml methods is feasible but the performance of the model is different in each station tables 2 and 3 evaluate the performance of the four ml models at 15 stations during the training stage and test stage the coloring principle is that the higher r2 and pearson r the smaller rmse and the redder tint the lower r2 and pearson r the larger rmse and the bluer tint according to the average r2 of four ml models during the training and test stages xd station was the best 0 8024 0 9664 xl station was the second 0 8100 0 9354 and ft station was the worst 0 6428 0 8119 in terms of rmse zdk performed best 0 0139 0 0445 followed by xd 0 0108 0 0653 and xt was the worst 0 1496 0 4252 the performance of the model at xd xl xjk zg xxh and zdk is better than at other stations in terms of the performance of the four ml models rf and mlpr generally performed better than cnn and lstm tables 2 and 3 the r2 ranges of rf mlpr cnn and lstm were 0 7180 0 9803 0 5717 to 0 9965 0 5566 0 9080 and 0 4457 0 9563 pearson r were 0 8201 0 9955 0 8373 0 9983 0 7469 0 9707 and 0 6209 0 9872 respectively the rmse ranges of rf mlpr cnn and lstm were 0 0073 0 4022 m 0 0010 0 3453 m 0 0260 0 3912 m and 0 0185 0 6552 m respectively the previous study usually reported dl models such as cnn and lstm outperformed traditional ml models wolanin et al 2020 however although lstm performed best at some stations in the test stage such as xl and ll the traditional rf model is generally more stable this may be due to the limited historical sub rainfall data and the low dimension of input variables considering that lstm performed better than rf in processing mutation data points and data with relatively high uncertainty fu et al 2020 mu et al 2020 xl had wl which was much higher than the average and the data distribution of the ll station was not very uniform this result was understandable in addition dl models such as cnn and rnn are better in the area of image recognition and classification and traditional ml models may perform better than dl models in solving regression problems sirsat et al 2018 specifically during the training stage rf attained the best performance at xd among all models with r2 pearson r and rmse of 0 9764 0 9892 and 0 0073 m respectively during the test stage mlpr attained the best performance at xxh with r2 of 0 9965 pearson r of 0 9983 and rmse of 0 0010 m respectively while cnn obtained the worst performance at xg with r2 pearson r and rmse of 0 5801 0 8143 and 0 1207 respectively according to the model performance in training and test stages ml models can be ranked as follows rf mlpr lstm cnn it is worth noting that mlpr outperforms rf in tg zg xxh and xg stations therefore mlpr is recommended for predicting water levels at tg zg xxh and xg stations while the rf model is recommended for other stations 3 2 2 comparison of computing costs of different models another evaluating criterion that cannot be ignored is the computing cost the calculation time of all models in training and test phases is presented in tables 2 and 3 the computing time does not include the time of grid search and parameter adjustment but includes the time of k fold cross validation during the training stage the computation time of rf is the least 0 1249 0 6075 s among all ml models it is understandable because compared with neural networks rf integrates simple but efficient decision trees therefore the operation time of rf is generally much less than that of dl models which is similar to previous studies hamrani et al 2020 given the calculation time and accuracy of rf it is a good choice to apply the rf model to predict wl values the training time of lstm is much longer than that of other models which is related to the optimized model structure it has been found that due to its recurrent nature lstm is difficult to adjust and the calculation cost is high bozheniuk et al 2020 it is worth noting that the expensive operation cost of ml models is only in the training stage as can be seen from table 3 the computing time of dl models in the test stage decreases rapidly even lower than that of traditional ml models in the test stage the calculation time of cnn and lstm ranged from 0 0003 s to 0 0312 s and 0 0015 s 0 0625 s respectively while the calculation time of rf and mlpr ranged from 0 0961 s to 0 2187 s and 0 0639 s 4 0633 s respectively 3 2 3 real time predictions in addition to verifying the model performance with historical rainfall events mentioned above another attempt is to apply the model to predict real time water levels four water levels meters have been installed which can automatically monitor and send data locally and released the ml based water level prediction results in real time through the programs deployed on alicloud servers fig a3 due to the limited installation time of the water level meters the flood disaster has not been detected nevertheless through the real time acquisition of wl and weather forecast the prediction and release of future water levels are realized the actual operation results are shown in fig 5 it can be found that the predicted water level was consistent with the measured real time water level with r2 ranging from 0 85 to 0 96 in addition the database of real time water level prediction was constructed and the early warning message of waterlogging risk could be sent to locals fig a4 since there is no need to repeat the training stage the process of real time prediction could be completed within the 30s 3 3 evaluation of flood disaster in sihu region according to the above results the rf model was applied in tg zg xxh and xg stations and the mlpr model was used in other stations to predict wl value fdpre model was applied to simulate the two scenarios as shown in fig 6 the spatial distribution of wl field water depth yield reduction rate and economic loss under the s1 scenario was further obtained under the condition of pumping station drainage at the maximum flow rate the maximum gate water level and field surface water depth at each station after rain ranged from 24 02 to 37 58 m and 0 1 31 m respectively among them the highest wl value of xl upstream was the highest 37 58 m and the wl value of xt downstream was the lowest 24 02 m in addition the field water level of the ft station was the highest while xt and xls were the lowest accordingly we calculated the rice yield reduction rate under the predicted water level fig 6c and estimated the yield loss fig 6d overall the yield reduction rate in the lower reaches of the sihu basin was higher than that in the upper reaches using ml and game theory avand et al 2021a have found that the downstream and along riverbanks were areas with high flood risks which was basically consistent with this study a previous study chen et al 2020 applied a hydrological model to predict the risk of rice waterlogging disaster in gaoyou jiangsu and found that at 327 6 mm of rainfall over 10 days the yield reduction rate in most places was less than 4 the yield reduction rate obtained in this study was higher which may be due to higher rainfall 50 mm and 100 mm per day and lower pumping station density in the scenario simulation of this study the area of gaoyou irrigation district 20 27 km2 is much smaller than that of sihu basin but there are four drainage pumping stations yield loss was calculated based on the yield reduction rate and local late rice yield in 2019 the areas with the highest yield loss more than 4000 t were xxh zg and ft although the yield reduction rates of pjh and xjk were only 3 68 and 4 15 respectively the total yield loss was larger 1486 42 t and 858 36 t due to the high rice yield according to the unit price of 2 5 yuan per kilogram of rice it is roughly estimated that the economic loss of farmland inundation for 1 day and 2 days are 12 69 and 17 19 million us dollars similarly the case of high precipitation 100 mm a day was also studied fig 7 in this scenario the wl values of xl and xt were 23 93 m and 38 06 m respectively which are also the maximum and minimum values of wl compared with low precipitation the water level of farmland in many areas has risen such as xxh xl gc and pjh the field water levels of tg zg ft zby and xxh were the highest ranging from 1 01 m to 1 64 m in addition high precipitation significantly increased the yield reduction rate of rice this may be attributed to the limited local drainage capacity and the backwater effects of high drainage ditch level le et al 2007 that is the accumulated water in the field exceeds the ridge depth and flush with the water level of the drainage ditch in heavy rain conditions which may lead to flood recharge and be conducive to waterlogging for example the reduction rate of xl increased from 1 40 to 11 78 and that of gc increased from 7 43 to 34 60 fig 7c due to the higher yield reduction rate there are five areas with a yield loss of more than 10000 t namely xl pjh zg ft and xxh based on the above data it can be estimated that under the condition of 100 mm rainfall in one day the economic loss of farmland inundation for 1 day and 2 days are 23 53 million and 31 86 million us dollars respectively a previous study found that providing flood warning measures is an effective measure to improve the ability of disaster resilience zhang et al 2021 given that rice cultivation requires a lot of water and must be planted near the river it is necessary to timely give early warming based on weather forecasts and adjust the flow of the pump station and plant more rice in areas with less risk of waterlogging in the future 3 4 sensitivity analysis in this study sensitivity analysis was also conducted to quantify the effects of input parameters fig 8 overall the sensitivities of the four ml models were relatively close among which cnn was the most sensitive to rainfall rainfall days and flow lstm was the most sensitive to wl0 which is probably due to the difference in model algorithm and input structure it can be observed that the initial wl0 has a significant effect on the model input with si ranges from 0 56 to 0 69 which is understandable because the wl0 used in this study is the elevation value of the water level and contains the altitude information of the station noticeably the si of rainfall and rainfall days were 0 16 0 27 and 0 17 0 21 respectively which was basically the same as a previous study wu et al 2020 this may be because the rainfall in this study was the cumulative values and the pumping stations were generally opened for flood discharge when rainfall occurred in history so the sensitivity is not particularly high in addition the si of the pumping station flow to the wl was between 0 11 and 0 21 which indicates that the drainage of the pumping station promotes the decline of the water level 3 5 limitations prospects and uncertainty analysis this study provides a simple method based on ml and weather forecasts to predict farmland waterlogging disasters after rain simple input parameters easily obtained prediction results are undoubtedly the advantages of the fdpre model the results of the model have reached a credible level in the study area however the current model still has some limitations the purpose of this study is to propose a simple warning approach for farmland waterlogging some simplifications are made for example there is a certain coefficient relationship between the change of water level on the sluice and the depth of farmland waterlogging but we simplified it to 1 it is necessary to calibrate in future research in addition the meteorological forecast has a great influence on the results since the local terrain is flat and belongs to the polder area we assume that the water level in the control range of the pumping station is the same when the flood disaster occurs further development of waterlogging prediction systems based on platforms such as google earth engine gee is promising in the future it is necessary to combine ml methods with gis platforms and remote sensing assimilation to predict farmland waterlogging the plant height and submergence depth of rice is based on an empirical formula so the yield reduction rate and economic loss may be overestimated another limitation is the interpretability of ml methods how to further mine the relationship between variables from the regression relationship obtained by ml algorithms is the direction we need to pay attention to in future research in addition the combination of the hydrological model and ml algorithm may be the direction of future research a large number of data generated from the hydrological model can be used for ml training or the relationship obtained from the ml algorithm can be added to the hydrological model since the purpose of this study is to develop a simple but accurate wl prediction model based on ml and weather forecast while gee and hydrological models are more complex and need more input parameters we will conduct related research in the future nevertheless the prediction method of farmland waterlogging proposed in this study can provide some guidance for disaster early warning and loss assessment in addition we have constructed a runoff generation and concentration model about 1333 ha in the sihu basin in another previous study liu et al 2021a which has been verified that it can also be used for flood prediction however it requires many parameters which is difficult to obtain when applied in other places and requires high operating conditions another problem to be solved in the following study is to predict the retention time of waterlogging which needs to be carried out based on analyzing topography hydrology and climate currently it is judged by real time prediction of water level changes in the next few hours it should not be ignored that the accuracy of weather forecasts could influence the model performance thus the predicted weather data and the observation were also compared fig a5 it can be observed that the captured 3 h rainfall prediction data and the measured values have reached a good fitting level rmse 1 28 mm and the prediction effect of weather variables decreased with the increase of prediction time moreover the effects of lead time on the accuracy of the model were studied in short considering that it is difficult to accurately predict rainfall for a long time we compared the wl prediction results obtained by using the rainfall prediction data of 3h 6h and 24h in the future fig a6 overall the results of using 3 h weather forecast data were the most accurate with the r2 of 0 85 while the results of 6 h and 24 h dropped to 0 70 and 0 61 respectively therefore the wl prediction value obtained from the short term rainfall forecasts is more reliable previous studies kang et al 2016 rahmat et al 2018 have also noticed that openweathermap performs well in predicting meteorological factors such as the short term rainfall which was consistent with this study future research can also consider using ml methods such as the lower upper bound estimation for interval prediction taormina and chau 2015 4 conclusion fdpre a simple model for predicting water level after rain based on ml and weather forecasts was developed in this study based on ten years observation data four ml algorithms were used to establish the relationship between the maximum water level and initial water level rainfall rainfall days and drainage flow of drainage pumping stations in the sihu basin the model has built in api access to future weather forecasts for water level prediction the results of model training and test show that ml can accurately predict farmland waterlogging at 15 stations in general rf and mlpr performed better than cnn and lstm although lstm performed best at some stations overall rf has higher r2 lower rmse and much less computing cost however dl models only consumed a lot of computing time in the training stage and the computing time in the test period was even lower than ml models the rice yield reduction rate and economic loss under the s2 scenario were much higher than those under s1 in general the input parameters required by the fdpre model are simple and the results are accurate future research should pay more attention to the relationship between the depth of farmland flooding and the change of water level on the sluice to better predict the farmland waterlogging disaster and assess the loss in addition coupling ml and weather forecasts to issue early warnings in time adjusting the pumping station flow and rice planting layout are necessary to reduce waterlogging loss 4 1 software availability the model was developed by hohai university using python language this system can be run on a standard pc the software is available on the github https github com jzw787 fdpre tree master credit authors contribution statement zewei jiang conceptualization data curation writing original draft shihong yang funding acquisition writing review editing zhenyang liu software validation yi xu visualization writing review editing yujiang xiong software writing review editing suting qi methodology data curation qingqing pang methodology writing review editing junzeng xu data curation investigation fangping liu visualization tao xu validation supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the fundamental research funds for the central universities b220203009 jiangxi water conservancy science and technology project 202124zdkt09 the postgraduate research practice innovation program of jiangsu province kycx22 0669 the national natural science foundation of china 51879076 51709010 and central public interest scientific institution basal research fund for changjiang river scientific research institute cksf2019174 ny appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105436 
25555,accurate water level prediction is the premise of farmland waterlogging prediction a simple water level prediction model fdpre based on four machine learning ml algorithms and weather forecasts were developed the model can not only predict two key driving factors of waterlogging rainfall and node water level but also estimate disaster losses the results showed that the random forest and multiple perception model r2 ranged from 0 7180 to 0 9803 and 0 5717 to 0 9965 performed best in the case of flooding lasting for one day the economic loss of waterlogging under the 100 mm rainfall scenario 23 53 million dollars was much higher than that under the 50 mm rainfall 12 69 million dollars under the two rainfall scenarios the yield reduction rate in the lower reaches of the sihu basin was higher than that in the upper reaches the method of coupling ml and weather forecasts can well predict farmland waterlogging keywords farmland flood disaster forecast machine learning weather forecast fdpre model flood disaster loss 1 introduction in the context of climate change frequent extreme rainstorms and floods have caused huge economic losses zhang et al 2012 as one of the major agricultural disasters in humid and sub humid areas waterlogging affects 20 35 of agricultural soil in china yang et al 2017 the anoxic condition of waterlogging stress limited root respiration shaw and meyer 2015 affected the absorption of water and nutrients sairam et al 2008 and increased plant senescence hossain et al 2011 meanwhile the pesticides and chemical fertilizers that farmland waterlogging may carry also increase the risk of water pollution and soil salinization chen et al 2020 it has been found that waterlogging may lead to the reduction of crop yield by more than 32 9 tian et al 2021 rice feeds 60 of china s population and accounts for more than 20 of china s arable land according to a previous study yang et al 2015 the loss of rice production caused by waterlogging accounts for a quarter of all natural disaster losses in china every year in addition the yangtze river basin is the most serious waterlogging affected area in china chen et al 2018 therefore it is necessary to predict farmland flood disasters in the yangtze river basin anthropogenic climate change increases the risk of heavy storms and waterlogging pall et al 2011 water level forecasting is important for the prediction and mitigation of flood disasters sapitang et al 2020 precipitation is undoubtedly a crucial factor affecting the extent of flood disasters and the effects of human intervention such as pumping stations on drainage should also be considered however previous studies either focused on the impact of urban waterlogging liu et al 2020 real time prediction of the reservoir or river water levels nur adli zakaria et al 2021 or simulation of flood hydrographs kaya et al 2019 there are few reports on the early warning of farmland waterlogging and the prediction of water level on the gate after rainfall in addition the improvement of remote sensing satellite resolution and the development of meteorological science makes it easy to obtain solid and reliable weather forecasts hou et al 2014 kendon et al 2014 this also makes the prediction of farmland flood disasters more feasible given the time consuming and laborious drainage experiments at a regional scale tandy et al 2018 it is necessary to apply the modeling methods to study regional waterlogging many hydraulic models have been used in waterlogging simulation such as soil and water assessment swat wei and bailey 2019 model based and incremental knowledge engineering flood mike flood li et al 2018 and drainmod shaw et al 2013 however although those process based hydraulic models have been widely used there also have some limitations for example complex hydraulic models require high professional skills of users and river network zoning and model operation is time consuming which are difficult to be used for forecasting and early warning machine learning ml based on big data science and artificial intelligence can be used as an alternative and complementary approach to those models ml has been widely used in agriculture wolanin et al 2020 hydrology qiu et al 2017 zhang et al 2018 and other domains among them computational intelligence methods such as fuzzy systems neural networks and evolutionary algorithms have been introduced to flood prediction fotovatikhah et al 2018 and rainfall time series forecasting wu and chau 2013 moreover deep learning dl with a multi layer structure such as convolutional neural network cnn and recurrent neural network rnn has developed rapidly in recent years and has been successfully applied in crop yield prediction kamilaris and prenafeta boldú 2018 streamflow forecasting ha et al 2021 and so on li et al 2021 however the studies on ml application in waterlogging prediction mainly focus on image or text recognition of urban waterlogging liu et al 2021b or flood susceptibility mapping avand et al 2021b while the studies on farmland waterlogging prediction under artificial intervention are still limited for example cnn was used to extract urban waterlogging depth from video images to improve monitoring capability jiang et al 2020 ml is also used to detect and classify cotton leaves under waterlogging stress zhao et al 2021 although ml has been used to predict water level the prediction of the most unfavorable conditions after rainfall is less and it is not combined with the weather forecasts farmland waterlogging is affected by water level rainfall and human intervention hence we assumed that the ml algorithm the initial water level on the sluice and the drainage capacity of pumping stations can be used to predict the farmland waterlogging after rainfall in this study four ml models random forest rf multiple perception mlp cnn and long short term memory lstm were selected compared with the complex inputs of the hydrological model and high requirements for users these four models are relatively simple and have good performance in regression issues among them rf and mlp are two common ml models which are widely used in solving regression problems because they are not easy to overfit and have good approximation ability while as two typical representatives of deep learning dl cnn and lstm perform well in predicting nonlinear problems such as time series barzegar et al 2021 current research on flood disaster prediction is mainly concentrated in cities and there is little study on farmland waterlogging considering that extreme precipitation node water level and other disaster driving factors are the key indicators of flood disaster wu et al 2020 the underlying question is how to build a simple and accurate key node water level prediction method while obtaining rainfall prediction to the best of the authors knowledge this is the first study to combine the ml method and weather forecast in predicting farmland flood disasters in this study a typical rice planting plain sihu basin in the yangtze river basin was selected to study waterlogging since its flat terrain and well preserved historical monitoring data of waterlogging and hydrological it has always been a hotspot of waterlogging research yuan et al 2021 the objectives of this study are 1 to develop a software for predicting water level after rainfall based on ml and weather forecast 2 to verify whether the ml method can be used to predict the most dangerous situation after rainfall i e the highest water level on the sluice and determine the best ml model 3 to predict the spatial distribution of farmland flood disaster and the loss of yield reduction under different heavy rainfall scenarios 2 material and methods 2 1 study area description the study area is in sihu river basin 29 21 30 00 n 112 00 114 05 n jianghan plain which located in the middle reaches of the yangtze river hubei province of china it is adjacent to the yangtze river in the east south and west the hanjiang river and dongjing river in the north and the yizhang mountain area in the northwest there are four lakes namely changhu lake sha lake bailu lake and honghu lake fig 1 which are also the origin of the name of the sihu region the total area of this basin is 11547 5 km2 the study area is in a subtropical monsoon climate zone with an average annual net flow total precipitation evaporation and forest free period of 3 5 billion m3 1200 mm 1300 mm and 260 days respectively the local has an average annual air pressure of 1016 3 hpa a mean sunshine duration of 2000 h and an annual average temperature of 16 c the region has been suffering from flood disasters with about 70 of the rainfall during the flood season the location of the study area and the layout of the 15 main drainage pumping stations in the basin are shown in fig 1 the 15 stations are xijiakou xjk tianguan tg xinglong xl liuling ll gaochang gc xulisi xls pengjiahe pjh zhougou zg futian ft zibeiyuan zby xiaxinhe xxh xiaogang xg zhangdakou zdk xindi xd xintan xt 2 2 data collection 2 2 1 historical data in this study the historical hydrological data of 15 main drainage pumping stations in the sihu basin in the last ten years 2010 2020 were collected from the sihu engineering administration bureau of jingzhou city http www jzshglj com cn and the local hydrological bureau specifically the dataset includes the water level before rainfall wl0 rainfall the number of rain days tr pumping station flow fl and water level after rainfall wl among which wl is the output and other parameters are the inputs these input parameters were chosen because their observed and predicted values were easy to obtain and closely related to water level dynamics which can be easily extended to places with limited data zhu et al 2020 more complex meteorological data especially under future prediction are more difficult to obtain which might increase the uncertainty furthermore historical yield data 2019 of late rice in the sihu basin were obtained from the hubei provincial bureau of statistics http tjj hubei gov cn tjsj sjkscx tjnj qstjnj 2 2 2 weather forecast and cloud platform development openweathermap https openweathermap org is an external weather provider that provides a scientific yet simple approach api interface to forecast the weather including hourly weather maximum and minimum temperature air pressure wind speed precipitation and other meteorological variables every 3 h in the next 5 days colezea et al 2018 in this study a cloud platform that can automatically capture future weather forecasts is built and developed on alibaba could server ip address 39 105 36 150 the schematic diagram of the automatic cloud platform operation is shown in fig a1 after using a crawler to capture real time future weather forecasts the cloud platform automatically processes the data into comma separated value csv format and saves them in the cloud in addition we compared the local measured rainfall from september to october in 2019 with the predicted value fig a1 and found that the results reached a credible level r2 0 99 2 3 machine learning models 2 3 1 random forest rf rf is a bagging algorithm combining tree predictors breiman 2001 which has shown good performance in the previous research without over fitting data multiple classifications and regression trees cart were constructed randomly by taking bootstrap samples and the number of predictors for optimal segmentation was found at each node integrating different cart rf can solve the problems of classification and regression due to the advantages of fewer hyperparameters and parallel training rf has been widely used in ecology hydrology and agriculture prasad et al 2006 rodriguez galiano et al 2014 2 3 2 multiple perception mlp mlp network feedforwarded network is a popular artificial neural network ann that consists of interconnected neurons or nodes its basic form is a three layer structure including an input layer a hidden layer and an output layer hu and weng 2009 mlp has a strong ability to approximate any function park and jo 2016 so it performs well in solving regression problems the performance of mlp regression mlpr is mainly affected by the model structure activation function and connection weight updating mode tien bui et al 2015 mlp has been widely used in flood prediction due to its good generalization ability simple and nonlinear activation mosavi et al 2018 2 3 3 convolutional neural networks cnn cnn has strong feature representation power and has achieved great success in image classification and recognition cheng et al 2016 local connections shared weights pooling and using multiple layers are the basic principles of cnn when the softmax layer is replaced by a linear or sigmoid activated fully connected regression layer namely vanilla deep regression lathuiliere et al 2020 cnn can also be used to solve regression problems 2 3 4 long short term memory lstm lstm is essentially a type of recurrent neural network rnn lstm performs well in sequence learning by building a directed cycle to establish cell connections the memory cell of the hidden layer of lstm adds an input gate forget gate and output gate to adjust the state so it is good at overcoming the gradient disappearance and explosion of rnn fischer and krauss 2018 more details are available from a previous study zhao et al 2017 k fold cross validation method has been widely used in ml validation when comparing the prediction accuracy of two or more methods researchers usually divide the original data into k random subsets to minimize the bias related to the random sampling of training and holdout data samples delen et al 2005 in this study a commonly used 5 fold cross validation was carried out to ensure that the data appeared in training and test datasets as much as possible and avoid overfitting because other methods such as the leave one out were too computationally complex the ml models rf and mlpr were implemented using python 3 6 by sklearn package https scikit learn org the dl models cnn and lstm were implemented with pytorch 1 8 1 deep learning framework https pytorch org all the codes were written in python language on a laptop equipped with intel core i5 8300h cpu 2 30 ghz 8g ram and navid getforce gtx 1050 ti gpu 2 4 model inputs and evaluation criteria model inputs included wl0 rainfall tr and fl while the outputs were the wl after rainfall for each pumping station the inputs of rf and mlp models were a four dimensional wl0 rainfall tr and fl data frame with a total size of 1238 which contains the information on rainfall events in the past decade and the output was a vector wl according to the splitting ratio of 8 1 1 the training validation and test data size were 990 124 and 124 respectively when it comes to the cnn and lstm the input data set was also divided into those three parts and appropriate format conversion was also required before training with pytorch to meet the input requirements of these dl algorithms the filter size of cnn channels and minimum batch size were selected as 2 3 4 32 and 64 thus the input format of cnn is 64 1 4 1 where the first 1 represents the channel signal meeting the input requirements and the last 4 and 1 represent four dimensional variables and one dimensional output respectively through the three dimensional convolution filters 2 3 4 three characteristic matrices were obtained and spliced into the shape of 64 96 similarly the mini batch size and the hidden layer size of the lstm model were selected as 64 and a number in 256 356 respectively and the number of layers was set to 1 or 2 according to the optimization results thus a three dimensional 64 4 1 matrix was obtained the inputs also included a time series the output of the last time step was obtained each time as the result of a rainfall event all the model hyperparameters are adjusted by the grid search method and trial and error method table 1 to avoid reusing the weather conditions the model was trained using the historical rainfall events specifically the historical dataset was randomly subdivided into three subsets 80 for the training 10 for the validating and 10 for the testing when the trained model was applied for prediction the captured weather forecast was used the discussion of rainfall prediction accuracy can be found in section 3 2 3 three criteria were adopted to evaluate the performance of ml models root mean squared error rmse determine coefficient r2 and pearson correlation coefficient pearson r cueff et al 2021 petković et al 2015 these indicators were selected because they are simple and robust and are often used to compare ml models in different fields elzain et al 2022 they were calculated as follows 1 r m s e a i 1 n o i p i 2 n 2 r 2 i 1 n o i o a v g p i p a v g i 1 n o i o a v g 2 i 1 n p i p a v g 2 2 3 p e a r s o n r n i 1 n o i p i i 1 n o i i 1 n p i n i 1 n o i 2 i 1 n o i 2 n i 1 n p i 2 i 1 n p i 2 where o i represents the observed value p i is the predicted value n is the number of samples o avg and p avg represent the average observed and predicted value respectively the value range of r 2 and pearson r are both 0 to 1 previous studies suggested that the smaller rmse the closer r 2 and pearson r to 1 the better the performance of the model jiang et al 2021 the sensitivity index si was also used to evaluate the sensitivity of model inputs as follows 4a s i r p max r p min r p a v g p max p min p a v g where p max p min and p avg are the maximum minimum and average values of input parameters r pmax r pmin and r pavg are the corresponding simulation results respectively 2 5 scenario setting and loss assessment of farmland flood disaster considering that the extreme value of local rainfall in the past decade is about 106 mm two scenarios were set to evaluate the loss of farmland flood disaster s1 conventional rainstorm i e 50 mm rainfall in one day according to the china meteorological administration http www cma gov cn s2 95 of the extreme rainfall value 100 mm rainfall in one day the scenario settings refer to previous studies hu et al 2019 siswanto et al 2016 tr was 1 day wl0 was set as the most frequent water level in the history of each station given the heavy rain in the scenario simulation fl was set as the maximum drainage flow of the pumping station tab a1 to quantify the impact of the assumption a sensitivity analysis was performed and can be found in section 3 4 given that the local area is a flat polder and the drainage capacity of the pumping station has been considered in the predicted wl we assumed that the increased value of the water level of the sluice before and after rainfall is approximately equal to the water depth of the local farmland under the condition of high water level yield reduction rate y is calculated according to a previous study xiong et al 2018 as follows eqs 4 and 5 the reason for using this formula is that it was derived from waterlogging test data rather than model simulation and has the coefficient of hubei province where this study is located in addition its reliability has been demonstrated in a previous research chen et al 2020 4b y 0 h h c a h w h r b t c h h c 5 h r 1 0 0059 d 2 1 4518 d 15 09 100 where hc hw and hr are the critical waterlogging depth m waterlogging depth m and rice height m respectively t is the accumulated waterlogging time d a b and c are empirical parameters d is the day after sowing the jointing and booting stage is one of the most serious periods of rice yield reduction caused by waterlogging when hc a b and c are 0 2m 36 909 2 084 and 0 437 respectively xiong et al 2018 to simplify the study we selected the local late rice with the highest risk of waterlogging assumed that the heavy rain occurred during the jointing and booting period and last for 1 day hr was 0 86 m and calculated the yield losses in different regions of the basin 3 results and discussion 3 1 the development of fdpre model to combine machine learning with the weather forecasts to predict farmland waterlogging in real time the software fdpre was developed the main interface and workflow of the model are shown in fig 2 the fdpre model mainly consists of three modules a water level prediction b crawler acquisition of weather forecast and c interactive chart drawing four ml algorithms namely rf mlpr cnn and lstm were embedded in the fdpre model on the one hand model users can select typical sluice stations or build new ones import historical hydrological data for ml training and then predict the maximum water level on the sluice after rainfall and draw interactive charts on the other hand the model can automatically obtain real time rainfall forecasts through a built in crawler program and import the rainfall into the water level prediction module for the forecast the model can be downloaded from github https github com jzw787 fdpre tree master detailed instruction for the model is provided in the supplementary material 3 2 the performance of fdpre 3 2 1 comparison of the performance of different machine learning models figs 3 and 4 show the comparison of the observed and predicted wl values obtained by fdrpe at 15 stations during the training and test stages overall the wl values predicted by ml are distributed near the 1 1 line it is illustrated that predicting wl by ml methods is feasible but the performance of the model is different in each station tables 2 and 3 evaluate the performance of the four ml models at 15 stations during the training stage and test stage the coloring principle is that the higher r2 and pearson r the smaller rmse and the redder tint the lower r2 and pearson r the larger rmse and the bluer tint according to the average r2 of four ml models during the training and test stages xd station was the best 0 8024 0 9664 xl station was the second 0 8100 0 9354 and ft station was the worst 0 6428 0 8119 in terms of rmse zdk performed best 0 0139 0 0445 followed by xd 0 0108 0 0653 and xt was the worst 0 1496 0 4252 the performance of the model at xd xl xjk zg xxh and zdk is better than at other stations in terms of the performance of the four ml models rf and mlpr generally performed better than cnn and lstm tables 2 and 3 the r2 ranges of rf mlpr cnn and lstm were 0 7180 0 9803 0 5717 to 0 9965 0 5566 0 9080 and 0 4457 0 9563 pearson r were 0 8201 0 9955 0 8373 0 9983 0 7469 0 9707 and 0 6209 0 9872 respectively the rmse ranges of rf mlpr cnn and lstm were 0 0073 0 4022 m 0 0010 0 3453 m 0 0260 0 3912 m and 0 0185 0 6552 m respectively the previous study usually reported dl models such as cnn and lstm outperformed traditional ml models wolanin et al 2020 however although lstm performed best at some stations in the test stage such as xl and ll the traditional rf model is generally more stable this may be due to the limited historical sub rainfall data and the low dimension of input variables considering that lstm performed better than rf in processing mutation data points and data with relatively high uncertainty fu et al 2020 mu et al 2020 xl had wl which was much higher than the average and the data distribution of the ll station was not very uniform this result was understandable in addition dl models such as cnn and rnn are better in the area of image recognition and classification and traditional ml models may perform better than dl models in solving regression problems sirsat et al 2018 specifically during the training stage rf attained the best performance at xd among all models with r2 pearson r and rmse of 0 9764 0 9892 and 0 0073 m respectively during the test stage mlpr attained the best performance at xxh with r2 of 0 9965 pearson r of 0 9983 and rmse of 0 0010 m respectively while cnn obtained the worst performance at xg with r2 pearson r and rmse of 0 5801 0 8143 and 0 1207 respectively according to the model performance in training and test stages ml models can be ranked as follows rf mlpr lstm cnn it is worth noting that mlpr outperforms rf in tg zg xxh and xg stations therefore mlpr is recommended for predicting water levels at tg zg xxh and xg stations while the rf model is recommended for other stations 3 2 2 comparison of computing costs of different models another evaluating criterion that cannot be ignored is the computing cost the calculation time of all models in training and test phases is presented in tables 2 and 3 the computing time does not include the time of grid search and parameter adjustment but includes the time of k fold cross validation during the training stage the computation time of rf is the least 0 1249 0 6075 s among all ml models it is understandable because compared with neural networks rf integrates simple but efficient decision trees therefore the operation time of rf is generally much less than that of dl models which is similar to previous studies hamrani et al 2020 given the calculation time and accuracy of rf it is a good choice to apply the rf model to predict wl values the training time of lstm is much longer than that of other models which is related to the optimized model structure it has been found that due to its recurrent nature lstm is difficult to adjust and the calculation cost is high bozheniuk et al 2020 it is worth noting that the expensive operation cost of ml models is only in the training stage as can be seen from table 3 the computing time of dl models in the test stage decreases rapidly even lower than that of traditional ml models in the test stage the calculation time of cnn and lstm ranged from 0 0003 s to 0 0312 s and 0 0015 s 0 0625 s respectively while the calculation time of rf and mlpr ranged from 0 0961 s to 0 2187 s and 0 0639 s 4 0633 s respectively 3 2 3 real time predictions in addition to verifying the model performance with historical rainfall events mentioned above another attempt is to apply the model to predict real time water levels four water levels meters have been installed which can automatically monitor and send data locally and released the ml based water level prediction results in real time through the programs deployed on alicloud servers fig a3 due to the limited installation time of the water level meters the flood disaster has not been detected nevertheless through the real time acquisition of wl and weather forecast the prediction and release of future water levels are realized the actual operation results are shown in fig 5 it can be found that the predicted water level was consistent with the measured real time water level with r2 ranging from 0 85 to 0 96 in addition the database of real time water level prediction was constructed and the early warning message of waterlogging risk could be sent to locals fig a4 since there is no need to repeat the training stage the process of real time prediction could be completed within the 30s 3 3 evaluation of flood disaster in sihu region according to the above results the rf model was applied in tg zg xxh and xg stations and the mlpr model was used in other stations to predict wl value fdpre model was applied to simulate the two scenarios as shown in fig 6 the spatial distribution of wl field water depth yield reduction rate and economic loss under the s1 scenario was further obtained under the condition of pumping station drainage at the maximum flow rate the maximum gate water level and field surface water depth at each station after rain ranged from 24 02 to 37 58 m and 0 1 31 m respectively among them the highest wl value of xl upstream was the highest 37 58 m and the wl value of xt downstream was the lowest 24 02 m in addition the field water level of the ft station was the highest while xt and xls were the lowest accordingly we calculated the rice yield reduction rate under the predicted water level fig 6c and estimated the yield loss fig 6d overall the yield reduction rate in the lower reaches of the sihu basin was higher than that in the upper reaches using ml and game theory avand et al 2021a have found that the downstream and along riverbanks were areas with high flood risks which was basically consistent with this study a previous study chen et al 2020 applied a hydrological model to predict the risk of rice waterlogging disaster in gaoyou jiangsu and found that at 327 6 mm of rainfall over 10 days the yield reduction rate in most places was less than 4 the yield reduction rate obtained in this study was higher which may be due to higher rainfall 50 mm and 100 mm per day and lower pumping station density in the scenario simulation of this study the area of gaoyou irrigation district 20 27 km2 is much smaller than that of sihu basin but there are four drainage pumping stations yield loss was calculated based on the yield reduction rate and local late rice yield in 2019 the areas with the highest yield loss more than 4000 t were xxh zg and ft although the yield reduction rates of pjh and xjk were only 3 68 and 4 15 respectively the total yield loss was larger 1486 42 t and 858 36 t due to the high rice yield according to the unit price of 2 5 yuan per kilogram of rice it is roughly estimated that the economic loss of farmland inundation for 1 day and 2 days are 12 69 and 17 19 million us dollars similarly the case of high precipitation 100 mm a day was also studied fig 7 in this scenario the wl values of xl and xt were 23 93 m and 38 06 m respectively which are also the maximum and minimum values of wl compared with low precipitation the water level of farmland in many areas has risen such as xxh xl gc and pjh the field water levels of tg zg ft zby and xxh were the highest ranging from 1 01 m to 1 64 m in addition high precipitation significantly increased the yield reduction rate of rice this may be attributed to the limited local drainage capacity and the backwater effects of high drainage ditch level le et al 2007 that is the accumulated water in the field exceeds the ridge depth and flush with the water level of the drainage ditch in heavy rain conditions which may lead to flood recharge and be conducive to waterlogging for example the reduction rate of xl increased from 1 40 to 11 78 and that of gc increased from 7 43 to 34 60 fig 7c due to the higher yield reduction rate there are five areas with a yield loss of more than 10000 t namely xl pjh zg ft and xxh based on the above data it can be estimated that under the condition of 100 mm rainfall in one day the economic loss of farmland inundation for 1 day and 2 days are 23 53 million and 31 86 million us dollars respectively a previous study found that providing flood warning measures is an effective measure to improve the ability of disaster resilience zhang et al 2021 given that rice cultivation requires a lot of water and must be planted near the river it is necessary to timely give early warming based on weather forecasts and adjust the flow of the pump station and plant more rice in areas with less risk of waterlogging in the future 3 4 sensitivity analysis in this study sensitivity analysis was also conducted to quantify the effects of input parameters fig 8 overall the sensitivities of the four ml models were relatively close among which cnn was the most sensitive to rainfall rainfall days and flow lstm was the most sensitive to wl0 which is probably due to the difference in model algorithm and input structure it can be observed that the initial wl0 has a significant effect on the model input with si ranges from 0 56 to 0 69 which is understandable because the wl0 used in this study is the elevation value of the water level and contains the altitude information of the station noticeably the si of rainfall and rainfall days were 0 16 0 27 and 0 17 0 21 respectively which was basically the same as a previous study wu et al 2020 this may be because the rainfall in this study was the cumulative values and the pumping stations were generally opened for flood discharge when rainfall occurred in history so the sensitivity is not particularly high in addition the si of the pumping station flow to the wl was between 0 11 and 0 21 which indicates that the drainage of the pumping station promotes the decline of the water level 3 5 limitations prospects and uncertainty analysis this study provides a simple method based on ml and weather forecasts to predict farmland waterlogging disasters after rain simple input parameters easily obtained prediction results are undoubtedly the advantages of the fdpre model the results of the model have reached a credible level in the study area however the current model still has some limitations the purpose of this study is to propose a simple warning approach for farmland waterlogging some simplifications are made for example there is a certain coefficient relationship between the change of water level on the sluice and the depth of farmland waterlogging but we simplified it to 1 it is necessary to calibrate in future research in addition the meteorological forecast has a great influence on the results since the local terrain is flat and belongs to the polder area we assume that the water level in the control range of the pumping station is the same when the flood disaster occurs further development of waterlogging prediction systems based on platforms such as google earth engine gee is promising in the future it is necessary to combine ml methods with gis platforms and remote sensing assimilation to predict farmland waterlogging the plant height and submergence depth of rice is based on an empirical formula so the yield reduction rate and economic loss may be overestimated another limitation is the interpretability of ml methods how to further mine the relationship between variables from the regression relationship obtained by ml algorithms is the direction we need to pay attention to in future research in addition the combination of the hydrological model and ml algorithm may be the direction of future research a large number of data generated from the hydrological model can be used for ml training or the relationship obtained from the ml algorithm can be added to the hydrological model since the purpose of this study is to develop a simple but accurate wl prediction model based on ml and weather forecast while gee and hydrological models are more complex and need more input parameters we will conduct related research in the future nevertheless the prediction method of farmland waterlogging proposed in this study can provide some guidance for disaster early warning and loss assessment in addition we have constructed a runoff generation and concentration model about 1333 ha in the sihu basin in another previous study liu et al 2021a which has been verified that it can also be used for flood prediction however it requires many parameters which is difficult to obtain when applied in other places and requires high operating conditions another problem to be solved in the following study is to predict the retention time of waterlogging which needs to be carried out based on analyzing topography hydrology and climate currently it is judged by real time prediction of water level changes in the next few hours it should not be ignored that the accuracy of weather forecasts could influence the model performance thus the predicted weather data and the observation were also compared fig a5 it can be observed that the captured 3 h rainfall prediction data and the measured values have reached a good fitting level rmse 1 28 mm and the prediction effect of weather variables decreased with the increase of prediction time moreover the effects of lead time on the accuracy of the model were studied in short considering that it is difficult to accurately predict rainfall for a long time we compared the wl prediction results obtained by using the rainfall prediction data of 3h 6h and 24h in the future fig a6 overall the results of using 3 h weather forecast data were the most accurate with the r2 of 0 85 while the results of 6 h and 24 h dropped to 0 70 and 0 61 respectively therefore the wl prediction value obtained from the short term rainfall forecasts is more reliable previous studies kang et al 2016 rahmat et al 2018 have also noticed that openweathermap performs well in predicting meteorological factors such as the short term rainfall which was consistent with this study future research can also consider using ml methods such as the lower upper bound estimation for interval prediction taormina and chau 2015 4 conclusion fdpre a simple model for predicting water level after rain based on ml and weather forecasts was developed in this study based on ten years observation data four ml algorithms were used to establish the relationship between the maximum water level and initial water level rainfall rainfall days and drainage flow of drainage pumping stations in the sihu basin the model has built in api access to future weather forecasts for water level prediction the results of model training and test show that ml can accurately predict farmland waterlogging at 15 stations in general rf and mlpr performed better than cnn and lstm although lstm performed best at some stations overall rf has higher r2 lower rmse and much less computing cost however dl models only consumed a lot of computing time in the training stage and the computing time in the test period was even lower than ml models the rice yield reduction rate and economic loss under the s2 scenario were much higher than those under s1 in general the input parameters required by the fdpre model are simple and the results are accurate future research should pay more attention to the relationship between the depth of farmland flooding and the change of water level on the sluice to better predict the farmland waterlogging disaster and assess the loss in addition coupling ml and weather forecasts to issue early warnings in time adjusting the pumping station flow and rice planting layout are necessary to reduce waterlogging loss 4 1 software availability the model was developed by hohai university using python language this system can be run on a standard pc the software is available on the github https github com jzw787 fdpre tree master credit authors contribution statement zewei jiang conceptualization data curation writing original draft shihong yang funding acquisition writing review editing zhenyang liu software validation yi xu visualization writing review editing yujiang xiong software writing review editing suting qi methodology data curation qingqing pang methodology writing review editing junzeng xu data curation investigation fangping liu visualization tao xu validation supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the fundamental research funds for the central universities b220203009 jiangxi water conservancy science and technology project 202124zdkt09 the postgraduate research practice innovation program of jiangsu province kycx22 0669 the national natural science foundation of china 51879076 51709010 and central public interest scientific institution basal research fund for changjiang river scientific research institute cksf2019174 ny appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105436 
25556,e planner is a free web based application which provides land managers with fine scale maps of the suitability of agricultural land in great britain for environmental enhancement e planner is designed to streamline decision making around the choice and spatial targeting of environmental management interventions suitability scores are calculated by integrating a range of biophysical data sets and presented as easy to interpret maps at fine resolutions 5 m equivalent to those used by precision agriculture technology the information provided by e planner is important for landscape to field scale spatial targeting of farm management to maximise the efficiency of both crop production and environmental delivery whilst many datasets and tools support the former equivalents for environmental factors are not widely available the methods used by e planner for collating and presenting data on environmental constraints and drivers are widely applicable and efficient spatial targeting of agronomic and environmental management forms an essential step towards sustainable agriculture a global issue keywords sustainable agriculture decision support fine resolution precision agriculture agri environment 1 introduction 1 1 sustainable agriculture the challenge of an ever increasing global demand for sufficient nutritious food has historically been met by the expansion and intensification of agricultural systems foley et al 2005 pretty 2007 lambin and meyfroidt 2011 stevenson et al 2013 however this has come at the cost of widespread declines in environmental quality including biodiversity firbank et al 2008 henle et al 2008 kleijn et al 2009 water quality moss 2008 mateo sagasta et al 2018 air quality bauer et al 2016 giannadaki et al 2018 and soil health jones et al 2013 smith et al 2016 these declines affect the ecosystem services provided by the environment mace et al 2012 on which agricultural production itself depends tscharntke et al 2005 emmerson et al 2016 with global food demand continuing to increase godfray et al 2010 agriculture must remain productive but must also become more environmentally sustainable the goals of keeping agriculture productive whilst reducing its environmental footprint are not mutually exclusive actions which deliver environmental benefits can have beneficial effects on agricultural production and vice versa examples include the spill over of services delivered by insects e g pollination natural pest control from non crop habitats woodcock et al 2016 the positive effects of increasing soil organic matter on both carbon sequestration and crop yields dexter et al 2008 johnston et al 2009 jensen et al 2019 or the increased resistance to extreme weather events conferred by biodiverse habitats within agricultural landscapes redhead et al 2020 these findings have led to such concepts as ecological intensification bommarco et al 2013 kleijn et al 2019 sustainable intensification struik and kuyper 2017 agroecology wezel et al 2020 and regenerative agriculture lacanne and lundgren 2018 these vary in their precise definitions but all aim to achieve simultaneous increases in the productivity and long term viability of agriculture and reductions in its environmental impacts achieving this is challenging because the same land must at some scale be used to deliver multiple outcomes and because these outcomes are often highly context dependent success thus requires a high level of farmer engagement and knowledge the ability to balance farming for yield vs farming for nature de snoo et al 2013 and the capacity to make informed decisions at fine spatial scales one existing mechanism for the delivery of sustainable agriculture is agri environment schemes aes these offer financial incentives to offset costs or loss of production to farmers for specific environmentally beneficial management actions often referred to as aes options aes have been established in many countries for decades but their effectiveness in delivering their environmental goals is often inconsistent kleijn and sutherland 2003 kleijn et al 2006 2011 the chances of an individual aes option delivering the environmental benefits for which it was conceived vary widely with the quality of implementation and management these in turn depend on the level of experience knowledge and engagement of the farmer lobley et al 2013 mccracken et al 2015 there is evidence that providing basic training and easily accessible information significantly increases the success of aes options lobley et al 2013 mccracken et al 2015 1 2 decision support tools for sustainable agriculture one frequently identified potential solution to the challenges of complex spatial planning required by sustainable agriculture and aes is the use of agricultural decision support systems adss doré et al 2011 francis et al 2017 lindblom et al 2017 zhai et al 2020 the definition of adss is usually broad enough to include any computer based system for collating analysing and displaying data in a way that supports farm decision making zhai et al 2020 talari et al 2021 however advances in remote sensing and information technology mean that effective modern adss are expected to support dynamic data e g real time data on climate or crop condition the ability to interactively explore alternative management options via what if modelling and intuitive multi scale graphical user interfaces often supported by open webgis terribile et al 2015 a key development that has driven the development of many recent adss is the uptake of precision agriculture precision agriculture implies the use of technology to collect and supply data on spatial and temporal variability in agricultural production on which to base agricultural management decisions bongiovanni and lowenberg deboer 2004 mcbratney et al 2005 the fine spatial and temporal resolution of precision agriculture data enables targeting and application of management at the sub field scale however this creates very large volumes of data that must be presented in a way that farmers can access and interpret to inform management decisions rose et al 2018b hence the need for adss a functional adss is likely to be composed of several interlinked tools and services providing specific functionality e g manna et al 2020 bancheri et al 2022 very large numbers of adss and their component tools are now available rose et al 2016 zhai et al 2020 ranging from proprietary software for integrating data from particular precision agricultural sensors examples in zhai et al 2020 to comprehensive web based openly accessible adss platforms e g terribile et al 2015 many of these tools and systems support sustainable agriculture in the sense that they reduce waste improve the targeting of actions with negative environmental consequences bongiovanni and lowenberg deboer 2004 lindblom et al 2017 rose et al 2018a or explore the consequences of management actions for natural resources such as soils terribile et al 2015 and groundwater bancheri et al 2022 however few tools within existing adss directly address the planning and management of actions intended to promote specific environmental benefits e g aes options where tools do address environmental outcomes directly they tend to operate at the whole farm level e g hillier et al 2011 gooday et al 2014 whilst these are helpful in suggesting which environmental management actions a farmer may wish to undertake they do not operate at the same fine spatial resolution as precision agricultural data nor help to spatially target actions within the farm or field so on the one hand farmers are increasingly used to having access to agronomic data and tools indicating which areas of land are least agriculturally productive and may thus be better suited to an alternative environmentally focussed management or most productive and thus require additional support from environmentally focussed management that may help to bolster resilience on the other hand they do not currently have the equivalent data and tools to support decisions on which management actions might be best suited to such areas or to identify the most beneficial locations for particular environmental management actions 1 3 aims we created the e planner tool to address this gap by providing data on the suitability of land for different environmental management actions at the fine spatial resolutions typically associated with precision agricultural data whilst covering the full extent of agricultural land in great britain gb e planner is a free web based tool that presents complex environmental data as easy to interpret webgis maps e planner presents static information on long term drivers of environmental potential and thus does not contain the dynamic elements of a full adss this is partly because even the shortest term aes options remain in place for at least a year so compared to agronomic variables there is less requirement for real time updates on current conditions or the need to quickly explore potential responses we also avoid duplicating functionality from existing adss e g ability to digitize and export management zones to keep the tool interface simple and avoid apparent competition instead we aimed to produce a free tool that can either be used as a standalone or by producing mapped outputs accessible via web services integrated with existing adss lack of cost ease of use and presentation of visual outputs have all repeatedly been identified as important drivers of uptake in agricultural decision support tools see reviews in rose et al 2016 lindblom et al 2017 rose and bruce 2018 e planner is intended to enhance not replace local knowledge and field surveys fig 1 and therefore does not attempt to identify an optimal solution but instead focusses on providing easy to interpret contextual data to inform the decision making process a goal for decision support tools that generally receives greater support from farmers rose et al 2018a 2018b although e planner has been designed to work in tandem with precision agricultural data or adss by presenting interoperable outputs at similar resolutions and in similar ways such data are not a prerequisite and e planner can be used independently based on local knowledge 2 methods 2 1 mapping suitability of land for environmental opportunities the e planner tool provides freely accessible and easy to interpret maps representing the suitability of land for a range of environmental opportunities fig 1 we define environmental opportunities as suites of potential actions aiming to deliver an environmentally beneficial environmental outcome by changing the management of farmed land or taking it out of production each opportunity has a number of ways in which it could be implemented e g aes options depending on the farming system and local context for example the environmental opportunity of creation of flower rich pollinator habitat might be achieved by sowing of annual pollen and nectar mixes in intensive arable systems the creation of perennial wildflower areas where land can be removed from production longer term or the restoration of species rich grassland in pasture systems we did not attempt to model specific outcomes from implementing the environmental opportunity e g ecosystem services which would be highly dependent on local context quality of implementation and other factors which are difficult to quantify at fine resolution across national extents and which interact in complex ways for example crop pollination service is dependent on complex interactions between landscape and the composition of the local pollinator community ricketts et al 2008 senapathi et al 2015 which is itself dependant the quality and extent of pollinator habitats in the landscape kennedy et al 2013 senapathi et al 2017 instead e planner presents maps of relative suitability assessed from combinations of biophysical variables which affect i the likelihood that a given opportunity can be implemented ii increase its potential for delivery or iii decrease the chance for unintended detrimental effects these variables were identified from academic papers e g cane 1991 maher et al 2019 published guidance e g woodland trust 2015 nowakowski and pywell 2016 rothero et al 2016 aes handbooks e g natural england 2018 and consultations with experts in the field of agricultural habitat creation restoration and management once identified and derived from a suitable spatial data source see section 2 2 each of these variables was then scaled to a range of 0 1 and the relevant scaled variables summed to produce each suitability map thus giving each variable equal weighting the final suitability maps are then rescaled by the web application so that the user is presented with simple heat maps ranging from values 0 least suitable to 1 most suitable for their chosen area of interest this approach makes it very simple to add or substitute variables when new datasets become available or new research suggests their importance all suitability maps presented by e planner are 5 m resolution rasters which is sufficiently fine scale to target habitat management actions at the sub field scale e g many aes arable field margins in gb are 6 12 m in width and similar to the resolutions commonly produced by precision agricultural data and tools as discussed above it is important that e planner uses a similar resolution and presentation of the data to existing precision adss because e planner is intended to work in tandem with such software where available this integration will help identify the areas where changing management to deliver environmental benefits would have minimal neutral or even beneficial impacts on productivity the visualisation of the suitability maps and user interface were designed using an iterative co design approach with repeated consultations with focus groups of farmers agronomists and farm advisers since the incorporation of user feedback is critical to delivering a successful agricultural decision support tool rose et al 2016 lindblom et al 2017 rose and bruce 2018 rose et al 2018a 2018b the e planner user guidance places a strong emphasis on the importance of practitioners sense checking or ground truthing the results by local site inspection fig 1 as local factors e g disturbance site access pest pressure visual impact are likely to affect suitability in ways that cannot be readily predicted by e planner this is also important in selecting exactly which management method is best suited to implementing an environmental opportunity in a given area for example the wet grassland for which e planner maps suitability for restoration see table 1 covers both floodplain floristically diverse meadows and grazed wet grassland suitable for waders and wildfowl the former are much more restricted in the range of environmental conditions under which they can be successfully established so only field data and local knowledge can distinguish exactly which management to implement within an area estimated as highly suitable for an overarching environmental opportunity 2 2 sourcing and processing input spatial data suitability maps were created for five environmental opportunities the variables used in the calculations of suitability and the potential different management actions for implementing each opportunity are listed in table 1 existing high quality wildlife habitats were also deliberately excluded from the e planner maps as they are not appropriate for some management activities and these practices may cause unintended damage for each biophysical variable identified as being indicative of suitability as listed in table 1 we identified a source spatial dataset spatial datasets had to have gb wide extent or equivalent national extent datasets for england scotland and wales and sufficiently fine resolution to indicate relative differences in suitability at the within field scale i e raster resolutions of 50 m 50 m or finer or vector data mapped at 1 50000 scale or finer source datasets are listed in table 2 in the majority of cases spatial datasets required further processing to derive the variables required for the suitability maps regardless of the format vector or raster and resolution of the data sources used to create the spatial datasets listed in table 2 they were all generated as collections of 10 km 10 km raster tiles at 5 m resolution splitting these large high resolution datasets into 10 km tiles made them easier to handle and amenable to parallel processing techniques for many variables the value at any given 5 m pixel depended on the values of pixels elsewhere in the landscape e g assessments of proximity and connectivity so to avoid edge effects tiles were buffered e planner also incorporates information on national level priority of the different environmental opportunities to help place the user defined area in its wider context inform decisions about which opportunities to prioritise and to break ties if areas are equally well suited to different opportunities these data are derived from a centile based classification low 50th centile medium 50th 75th centile high 75 centile of a single data layer for each opportunity summarised in table 3 these are then used to present a traffic light indicator of national priority for each map in the tool interface 2 3 e planner construction and technical infrastructure all handling and pre processing of spatial datasets was performed in r r core team 2019 making use of the raster hijmans 2020 sp pebesma 2018 insol corripio 2021 fasterize ross 2020 nabor elseberg et al 2012 and whitebox wu 2020 packages for a full description of how variables were derived from source data including relevant formulae r functions and packages see supplementary material appendix a the jasmin lotus batch and parallel processing cluster was used to carry out the processing with each job working on a single 10 km tile jasmin is a national data computing facility for the environmental sciences community operated by the science technology facilities council stfc on behalf of the natural environment research council nerc it provides large scale storage 45 pb and processing 12 000 cores to enable data intensive environmental science running an operation on one of the datasets typically involved running a batch of approximately 2 800 jobs one per 10 km tile all of which could potentially be run in parallel the tiled opportunity map data then reside on the uk centre of ecology and hydrology ukceh storage area network san this file system sits behind ukceh s firewall but is accessible to a kubernetes cluster hosted by ukceh the e planner tool is then comprised of three services deployed in docker containers running on the kubernetes cluster these are 1 the web application and associated interface 2 the service providing the opportunity maps to the web application 3 a web mapping service wms serving os mastermap field boundary data to the web application each of these is described below and illustrated in fig 2 firstly the e planner web application was written in javascript with the main architecture provided by the react framework and the mobile first graphical user interface gui components provided by the ionic framework the ionic gui components provide a responsive interface that works well on both large computer screens and small mobile devices the mapping functionality is implemented using the leaflet framework with basemaps provided by openstreetmap e planner is best thought of as a web app but the frameworks employed give the mobile user an experience close to that of a native app as a web app built with javascript the majority of the code runs in the user s browser however the web app does not itself host any of the data presented to the user e g the opportunity maps these are provided to the app via the second backend service with which the web app communicates over standard http protocols to view an opportunity map layer a user must first specify an area of interest which can be done in one of three ways dragging a box over the basemap uploading a shapefile or by entering a single business identifier sbi an sbi is a code that uniquely identifies a farm to the english rural payments agency rpa if a user enters an sbi the web app contacts an external application programming interface api provided by the rpa and retrieves the boundary for the farm once an area of interest has been identified the user can request one or more opportunity maps for that area and the web app sends a series of http requests to the opportunity map service this second service runs r code behind a rest api implemented in plumber the service connects to the file system hosting the 10 km tiles and retrieves any required to cover the area specified in the rest request from the web app the tiles are then mosaicked and clipped to the area of interest specified in the rest call at this point the service is holding in memory a single band raster for the specified opportunity layers covering the area of interest specified by the user this raster is then re scaled over the area of interest before being converted to a colour image using a palette specified in the rest call and streamed back to the web app which displays it over the leaflet osm basemap the local scaling of the opportunity maps carried out by the opportunity map service is important to highlight the relative importance of different areas within a single farm in terms of their opportunity for the different biodiversity interventions so a farm with very little land that is suitable for a given opportunity will still show the areas that are relatively best suited to it and a farm where all the land is well suited to a given opportunity will still highlight the least well suited areas this avoids situations where the entirety of a selected area might be assigned uniformly intermediate suitability but does mean that the maps should be always be interpreted as comparing relative suitability for the chosen user defined area the relative suitability of the farm to its surroundings can be assessed by the user selecting a wider area for comparison and using the national scale priority indicators this need for local scaling requires processing by the r code in the service because this service is relatively slow there is a danger that calls to the service could back up and impact performance the kubernetes platform used to host this service is important in mitigating this risk kubernetes enables replicas of docker containers to be run concurrently and routes calls to the service to any of the replicas that are available the third service is required because the basemap osm does not include field boundaries which are important for interpretation of maps in agricultural contexts these are provided via a wms created from field boundaries derived under license from ordnance survey mastermap and displayed over the opportunity map layer a glossary of software and hardware terminology and brief description of key functions is provided in appendix b 2 4 validation on study farms in addition to iterative feedback from potential users throughout e planner s development we also undertook a formal validation exercise for this two large farms 222 and 276 ha mixed dairy and arable enterprises in the south of england were used these farms were not used for the initial development and testing of e planner for each farm we split each suitability map into five zones using suitability score intervals of 0 2 0 4 0 6 and 0 8 with zone 1 being least suitable i e a score of 0 0 2 and zone 5 being most suitable i e a score of 0 8 1 0 an independent surveyor with expertise in farm environmental land management planning then visited at least one site within each zone for each opportunity five zones x five environmental opportunities 25 sites per farm the surveyor assigned and recorded a suitability score to each location based on their own experience and expertise 1 least suitable 5 most suitable the surveyor was instructed to base their scoring on the potential of the location to deliver the relevant environmental opportunity regardless of current management the surveyor also recorded the reason for their score including local factors that modified the basic suitability of a site e g disturbance pest pressure farm access visual impact but for which we did not have national extent data and which do not thus contribute to the suitability scores produced by e planner the surveyor could also record scores for additional sites e g those they saw as being particularly highly suitable or unsuitable for a particular opportunity 3 results 3 1 the e planner tool and interface the e planner tool can be accessed via a web browser on a desktop or mobile device at https assist e planner ceh ac uk although the exact appearance of e planner varies depending on the device and browser used for access functionality remains virtually unchanged the e planner site consists of four main pages about e planner user guide e planner tool and next steps the first of these gives the background to e planner and a brief summary of how the tool works and the suggested user workflow see fig 1 the second outlines the functions of the various controls in the tool and how to interpret the output maps the next steps page provides links to existing best practice guidance on how to implement and manage the different opportunities and to some sources of potential agricultural subsidies for doing so i e aes the tool is split over two tabs the first of these the select tab fig 3 controls user defined selection of an area of interest for which to show the suitability maps referred to as opportunity maps in the e planner interface with the options to draw an area of interest upload one in shapefile format or use the rural payments agency land data api to access the field boundaries of a single farm based on the user inputting a unique farm identifier sbi the user can zoom or pan around the map to make their selection and can remove an area of interest the opportunity maps to be displayed up to four at once and transparency and colour palette for visualisation of the maps can also be chosen on this page before viewing the maps once the desired area is chosen the user selects that they wish to load the opportunity maps attempting to retrieve the maps for a selected area over 25 km2 in area will display a message requesting that a smaller area is chosen to prevent very high volumes of data being retrieved and ensuring rapid and reliable running of the tool for multiple users loading the maps will automatically switch to the second tab the opportunities tab fig 4 this presents the opportunity maps and allows the user to explore them by comparing suitability within and between maps the user can make decisions about which areas to prioritise for further investigation in the field narrow down the list of opportunities to consider for a given location or identify the most suitable uses for areas already under consideration fig 5 illustrates the interpretation of the maps for some of these potential uses 3 2 validation on study farms agreement between the scores derived from e planner scores and field based scores assigned by the expert surveyor was generally high spearman s correlation coefficient 0 80 p 0 01 for both farms combined this was especially so for the most and least suitable areas with more variation around intermediate scores fig 6 a agreement between the scores did not appear to differ greatly between the two farms fig 6b and c spearman s correlation coefficients 0 78 and 0 82 respectively p 0 01 for both the different environmental opportunities showed some variation in agreement between scores fig 7 agreement was generally high for all but it was notably stronger for wet grassland creation and weaker for winter bird food spearman s correlation coefficients flower rich pollinator habitat 0 77 water resource protection 0 80 wet grassland restoration 0 97 winter bird food 0 54 woodland creation 0 83 however it is important to note that where there were discrepancies between scores e planner generally tended to underestimate suitability for all opportunities i e most points are on or above the 1 1 line on fig 7 including for winter bird food rather than assigning misleadingly high suitability scores to unsuitable areas the most common factors recorded by the surveyor as reducing the field assigned score of otherwise suitable areas were the possible effects of wildlife disturbance from roads and footpaths the e planner suitability scores also did not always reflect the presence of wet ditches which would make good potential candidates for water resource protection because these are not consistently mapped in the national watercourse data that e planner uses 4 discussion 4 1 use and limitations of e planner e planner was launched in june 2020 feedback from users has been generally positive although we have also used feedback to make improvements to the user interface and will continue to do so over the lifetime of e planner the need for users to quickly understand the purpose of e planner and what it does and does not consider led to the creation of the user guide and suggested user workflow fig 1 the workflow places e planner within the wider context of farmer led decision making and adss this includes the importance of making an initial assessment of land that might be available for environmentally focussed uses and of the context of sustainable agriculture and its goals of enhancing both environmental delivery and crop production in some systems neutral or beneficial impacts on crop production can be achieved by removing areas from production this can occur where yields do not outweigh the input costs where potential recompense from aes outweighs the income from crop production or where increased ecosystem services from the environmental opportunity increase productivity in the remaining cropped land pywell et al 2015 woodcock et al 2016 in other situations land is not taken out of agricultural production to implement an environmental opportunity e g restoration of species rich grassland or floodplain meadow on improved pasture so users may wish to target already productive areas to increase ecosystem service delivery lawson et al 2018 or deliver additional benefits such as increased climate change resilience of more biodiverse swards isbell et al 2015 e planner scales the maps to highlight the most and least suitable areas from each opportunity within the selected area this was chosen as being most intuitive to farmers and because existing research suggests that farmers prefer decision support outputs which are made relevant to their particular farm rather than applying rulesets based on national or regional averages rose et al 2016 that said it should be borne in mind that this scaling means that e planner will still identify the best areas in an area of interest that is uniformly poorly suited or the worst areas in an area that is uniformly well suited we found in preliminary analyses that the difference between maps which were scaled to a user defined area of interest and those which were not scaled but retained national scale suitability were not as extreme as might be imagined unless the user selects an area of interest which is very small i e a single field this is because of the multiple variables making up each index of suitability the scaling of each variable to a 0 1 scale and the high resolution of the input data this issue therefore unlikely to lead to misleading interpretations of the maps unless a user selects only a very small area and neglects to check the recommendations of e planner on the ground we consider the latter unlikely given our strong emphasis on site inspection to validate selection in the suggested user workflow and the general disinclination of farmers to act on the recommendation of decision support tools without first checking that these match with their own experience rose et al 2018a however the issue of scaling may have driven the reduced correlation between the e planner scores and expert scores for bird food in the validation exercise as the surveyor reported that much of the surveyed farms was potentially well suited to this opportunity and thus scaling resulted in low suitability scores for areas which were in fact moderately suitable it should be borne in mind that our validation of e planner is based on only two farm estates and a single expert ideally we would complete a large scale validation exercise with multiple experts over multiple sites so that potential biases from site or expert are averaged out and it becomes possible to explore the relative performance of e planner under different situations e g farming systems however such an exercise is currently prohibitively time consuming and costly so we are continuing to validate the tool based on user feedback and comparison with other local data sources where available ultimately even if e planner is widely used and accurate the successful implementation of environmentally focused management actions requires more than just an understanding of where best to put them and farmers must learn new skills and knowledge to implement maintain and monitor such environmental management de snoo et al 2013 lobley et al 2013 mccracken et al 2015 francis et al 2017 however there is a wealth of information available on these skills as well as separate decision support tools and e planner provides links to these in its next steps page 4 2 integration with agricultural decision support systems e planner is not in itself a full adss and is focused on delivering intuitive display of a specific set of mapped outputs however the map based webgis framework and underlying web services of e planner make it easy to incorporate the suitability maps into existing adss that are already widely used by farmers for example the e planner maps are now available via the xarvio field manager platform this allows the direct overlay or comparison of the e planner maps with agronomic data from precision agriculture systems e g real time crop condition soil conditions yield historic average productivity to identify areas that satisfy both agronomic and environmental criteria users can then digitize environmental management zones and set them as automated exemptions from farm machinery operations bringing several aspects of the workflow required to balance farming for yield vs farming for nature into the same integrated adss such integration could potentially also be used to bringing more dynamic elements to e planner itself under the current framework for example by dynamically updating the suitability models based on hypothetical new habitat patches digitised by the user links to other adss would also bring the possibility of directly simulating the impact of potential changes to environmental management on key environmental indicators or ecosystem service using predictive models already implemented in adss with the functionality to collate data from other systems and tool on the complex local factors required to make accurate estimates 5 conclusions the ultimate criterion for success of any decision support tool is for its recommendations to be translated into successful action there are already a vast number of agricultural decision support tools potentially available to farmers rose et al 2016 and the uptake of any given tool is often poor terribile et al 2015 rose et al 2016 lindblom et al 2017 rose and bruce 2018 tools can be too expensive or time consuming overly simple or overly complex or critically fail to meet the needs of their intended users we have attempted to design e planner is such a way as to maximise its chances of uptake it is freely accessible easy to use visually oriented co designed and supports integration with existing adss although currently operational only for great britain the methods used by e planner for collating and presenting data on environmental constraints and drivers are widely applicable limited only by the availability and resolution of environmental datasets the efficient spatial targeting of agronomic and environmental management aided by e planner in combination with other adss forms an essential step towards the globally relevant goal of sustainable agriculture finally e planner is likely to have wider relevance to habitat restoration beyond farmland for example the uk government s draft environment bill states that after 2023 new developments will be legally required to achieve biodiversity net gain developers will be required to increase the area or quality of appropriate habitats over and above that affected by the development spatial planning tools such as e planner could have a key role in supporting improved outcomes of this policy through provision of guidance on where best to target on or off site habitat restoration software availability name of software e planner developer richard burkmar uk centre for ecology hydrology lancaster environment centre library avenue bailrigg lancaster la1 4ap uk year first available 2020 hardware required web enabled desktop or mobile device software required web browser availability openly accessible web based application cost free author contributions j w r and r f p conceived the tool j w r collated and analysed data and coordinated validation r b analysed data and led construction of the tool and its interface with support from m b all authors contributed to the design of the tool and the writing of the paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the development of this tool was funded by the natural environment research council nerc under research programme ne n018125 1 assist achieving sustainable agricultural systems www assist ceh ac uk assist is an initiative jointly supported by nerc and the biotechnology and biological sciences research council bbsrc validation of e planner was funded by arla uk arla foods uk plc under the arla uk 360 program this work used jasmin the uk collaborative data analysis facility the authors would like to thank everyone in the agricultural sector who provided feedback on e planner during its development especial thanks to marek nowakowski of the wildlife farming company for providing expert assessments used in validation thanks also to experts on habitat creation restoration and management who helped to identify input variables and datasets for the suitability indices in particular emma rothero and the floodplain meadows partnership we are grateful to four anonymous reviewers for their constructive comments on previous versions of this manuscript appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105437 
25556,e planner is a free web based application which provides land managers with fine scale maps of the suitability of agricultural land in great britain for environmental enhancement e planner is designed to streamline decision making around the choice and spatial targeting of environmental management interventions suitability scores are calculated by integrating a range of biophysical data sets and presented as easy to interpret maps at fine resolutions 5 m equivalent to those used by precision agriculture technology the information provided by e planner is important for landscape to field scale spatial targeting of farm management to maximise the efficiency of both crop production and environmental delivery whilst many datasets and tools support the former equivalents for environmental factors are not widely available the methods used by e planner for collating and presenting data on environmental constraints and drivers are widely applicable and efficient spatial targeting of agronomic and environmental management forms an essential step towards sustainable agriculture a global issue keywords sustainable agriculture decision support fine resolution precision agriculture agri environment 1 introduction 1 1 sustainable agriculture the challenge of an ever increasing global demand for sufficient nutritious food has historically been met by the expansion and intensification of agricultural systems foley et al 2005 pretty 2007 lambin and meyfroidt 2011 stevenson et al 2013 however this has come at the cost of widespread declines in environmental quality including biodiversity firbank et al 2008 henle et al 2008 kleijn et al 2009 water quality moss 2008 mateo sagasta et al 2018 air quality bauer et al 2016 giannadaki et al 2018 and soil health jones et al 2013 smith et al 2016 these declines affect the ecosystem services provided by the environment mace et al 2012 on which agricultural production itself depends tscharntke et al 2005 emmerson et al 2016 with global food demand continuing to increase godfray et al 2010 agriculture must remain productive but must also become more environmentally sustainable the goals of keeping agriculture productive whilst reducing its environmental footprint are not mutually exclusive actions which deliver environmental benefits can have beneficial effects on agricultural production and vice versa examples include the spill over of services delivered by insects e g pollination natural pest control from non crop habitats woodcock et al 2016 the positive effects of increasing soil organic matter on both carbon sequestration and crop yields dexter et al 2008 johnston et al 2009 jensen et al 2019 or the increased resistance to extreme weather events conferred by biodiverse habitats within agricultural landscapes redhead et al 2020 these findings have led to such concepts as ecological intensification bommarco et al 2013 kleijn et al 2019 sustainable intensification struik and kuyper 2017 agroecology wezel et al 2020 and regenerative agriculture lacanne and lundgren 2018 these vary in their precise definitions but all aim to achieve simultaneous increases in the productivity and long term viability of agriculture and reductions in its environmental impacts achieving this is challenging because the same land must at some scale be used to deliver multiple outcomes and because these outcomes are often highly context dependent success thus requires a high level of farmer engagement and knowledge the ability to balance farming for yield vs farming for nature de snoo et al 2013 and the capacity to make informed decisions at fine spatial scales one existing mechanism for the delivery of sustainable agriculture is agri environment schemes aes these offer financial incentives to offset costs or loss of production to farmers for specific environmentally beneficial management actions often referred to as aes options aes have been established in many countries for decades but their effectiveness in delivering their environmental goals is often inconsistent kleijn and sutherland 2003 kleijn et al 2006 2011 the chances of an individual aes option delivering the environmental benefits for which it was conceived vary widely with the quality of implementation and management these in turn depend on the level of experience knowledge and engagement of the farmer lobley et al 2013 mccracken et al 2015 there is evidence that providing basic training and easily accessible information significantly increases the success of aes options lobley et al 2013 mccracken et al 2015 1 2 decision support tools for sustainable agriculture one frequently identified potential solution to the challenges of complex spatial planning required by sustainable agriculture and aes is the use of agricultural decision support systems adss doré et al 2011 francis et al 2017 lindblom et al 2017 zhai et al 2020 the definition of adss is usually broad enough to include any computer based system for collating analysing and displaying data in a way that supports farm decision making zhai et al 2020 talari et al 2021 however advances in remote sensing and information technology mean that effective modern adss are expected to support dynamic data e g real time data on climate or crop condition the ability to interactively explore alternative management options via what if modelling and intuitive multi scale graphical user interfaces often supported by open webgis terribile et al 2015 a key development that has driven the development of many recent adss is the uptake of precision agriculture precision agriculture implies the use of technology to collect and supply data on spatial and temporal variability in agricultural production on which to base agricultural management decisions bongiovanni and lowenberg deboer 2004 mcbratney et al 2005 the fine spatial and temporal resolution of precision agriculture data enables targeting and application of management at the sub field scale however this creates very large volumes of data that must be presented in a way that farmers can access and interpret to inform management decisions rose et al 2018b hence the need for adss a functional adss is likely to be composed of several interlinked tools and services providing specific functionality e g manna et al 2020 bancheri et al 2022 very large numbers of adss and their component tools are now available rose et al 2016 zhai et al 2020 ranging from proprietary software for integrating data from particular precision agricultural sensors examples in zhai et al 2020 to comprehensive web based openly accessible adss platforms e g terribile et al 2015 many of these tools and systems support sustainable agriculture in the sense that they reduce waste improve the targeting of actions with negative environmental consequences bongiovanni and lowenberg deboer 2004 lindblom et al 2017 rose et al 2018a or explore the consequences of management actions for natural resources such as soils terribile et al 2015 and groundwater bancheri et al 2022 however few tools within existing adss directly address the planning and management of actions intended to promote specific environmental benefits e g aes options where tools do address environmental outcomes directly they tend to operate at the whole farm level e g hillier et al 2011 gooday et al 2014 whilst these are helpful in suggesting which environmental management actions a farmer may wish to undertake they do not operate at the same fine spatial resolution as precision agricultural data nor help to spatially target actions within the farm or field so on the one hand farmers are increasingly used to having access to agronomic data and tools indicating which areas of land are least agriculturally productive and may thus be better suited to an alternative environmentally focussed management or most productive and thus require additional support from environmentally focussed management that may help to bolster resilience on the other hand they do not currently have the equivalent data and tools to support decisions on which management actions might be best suited to such areas or to identify the most beneficial locations for particular environmental management actions 1 3 aims we created the e planner tool to address this gap by providing data on the suitability of land for different environmental management actions at the fine spatial resolutions typically associated with precision agricultural data whilst covering the full extent of agricultural land in great britain gb e planner is a free web based tool that presents complex environmental data as easy to interpret webgis maps e planner presents static information on long term drivers of environmental potential and thus does not contain the dynamic elements of a full adss this is partly because even the shortest term aes options remain in place for at least a year so compared to agronomic variables there is less requirement for real time updates on current conditions or the need to quickly explore potential responses we also avoid duplicating functionality from existing adss e g ability to digitize and export management zones to keep the tool interface simple and avoid apparent competition instead we aimed to produce a free tool that can either be used as a standalone or by producing mapped outputs accessible via web services integrated with existing adss lack of cost ease of use and presentation of visual outputs have all repeatedly been identified as important drivers of uptake in agricultural decision support tools see reviews in rose et al 2016 lindblom et al 2017 rose and bruce 2018 e planner is intended to enhance not replace local knowledge and field surveys fig 1 and therefore does not attempt to identify an optimal solution but instead focusses on providing easy to interpret contextual data to inform the decision making process a goal for decision support tools that generally receives greater support from farmers rose et al 2018a 2018b although e planner has been designed to work in tandem with precision agricultural data or adss by presenting interoperable outputs at similar resolutions and in similar ways such data are not a prerequisite and e planner can be used independently based on local knowledge 2 methods 2 1 mapping suitability of land for environmental opportunities the e planner tool provides freely accessible and easy to interpret maps representing the suitability of land for a range of environmental opportunities fig 1 we define environmental opportunities as suites of potential actions aiming to deliver an environmentally beneficial environmental outcome by changing the management of farmed land or taking it out of production each opportunity has a number of ways in which it could be implemented e g aes options depending on the farming system and local context for example the environmental opportunity of creation of flower rich pollinator habitat might be achieved by sowing of annual pollen and nectar mixes in intensive arable systems the creation of perennial wildflower areas where land can be removed from production longer term or the restoration of species rich grassland in pasture systems we did not attempt to model specific outcomes from implementing the environmental opportunity e g ecosystem services which would be highly dependent on local context quality of implementation and other factors which are difficult to quantify at fine resolution across national extents and which interact in complex ways for example crop pollination service is dependent on complex interactions between landscape and the composition of the local pollinator community ricketts et al 2008 senapathi et al 2015 which is itself dependant the quality and extent of pollinator habitats in the landscape kennedy et al 2013 senapathi et al 2017 instead e planner presents maps of relative suitability assessed from combinations of biophysical variables which affect i the likelihood that a given opportunity can be implemented ii increase its potential for delivery or iii decrease the chance for unintended detrimental effects these variables were identified from academic papers e g cane 1991 maher et al 2019 published guidance e g woodland trust 2015 nowakowski and pywell 2016 rothero et al 2016 aes handbooks e g natural england 2018 and consultations with experts in the field of agricultural habitat creation restoration and management once identified and derived from a suitable spatial data source see section 2 2 each of these variables was then scaled to a range of 0 1 and the relevant scaled variables summed to produce each suitability map thus giving each variable equal weighting the final suitability maps are then rescaled by the web application so that the user is presented with simple heat maps ranging from values 0 least suitable to 1 most suitable for their chosen area of interest this approach makes it very simple to add or substitute variables when new datasets become available or new research suggests their importance all suitability maps presented by e planner are 5 m resolution rasters which is sufficiently fine scale to target habitat management actions at the sub field scale e g many aes arable field margins in gb are 6 12 m in width and similar to the resolutions commonly produced by precision agricultural data and tools as discussed above it is important that e planner uses a similar resolution and presentation of the data to existing precision adss because e planner is intended to work in tandem with such software where available this integration will help identify the areas where changing management to deliver environmental benefits would have minimal neutral or even beneficial impacts on productivity the visualisation of the suitability maps and user interface were designed using an iterative co design approach with repeated consultations with focus groups of farmers agronomists and farm advisers since the incorporation of user feedback is critical to delivering a successful agricultural decision support tool rose et al 2016 lindblom et al 2017 rose and bruce 2018 rose et al 2018a 2018b the e planner user guidance places a strong emphasis on the importance of practitioners sense checking or ground truthing the results by local site inspection fig 1 as local factors e g disturbance site access pest pressure visual impact are likely to affect suitability in ways that cannot be readily predicted by e planner this is also important in selecting exactly which management method is best suited to implementing an environmental opportunity in a given area for example the wet grassland for which e planner maps suitability for restoration see table 1 covers both floodplain floristically diverse meadows and grazed wet grassland suitable for waders and wildfowl the former are much more restricted in the range of environmental conditions under which they can be successfully established so only field data and local knowledge can distinguish exactly which management to implement within an area estimated as highly suitable for an overarching environmental opportunity 2 2 sourcing and processing input spatial data suitability maps were created for five environmental opportunities the variables used in the calculations of suitability and the potential different management actions for implementing each opportunity are listed in table 1 existing high quality wildlife habitats were also deliberately excluded from the e planner maps as they are not appropriate for some management activities and these practices may cause unintended damage for each biophysical variable identified as being indicative of suitability as listed in table 1 we identified a source spatial dataset spatial datasets had to have gb wide extent or equivalent national extent datasets for england scotland and wales and sufficiently fine resolution to indicate relative differences in suitability at the within field scale i e raster resolutions of 50 m 50 m or finer or vector data mapped at 1 50000 scale or finer source datasets are listed in table 2 in the majority of cases spatial datasets required further processing to derive the variables required for the suitability maps regardless of the format vector or raster and resolution of the data sources used to create the spatial datasets listed in table 2 they were all generated as collections of 10 km 10 km raster tiles at 5 m resolution splitting these large high resolution datasets into 10 km tiles made them easier to handle and amenable to parallel processing techniques for many variables the value at any given 5 m pixel depended on the values of pixels elsewhere in the landscape e g assessments of proximity and connectivity so to avoid edge effects tiles were buffered e planner also incorporates information on national level priority of the different environmental opportunities to help place the user defined area in its wider context inform decisions about which opportunities to prioritise and to break ties if areas are equally well suited to different opportunities these data are derived from a centile based classification low 50th centile medium 50th 75th centile high 75 centile of a single data layer for each opportunity summarised in table 3 these are then used to present a traffic light indicator of national priority for each map in the tool interface 2 3 e planner construction and technical infrastructure all handling and pre processing of spatial datasets was performed in r r core team 2019 making use of the raster hijmans 2020 sp pebesma 2018 insol corripio 2021 fasterize ross 2020 nabor elseberg et al 2012 and whitebox wu 2020 packages for a full description of how variables were derived from source data including relevant formulae r functions and packages see supplementary material appendix a the jasmin lotus batch and parallel processing cluster was used to carry out the processing with each job working on a single 10 km tile jasmin is a national data computing facility for the environmental sciences community operated by the science technology facilities council stfc on behalf of the natural environment research council nerc it provides large scale storage 45 pb and processing 12 000 cores to enable data intensive environmental science running an operation on one of the datasets typically involved running a batch of approximately 2 800 jobs one per 10 km tile all of which could potentially be run in parallel the tiled opportunity map data then reside on the uk centre of ecology and hydrology ukceh storage area network san this file system sits behind ukceh s firewall but is accessible to a kubernetes cluster hosted by ukceh the e planner tool is then comprised of three services deployed in docker containers running on the kubernetes cluster these are 1 the web application and associated interface 2 the service providing the opportunity maps to the web application 3 a web mapping service wms serving os mastermap field boundary data to the web application each of these is described below and illustrated in fig 2 firstly the e planner web application was written in javascript with the main architecture provided by the react framework and the mobile first graphical user interface gui components provided by the ionic framework the ionic gui components provide a responsive interface that works well on both large computer screens and small mobile devices the mapping functionality is implemented using the leaflet framework with basemaps provided by openstreetmap e planner is best thought of as a web app but the frameworks employed give the mobile user an experience close to that of a native app as a web app built with javascript the majority of the code runs in the user s browser however the web app does not itself host any of the data presented to the user e g the opportunity maps these are provided to the app via the second backend service with which the web app communicates over standard http protocols to view an opportunity map layer a user must first specify an area of interest which can be done in one of three ways dragging a box over the basemap uploading a shapefile or by entering a single business identifier sbi an sbi is a code that uniquely identifies a farm to the english rural payments agency rpa if a user enters an sbi the web app contacts an external application programming interface api provided by the rpa and retrieves the boundary for the farm once an area of interest has been identified the user can request one or more opportunity maps for that area and the web app sends a series of http requests to the opportunity map service this second service runs r code behind a rest api implemented in plumber the service connects to the file system hosting the 10 km tiles and retrieves any required to cover the area specified in the rest request from the web app the tiles are then mosaicked and clipped to the area of interest specified in the rest call at this point the service is holding in memory a single band raster for the specified opportunity layers covering the area of interest specified by the user this raster is then re scaled over the area of interest before being converted to a colour image using a palette specified in the rest call and streamed back to the web app which displays it over the leaflet osm basemap the local scaling of the opportunity maps carried out by the opportunity map service is important to highlight the relative importance of different areas within a single farm in terms of their opportunity for the different biodiversity interventions so a farm with very little land that is suitable for a given opportunity will still show the areas that are relatively best suited to it and a farm where all the land is well suited to a given opportunity will still highlight the least well suited areas this avoids situations where the entirety of a selected area might be assigned uniformly intermediate suitability but does mean that the maps should be always be interpreted as comparing relative suitability for the chosen user defined area the relative suitability of the farm to its surroundings can be assessed by the user selecting a wider area for comparison and using the national scale priority indicators this need for local scaling requires processing by the r code in the service because this service is relatively slow there is a danger that calls to the service could back up and impact performance the kubernetes platform used to host this service is important in mitigating this risk kubernetes enables replicas of docker containers to be run concurrently and routes calls to the service to any of the replicas that are available the third service is required because the basemap osm does not include field boundaries which are important for interpretation of maps in agricultural contexts these are provided via a wms created from field boundaries derived under license from ordnance survey mastermap and displayed over the opportunity map layer a glossary of software and hardware terminology and brief description of key functions is provided in appendix b 2 4 validation on study farms in addition to iterative feedback from potential users throughout e planner s development we also undertook a formal validation exercise for this two large farms 222 and 276 ha mixed dairy and arable enterprises in the south of england were used these farms were not used for the initial development and testing of e planner for each farm we split each suitability map into five zones using suitability score intervals of 0 2 0 4 0 6 and 0 8 with zone 1 being least suitable i e a score of 0 0 2 and zone 5 being most suitable i e a score of 0 8 1 0 an independent surveyor with expertise in farm environmental land management planning then visited at least one site within each zone for each opportunity five zones x five environmental opportunities 25 sites per farm the surveyor assigned and recorded a suitability score to each location based on their own experience and expertise 1 least suitable 5 most suitable the surveyor was instructed to base their scoring on the potential of the location to deliver the relevant environmental opportunity regardless of current management the surveyor also recorded the reason for their score including local factors that modified the basic suitability of a site e g disturbance pest pressure farm access visual impact but for which we did not have national extent data and which do not thus contribute to the suitability scores produced by e planner the surveyor could also record scores for additional sites e g those they saw as being particularly highly suitable or unsuitable for a particular opportunity 3 results 3 1 the e planner tool and interface the e planner tool can be accessed via a web browser on a desktop or mobile device at https assist e planner ceh ac uk although the exact appearance of e planner varies depending on the device and browser used for access functionality remains virtually unchanged the e planner site consists of four main pages about e planner user guide e planner tool and next steps the first of these gives the background to e planner and a brief summary of how the tool works and the suggested user workflow see fig 1 the second outlines the functions of the various controls in the tool and how to interpret the output maps the next steps page provides links to existing best practice guidance on how to implement and manage the different opportunities and to some sources of potential agricultural subsidies for doing so i e aes the tool is split over two tabs the first of these the select tab fig 3 controls user defined selection of an area of interest for which to show the suitability maps referred to as opportunity maps in the e planner interface with the options to draw an area of interest upload one in shapefile format or use the rural payments agency land data api to access the field boundaries of a single farm based on the user inputting a unique farm identifier sbi the user can zoom or pan around the map to make their selection and can remove an area of interest the opportunity maps to be displayed up to four at once and transparency and colour palette for visualisation of the maps can also be chosen on this page before viewing the maps once the desired area is chosen the user selects that they wish to load the opportunity maps attempting to retrieve the maps for a selected area over 25 km2 in area will display a message requesting that a smaller area is chosen to prevent very high volumes of data being retrieved and ensuring rapid and reliable running of the tool for multiple users loading the maps will automatically switch to the second tab the opportunities tab fig 4 this presents the opportunity maps and allows the user to explore them by comparing suitability within and between maps the user can make decisions about which areas to prioritise for further investigation in the field narrow down the list of opportunities to consider for a given location or identify the most suitable uses for areas already under consideration fig 5 illustrates the interpretation of the maps for some of these potential uses 3 2 validation on study farms agreement between the scores derived from e planner scores and field based scores assigned by the expert surveyor was generally high spearman s correlation coefficient 0 80 p 0 01 for both farms combined this was especially so for the most and least suitable areas with more variation around intermediate scores fig 6 a agreement between the scores did not appear to differ greatly between the two farms fig 6b and c spearman s correlation coefficients 0 78 and 0 82 respectively p 0 01 for both the different environmental opportunities showed some variation in agreement between scores fig 7 agreement was generally high for all but it was notably stronger for wet grassland creation and weaker for winter bird food spearman s correlation coefficients flower rich pollinator habitat 0 77 water resource protection 0 80 wet grassland restoration 0 97 winter bird food 0 54 woodland creation 0 83 however it is important to note that where there were discrepancies between scores e planner generally tended to underestimate suitability for all opportunities i e most points are on or above the 1 1 line on fig 7 including for winter bird food rather than assigning misleadingly high suitability scores to unsuitable areas the most common factors recorded by the surveyor as reducing the field assigned score of otherwise suitable areas were the possible effects of wildlife disturbance from roads and footpaths the e planner suitability scores also did not always reflect the presence of wet ditches which would make good potential candidates for water resource protection because these are not consistently mapped in the national watercourse data that e planner uses 4 discussion 4 1 use and limitations of e planner e planner was launched in june 2020 feedback from users has been generally positive although we have also used feedback to make improvements to the user interface and will continue to do so over the lifetime of e planner the need for users to quickly understand the purpose of e planner and what it does and does not consider led to the creation of the user guide and suggested user workflow fig 1 the workflow places e planner within the wider context of farmer led decision making and adss this includes the importance of making an initial assessment of land that might be available for environmentally focussed uses and of the context of sustainable agriculture and its goals of enhancing both environmental delivery and crop production in some systems neutral or beneficial impacts on crop production can be achieved by removing areas from production this can occur where yields do not outweigh the input costs where potential recompense from aes outweighs the income from crop production or where increased ecosystem services from the environmental opportunity increase productivity in the remaining cropped land pywell et al 2015 woodcock et al 2016 in other situations land is not taken out of agricultural production to implement an environmental opportunity e g restoration of species rich grassland or floodplain meadow on improved pasture so users may wish to target already productive areas to increase ecosystem service delivery lawson et al 2018 or deliver additional benefits such as increased climate change resilience of more biodiverse swards isbell et al 2015 e planner scales the maps to highlight the most and least suitable areas from each opportunity within the selected area this was chosen as being most intuitive to farmers and because existing research suggests that farmers prefer decision support outputs which are made relevant to their particular farm rather than applying rulesets based on national or regional averages rose et al 2016 that said it should be borne in mind that this scaling means that e planner will still identify the best areas in an area of interest that is uniformly poorly suited or the worst areas in an area that is uniformly well suited we found in preliminary analyses that the difference between maps which were scaled to a user defined area of interest and those which were not scaled but retained national scale suitability were not as extreme as might be imagined unless the user selects an area of interest which is very small i e a single field this is because of the multiple variables making up each index of suitability the scaling of each variable to a 0 1 scale and the high resolution of the input data this issue therefore unlikely to lead to misleading interpretations of the maps unless a user selects only a very small area and neglects to check the recommendations of e planner on the ground we consider the latter unlikely given our strong emphasis on site inspection to validate selection in the suggested user workflow and the general disinclination of farmers to act on the recommendation of decision support tools without first checking that these match with their own experience rose et al 2018a however the issue of scaling may have driven the reduced correlation between the e planner scores and expert scores for bird food in the validation exercise as the surveyor reported that much of the surveyed farms was potentially well suited to this opportunity and thus scaling resulted in low suitability scores for areas which were in fact moderately suitable it should be borne in mind that our validation of e planner is based on only two farm estates and a single expert ideally we would complete a large scale validation exercise with multiple experts over multiple sites so that potential biases from site or expert are averaged out and it becomes possible to explore the relative performance of e planner under different situations e g farming systems however such an exercise is currently prohibitively time consuming and costly so we are continuing to validate the tool based on user feedback and comparison with other local data sources where available ultimately even if e planner is widely used and accurate the successful implementation of environmentally focused management actions requires more than just an understanding of where best to put them and farmers must learn new skills and knowledge to implement maintain and monitor such environmental management de snoo et al 2013 lobley et al 2013 mccracken et al 2015 francis et al 2017 however there is a wealth of information available on these skills as well as separate decision support tools and e planner provides links to these in its next steps page 4 2 integration with agricultural decision support systems e planner is not in itself a full adss and is focused on delivering intuitive display of a specific set of mapped outputs however the map based webgis framework and underlying web services of e planner make it easy to incorporate the suitability maps into existing adss that are already widely used by farmers for example the e planner maps are now available via the xarvio field manager platform this allows the direct overlay or comparison of the e planner maps with agronomic data from precision agriculture systems e g real time crop condition soil conditions yield historic average productivity to identify areas that satisfy both agronomic and environmental criteria users can then digitize environmental management zones and set them as automated exemptions from farm machinery operations bringing several aspects of the workflow required to balance farming for yield vs farming for nature into the same integrated adss such integration could potentially also be used to bringing more dynamic elements to e planner itself under the current framework for example by dynamically updating the suitability models based on hypothetical new habitat patches digitised by the user links to other adss would also bring the possibility of directly simulating the impact of potential changes to environmental management on key environmental indicators or ecosystem service using predictive models already implemented in adss with the functionality to collate data from other systems and tool on the complex local factors required to make accurate estimates 5 conclusions the ultimate criterion for success of any decision support tool is for its recommendations to be translated into successful action there are already a vast number of agricultural decision support tools potentially available to farmers rose et al 2016 and the uptake of any given tool is often poor terribile et al 2015 rose et al 2016 lindblom et al 2017 rose and bruce 2018 tools can be too expensive or time consuming overly simple or overly complex or critically fail to meet the needs of their intended users we have attempted to design e planner is such a way as to maximise its chances of uptake it is freely accessible easy to use visually oriented co designed and supports integration with existing adss although currently operational only for great britain the methods used by e planner for collating and presenting data on environmental constraints and drivers are widely applicable limited only by the availability and resolution of environmental datasets the efficient spatial targeting of agronomic and environmental management aided by e planner in combination with other adss forms an essential step towards the globally relevant goal of sustainable agriculture finally e planner is likely to have wider relevance to habitat restoration beyond farmland for example the uk government s draft environment bill states that after 2023 new developments will be legally required to achieve biodiversity net gain developers will be required to increase the area or quality of appropriate habitats over and above that affected by the development spatial planning tools such as e planner could have a key role in supporting improved outcomes of this policy through provision of guidance on where best to target on or off site habitat restoration software availability name of software e planner developer richard burkmar uk centre for ecology hydrology lancaster environment centre library avenue bailrigg lancaster la1 4ap uk year first available 2020 hardware required web enabled desktop or mobile device software required web browser availability openly accessible web based application cost free author contributions j w r and r f p conceived the tool j w r collated and analysed data and coordinated validation r b analysed data and led construction of the tool and its interface with support from m b all authors contributed to the design of the tool and the writing of the paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the development of this tool was funded by the natural environment research council nerc under research programme ne n018125 1 assist achieving sustainable agricultural systems www assist ceh ac uk assist is an initiative jointly supported by nerc and the biotechnology and biological sciences research council bbsrc validation of e planner was funded by arla uk arla foods uk plc under the arla uk 360 program this work used jasmin the uk collaborative data analysis facility the authors would like to thank everyone in the agricultural sector who provided feedback on e planner during its development especial thanks to marek nowakowski of the wildlife farming company for providing expert assessments used in validation thanks also to experts on habitat creation restoration and management who helped to identify input variables and datasets for the suitability indices in particular emma rothero and the floodplain meadows partnership we are grateful to four anonymous reviewers for their constructive comments on previous versions of this manuscript appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105437 
25557,soil profile horizon delineation 4 2 key parameters of pef modflow 4 3 impact of smartphone images on results 4 4 limitations and prospects of pef modflow 5 conclusions 6 software availability acknowledgments appendix a supplementary data achanta 2012 2274 2282 r aitkenhead 2016 89 110 m estimatingsoilpropertiesamobilephonedigitalsoilmorphometricsprogressinsoilscience aitkenhead 2020 105322 m bholowalia 2014 17 24 p boone 1999 3 28 r standardsoilmethodsforlongtermecologicalresearch soilsamplingpreparationarchivingqualitycontrol bridges 1993 363 373 e 1978 recommendationsuniformcolorspacescolordifferenceequationspsychometticcolorterms churchman 2010 214 221 g churchward 1961 73 86 h delaval 2021 105005 a ferraro 2015 1 16 m ganesan 2010 393 397 p segmentationedgedetectioncolorimagesusingcielabcolorspaceedgedetectors gong 2001 z chinesesoiltaxonomyrevisedinenglish gurrin 2013 308 313 c hartemink 2014 305 317 a hartemink 2020 125 185 a soilhorizonvariationareview hernandezgomez 2009 107 110 g 2009internationalconferenceelectricalcommunicationscomputers naturalimagesegmentationusingcielabspace hettiarachchi 2017 119 135 r igoe 2014 586 592 d iussworkinggroupwrb 2015 worldreferencebaseforsoilresources2014internationalsoilclassificationsystemfornamingsoilscreatinglegendsforsoilmapsworldsoilresourcesreportsno106 johnson 1990 306 319 d lee 2012 504 529 s liu 2020 114556 f marutho 2018 533 538 d 2018internationalseminarapplicationfortechnologyinformationcommunicationisemantic18semarangindonesia determinationclusternumberkmeanusingelbowmethodpurityevaluationheadlinenews mcbratney 1997 85 113 a nagy 2016 365 381 j digitalsoilmorphometrics digitalsoilmorphometricsbringsrevolutionsoilclassification neubert 2012 p proceedingsforumbildverarbeitung superpixelbenchmarkcomparison roudier 2016 113 132 p digitalsoilmorphometrics advancestowardsquantitativeassessmentssoilprofileproperties schoeneberger 2012 p fieldbookfordescribingsamplingsoilsversion30 2014 keyssoiltaxonomy sun 2012 24 34 x swetha 2020 r wang 2017 t soilserieschinahubeivolume wang 2020 t soilserieschinajiangxivolume yang 2021 115365 j 2001 soilseriesresearchmappinginchinese zhang 2012 g soilsurveylaboratorymethodsinchinese zhang 2019 97 115 y yangx2022x105423 yangx2022x105423xj 2024 06 08t00 00 00 000z 2024 06 08t00 00 00 000z http creativecommons org licenses by nc nd 4 0 2022 elsevier ltd all rights reserved 2022 06 12t06 17 09 602z http vtw elsevier com data voc addontypes 50 7 aggregated refined 0 https doi org 10 15223 policy 017 https doi org 10 15223 policy 037 https doi org 10 15223 policy 012 https doi org 10 15223 policy 029 https doi org 10 15223 policy 004 item s1364 8152 22 00129 3 s1364815222001293 1 s2 0 s1364815222001293 10 1016 j envsoft 2022 105423 271872 2022 10 15t20 46 01 532924z 2022 09 01 2022 09 30 1 s2 0 s1364815222001293 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 main application pdf be1451e6d15ff2ef91097e63d6ffb2b3 main pdf main pdf pdf true 15244963 main 15 1 s2 0 s1364815222001293 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 preview image png 86eec524788fe5df5c6efe374f7c5c52 main 1 png main 1 png png 60351 849 656 image web pdf 1 1 s2 0 s1364815222001293 gr9 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr9 downsampled image jpeg 8791586ae7ddf47e37ae9bd29e860b36 gr9 jpg gr9 gr9 jpg jpg 216494 433 1024 image downsampled 1 s2 0 s1364815222001293 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr1 downsampled image jpeg 65e6f2f9bd9c354970d3e1540e23f913 gr1 jpg gr1 gr1 jpg jpg 270344 672 624 image downsampled 1 s2 0 s1364815222001293 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr2 downsampled image jpeg bcbf75871ace0974a0b4ed93cdda4b13 gr2 jpg gr2 gr2 jpg jpg 162158 405 810 image downsampled 1 s2 0 s1364815222001293 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr3 downsampled image jpeg 687a5d5e28b94a6114b79f8057f8327f gr3 jpg gr3 gr3 jpg jpg 140315 476 597 image downsampled 1 s2 0 s1364815222001293 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr4 downsampled image jpeg 14ae6557bd2e50219eafbb875ffedb84 gr4 jpg gr4 gr4 jpg jpg 107922 454 388 image downsampled 1 s2 0 s1364815222001293 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr5 downsampled image jpeg a2badb72a0d36fb1f88cb4f6c8b25a10 gr5 jpg gr5 gr5 jpg jpg 245520 402 1024 image downsampled 1 s2 0 s1364815222001293 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr6 downsampled image jpeg e35a48a49bce856e374c27b094ad3c80 gr6 jpg gr6 gr6 jpg jpg 191505 386 1025 image downsampled 1 s2 0 s1364815222001293 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr7 downsampled image jpeg 3a32e47f45826dc29e9136e707be6cf3 gr7 jpg gr7 gr7 jpg jpg 180615 408 1024 image downsampled 1 s2 0 s1364815222001293 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr8 downsampled image jpeg b9451d6bc33493c7768919c3876aa8c8 gr8 jpg gr8 gr8 jpg jpg 168996 343 811 image downsampled 1 s2 0 s1364815222001293 gr10 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr10 downsampled image jpeg b15a96037b563543543905c32781ae10 gr10 jpg gr10 gr10 jpg jpg 139650 314 811 image downsampled 1 s2 0 s1364815222001293 gr9 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr9 thumbnail image gif a187d8c817eb3a469a2615d9cf7539ee gr9 sml gr9 gr9 sml sml 83420 93 219 image thumbnail 1 s2 0 s1364815222001293 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr1 thumbnail image gif d489e17b1fd9fb5c9f93447289dec9d7 gr1 sml gr1 gr1 sml sml 91950 164 152 image thumbnail 1 s2 0 s1364815222001293 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr2 thumbnail image gif b58e252e46b3a9024fcff4f982c585b0 gr2 sml gr2 gr2 sml sml 76002 109 219 image thumbnail 1 s2 0 s1364815222001293 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr3 thumbnail image gif 7fe02d9b3ff90abaac68ce2f4230283f gr3 sml gr3 gr3 sml sml 78095 164 205 image thumbnail 1 s2 0 s1364815222001293 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr4 thumbnail image gif 4a7fb5340eb549cc0e21f2866443496c gr4 sml gr4 gr4 sml sml 75400 164 140 image thumbnail 1 s2 0 s1364815222001293 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr5 thumbnail image gif e3d60386f136458e6aee5355c3ebcef9 gr5 sml gr5 gr5 sml sml 79752 86 219 image thumbnail 1 s2 0 s1364815222001293 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr6 thumbnail image gif 79496fc301a78ec8e17dfebf7b2b87b6 gr6 sml gr6 gr6 sml sml 77665 83 219 image thumbnail 1 s2 0 s1364815222001293 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr7 thumbnail image gif 007ca18889c300b16ead0947f02bcd82 gr7 sml gr7 gr7 sml sml 75108 87 219 image thumbnail 1 s2 0 s1364815222001293 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr8 thumbnail image gif de58f7d473832e90d079e5740486cab2 gr8 sml gr8 gr8 sml sml 80456 93 219 image thumbnail 1 s2 0 s1364815222001293 gr10 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr10 thumbnail image gif ed75a3adb8ac3674e0da56889fbd02bd gr10 sml gr10 gr10 sml sml 74077 85 219 image thumbnail 1 s2 0 s1364815222001293 gr9 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr9 highres image jpeg 733bded280153100a1df6158d6bd7448 gr9 lrg jpg gr9 gr9 lrg jpg jpg 1206919 1918 4535 image high res 1 s2 0 s1364815222001293 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr1 highres image jpeg 9faaa6aaacba81972ea67d792b63e266 gr1 lrg jpg gr1 gr1 lrg jpg jpg 2478771 2978 2764 image high res 1 s2 0 s1364815222001293 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr2 highres image jpeg 09c72e4efbca1e7da8d769309e954041 gr2 lrg jpg gr2 gr2 lrg jpg jpg 845473 1792 3588 image high res 1 s2 0 s1364815222001293 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr3 highres image jpeg 4c5937a79a8b36c4e1f412b17e6b02c6 gr3 lrg jpg gr3 gr3 lrg jpg jpg 634878 2111 2646 image high res 1 s2 0 s1364815222001293 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr4 highres image jpeg d16ffa4467bd889cadeb665874c634bc gr4 lrg jpg gr4 gr4 lrg jpg jpg 322674 2015 1721 image high res 1 s2 0 s1364815222001293 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr5 highres image jpeg f3d2f6676b82e5bcd2fa1357953ce25a gr5 lrg jpg gr5 gr5 lrg jpg jpg 1780459 1781 4534 image high res 1 s2 0 s1364815222001293 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr6 highres image jpeg 0aab5bce91282d4ef4871738d544d41c gr6 lrg jpg gr6 gr6 lrg jpg jpg 1012094 1709 4536 image high res 1 s2 0 s1364815222001293 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr7 highres image jpeg eca12dfb44607e93e6f879529c3e19f4 gr7 lrg jpg gr7 gr7 lrg jpg jpg 941914 1806 4535 image high res 1 s2 0 s1364815222001293 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr8 highres image jpeg ee970e6af49ff7f618f5dd9735cad628 gr8 lrg jpg gr8 gr8 lrg jpg jpg 966728 1519 3590 image high res 1 s2 0 s1364815222001293 gr10 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr10 highres image jpeg b568579a2dcd247a1ce9824a42e8fa9e gr10 lrg jpg gr10 gr10 lrg jpg jpg 689152 1389 3591 image high res 1 s2 0 s1364815222001293 mmc1 pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 mmc1 main application pdf c08e2a3dd68b8ac0cba52d7cea64dc22 mmc1 pdf mmc1 mmc1 pdf pdf false 1832204 application 1 s2 0 s1364815222001293 mmc2 pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 mmc2 main application pdf 9cf7267034ca8df4bac3afd08a44afc0 mmc2 pdf mmc2 mmc2 pdf pdf false 612913 application 1 s2 0 s1364815222001293 si19 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml b10574c037d99767bdf668f06931f838 si19 svg si19 si19 svg svg 23426 altimg 1 s2 0 s1364815222001293 si14 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml d3b6d106c436a96848d07e5e7a27e4e9 si14 svg si14 si14 svg svg 59306 altimg 1 s2 0 s1364815222001293 si9 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 01676724f9d2a2cf991c83071d68649b si9 svg si9 si9 svg svg 20951 altimg 1 s2 0 s1364815222001293 si16 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml eb3f1940d9f6131a0f71b41fcfc90076 si16 svg si16 si16 svg svg 26458 altimg 1 s2 0 s1364815222001293 si7 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml e0d973f2090474a662ef85cc9f437a24 si7 svg si7 si7 svg svg 22356 altimg 1 s2 0 s1364815222001293 si15 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml ea6852c500050d39448af35287794f3f si15 svg si15 si15 svg svg 65544 altimg 1 s2 0 s1364815222001293 si17 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 041452a360f2210eab738106c5a221a1 si17 svg si17 si17 svg svg 19130 altimg 1 s2 0 s1364815222001293 si2 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 8ab75ebb96444efce0add2172d6e45d7 si2 svg si2 si2 svg svg 55194 altimg 1 s2 0 s1364815222001293 si21 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 8707546876e1145ce6e28563d7172025 si21 svg si21 si21 svg svg 70096 altimg 1 s2 0 s1364815222001293 si22 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml a9f7f83a96edb37aff269bffe5dd4453 si22 svg si22 si22 svg svg 22565 altimg 1 s2 0 s1364815222001293 si12 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 7f6eb0250400018c9a7d8c382669c9ca si12 svg si12 si12 svg svg 52263 altimg 1 s2 0 s1364815222001293 si6 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml a924fde654255104a123e2461ecb6b76 si6 svg si6 si6 svg svg 67860 altimg 1 s2 0 s1364815222001293 si18 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 983331a22d0694f40d5e3327b24f7dc3 si18 svg si18 si18 svg svg 71867 altimg 1 s2 0 s1364815222001293 si3 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 67a6e963261eb92fc27c96f94c631a0d si3 svg si3 si3 svg svg 53023 altimg 1 s2 0 s1364815222001293 si20 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 3c8c28c696141d87e81b4ab86701228b si20 svg si20 si20 svg svg 22200 altimg 1 s2 0 s1364815222001293 si4 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml a2bede4a760f9d3cf713fc92f19ebfe6 si4 svg si4 si4 svg svg 25071 altimg 1 s2 0 s1364815222001293 si1 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 177a5383667605254ddfb8b1dc9505fe si1 svg si1 si1 svg svg 48759 altimg 1 s2 0 s1364815222001293 si5 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 263e10adecfaeb34bd021ad7d2b69f88 si5 svg si5 si5 svg svg 64221 altimg 1 s2 0 s1364815222001293 si13 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml c94b0135301a0c95548f2063c18fe96d si13 svg si13 si13 svg svg 24145 altimg 1 s2 0 s1364815222001293 si8 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml c8f479ee9e90a436458e4cdcae3fd8a6 si8 svg si8 si8 svg svg 77572 altimg 1 s2 0 s1364815222001293 si11 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml ee5f2f30a97b335e59a58aceb010a522 si11 svg si11 si11 svg svg 14244 altimg 1 s2 0 s1364815222001293 si10 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 6ae030c538e5db02a5122517162d8232 si10 svg si10 si10 svg svg 24979 altimg 1 s2 0 s1364815222001293 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 10q3w4rmjv3 main application pdf 193ccaf6179742bbcfb202662c928bd4 am pdf am am pdf pdf false 24811583 aam pdf enso 105423 105423 s1364 8152 22 00129 3 10 1016 j envsoft 2022 105423 elsevier ltd fig 1 fifteen soil profile images acquired by smartphone fig 1 fig 2 an overview of applying the pef modflow for soil profile horizon delineation based on soil color captured by smartphone images fig 2 fig 3 reading rules for the elbow method based on 200 soil profile test images fig 3 fig 4 diagram of the four topography from field book for describing and sampling soils version 3 0 schoeneberger et al 2012 fig 4 fig 5 mean values of l a and b for each row of pixels in the fifteen soil profile images fig 5 fig 6 differences in the mean values of l a and b color between adjacent horizons in the field division of fifteen soil profiles fig 6 fig 7 sse results for ten evaluation profiles with ten repeated runs of the elbow method fig 7 fig 8 horizon shape boundaries by field and pef modflow delineation for the ten evaluation profiles fig 8 fig 9 horizon linear boundary of the field and pef modflow delineation for the ten evaluation profiles fig 9 fig 10 the absolute error ae of horizon linear boundary for the ten evaluation profile fig 10 table 1 description of the fifteen profiles used in this study table 1 id reference wrb soil groups number of horizons horizons and their depth t1 anthrosols 5 ap1 0 13 cm ap2 13 18 cm btr1 18 48 cm btr2 48 87 cm btr3 87 120 cm t2 anthrosols 7 ap1 0 15 cm ap2 15 27 cm btr1 27 43 cm btr2 43 58 cm btr3 58 93 cm btr4 93 100 cm c 100 122 cm t3 cambisols 5 ah 0 12 cm ab 12 45 cm bw 45 60 cm bc 60 100 cm c 100 120 cm t4 luvisols 4 ah 0 25 cm bt 25 50 cm bw 50 65 cm c 65 103 cm t5 lixisols 3 ah 0 25 cm bt1 25 75 cm bt2 75 120 cm v1 anthrosols 5 ap1 0 18 cm ap2 18 32 cm bg 32 56 cm br1 56 83 cm br2 83 120 cm v2 anthrosols 4 ap1 0 22 cm ap2 22 42 cm br1 42 80 cm br2 80 115 cm v3 anthrosols 4 ap1 0 16 cm ap2 16 21 cm br1 21 39 cm br2 39 90 cm v4 acrisols 4 ah 0 15 cm bt 15 65 cm btr1 56 93 cm btr2 93 130 cm v5 acrisols 4 ah 0 18 cm bt1 18 47 cm bt2 47 82 cm c 82 90 cm v6 cambisols 6 ah 0 12 cm br1 12 24 cm br2 24 48 cm br3 48 72 cm br4 72 97 cm bg 97 130 cm v7 cambisols 4 ap 0 20 cm bw1 20 60 cm bw2 60 90 cm bw3 90 110 cm v8 luvisols 3 ah 0 25 cm bt1 25 60 cm bt2 60 120 cm v9 luvisols 4 ah 0 20 cm bt 20 40 cm br 40 75 cm c 75 120 cm v10 luvisols 3 ah 0 16 cm bt1 16 56 cm bt2 56 110 cm table 2 criteria of four topography from field book for describing and sampling soils version 3 0 schoeneberger et al 2012 table 2 topography criteria smooth planar with few or no irregularities wavy width of undulation is than depth irregular depth of undulation is than width broken discontinuous horizons discrete but intermingled or irregular pockets table 3 the optimum number of clusters corresponds to fig 7 table 3 id number of horizons delineated in the field results of elbow method mean standard deviation average absolute error 1 2 3 4 5 6 7 8 9 10 v1 5 7 7 7 7 7 7 7 7 7 7 7 00 0 00 2 00 v2 4 5 5 5 5 5 5 5 5 5 5 5 00 0 00 1 00 v3 4 4 4 4 4 4 4 4 4 4 4 4 00 0 00 0 00 v4 4 4 4 4 4 4 4 4 4 4 4 4 00 0 00 0 00 v5 4 4 4 4 4 4 4 4 4 4 4 4 00 0 00 0 00 v6 6 5 5 5 5 5 5 5 5 5 5 5 00 0 00 1 00 v7 4 3 3 3 3 3 3 3 3 3 3 3 00 0 00 1 00 v8 3 3 3 3 3 3 3 3 3 3 3 3 00 0 00 0 00 v9 4 3 3 3 3 3 3 3 3 3 3 3 00 0 00 1 00 v10 3 3 3 3 3 3 3 3 3 3 3 3 00 0 00 0 00 table 4 evaluation results of horizon shape boundaries by field and pef modflow delineation table 4 field fcm id horizon boundary topography distinctness id horizon boundary topography distinctness v1 ap1 ap2 wavy gradual v1 ap1 ap2 wavy gradual ap2 bg wavy clear ap2 bg wavy gradual bg br1 wavy clear bg br1 wavy diffuse br1 br2 wavy gradual br1 br2 wavy clear v2 ap1 ap2 wavy abrupt br2 br3 wavy gradual ap2 br1 wavy clear br3 br4 irregular gradual br1 br2 wavy clear v2 ap1 ap2 wavy gradual v3 ap1 ap2 smooth clear ap2 br1 irregular gradual ap2 br1 wavy clear br1 br2 wavy clear br1 br2 irregular gradual br2 br3 irregular gradual v4 ah bt wavy gradual v3 ap1 ap2 smooth clear bt btr1 irregular diffuse ap2 br1 irregular gradual btr1 btr2 wavy gradual br1 br2 irregular gradual v5 ah bt1 wavy gradual v4 ah bt wavy clear bt1 bt2 wavy gradual bt btr1 irregular diffuse bt2 c wavy clear btr1 btr2 wavy gradual v6 ah br1 wavy clear v5 ah bt1 irregular gradual br1 br2 smooth abrupt bt1 bt2 irregular diffuse br2 br3 smooth abrupt bt2 c wavy clear br3 br4 wavy clear v6 ah br1 wavy clear br4 bg irregular gradual br1 br2 irregular gradual v7 ap bw1 wavy gradual br2 br3 wavy diffuse bw1 bw2 smooth very abrupt br3 bg wavy gradual bw2 bw3 wavy clear v7 ap bw1 irregular clear v8 ah bt1 wavy gradual bw1 bw2 irregular diffuse bt1 bt2 irregular gradual v8 ah bt1 wavy clear v9 ah bt smooth very abrupt bt1 bt2 irregular diffuse bt br wavy clear v9 ah btr wavy clear br c wavy gradual btr c wavy diffuse v10 ah bt1 wavy gradual v10 ah bt1 wavy gradual bt1 bt2 wavy abrupt bt1 bt2 irregular diffuse table 5 the mean average error mae of horizon linear boundary for the ten evaluation profiles table 5 id v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 mae cm 9 7 11 5 18 3 5 7 2 3 27 4 8 0 9 5 32 3 13 5 pef modflow a framework for preliminary soil profile horizon delineation based on soil color captured by smartphone images jiawei yang a feilong shen a tianwei wang a lingyun wu a zhaoxia li a nian li a lilong dai a jinghui liang b jian zhang a a college of resource and environment huazhong agricultural university wuhan 430070 china college of resource and environment huazhong agricultural university wuhan 430070 china college of resource and environment huazhong agricultural university wuhan 430070 china b college of public administration huazhong agricultural university wuhan 430070 china college of public administration huazhong agricultural university wuhan 430070 china college of public administration huazhong agricultural university wuhan 430070 china corresponding author soil profile horizon delineation provides a vehicle for recording and communicating observations of soil profiles a framework was developed to achieve soil profile horizon delineation based on soil color captured by smartphone images consisting of a preprocessing module an elbow method module and a fuzzy c means fcm algorithm module referred to as pef modflow we have analyzed the effect of key parameters in pef modflow on the results and demonstrate the high performance and interpretability of the pef modflow in the ten validation profiles this work shows that smartphones can be an effective tool for soil profile information collection the results of soil profile horizon delineation are still preliminary due to the limitations of the information available in the soil profile images the flexible adjustment capability of the input data gives the pef modflow the potential for further improvement the pef modflow represents a key step towards smartphone based access to soil profile information keywords digital soil morphometrics soil profile modflow superpixel preprocessing elbow method 1 introduction soil profiles record past and present soil processes and can be used to study soil formation soil processes and soil properties churchward 1961 johnson et al 1990 roudier et al 2016 horizon delineation provides a vehicle for recording and communicating soil profile observations which can be considered the most important part of the soil profile bridges 1993 hartemink et al 2020 however achieving soil profile horizon delineation is a product of combined training and experience in soil science and is undoubtedly difficult for non specialists churchman 2010 hartemink et al 2020 furthermore even for highly experienced soil surveyors there are big differences in how much difference is needed to identify a horizon or subhorizon hartemink et al 2020 currently digital soil morphometrics which aims to describe digitally extract and quantify soil profile properties continue to progress hartemink and minasny 2014 among them proximal sensors such as portable x ray fluorescence pxrf and visible near infrared vis nir hyperspectral imaging were applied to soil profile property identification and horizon delineation studies hartemink and minasny 2014 in addition researchers have implemented horizon clustering delineation based on the profile images acquired by the professional camera zhang and hartemink 2019 the popularity of smartphones has provided unprecedented opportunities for a wider group of people to participate in collaborative scientific observations that were once out of reach due to cost accessibility and ease of use igoe et al 2014 currently smartphones with built in low cost cameras have been widely used in scientific research including in environmental monitoring and medicine gurrin et al 2013 lee et al 2012 soil color is one of the most important bases for soil profile horizon delineation smartphones can easily access the rich color information of soil and the lower price and simple operation method compared to pxrf and other professional equipment give it strong potential for application yang et al 2021 the researchers have implemented methods to predict soil organic matter ph and soil texture based on soil color captured from smartphone images aitkenhead et al 2016 2020 swetha et al 2020 yang et al 2021 texture information in soil images acquired by smartphones can be used to predict soil texture but current technology does not allow for an accurate assessment of soil texture in soil profile images swetha et al 2020 previous studies have focused mainly on the use of smartphones in soil samples soil profile analysis based on smartphone images would have broader application prospects horizon delineation based on soil profile images obtained from smartphones has an important significance for the rapid acquisition of soil profile information unsupervised learning clustering is the mainstream approach to image segmentation in which image information is assigned to different categories according to similarity differences hettiarachchi and peters 2017 as key information for unsupervised learning of cluster segmentation the representation of color in soil profiles is complex for example there are still color differences within the same horizon of a soil profile and color differences between adjacent horizons may not be significant hartemink et al 2020 soil survey staff 2014 the complex color characteristics create difficulties for horizon delineation soil scientists usually emphasize the importance of major feature differences between soil profile horizons in the field horizon delineation process hartemink et al 2020 effective image preprocessing methods that weaken the smaller color differences within the soil profile horizons and enhance the color differences between horizons are key to improving the accuracy of the horizon delineation results in addition the number of clusters i e the number of horizons is a mandatory input element for image clustering segmentation however for the characteristics of the soil profile it is still difficult how this information can be accurately obtained from the soil profile image the horizon boundaries of the soil profile can be divided into two main forms horizon shape boundary and horizon linear boundary the horizon shape boundary is usually a curve roughly parallel to the surface and has many different shapes mainly related to soil development processes schoeneberger et al 2012 zhang 2001 the horizon linear boundary is the depth range of the soil horizon determined based on the horizon shape boundary which is more beneficial for the communication and dissemination of the horizon delineation results horizon linear boundary information is an important result of the soil profile survey project the purpose of this study is to develop a framework for preliminary soil profile horizon delineation based on soil color captured from smartphone images the framework contains three major components preprocessing of soil profile images elbow method based optimum number of profile horizons and fcm based horizon delineation referred to as pef modflow we have analyzed the parameter configuration and evaluated the performance of pef modflow using fifteen soil profiles from the hubei and jiangxi provinces in china the pef modflow is the first application framework for soil profile analysis based on smartphone images that can advance the development of research and applications for automated access to soil subsurface information 2 materials and methods 2 1 study profiles a total of fifteen soil profiles were excavated in jiangxi and hubei provinces china for the key parameters acquisition and method evaluation of the pef modflow these profiles are part of the soil series of china hubei volume and jiangxi volume wang 2017 wang and chen 2020 all profiles were kept in direct sunlight under natural conditions and maintained as smooth a plane as possible while remaining uniformly moist the images were taken vertically on the soil profile with a smartphone model htc one and saved in jpg high resolution format the htc one has an ultrapixel sensor an aperture size of f 2 0 and output pixels of 4 million we used a standard whiteboard to calibrate the white balance of the smartphone camera before shooting the soil horizons were divided in the field based on the standards of the soil series research and mapping standard zhang 2001 while samples were collected for indoor property analysis to determine the soil classification names gong 2001 zhang and gong 2012 by considering soil type the range of color distribution the number of horizons five soil profiles were selected for analysis of the effect of the number of key parameters on pef modflow t1 t5 containing two anthrosols one cambisols one luvisols and one lixisols the remaining soil profiles were used for evaluating the accuracy of the pef modflow results v1 v10 containing three anthrosols two acrisols two cambisols and three luvisols the fifteen soil profile images acquired by the smartphone are shown in fig 1 and specific information on these profiles is presented in table 1 the horizon shape boundaries and horizon linear boundaries determined in the field for the fifteen soil profiles are detailed in fig s1 and fig s2 2 2 pef modflow the pef modflow consists of three major components i preprocessing of soil profile images ii elbow method based to obtain the optimum number of profile horizons and iii fcm based horizon delineation the overall framework is presented in fig 2 and the codes were written in the matlab programming language the details of the three major components of the framework are introduced below 2 2 1 preprocessing of soil profile images 2 2 1 1 image cropping the purpose of image cropping is to remove distracting information that may be present in the image such as the sky and plant leaves the disturbance information differs considerably from the color of the soil profile and can be easily classified as a separate category thus affecting the final result image cropping is performed by the imcrop function in matlab as shown in fig 2 the image is cropped to a rectangle along the upper boundary line of the soil profile 2 2 1 2 color conversion the color parameters are important input data for the pef modflow the ciel a b color system has been widely used in natural image segmentation research ganesan et al 2010 hernandez gomez et al 2009 the ciel a b color system was defined by the international commission on illumination cie where l represents lightness ranging from 0 black to 100 white and a and b are chromaticity coordinates representing the change from red a to green a and from yellow b to blue b respectively c i e 1978 the ciel a b color system can represent most soil colors and is consistent with human vision in perception and has been shown to highlight color differences between soil profile horizons more than the rgb color system c i e 1978 zhang and hartemink 2019 therefore the rgb color system of the cropped image is converted to the ciel a b color system color conversion is performed by the rgb2lab function in matlab the pef modflow uses it as the only color input data thus connecting the components 2 2 1 3 superpixel preprocessing superpixel preprocessing is the splitting of an image into many connected groups of homogeneous regions fig 2 a superpixel is a group of pixels nearby and with similar color characteristics within which all the initial pixels are set to the same color superpixel preprocessing is performed by the superpixels function in matlab the core of which is the simple linear iterative cluster slic superpixels slic superpixel is an adaptive k means clustering method where the operator simply sets the number of superpixels s and s initial clustering centers will be generated in a regular network spaced by s pixels achanta et al 2012 the l a b values of the ciel a b color system of each pixel and its x and y coordinates were used as input data all pixels were grouped into clusters through multiple iterations and the orphaned pixels were grouped into the nearest cluster center label the iterative process assigns each pixel to the nearest cluster center using the distance measure d eq 1 which combines the color distance eq 2 and the spatial distance eq 3 1 d d c c 2 d s n 2 2 d c l j l i 2 a j a i 2 b j b i 2 3 d s x j x i 2 y j y i 2 where dc and ds indicate the color distance and spatial distance of pixels i xi yi and i xj yj respectively in which the color uses the l a b color parameters c is the abbreviation for compactness which as an indicator that controls the relative importance of color distance and spatial distance in distance measure d the higher the value of compactness the more regular the shape of the superpixel i e the closer it is to a square the lower the value the more closely the superpixel fits the boundary making it more irregular in shape this indicator is generally chosen as a fixed value achanta et al 2012 indicated that its range is generally 1 40 we evaluated the impact of changes in the number of compactness on the results n is the sampling interval of the clustered centroid with n a s where a is the total number of all pixels in the image and s is the number of superpixels unlike conventional k means clustering methods slic does not compare each pixel with all other pixels but chooses localized comparisons 2 n 2 n to avoid redundant computations and thus improve computational speed the color averages in each superpixel region were calculated separately and assigned to all pixels within the superpixel region 2 2 2 elbow method based optimum number of profile horizons 2 2 2 1 elbow method the elbow method was used to determine the optimum number of clusters corresponding to the number of horizons which was represented by the sum of squared error sse results the sse denotes the sum of the average euclidean distance of each pixel point relative to the centroid eq 4 4 s s e i 1 n p n i p m i 2 where n indicates the number of clusters n i indicates the ith cluster among n clusters p indicates the color parameters of all pixel points in n i that are set as l a b color information in this study and mi indicates the average of the corresponding color parameters of all pixels in n i starting from n 2 as the number of clusters increases the n value gradually approaches the true number of clusters and the sse gradually decreases until the sse no longer varies dramatically therefore the value corresponding to the inflection point of the sse shape is often considered the optimum number of clusters the number of iterations of the elbow method is set to 100 the shape of the sse of the soil profile before and after slic superpixel preprocessing is shown in fig s3 the figure shows that the slic superpixel preprocessing effectively reduces the complex color distribution conditions of the soil profile images which in turn provides the conditions for the application of the elbow method 2 2 2 2 reading rules of the elbow method during the use of the elbow method in existing studies researchers found that not all sse distributions were uniform especially for complex segmented objects bholowalia and kumar 2014 marutho et al 2018 therefore it is necessary to unify the reading rules of the elbow method by searching the literature we found that there were few reports on the reading rules of the elbow method we have classified the distribution of sse into the following six cases based on the test results of the elbow method on 200 soil profiles from the soil series of china hubei volume and jiangxi volume wang 2017 wang and chen 2020 in general the number of soil horizons does not exceed 10 so this study proposes the reading rules for the elbow method with 2 10 clusters fig 3 a the continuous decline followed by leveling off and selection of the continuous decline end threshold b the rapid decline was followed by a rise and a gentle decline and selection of the rapid decline end threshold c the rapid decline followed by a rise again followed by another rapid decline and selection of the another rapid decline end threshold d the rapid decline followed by a gentle decline and selection of the rapid decline end threshold e the rapid decline followed by a leveling off 1 2 cluster numbers and again with a rapid decline and selection of the final rapid decline end threshold f the rise occurs first followed by rapid decline again and selection of the rapid decline end threshold 2 2 3 fcm based horizon delineation 2 2 3 1 fuzzy c means algorithm fuzzy c means fcm is a prevalent clustering algorithm delaval et al 2021 mcbratney and odeh 1997 the robustness brought by fcm compared to clear classification is significantly improved in terms of efficiency and convergence ferraro and giordani 2015 as an iterative algorithm in fcm the target object is assigned to c consecutive types individual i in the target object is assigned a membership value of type j each value ranging from 0 to 1 and the sum of the membership equals 1 overall type the membership value is larger if the individual i is more likely to belong to type j the pef modflow obtains the closest type by assigning each image pixel to the class with the largest membership value the fcm iterative process specifies the minimization objective function j and the procedure ends when j reaches an inferior threshold in eq 5 the j inferior threshold is set to 0 001 in the pef modflow 5 j m u c i 1 c i 1 n u i j m d i j 2 where u denotes the membership matrix c denotes the number of clusters n denotes the number of samples uij denotes the membership of sample i in cluster j dij 2 denotes the squared distance from sample i to the center of cluster j and m denotes the fuzzy exponent the larger is m the greater is the fuzziness the cie l a b color system was the unique input color system as the most important parameter in the pef modflow the number of clusters c is determined based on the results of the elbow method however the fuzzy exponent m which defines the fuzziness between clustered clusters is also a non deterministic value we evaluated the impact of changes in the number of fuzzy exponent m on the results 2 2 3 2 rules for boundary determination soil profile horizon boundaries cannot be obtained directly from the fcm clustering results therefore we formulate the following rules for obtaining horizon shape boundaries and horizon linear boundaries based on clustering results where the functions are taken from the matlab 2018b software a the clustering results were extracted in order of c value size and converted to binary images using the imbinarize function in turn b the imfill function was used to fill the holes in the binary image and the bwlabel function and ismember function were used to relabel the connected domain c the boundaries of the largest patches of the binary image were determined using the bwboundaries function d the left right and lower boundaries of each clustering unit were removed and the upper boundary was used as the adjacent boundary between the two horizons e detects the starting and ending pixel rows of the boundary and calculates the average value of the existing columns to make up for the missing values f the movmean function was used to smooth all boundaries in turn g the flows of all clusters were completed in turn and all horizon boundaries were displayed in the soil image to complete soil profile horizon shape boundaries delineation h calculate the column means of the horizon shape boundary from the top down and fill in all the row values to complete soil profile horizon linear boundaries delineation 3 depth range of horizons based on the total depth of the profile sd entered by the user the depth range for each horizon h n is calculated using the following formula eq 6 6 h n 0 d 1 n 1 d n 1 d n 1 n n d n 1 s d n n where h n 1 denotes the depth range of the first horizon h n n denotes the depth range of the last horizon n denotes the number of horizons acquired by the elbow method sd denotes the total depth cm of the soil profile as entered by the user d n denotes the depth cm of the n 1 st boundary acquired by pef modflow eq 7 7 d n b d n 1 s d c o l where b d n 1 denotes the number of columns at the n 1 st boundary sd denotes the total depth cm of the soil profile as entered by the user col denotes the total number of columns in the soil profile image 2 3 evaluation of the delineation effect 2 3 1 accuracy and stability of the elbow method the results of sse are affected by the location of the clustered centroid which is unstable and requires multiple tests in general to investigate the accuracy and stability of the elbow method we performed 10 replications of the same soil profile and calculated its standard deviation and mean absolute error 2 3 2 the accuracy of the horizon linear boundary the horizon shape boundary was described using distinctness and topography schoeneberger et al 2012 zhang 2001 zhang and hartemink 2019 the distinctness indicates the difference between the highest and lowest point of the horizon shape boundary d which were defined as five levels very abrupt d 0 5 cm abrupt 0 5 d 2 cm clear 2 d 5 cm gradual 5 d 15 cm and diffuse d 15 cm the topography indicates the lateral undulation and continuity of the boundary which were divided into four levels smooth wavy irregular and broken the specific criteria are shown in table 2 and fig 4 2 3 3 the accuracy of the horizon linear boundary the accuracy of the horizon linear boundary was evaluated by the absolute error ae eq 8 and the mean absolute error mae eq 9 8 a e n n 1 d n d f n 1 n n 9 m a e 1 n 1 n 1 n 1 a e n 1 n n where a e n 1 n denotes the absolute error of the linear boundary between the nth and n 1 th horizon d n denotes the depth cm of the n 1 st boundary acquired by pef modflow eq 7 d f n denotes the depth cm of the nth horizon acquired by field delineation n denotes the number of clustered horizons mae denotes the mean absolute error of the horizon linear boundary if number of clustered horizons number of field delineated horizons then the average error in the remaining part of the horizon linear boundary is shown in eq 10 10 a e m d m d f m m m n where a e m denotes the average error in the remaining part of the horizon linear boundary m denotes the number of field delineated horizons d f m denotes the lower boundary depth cm of the last horizon of the field delineated 11 m m a e 1 p p 1 p m a e n 1 p p where m m a e denotes the mean of multiple maes p denotes the number of maes 3 results 3 1 color distribution of soil profiles the mean values of l a and b were calculated separately for each row of pixels in the fifteen soil profile images fig 5 the ranges of color mean in all images were 16 5 87 5 l 9 3 to 36 5 a and 13 9 to 49 9 b indicating that l has a wider distribution and that the soil color was reddish yellow overall as the depth of the profile changes l shows greater fluctuation partly due to the presence of features such as mottling in the profile t1 v3 v10 etc and partly due to unevenness in the profile caused by texture and gravels t3 t5 v1 v8 etc small amounts of rhizomes and leaves may be present in the surface layer of soil profile under some conditions which causes a to change around the 0 scales a change in the color parameter a from negative to positive indicates a change in color from green to red t1 t2 v2 and v3 based on the results of the horizon linear boundary delineated in the field we calculated the color difference between adjacent horizons separately fig 6 based on the 15 profiles the difference between l a and b ranges from 15 0 to 19 1 9 1 to 13 7 and 9 6 to 21 6 respectively the difference in l between horizons was generally higher for 72 0 of adjacent horizons l had the maximum difference the color variability between broad classes of horizons is higher than between sub horizons the mean l a and b differences between horizons of a b are 8 0 4 1 and 5 9 respectively and between horizons of b c are 9 6 3 6 and 4 2 respectively while the mean l a and b differences between all sub horizons are 6 1 2 7 and 4 6 respectively 3 2 evaluation of the pef modflow the influence of the key parameters compactness c superpixels s and fuzzy exponent m in pef modflow on the results of horizon linear boundary delineation was analyzed in five test profiles details of which are provided in the supplementary material by testing the optimum number of parameters was chosen as c 30 s 200 and m 2 25 ten validation profiles were used to evaluate the complete performance of the pef modflow 3 2 1 number of soil profile horizons we repeated the elbow method operation ten times for the ten evaluation profiles under the 100 iteration conditions fig 7 the optimum number of clusters was obtained according to the read rule of the elbow method table 3 the shape of the sse distribution for the ten profiles has obvious inflection points allowing the optimum number of horizons to be obtained quickly ten runs of the elbow method in each profile resulted in similar sse shapes which in turn obtained the same optimum number of horizons the optimum number of clusters for five soil profiles was the same as the number in the field including one anthrosols v3 two acrisols v4 and v5 and two luvisols v8 and v10 although the remaining five profiles did not achieve the desired elbow method results they exhibited small errors in the number of horizons with a maximum of 2 3 2 2 horizon shape boundary the horizon shape boundaries delineated by field and pef modflow are shown in fig 8 the results of the evaluation of horizon shape boundaries are shown in table 4 of the horizon shape boundaries delineated by pef modflow 45 2 are gradual followed by diffuse 29 0 and clear 25 8 overall the boundary between horizons a and b is more clearly delineated while the boundary between sub horizons within horizon b is more gradual most of the horizon shape boundaries delineated by pef modflow are wavy and then irregular the field delineated horizon shape boundaries are also gradual and wavy has the highest percentage the pef modflow and field delineated horizon shape boundaries have the same topography in 64 3 and the same distinctness in 35 7 of the horizon boundaries 3 2 3 horizon linear boundary the horizon linear boundaries delineated by field and pef modflow are shown in fig 9 the absolute error ae and mean absolute error mae of the horizon linear boundaries were further calculated and are shown in fig 10 and table 5 respectively the mmae for the 10 validation profiles was 13 8 cm with a standard deviation of 9 1 cm among them v5 has the smallest mae with a value of 2 3 cm and v9 has the largest with a value of 32 3 cm the number of horizons obtained by elbow based methods affects the accuracy of the horizon linear boundaries the accuracy of horizon linear boundaries is influenced by the accuracy of the number of horizons the mmae in the five profiles where the number of horizons obtained based on the elbow method was the same as the number of horizons delineated in the field was 9 9 cm the mmae in the remaining five validation profiles was 17 8 cm the absolute error of the horizon linear boundary is influenced by the type of adjacent horizons the adjacent boundaries of ap1 and ap2 in anthrosols generally showed the most excellent accuracy as shown in fig 10 the absolute errors of ap1 ap2 in v1 v2 and v3 are 1 cm 1 cm and 5 cm respectively the linear boundaries of the adjacent horizons in horizons a and b are more accurate than the results of the delineation of adjacent sub horizon boundaries within horizon b the total mean absolute error between the adjacent horizon linear boundaries of horizons a and b for the 10 validation profiles was 9 4 cm and the mean total absolute error between adjacent sub horizons within horizon b was 16 8 cm 4 discussion 4 1 soil profile horizon delineation the soil profile horizon is an important object for soil profile observation and provides a vehicle for recording and communicating soil profile observation results hartemink et al 2020 differences in soil formation processes result in a variety of horizon shapes but they are generally defined as being approximately parallel to the soil surface iuss working group wrb 2015 soil survey staff 2014 as a result most scientists were trained to prioritize the identification of areas with parallel soil surfaces based on field observations hartemink et al 2020 the process of delineating soil profile horizons in the field involves some subjective judgment boone et al 1999 hartemink et al 2020 some soil scientists tend to distinguish horizons based on small variations such as color and structure while others only if they consider the variation in at least one property to be significant boone et al 1999 the pef modflow is the first complete framework for soil profile horizon delineation based on soil color captured by smartphone images the stable application framework defines uniform and objective standards thus enhancing the stability and comparability of the output the number of horizons is the key information for the soil profile horizon delineation and is also vulnerable to changes in anthropogenic delineation standards under fixed depth conditions changes in the number of horizons can directly alter the thickness information of multiple horizons thus affecting sample collection and subsequent analysis results hartemink et al 2020 the elbow method module follows the principle of minimum variation within classes and maximum variation between classes in clustering therefore the number of horizons obtained by elbow methods has a more objective character combining the emphasis of soil scientists on major horizon feature differences we used slic superpixel preprocessing as the core process for the preprocessing module in pef modflow slic superpixel preprocessing divides the soil profile image into several hundred segments thus effectively reducing the complex color characteristics of soil profile images while preserving the color variability between horizons thereby improving the accuracy of subsequent horizon delineation the results of the evaluation show that pef modflow enables stable acquisition of the number of horizons as well as rapid delineation of horizon shape boundaries and linear boundaries in the soil profile in addition pef modflow can be used to obtain the depth range of individual horizons by entering the total depth of the horizon 4 2 key parameters of pef modflow the compactness c superpixels s and fuzzy exponent m are key parameters that can be adjusted in pef modflow in the supplementary material we evaluated the effect of the three parameters on the accuracy of the horizon linear boundaries mmae using a cross combination approach different combinations of c s m were obtained in the five profiles as optimum parameters for pef modflow depending on the results of mmae the low sensitivity of mmae to changes in the number of c fig s5 means that there is a wide range of c values that can be applied widely in contrast the sensitivity of mmae to changes in the number of s and m was higher and showed a different distribution pattern figs s5 and s6 as s varies the mmae appears to vary irregularly fig s5 b which means that it is difficult to find a uniform optimum number an excessively low number of superpixels creates a risk of objects of interest being hidden neubert and protzel 2012 an excessively high number of superpixels does not simplify the soil profile information and thus reduces the performance of the pre processing module we found that the stability and accuracy of mmae were improved when m was between 2 and 2 5 figs s5 c and s7 this means that there is an optimal range of m values that can be applied to pef modflwow this result was also confirmed in a digital soil mapping case study sun et al 2012 we selected the three parameter values that correspond to the smallest tm value to be applied to pef modflow c 30 s 200 m 2 25 these numbers of parameters have shown good performance in 10 validation profiles this research only provided a limited number of case studies due to a limited budget future studies should focus on the relationship between the number of parameters and the characteristics of the soil profile image 4 3 impact of smartphone images on results the smartphone s built in camera has the same imaging theory as a professional camera but the differences in hardware lead to deficiencies in image quality for example the size of the smartphone s camera sensor is smaller due to the bulk limitation which has an impact on the sharpness and color saturation of images in addition most smartphones do not have the unique features of a professional camera such as shutter speed adjustment smartphones can operate over a fairly wide range of light intensities but changes in lighting conditions can have a variety of effects on the results of smartphone photography aitkenhead et al 2016 therefore we recommend using a smartphone to photograph the soil profile in daylight hours when the light intensity is suitable and a whiteboard or greyboard to calibrate the colors the global spread of smartphones has created a platform for the promotion of scientific research igoe et al 2014 we have therefore tested the pef modflow on soil profile images obtained from smartphones the color difference between horizons is the key to obtaining the desired results with pef modflow for example the color difference between horizons a and b is greater resulting in higher horizon linear boundary accuracy the widespread color differences between soil profile horizons in china have been confirmed liu et al 2020 which means that the pef modflow has a great potential application the results of the application show that smartphones can restore the color differences between horizons thus enabling soil profile horizon delineation based on soil color captured by smartphone images thus for soil profiles the images acquired by smartphones can meet the application requirements although we recognize that further testing of the pef modflow is needed these results show considerable promise 4 4 limitations and prospects of pef modflow the pef modflow implements soil profile horizon delineation based on soil color captured by smartphone images the current soil profile horizon delineation is preliminary due to the limitations of the input information field conditions allow soil scientists to distinguish horizons based on differences in color structure plinthite argillan and other features considering the importance of soil color in the horizon delineation and its more mature acquisition method we have used the color parameter as input information for the pef modflow the main advantages of the pef modflow are its flexibility of customization scalability and interpretability currently digital soil morphometrics has attracted widespread interest among soil scientists as technology improves the information in the soil profile image will be exploited even more hartemink and minasny 2014 future research could focus on obtaining information such as texture structure plinthite and argillan based on soil profile images with the refinement of the above methods the number of soil horizons and the accuracy of the delineation results will be further improved the long term goal of digital soil morphometrics aims to achieve rapid access to information on soil profiles hartemink and minasny 2014 nagy et al 2016 pef modflow based on soil profile images acquired by smartphones can make substantial progress towards this goal as the first complete application framework for automated soil profile analysis pef modflow can be used as the basis for smartphone software for soil profile analysis 5 conclusions this study provides the pef modflow for achieving preliminary soil profile horizon delineation based on soil color captured by smartphone images the horizon linear boundary results have varying degrees of sensitivity to compactness c superpixels s and fuzzy exponent m the compactness and fuzzy exponent exists for a suitable range to maintain a stable and accurate result output for pef modflow the results show that the preprocessing module effectively enhances the color differences between soil profile horizons the elbow module enables rapid acquisition of the optimum number of soil profile horizons the fcm based horizon delineation module obtains horizon shape boundaries and horizon linear boundaries the pef modflow is the first complete framework for automated soil profile horizon delineation the current soil profile horizons delineated by pef modflow are still preliminary limited by the lack of information available in the soil profile image the main advantages of the pef modflow in addition to its performance are its flexibility of customization scalability and interpretability as the ability to visualize soil profile information improves the accuracy of pef modflow after refining the input data is bound to be further improved smartphones have been shown to store color differences in soil profile horizons which can be used as a tool for automated soil profile analysis this is an exciting result considering that the pervasiveness of smartphones promises to make it a reality for anyone to quickly access information in the soil subsurface future research could be devoted to the visualization of information in soil profile images acquired based on smartphones and the simultaneous development of applications for attribute identification soil type analysis and management decisions 6 software availability we implemented the pef modflow with matlab r2018b software and the code consists of the main code and 11 function codes the 11 functions contain 1 pre processing function 1 elbow method function and 9 different numbers of horizon delineation functions the source code for pef modflow is available at https github com y jiawei pef modflow git the operating manual for the pef modflow is provided in appendix b declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was jointly financially supported by the national key r d program of china 2021yfd1500803 the national natural science foundation of china 41877071 42077065 42171349 and the special project of national science and technology basic research of china no 2014fy110200a16 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105423 
25557,soil profile horizon delineation 4 2 key parameters of pef modflow 4 3 impact of smartphone images on results 4 4 limitations and prospects of pef modflow 5 conclusions 6 software availability acknowledgments appendix a supplementary data achanta 2012 2274 2282 r aitkenhead 2016 89 110 m estimatingsoilpropertiesamobilephonedigitalsoilmorphometricsprogressinsoilscience aitkenhead 2020 105322 m bholowalia 2014 17 24 p boone 1999 3 28 r standardsoilmethodsforlongtermecologicalresearch soilsamplingpreparationarchivingqualitycontrol bridges 1993 363 373 e 1978 recommendationsuniformcolorspacescolordifferenceequationspsychometticcolorterms churchman 2010 214 221 g churchward 1961 73 86 h delaval 2021 105005 a ferraro 2015 1 16 m ganesan 2010 393 397 p segmentationedgedetectioncolorimagesusingcielabcolorspaceedgedetectors gong 2001 z chinesesoiltaxonomyrevisedinenglish gurrin 2013 308 313 c hartemink 2014 305 317 a hartemink 2020 125 185 a soilhorizonvariationareview hernandezgomez 2009 107 110 g 2009internationalconferenceelectricalcommunicationscomputers naturalimagesegmentationusingcielabspace hettiarachchi 2017 119 135 r igoe 2014 586 592 d iussworkinggroupwrb 2015 worldreferencebaseforsoilresources2014internationalsoilclassificationsystemfornamingsoilscreatinglegendsforsoilmapsworldsoilresourcesreportsno106 johnson 1990 306 319 d lee 2012 504 529 s liu 2020 114556 f marutho 2018 533 538 d 2018internationalseminarapplicationfortechnologyinformationcommunicationisemantic18semarangindonesia determinationclusternumberkmeanusingelbowmethodpurityevaluationheadlinenews mcbratney 1997 85 113 a nagy 2016 365 381 j digitalsoilmorphometrics digitalsoilmorphometricsbringsrevolutionsoilclassification neubert 2012 p proceedingsforumbildverarbeitung superpixelbenchmarkcomparison roudier 2016 113 132 p digitalsoilmorphometrics advancestowardsquantitativeassessmentssoilprofileproperties schoeneberger 2012 p fieldbookfordescribingsamplingsoilsversion30 2014 keyssoiltaxonomy sun 2012 24 34 x swetha 2020 r wang 2017 t soilserieschinahubeivolume wang 2020 t soilserieschinajiangxivolume yang 2021 115365 j 2001 soilseriesresearchmappinginchinese zhang 2012 g soilsurveylaboratorymethodsinchinese zhang 2019 97 115 y yangx2022x105423 yangx2022x105423xj 2024 06 08t00 00 00 000z 2024 06 08t00 00 00 000z http creativecommons org licenses by nc nd 4 0 2022 elsevier ltd all rights reserved 2022 06 12t06 17 09 602z http vtw elsevier com data voc addontypes 50 7 aggregated refined 0 https doi org 10 15223 policy 017 https doi org 10 15223 policy 037 https doi org 10 15223 policy 012 https doi org 10 15223 policy 029 https doi org 10 15223 policy 004 item s1364 8152 22 00129 3 s1364815222001293 1 s2 0 s1364815222001293 10 1016 j envsoft 2022 105423 271872 2022 10 15t20 46 01 532924z 2022 09 01 2022 09 30 1 s2 0 s1364815222001293 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 main application pdf be1451e6d15ff2ef91097e63d6ffb2b3 main pdf main pdf pdf true 15244963 main 15 1 s2 0 s1364815222001293 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 preview image png 86eec524788fe5df5c6efe374f7c5c52 main 1 png main 1 png png 60351 849 656 image web pdf 1 1 s2 0 s1364815222001293 gr9 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr9 downsampled image jpeg 8791586ae7ddf47e37ae9bd29e860b36 gr9 jpg gr9 gr9 jpg jpg 216494 433 1024 image downsampled 1 s2 0 s1364815222001293 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr1 downsampled image jpeg 65e6f2f9bd9c354970d3e1540e23f913 gr1 jpg gr1 gr1 jpg jpg 270344 672 624 image downsampled 1 s2 0 s1364815222001293 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr2 downsampled image jpeg bcbf75871ace0974a0b4ed93cdda4b13 gr2 jpg gr2 gr2 jpg jpg 162158 405 810 image downsampled 1 s2 0 s1364815222001293 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr3 downsampled image jpeg 687a5d5e28b94a6114b79f8057f8327f gr3 jpg gr3 gr3 jpg jpg 140315 476 597 image downsampled 1 s2 0 s1364815222001293 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr4 downsampled image jpeg 14ae6557bd2e50219eafbb875ffedb84 gr4 jpg gr4 gr4 jpg jpg 107922 454 388 image downsampled 1 s2 0 s1364815222001293 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr5 downsampled image jpeg a2badb72a0d36fb1f88cb4f6c8b25a10 gr5 jpg gr5 gr5 jpg jpg 245520 402 1024 image downsampled 1 s2 0 s1364815222001293 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr6 downsampled image jpeg e35a48a49bce856e374c27b094ad3c80 gr6 jpg gr6 gr6 jpg jpg 191505 386 1025 image downsampled 1 s2 0 s1364815222001293 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr7 downsampled image jpeg 3a32e47f45826dc29e9136e707be6cf3 gr7 jpg gr7 gr7 jpg jpg 180615 408 1024 image downsampled 1 s2 0 s1364815222001293 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr8 downsampled image jpeg b9451d6bc33493c7768919c3876aa8c8 gr8 jpg gr8 gr8 jpg jpg 168996 343 811 image downsampled 1 s2 0 s1364815222001293 gr10 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr10 downsampled image jpeg b15a96037b563543543905c32781ae10 gr10 jpg gr10 gr10 jpg jpg 139650 314 811 image downsampled 1 s2 0 s1364815222001293 gr9 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr9 thumbnail image gif a187d8c817eb3a469a2615d9cf7539ee gr9 sml gr9 gr9 sml sml 83420 93 219 image thumbnail 1 s2 0 s1364815222001293 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr1 thumbnail image gif d489e17b1fd9fb5c9f93447289dec9d7 gr1 sml gr1 gr1 sml sml 91950 164 152 image thumbnail 1 s2 0 s1364815222001293 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr2 thumbnail image gif b58e252e46b3a9024fcff4f982c585b0 gr2 sml gr2 gr2 sml sml 76002 109 219 image thumbnail 1 s2 0 s1364815222001293 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr3 thumbnail image gif 7fe02d9b3ff90abaac68ce2f4230283f gr3 sml gr3 gr3 sml sml 78095 164 205 image thumbnail 1 s2 0 s1364815222001293 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr4 thumbnail image gif 4a7fb5340eb549cc0e21f2866443496c gr4 sml gr4 gr4 sml sml 75400 164 140 image thumbnail 1 s2 0 s1364815222001293 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr5 thumbnail image gif e3d60386f136458e6aee5355c3ebcef9 gr5 sml gr5 gr5 sml sml 79752 86 219 image thumbnail 1 s2 0 s1364815222001293 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr6 thumbnail image gif 79496fc301a78ec8e17dfebf7b2b87b6 gr6 sml gr6 gr6 sml sml 77665 83 219 image thumbnail 1 s2 0 s1364815222001293 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr7 thumbnail image gif 007ca18889c300b16ead0947f02bcd82 gr7 sml gr7 gr7 sml sml 75108 87 219 image thumbnail 1 s2 0 s1364815222001293 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr8 thumbnail image gif de58f7d473832e90d079e5740486cab2 gr8 sml gr8 gr8 sml sml 80456 93 219 image thumbnail 1 s2 0 s1364815222001293 gr10 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr10 thumbnail image gif ed75a3adb8ac3674e0da56889fbd02bd gr10 sml gr10 gr10 sml sml 74077 85 219 image thumbnail 1 s2 0 s1364815222001293 gr9 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr9 highres image jpeg 733bded280153100a1df6158d6bd7448 gr9 lrg jpg gr9 gr9 lrg jpg jpg 1206919 1918 4535 image high res 1 s2 0 s1364815222001293 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr1 highres image jpeg 9faaa6aaacba81972ea67d792b63e266 gr1 lrg jpg gr1 gr1 lrg jpg jpg 2478771 2978 2764 image high res 1 s2 0 s1364815222001293 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr2 highres image jpeg 09c72e4efbca1e7da8d769309e954041 gr2 lrg jpg gr2 gr2 lrg jpg jpg 845473 1792 3588 image high res 1 s2 0 s1364815222001293 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr3 highres image jpeg 4c5937a79a8b36c4e1f412b17e6b02c6 gr3 lrg jpg gr3 gr3 lrg jpg jpg 634878 2111 2646 image high res 1 s2 0 s1364815222001293 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr4 highres image jpeg d16ffa4467bd889cadeb665874c634bc gr4 lrg jpg gr4 gr4 lrg jpg jpg 322674 2015 1721 image high res 1 s2 0 s1364815222001293 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr5 highres image jpeg f3d2f6676b82e5bcd2fa1357953ce25a gr5 lrg jpg gr5 gr5 lrg jpg jpg 1780459 1781 4534 image high res 1 s2 0 s1364815222001293 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr6 highres image jpeg 0aab5bce91282d4ef4871738d544d41c gr6 lrg jpg gr6 gr6 lrg jpg jpg 1012094 1709 4536 image high res 1 s2 0 s1364815222001293 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr7 highres image jpeg eca12dfb44607e93e6f879529c3e19f4 gr7 lrg jpg gr7 gr7 lrg jpg jpg 941914 1806 4535 image high res 1 s2 0 s1364815222001293 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr8 highres image jpeg ee970e6af49ff7f618f5dd9735cad628 gr8 lrg jpg gr8 gr8 lrg jpg jpg 966728 1519 3590 image high res 1 s2 0 s1364815222001293 gr10 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 gr10 highres image jpeg b568579a2dcd247a1ce9824a42e8fa9e gr10 lrg jpg gr10 gr10 lrg jpg jpg 689152 1389 3591 image high res 1 s2 0 s1364815222001293 mmc1 pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 mmc1 main application pdf c08e2a3dd68b8ac0cba52d7cea64dc22 mmc1 pdf mmc1 mmc1 pdf pdf false 1832204 application 1 s2 0 s1364815222001293 mmc2 pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 mmc2 main application pdf 9cf7267034ca8df4bac3afd08a44afc0 mmc2 pdf mmc2 mmc2 pdf pdf false 612913 application 1 s2 0 s1364815222001293 si19 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml b10574c037d99767bdf668f06931f838 si19 svg si19 si19 svg svg 23426 altimg 1 s2 0 s1364815222001293 si14 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml d3b6d106c436a96848d07e5e7a27e4e9 si14 svg si14 si14 svg svg 59306 altimg 1 s2 0 s1364815222001293 si9 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 01676724f9d2a2cf991c83071d68649b si9 svg si9 si9 svg svg 20951 altimg 1 s2 0 s1364815222001293 si16 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml eb3f1940d9f6131a0f71b41fcfc90076 si16 svg si16 si16 svg svg 26458 altimg 1 s2 0 s1364815222001293 si7 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml e0d973f2090474a662ef85cc9f437a24 si7 svg si7 si7 svg svg 22356 altimg 1 s2 0 s1364815222001293 si15 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml ea6852c500050d39448af35287794f3f si15 svg si15 si15 svg svg 65544 altimg 1 s2 0 s1364815222001293 si17 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 041452a360f2210eab738106c5a221a1 si17 svg si17 si17 svg svg 19130 altimg 1 s2 0 s1364815222001293 si2 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 8ab75ebb96444efce0add2172d6e45d7 si2 svg si2 si2 svg svg 55194 altimg 1 s2 0 s1364815222001293 si21 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 8707546876e1145ce6e28563d7172025 si21 svg si21 si21 svg svg 70096 altimg 1 s2 0 s1364815222001293 si22 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml a9f7f83a96edb37aff269bffe5dd4453 si22 svg si22 si22 svg svg 22565 altimg 1 s2 0 s1364815222001293 si12 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 7f6eb0250400018c9a7d8c382669c9ca si12 svg si12 si12 svg svg 52263 altimg 1 s2 0 s1364815222001293 si6 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml a924fde654255104a123e2461ecb6b76 si6 svg si6 si6 svg svg 67860 altimg 1 s2 0 s1364815222001293 si18 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 983331a22d0694f40d5e3327b24f7dc3 si18 svg si18 si18 svg svg 71867 altimg 1 s2 0 s1364815222001293 si3 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 67a6e963261eb92fc27c96f94c631a0d si3 svg si3 si3 svg svg 53023 altimg 1 s2 0 s1364815222001293 si20 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 3c8c28c696141d87e81b4ab86701228b si20 svg si20 si20 svg svg 22200 altimg 1 s2 0 s1364815222001293 si4 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml a2bede4a760f9d3cf713fc92f19ebfe6 si4 svg si4 si4 svg svg 25071 altimg 1 s2 0 s1364815222001293 si1 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 177a5383667605254ddfb8b1dc9505fe si1 svg si1 si1 svg svg 48759 altimg 1 s2 0 s1364815222001293 si5 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 263e10adecfaeb34bd021ad7d2b69f88 si5 svg si5 si5 svg svg 64221 altimg 1 s2 0 s1364815222001293 si13 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml c94b0135301a0c95548f2063c18fe96d si13 svg si13 si13 svg svg 24145 altimg 1 s2 0 s1364815222001293 si8 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml c8f479ee9e90a436458e4cdcae3fd8a6 si8 svg si8 si8 svg svg 77572 altimg 1 s2 0 s1364815222001293 si11 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml ee5f2f30a97b335e59a58aceb010a522 si11 svg si11 si11 svg svg 14244 altimg 1 s2 0 s1364815222001293 si10 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001293 stripin image svg xml 6ae030c538e5db02a5122517162d8232 si10 svg si10 si10 svg svg 24979 altimg 1 s2 0 s1364815222001293 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 10q3w4rmjv3 main application pdf 193ccaf6179742bbcfb202662c928bd4 am pdf am am pdf pdf false 24811583 aam pdf enso 105423 105423 s1364 8152 22 00129 3 10 1016 j envsoft 2022 105423 elsevier ltd fig 1 fifteen soil profile images acquired by smartphone fig 1 fig 2 an overview of applying the pef modflow for soil profile horizon delineation based on soil color captured by smartphone images fig 2 fig 3 reading rules for the elbow method based on 200 soil profile test images fig 3 fig 4 diagram of the four topography from field book for describing and sampling soils version 3 0 schoeneberger et al 2012 fig 4 fig 5 mean values of l a and b for each row of pixels in the fifteen soil profile images fig 5 fig 6 differences in the mean values of l a and b color between adjacent horizons in the field division of fifteen soil profiles fig 6 fig 7 sse results for ten evaluation profiles with ten repeated runs of the elbow method fig 7 fig 8 horizon shape boundaries by field and pef modflow delineation for the ten evaluation profiles fig 8 fig 9 horizon linear boundary of the field and pef modflow delineation for the ten evaluation profiles fig 9 fig 10 the absolute error ae of horizon linear boundary for the ten evaluation profile fig 10 table 1 description of the fifteen profiles used in this study table 1 id reference wrb soil groups number of horizons horizons and their depth t1 anthrosols 5 ap1 0 13 cm ap2 13 18 cm btr1 18 48 cm btr2 48 87 cm btr3 87 120 cm t2 anthrosols 7 ap1 0 15 cm ap2 15 27 cm btr1 27 43 cm btr2 43 58 cm btr3 58 93 cm btr4 93 100 cm c 100 122 cm t3 cambisols 5 ah 0 12 cm ab 12 45 cm bw 45 60 cm bc 60 100 cm c 100 120 cm t4 luvisols 4 ah 0 25 cm bt 25 50 cm bw 50 65 cm c 65 103 cm t5 lixisols 3 ah 0 25 cm bt1 25 75 cm bt2 75 120 cm v1 anthrosols 5 ap1 0 18 cm ap2 18 32 cm bg 32 56 cm br1 56 83 cm br2 83 120 cm v2 anthrosols 4 ap1 0 22 cm ap2 22 42 cm br1 42 80 cm br2 80 115 cm v3 anthrosols 4 ap1 0 16 cm ap2 16 21 cm br1 21 39 cm br2 39 90 cm v4 acrisols 4 ah 0 15 cm bt 15 65 cm btr1 56 93 cm btr2 93 130 cm v5 acrisols 4 ah 0 18 cm bt1 18 47 cm bt2 47 82 cm c 82 90 cm v6 cambisols 6 ah 0 12 cm br1 12 24 cm br2 24 48 cm br3 48 72 cm br4 72 97 cm bg 97 130 cm v7 cambisols 4 ap 0 20 cm bw1 20 60 cm bw2 60 90 cm bw3 90 110 cm v8 luvisols 3 ah 0 25 cm bt1 25 60 cm bt2 60 120 cm v9 luvisols 4 ah 0 20 cm bt 20 40 cm br 40 75 cm c 75 120 cm v10 luvisols 3 ah 0 16 cm bt1 16 56 cm bt2 56 110 cm table 2 criteria of four topography from field book for describing and sampling soils version 3 0 schoeneberger et al 2012 table 2 topography criteria smooth planar with few or no irregularities wavy width of undulation is than depth irregular depth of undulation is than width broken discontinuous horizons discrete but intermingled or irregular pockets table 3 the optimum number of clusters corresponds to fig 7 table 3 id number of horizons delineated in the field results of elbow method mean standard deviation average absolute error 1 2 3 4 5 6 7 8 9 10 v1 5 7 7 7 7 7 7 7 7 7 7 7 00 0 00 2 00 v2 4 5 5 5 5 5 5 5 5 5 5 5 00 0 00 1 00 v3 4 4 4 4 4 4 4 4 4 4 4 4 00 0 00 0 00 v4 4 4 4 4 4 4 4 4 4 4 4 4 00 0 00 0 00 v5 4 4 4 4 4 4 4 4 4 4 4 4 00 0 00 0 00 v6 6 5 5 5 5 5 5 5 5 5 5 5 00 0 00 1 00 v7 4 3 3 3 3 3 3 3 3 3 3 3 00 0 00 1 00 v8 3 3 3 3 3 3 3 3 3 3 3 3 00 0 00 0 00 v9 4 3 3 3 3 3 3 3 3 3 3 3 00 0 00 1 00 v10 3 3 3 3 3 3 3 3 3 3 3 3 00 0 00 0 00 table 4 evaluation results of horizon shape boundaries by field and pef modflow delineation table 4 field fcm id horizon boundary topography distinctness id horizon boundary topography distinctness v1 ap1 ap2 wavy gradual v1 ap1 ap2 wavy gradual ap2 bg wavy clear ap2 bg wavy gradual bg br1 wavy clear bg br1 wavy diffuse br1 br2 wavy gradual br1 br2 wavy clear v2 ap1 ap2 wavy abrupt br2 br3 wavy gradual ap2 br1 wavy clear br3 br4 irregular gradual br1 br2 wavy clear v2 ap1 ap2 wavy gradual v3 ap1 ap2 smooth clear ap2 br1 irregular gradual ap2 br1 wavy clear br1 br2 wavy clear br1 br2 irregular gradual br2 br3 irregular gradual v4 ah bt wavy gradual v3 ap1 ap2 smooth clear bt btr1 irregular diffuse ap2 br1 irregular gradual btr1 btr2 wavy gradual br1 br2 irregular gradual v5 ah bt1 wavy gradual v4 ah bt wavy clear bt1 bt2 wavy gradual bt btr1 irregular diffuse bt2 c wavy clear btr1 btr2 wavy gradual v6 ah br1 wavy clear v5 ah bt1 irregular gradual br1 br2 smooth abrupt bt1 bt2 irregular diffuse br2 br3 smooth abrupt bt2 c wavy clear br3 br4 wavy clear v6 ah br1 wavy clear br4 bg irregular gradual br1 br2 irregular gradual v7 ap bw1 wavy gradual br2 br3 wavy diffuse bw1 bw2 smooth very abrupt br3 bg wavy gradual bw2 bw3 wavy clear v7 ap bw1 irregular clear v8 ah bt1 wavy gradual bw1 bw2 irregular diffuse bt1 bt2 irregular gradual v8 ah bt1 wavy clear v9 ah bt smooth very abrupt bt1 bt2 irregular diffuse bt br wavy clear v9 ah btr wavy clear br c wavy gradual btr c wavy diffuse v10 ah bt1 wavy gradual v10 ah bt1 wavy gradual bt1 bt2 wavy abrupt bt1 bt2 irregular diffuse table 5 the mean average error mae of horizon linear boundary for the ten evaluation profiles table 5 id v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 mae cm 9 7 11 5 18 3 5 7 2 3 27 4 8 0 9 5 32 3 13 5 pef modflow a framework for preliminary soil profile horizon delineation based on soil color captured by smartphone images jiawei yang a feilong shen a tianwei wang a lingyun wu a zhaoxia li a nian li a lilong dai a jinghui liang b jian zhang a a college of resource and environment huazhong agricultural university wuhan 430070 china college of resource and environment huazhong agricultural university wuhan 430070 china college of resource and environment huazhong agricultural university wuhan 430070 china b college of public administration huazhong agricultural university wuhan 430070 china college of public administration huazhong agricultural university wuhan 430070 china college of public administration huazhong agricultural university wuhan 430070 china corresponding author soil profile horizon delineation provides a vehicle for recording and communicating observations of soil profiles a framework was developed to achieve soil profile horizon delineation based on soil color captured by smartphone images consisting of a preprocessing module an elbow method module and a fuzzy c means fcm algorithm module referred to as pef modflow we have analyzed the effect of key parameters in pef modflow on the results and demonstrate the high performance and interpretability of the pef modflow in the ten validation profiles this work shows that smartphones can be an effective tool for soil profile information collection the results of soil profile horizon delineation are still preliminary due to the limitations of the information available in the soil profile images the flexible adjustment capability of the input data gives the pef modflow the potential for further improvement the pef modflow represents a key step towards smartphone based access to soil profile information keywords digital soil morphometrics soil profile modflow superpixel preprocessing elbow method 1 introduction soil profiles record past and present soil processes and can be used to study soil formation soil processes and soil properties churchward 1961 johnson et al 1990 roudier et al 2016 horizon delineation provides a vehicle for recording and communicating soil profile observations which can be considered the most important part of the soil profile bridges 1993 hartemink et al 2020 however achieving soil profile horizon delineation is a product of combined training and experience in soil science and is undoubtedly difficult for non specialists churchman 2010 hartemink et al 2020 furthermore even for highly experienced soil surveyors there are big differences in how much difference is needed to identify a horizon or subhorizon hartemink et al 2020 currently digital soil morphometrics which aims to describe digitally extract and quantify soil profile properties continue to progress hartemink and minasny 2014 among them proximal sensors such as portable x ray fluorescence pxrf and visible near infrared vis nir hyperspectral imaging were applied to soil profile property identification and horizon delineation studies hartemink and minasny 2014 in addition researchers have implemented horizon clustering delineation based on the profile images acquired by the professional camera zhang and hartemink 2019 the popularity of smartphones has provided unprecedented opportunities for a wider group of people to participate in collaborative scientific observations that were once out of reach due to cost accessibility and ease of use igoe et al 2014 currently smartphones with built in low cost cameras have been widely used in scientific research including in environmental monitoring and medicine gurrin et al 2013 lee et al 2012 soil color is one of the most important bases for soil profile horizon delineation smartphones can easily access the rich color information of soil and the lower price and simple operation method compared to pxrf and other professional equipment give it strong potential for application yang et al 2021 the researchers have implemented methods to predict soil organic matter ph and soil texture based on soil color captured from smartphone images aitkenhead et al 2016 2020 swetha et al 2020 yang et al 2021 texture information in soil images acquired by smartphones can be used to predict soil texture but current technology does not allow for an accurate assessment of soil texture in soil profile images swetha et al 2020 previous studies have focused mainly on the use of smartphones in soil samples soil profile analysis based on smartphone images would have broader application prospects horizon delineation based on soil profile images obtained from smartphones has an important significance for the rapid acquisition of soil profile information unsupervised learning clustering is the mainstream approach to image segmentation in which image information is assigned to different categories according to similarity differences hettiarachchi and peters 2017 as key information for unsupervised learning of cluster segmentation the representation of color in soil profiles is complex for example there are still color differences within the same horizon of a soil profile and color differences between adjacent horizons may not be significant hartemink et al 2020 soil survey staff 2014 the complex color characteristics create difficulties for horizon delineation soil scientists usually emphasize the importance of major feature differences between soil profile horizons in the field horizon delineation process hartemink et al 2020 effective image preprocessing methods that weaken the smaller color differences within the soil profile horizons and enhance the color differences between horizons are key to improving the accuracy of the horizon delineation results in addition the number of clusters i e the number of horizons is a mandatory input element for image clustering segmentation however for the characteristics of the soil profile it is still difficult how this information can be accurately obtained from the soil profile image the horizon boundaries of the soil profile can be divided into two main forms horizon shape boundary and horizon linear boundary the horizon shape boundary is usually a curve roughly parallel to the surface and has many different shapes mainly related to soil development processes schoeneberger et al 2012 zhang 2001 the horizon linear boundary is the depth range of the soil horizon determined based on the horizon shape boundary which is more beneficial for the communication and dissemination of the horizon delineation results horizon linear boundary information is an important result of the soil profile survey project the purpose of this study is to develop a framework for preliminary soil profile horizon delineation based on soil color captured from smartphone images the framework contains three major components preprocessing of soil profile images elbow method based optimum number of profile horizons and fcm based horizon delineation referred to as pef modflow we have analyzed the parameter configuration and evaluated the performance of pef modflow using fifteen soil profiles from the hubei and jiangxi provinces in china the pef modflow is the first application framework for soil profile analysis based on smartphone images that can advance the development of research and applications for automated access to soil subsurface information 2 materials and methods 2 1 study profiles a total of fifteen soil profiles were excavated in jiangxi and hubei provinces china for the key parameters acquisition and method evaluation of the pef modflow these profiles are part of the soil series of china hubei volume and jiangxi volume wang 2017 wang and chen 2020 all profiles were kept in direct sunlight under natural conditions and maintained as smooth a plane as possible while remaining uniformly moist the images were taken vertically on the soil profile with a smartphone model htc one and saved in jpg high resolution format the htc one has an ultrapixel sensor an aperture size of f 2 0 and output pixels of 4 million we used a standard whiteboard to calibrate the white balance of the smartphone camera before shooting the soil horizons were divided in the field based on the standards of the soil series research and mapping standard zhang 2001 while samples were collected for indoor property analysis to determine the soil classification names gong 2001 zhang and gong 2012 by considering soil type the range of color distribution the number of horizons five soil profiles were selected for analysis of the effect of the number of key parameters on pef modflow t1 t5 containing two anthrosols one cambisols one luvisols and one lixisols the remaining soil profiles were used for evaluating the accuracy of the pef modflow results v1 v10 containing three anthrosols two acrisols two cambisols and three luvisols the fifteen soil profile images acquired by the smartphone are shown in fig 1 and specific information on these profiles is presented in table 1 the horizon shape boundaries and horizon linear boundaries determined in the field for the fifteen soil profiles are detailed in fig s1 and fig s2 2 2 pef modflow the pef modflow consists of three major components i preprocessing of soil profile images ii elbow method based to obtain the optimum number of profile horizons and iii fcm based horizon delineation the overall framework is presented in fig 2 and the codes were written in the matlab programming language the details of the three major components of the framework are introduced below 2 2 1 preprocessing of soil profile images 2 2 1 1 image cropping the purpose of image cropping is to remove distracting information that may be present in the image such as the sky and plant leaves the disturbance information differs considerably from the color of the soil profile and can be easily classified as a separate category thus affecting the final result image cropping is performed by the imcrop function in matlab as shown in fig 2 the image is cropped to a rectangle along the upper boundary line of the soil profile 2 2 1 2 color conversion the color parameters are important input data for the pef modflow the ciel a b color system has been widely used in natural image segmentation research ganesan et al 2010 hernandez gomez et al 2009 the ciel a b color system was defined by the international commission on illumination cie where l represents lightness ranging from 0 black to 100 white and a and b are chromaticity coordinates representing the change from red a to green a and from yellow b to blue b respectively c i e 1978 the ciel a b color system can represent most soil colors and is consistent with human vision in perception and has been shown to highlight color differences between soil profile horizons more than the rgb color system c i e 1978 zhang and hartemink 2019 therefore the rgb color system of the cropped image is converted to the ciel a b color system color conversion is performed by the rgb2lab function in matlab the pef modflow uses it as the only color input data thus connecting the components 2 2 1 3 superpixel preprocessing superpixel preprocessing is the splitting of an image into many connected groups of homogeneous regions fig 2 a superpixel is a group of pixels nearby and with similar color characteristics within which all the initial pixels are set to the same color superpixel preprocessing is performed by the superpixels function in matlab the core of which is the simple linear iterative cluster slic superpixels slic superpixel is an adaptive k means clustering method where the operator simply sets the number of superpixels s and s initial clustering centers will be generated in a regular network spaced by s pixels achanta et al 2012 the l a b values of the ciel a b color system of each pixel and its x and y coordinates were used as input data all pixels were grouped into clusters through multiple iterations and the orphaned pixels were grouped into the nearest cluster center label the iterative process assigns each pixel to the nearest cluster center using the distance measure d eq 1 which combines the color distance eq 2 and the spatial distance eq 3 1 d d c c 2 d s n 2 2 d c l j l i 2 a j a i 2 b j b i 2 3 d s x j x i 2 y j y i 2 where dc and ds indicate the color distance and spatial distance of pixels i xi yi and i xj yj respectively in which the color uses the l a b color parameters c is the abbreviation for compactness which as an indicator that controls the relative importance of color distance and spatial distance in distance measure d the higher the value of compactness the more regular the shape of the superpixel i e the closer it is to a square the lower the value the more closely the superpixel fits the boundary making it more irregular in shape this indicator is generally chosen as a fixed value achanta et al 2012 indicated that its range is generally 1 40 we evaluated the impact of changes in the number of compactness on the results n is the sampling interval of the clustered centroid with n a s where a is the total number of all pixels in the image and s is the number of superpixels unlike conventional k means clustering methods slic does not compare each pixel with all other pixels but chooses localized comparisons 2 n 2 n to avoid redundant computations and thus improve computational speed the color averages in each superpixel region were calculated separately and assigned to all pixels within the superpixel region 2 2 2 elbow method based optimum number of profile horizons 2 2 2 1 elbow method the elbow method was used to determine the optimum number of clusters corresponding to the number of horizons which was represented by the sum of squared error sse results the sse denotes the sum of the average euclidean distance of each pixel point relative to the centroid eq 4 4 s s e i 1 n p n i p m i 2 where n indicates the number of clusters n i indicates the ith cluster among n clusters p indicates the color parameters of all pixel points in n i that are set as l a b color information in this study and mi indicates the average of the corresponding color parameters of all pixels in n i starting from n 2 as the number of clusters increases the n value gradually approaches the true number of clusters and the sse gradually decreases until the sse no longer varies dramatically therefore the value corresponding to the inflection point of the sse shape is often considered the optimum number of clusters the number of iterations of the elbow method is set to 100 the shape of the sse of the soil profile before and after slic superpixel preprocessing is shown in fig s3 the figure shows that the slic superpixel preprocessing effectively reduces the complex color distribution conditions of the soil profile images which in turn provides the conditions for the application of the elbow method 2 2 2 2 reading rules of the elbow method during the use of the elbow method in existing studies researchers found that not all sse distributions were uniform especially for complex segmented objects bholowalia and kumar 2014 marutho et al 2018 therefore it is necessary to unify the reading rules of the elbow method by searching the literature we found that there were few reports on the reading rules of the elbow method we have classified the distribution of sse into the following six cases based on the test results of the elbow method on 200 soil profiles from the soil series of china hubei volume and jiangxi volume wang 2017 wang and chen 2020 in general the number of soil horizons does not exceed 10 so this study proposes the reading rules for the elbow method with 2 10 clusters fig 3 a the continuous decline followed by leveling off and selection of the continuous decline end threshold b the rapid decline was followed by a rise and a gentle decline and selection of the rapid decline end threshold c the rapid decline followed by a rise again followed by another rapid decline and selection of the another rapid decline end threshold d the rapid decline followed by a gentle decline and selection of the rapid decline end threshold e the rapid decline followed by a leveling off 1 2 cluster numbers and again with a rapid decline and selection of the final rapid decline end threshold f the rise occurs first followed by rapid decline again and selection of the rapid decline end threshold 2 2 3 fcm based horizon delineation 2 2 3 1 fuzzy c means algorithm fuzzy c means fcm is a prevalent clustering algorithm delaval et al 2021 mcbratney and odeh 1997 the robustness brought by fcm compared to clear classification is significantly improved in terms of efficiency and convergence ferraro and giordani 2015 as an iterative algorithm in fcm the target object is assigned to c consecutive types individual i in the target object is assigned a membership value of type j each value ranging from 0 to 1 and the sum of the membership equals 1 overall type the membership value is larger if the individual i is more likely to belong to type j the pef modflow obtains the closest type by assigning each image pixel to the class with the largest membership value the fcm iterative process specifies the minimization objective function j and the procedure ends when j reaches an inferior threshold in eq 5 the j inferior threshold is set to 0 001 in the pef modflow 5 j m u c i 1 c i 1 n u i j m d i j 2 where u denotes the membership matrix c denotes the number of clusters n denotes the number of samples uij denotes the membership of sample i in cluster j dij 2 denotes the squared distance from sample i to the center of cluster j and m denotes the fuzzy exponent the larger is m the greater is the fuzziness the cie l a b color system was the unique input color system as the most important parameter in the pef modflow the number of clusters c is determined based on the results of the elbow method however the fuzzy exponent m which defines the fuzziness between clustered clusters is also a non deterministic value we evaluated the impact of changes in the number of fuzzy exponent m on the results 2 2 3 2 rules for boundary determination soil profile horizon boundaries cannot be obtained directly from the fcm clustering results therefore we formulate the following rules for obtaining horizon shape boundaries and horizon linear boundaries based on clustering results where the functions are taken from the matlab 2018b software a the clustering results were extracted in order of c value size and converted to binary images using the imbinarize function in turn b the imfill function was used to fill the holes in the binary image and the bwlabel function and ismember function were used to relabel the connected domain c the boundaries of the largest patches of the binary image were determined using the bwboundaries function d the left right and lower boundaries of each clustering unit were removed and the upper boundary was used as the adjacent boundary between the two horizons e detects the starting and ending pixel rows of the boundary and calculates the average value of the existing columns to make up for the missing values f the movmean function was used to smooth all boundaries in turn g the flows of all clusters were completed in turn and all horizon boundaries were displayed in the soil image to complete soil profile horizon shape boundaries delineation h calculate the column means of the horizon shape boundary from the top down and fill in all the row values to complete soil profile horizon linear boundaries delineation 3 depth range of horizons based on the total depth of the profile sd entered by the user the depth range for each horizon h n is calculated using the following formula eq 6 6 h n 0 d 1 n 1 d n 1 d n 1 n n d n 1 s d n n where h n 1 denotes the depth range of the first horizon h n n denotes the depth range of the last horizon n denotes the number of horizons acquired by the elbow method sd denotes the total depth cm of the soil profile as entered by the user d n denotes the depth cm of the n 1 st boundary acquired by pef modflow eq 7 7 d n b d n 1 s d c o l where b d n 1 denotes the number of columns at the n 1 st boundary sd denotes the total depth cm of the soil profile as entered by the user col denotes the total number of columns in the soil profile image 2 3 evaluation of the delineation effect 2 3 1 accuracy and stability of the elbow method the results of sse are affected by the location of the clustered centroid which is unstable and requires multiple tests in general to investigate the accuracy and stability of the elbow method we performed 10 replications of the same soil profile and calculated its standard deviation and mean absolute error 2 3 2 the accuracy of the horizon linear boundary the horizon shape boundary was described using distinctness and topography schoeneberger et al 2012 zhang 2001 zhang and hartemink 2019 the distinctness indicates the difference between the highest and lowest point of the horizon shape boundary d which were defined as five levels very abrupt d 0 5 cm abrupt 0 5 d 2 cm clear 2 d 5 cm gradual 5 d 15 cm and diffuse d 15 cm the topography indicates the lateral undulation and continuity of the boundary which were divided into four levels smooth wavy irregular and broken the specific criteria are shown in table 2 and fig 4 2 3 3 the accuracy of the horizon linear boundary the accuracy of the horizon linear boundary was evaluated by the absolute error ae eq 8 and the mean absolute error mae eq 9 8 a e n n 1 d n d f n 1 n n 9 m a e 1 n 1 n 1 n 1 a e n 1 n n where a e n 1 n denotes the absolute error of the linear boundary between the nth and n 1 th horizon d n denotes the depth cm of the n 1 st boundary acquired by pef modflow eq 7 d f n denotes the depth cm of the nth horizon acquired by field delineation n denotes the number of clustered horizons mae denotes the mean absolute error of the horizon linear boundary if number of clustered horizons number of field delineated horizons then the average error in the remaining part of the horizon linear boundary is shown in eq 10 10 a e m d m d f m m m n where a e m denotes the average error in the remaining part of the horizon linear boundary m denotes the number of field delineated horizons d f m denotes the lower boundary depth cm of the last horizon of the field delineated 11 m m a e 1 p p 1 p m a e n 1 p p where m m a e denotes the mean of multiple maes p denotes the number of maes 3 results 3 1 color distribution of soil profiles the mean values of l a and b were calculated separately for each row of pixels in the fifteen soil profile images fig 5 the ranges of color mean in all images were 16 5 87 5 l 9 3 to 36 5 a and 13 9 to 49 9 b indicating that l has a wider distribution and that the soil color was reddish yellow overall as the depth of the profile changes l shows greater fluctuation partly due to the presence of features such as mottling in the profile t1 v3 v10 etc and partly due to unevenness in the profile caused by texture and gravels t3 t5 v1 v8 etc small amounts of rhizomes and leaves may be present in the surface layer of soil profile under some conditions which causes a to change around the 0 scales a change in the color parameter a from negative to positive indicates a change in color from green to red t1 t2 v2 and v3 based on the results of the horizon linear boundary delineated in the field we calculated the color difference between adjacent horizons separately fig 6 based on the 15 profiles the difference between l a and b ranges from 15 0 to 19 1 9 1 to 13 7 and 9 6 to 21 6 respectively the difference in l between horizons was generally higher for 72 0 of adjacent horizons l had the maximum difference the color variability between broad classes of horizons is higher than between sub horizons the mean l a and b differences between horizons of a b are 8 0 4 1 and 5 9 respectively and between horizons of b c are 9 6 3 6 and 4 2 respectively while the mean l a and b differences between all sub horizons are 6 1 2 7 and 4 6 respectively 3 2 evaluation of the pef modflow the influence of the key parameters compactness c superpixels s and fuzzy exponent m in pef modflow on the results of horizon linear boundary delineation was analyzed in five test profiles details of which are provided in the supplementary material by testing the optimum number of parameters was chosen as c 30 s 200 and m 2 25 ten validation profiles were used to evaluate the complete performance of the pef modflow 3 2 1 number of soil profile horizons we repeated the elbow method operation ten times for the ten evaluation profiles under the 100 iteration conditions fig 7 the optimum number of clusters was obtained according to the read rule of the elbow method table 3 the shape of the sse distribution for the ten profiles has obvious inflection points allowing the optimum number of horizons to be obtained quickly ten runs of the elbow method in each profile resulted in similar sse shapes which in turn obtained the same optimum number of horizons the optimum number of clusters for five soil profiles was the same as the number in the field including one anthrosols v3 two acrisols v4 and v5 and two luvisols v8 and v10 although the remaining five profiles did not achieve the desired elbow method results they exhibited small errors in the number of horizons with a maximum of 2 3 2 2 horizon shape boundary the horizon shape boundaries delineated by field and pef modflow are shown in fig 8 the results of the evaluation of horizon shape boundaries are shown in table 4 of the horizon shape boundaries delineated by pef modflow 45 2 are gradual followed by diffuse 29 0 and clear 25 8 overall the boundary between horizons a and b is more clearly delineated while the boundary between sub horizons within horizon b is more gradual most of the horizon shape boundaries delineated by pef modflow are wavy and then irregular the field delineated horizon shape boundaries are also gradual and wavy has the highest percentage the pef modflow and field delineated horizon shape boundaries have the same topography in 64 3 and the same distinctness in 35 7 of the horizon boundaries 3 2 3 horizon linear boundary the horizon linear boundaries delineated by field and pef modflow are shown in fig 9 the absolute error ae and mean absolute error mae of the horizon linear boundaries were further calculated and are shown in fig 10 and table 5 respectively the mmae for the 10 validation profiles was 13 8 cm with a standard deviation of 9 1 cm among them v5 has the smallest mae with a value of 2 3 cm and v9 has the largest with a value of 32 3 cm the number of horizons obtained by elbow based methods affects the accuracy of the horizon linear boundaries the accuracy of horizon linear boundaries is influenced by the accuracy of the number of horizons the mmae in the five profiles where the number of horizons obtained based on the elbow method was the same as the number of horizons delineated in the field was 9 9 cm the mmae in the remaining five validation profiles was 17 8 cm the absolute error of the horizon linear boundary is influenced by the type of adjacent horizons the adjacent boundaries of ap1 and ap2 in anthrosols generally showed the most excellent accuracy as shown in fig 10 the absolute errors of ap1 ap2 in v1 v2 and v3 are 1 cm 1 cm and 5 cm respectively the linear boundaries of the adjacent horizons in horizons a and b are more accurate than the results of the delineation of adjacent sub horizon boundaries within horizon b the total mean absolute error between the adjacent horizon linear boundaries of horizons a and b for the 10 validation profiles was 9 4 cm and the mean total absolute error between adjacent sub horizons within horizon b was 16 8 cm 4 discussion 4 1 soil profile horizon delineation the soil profile horizon is an important object for soil profile observation and provides a vehicle for recording and communicating soil profile observation results hartemink et al 2020 differences in soil formation processes result in a variety of horizon shapes but they are generally defined as being approximately parallel to the soil surface iuss working group wrb 2015 soil survey staff 2014 as a result most scientists were trained to prioritize the identification of areas with parallel soil surfaces based on field observations hartemink et al 2020 the process of delineating soil profile horizons in the field involves some subjective judgment boone et al 1999 hartemink et al 2020 some soil scientists tend to distinguish horizons based on small variations such as color and structure while others only if they consider the variation in at least one property to be significant boone et al 1999 the pef modflow is the first complete framework for soil profile horizon delineation based on soil color captured by smartphone images the stable application framework defines uniform and objective standards thus enhancing the stability and comparability of the output the number of horizons is the key information for the soil profile horizon delineation and is also vulnerable to changes in anthropogenic delineation standards under fixed depth conditions changes in the number of horizons can directly alter the thickness information of multiple horizons thus affecting sample collection and subsequent analysis results hartemink et al 2020 the elbow method module follows the principle of minimum variation within classes and maximum variation between classes in clustering therefore the number of horizons obtained by elbow methods has a more objective character combining the emphasis of soil scientists on major horizon feature differences we used slic superpixel preprocessing as the core process for the preprocessing module in pef modflow slic superpixel preprocessing divides the soil profile image into several hundred segments thus effectively reducing the complex color characteristics of soil profile images while preserving the color variability between horizons thereby improving the accuracy of subsequent horizon delineation the results of the evaluation show that pef modflow enables stable acquisition of the number of horizons as well as rapid delineation of horizon shape boundaries and linear boundaries in the soil profile in addition pef modflow can be used to obtain the depth range of individual horizons by entering the total depth of the horizon 4 2 key parameters of pef modflow the compactness c superpixels s and fuzzy exponent m are key parameters that can be adjusted in pef modflow in the supplementary material we evaluated the effect of the three parameters on the accuracy of the horizon linear boundaries mmae using a cross combination approach different combinations of c s m were obtained in the five profiles as optimum parameters for pef modflow depending on the results of mmae the low sensitivity of mmae to changes in the number of c fig s5 means that there is a wide range of c values that can be applied widely in contrast the sensitivity of mmae to changes in the number of s and m was higher and showed a different distribution pattern figs s5 and s6 as s varies the mmae appears to vary irregularly fig s5 b which means that it is difficult to find a uniform optimum number an excessively low number of superpixels creates a risk of objects of interest being hidden neubert and protzel 2012 an excessively high number of superpixels does not simplify the soil profile information and thus reduces the performance of the pre processing module we found that the stability and accuracy of mmae were improved when m was between 2 and 2 5 figs s5 c and s7 this means that there is an optimal range of m values that can be applied to pef modflwow this result was also confirmed in a digital soil mapping case study sun et al 2012 we selected the three parameter values that correspond to the smallest tm value to be applied to pef modflow c 30 s 200 m 2 25 these numbers of parameters have shown good performance in 10 validation profiles this research only provided a limited number of case studies due to a limited budget future studies should focus on the relationship between the number of parameters and the characteristics of the soil profile image 4 3 impact of smartphone images on results the smartphone s built in camera has the same imaging theory as a professional camera but the differences in hardware lead to deficiencies in image quality for example the size of the smartphone s camera sensor is smaller due to the bulk limitation which has an impact on the sharpness and color saturation of images in addition most smartphones do not have the unique features of a professional camera such as shutter speed adjustment smartphones can operate over a fairly wide range of light intensities but changes in lighting conditions can have a variety of effects on the results of smartphone photography aitkenhead et al 2016 therefore we recommend using a smartphone to photograph the soil profile in daylight hours when the light intensity is suitable and a whiteboard or greyboard to calibrate the colors the global spread of smartphones has created a platform for the promotion of scientific research igoe et al 2014 we have therefore tested the pef modflow on soil profile images obtained from smartphones the color difference between horizons is the key to obtaining the desired results with pef modflow for example the color difference between horizons a and b is greater resulting in higher horizon linear boundary accuracy the widespread color differences between soil profile horizons in china have been confirmed liu et al 2020 which means that the pef modflow has a great potential application the results of the application show that smartphones can restore the color differences between horizons thus enabling soil profile horizon delineation based on soil color captured by smartphone images thus for soil profiles the images acquired by smartphones can meet the application requirements although we recognize that further testing of the pef modflow is needed these results show considerable promise 4 4 limitations and prospects of pef modflow the pef modflow implements soil profile horizon delineation based on soil color captured by smartphone images the current soil profile horizon delineation is preliminary due to the limitations of the input information field conditions allow soil scientists to distinguish horizons based on differences in color structure plinthite argillan and other features considering the importance of soil color in the horizon delineation and its more mature acquisition method we have used the color parameter as input information for the pef modflow the main advantages of the pef modflow are its flexibility of customization scalability and interpretability currently digital soil morphometrics has attracted widespread interest among soil scientists as technology improves the information in the soil profile image will be exploited even more hartemink and minasny 2014 future research could focus on obtaining information such as texture structure plinthite and argillan based on soil profile images with the refinement of the above methods the number of soil horizons and the accuracy of the delineation results will be further improved the long term goal of digital soil morphometrics aims to achieve rapid access to information on soil profiles hartemink and minasny 2014 nagy et al 2016 pef modflow based on soil profile images acquired by smartphones can make substantial progress towards this goal as the first complete application framework for automated soil profile analysis pef modflow can be used as the basis for smartphone software for soil profile analysis 5 conclusions this study provides the pef modflow for achieving preliminary soil profile horizon delineation based on soil color captured by smartphone images the horizon linear boundary results have varying degrees of sensitivity to compactness c superpixels s and fuzzy exponent m the compactness and fuzzy exponent exists for a suitable range to maintain a stable and accurate result output for pef modflow the results show that the preprocessing module effectively enhances the color differences between soil profile horizons the elbow module enables rapid acquisition of the optimum number of soil profile horizons the fcm based horizon delineation module obtains horizon shape boundaries and horizon linear boundaries the pef modflow is the first complete framework for automated soil profile horizon delineation the current soil profile horizons delineated by pef modflow are still preliminary limited by the lack of information available in the soil profile image the main advantages of the pef modflow in addition to its performance are its flexibility of customization scalability and interpretability as the ability to visualize soil profile information improves the accuracy of pef modflow after refining the input data is bound to be further improved smartphones have been shown to store color differences in soil profile horizons which can be used as a tool for automated soil profile analysis this is an exciting result considering that the pervasiveness of smartphones promises to make it a reality for anyone to quickly access information in the soil subsurface future research could be devoted to the visualization of information in soil profile images acquired based on smartphones and the simultaneous development of applications for attribute identification soil type analysis and management decisions 6 software availability we implemented the pef modflow with matlab r2018b software and the code consists of the main code and 11 function codes the 11 functions contain 1 pre processing function 1 elbow method function and 9 different numbers of horizon delineation functions the source code for pef modflow is available at https github com y jiawei pef modflow git the operating manual for the pef modflow is provided in appendix b declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was jointly financially supported by the national key r d program of china 2021yfd1500803 the national natural science foundation of china 41877071 42077065 42171349 and the special project of national science and technology basic research of china no 2014fy110200a16 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105423 
25558,the flow over uneven topography is a problem of interest in environmental fluid flow modeling including flows over river bedforms exchange flows over oceanic sills or the airflow over mountains the common experimental procedure to investigate these flows moving a small obstacle in a laboratory flume yields experimental difficulties whereas modeling using non linear shallow flow equations does not explain all the flow phenomena novel alternative procedures are presented for the experimentation and shallow water representation of flow interaction with obstacles a large scale obstacle model is constructed in a dam break set up and used to generate flow phenomena over topography including dispersive and broken surges wave reflection hydraulic jumps and non hydrostatic sill overflows simulations are conducted with a shallow water weighted averaged residual flow software for turbulent flows the proposed software reproduces the experiments satisfactorily supporting its use in modeling whereas the new experimental database can be used by modelers to test their software keywords non hydrostatic flows laboratory experiments weighted averaged residual equations lee side waves obstacles software availability name of software waves transformation model software contact emails ag2gaojp uco es z12cachf uco es ag2caoro uco es requirements matlab availability the software platform is freely available at https github com frncch waves transformation model software 1 introduction the study of two dimensional shallow water flows over obstacles is relevant in several branches of environmental mechanics as in the river flow over bedforms like dunes and antidunes in oceanographic exchange flows over a seamount or in the mesoscale atmospheric flow past a steep mountain nadiga et al 1996 zhu and lawrence 1998 a common mathematical procedure to study these flows consists in setting instantaneously an obstacle into an initially steady and uniform stream and then study the time dependent flow adjustment that takes place and the ensuing asymptotic steady state long 1954 1970 pratt 1983 nadiga et al 1996 typically using either the shallow water dispersionless saint venant equations houghton and kasahara 1968 pratt 1983 cea et al 2011 or the dispersive serre green naghdi sgn equations nadiga et al 1996 the experimental procedure to generate these flows consists in rapidly accelerate up to a target constant velocity an obstacle initially at rest in a flume filled with water long 1954 1970 in long s 1970 experiments the obstacle was moved in the flume by a thin line wrapped around a cylindrical winder driven by a motor the velocity of obstacle displacement was experimentally determined by counting the revolutions and then this velocity was used to deduce the flow patterns observed moving with the obstacle e g the equivalent flow with a static obstacle long 1970 this experimental procedure is however not simple and is prone to large number of problems first the experimental design is only economical thus feasible using small obstacles e g in long s 1970 experiments rather small ones only of 9 1 and 2 cm high were installed second the wave generation around the obstacle is induced by moving it thereby producing leakage at the joins with the flume sidewalls and at the obstacle bottom these problems are hardly solvable especially for high froude numbers involving supercritical flows long 1970 in addition to the experimental configuration problems the quality and type of experimental data that can be extracted from this set up are limited for example instantaneous free surface profile measurements of these flows are not available in the literature which would be especially desirable to compare with the theoretical predictions of shallow water models the typical data extracted from long s experiments consists only in the upstream and crest flow depths as well as the froude number long 1970 further the flow over a curved obstacle involves vertical accelerations and thus non hydrostatic bed pressures nadiga et al 1996 zhu and lawrence 1998 castro orgaz and hager 2013 gamero et al 2020 which should be experimentally determined however in long s 1970 set up the obstacle was moving thus it would have been a very challenging task to take bed pressure measurements for the asymptotic steady flow over the obstacle by installing pressure taps at the obstacle surface note that the instantaneous appearance of an obstacle in a stream is not a realistic mechanism of forcing in geophysical fluid flows despite its wide use by environmental flow modelers pratt 1983 thus an alternative method to study wave interaction with obstacles in environmental flows was considered as commented below during the time dependent flow adjustment towards a steady flow over an obstacle shocks are formed moving upstream and downstream of it long 1954 1970 houghton and kasahara 1968 pratt 1983 nadiga et al 1996 with a transition from subcritical to supercritical flow conditions occurring in the vicinity of the obstacle crest naghdi and vongsarnpigoon 1986 zhu and lawrence 1998 at the lee side of the obstacle the flow typically changes from supercritical to subcritical flow conditions through a moving shock displacing away from the obstacle the shocks at the lee side of the obstacle may be either undular or broken depending on the froude number these instantaneous waves are hardly characterized experimentally in the literature and simulations using sgn models nadiga et al 1996 showed inability of this shallow water representation to mimic the turbulent flow processes occurring given that wave breaking bayon et al 2016 gualtieri and chanson 2021 is not modeled if the froude number at the lee side face of the obstacle is high turbulent breaking occurs and the undular waves predicted by sgn models become unrealistic nadiga et al 1996 castro orgaz and chanson 2017 turbulent breaking at the lee side of obstacles is important as for example in a oceanographic flow of salt water moving over a sill in a fresh water environment farmer and denton 1985 denton 1987 in these flows turbulent breaking of the lee side waves induces mixing between layers and thus provides nutrients and dissolved oxygen for the deep water it further affects the dispersion of any pollutant therefore a shallow water turbulent flow model with ability to simulate both undular and broken waves is needed to simulate flow over obstacles the main aim of this research is to characterize the complex features of open channel flows downstream of a bottom hump which cannot be simulated by the standard shallow water equations given that the lee side obstacle waves are neither characterized with detailed experiments nor simulated with shallow water turbulent non hydrostatic models the experimental and numerical modeling of these flows are the two major objectives of this work as described in section 2 of this work lee side obstacle waves are generated experimentally in a large scale obstacle model with a new and entirely different procedure from that of long s a dam break like set up was installed in a long experimental flume consisting in a gate installed downstream of the obstacle and equipped with an instantaneous opening mechanism thereby permitting to generate lee side waves once the gate was opened instantaneously the type of waves generated at the lee face of the obstacle depends on the upstream water depth at the gate which was an experimental parameter to generate the flows this new experimental procedure is different from that of long but it permits to generate waves at the lee side of obstacles and study their interaction with it which was the major objective in long s set up this new experimental set up has the major advantage of permitting to execute accurate experiments at a large scale obstacle model and collecting high quality data including the instantaneous free surface profiles and the steady bed pressures over the obstacle note that the instantaneous appearance of an obstacle in a stream as in long s approach is not a realistic flow however this experimental procedure in addition to serve to study the flow adjustment over an obstacle in response to the generated waves conceptually represents an abrupt drop in the water levels around the obstacle in response to a downstream forcing the value of the new experimental dataset generated in this work relies on two major aspects 1 in long s 1970 experiments the free surface profiles are presented only qualitatively using photos but measurements are not available given the complications to take such readings with a moving obstacle and 2 to the authors knowledge there is no previous work in the literature where waves over a curved obstacle are generated with a dam break like setting shallow water modeling is considered in section 3 a shallow water non hydrostatic flow model was constructed from the reynolds averaged navier stokes equations by using a weighted averaged residual method of galerkin type the model accounts for a non hydrostatic fluid pressure which has the ability to model undular waves further the turbulent velocity profile and turbulent stresses are modeled in the weighted averaged equations permitting to simulate broken waves during the flow adjustment over the obstacle a high resolution finite volume finite difference numerical solver is developed and extensively verified in section 4 using both benchmark numerical tests and the new experimental measurements conducted in this work conclusions are presented in section 5 the model software is freely available on github https github com frncch waves transformation model software whereas the experimental database is provided as supplementary material in the file experiments ems2022 xls 2 experimental characterization 2 1 experimental flume and equips the experiments were conducted in a 15 m long 1 m high 1 m width tilting experimental flume in the hydraulics laboratory at the university of córdoba spain a reduction of the flume width to 0 405 m was accomplished by a moving division wall fig 1 and the flume slope for the experimental series conducted was 0 0015 m m the tailwater portion of the flume from 9 634 m to 15 m downstream the inlet section was structurally a cantilever and the beam deformation though small was considered in the simulations to accurately define the actual bed profile of the flume the flume was equipped with a recirculation pump of 0 078 m3 s maximum discharge connected to a downstream water tank allowing to work in closed circuit a water tank with flow straightener was located at the flume inlet to reduce flow disturbances the tailgate of the flume was fully open or closed depending on the type of experiment e g with or without reflection at the flume end a large scale obstacle of gaussian profile z bg 0 209 x exp 1 2 x x x crest 0 254 2 where z bg is the local obstacle height above the flume bed and x crest the longitudinal location of the crest was installed at x crest 6 565 m a gaussian obstacle shape was selected because it permits to mathematically adjust their parameters in the design phase prior to construction in this way it was possible to adjust the crest curvature until producing the desired degree of non hydrostaticity in the flow over the obstacle further this shape is easy to construct in metal by a manufacturer along the longitudinal symmetry axis of the obstacle 17 piezometric tapings were installed to take bottom pressure head readings in a piezometric panel the flume was equipped with a dam break like set up consisting in a sluice gate of high speed release induced by a pneumatic drive system the gate opening time was less than 0 15 s in all the experiments conducted in this work thus the opening operation can be considered instantaneous a high speed camera fastec ts5 with 50 mm focal length lens to avoid image distortion capturing at up to 253 fps at maximum resolution was used to characterize the movement of the gate and ensure that the operation can be considered fast enough to reproduce dam break like waves flow visualization during the experiments was accomplished through the eight lateral crystal windows 1 875 m wide by 0 975 m high of the flume each window was monitored by a camera perpendicularly installed in front of the flume fig 1 the monitoring video system comprises eight basler ace aca1920 40uc cameras with 6 mm focal length lens to allow capturing the whole width of each lateral crystal window recording 40 frames per second fps maximum at full resolution and a laptop intel core i7 9750h with software for image capture synchronization assembling and processing the system automatically assembles the images collected by the 8 cameras in a synchronized way correcting distortion errors and thereby providing instantaneous experimental images of the 15 m of flume the novelty of this experimental research is in the experimental procedure and set up used to generate waves evolving over obstacles in long s 1970 classical experiments an obstacle is moved in a flume filled with initially still water pratt and whitehead 2007 whereas in this work we have generated the unsteady waves over a fixed obstacle using a dam break like set up 2 2 experimental series two kinds of experimental series were produced the first series consisted of dam break wave experiments generated by using the high speed sluice gate with no inlet flow q 0 and the tailgate fully closed to allow wave reflection the second series consisted of steady flow experiments with various q up to the maximum discharge with the high speed gate deactivated no operation positioned above the flume and the tailgate fully opened the dam break experiments were designed using different downstream to upstream water depth ratios r h d h u with zero inlet discharge and closed tailgate which were organized in tests series comprising r 0 0 397 0 6 and 0 8 table 1 first the flume was filled with water up to the level h d considered and then the high speed sluice gate closed thereafter the upstream side of the gate further filled up to the desired r afterwards the high speed sluice gate was released fig 2 a the monitoring system was set to record images at 25 fps which was enough to take a detailed experimental characterization of the unsteady water waves each test was recorded during 12 s enabling to capture all the relevant hydraulic processes namely the positive and negative dam break wave generation wave reflection at the closed tailwater gate formation of a hydraulic jump at the lee side of the obstacle and interaction of the reflected wave with the flow developing over the obstacle once the images collected by the system of cameras were assembled and distortion errors were corrected for selected instants of time they were used to extract the instantaneous flow profiles fig 2b and c the free surface was digitalized from the images over the free surface curve in the crystal wall using golden software grapher 13 3 754 by trial and error calibration tests it was found that the procedure permits to measure the experimental flow profile with 0 1 cm accuracy for illustration purposes panoramic instants at t 1 6 and 10 s for test 1 are shown in fig 3 using the corrected images of the cameras 4 5 and 6 from left to right in each subfigure an accurate modeling of the upstream boundary condition in the numerical solver requires use as input the time variation of the static upstream water level at the inlet tank solid wall h uw this time dependent variable was measured during the experiments with the high speed camera fastec ts5 and from the ensuing measurements the following 4th order polynomial was found to describe inlet flow conditions h uw m 0 0015 t 4 0 032 t 3 0 1656 t 2 0 2503 t 0 0393 100 h 1 100 r 2 0 98 where h uw is the flow depth at the upstream wall of the water tank t is the time and h 1 is the flow depth level at the upstream section of the flume see fig 1 steady flow experiments were conducted with fully open tail and high speed sluice gates with various discharges up to the maximum of the system 0 1826 m2 s the bed pressure head was measured by visual observation of a piezometric panel allowing readings of accuracy 0 1 cm 3 shallow water turbulent flow modeling 3 1 weighted averaged residual equations consider steady two dimensional flow in a vertical plane fig 4 as in the previously described experiments fig 3 the modeling approximation pursued here entails the development of weighted averaged residual equations from the rans equations of turbulent free surface flow following steffler and jin 1993 in a first step a sequence of vertically averaged and moment vam equations is produced by using the first shifted legendre polynomial as test function resulting steffler and jin 1993 khan and steffer 1996a b 1 h t h u x 0 2 h u t h u 2 x 1 ρ h p σ x x p b σ x b z b x τ x z b 3 h w t h u w x 1 ρ p b h τ x z x τ x z b z b x σ z b g h 4 h 2 h t h z u x z h u x h w 0 5 h z u t h z u 2 x z h u t z h u 2 x h u w 1 ρ h z p x z h p x h p b 2 z b x 1 ρ h z σ x x z h σ x x h σ x b 2 z b x h τ x z b 2 h τ x z 6 h z w t h z u w x z h w t z h u w x h w 2 h ρ p b 2 p 1 ρ h z τ x z x z h τ x z x h τ x z b 2 z b x h σ z b 2 h σ z where x and z are the horizontal and vertical cartesian coordinates respectively u x z t and w x z t are the horizontal and vertical velocity components z b x is the bed profile h x t is the flow depth p x z t is the fluid pressure p b x t is bottom pressure τ x z t and σ x z t are the reynolds tangential and normal stresses respectively ρ is the fluid density g is the gravitational acceleration and t is the time the overbar operator denotes vertically averaged quantities equations 1 3 are the continuity x and z momentum equations respectively while eqs 4 6 are the moment of continuity x and z momentum equations respectively note that z is the elevation of the centroid of a section z b h 2 equations 1 6 are weighted averaged open channel flow equations yet not residual given that u w p are still general in the vam model equations 1 6 predictors for the velocity components u w and the fluid pressure p are required steffler and jin 1993 used finite element type expansions consisting of a base of functions with a series of coefficients independent of the vertical coordinate in particular they expanded u using the first shifted legendre polynomial and w and p using the first and second shifted legendre polynomials resulting steffler and jin 1993 khan and steffer 1996a b 7 u x z t u 0 x t u 1 x t 2 η x z t 1 8 w x z t w b x t w 2 x t 4 η x z t 1 η x z t w s x t η x z t 9 p x z t ρ g h x t p 1 x t p 2 x t 4 η x z t 1 η x z t here u 0 is the depth averaged horizontal velocity u 1 is the x velocity at the free surface in excess of u 0 w 2 is the mid depth z velocity in excess of the average of the vertical velocities at the bed and free surface levels w b and w s are the vertical velocity at the bed and free surface levels respectively p 1 is the bed pressure in excess of hydrostatic p 2 is the mid depth deviation from the linear non hydrostatic law and η is the dimensionless vertical coordinate z z b h the kinematic boundary conditions are expressed as follows cantero chinchilla et al 2018 2020 10 w b u 0 u 1 z b x 11 w s h t u 0 u 1 z s x where z s is the free surface elevation h z b inserting eqs 7 8 which are the trial functions approximating u w p into equation 1 6 the following system of approximate residual partial differential equations pdes results 12 h t q x 0 13 q t x g h 2 2 q 2 h x u 1 2 h 3 h p 1 2 ρ 2 h p 2 3 ρ h σ x ρ g h z b x p 1 ρ z b x τ b ρ 14 h w t q w x 1 6 h u 1 w x p 1 ρ τ b ρ z b x 1 ρ h τ x z x 15 h 2 h t q z x 1 6 h 2 u 1 x h w 16 h u 1 t q u 1 x h 2 ρ p 1 x q u 1 h p 1 2 ρ h x u 1 q x 4 p 2 ρ z x 6 ρ τ b 2 τ x z 6 ρ h h z σ x x z h σ x x 17 t h 2 w 12 x h q w 12 h w 2 h t q w h u 1 w 6 z x x h 2 u 1 10 w w b 3 w s 3 h w 2 2 h p 2 3 ρ h τ b 2 ρ z b x h σ z ρ 1 ρ h z τ x z x z h τ x z x where q is the discharge per unit width hu 0 w is the vertical velocity difference between the bed and free surface levels w b w s τ b is the bed shear stress σ x and τ x z are the depth averaged normal and shear stresses respectively and w 2 w 2 1 12 w b 2 1 12 w s 2 1 6 w b w s 1 20 2 w w b w s 2 the mean vertical velocity is w w b 2 2w 2 3 w s 2 given that the base functions used in the trial solution are used as test functions the vam model is a galerkin type system of weighted averaged residual equations finlayson and scriven 1966 the moment of x momentum eq 16 differs from that previously used cantero chinchilla et al 2018 gamero et al 2020 in that the conservative variable is hu 1 instead of u 1 this modification was found to increase the numerical robustness of the vam model when handling dry wet fronts further eqs 16 and 17 include all turbulent stress terms originating from the weighted averaging process former models in cantero chinchilla et al 2018 and gamero et al 2020 only considered bed shear effects thus here the complete turbulent modeling terms were accounted for in the model equations the following reactive equation is written based on the kinematic boundary conditions 18 w q h u 1 z b x q x q h u 1 z b h x it is a mathematical statement to be verified by the solution at any instant of time turbulence closure is required to estimate τ b σ x σ z τ x z z σ x and z τ x z in eqs 13 14 16 and 17 using an eddy viscosity approach these terms read after averaging 19 σ x 2 ρ ν x h q x u s z s x u b z b x 20 σ z 2 ρ ν z h w s w b 21 τ x z ρ ν z h 2 u 1 h w x w s z s x w b z b x 22 z σ x ρ ν x h 3 3 x q h u 1 x ρ ν x u 1 z b h 2 h x 4 z b x ρ ν x 3 4 u 1 h x 6 z b x q h 6 u 1 z b x 23 z τ x z ρ ν z u 1 2 ρ ν z u 1 z b h ρ ν z h 2 x w w b 6 w s 6 ρ ν z w w b 6 5 w s 6 h x ρ ν z z b w x ρ ν z w w s z b x ρ ν z z b h w w s h x ρ ν z w z b h z b x where ν x and ν z are the vertically averaged eddy viscosities in the x and z directions respectively in the traditional shallow water quasi 3d approach e g rodi 1993 the pressure is assumed to be vertically hydrostatic with corrections using turbulent stresses based on the depth averaged horizontal velocity u 0 in the present formulation pressures are dynamic ones with perturbation parameters p 1 and p 2 which are corrected in the model equations by the depth averaged turbulent stresses considering the non uniform variation of the u velocity with elevation given by u s u b and u 1 as well as the vertical velocity w variation determined by w s w b and w 2 following ghamry and steffler 2002a b the depth averaged eddy viscosities are estimated as fischer et al 1979 24 ν x 0 5 u h 25 ν z 0 07 u h where u is the shear velocity τ b ρ 1 2 the bed shear stress is modeled using manning s formula including vertical velocity effects as follows castro orgaz and hager 2017 cantero chinchilla et al 2018 2020 26 τ b ρ g n 2 u b 2 w b 2 h 1 3 where n is the manning s roughness coefficient 3 2 numerical modeling and software development the software development entails the solution of the turbulent vam model eqs 12 17 through numerical techniques given that an analytical solution of the system of pdes equations is unknown a semi implicit finite volume fv finite difference fd scheme is developed based on former works cantero chinchilla et al 2018 2020 gamero et al 2020 former solvers were prone to numerical instabilities if shocks or moving hydraulic jumps were progressively developed in the solution causing the solution failure in some cases therefore a special feature of the new solver constructed is its ability for handling the formation of shocks with robustness as described below the vam model is in matrix form 27 u t f x s o s τ where vectors u f and s enclose respectively the flow conservative variables fluxes and source terms 28 u h q h w h u 1 h 2 w 12 f q q 2 h g h 2 2 q w q u 1 h q w 12 29 s o 0 x u 1 2 h 3 h p 1 2 ρ 2 h p 2 3 ρ g h z b x p 1 ρ z b x 1 6 h u 1 w x p 1 ρ h 2 ρ p 1 x q u 1 h p 1 2 ρ h x u 1 q x 4 p 2 ρ z x h w 2 q x q w h u 1 w 6 z x x h 2 u 1 10 w w b 3 w s 3 h w 2 2 h p 2 3 ρ s τ 0 1 ρ h σ x x τ b ρ τ b ρ z b x 1 ρ h τ x z x 6 ρ τ b 2 τ x z 6 ρ h h z σ x x z h σ x x h τ b 2 ρ z b x h σ z ρ 1 ρ h z τ x z x z h τ x z x the subscripts o and τ refer to the inviscid and the turbulent stress source terms respectively note that only transport eqs 12 14 and 16 and 17 are contained into eq 27 which together with the reactive eqs 15 and eq 18 conforms the vam model this shallow water vam model is more complex than the saint venant equations but its features are significantly better it may be noted that the rans equations are simpler in its formulation however the computational cost of a full 3d approach is still high at a river scale katopodes 2019 involving the determination of the free surface boundary using the volume of fluid or level set methods in contrast the position of the free surface is directly resolved in this depth averaged formulation note that the vam equations look complex given their long source terms see eqs 29 but the architecture of the equations is similar to that of the standard shallow water equations see eq 27 the solution of this model is vectorized in the present code such that implementation is easy the semi implicit fv fd scheme follows a splitting approach in two stages i a hyperbolic step and ii an elliptical step dividing the x t plane into quadrilateral finite volume cells of dimensions δx δt in the hyperbolic step an intermediate solution is obtained from the homogenous part of eq 27 using a godunov type finite volume scheme toro 2001 2009 30 u ˆ i u i k δ t δ x f i 1 2 f i 1 2 in eq 30 u and f are space and time averaged vectors i is the cell index k is a time index δx is the x dimension of the control volume δt is the t dimension of the control volume the indices i 1 2 refer to the control volume interfaces between cells i and i 1 the numerical flux f i 1 2 is determined using the approximate riemann solver hllc toro 2001 2009 note that contact waves are accounted for in the solution of eq 30 as those described by the conservative variables h w h u 1 and 1 12 h 2 w here the muscl hancock scheme toro 2001 2009 which is the second order accurate in space and time is applied to reconstruct u this produces more efficient and robust computations than former solvers based on 4th order accurate reconstructions cantero chinchilla et al 2018 gamero et al 2020 besides to avoid unphysical numerical flux during the reconstruction of the flow depth over uneven topography the weighted surface depth gradient method wsdgm aureli et al 2008 is used where at the cell interfaces the water depth is determined as an average of the values obtained reconstructing the free surface and the water depth independently dry cells are identified as those where the flow depth h is below a prescribed tolerance which is adopted as h tol 10 6 in this work at a dry cell all variables are reset to zero in the elliptical step the solution is updated in two stages using finite difference schemes the first stage of the elliptical step is called inviscid finite difference step which is designed to incorporate the non hydrostatic pressure effects into the solution using an implicit scheme using the backward euler formula the system of equations to solve is in compact form 31 u i u ˆ i δ t s 0 u i the implicit system of equations described by eq 31 is coupled to the reactive eqs 15 and 18 and solved using a newton raphson nr method to obtain u the spatial derivates in eqs 15 18 and 31 are discretized using second order central finite differences if shocks are formed in any portion of the computational domain the gradients of some of the flow variables may reach large values these may result in numerical instabilities a new special method for handling shock development is presented in the next section here it is assumed that the solution is smooth throughout the computational domain setting appropriate initial values for the unknown variables q u 1 p 1 p 2 w and w a vector of residuals r u i m u ˆ i δ t s 0 u i m is defined where m is a recursion index residuals are reduced by computing an analytical jacobian j for the unknown variables at nodes i 1 i and i 1 yielding a 6n 6n diagonal matrix formed by 108 partial derivates per cell where n is the number of cells the matrix equation r j d u i is solved at each iteration of the solution where d u i is the vector of corrections should a dry cell be detected the corresponding residuals are set to zero all except eq 13 which is not updated once d u i is determined through the pertinent matrix inversion it is employed to update the solution vector i e u i m 1 u i m d u i where u i m 1 is the updated solution this process is repeated in the nr method at every time step until convergence the convergence criterion suggested by khan and steffler 1996a is implemented stopping the iterations if the mean relative error is below a prescribed tolerance settled as 10 6 in this work to save computational cost at each time step j is frozen i e computed at the start of the loop cantero chinchilla et al 2018 2020 the second stage of the elliptical step consists in an explicit update of the solution vector u by incorporating the turbulent source terms s τ resulting using the forward euler formula 32 u i k 1 u i δ t s τ u i fig 5 shows a flow chart of the numerical sequence described above which is followed in every time step the courant friedrichs lewy cfl condition is used to compute δt thus ensuring numerical stability of the hybrid fv fd scheme i e δt cfl δx u 0 c where c gh 1 2 is the long wave celerity and cfl 0 5 by numerical experimentation although the numerical scheme is stable for cfl 0 5 we generally used cfl 0 2 in the computations presented to reduce truncation errors in the output solutions 3 2 1 detection of shock development during wave propagation simulations the vam model has the ability to generate shocks or moving hydraulic jumps which are mathematically represented as a weak solution of the system of conservation laws involving discontinuities in one or some of the flow variables namely q z s u 1 w p 1 and p 2 the shocks or discontinuity like portions of the solutions are generated in the hyperbolic solver of the software when these portions of the solution are processed by the elliptic solver the computation of the gradients of the flow variables near such steep fronts produces quantities with an extremely large magnitude producing numerical instabilities when the jacobian matrix is formed and inverted if feasible the pathological computations described are especially dramatic if one attempts to make a mesh refinement study of the solution which is mandatory when presenting numerical solutions if δx is progressively reduced to get mesh independent results the hyperbolic solver produces sharper discontinuities given the increased resolution processing of these solutions by the elliptic solver encounters not only shaper shocks but also a smaller δx thus much higher gradients thereby guaranteeing solution crashing previous solvers cantero chinchilla et al 2018 gamero et al 2020 were found to suffer from this issue while dealing with the moving hydraulic jumps experimentally generated in this research thus a special solver for robust handling of shocks was developed as follows the following gradients of the flow variables are used as shock development sensors 33 z s x x q 2 g h 2 x u 1 2 g x w 2 g x p 1 γ x p 2 γ thereby permitting to detect formation of shocks in the output of the hyperbolic solver a threshold value for the gradients φ thr is defined which is set at 75 by numerical experimentation for meshes involving δx 0 01 m a shock is considered formed in the output of any of the flow variables if the corresponding gradient is above this threshold if this occurs in a cell the non hydrostatic flow variables are reset to zero in a bandwidth equal to the stencil used to compute dispersive terms e g 2δx this process eliminates the gradients near sharp discontinuities and permits a robust numerical handling at the initial stage of the dam break flow generation this process is not applied given that the initial condition is itself a discontinuity thus the shock development detection is applied for t t 0 where t 0 is the hydrodynamic time scale h u g 1 2 wu and wang 2007 which is of the order of the gate opening time in the experiments 3 2 2 boundary conditions in dam break wave experiments to mimic numerically the experiments conducted in the flume boundary conditions must be modeled with accuracy the boundary conditions are incorporated in the mathematical model using ghost cells at boundaries computational cells are from i 1 to i n at the tailwater section of the flume the closed gate is modeled as a reflective boundary condition i e h n 2 h n 1 h n and q n 2 q n 1 q n toro 2001 with the remaining variables in the vam model reset to zero this was found to reproduce well the experimental observations at the upstream end of the flume a transmissive boundary condition is implemented for all variables but except the flow depth i e q u 1 p 1 p 2 w w 1 q u 1 p 1 p 2 w w 2 and q u 1 p 1 p 2 w w 0 q u 1 p 1 p 2 w w 1 the flow depth in the cells i 1 and i 0 is computed by setting energy conservation in the water tank using the experimentally determined time variation of h uw 34 e 1 e 0 h u w 1 100 0 0015 t 4 0 032 t 3 0 1656 t 2 0 2503 t 0 0393 h 1 here e is the specific energy flow depths are thus obtained by analytical inversion of the specific energy diagram as follows 35 h 1 h 0 e 1 1 3 2 3 cos γ 1 3 e 0 1 3 2 3 cos γ 0 3 where 36 γ 1 γ 0 arcos 1 27 4 e 1 h c 1 3 arcos 1 27 4 e 0 h c 0 3 34 h c 1 h c 0 q 2 2 g 1 3 q 1 2 g 1 3 the flow depth computed using eq 35 corresponds to the subcritical root of the specific energy diagram castro orgaz and hager 2019 the inviscid part of the vam equations is not new in the solver presented but the model equations here include the turbulent stresses the numerical solver is new and includes the following important capabilities not available in a previous model by cantero chinchilla et al 2018 1 inclusion of turbulence by an eddy viscosity approach 2 inclusion of a new module for shock detection in the elliptic step allowing mesh refinement 3 use of the robust muscl hancock scheme in the hyperbolic step 4 dry bed treatment allowed the old vam 2018 model failed during trials to run it for the new experimental conditions presented due to the formation of hydraulic jumps and the dry bed zones resulting in a collapse in computations 4 results 4 1 benchmark numerical tests several challenging benchmark tests are selected in this section to evaluate the ability of the vam model to deal with discontinuous topography to grant the c property and to tackle dry wet unsteady fronts first a tidal wave test over a submerged rectangular step by bermudez and vazquez 1994 is used to validate the capability of the model on dealing with discontinuous topography zhou et al 2002 liang and marche 2009 the tidal wave is designed to occur in a 1500 m long frictionless channel with asymptotic analytical flow solution as follows 35 h 20 z b 4 sin π 4 t 86400 1 2 36 u 0 x l π 5400 h cos π 4 t 86400 1 2 where l 1500 m the bed profile is 37 z b 8 if x 750 187 5 0 otherwise the test is conducted herein using δx 7 5 m and cfl 0 8 until t 10800 s is reached equations 38 and 39 are used to set the initial conditions i e substituting t 0 s while eq 38 is used to impose the inlet boundary condition at x 0 m the right end of the channel is modeled using a reflective closed boundary condition vam model results for this test are shown in fig 6 depicting an excellent agreement with the analytical solution in both water surface fig 6a and unit discharge fig 6b predictions at t 10800 s where bed discontinuities did not impact the numerical solution second a still water test at a surface piercing hump liang and marche 2009 i e with dry wet interfaces is selected to evaluate the preservation of c property by the proposed vam model for this test the channel is 1 m long and frictionless with the surface piercing hump defined by 38 z b max 0 0 25 5 x 0 5 2 where the flow is at rest with maximum depth of 0 1 m the test is conducted herein using δx 0 01 m and cfl 0 4 until t 200 s fig 7 shows the vam model results for the free surface and the unit discharge after 200 s of computation where the agreement with the static solution at rest is excellent finally the analytic solutions by thacker 1981 for an oscillating shoreline in a parabolic bowl are used to evaluate the ability of the proposed vam model to tackle dry wet fronts in movement over a non uniform slope liang and marche 2009 lai and khan 2018 the analytical solutions for the flow depth and the unit discharge in a rectangular channel with parabolic bed profile are thacker 1981 lai and khan 2018 39 z s max z b b 2 cos 2 ω t b 2 4 b x ω cos ω t 4 g 40 q max 0 z s z b b sin ω t with 41 ω 2 π t 2 g h 0 l 0 where t is the period ω is the frequency b is a speed parameter and h 0 and l 0 are definition parameter for the parabolic bed which is z b h 0 x l 0 2 1 this test is conducted herein using δx 0 01 m cfl 0 4 h 0 10 m l 0 600 m and b 5 m s until t t 269 s the domain extended to x є 1000 1000 m thus yielding the boundaries unaffected by the flow during the computation fig 8 shows the vam model results for flow depth and unit discharge at three different instants i at t t 2 when the flow reaches its highest position to the right fig 8a and b ii at t 3t 4 when the flow passes through the horizontal position advancing towards the left end fig 8c and d and iii at t t when the flow reaches its highest position to the left fig 8e and f the vam model results show an accurate agreement with the analytical solutions at all computed instants where the moving dry bed fronts are accurately tackled validating the wetting drying algorithm in the model 4 2 lee side waves the ability of the vam model on reproducing the challenging experimental flow tests produced in this work is evaluated is this section to this end all tests described in section 2 2 table 1 were simulated using δx 0 01 m cfl 0 2 and n 0 01 ms 1 3 the numerical tests were repeated using cfl 0 1 thereby confirming mesh independence of the numerical results the vam model results for the tests 1 4 are shown in fig 10 13 respectively where four instants were selected to illustrate the main phenomena in the transformation of waves in the test series i initial stages of dam break flow generation at t 1 s or 2 s ii wave reflection at the right end of the flume at t 5 s iii wave interaction with the unsteady hydraulic jump formed at the toe of the obstacle at t 8 s and iv reflected and diffracted waves produced after the incoming surge surpassed the obstacle at t 12 s fig 9 shows the vam model results in comparison with the experimental data for test 1 where r 0 397 as well as the saint venant equations dsv model predictions in test 1 the non linearity effects are the highest among the test series also depicting the most challenging unsteady hydraulic jump formed at the toe of the lee slope of the obstacle the vam model accurately predicts the initial stages of the dam break flow as shown in fig 9a which represents the experimental data at t 1 s the amplitude of the leading broken wave fig 9a x 10 7 m is however slightly overestimated while the vam model is able to accurately approximate the advancing bore position the dsv model anticipates its location fig 10a x 10 9 m fig 9b represents the experimental data at t 5 s after the flow reflection at the right end of the flume the vam model results show excellent agreement with the experimental data of the reflected train of waves the hydraulic jump profile and the drawdown in the upstream water level as the rarefaction wave advances backwards the dsv model however fails to approximate the reflected train of waves which is attributed to the lack of dispersive terms the experimental data at t 8 s is represented in fig 9c depicting the interaction between the reflected train of waves and the moving forward hydraulic jump at the toe of the obstacle while the effect of the rarefaction wave as well as the reflected train of waves near the flume end are accurately predicted by the vam model discrepancies between the model results and the data are found in both the interaction zone and the hydraulic jump with a peak predicted by the vam model not observed in the experiments the vam model computed the dynamic pressures in an elliptic step by solving the relevant system of equations iteratively in the vicinity of a hydraulic jump the perturbation parameters determining the pressure field p 1 and p 2 are subjected to abrupt changes and thus inviscid pressure peaks are generated as solution in the elliptic step thereby resulting also in abrupt wave peaks in the free surface profile at the next time step turbulence is incorporated in the model using an eddy viscosity approach the mismatch of computations and predictions at t 8 s indicates that the modeled turbulent stresses are not strong enough to suppress the effect of the inviscid pressure peaks at some time instants the approximated turbulence closure in the proposed vam model which incorporates the bed dominating turbulence approximation for eddy viscosity by ghamry and steffler 2002a b may be the origin of this misprediction a more sophisticated approach including turbulence diffusion and production may be needed to characterize the interaction zone of the hydraulic jump and the reflected wave finally fig 9d shows the experimental data at t 12 s in comparison with vam and dsv model results overall the vam model is capable of predicting most of the reflected and diffracted waves leading to some mispredictions especially for the leading wave amplitude in the reflected bore these discrepancies may stem from the misprediction of the flow interaction around t 8 s fig 9c but are unrelated to stability issues of the model despite this weakness the turbulent vam model prediction of the experimental data by test 1 is accurate as compared to that of the dsv model l and l 2 norms for vam and dsv simulations are given in table 2 fig 10 shows the vam model results in comparison with the experimental data extracted for test 2 where r 0 6 as well as the dsv model predictions in test 2 the non linear effects in the advancing bore are expected to decrease as compared to those given by test 1 thus leading to a non breaking bore at the initial stages of the dam break flow as observed during the experimentation in addition during the test the unsteady hydraulic jump was observed to develop a shorter front than that by test 1 fig 10a shows the experimental data at t 2 s i e the dam break flow right after the initial stages in comparison with the vam and dsv model results here the vam model provides the results in excellent agreement with the experimental data but for the leading wave amplitude fig 10a x 12 m it is slightly underestimated in fig 10b the experimental data at t 5 s is shown where the dam break flow has already been reflected at the right end of the flume and the hydraulic jump at the toe of the obstacle begins to develop in comparison with the results by both models while the dsv model less predicts the reflected train of waves the vam model is able to accurately approximate the experimental data at t 5 s however it leads to an overestimation of the zone of development of the hydraulic jump at the toe of the obstacle note that the latter referred zone is mainly turbulence dominated and thus the accuracy of vam model results may suffer from the approximate turbulence closure considered in this work the overestimation of the initial stages of the hydraulic jump is also evident in fig 10c where the experimental data at t 8 s is plotted against the vam and dsv model results albeit the two leading waves at the front of the reflected bore are accurately approximated by the vam model the amplitude and phase of the train of waves are slightly misinterpreted however the impact of this underestimation in the train of waves at t 8 s is minimal on the approximation of the subsequent test data as shown in fig 10d for the experimental data at t 12 s in comparison with both models results the results by the vam model are overall satisfactory for t 12 s where the major discrepancies with respect to the experimental data focus on the amplitude of the train of waves upstream and downstream the obstacle l and l 2 norms for vam and dsv simulations are given in table 3 fig 11 shows the vam model results in comparison with the experimental data extracted for test 3 where r 0 8 as well as the dsv model predictions in test 3 the wave non linearity is minimum as the dam break depth ratio approaches unity during the experiments the unsteady hydraulic jump at the toe of the obstacle was observed to be very close to the crest the experimental data at t 2 s is shown in fig 11a where the vam model provides a good approximation of the data in contrast to the prediction by the dsv model which not only anticipates the bore position and mispredicts the train of waves but also depicts a fictitious incipient hydraulic jump at the lee slope of the obstacle it is noteworthy that during the experimentation in test 3 a notable surface water splash was observed after the sluice gate opening thus contaminating the experimental data in the subsequent time instants in consequence the experimental free surface was hardly tracked at t 2 s and therefore those time instants are not shown in fig 11 in fig 11b the experimental data at t 5 s is shown here the vam model is demonstrated to be able to predict all the data however hydraulic jump and the amplitude of the leading reflected wave are slightly overestimated and underestimated respectively the experimental data at t 8 s is presented in fig 11c where the vam model accurately approximates the experimental data of the reflected train of waves before they encounter back the obstacle fig 11d presents the experimental data at t 12 s after the transformation of the reflected dam break waves and the interaction back with the obstacle in line with the results for the experimental data at t 12 s in the experiments 1 and 2 the vam model provides a fair approximation of the experimental data where only major mispredictions are found in the phase of the train of waves upstream the obstacle l and l 2 norms for vam and dsv simulations are given in table 4 fig 12 shows the vam and dsv model results in comparison with the experimental data extracted for test 4 where r 0 i e dam break flow under dry bed conditions downstream castro orgaz and chanson 2017 in test 4 the parabolic like profile of the dry bed dam break bore advanced towards the right end of the flume encountering minor bed irregularities at the structural joints which yielded little free surface perturbations right after the reflection of the dam break bore two unsteady hydraulic jumps are formed i a moving forward one at the toe of the lee slope of the obstacle and ii a moving backward one at the front of the reflected bore fig 13a shows the experimental data at t 1 s for the dry bed dam break waves at the initial stages where the vam model results merely differ from the dsv predictions in the profile of the rarefaction wave the experimental data at t 5 s depicts the first instants after the bore reflection where an unsteady hydraulic jump with smooth transition is developed fig 13b here the vam model proposed in this study is shown to be unable to tackle the smooth hydraulic jump front however showing a direct transition which is in line with the predictions of the dsv model due to the shock detection in this zone the latter suggests that the turbulence closure may need to be enhanced some discrepancies have also been found in the prediction of direct hydraulic jumps as shown in fig 12c x 11 3 m for the experimental data at t 8 s the last experimental data extracted for test 4 corresponds to t 12 s fig 12d where the flow interaction between the two unsteady hydraulic jumps is shown leading to a challenging turbulence dominated phenomena however the vam model provides a fair approximation of the test of data showing non hydrostatic free surface waves after the hydraulic jump l and l 2 norms for vam and dsv simulations are given in table 5 4 3 steady flow over the obstacle a steady flow experiment conducted in this work for the maximum discharge q 0 1826 m2 s was used fig 13a and b to validate the vam model results over an obstacle the obstacle is a gaussian profile z bg 0 209 x exp 1 2 x x x crest 0 254 2 where z bg is the local obstacle height above the flume bed and x crest is the longitudinal location of the crest installed at x crest 6 565 m the experimental obstacle is similar to that of sivakumaran et al 1983 but q is higher in the present experiments and thus the degree of non hydrostaticity of the flow is stronger in fig 13 we have included the experimental measurements of the free surface profile z s x and piezometric bed pressure head p b γ z b x in this obstacle model comparison of the simulated results for the free surface and bed piezometric pressure head p b γ z b obtained from the vam model in fig 13 shows the accuracy of this shallow water formulation predicting the flow features over the obstacle the mesh size independence of the results was evaluated progressively reducing δx and cfl 5 conclusions a new experimental procedure to investigate wave interaction and flow d dd adjustment over obstacles is presented by constructing a large scale obstacle model in a flume equipped with a wave generation mechanism based on a dam break like set up the experiments were used to produce a variety of relevant phenomena over topography as broken and dispersive undular waves hydraulic jumps non hydrostatic critical flow over a sill crest and wave reflection in addition to the novelty of the procedure to study flow interaction with obstacles the experimental database generated is itself of utility for environmental fluid flow modelers given that it can be directly used as benchmark test cases while testing their models steady flow tests were used additionally to determine the dynamic fluid pressures over the obstacle a new shallow water weighted averaged residual model with the ability to mimic turbulent breaking processes trough the formation of shocks or moving hydraulic jumps is presented this is due to the inclusion of the turbulent velocity profile and reynolds stresses into the model equations with a new shock detection algorithm conferring robustness to the numerical solver dispersive effects and non hydrostatic bed pressures are further tackled by the model given the inclusion of the vertical accelerations these features make the weighted averaged residual model presented a suitable tool for environmental modeling of flows over topography with sills the turbulent flow model developed reproduces the main features observed during experimentation namely undular and broken surges dispersive wave reflection hydraulic jumps and non hydrostatic critical flow at sill crest with high non hydrostatic pressures with enough accuracy for practical modeling purposes the dispersionless dsv equations which is the frequent shallow water flow representation used to study flow adjustment over obstacles produces only rough estimates or simply does not reproduce the observed experimental phenomena the main outcome of this research is a contribution to the physical understanding of the flow adjustment over an obstacle with the new experiments conducted and by producing a new and robust shallow water solver with capabilities to deal with several hydraulic phenomena not accounted for in other solvers data availability the experimental database generated is available as supplementary material in the file experiments ems2022 xls declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the work of o castro orgaz project leader was supported by the spanish projects ctm2017 85171 c2 1 r and pid2020 114688rb i00 and grant maría de maeztu for centers and units of excellence in r d ref cex2019 000968 m fncc was partly funded by mcin aei 10 13039 501100011033 and the nextgeneration eu prtr through juan de la cierva program ijc2020 042646 i and a selection of doctoral researchers grant by the junta de andalucía government spain ref doc 00996 rjb was partly funded by mcin aei 10 13039 501100011033 through juan de la cierva program ijc2019 038848 i we thank the two anonymous reviewers of this work for the comments offered to improve it appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105422 
25558,the flow over uneven topography is a problem of interest in environmental fluid flow modeling including flows over river bedforms exchange flows over oceanic sills or the airflow over mountains the common experimental procedure to investigate these flows moving a small obstacle in a laboratory flume yields experimental difficulties whereas modeling using non linear shallow flow equations does not explain all the flow phenomena novel alternative procedures are presented for the experimentation and shallow water representation of flow interaction with obstacles a large scale obstacle model is constructed in a dam break set up and used to generate flow phenomena over topography including dispersive and broken surges wave reflection hydraulic jumps and non hydrostatic sill overflows simulations are conducted with a shallow water weighted averaged residual flow software for turbulent flows the proposed software reproduces the experiments satisfactorily supporting its use in modeling whereas the new experimental database can be used by modelers to test their software keywords non hydrostatic flows laboratory experiments weighted averaged residual equations lee side waves obstacles software availability name of software waves transformation model software contact emails ag2gaojp uco es z12cachf uco es ag2caoro uco es requirements matlab availability the software platform is freely available at https github com frncch waves transformation model software 1 introduction the study of two dimensional shallow water flows over obstacles is relevant in several branches of environmental mechanics as in the river flow over bedforms like dunes and antidunes in oceanographic exchange flows over a seamount or in the mesoscale atmospheric flow past a steep mountain nadiga et al 1996 zhu and lawrence 1998 a common mathematical procedure to study these flows consists in setting instantaneously an obstacle into an initially steady and uniform stream and then study the time dependent flow adjustment that takes place and the ensuing asymptotic steady state long 1954 1970 pratt 1983 nadiga et al 1996 typically using either the shallow water dispersionless saint venant equations houghton and kasahara 1968 pratt 1983 cea et al 2011 or the dispersive serre green naghdi sgn equations nadiga et al 1996 the experimental procedure to generate these flows consists in rapidly accelerate up to a target constant velocity an obstacle initially at rest in a flume filled with water long 1954 1970 in long s 1970 experiments the obstacle was moved in the flume by a thin line wrapped around a cylindrical winder driven by a motor the velocity of obstacle displacement was experimentally determined by counting the revolutions and then this velocity was used to deduce the flow patterns observed moving with the obstacle e g the equivalent flow with a static obstacle long 1970 this experimental procedure is however not simple and is prone to large number of problems first the experimental design is only economical thus feasible using small obstacles e g in long s 1970 experiments rather small ones only of 9 1 and 2 cm high were installed second the wave generation around the obstacle is induced by moving it thereby producing leakage at the joins with the flume sidewalls and at the obstacle bottom these problems are hardly solvable especially for high froude numbers involving supercritical flows long 1970 in addition to the experimental configuration problems the quality and type of experimental data that can be extracted from this set up are limited for example instantaneous free surface profile measurements of these flows are not available in the literature which would be especially desirable to compare with the theoretical predictions of shallow water models the typical data extracted from long s experiments consists only in the upstream and crest flow depths as well as the froude number long 1970 further the flow over a curved obstacle involves vertical accelerations and thus non hydrostatic bed pressures nadiga et al 1996 zhu and lawrence 1998 castro orgaz and hager 2013 gamero et al 2020 which should be experimentally determined however in long s 1970 set up the obstacle was moving thus it would have been a very challenging task to take bed pressure measurements for the asymptotic steady flow over the obstacle by installing pressure taps at the obstacle surface note that the instantaneous appearance of an obstacle in a stream is not a realistic mechanism of forcing in geophysical fluid flows despite its wide use by environmental flow modelers pratt 1983 thus an alternative method to study wave interaction with obstacles in environmental flows was considered as commented below during the time dependent flow adjustment towards a steady flow over an obstacle shocks are formed moving upstream and downstream of it long 1954 1970 houghton and kasahara 1968 pratt 1983 nadiga et al 1996 with a transition from subcritical to supercritical flow conditions occurring in the vicinity of the obstacle crest naghdi and vongsarnpigoon 1986 zhu and lawrence 1998 at the lee side of the obstacle the flow typically changes from supercritical to subcritical flow conditions through a moving shock displacing away from the obstacle the shocks at the lee side of the obstacle may be either undular or broken depending on the froude number these instantaneous waves are hardly characterized experimentally in the literature and simulations using sgn models nadiga et al 1996 showed inability of this shallow water representation to mimic the turbulent flow processes occurring given that wave breaking bayon et al 2016 gualtieri and chanson 2021 is not modeled if the froude number at the lee side face of the obstacle is high turbulent breaking occurs and the undular waves predicted by sgn models become unrealistic nadiga et al 1996 castro orgaz and chanson 2017 turbulent breaking at the lee side of obstacles is important as for example in a oceanographic flow of salt water moving over a sill in a fresh water environment farmer and denton 1985 denton 1987 in these flows turbulent breaking of the lee side waves induces mixing between layers and thus provides nutrients and dissolved oxygen for the deep water it further affects the dispersion of any pollutant therefore a shallow water turbulent flow model with ability to simulate both undular and broken waves is needed to simulate flow over obstacles the main aim of this research is to characterize the complex features of open channel flows downstream of a bottom hump which cannot be simulated by the standard shallow water equations given that the lee side obstacle waves are neither characterized with detailed experiments nor simulated with shallow water turbulent non hydrostatic models the experimental and numerical modeling of these flows are the two major objectives of this work as described in section 2 of this work lee side obstacle waves are generated experimentally in a large scale obstacle model with a new and entirely different procedure from that of long s a dam break like set up was installed in a long experimental flume consisting in a gate installed downstream of the obstacle and equipped with an instantaneous opening mechanism thereby permitting to generate lee side waves once the gate was opened instantaneously the type of waves generated at the lee face of the obstacle depends on the upstream water depth at the gate which was an experimental parameter to generate the flows this new experimental procedure is different from that of long but it permits to generate waves at the lee side of obstacles and study their interaction with it which was the major objective in long s set up this new experimental set up has the major advantage of permitting to execute accurate experiments at a large scale obstacle model and collecting high quality data including the instantaneous free surface profiles and the steady bed pressures over the obstacle note that the instantaneous appearance of an obstacle in a stream as in long s approach is not a realistic flow however this experimental procedure in addition to serve to study the flow adjustment over an obstacle in response to the generated waves conceptually represents an abrupt drop in the water levels around the obstacle in response to a downstream forcing the value of the new experimental dataset generated in this work relies on two major aspects 1 in long s 1970 experiments the free surface profiles are presented only qualitatively using photos but measurements are not available given the complications to take such readings with a moving obstacle and 2 to the authors knowledge there is no previous work in the literature where waves over a curved obstacle are generated with a dam break like setting shallow water modeling is considered in section 3 a shallow water non hydrostatic flow model was constructed from the reynolds averaged navier stokes equations by using a weighted averaged residual method of galerkin type the model accounts for a non hydrostatic fluid pressure which has the ability to model undular waves further the turbulent velocity profile and turbulent stresses are modeled in the weighted averaged equations permitting to simulate broken waves during the flow adjustment over the obstacle a high resolution finite volume finite difference numerical solver is developed and extensively verified in section 4 using both benchmark numerical tests and the new experimental measurements conducted in this work conclusions are presented in section 5 the model software is freely available on github https github com frncch waves transformation model software whereas the experimental database is provided as supplementary material in the file experiments ems2022 xls 2 experimental characterization 2 1 experimental flume and equips the experiments were conducted in a 15 m long 1 m high 1 m width tilting experimental flume in the hydraulics laboratory at the university of córdoba spain a reduction of the flume width to 0 405 m was accomplished by a moving division wall fig 1 and the flume slope for the experimental series conducted was 0 0015 m m the tailwater portion of the flume from 9 634 m to 15 m downstream the inlet section was structurally a cantilever and the beam deformation though small was considered in the simulations to accurately define the actual bed profile of the flume the flume was equipped with a recirculation pump of 0 078 m3 s maximum discharge connected to a downstream water tank allowing to work in closed circuit a water tank with flow straightener was located at the flume inlet to reduce flow disturbances the tailgate of the flume was fully open or closed depending on the type of experiment e g with or without reflection at the flume end a large scale obstacle of gaussian profile z bg 0 209 x exp 1 2 x x x crest 0 254 2 where z bg is the local obstacle height above the flume bed and x crest the longitudinal location of the crest was installed at x crest 6 565 m a gaussian obstacle shape was selected because it permits to mathematically adjust their parameters in the design phase prior to construction in this way it was possible to adjust the crest curvature until producing the desired degree of non hydrostaticity in the flow over the obstacle further this shape is easy to construct in metal by a manufacturer along the longitudinal symmetry axis of the obstacle 17 piezometric tapings were installed to take bottom pressure head readings in a piezometric panel the flume was equipped with a dam break like set up consisting in a sluice gate of high speed release induced by a pneumatic drive system the gate opening time was less than 0 15 s in all the experiments conducted in this work thus the opening operation can be considered instantaneous a high speed camera fastec ts5 with 50 mm focal length lens to avoid image distortion capturing at up to 253 fps at maximum resolution was used to characterize the movement of the gate and ensure that the operation can be considered fast enough to reproduce dam break like waves flow visualization during the experiments was accomplished through the eight lateral crystal windows 1 875 m wide by 0 975 m high of the flume each window was monitored by a camera perpendicularly installed in front of the flume fig 1 the monitoring video system comprises eight basler ace aca1920 40uc cameras with 6 mm focal length lens to allow capturing the whole width of each lateral crystal window recording 40 frames per second fps maximum at full resolution and a laptop intel core i7 9750h with software for image capture synchronization assembling and processing the system automatically assembles the images collected by the 8 cameras in a synchronized way correcting distortion errors and thereby providing instantaneous experimental images of the 15 m of flume the novelty of this experimental research is in the experimental procedure and set up used to generate waves evolving over obstacles in long s 1970 classical experiments an obstacle is moved in a flume filled with initially still water pratt and whitehead 2007 whereas in this work we have generated the unsteady waves over a fixed obstacle using a dam break like set up 2 2 experimental series two kinds of experimental series were produced the first series consisted of dam break wave experiments generated by using the high speed sluice gate with no inlet flow q 0 and the tailgate fully closed to allow wave reflection the second series consisted of steady flow experiments with various q up to the maximum discharge with the high speed gate deactivated no operation positioned above the flume and the tailgate fully opened the dam break experiments were designed using different downstream to upstream water depth ratios r h d h u with zero inlet discharge and closed tailgate which were organized in tests series comprising r 0 0 397 0 6 and 0 8 table 1 first the flume was filled with water up to the level h d considered and then the high speed sluice gate closed thereafter the upstream side of the gate further filled up to the desired r afterwards the high speed sluice gate was released fig 2 a the monitoring system was set to record images at 25 fps which was enough to take a detailed experimental characterization of the unsteady water waves each test was recorded during 12 s enabling to capture all the relevant hydraulic processes namely the positive and negative dam break wave generation wave reflection at the closed tailwater gate formation of a hydraulic jump at the lee side of the obstacle and interaction of the reflected wave with the flow developing over the obstacle once the images collected by the system of cameras were assembled and distortion errors were corrected for selected instants of time they were used to extract the instantaneous flow profiles fig 2b and c the free surface was digitalized from the images over the free surface curve in the crystal wall using golden software grapher 13 3 754 by trial and error calibration tests it was found that the procedure permits to measure the experimental flow profile with 0 1 cm accuracy for illustration purposes panoramic instants at t 1 6 and 10 s for test 1 are shown in fig 3 using the corrected images of the cameras 4 5 and 6 from left to right in each subfigure an accurate modeling of the upstream boundary condition in the numerical solver requires use as input the time variation of the static upstream water level at the inlet tank solid wall h uw this time dependent variable was measured during the experiments with the high speed camera fastec ts5 and from the ensuing measurements the following 4th order polynomial was found to describe inlet flow conditions h uw m 0 0015 t 4 0 032 t 3 0 1656 t 2 0 2503 t 0 0393 100 h 1 100 r 2 0 98 where h uw is the flow depth at the upstream wall of the water tank t is the time and h 1 is the flow depth level at the upstream section of the flume see fig 1 steady flow experiments were conducted with fully open tail and high speed sluice gates with various discharges up to the maximum of the system 0 1826 m2 s the bed pressure head was measured by visual observation of a piezometric panel allowing readings of accuracy 0 1 cm 3 shallow water turbulent flow modeling 3 1 weighted averaged residual equations consider steady two dimensional flow in a vertical plane fig 4 as in the previously described experiments fig 3 the modeling approximation pursued here entails the development of weighted averaged residual equations from the rans equations of turbulent free surface flow following steffler and jin 1993 in a first step a sequence of vertically averaged and moment vam equations is produced by using the first shifted legendre polynomial as test function resulting steffler and jin 1993 khan and steffer 1996a b 1 h t h u x 0 2 h u t h u 2 x 1 ρ h p σ x x p b σ x b z b x τ x z b 3 h w t h u w x 1 ρ p b h τ x z x τ x z b z b x σ z b g h 4 h 2 h t h z u x z h u x h w 0 5 h z u t h z u 2 x z h u t z h u 2 x h u w 1 ρ h z p x z h p x h p b 2 z b x 1 ρ h z σ x x z h σ x x h σ x b 2 z b x h τ x z b 2 h τ x z 6 h z w t h z u w x z h w t z h u w x h w 2 h ρ p b 2 p 1 ρ h z τ x z x z h τ x z x h τ x z b 2 z b x h σ z b 2 h σ z where x and z are the horizontal and vertical cartesian coordinates respectively u x z t and w x z t are the horizontal and vertical velocity components z b x is the bed profile h x t is the flow depth p x z t is the fluid pressure p b x t is bottom pressure τ x z t and σ x z t are the reynolds tangential and normal stresses respectively ρ is the fluid density g is the gravitational acceleration and t is the time the overbar operator denotes vertically averaged quantities equations 1 3 are the continuity x and z momentum equations respectively while eqs 4 6 are the moment of continuity x and z momentum equations respectively note that z is the elevation of the centroid of a section z b h 2 equations 1 6 are weighted averaged open channel flow equations yet not residual given that u w p are still general in the vam model equations 1 6 predictors for the velocity components u w and the fluid pressure p are required steffler and jin 1993 used finite element type expansions consisting of a base of functions with a series of coefficients independent of the vertical coordinate in particular they expanded u using the first shifted legendre polynomial and w and p using the first and second shifted legendre polynomials resulting steffler and jin 1993 khan and steffer 1996a b 7 u x z t u 0 x t u 1 x t 2 η x z t 1 8 w x z t w b x t w 2 x t 4 η x z t 1 η x z t w s x t η x z t 9 p x z t ρ g h x t p 1 x t p 2 x t 4 η x z t 1 η x z t here u 0 is the depth averaged horizontal velocity u 1 is the x velocity at the free surface in excess of u 0 w 2 is the mid depth z velocity in excess of the average of the vertical velocities at the bed and free surface levels w b and w s are the vertical velocity at the bed and free surface levels respectively p 1 is the bed pressure in excess of hydrostatic p 2 is the mid depth deviation from the linear non hydrostatic law and η is the dimensionless vertical coordinate z z b h the kinematic boundary conditions are expressed as follows cantero chinchilla et al 2018 2020 10 w b u 0 u 1 z b x 11 w s h t u 0 u 1 z s x where z s is the free surface elevation h z b inserting eqs 7 8 which are the trial functions approximating u w p into equation 1 6 the following system of approximate residual partial differential equations pdes results 12 h t q x 0 13 q t x g h 2 2 q 2 h x u 1 2 h 3 h p 1 2 ρ 2 h p 2 3 ρ h σ x ρ g h z b x p 1 ρ z b x τ b ρ 14 h w t q w x 1 6 h u 1 w x p 1 ρ τ b ρ z b x 1 ρ h τ x z x 15 h 2 h t q z x 1 6 h 2 u 1 x h w 16 h u 1 t q u 1 x h 2 ρ p 1 x q u 1 h p 1 2 ρ h x u 1 q x 4 p 2 ρ z x 6 ρ τ b 2 τ x z 6 ρ h h z σ x x z h σ x x 17 t h 2 w 12 x h q w 12 h w 2 h t q w h u 1 w 6 z x x h 2 u 1 10 w w b 3 w s 3 h w 2 2 h p 2 3 ρ h τ b 2 ρ z b x h σ z ρ 1 ρ h z τ x z x z h τ x z x where q is the discharge per unit width hu 0 w is the vertical velocity difference between the bed and free surface levels w b w s τ b is the bed shear stress σ x and τ x z are the depth averaged normal and shear stresses respectively and w 2 w 2 1 12 w b 2 1 12 w s 2 1 6 w b w s 1 20 2 w w b w s 2 the mean vertical velocity is w w b 2 2w 2 3 w s 2 given that the base functions used in the trial solution are used as test functions the vam model is a galerkin type system of weighted averaged residual equations finlayson and scriven 1966 the moment of x momentum eq 16 differs from that previously used cantero chinchilla et al 2018 gamero et al 2020 in that the conservative variable is hu 1 instead of u 1 this modification was found to increase the numerical robustness of the vam model when handling dry wet fronts further eqs 16 and 17 include all turbulent stress terms originating from the weighted averaging process former models in cantero chinchilla et al 2018 and gamero et al 2020 only considered bed shear effects thus here the complete turbulent modeling terms were accounted for in the model equations the following reactive equation is written based on the kinematic boundary conditions 18 w q h u 1 z b x q x q h u 1 z b h x it is a mathematical statement to be verified by the solution at any instant of time turbulence closure is required to estimate τ b σ x σ z τ x z z σ x and z τ x z in eqs 13 14 16 and 17 using an eddy viscosity approach these terms read after averaging 19 σ x 2 ρ ν x h q x u s z s x u b z b x 20 σ z 2 ρ ν z h w s w b 21 τ x z ρ ν z h 2 u 1 h w x w s z s x w b z b x 22 z σ x ρ ν x h 3 3 x q h u 1 x ρ ν x u 1 z b h 2 h x 4 z b x ρ ν x 3 4 u 1 h x 6 z b x q h 6 u 1 z b x 23 z τ x z ρ ν z u 1 2 ρ ν z u 1 z b h ρ ν z h 2 x w w b 6 w s 6 ρ ν z w w b 6 5 w s 6 h x ρ ν z z b w x ρ ν z w w s z b x ρ ν z z b h w w s h x ρ ν z w z b h z b x where ν x and ν z are the vertically averaged eddy viscosities in the x and z directions respectively in the traditional shallow water quasi 3d approach e g rodi 1993 the pressure is assumed to be vertically hydrostatic with corrections using turbulent stresses based on the depth averaged horizontal velocity u 0 in the present formulation pressures are dynamic ones with perturbation parameters p 1 and p 2 which are corrected in the model equations by the depth averaged turbulent stresses considering the non uniform variation of the u velocity with elevation given by u s u b and u 1 as well as the vertical velocity w variation determined by w s w b and w 2 following ghamry and steffler 2002a b the depth averaged eddy viscosities are estimated as fischer et al 1979 24 ν x 0 5 u h 25 ν z 0 07 u h where u is the shear velocity τ b ρ 1 2 the bed shear stress is modeled using manning s formula including vertical velocity effects as follows castro orgaz and hager 2017 cantero chinchilla et al 2018 2020 26 τ b ρ g n 2 u b 2 w b 2 h 1 3 where n is the manning s roughness coefficient 3 2 numerical modeling and software development the software development entails the solution of the turbulent vam model eqs 12 17 through numerical techniques given that an analytical solution of the system of pdes equations is unknown a semi implicit finite volume fv finite difference fd scheme is developed based on former works cantero chinchilla et al 2018 2020 gamero et al 2020 former solvers were prone to numerical instabilities if shocks or moving hydraulic jumps were progressively developed in the solution causing the solution failure in some cases therefore a special feature of the new solver constructed is its ability for handling the formation of shocks with robustness as described below the vam model is in matrix form 27 u t f x s o s τ where vectors u f and s enclose respectively the flow conservative variables fluxes and source terms 28 u h q h w h u 1 h 2 w 12 f q q 2 h g h 2 2 q w q u 1 h q w 12 29 s o 0 x u 1 2 h 3 h p 1 2 ρ 2 h p 2 3 ρ g h z b x p 1 ρ z b x 1 6 h u 1 w x p 1 ρ h 2 ρ p 1 x q u 1 h p 1 2 ρ h x u 1 q x 4 p 2 ρ z x h w 2 q x q w h u 1 w 6 z x x h 2 u 1 10 w w b 3 w s 3 h w 2 2 h p 2 3 ρ s τ 0 1 ρ h σ x x τ b ρ τ b ρ z b x 1 ρ h τ x z x 6 ρ τ b 2 τ x z 6 ρ h h z σ x x z h σ x x h τ b 2 ρ z b x h σ z ρ 1 ρ h z τ x z x z h τ x z x the subscripts o and τ refer to the inviscid and the turbulent stress source terms respectively note that only transport eqs 12 14 and 16 and 17 are contained into eq 27 which together with the reactive eqs 15 and eq 18 conforms the vam model this shallow water vam model is more complex than the saint venant equations but its features are significantly better it may be noted that the rans equations are simpler in its formulation however the computational cost of a full 3d approach is still high at a river scale katopodes 2019 involving the determination of the free surface boundary using the volume of fluid or level set methods in contrast the position of the free surface is directly resolved in this depth averaged formulation note that the vam equations look complex given their long source terms see eqs 29 but the architecture of the equations is similar to that of the standard shallow water equations see eq 27 the solution of this model is vectorized in the present code such that implementation is easy the semi implicit fv fd scheme follows a splitting approach in two stages i a hyperbolic step and ii an elliptical step dividing the x t plane into quadrilateral finite volume cells of dimensions δx δt in the hyperbolic step an intermediate solution is obtained from the homogenous part of eq 27 using a godunov type finite volume scheme toro 2001 2009 30 u ˆ i u i k δ t δ x f i 1 2 f i 1 2 in eq 30 u and f are space and time averaged vectors i is the cell index k is a time index δx is the x dimension of the control volume δt is the t dimension of the control volume the indices i 1 2 refer to the control volume interfaces between cells i and i 1 the numerical flux f i 1 2 is determined using the approximate riemann solver hllc toro 2001 2009 note that contact waves are accounted for in the solution of eq 30 as those described by the conservative variables h w h u 1 and 1 12 h 2 w here the muscl hancock scheme toro 2001 2009 which is the second order accurate in space and time is applied to reconstruct u this produces more efficient and robust computations than former solvers based on 4th order accurate reconstructions cantero chinchilla et al 2018 gamero et al 2020 besides to avoid unphysical numerical flux during the reconstruction of the flow depth over uneven topography the weighted surface depth gradient method wsdgm aureli et al 2008 is used where at the cell interfaces the water depth is determined as an average of the values obtained reconstructing the free surface and the water depth independently dry cells are identified as those where the flow depth h is below a prescribed tolerance which is adopted as h tol 10 6 in this work at a dry cell all variables are reset to zero in the elliptical step the solution is updated in two stages using finite difference schemes the first stage of the elliptical step is called inviscid finite difference step which is designed to incorporate the non hydrostatic pressure effects into the solution using an implicit scheme using the backward euler formula the system of equations to solve is in compact form 31 u i u ˆ i δ t s 0 u i the implicit system of equations described by eq 31 is coupled to the reactive eqs 15 and 18 and solved using a newton raphson nr method to obtain u the spatial derivates in eqs 15 18 and 31 are discretized using second order central finite differences if shocks are formed in any portion of the computational domain the gradients of some of the flow variables may reach large values these may result in numerical instabilities a new special method for handling shock development is presented in the next section here it is assumed that the solution is smooth throughout the computational domain setting appropriate initial values for the unknown variables q u 1 p 1 p 2 w and w a vector of residuals r u i m u ˆ i δ t s 0 u i m is defined where m is a recursion index residuals are reduced by computing an analytical jacobian j for the unknown variables at nodes i 1 i and i 1 yielding a 6n 6n diagonal matrix formed by 108 partial derivates per cell where n is the number of cells the matrix equation r j d u i is solved at each iteration of the solution where d u i is the vector of corrections should a dry cell be detected the corresponding residuals are set to zero all except eq 13 which is not updated once d u i is determined through the pertinent matrix inversion it is employed to update the solution vector i e u i m 1 u i m d u i where u i m 1 is the updated solution this process is repeated in the nr method at every time step until convergence the convergence criterion suggested by khan and steffler 1996a is implemented stopping the iterations if the mean relative error is below a prescribed tolerance settled as 10 6 in this work to save computational cost at each time step j is frozen i e computed at the start of the loop cantero chinchilla et al 2018 2020 the second stage of the elliptical step consists in an explicit update of the solution vector u by incorporating the turbulent source terms s τ resulting using the forward euler formula 32 u i k 1 u i δ t s τ u i fig 5 shows a flow chart of the numerical sequence described above which is followed in every time step the courant friedrichs lewy cfl condition is used to compute δt thus ensuring numerical stability of the hybrid fv fd scheme i e δt cfl δx u 0 c where c gh 1 2 is the long wave celerity and cfl 0 5 by numerical experimentation although the numerical scheme is stable for cfl 0 5 we generally used cfl 0 2 in the computations presented to reduce truncation errors in the output solutions 3 2 1 detection of shock development during wave propagation simulations the vam model has the ability to generate shocks or moving hydraulic jumps which are mathematically represented as a weak solution of the system of conservation laws involving discontinuities in one or some of the flow variables namely q z s u 1 w p 1 and p 2 the shocks or discontinuity like portions of the solutions are generated in the hyperbolic solver of the software when these portions of the solution are processed by the elliptic solver the computation of the gradients of the flow variables near such steep fronts produces quantities with an extremely large magnitude producing numerical instabilities when the jacobian matrix is formed and inverted if feasible the pathological computations described are especially dramatic if one attempts to make a mesh refinement study of the solution which is mandatory when presenting numerical solutions if δx is progressively reduced to get mesh independent results the hyperbolic solver produces sharper discontinuities given the increased resolution processing of these solutions by the elliptic solver encounters not only shaper shocks but also a smaller δx thus much higher gradients thereby guaranteeing solution crashing previous solvers cantero chinchilla et al 2018 gamero et al 2020 were found to suffer from this issue while dealing with the moving hydraulic jumps experimentally generated in this research thus a special solver for robust handling of shocks was developed as follows the following gradients of the flow variables are used as shock development sensors 33 z s x x q 2 g h 2 x u 1 2 g x w 2 g x p 1 γ x p 2 γ thereby permitting to detect formation of shocks in the output of the hyperbolic solver a threshold value for the gradients φ thr is defined which is set at 75 by numerical experimentation for meshes involving δx 0 01 m a shock is considered formed in the output of any of the flow variables if the corresponding gradient is above this threshold if this occurs in a cell the non hydrostatic flow variables are reset to zero in a bandwidth equal to the stencil used to compute dispersive terms e g 2δx this process eliminates the gradients near sharp discontinuities and permits a robust numerical handling at the initial stage of the dam break flow generation this process is not applied given that the initial condition is itself a discontinuity thus the shock development detection is applied for t t 0 where t 0 is the hydrodynamic time scale h u g 1 2 wu and wang 2007 which is of the order of the gate opening time in the experiments 3 2 2 boundary conditions in dam break wave experiments to mimic numerically the experiments conducted in the flume boundary conditions must be modeled with accuracy the boundary conditions are incorporated in the mathematical model using ghost cells at boundaries computational cells are from i 1 to i n at the tailwater section of the flume the closed gate is modeled as a reflective boundary condition i e h n 2 h n 1 h n and q n 2 q n 1 q n toro 2001 with the remaining variables in the vam model reset to zero this was found to reproduce well the experimental observations at the upstream end of the flume a transmissive boundary condition is implemented for all variables but except the flow depth i e q u 1 p 1 p 2 w w 1 q u 1 p 1 p 2 w w 2 and q u 1 p 1 p 2 w w 0 q u 1 p 1 p 2 w w 1 the flow depth in the cells i 1 and i 0 is computed by setting energy conservation in the water tank using the experimentally determined time variation of h uw 34 e 1 e 0 h u w 1 100 0 0015 t 4 0 032 t 3 0 1656 t 2 0 2503 t 0 0393 h 1 here e is the specific energy flow depths are thus obtained by analytical inversion of the specific energy diagram as follows 35 h 1 h 0 e 1 1 3 2 3 cos γ 1 3 e 0 1 3 2 3 cos γ 0 3 where 36 γ 1 γ 0 arcos 1 27 4 e 1 h c 1 3 arcos 1 27 4 e 0 h c 0 3 34 h c 1 h c 0 q 2 2 g 1 3 q 1 2 g 1 3 the flow depth computed using eq 35 corresponds to the subcritical root of the specific energy diagram castro orgaz and hager 2019 the inviscid part of the vam equations is not new in the solver presented but the model equations here include the turbulent stresses the numerical solver is new and includes the following important capabilities not available in a previous model by cantero chinchilla et al 2018 1 inclusion of turbulence by an eddy viscosity approach 2 inclusion of a new module for shock detection in the elliptic step allowing mesh refinement 3 use of the robust muscl hancock scheme in the hyperbolic step 4 dry bed treatment allowed the old vam 2018 model failed during trials to run it for the new experimental conditions presented due to the formation of hydraulic jumps and the dry bed zones resulting in a collapse in computations 4 results 4 1 benchmark numerical tests several challenging benchmark tests are selected in this section to evaluate the ability of the vam model to deal with discontinuous topography to grant the c property and to tackle dry wet unsteady fronts first a tidal wave test over a submerged rectangular step by bermudez and vazquez 1994 is used to validate the capability of the model on dealing with discontinuous topography zhou et al 2002 liang and marche 2009 the tidal wave is designed to occur in a 1500 m long frictionless channel with asymptotic analytical flow solution as follows 35 h 20 z b 4 sin π 4 t 86400 1 2 36 u 0 x l π 5400 h cos π 4 t 86400 1 2 where l 1500 m the bed profile is 37 z b 8 if x 750 187 5 0 otherwise the test is conducted herein using δx 7 5 m and cfl 0 8 until t 10800 s is reached equations 38 and 39 are used to set the initial conditions i e substituting t 0 s while eq 38 is used to impose the inlet boundary condition at x 0 m the right end of the channel is modeled using a reflective closed boundary condition vam model results for this test are shown in fig 6 depicting an excellent agreement with the analytical solution in both water surface fig 6a and unit discharge fig 6b predictions at t 10800 s where bed discontinuities did not impact the numerical solution second a still water test at a surface piercing hump liang and marche 2009 i e with dry wet interfaces is selected to evaluate the preservation of c property by the proposed vam model for this test the channel is 1 m long and frictionless with the surface piercing hump defined by 38 z b max 0 0 25 5 x 0 5 2 where the flow is at rest with maximum depth of 0 1 m the test is conducted herein using δx 0 01 m and cfl 0 4 until t 200 s fig 7 shows the vam model results for the free surface and the unit discharge after 200 s of computation where the agreement with the static solution at rest is excellent finally the analytic solutions by thacker 1981 for an oscillating shoreline in a parabolic bowl are used to evaluate the ability of the proposed vam model to tackle dry wet fronts in movement over a non uniform slope liang and marche 2009 lai and khan 2018 the analytical solutions for the flow depth and the unit discharge in a rectangular channel with parabolic bed profile are thacker 1981 lai and khan 2018 39 z s max z b b 2 cos 2 ω t b 2 4 b x ω cos ω t 4 g 40 q max 0 z s z b b sin ω t with 41 ω 2 π t 2 g h 0 l 0 where t is the period ω is the frequency b is a speed parameter and h 0 and l 0 are definition parameter for the parabolic bed which is z b h 0 x l 0 2 1 this test is conducted herein using δx 0 01 m cfl 0 4 h 0 10 m l 0 600 m and b 5 m s until t t 269 s the domain extended to x є 1000 1000 m thus yielding the boundaries unaffected by the flow during the computation fig 8 shows the vam model results for flow depth and unit discharge at three different instants i at t t 2 when the flow reaches its highest position to the right fig 8a and b ii at t 3t 4 when the flow passes through the horizontal position advancing towards the left end fig 8c and d and iii at t t when the flow reaches its highest position to the left fig 8e and f the vam model results show an accurate agreement with the analytical solutions at all computed instants where the moving dry bed fronts are accurately tackled validating the wetting drying algorithm in the model 4 2 lee side waves the ability of the vam model on reproducing the challenging experimental flow tests produced in this work is evaluated is this section to this end all tests described in section 2 2 table 1 were simulated using δx 0 01 m cfl 0 2 and n 0 01 ms 1 3 the numerical tests were repeated using cfl 0 1 thereby confirming mesh independence of the numerical results the vam model results for the tests 1 4 are shown in fig 10 13 respectively where four instants were selected to illustrate the main phenomena in the transformation of waves in the test series i initial stages of dam break flow generation at t 1 s or 2 s ii wave reflection at the right end of the flume at t 5 s iii wave interaction with the unsteady hydraulic jump formed at the toe of the obstacle at t 8 s and iv reflected and diffracted waves produced after the incoming surge surpassed the obstacle at t 12 s fig 9 shows the vam model results in comparison with the experimental data for test 1 where r 0 397 as well as the saint venant equations dsv model predictions in test 1 the non linearity effects are the highest among the test series also depicting the most challenging unsteady hydraulic jump formed at the toe of the lee slope of the obstacle the vam model accurately predicts the initial stages of the dam break flow as shown in fig 9a which represents the experimental data at t 1 s the amplitude of the leading broken wave fig 9a x 10 7 m is however slightly overestimated while the vam model is able to accurately approximate the advancing bore position the dsv model anticipates its location fig 10a x 10 9 m fig 9b represents the experimental data at t 5 s after the flow reflection at the right end of the flume the vam model results show excellent agreement with the experimental data of the reflected train of waves the hydraulic jump profile and the drawdown in the upstream water level as the rarefaction wave advances backwards the dsv model however fails to approximate the reflected train of waves which is attributed to the lack of dispersive terms the experimental data at t 8 s is represented in fig 9c depicting the interaction between the reflected train of waves and the moving forward hydraulic jump at the toe of the obstacle while the effect of the rarefaction wave as well as the reflected train of waves near the flume end are accurately predicted by the vam model discrepancies between the model results and the data are found in both the interaction zone and the hydraulic jump with a peak predicted by the vam model not observed in the experiments the vam model computed the dynamic pressures in an elliptic step by solving the relevant system of equations iteratively in the vicinity of a hydraulic jump the perturbation parameters determining the pressure field p 1 and p 2 are subjected to abrupt changes and thus inviscid pressure peaks are generated as solution in the elliptic step thereby resulting also in abrupt wave peaks in the free surface profile at the next time step turbulence is incorporated in the model using an eddy viscosity approach the mismatch of computations and predictions at t 8 s indicates that the modeled turbulent stresses are not strong enough to suppress the effect of the inviscid pressure peaks at some time instants the approximated turbulence closure in the proposed vam model which incorporates the bed dominating turbulence approximation for eddy viscosity by ghamry and steffler 2002a b may be the origin of this misprediction a more sophisticated approach including turbulence diffusion and production may be needed to characterize the interaction zone of the hydraulic jump and the reflected wave finally fig 9d shows the experimental data at t 12 s in comparison with vam and dsv model results overall the vam model is capable of predicting most of the reflected and diffracted waves leading to some mispredictions especially for the leading wave amplitude in the reflected bore these discrepancies may stem from the misprediction of the flow interaction around t 8 s fig 9c but are unrelated to stability issues of the model despite this weakness the turbulent vam model prediction of the experimental data by test 1 is accurate as compared to that of the dsv model l and l 2 norms for vam and dsv simulations are given in table 2 fig 10 shows the vam model results in comparison with the experimental data extracted for test 2 where r 0 6 as well as the dsv model predictions in test 2 the non linear effects in the advancing bore are expected to decrease as compared to those given by test 1 thus leading to a non breaking bore at the initial stages of the dam break flow as observed during the experimentation in addition during the test the unsteady hydraulic jump was observed to develop a shorter front than that by test 1 fig 10a shows the experimental data at t 2 s i e the dam break flow right after the initial stages in comparison with the vam and dsv model results here the vam model provides the results in excellent agreement with the experimental data but for the leading wave amplitude fig 10a x 12 m it is slightly underestimated in fig 10b the experimental data at t 5 s is shown where the dam break flow has already been reflected at the right end of the flume and the hydraulic jump at the toe of the obstacle begins to develop in comparison with the results by both models while the dsv model less predicts the reflected train of waves the vam model is able to accurately approximate the experimental data at t 5 s however it leads to an overestimation of the zone of development of the hydraulic jump at the toe of the obstacle note that the latter referred zone is mainly turbulence dominated and thus the accuracy of vam model results may suffer from the approximate turbulence closure considered in this work the overestimation of the initial stages of the hydraulic jump is also evident in fig 10c where the experimental data at t 8 s is plotted against the vam and dsv model results albeit the two leading waves at the front of the reflected bore are accurately approximated by the vam model the amplitude and phase of the train of waves are slightly misinterpreted however the impact of this underestimation in the train of waves at t 8 s is minimal on the approximation of the subsequent test data as shown in fig 10d for the experimental data at t 12 s in comparison with both models results the results by the vam model are overall satisfactory for t 12 s where the major discrepancies with respect to the experimental data focus on the amplitude of the train of waves upstream and downstream the obstacle l and l 2 norms for vam and dsv simulations are given in table 3 fig 11 shows the vam model results in comparison with the experimental data extracted for test 3 where r 0 8 as well as the dsv model predictions in test 3 the wave non linearity is minimum as the dam break depth ratio approaches unity during the experiments the unsteady hydraulic jump at the toe of the obstacle was observed to be very close to the crest the experimental data at t 2 s is shown in fig 11a where the vam model provides a good approximation of the data in contrast to the prediction by the dsv model which not only anticipates the bore position and mispredicts the train of waves but also depicts a fictitious incipient hydraulic jump at the lee slope of the obstacle it is noteworthy that during the experimentation in test 3 a notable surface water splash was observed after the sluice gate opening thus contaminating the experimental data in the subsequent time instants in consequence the experimental free surface was hardly tracked at t 2 s and therefore those time instants are not shown in fig 11 in fig 11b the experimental data at t 5 s is shown here the vam model is demonstrated to be able to predict all the data however hydraulic jump and the amplitude of the leading reflected wave are slightly overestimated and underestimated respectively the experimental data at t 8 s is presented in fig 11c where the vam model accurately approximates the experimental data of the reflected train of waves before they encounter back the obstacle fig 11d presents the experimental data at t 12 s after the transformation of the reflected dam break waves and the interaction back with the obstacle in line with the results for the experimental data at t 12 s in the experiments 1 and 2 the vam model provides a fair approximation of the experimental data where only major mispredictions are found in the phase of the train of waves upstream the obstacle l and l 2 norms for vam and dsv simulations are given in table 4 fig 12 shows the vam and dsv model results in comparison with the experimental data extracted for test 4 where r 0 i e dam break flow under dry bed conditions downstream castro orgaz and chanson 2017 in test 4 the parabolic like profile of the dry bed dam break bore advanced towards the right end of the flume encountering minor bed irregularities at the structural joints which yielded little free surface perturbations right after the reflection of the dam break bore two unsteady hydraulic jumps are formed i a moving forward one at the toe of the lee slope of the obstacle and ii a moving backward one at the front of the reflected bore fig 13a shows the experimental data at t 1 s for the dry bed dam break waves at the initial stages where the vam model results merely differ from the dsv predictions in the profile of the rarefaction wave the experimental data at t 5 s depicts the first instants after the bore reflection where an unsteady hydraulic jump with smooth transition is developed fig 13b here the vam model proposed in this study is shown to be unable to tackle the smooth hydraulic jump front however showing a direct transition which is in line with the predictions of the dsv model due to the shock detection in this zone the latter suggests that the turbulence closure may need to be enhanced some discrepancies have also been found in the prediction of direct hydraulic jumps as shown in fig 12c x 11 3 m for the experimental data at t 8 s the last experimental data extracted for test 4 corresponds to t 12 s fig 12d where the flow interaction between the two unsteady hydraulic jumps is shown leading to a challenging turbulence dominated phenomena however the vam model provides a fair approximation of the test of data showing non hydrostatic free surface waves after the hydraulic jump l and l 2 norms for vam and dsv simulations are given in table 5 4 3 steady flow over the obstacle a steady flow experiment conducted in this work for the maximum discharge q 0 1826 m2 s was used fig 13a and b to validate the vam model results over an obstacle the obstacle is a gaussian profile z bg 0 209 x exp 1 2 x x x crest 0 254 2 where z bg is the local obstacle height above the flume bed and x crest is the longitudinal location of the crest installed at x crest 6 565 m the experimental obstacle is similar to that of sivakumaran et al 1983 but q is higher in the present experiments and thus the degree of non hydrostaticity of the flow is stronger in fig 13 we have included the experimental measurements of the free surface profile z s x and piezometric bed pressure head p b γ z b x in this obstacle model comparison of the simulated results for the free surface and bed piezometric pressure head p b γ z b obtained from the vam model in fig 13 shows the accuracy of this shallow water formulation predicting the flow features over the obstacle the mesh size independence of the results was evaluated progressively reducing δx and cfl 5 conclusions a new experimental procedure to investigate wave interaction and flow d dd adjustment over obstacles is presented by constructing a large scale obstacle model in a flume equipped with a wave generation mechanism based on a dam break like set up the experiments were used to produce a variety of relevant phenomena over topography as broken and dispersive undular waves hydraulic jumps non hydrostatic critical flow over a sill crest and wave reflection in addition to the novelty of the procedure to study flow interaction with obstacles the experimental database generated is itself of utility for environmental fluid flow modelers given that it can be directly used as benchmark test cases while testing their models steady flow tests were used additionally to determine the dynamic fluid pressures over the obstacle a new shallow water weighted averaged residual model with the ability to mimic turbulent breaking processes trough the formation of shocks or moving hydraulic jumps is presented this is due to the inclusion of the turbulent velocity profile and reynolds stresses into the model equations with a new shock detection algorithm conferring robustness to the numerical solver dispersive effects and non hydrostatic bed pressures are further tackled by the model given the inclusion of the vertical accelerations these features make the weighted averaged residual model presented a suitable tool for environmental modeling of flows over topography with sills the turbulent flow model developed reproduces the main features observed during experimentation namely undular and broken surges dispersive wave reflection hydraulic jumps and non hydrostatic critical flow at sill crest with high non hydrostatic pressures with enough accuracy for practical modeling purposes the dispersionless dsv equations which is the frequent shallow water flow representation used to study flow adjustment over obstacles produces only rough estimates or simply does not reproduce the observed experimental phenomena the main outcome of this research is a contribution to the physical understanding of the flow adjustment over an obstacle with the new experiments conducted and by producing a new and robust shallow water solver with capabilities to deal with several hydraulic phenomena not accounted for in other solvers data availability the experimental database generated is available as supplementary material in the file experiments ems2022 xls declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the work of o castro orgaz project leader was supported by the spanish projects ctm2017 85171 c2 1 r and pid2020 114688rb i00 and grant maría de maeztu for centers and units of excellence in r d ref cex2019 000968 m fncc was partly funded by mcin aei 10 13039 501100011033 and the nextgeneration eu prtr through juan de la cierva program ijc2020 042646 i and a selection of doctoral researchers grant by the junta de andalucía government spain ref doc 00996 rjb was partly funded by mcin aei 10 13039 501100011033 through juan de la cierva program ijc2019 038848 i we thank the two anonymous reviewers of this work for the comments offered to improve it appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105422 
25559,environmental data often include outliers that may significantly affect further modelling and data analysis although a number of outlier detection methods have been proposed their use is usually complicated by the assumption of the distribution or model of the analyzed data however environmental variables are quite often influenced by many different factors and their distribution is difficult to estimate the envoutliers package has been developed to provide users with a choice of recently presented semi parametric outlier detection methods that do not impose requirements on the distribution of the original data this paper briefly describes the methodology as well as its implementation in the package the application is illustrated on real data examples keywords outlier data validation kernel regression environmental data r package 1 introduction outliers in time series are observations that significantly differ from patterns of the other values in the sequence the common interpretation is that the outliers are drawn by a mechanism that differs from the principles of the underlying series the measurements that appear inconsistent with the rest of the data are often present in large environmental data sets it is known that outlier measurements may result from unexpected behavior of the analyzed variable unusual measurement conditions or from numerous experimental errors because the outliers may significantly affect further analysis and modelling their detection and subsequent interpretation is an important part in environmental data validation and treatment filzmoser 2005 garces and sbarbaro 2011 although large data sets are measured a large amount of data are still managed manually which is time consuming in addition manual data assessment is influenced by the subjective evaluation of the specialist therefore there is a growing need to detect outliers in environmental time series automatically over the years a number of outlier detection methods have been proposed some of which may be found for example in fox 1972 burman and otto 1988 gupta et al 2014 chandola et al 2009 shaadan et al 2015 however most formal outlier detection methods are either limited by the requirements on the distribution of the data or do not consider trends in the data since the data from numerous environmental areas are influenced by many factors which are quite often unknown their distribution cannot be easily estimated this paper presents the r package envoutliers čampulová and čampula 2019 which includes three semi parametric recently presented methods čampulová et al 2017 2018 holešovský et al 2018 for automatic detection of outliers in univariate time series with emphasis on environmental sequences the methods were originally developed for local point wise outliers but they are also suitable on a global scale and mostly also for a small number of collective i e innovational or masking outliers moreover the method of holešovský et al 2018 has been significantly extended to a more general times series the package is available for r r core team 2018 from the comprehensive r archive network cran at http cran r project org package envoutliers all three algorithms are based on non parametric smoothing of the data by removing the variable frequency term and subsequent analysis of the smoothing residuals whereas assumptions on the distribution of the original data are not imposed the automatic detection of outliers is especially useful during the inspection and validation of large data sets where manual quality assessment is very time consuming the below presented approaches may be applied for example as auxiliary techniques providing support in indication of extraordinary observations the attention of a quality controller may afterwards be paid solely to such automatically identified outliers and does not need to focus on the entire data set moreover all the data are evaluated based on the same criterion that is not influenced by subjective assessment besides the time savings this is another advantage of automatic outlier detection several r packages for point outlier detection are available and are briefly described in the following paragraphs the outliers package komsta 2011 provides commonly used statistical tests and basic approaches for the detection and treatment of outliers the tsoutliers package de lacalle 2019 implements a procedure for automatic detection of outliers in arima time series models the implemented algorithm is presented in chen and liu 1993 and considers several types of outliers such as innovational outliers additive outliers level shifts temporary changes and seasonal level shifts the anomaly package fisch et al 2020 implements several procedures based on parametric models the collective and point anomaly detection approach fisch et al 2019a multi variate collective and point anomaly detection fisch et al 2019b proportion adaptive segment selection jeng et al 2013 and the bayesian abnormal region detector bardwell and fearnhead 2017 the anomalize package dancho and vaughan 2019 is based on the decomposition of a time series by loess or by piecewise medians in order to remove trend and seasonal components subsequently outliers anomalies are detected on remainders using the inter quartile range iqr or generalized extreme studentized deviate gesd test however the iqr approach may not be as accurate in detecting anomalies since high leverage anomalies may skew the centerline of the iqr the disadvantage of using the gesd test is the assumption of normality the outlierdetection package tiwari and kashikar 2019 implements several procedures such as the algorithm suggested in barnett and lewis 1998 for outlier detection in data following weibull distribution the distance based outlier detection procedure presented in hautamaki et al 2004 dispersion based outlier detection ben gal 2005 depth based outlier detection in a 2 d dataset following some definition of depth johnson et al 1998 and density based outlier detection for spatial data ester et al 1996 the otsad package iturria et al 2019 implements several online outlier detection procedures for time series these are based either on the probabilistic exponentially weighted moving average pewma method proposed in carter and streilein 2012 and its variations raza et al 2015 or on extensions to the k nearest neighbor algorithm ishimtsev et al 2017 laxhammar and falkman 2015 burnaev and ishimtsev 2016 as the pewma method or similar is suggested mostly for stationary data the outlier detection approaches in the first case have very low adaptability to the abrupt changes in data trends or variability the disadvantage of the latter is the need for a reference sample therefore the methodology is not directly applicable and the model is dependent on the quality of the training set the methods implemented in the envoutliers package presented in this paper are capable of considering trends in the time series data and are suitable for non stationary sequences with variable means at the same time the methods have no requirements on the specification of the distribution or a model of the original data and no training set is required which are the main advantages over the other existing packages since kernel smoothing based on local bandwidth is performed in the first stage the implemented algorithms are adaptable to abrupt changes in data trends and or variability the user is able to omit the initial smoothing moreover the approaches implemented in the envoutliers package do not rely on any heuristic arguments but make use of significant theoretical results of recently developed areas such as the kernel regression extreme value theory or changepoint analysis the remainder of the paper is structured as follows section 2 briefly describes the outlier detection methods and their accessibility from the envoutliers package the performance of the functions using real data examples is illustrated in section 3 and a summary of the paper is given in section 4 specific details on the envoutliers package are given in the appendix 2 outlier detection methods and the envoutliers package in this section we briefly introduce the methods for the identification of outliers presented in čampulová et al 2018 čampulová et al 2017 and holešovský et al 2018 and we put into context the procedures implemented within the envoutliers package as already mentioned the main aim of all three methods is to smooth the original data and subsequently analyze the residuals the smoothing is performed using kernel regression with a local bandwidth herrmann 1997 the local bandwidth adapts to the structure of the data and the curvature of the resulting smoothing line changes with respect to the variability of the data therefore regression with the local bandwidth describes fast changes in data variability in more detail than regression with a global bandwidth thereby it follows that the methods are generally suitable for global as well as for local point wise outliers i e observations that are outlying in a certain context or time period possible extensions to other types of outliers differs with respect to a particular method that is applied in the subsequent analysis although not intended to the methods discussed below except for those in section 2 1 should also be able to identify a relatively small number of collective or masking outliers assume we have the observations y 1 y n measured at n time instants t 1 t 2 t n lying in interval a b the heteroscedastic regression model is given by 1 y i m t i σ t i ϵ i i 1 n where m is an unknown regression function ϵ i are i i d random variables with zero mean and unit variance and σ t i is the standard deviation function describing the variability of y i the regression function m is estimated using the gasser müller estimate gasser and müller 1984 in the form 2 m t h t i 1 n y i l i 1 l i 1 h t k t u h t d u for t a h t b h t where k is a kernel of some order k and h t is a bandwidth at point t the limits of the integral are given by l 0 a l i 0 5 t i t i 1 for i 1 2 n 1 and l n b although kernel regression with a local bandwidth herrmann 1997 gives relatively good results compared to the other methods frequently used for environmental data smoothing čampulová 2018 in the outlier detection functions from the envoutliers package the user may also choose kernel smoothing based on a global bandwidth gasser et al 1991 for both local and global bandwidths the optimal kernels gasser et al 1985 of the order k 2 or k 4 are used respectively in the second step the smoothing residuals x i y i m t i i 1 n are analyzed for the presence of outliers the three methods implemented in the envoutliers package make use of different principles an overview of which is given below each of the methods requires at the input one or more auxiliary parameters determining the criterion for outlier detection naturally the choice of these parameters controlling the number of detected outliers is important and should be made with regards to the specific application requirements and empirical experience with historical data since the true reason for the presence of identified outliers cannot be clarified automatically the quality of the detected outliers must be further evaluated by a specialist who considers information about unusual measurement conditions therefore the identification of false outliers observations that were incorrectly identified as outliers is not regarded as undesirable 2 1 method based on changepoint analysis suppose that the residuals are independent observations with cumulative distribution functions f 1 f n the aim is to detect changes in the distribution of the residuals together with an unknown number of change points τ 1 τ m such that 1 τ 1 τ m n and f 1 f τ 1 f τ 1 1 f τ 2 f τ 2 1 f τ m f τ m 1 f n the binary segmentation method proposed by scott and knott 1974 is probably the most widely used changepoint search method applications of this or a modified approach may be found in different fields see e g olshen et al 2004 neubauer and veselý 2013 fryzlewicz 2014 raveendran and sofronov 2017 binary segmentation is an approximate algorithm but it is computationally fast currently algorithms are available that are exact and efficient in terms of computing time therefore the pelt algorithm killick et al 2012 based on a parametric model or the non parametric e divisive method matteson and james 2014 is preferred although the running time of the e divisive method is quadratic in the length of the analyzed data it may be effectively used for data of a length that is commonly used in environmental applications the advantage of this method is that it generates consistent changepoint estimates the detected estimates of changepoints τ 1 τ m partition the residuals into m 1 homogeneous segments s j x τ j 1 1 x τ j j 1 m 1 where the outlier residuals are going to be identified in case that the residuals in homogeneous segments are approximately normally distributed or may be transformed to normality using the box cox transformation box and cox 1964 the outlier residuals are detected using the grubbs test grubbs 1950 or quantiles of normal distribution otherwise the chebyshev inequality based approach is applied the latter identifies outlier residuals such that their values exceed the limits ls j where s j denotes the sample standard deviation of the residuals on the given j th segment and l is a suitably chosen constant residual analysis based on normal quantiles determines l equal to the α 2 quantile u α 2 of the standard normal distribution for 0 α 1 the principle of using the grubbs test lies in consecutive assessment of the most deviated residual i e the residual with the largest absolute value in the event that the deviation in a particular step is indicated as being significant by the grubbs test the corresponding value is labelled as an outlier and excluded from further analysis the procedure is then repeated until no significant difference is detected a significant weakness of the grubbs test is that it is particularly unsuitable for detection of masking outliers the approach for the residual analysis on homogeneous segments within the envoutliers package is selected automatically based on normality results by default the residuals on homogeneous segments are tested for normality using the robust medcouple test mc lr brys et al 2008 in case that the normality is rejected the box cox transformation is applied and the mc lr test is performed on the transformed residuals if raw or the transformed residuals are normally distributed the grubbs test and the normal quantiles based approach are performed on either raw or transformed residuals in other cases the approach based on chebyshev inequality is preferred the changepoint analysis is implemented in the krdetect outliers changepoint function within the envoutliers package depending on the input arguments the user may decide for example whether the kernel smoothing or the changepoint based segmentation should be performed or not choose what type of bandwidth should be considered as well select particular values or which outlier detection method to use among other things see the appendix for more details the method in the envoutliers package is parameterized by a l default parameter for residual analysis based on chebyshev inequality and by an alpha default parameter for residual analysis based on quantiles of normal distribution these parameters which determine the criteria for detection of outlier residuals may be found using the data driven algorithm a1 for l default and modified algorithm a1 for alpha default suggested in čampulová et al 2018 the results of the changepoint analysis as well as of other methods discussed below may be visualized and summarized using the plot and summary functions the function for kernel smoothing in the envoutliers package is accessed using the smoothing function 2 2 method based on control charts the method based on control charts shewhart 1931 assumes that the residuals x 1 x n represent a process with the mean μ and standard deviation σ x the aim is to evaluate the statistical stability of the residuals for which the unbiased estimates of μ and σ x are needed to obtain these estimates the residuals are partitioned into disjointed segments i e subgroups of size n the parameters μ and σ x for the whole residual process x 1 x n are estimated on the basis of sample means sample standard deviations and sample ranges at individual segments as described in e g čampulová et al 2017 subsequently three types of control charts x chart s chart and r chart are used to assess the statistical stability of the residuals and to identify the segments of size n where the outliers occur changes in the mean may be detected using the x chart changes in the variance are identified by the control charts r and s control charts graphically visualize changes in residual characteristics over time together with two horizontal lines representing the upper and lower control limits the limits for the x chart the r chart and the s chart are consecutively in the form 3 l c l x l x σ s n u c l x l x σ s n 4 l c l r max 0 r l r d 3 n σ r u c l r r l r d 3 n σ r 5 l c l s max 0 s l s σ s s 2 u c l s s l s σ s s 2 where x is the sample mean evaluated from the entire residual process and r s are the means of sample ranges and sample standard deviations respectively obtained at disjoint segments the parameters σ s σ r are suitable estimates of σ x see wild and seber 2000 for more details the term d 3 n is a correction factor proposed by cary 1999 the three auxiliary parameters l x l r l s are of a similar nature they govern the width of particular control limits and hence determine the outlier detection criterion the residual process is considered to be statistically stable if the estimates of particular characteristics at disjoint segments lie between the corresponding control limits lcl and ucl determined by 3 4 or 5 respectively in this case the process is considered to be stable and no outliers are detected otherwise the segments of observations corresponding to sample means sample standard deviations or sample ranges exceeding corresponding control limits are considered to contain outliers and are detected as such hence the application of a method based on control charts does not lead to the identification of particular outliers but rather to the identification of observation segments where the outliers occur more details of the method may also be found in čampulová et al 2017 together with recommendations for the optimal size of segments within the envoutliers package there are charts that are implemented in the krdetect outliers controlchart function the charts are created for the smoothing residuals under the default settings of kernel regression however the user is also offered the option to adjust the smoothing conditions as well as to completely disable the kernel regression different sizes of subgroups for each of the three control charts may be chosen by setting different values of corresponding parameters see the appendix for more details 2 3 method based on extreme value theory additive residuals may also be analyzed on the basis of extreme value ev theory coles 2001 and its generalization for dependent variables leadbetter et al 1983 beirlant et al 2004 the core idea is to detect the outlier in such a way that the corresponding residuals exceed an extremely high level hence an outlier is understood as an extraordinary observation with only negligible probability of being obtained in terms of ev theory such a high value is commonly referred to as a r return level for some large integer r i e a value exceeded once every r observations on average put simply the r return level is the 1 r 1 quantile of the residual model distribution suppose that the residuals x 1 x n is a random sequence with an unknown distribution in practical situations the extreme values are usually estimated on the basis of the peaks over threshold pot approach holešovský et al 2015 fawcett and walshaw 2012 scarrott and macdonald 2012 in the pot model the residuals exceeding a given threshold u large enough are analyzed it may be derived that the threshold exceedances may be modelled by the generalized pareto gp distribution the outlier detection method from holešovský et al 2018 uses originally maximum likelihood ml estimates of the gp parameters the ml estimates are however suitable only under certain regularity conditions smith 1985 zhou 2009 therefore one of the extensions of the approach of holešovský et al 2018 to a more general case that we propose in this paper is made by incorporating semi parametric moment estimators of gp parameters de haan and ferreira 2006 to the envoutliers package the moment estimators have many desirable properties and belong to the most commonly applied gp estimators holešovský et al 2015 draisma et al 1999 note that in the semi parametric context the threshold exceedances correspond to a set of k largest order statistics denoted x n x n 1 x n k 1 hence the issue of a suitable threshold choice now corresponds to the issue of the proper selection of k a proper threshold selection say u 0 is one of the main issues of the pot model affecting the estimation quality some approaches for choosing a suitable threshold value are given in e g coles 2001 or scarrott and macdonald 2012 which include traditional graphical diagnostics as well as more recently developed adaptive methods in practical situations the threshold u 0 is often set to a high data quantile with the aim of bringing automation to the outlier detection procedure this approach is also applied in the envoutliers package whereby the threshold value is selected as a 90 quantile of smoothing residuals by default in environmental or economical series it is common that the dependence of the observations is mostly present in the form of a short time dependence whereby distant observations are approximately independent the measure of this short time dependence at extreme levels may be described by a parameter θ a so called extremal index several extremal index estimators have been proposed in the literature the methodology in holešovský et al 2018 uses the block maxima estimator of gomes 1993 by default in envoutliers the modified maxima estimator of ancona navarrete and tawn 2000 and the sliding blocks estimator northrop 2015 are implemented with the envoutliers package in addition an estimator based on threshold inter exceedance times i e the runs estimator smith and weissman 1994 the intervals estimator ferro and segers 2003 the k gaps estimator süveges 2007 süveges and davison 2010 and the recently proposed censored estimator holešovský and fusek 2020 with many desirable properties are also included note that each of the particular estimators depends on one or more auxiliary parameters some of them are preset as default for more details see the corresponding papers finally the r observation return level z r max is estimated using the relation 6 z r m a x u σ ξ λ u 1 1 1 r 1 θ 1 ξ 1 where the parameter λ u and the gp parameters ξ σ should be replaced with their corresponding estimates see holešovský et al 2018 2015 for more details in the case of the semi parametric moment gp estimators the return level z r max is obtained again from relation 6 whereby u and λ u should be replaced by their non parametric equivalents i e the n k th largest residual x n k and the fraction k n similarly the return level z r min for extremely low values is constructed from the negative residuals given the return levels z r min and z r max for residuals the observations exceeding these limits are identified as outliers more details on the method as well as recommendations for the optimal values of the parameters may be found in holešovský et al 2018 the envoutliers package implements the ev methodology in the krdetect outliers ev function the user may again control the smoothing conditions but also select different methods and auxiliary parameters intended for the ev analysis the package also includes the mrl plot and stability plot functions providing graphical diagnostic tools for the threshold selection following scarrott and macdonald 2012 the parameter r is implemented as return period in the envoutliers package 3 application and illustration this section explores the use of the functions introduced in the previous section on selected data sets available in r package openair carslaw and ropkins 2012 the data sets contain hourly measurements of wind speed wind direction and concentrations of no x no2 o3 pm10 so2 co and pm25 observed at marylebone london between january 1 1998 and june 23 2005 for this illustrative example we consider one part of the data namely the o3 concentrations measured in december 2002 the selected data includes a total of 744 observations and we begin by its loading and selecting it based on the date image 1 the data contain several missing values however this is not a limitation for the use of outlier detection methods presented in the foregoing section fig 1 visualizes the o3 concentrations in parts per billion ppb measured in the period of interest it is evident that several values deviating from the neighboring measurements are present in the data this motivates a search for the outliers 3 1 outlier detection based on changepoint analysis firstly we demonstrate the detection of outliers using the algorithm based on changepoint analysis and the automatic selection of detection method for residual analysis image 2 from the summary of the results we are able to see that the approach based on the grubbs test was automatically selected for the analysis which follows from the fact that the normality of the residuals was not rejected and the box cox transformation was not necessary the method identified four outliers 0 53763 of the total data visualized in fig 2 a the figure also shows that the non parametric changepoint analysis leads to detection of two changes in the residual variability that partition the analyzed residuals into three homogeneous segments visualized by dashed vertical lines next the outliers are detected using the approach based on normal quantiles the alpha default parameter determining the criterion for the detection of outliers is chosen automatically using the modified algorithm a1 čampulová et al 2018 as given by default the previous example showed that residuals on individual segments may be considered approximately normally distributed therefore the parametric changepoint analysis set by default may be used image 3 as it may be seen from the summary of the results and from the plot visualized in fig 2b ten outliers were detected in total notice that two outliers close to each other were identified which results in their overlapping in the graph these are the observations with indices 607 and 610 corresponding to time instants 6 00 a m and 9 00 a m on december 26 and both taking the value of 13 ppb the changepoints identified using the non parametric analysis visualized by dashed vertical lines do not differ significantly from the changepoints estimated using the parametric approach this may be considered as evidence of the normality of the segmented residuals to see the values of the alpha default parameter used for detection of outlier residuals on individual segments the following command is used image 4 the particular values of alpha default parameter on individual segments were found using the modified algorithm a1 on the first two segments outliers were detected based on alpha default 0 0005 while on the third segment alpha default 0 0085 was used finally outliers are detected using chebyshev inequality and the same value of the l default parameter on the individual segments of course analogous to the previous example the values of the l default parameter determining the criterion for the detection of outliers on individual segments may be chosen automatically using algorithm a1 čampulová et al 2018 image 5 a summary of the results reveals that nine outliers were found the plot in fig 2c shows the detected outliers together with detected changepoints visualized by dashed vertical lines it is important to remember that the choice of the appropriate value of the alpha default parameter and l default parameter is an open question and depends on many factors including the character of the data or the requirements of the analysts 3 2 outlier detection based on control charts recall from section 2 2 that the algorithm based on control charts is parameterized by parameters l x l s and l r used for the computation limits of the x chart s chart and r chart and whereby determining the criterion for the detection of outliers čampulová et al 2017 firstly the parameters are set as l x 3 l s 3 and l r 3 which is given by default image 6 performing the analysis using the krdetect outliers controlchart function produces warning messages which inform the user that the first two data values are excluded from the control chart analysis this data exclusion if necessary may be avoided by a more appropriate selection of subgroup size ensuring that the data except the missing values may be equidistantly partitioned into subgroups as it may be seen from the summary of the results no outlier was detected based on control chart x however a relatively large number of data values 39 in total were identified as outliers based on control charts s and r this may be explained by the fact that smoothing residuals are randomly distributed around a zero value outlying o3 concentrations correspond to changes in the variability of the residuals that are detected using control charts s and r while control chart x controls the changes in the mean value it is also important to remember that the method based on control charts does not identify individual outliers but detects the segments of observations where the residual process gets out of control due to the presence of outliers therefore it is predictable that the number of outliers detected using krdetect outliers controlchart function is relatively large all the detected outliers may be plotted using the following command the corresponding graph is shown in fig 2d to plot only the outliers detected based on the x chart r chart or s chart the plot type parameter is set to plot type x plot type r and plot type s respectively image 7 to obtain a less strict criterion for detection of the outliers the parameters are set to l s 5 and l r 5 clearly increasing the values of parameters l s and l r leads to a reduction in the number of detected outliers this is also evident from fig 2 or from the following summary of the results image 8 3 2 1 outlier detection based on ev theory the algorithm implemented in the krdetect outliers ev function enables default selection of all necessary parameters which makes the procedure easy to use however the user has the option to set all the parameters based on his or her requirements on the data control which we illustrate here firstly we detect outliers based on ev theory with maximum likelihood estimates of gp distribution parameters as mentioned in section 2 3 for maximum likelihood estimates based on the pot model a threshold value is needed although the algorithm implemented in the krdetect outliers ev function estimates the threshold as a 90 quantile of smoothing residuals by default the threshold may be selected based on an mrl plot and or parameter stability plots since the mrl plot and stability plot functions construct the plots directly from the input data smoothing residuals need to be extracted prior to the constructions of the plots the following command is used for this image 9 subsequently the mrl plot and the stability plots for the maximum values of the residuals together with the dependency lines are constructed the dependency lines visualize the conformity of the estimated paths with the theoretical threshold dependencies for thresholds above the chosen u0 see scarrott and macdonald 2012 for more details here we select u0 0 75 and plot the lines within the range umin and umax corresponding to 80 and 95 of the sample quantiles respectively image 10 the plots for the minimum values of the residuals are constructed in the same way with the residuals replaced by their negatives here we select the bounds corresponding to 80 and 98 of the sample quantile of the negative residuals for the dependency lines we choose u0 0 95 image 11 mrl and stability plots for maximum values of the residuals are shown in figs 3 and 4 while the plots for minimum values of the residuals are given in figs 5 and 6 it should be taken into account that both kinds of plots provide graphical diagnostics that may lead to subjective conclusions nevertheless they give some support for initial statistical inference in case an appropriate threshold is unknown in both the above instances is the choice of u0 is made based on a compromise among the all figures there are no significant deviations of the sample paths from the dependency for threshold larger than u0 the selected thresholds u0 correspond to 92 8 and 95 3 of the sample quantile of the residuals and negative residuals respectively this is also consistent with the theoretical requirement that the threshold should be sufficiently large hence the value u0 is considered suitable for further approximation of the threshold exceedances by the gp distribution in both cases finally the outliers based on the selected thresholds may be found however except for the threshold the estimate of extremal index is needed although the function krdetect outliers ev function estimates the extremal index based on the block maxima approach gomes 1993 by default here we prefer the intervals estimator also the return period parameter determining the r observation return level needs to be chosen in analogy to the previous examples the choice of return period is an open question and depends on the character of the data as well as on the assumptions of the data analysis firstly we select return period 24 5 120 as given by default this means that only o3 concentrations corresponding to residuals whose values are exceeded on average once every five days will be sought image 12 as it may be seen from the summary of the results 19 outliers were detected in total seven of the outliers correspond to smoothing residuals with extremely low values while twelve correspond to smoothing residuals with extremely high values all the detected outliers may be plotted using the following command which produces the graph in fig 7 a to plot only the outliers corresponding to smoothing residuals with extremely low and high values the plot type parameter is set to plot type min and plot type max respectively image 13 similarly to the inference based on changepoint analysis using normal quantiles see fig 2b the method identified outliers at the two observations with indices 607 and 610 which overlap in the graph for illustrative purposes the outliers are detected also using moment estimates of the gp distribution parameters recall that for moment estimates the values of the k min and k max parameters giving the numbers of the largest order statistics used to find the moment estimates for residuals with low and high values are needed these values may be selected again based on mrl and stability plots or they may be set by default by the algorithm which we prefer here to estimate the extremal index parameter the sliding block estimator northrop 2015 is used and the return period is set as return period 24 10 240 image 14 in this case only eight outliers were detected again including the close observations with indices 607 and 610 overlapping in the graph the results are shown in fig 7b 3 3 remark for outlier detection approaches the data considered for illustration of the methods in section 3 contained a trend however in cases where the data are randomly dispersed around a zero value performing data smoothing may not be necessary and residual analysis may be applied directly to the raw data in these situations the user may specify for each of the three presented outlier detection functions that no smoothing is performed by setting the perform smoothing false parameter similarly for homogeneous residuals and the changepoint based method changepoint analysis may be omitted from the analysis by setting the perform cp analysis false parameter 3 4 comparison of presented detection methods it is evident from figs 2 and 7 that the results differ according to particular method meant for the analysis of residuals moreover in the specific cases the number of identified outliers also depends on the auxiliary parameters that determine the criterion for outlier detection we may see that the smallest number of outliers was found using the changepoint analysis based on the grubbs test notice that this approach is the least variable one of all the methods implemented in the envoutliers package this is convenient as the user may use it without extensive testing of any auxiliary parameters the changepoint inference based on normal quantiles or the chebyshev inequality results among others in the identification of the same outliers as those determined by the grubbs test nevertheless the methods differ significantly in the outliers not captured by the grubbs test this is a consequence of various approaches for setting the parameters alpha and l defining the criterion for detection of outliers within the changepoint based method overall the main advantage of the changepoint analysis is its simplicity and clarity making it suitable for a common user in any modification be it the grubbs test normal quantiles or the chebyshev inequality moreover the grubbs test is a classical and widely used method for outlier identification under the assumption of normal distribution the results for the corresponding values of auxiliary parameters are comparable on the other hand violation of the normality or of the distribution symmetry may lead to significant distortion of the results these methods are also mostly inappropriate for collective or masking outliers as it is typical for the grubbs test the largest number of outliers was found using the control charts with parameters l x l s l r 3 however setting the parameters to l x l s l r 5 i e under a milder criterion for the outlier detection the results are comparable to those obtained using the methodology based on ev theory with return period 240 a relatively large number of outliers were detected using ev theory with return period 120 this confirms the fact that the choice of the parameters defining the criterion for outlier detection is essential the approach based on ev theory makes use of complex mathematical theory which may be beneficial for some users the outlier detection limits are evaluated independently unlike the changepoint analysis based on normal quantiles or the chebyshev inequality which makes the methodology more suitable if the distribution of residuals is markedly asymmetrical moreover the use of this approach is supported by theoretical aspects making it more appropriate for application on data with non negligible dependence on the other hand a suitable choice of the auxiliary parameters is often unclear and requires thorough analysis some improvements on the current procedures may be considered the first one concerns the changepoint analysis namely the outlier identification on homogeneous segments the step accounting for the normal quantiles or the chebyshev inequality should be generalized in order to make the method suitable also for asymmetrical distributions of the residuals some non parametric quantile estimators may be employed here an improvement to the method based on ev theory may be also made in its current form the return levels are evaluated on the basis of rare events including the outliers however it is known that the presence of the outliers may lead to significant bias of any estimates capturing the dependence structure in return level estimates may lead to more appropriate assessment of the outliers 4 summary the r package envoutliers with three recently published semi parametric outlier detection methods is presented the main aim of each of the three methods is to smooth original time series data and subsequently analyze the smoothing residuals the first method analyzes the residuals using changepoint analysis and classical approaches for outlier detection the second method evaluates the residuals using control charts and the last method is based on extreme value theory in the latter case other advanced estimates of the extreme values have been incorporated into the package this now makes the original methodology of holešovský et al 2018 newly applicable to a wider class of sequences the methodology of each of the three methods for detection of outliers is briefly described and the syntax is presented the use of the presented functions does not require assumptions on the distribution of the original data which is an advantage especially for data from environmental areas whose distribution cannot be easily estimated the use of the envoutliers package is illustrated on real data of o3 concentrations since outliers may result from various experimental errors the true reason for their presence in the data cannot be determined by any of the methods implemented in the envoutliers package in practice this means that the quality of the automatically detected outliers needs to be further investigated by a specialized operator who determines how to deal with the detected outliers a correct measurement that appears to be outlying from the rest of the data carries useful information about rare behavior of the studied variable therefore the observations identified as outliers may be removed retained or revised the unique contribution of the package is that the number of observations for manual data evaluation is reduced this is extremely helpful for users who are responsible for evaluating the quality of data measured continuously with high temporal resolution using the functions within the package users may study only the automatically detected outliers and do not have to control the entire data set this saves a great deal of time since larger data sets require more time for data evaluation and validation is also needed moreover the analysis of the data is based on the same criterion for all observations and is not influenced by the subjective judgement of a specialized operator 5 computational details the results in this paper were obtained using r 3 4 1 with the lokern 1 1 8 package changepoint 2 2 2 package ismev 1 42 package stats 0 1 0 package mass 7 3 51 4 package ecp 3 1 1 package car 3 0 3 package robustbase 0 93 5 package and openair 2 6 5 package r itself and all the packages used are available from the comprehensive r archive network cran at https cran r project org declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this article was produced with the financial support of the ministry of transport of the czech republic within the programme of long term conceptual development of research institutions and with the support of the specific research project no fast s 22 7867 at brno university of technology a structure of the envoutliers package in this section we describe the syntax and structure of the functions implemented in the envoutliers package within each outlier detection function smoothing of the series is provided in the first step which is based on local bandwidth and kernel of order 2 subsequently smoothing residuals are inspected for the presence of outliers however the user may select either a local or the global bandwidth the order of the optimal kernel used for smoothing or choose if smoothing should be performed or not with that in mind notice that in the following section the code symbol x stands either for the underlying time series y 1 y n or for the sequence of residuals x 1 x n depending on the user s settings this applies with exception to the mrl plot and the stability plots which are conducted directly on the input data hence the smoothing must be performed in advance in order to perform the plots on the residuals for this reason the envoutliers package also includes a function for separate kernel smoothing in the event that the smoothing is not disabled and the bandwidth is not given by the input a data adaptive local plug in herrmann 1997 for local smoothing or data adaptive global plug in gasser et al 1991 for global smoothing bandwidth is used both bandwidth estimation and kernel smoothing are performed via the lokerns and glkerns functions from the lokern package herrmann and maechler 2013 a 1 method based on changepoint analysis the method for outlier detection based on changepoint analysis is accessed within the envoutliers package using the krdetect outliers changepoint function for a parametric changepoint analysis the cpt var and cpts functions from the changepoint package killick et al 2015 are employed non parametric changepoint analysis is performed via the e divisive function from the ecp package james and matteson 2014 and boxcox from the mass package venables and ripley 2002 is used for box cox transformation of the residuals function the tests performed within the function are performed via the levenetest function from the car package fox and weisberg 2011 and the mc function from the robustbase package todorov and filzmoser 2009 the function is structured as follows image 15 the arguments of this function are x data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo perform smoothing a logical value specifying if data smoothing is performed if true default data are smoothed perform cp analysis a logical value specifying if changepoint analysis is performed if true default smoothing residuals are partitioned into homogeneous segments bandwidth type a character string specifying the type of bandwidth possible options are local default to use local bandwidth global to use global bandwidth bandwidth value a local bandwidth array for bandwidth type local or global bandwidth value for bandwidth type global for kernel regression estimation if bandwidth value null default a data adaptive local plug in herrmann 1997 for bandwidth type local or data adaptive global plug in gasser et al 1991 for bandwidth type global bandwidth is used instead kernel order a nonnegative integer giving the order of the optimal kernel gasser et al 1985 used for smoothing possible options are kernel order 2 default kernel order 4 cp analysis type a character string specifying the type of changepoint analysis possible options are parametric default to perform changepoint analysis using pelt algorithm killick et al 2012 nonparametric to perform a nonparametric approach for multiple changepoints matteson and james 2014 pen value a character string giving the formula for manual penalty used in pelt algorithm only required for cp analysis type parametric default is pen value 5 log n alpha edivisive a numeric value giving the moment index used for determining the distance between and within segments in non parametric changepoint model default is alpha edivisive 0 3 min segment length a numeric value giving the minimal required number of observations on segments from changepoint analysis if a segment contains less than the min segment length observations and the variances of data on the segment and the previous one are supposed to be equal based on levene s test fox 2016 for the homogeneity of variances the segment is merged with the previous one analogous the first segment can be merged with the second one default is min segment length 30 segment length for merge a numeric value giving the minimal required number of observations on segments for performing the homogeneity test within changepoint split control a segment with less data than segment length for merge is merged with the previous one without testing the homogeneity of variances the first segment is merged with the second one default is min segment length for merge 15 method a character string specifying the method for identification of outlier residuals possible options are auto default for automatic selection based on the structure of the residuals grubbs test for grubbs test normal distribution for quantiles of normal distribution chebyshev inequality for chebyshev inequality prefer grubbs a logical variable specifing if grubbs test for identification of outlier residuals is preferred to quantiles of normal distribution true default means that grubbs test is preferred only required for method auto alpha default a numeric value from interval 0 1 of the parameter α determining the criterion for residual outlier detection the limits for outlier residuals on individual segments are set as α 2 quantile of normal distribution with parameters corresponding to residuals on studied segment sample standard deviation of residuals on corresponding segment if alpha default null default its value on individual segments is estimated using the modified algorithm a1 čampulová et al 2018 l default a numeric value of l parameter determining the criterion for outlier residual detection the limits for outlier residuals on individual segments are set as l sample standard deviation of residuals on corresponding segment if l default null default its value on individual segments is estimated using the algorithm a1 čampulová et al 2018 the choice of parameters alpha and l that define the criterion for outlier detection is crucial for the method these values can be specified by the user or estimated automatically using data driven algorithms čampulová et al 2018 the output of the method is a krdetect object which contains a list with elements method type a character string giving the type of method used for outlier identification x a numeric vector of observations index a numeric vector of index design points assigned to individual observations smoothed a numeric vector of estimates of the kernel regression function smoothed data changepoints an integer membership vector for individual segments normality results a data frame of normality results of residuals on individual segments detection method a character string giving the type of method used for identification of outlier residuals alpha a numeric vector of α parameters used for outlier identification on individual segments l a numeric vector of l parameters used for outlier identification on individual segments outlier a logical vector specifying the identified outliers true means that the corresponding observation from vector x is detected as an outlier a 2 method based on control charts the method for outlier detection based on control charts is in the envoutliers package accessed using the krdetect outliers controlchart function the function is structured as follows image 16 the arguments of the function are x data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo perform smoothing a logical value specifying if data smoothing is performed if true default data are smoothed bandwidth type a character string specifying the type of bandwidth possible options are local default to use local bandwidth global to use global bandwidth bandwidth value a local bandwidth array for bandwidth type local or global bandwidth value for bandwidth type global for kernel regression estimation if bandwidth value null default a data adaptive local plug in herrmann 1997 for bandwidth type local or data adaptive global plug in gasser et al 1991 for bandwidth type global bandwidth is used instead kernel order a nonnegative integer giving the order of the optimal kernel gasser et al 1985 used for smoothing possible options are kernel order 2 default kernel order 4 method a character string specifying the preferred estimate of the standard deviation parameter see čampulová et al 2017 possible options are range default for estimation based on sample ranges sd for estimation based on sample standard deviations group size x a positive integer giving the number of observations in individual segments used for computation of x chart control limits if the data cannot be equidistantly divided the first extra values will be excluded from the analysis default is group size x 3 group size r a positive integer giving the number of observations in individual segments used for computation of r chart control limits if the data cannot be equidistantly divided the first extra values will be excluded from the analysis default is group size r 3 group size s a positive integer giving the number of observations in individual segments used for computation of s chart control limits if the data cannot be equidistantly divided the first extra values will be excluded from the analysis default is group size s 3 l x a positive numeric value giving parameter l specifying the width of x chart control limits default is l x 3 l r a positive numeric value giving parameter l specifying the width of r chart control limits default is l r 3 l s a positive numeric value giving parameter l specifying the width of s chart control limits default is l s 3 the choice of parameters group size x group size s group size r l x l r and l s specifying the width of control limits is crucial for the method especially different values of parameters l x l r and l s determine different criteria for outlier detection and can significantly influence the results for more information and recommendations see čampulová et al 2017 the output of the method is a krdetect object which contains a list with elements method type a character string giving the type of method used for outlier identification x a numeric vector of observations index a numeric vector of index design points assigned to individual observations smoothed a numeric vector of estimates of the kernel regression function smoothed data outlier x a logical vector specifying the identified outliers based on the limits of control chart x true means that the corresponding observation from vector x is detected as an outlier outlier r a logical vector specifying the identified outliers based on the limits of control chart r true means that the corresponding observation from vector x is detected as an outlier outlier s a logical vector specifying the identified outliers based on the limits of control chart s true means that the corresponding observation from vector x is detected as an outlier outlier a logical vector specifying the identified outliers based on at least one type of control limits true means that the corresponding observation from vector x is detected as an outlier the description of other output parameters is accessed via command image 17 a 3 method based on extreme value theory the method for outlier detection based on extreme value theory is in the envoutliers package accessed using the krdetect outliers ev function the fit of smoothing residuals to gp distribution is done via the function gpd fit from the ismev package heffernan and stephenson 2016 for the estimation of the extremal index based on block maxima approach gomes 1993 the function gev fit from the same package is used the function is structured as follows image 18 the arguments of the function are x data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo perform smoothing a logical value specifying if data smoothing is performed if true default data are smoothed bandwidth type a character string specifying the type of bandwidth possible options are local default to use local bandwidth global to use global bandwidth bandwidth value a local bandwidth array for bandwidth type local or global bandwidth value for bandwidth type global for kernel regression estimation if bandwidth value null default a data adaptive local plug in herrmann 1997 for bandwidth type local or data adaptive global plug in gasser et al 1991 for bandwidth type global bandwidth is used instead kernel order a nonnegative integer giving the order of the optimal kernel gasser et al 1985 used for smoothing possible options are kernel order 2 default kernel order 4 gpd fit method a character string specifying the method used for the estimate of the scale and shape parameters of gp distribution possible options are mle default for maximum likelihood estimates coles 2001 moment for moment estimates de haan and ferreira 2006 threshold min a threshold value for residuals with low values that is used to find the maximum likelihood estimates of shape and scale parameters of gp distribution and selected types of extremal index estimates specifically intervals estimator ferro and segers 2003 censored estimator holešovský and fusek 2020 k gaps estimator süveges and davison 2010 runs estimator smith and weissman 1994 if threshold min null default threshold is estimated as 90 quantile of negated smoothing residuals threshold max a threshold value for residuals with high values that is used to find the maximum likelihood estimates of shape and scale parameters of gp distribution and selected types of extremal index estimates specifically intervals estimator ferro and segers 2003 censored estimator holešovský and fusek 2020 k gaps estimator süveges and davison 2010 runs estimator smith and weissman 1994 if threshold min null default threshold is estimated as 90 quantile of smoothing residuals k min a positive integer for residuals with low values giving the number of largest order statistics used to find the moment estimates de haan and ferreira 2006 of shape and scale parameters of gp distribution default is k min round length x 0 1 k max a positive integer for residuals with high values giving the number of largest order statistics used to find the moment estimates de haan and ferreira 2006 of shape and scale parameters of gp distribution default is k max round length x 0 1 extremal index min a numeric value giving the extremal index for the identification of outliers with extremely low value if extremal index max null default the extremal index is estimated using the method specified by the parameter extremal index type extremal index max a numeric value giving the extremal index for the identification of outliers with extremely high value if extremal index max null default the extremal index is estimated using the method specified by the parameter extremal index type extremal index type a character string specifying the type of extremal index estimate possible options are block maxima default for block maxima estimator gomes 1993 intervals for intervals estimator ferro and segers 2003 censored for censored estimator holešovský and fusek 2020 kgaps for k gaps estimator süveges and davison 2010 sliding blocks for sliding blocks estimator northrop 2015 runs for runs estimator smith and weissman 1994 block length min a numeric value for residuals with low values giving the length of blocks for estimation of extremal index only required for extremal index type block maxima and extremal index type sliding blocks default is block length min round sqrt length x block length max a numeric value for residuals with high values giving the length of blocks for estimation of extremal index only required for extremal index type block maxima and extremal index type sliding blocks default is block length max round sqrt length x d min a nonnegative integer for residuals with low values giving the value of d parameter used for censored extremal index estimate holešovský and fusek 2020 only required for extremal index type censored d max a nonnegative integer for residuals with high values giving the value of d parameter used for censored extremal index estimate holešovský and fusek 2020 only required for extremal index type censored k min a nonnegative integer for residuals with low values giving the value of k parameter used for k gaps extremal index estimate süveges and davison 2010 only required for extremal index type kgaps k max a nonnegative integer for residuals with high values giving the value of k parameter used for k gaps extremal index estimate süveges and davison 2010 only required for extremal index type kgaps r min a positive integer for residuals with low values giving the value of runs parameter of runs extremal index estimate smith and weissman 1994 only required for extremal index type runs r max a positive integer for residuals with high values giving the value of runs parameter of runs extremal index estimate smith and weissman 1994 only required for extremal index type runs return period a positive numeric value giving return period default is r 120 which means that observations whose values are exceeded on average once every 120 observations are detected as outliers the crucial thing for the method is the choice of return period r the parameter defining the criterion for outliers detection the outliers with extremely high values are detected as observations whose values are exceeded on average once every r observations likewise the outliers with extremely low values are identified for more information and recommendations see holešovský et al 2018 the output of the method is a krdetect object which contains a list with elements method type a character string giving the type of method used for outlier identification x a numeric vector of observations index a numeric vector of index design points assigned to individual observations smoothed a numeric vector of estimates of the kernel regression function smoothed data outlier min a logical vector specifying the identified outliers with extremely low value true means that the corresponding observation from vector x is detected as an outlier outlier max a logical vector specifying the identified outliers with extremely high value true means that the corresponding observation from vector x is detected as an outlier outlier a logical vector specifying the identified outliers with both extremely low and extremely high value true means that the corresponding observation from vector x is detected as an outlier the description of other output parameters is accessed via command image 19 a 4 plot and summary of outlier detection results the results of all three outlier detection methods can be visualized and summarized using the function plot and summary respectively the argument of those functions is the result obtained using previously described functions krdetect outliers changepoint krdetect outliers controlchart and krdetect outliers ev the description of input parameters is accessed via commands image 20 a 5 kernel smoothing the function for kernel smoothing is in the envoutliers package accessed using the smoothing function the function is structured as follows image 21 the arguments of the function are x a numeric vector of design points of the same length as y default is x c 1 length y y data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo bandwidth type a character string specifying the type of bandwidth possible options are local default to use local bandwidth global to use global bandwidth bandwidth value a local bandwidth array for bandwidth type local or global bandwidth value for bandwidth type global for kernel regression estimation if bandwidth type null default a data adaptive local plug in herrmann 1997 for bandwidth type local or data adaptive global plug in gasser et al 1991 for bandwidth type global bandwidth is used instead kernel order a nonnegative integer giving the order of the optimal kernel gasser et al 1985 used for smoothing possible options are kernel order 2 default kernel order 4 the function computes the estimate of kernel regression function using a local or global data adaptive plug in algorithm and optimal kernels gasser et al 1985 the output of the method is a list with the following elements data smoothed a numeric vector of estimates of the kernel regression function smoothed data residuals a numeric vector of smoothing residuals a 6 mean residual life plot the method for mean residual life plot is in the envoutliers package accessed using the mrl plot function the function is structured as follows image 22 the arguments of the function are x data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo umin the minimum threshold at which the mean residual life function is calculated based on maximum likelihood estimates default is umin quantile na omit x probs 0 8 umax the maximum threshold at which the mean residual life function is calculated based on maximum likelihood estimates default is umax quantile na omit x probs 0 95 kmin the minimum number of largest order statistics for which the mean residual life function is calculated based on moment estimates default is kmin round length na omit x 0 05 kmax the maximum number of largest order statistics for which the mean residual life function is calculated based on moment estimates default is kmax round length na omit x 0 2 nint the number of points at which the mean residual life function is calculated default is nint 100 conf the confidence coefficient for the confidence intervals depicted in the plot default is conf 0 95 est method a character string specifying the type of estimates for the scale and shape parameters of gp distribution possible options are mle default to use maximum likelihood estimates coles 2001 moment to use moment estimates de haan and ferreira 2006 u0 a numeric value giving the threshold meant for a gp approximation of the threshold exceedances default is u0 null k0 a numeric value giving the number k0 1 of largest observations meant for a gp approximation default is k0 null the function constructs mrl plot coles 2001 based on maximum likelihood or moment estimates for parameters of gp distribution if u0 or k0 respectively is given a gp mean threshold dependency line is plotted in addition to the mrl plot coles 2001 each of the lines provide the user an option to assess the suitability of u0 or k0 as a lower bound for the threshold exceedances for u0 or the number of upper order statistics for k0 to fit the gp distribution a 7 stability plots the stability plots of the gp parameter estimates are in the envoutliers package accessed using the stability plot function the function is structured as follows image 23 the arguments of the function are x data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo umin the minimum threshold at which the mean residual life function is calculated default is umin quantile na omit x probs 0 8 umax the maximum threshold at which the mean residual life function is calculated default is umax quantile na omit x probs 0 95 kmin the minimum number of largest order statistics for which the mean residual life function is calculated based on moment estimates default is kmin round length na omit x 0 05 kmax the maximum number of largest order statistics for which the mean residual life function is calculated based on moment estimates default is kmax round length na omit x 0 2 nint the number of points at which the mean residual life function is calculated default is nint 100 conf the confidence coefficient for the confidence intervals depicted in the plot default is conf 0 95 est method a character string specifying the type of estimates for the scale and shape parameters of gp distribution possible options are mle default to use maximum likelihood estimates coles 2001 moment to use moment estimates de haan and ferreira 2006 u0 a numeric value giving the threshold meant for a gp approximation of the threshold exceedances default is u0 null k0 a numeric value giving the number k0 1 of largest observations meant for a gp approximation default is k0 null the function estimates the gp parameters at a range of thresholds in case est method mle or a range of upper order statistics in case of est method moment and shows the sample paths of the estimates if u0 or k0 respectively is given a threshold dependency lines for the particular parameters are plotted in addition the lines provide the user an option to assess the suitability of u0 or k0 as a lower bound for the threshold exceedances for u0 or the number of upper order statistics for k0 to fit the gp distribution 
25559,environmental data often include outliers that may significantly affect further modelling and data analysis although a number of outlier detection methods have been proposed their use is usually complicated by the assumption of the distribution or model of the analyzed data however environmental variables are quite often influenced by many different factors and their distribution is difficult to estimate the envoutliers package has been developed to provide users with a choice of recently presented semi parametric outlier detection methods that do not impose requirements on the distribution of the original data this paper briefly describes the methodology as well as its implementation in the package the application is illustrated on real data examples keywords outlier data validation kernel regression environmental data r package 1 introduction outliers in time series are observations that significantly differ from patterns of the other values in the sequence the common interpretation is that the outliers are drawn by a mechanism that differs from the principles of the underlying series the measurements that appear inconsistent with the rest of the data are often present in large environmental data sets it is known that outlier measurements may result from unexpected behavior of the analyzed variable unusual measurement conditions or from numerous experimental errors because the outliers may significantly affect further analysis and modelling their detection and subsequent interpretation is an important part in environmental data validation and treatment filzmoser 2005 garces and sbarbaro 2011 although large data sets are measured a large amount of data are still managed manually which is time consuming in addition manual data assessment is influenced by the subjective evaluation of the specialist therefore there is a growing need to detect outliers in environmental time series automatically over the years a number of outlier detection methods have been proposed some of which may be found for example in fox 1972 burman and otto 1988 gupta et al 2014 chandola et al 2009 shaadan et al 2015 however most formal outlier detection methods are either limited by the requirements on the distribution of the data or do not consider trends in the data since the data from numerous environmental areas are influenced by many factors which are quite often unknown their distribution cannot be easily estimated this paper presents the r package envoutliers čampulová and čampula 2019 which includes three semi parametric recently presented methods čampulová et al 2017 2018 holešovský et al 2018 for automatic detection of outliers in univariate time series with emphasis on environmental sequences the methods were originally developed for local point wise outliers but they are also suitable on a global scale and mostly also for a small number of collective i e innovational or masking outliers moreover the method of holešovský et al 2018 has been significantly extended to a more general times series the package is available for r r core team 2018 from the comprehensive r archive network cran at http cran r project org package envoutliers all three algorithms are based on non parametric smoothing of the data by removing the variable frequency term and subsequent analysis of the smoothing residuals whereas assumptions on the distribution of the original data are not imposed the automatic detection of outliers is especially useful during the inspection and validation of large data sets where manual quality assessment is very time consuming the below presented approaches may be applied for example as auxiliary techniques providing support in indication of extraordinary observations the attention of a quality controller may afterwards be paid solely to such automatically identified outliers and does not need to focus on the entire data set moreover all the data are evaluated based on the same criterion that is not influenced by subjective assessment besides the time savings this is another advantage of automatic outlier detection several r packages for point outlier detection are available and are briefly described in the following paragraphs the outliers package komsta 2011 provides commonly used statistical tests and basic approaches for the detection and treatment of outliers the tsoutliers package de lacalle 2019 implements a procedure for automatic detection of outliers in arima time series models the implemented algorithm is presented in chen and liu 1993 and considers several types of outliers such as innovational outliers additive outliers level shifts temporary changes and seasonal level shifts the anomaly package fisch et al 2020 implements several procedures based on parametric models the collective and point anomaly detection approach fisch et al 2019a multi variate collective and point anomaly detection fisch et al 2019b proportion adaptive segment selection jeng et al 2013 and the bayesian abnormal region detector bardwell and fearnhead 2017 the anomalize package dancho and vaughan 2019 is based on the decomposition of a time series by loess or by piecewise medians in order to remove trend and seasonal components subsequently outliers anomalies are detected on remainders using the inter quartile range iqr or generalized extreme studentized deviate gesd test however the iqr approach may not be as accurate in detecting anomalies since high leverage anomalies may skew the centerline of the iqr the disadvantage of using the gesd test is the assumption of normality the outlierdetection package tiwari and kashikar 2019 implements several procedures such as the algorithm suggested in barnett and lewis 1998 for outlier detection in data following weibull distribution the distance based outlier detection procedure presented in hautamaki et al 2004 dispersion based outlier detection ben gal 2005 depth based outlier detection in a 2 d dataset following some definition of depth johnson et al 1998 and density based outlier detection for spatial data ester et al 1996 the otsad package iturria et al 2019 implements several online outlier detection procedures for time series these are based either on the probabilistic exponentially weighted moving average pewma method proposed in carter and streilein 2012 and its variations raza et al 2015 or on extensions to the k nearest neighbor algorithm ishimtsev et al 2017 laxhammar and falkman 2015 burnaev and ishimtsev 2016 as the pewma method or similar is suggested mostly for stationary data the outlier detection approaches in the first case have very low adaptability to the abrupt changes in data trends or variability the disadvantage of the latter is the need for a reference sample therefore the methodology is not directly applicable and the model is dependent on the quality of the training set the methods implemented in the envoutliers package presented in this paper are capable of considering trends in the time series data and are suitable for non stationary sequences with variable means at the same time the methods have no requirements on the specification of the distribution or a model of the original data and no training set is required which are the main advantages over the other existing packages since kernel smoothing based on local bandwidth is performed in the first stage the implemented algorithms are adaptable to abrupt changes in data trends and or variability the user is able to omit the initial smoothing moreover the approaches implemented in the envoutliers package do not rely on any heuristic arguments but make use of significant theoretical results of recently developed areas such as the kernel regression extreme value theory or changepoint analysis the remainder of the paper is structured as follows section 2 briefly describes the outlier detection methods and their accessibility from the envoutliers package the performance of the functions using real data examples is illustrated in section 3 and a summary of the paper is given in section 4 specific details on the envoutliers package are given in the appendix 2 outlier detection methods and the envoutliers package in this section we briefly introduce the methods for the identification of outliers presented in čampulová et al 2018 čampulová et al 2017 and holešovský et al 2018 and we put into context the procedures implemented within the envoutliers package as already mentioned the main aim of all three methods is to smooth the original data and subsequently analyze the residuals the smoothing is performed using kernel regression with a local bandwidth herrmann 1997 the local bandwidth adapts to the structure of the data and the curvature of the resulting smoothing line changes with respect to the variability of the data therefore regression with the local bandwidth describes fast changes in data variability in more detail than regression with a global bandwidth thereby it follows that the methods are generally suitable for global as well as for local point wise outliers i e observations that are outlying in a certain context or time period possible extensions to other types of outliers differs with respect to a particular method that is applied in the subsequent analysis although not intended to the methods discussed below except for those in section 2 1 should also be able to identify a relatively small number of collective or masking outliers assume we have the observations y 1 y n measured at n time instants t 1 t 2 t n lying in interval a b the heteroscedastic regression model is given by 1 y i m t i σ t i ϵ i i 1 n where m is an unknown regression function ϵ i are i i d random variables with zero mean and unit variance and σ t i is the standard deviation function describing the variability of y i the regression function m is estimated using the gasser müller estimate gasser and müller 1984 in the form 2 m t h t i 1 n y i l i 1 l i 1 h t k t u h t d u for t a h t b h t where k is a kernel of some order k and h t is a bandwidth at point t the limits of the integral are given by l 0 a l i 0 5 t i t i 1 for i 1 2 n 1 and l n b although kernel regression with a local bandwidth herrmann 1997 gives relatively good results compared to the other methods frequently used for environmental data smoothing čampulová 2018 in the outlier detection functions from the envoutliers package the user may also choose kernel smoothing based on a global bandwidth gasser et al 1991 for both local and global bandwidths the optimal kernels gasser et al 1985 of the order k 2 or k 4 are used respectively in the second step the smoothing residuals x i y i m t i i 1 n are analyzed for the presence of outliers the three methods implemented in the envoutliers package make use of different principles an overview of which is given below each of the methods requires at the input one or more auxiliary parameters determining the criterion for outlier detection naturally the choice of these parameters controlling the number of detected outliers is important and should be made with regards to the specific application requirements and empirical experience with historical data since the true reason for the presence of identified outliers cannot be clarified automatically the quality of the detected outliers must be further evaluated by a specialist who considers information about unusual measurement conditions therefore the identification of false outliers observations that were incorrectly identified as outliers is not regarded as undesirable 2 1 method based on changepoint analysis suppose that the residuals are independent observations with cumulative distribution functions f 1 f n the aim is to detect changes in the distribution of the residuals together with an unknown number of change points τ 1 τ m such that 1 τ 1 τ m n and f 1 f τ 1 f τ 1 1 f τ 2 f τ 2 1 f τ m f τ m 1 f n the binary segmentation method proposed by scott and knott 1974 is probably the most widely used changepoint search method applications of this or a modified approach may be found in different fields see e g olshen et al 2004 neubauer and veselý 2013 fryzlewicz 2014 raveendran and sofronov 2017 binary segmentation is an approximate algorithm but it is computationally fast currently algorithms are available that are exact and efficient in terms of computing time therefore the pelt algorithm killick et al 2012 based on a parametric model or the non parametric e divisive method matteson and james 2014 is preferred although the running time of the e divisive method is quadratic in the length of the analyzed data it may be effectively used for data of a length that is commonly used in environmental applications the advantage of this method is that it generates consistent changepoint estimates the detected estimates of changepoints τ 1 τ m partition the residuals into m 1 homogeneous segments s j x τ j 1 1 x τ j j 1 m 1 where the outlier residuals are going to be identified in case that the residuals in homogeneous segments are approximately normally distributed or may be transformed to normality using the box cox transformation box and cox 1964 the outlier residuals are detected using the grubbs test grubbs 1950 or quantiles of normal distribution otherwise the chebyshev inequality based approach is applied the latter identifies outlier residuals such that their values exceed the limits ls j where s j denotes the sample standard deviation of the residuals on the given j th segment and l is a suitably chosen constant residual analysis based on normal quantiles determines l equal to the α 2 quantile u α 2 of the standard normal distribution for 0 α 1 the principle of using the grubbs test lies in consecutive assessment of the most deviated residual i e the residual with the largest absolute value in the event that the deviation in a particular step is indicated as being significant by the grubbs test the corresponding value is labelled as an outlier and excluded from further analysis the procedure is then repeated until no significant difference is detected a significant weakness of the grubbs test is that it is particularly unsuitable for detection of masking outliers the approach for the residual analysis on homogeneous segments within the envoutliers package is selected automatically based on normality results by default the residuals on homogeneous segments are tested for normality using the robust medcouple test mc lr brys et al 2008 in case that the normality is rejected the box cox transformation is applied and the mc lr test is performed on the transformed residuals if raw or the transformed residuals are normally distributed the grubbs test and the normal quantiles based approach are performed on either raw or transformed residuals in other cases the approach based on chebyshev inequality is preferred the changepoint analysis is implemented in the krdetect outliers changepoint function within the envoutliers package depending on the input arguments the user may decide for example whether the kernel smoothing or the changepoint based segmentation should be performed or not choose what type of bandwidth should be considered as well select particular values or which outlier detection method to use among other things see the appendix for more details the method in the envoutliers package is parameterized by a l default parameter for residual analysis based on chebyshev inequality and by an alpha default parameter for residual analysis based on quantiles of normal distribution these parameters which determine the criteria for detection of outlier residuals may be found using the data driven algorithm a1 for l default and modified algorithm a1 for alpha default suggested in čampulová et al 2018 the results of the changepoint analysis as well as of other methods discussed below may be visualized and summarized using the plot and summary functions the function for kernel smoothing in the envoutliers package is accessed using the smoothing function 2 2 method based on control charts the method based on control charts shewhart 1931 assumes that the residuals x 1 x n represent a process with the mean μ and standard deviation σ x the aim is to evaluate the statistical stability of the residuals for which the unbiased estimates of μ and σ x are needed to obtain these estimates the residuals are partitioned into disjointed segments i e subgroups of size n the parameters μ and σ x for the whole residual process x 1 x n are estimated on the basis of sample means sample standard deviations and sample ranges at individual segments as described in e g čampulová et al 2017 subsequently three types of control charts x chart s chart and r chart are used to assess the statistical stability of the residuals and to identify the segments of size n where the outliers occur changes in the mean may be detected using the x chart changes in the variance are identified by the control charts r and s control charts graphically visualize changes in residual characteristics over time together with two horizontal lines representing the upper and lower control limits the limits for the x chart the r chart and the s chart are consecutively in the form 3 l c l x l x σ s n u c l x l x σ s n 4 l c l r max 0 r l r d 3 n σ r u c l r r l r d 3 n σ r 5 l c l s max 0 s l s σ s s 2 u c l s s l s σ s s 2 where x is the sample mean evaluated from the entire residual process and r s are the means of sample ranges and sample standard deviations respectively obtained at disjoint segments the parameters σ s σ r are suitable estimates of σ x see wild and seber 2000 for more details the term d 3 n is a correction factor proposed by cary 1999 the three auxiliary parameters l x l r l s are of a similar nature they govern the width of particular control limits and hence determine the outlier detection criterion the residual process is considered to be statistically stable if the estimates of particular characteristics at disjoint segments lie between the corresponding control limits lcl and ucl determined by 3 4 or 5 respectively in this case the process is considered to be stable and no outliers are detected otherwise the segments of observations corresponding to sample means sample standard deviations or sample ranges exceeding corresponding control limits are considered to contain outliers and are detected as such hence the application of a method based on control charts does not lead to the identification of particular outliers but rather to the identification of observation segments where the outliers occur more details of the method may also be found in čampulová et al 2017 together with recommendations for the optimal size of segments within the envoutliers package there are charts that are implemented in the krdetect outliers controlchart function the charts are created for the smoothing residuals under the default settings of kernel regression however the user is also offered the option to adjust the smoothing conditions as well as to completely disable the kernel regression different sizes of subgroups for each of the three control charts may be chosen by setting different values of corresponding parameters see the appendix for more details 2 3 method based on extreme value theory additive residuals may also be analyzed on the basis of extreme value ev theory coles 2001 and its generalization for dependent variables leadbetter et al 1983 beirlant et al 2004 the core idea is to detect the outlier in such a way that the corresponding residuals exceed an extremely high level hence an outlier is understood as an extraordinary observation with only negligible probability of being obtained in terms of ev theory such a high value is commonly referred to as a r return level for some large integer r i e a value exceeded once every r observations on average put simply the r return level is the 1 r 1 quantile of the residual model distribution suppose that the residuals x 1 x n is a random sequence with an unknown distribution in practical situations the extreme values are usually estimated on the basis of the peaks over threshold pot approach holešovský et al 2015 fawcett and walshaw 2012 scarrott and macdonald 2012 in the pot model the residuals exceeding a given threshold u large enough are analyzed it may be derived that the threshold exceedances may be modelled by the generalized pareto gp distribution the outlier detection method from holešovský et al 2018 uses originally maximum likelihood ml estimates of the gp parameters the ml estimates are however suitable only under certain regularity conditions smith 1985 zhou 2009 therefore one of the extensions of the approach of holešovský et al 2018 to a more general case that we propose in this paper is made by incorporating semi parametric moment estimators of gp parameters de haan and ferreira 2006 to the envoutliers package the moment estimators have many desirable properties and belong to the most commonly applied gp estimators holešovský et al 2015 draisma et al 1999 note that in the semi parametric context the threshold exceedances correspond to a set of k largest order statistics denoted x n x n 1 x n k 1 hence the issue of a suitable threshold choice now corresponds to the issue of the proper selection of k a proper threshold selection say u 0 is one of the main issues of the pot model affecting the estimation quality some approaches for choosing a suitable threshold value are given in e g coles 2001 or scarrott and macdonald 2012 which include traditional graphical diagnostics as well as more recently developed adaptive methods in practical situations the threshold u 0 is often set to a high data quantile with the aim of bringing automation to the outlier detection procedure this approach is also applied in the envoutliers package whereby the threshold value is selected as a 90 quantile of smoothing residuals by default in environmental or economical series it is common that the dependence of the observations is mostly present in the form of a short time dependence whereby distant observations are approximately independent the measure of this short time dependence at extreme levels may be described by a parameter θ a so called extremal index several extremal index estimators have been proposed in the literature the methodology in holešovský et al 2018 uses the block maxima estimator of gomes 1993 by default in envoutliers the modified maxima estimator of ancona navarrete and tawn 2000 and the sliding blocks estimator northrop 2015 are implemented with the envoutliers package in addition an estimator based on threshold inter exceedance times i e the runs estimator smith and weissman 1994 the intervals estimator ferro and segers 2003 the k gaps estimator süveges 2007 süveges and davison 2010 and the recently proposed censored estimator holešovský and fusek 2020 with many desirable properties are also included note that each of the particular estimators depends on one or more auxiliary parameters some of them are preset as default for more details see the corresponding papers finally the r observation return level z r max is estimated using the relation 6 z r m a x u σ ξ λ u 1 1 1 r 1 θ 1 ξ 1 where the parameter λ u and the gp parameters ξ σ should be replaced with their corresponding estimates see holešovský et al 2018 2015 for more details in the case of the semi parametric moment gp estimators the return level z r max is obtained again from relation 6 whereby u and λ u should be replaced by their non parametric equivalents i e the n k th largest residual x n k and the fraction k n similarly the return level z r min for extremely low values is constructed from the negative residuals given the return levels z r min and z r max for residuals the observations exceeding these limits are identified as outliers more details on the method as well as recommendations for the optimal values of the parameters may be found in holešovský et al 2018 the envoutliers package implements the ev methodology in the krdetect outliers ev function the user may again control the smoothing conditions but also select different methods and auxiliary parameters intended for the ev analysis the package also includes the mrl plot and stability plot functions providing graphical diagnostic tools for the threshold selection following scarrott and macdonald 2012 the parameter r is implemented as return period in the envoutliers package 3 application and illustration this section explores the use of the functions introduced in the previous section on selected data sets available in r package openair carslaw and ropkins 2012 the data sets contain hourly measurements of wind speed wind direction and concentrations of no x no2 o3 pm10 so2 co and pm25 observed at marylebone london between january 1 1998 and june 23 2005 for this illustrative example we consider one part of the data namely the o3 concentrations measured in december 2002 the selected data includes a total of 744 observations and we begin by its loading and selecting it based on the date image 1 the data contain several missing values however this is not a limitation for the use of outlier detection methods presented in the foregoing section fig 1 visualizes the o3 concentrations in parts per billion ppb measured in the period of interest it is evident that several values deviating from the neighboring measurements are present in the data this motivates a search for the outliers 3 1 outlier detection based on changepoint analysis firstly we demonstrate the detection of outliers using the algorithm based on changepoint analysis and the automatic selection of detection method for residual analysis image 2 from the summary of the results we are able to see that the approach based on the grubbs test was automatically selected for the analysis which follows from the fact that the normality of the residuals was not rejected and the box cox transformation was not necessary the method identified four outliers 0 53763 of the total data visualized in fig 2 a the figure also shows that the non parametric changepoint analysis leads to detection of two changes in the residual variability that partition the analyzed residuals into three homogeneous segments visualized by dashed vertical lines next the outliers are detected using the approach based on normal quantiles the alpha default parameter determining the criterion for the detection of outliers is chosen automatically using the modified algorithm a1 čampulová et al 2018 as given by default the previous example showed that residuals on individual segments may be considered approximately normally distributed therefore the parametric changepoint analysis set by default may be used image 3 as it may be seen from the summary of the results and from the plot visualized in fig 2b ten outliers were detected in total notice that two outliers close to each other were identified which results in their overlapping in the graph these are the observations with indices 607 and 610 corresponding to time instants 6 00 a m and 9 00 a m on december 26 and both taking the value of 13 ppb the changepoints identified using the non parametric analysis visualized by dashed vertical lines do not differ significantly from the changepoints estimated using the parametric approach this may be considered as evidence of the normality of the segmented residuals to see the values of the alpha default parameter used for detection of outlier residuals on individual segments the following command is used image 4 the particular values of alpha default parameter on individual segments were found using the modified algorithm a1 on the first two segments outliers were detected based on alpha default 0 0005 while on the third segment alpha default 0 0085 was used finally outliers are detected using chebyshev inequality and the same value of the l default parameter on the individual segments of course analogous to the previous example the values of the l default parameter determining the criterion for the detection of outliers on individual segments may be chosen automatically using algorithm a1 čampulová et al 2018 image 5 a summary of the results reveals that nine outliers were found the plot in fig 2c shows the detected outliers together with detected changepoints visualized by dashed vertical lines it is important to remember that the choice of the appropriate value of the alpha default parameter and l default parameter is an open question and depends on many factors including the character of the data or the requirements of the analysts 3 2 outlier detection based on control charts recall from section 2 2 that the algorithm based on control charts is parameterized by parameters l x l s and l r used for the computation limits of the x chart s chart and r chart and whereby determining the criterion for the detection of outliers čampulová et al 2017 firstly the parameters are set as l x 3 l s 3 and l r 3 which is given by default image 6 performing the analysis using the krdetect outliers controlchart function produces warning messages which inform the user that the first two data values are excluded from the control chart analysis this data exclusion if necessary may be avoided by a more appropriate selection of subgroup size ensuring that the data except the missing values may be equidistantly partitioned into subgroups as it may be seen from the summary of the results no outlier was detected based on control chart x however a relatively large number of data values 39 in total were identified as outliers based on control charts s and r this may be explained by the fact that smoothing residuals are randomly distributed around a zero value outlying o3 concentrations correspond to changes in the variability of the residuals that are detected using control charts s and r while control chart x controls the changes in the mean value it is also important to remember that the method based on control charts does not identify individual outliers but detects the segments of observations where the residual process gets out of control due to the presence of outliers therefore it is predictable that the number of outliers detected using krdetect outliers controlchart function is relatively large all the detected outliers may be plotted using the following command the corresponding graph is shown in fig 2d to plot only the outliers detected based on the x chart r chart or s chart the plot type parameter is set to plot type x plot type r and plot type s respectively image 7 to obtain a less strict criterion for detection of the outliers the parameters are set to l s 5 and l r 5 clearly increasing the values of parameters l s and l r leads to a reduction in the number of detected outliers this is also evident from fig 2 or from the following summary of the results image 8 3 2 1 outlier detection based on ev theory the algorithm implemented in the krdetect outliers ev function enables default selection of all necessary parameters which makes the procedure easy to use however the user has the option to set all the parameters based on his or her requirements on the data control which we illustrate here firstly we detect outliers based on ev theory with maximum likelihood estimates of gp distribution parameters as mentioned in section 2 3 for maximum likelihood estimates based on the pot model a threshold value is needed although the algorithm implemented in the krdetect outliers ev function estimates the threshold as a 90 quantile of smoothing residuals by default the threshold may be selected based on an mrl plot and or parameter stability plots since the mrl plot and stability plot functions construct the plots directly from the input data smoothing residuals need to be extracted prior to the constructions of the plots the following command is used for this image 9 subsequently the mrl plot and the stability plots for the maximum values of the residuals together with the dependency lines are constructed the dependency lines visualize the conformity of the estimated paths with the theoretical threshold dependencies for thresholds above the chosen u0 see scarrott and macdonald 2012 for more details here we select u0 0 75 and plot the lines within the range umin and umax corresponding to 80 and 95 of the sample quantiles respectively image 10 the plots for the minimum values of the residuals are constructed in the same way with the residuals replaced by their negatives here we select the bounds corresponding to 80 and 98 of the sample quantile of the negative residuals for the dependency lines we choose u0 0 95 image 11 mrl and stability plots for maximum values of the residuals are shown in figs 3 and 4 while the plots for minimum values of the residuals are given in figs 5 and 6 it should be taken into account that both kinds of plots provide graphical diagnostics that may lead to subjective conclusions nevertheless they give some support for initial statistical inference in case an appropriate threshold is unknown in both the above instances is the choice of u0 is made based on a compromise among the all figures there are no significant deviations of the sample paths from the dependency for threshold larger than u0 the selected thresholds u0 correspond to 92 8 and 95 3 of the sample quantile of the residuals and negative residuals respectively this is also consistent with the theoretical requirement that the threshold should be sufficiently large hence the value u0 is considered suitable for further approximation of the threshold exceedances by the gp distribution in both cases finally the outliers based on the selected thresholds may be found however except for the threshold the estimate of extremal index is needed although the function krdetect outliers ev function estimates the extremal index based on the block maxima approach gomes 1993 by default here we prefer the intervals estimator also the return period parameter determining the r observation return level needs to be chosen in analogy to the previous examples the choice of return period is an open question and depends on the character of the data as well as on the assumptions of the data analysis firstly we select return period 24 5 120 as given by default this means that only o3 concentrations corresponding to residuals whose values are exceeded on average once every five days will be sought image 12 as it may be seen from the summary of the results 19 outliers were detected in total seven of the outliers correspond to smoothing residuals with extremely low values while twelve correspond to smoothing residuals with extremely high values all the detected outliers may be plotted using the following command which produces the graph in fig 7 a to plot only the outliers corresponding to smoothing residuals with extremely low and high values the plot type parameter is set to plot type min and plot type max respectively image 13 similarly to the inference based on changepoint analysis using normal quantiles see fig 2b the method identified outliers at the two observations with indices 607 and 610 which overlap in the graph for illustrative purposes the outliers are detected also using moment estimates of the gp distribution parameters recall that for moment estimates the values of the k min and k max parameters giving the numbers of the largest order statistics used to find the moment estimates for residuals with low and high values are needed these values may be selected again based on mrl and stability plots or they may be set by default by the algorithm which we prefer here to estimate the extremal index parameter the sliding block estimator northrop 2015 is used and the return period is set as return period 24 10 240 image 14 in this case only eight outliers were detected again including the close observations with indices 607 and 610 overlapping in the graph the results are shown in fig 7b 3 3 remark for outlier detection approaches the data considered for illustration of the methods in section 3 contained a trend however in cases where the data are randomly dispersed around a zero value performing data smoothing may not be necessary and residual analysis may be applied directly to the raw data in these situations the user may specify for each of the three presented outlier detection functions that no smoothing is performed by setting the perform smoothing false parameter similarly for homogeneous residuals and the changepoint based method changepoint analysis may be omitted from the analysis by setting the perform cp analysis false parameter 3 4 comparison of presented detection methods it is evident from figs 2 and 7 that the results differ according to particular method meant for the analysis of residuals moreover in the specific cases the number of identified outliers also depends on the auxiliary parameters that determine the criterion for outlier detection we may see that the smallest number of outliers was found using the changepoint analysis based on the grubbs test notice that this approach is the least variable one of all the methods implemented in the envoutliers package this is convenient as the user may use it without extensive testing of any auxiliary parameters the changepoint inference based on normal quantiles or the chebyshev inequality results among others in the identification of the same outliers as those determined by the grubbs test nevertheless the methods differ significantly in the outliers not captured by the grubbs test this is a consequence of various approaches for setting the parameters alpha and l defining the criterion for detection of outliers within the changepoint based method overall the main advantage of the changepoint analysis is its simplicity and clarity making it suitable for a common user in any modification be it the grubbs test normal quantiles or the chebyshev inequality moreover the grubbs test is a classical and widely used method for outlier identification under the assumption of normal distribution the results for the corresponding values of auxiliary parameters are comparable on the other hand violation of the normality or of the distribution symmetry may lead to significant distortion of the results these methods are also mostly inappropriate for collective or masking outliers as it is typical for the grubbs test the largest number of outliers was found using the control charts with parameters l x l s l r 3 however setting the parameters to l x l s l r 5 i e under a milder criterion for the outlier detection the results are comparable to those obtained using the methodology based on ev theory with return period 240 a relatively large number of outliers were detected using ev theory with return period 120 this confirms the fact that the choice of the parameters defining the criterion for outlier detection is essential the approach based on ev theory makes use of complex mathematical theory which may be beneficial for some users the outlier detection limits are evaluated independently unlike the changepoint analysis based on normal quantiles or the chebyshev inequality which makes the methodology more suitable if the distribution of residuals is markedly asymmetrical moreover the use of this approach is supported by theoretical aspects making it more appropriate for application on data with non negligible dependence on the other hand a suitable choice of the auxiliary parameters is often unclear and requires thorough analysis some improvements on the current procedures may be considered the first one concerns the changepoint analysis namely the outlier identification on homogeneous segments the step accounting for the normal quantiles or the chebyshev inequality should be generalized in order to make the method suitable also for asymmetrical distributions of the residuals some non parametric quantile estimators may be employed here an improvement to the method based on ev theory may be also made in its current form the return levels are evaluated on the basis of rare events including the outliers however it is known that the presence of the outliers may lead to significant bias of any estimates capturing the dependence structure in return level estimates may lead to more appropriate assessment of the outliers 4 summary the r package envoutliers with three recently published semi parametric outlier detection methods is presented the main aim of each of the three methods is to smooth original time series data and subsequently analyze the smoothing residuals the first method analyzes the residuals using changepoint analysis and classical approaches for outlier detection the second method evaluates the residuals using control charts and the last method is based on extreme value theory in the latter case other advanced estimates of the extreme values have been incorporated into the package this now makes the original methodology of holešovský et al 2018 newly applicable to a wider class of sequences the methodology of each of the three methods for detection of outliers is briefly described and the syntax is presented the use of the presented functions does not require assumptions on the distribution of the original data which is an advantage especially for data from environmental areas whose distribution cannot be easily estimated the use of the envoutliers package is illustrated on real data of o3 concentrations since outliers may result from various experimental errors the true reason for their presence in the data cannot be determined by any of the methods implemented in the envoutliers package in practice this means that the quality of the automatically detected outliers needs to be further investigated by a specialized operator who determines how to deal with the detected outliers a correct measurement that appears to be outlying from the rest of the data carries useful information about rare behavior of the studied variable therefore the observations identified as outliers may be removed retained or revised the unique contribution of the package is that the number of observations for manual data evaluation is reduced this is extremely helpful for users who are responsible for evaluating the quality of data measured continuously with high temporal resolution using the functions within the package users may study only the automatically detected outliers and do not have to control the entire data set this saves a great deal of time since larger data sets require more time for data evaluation and validation is also needed moreover the analysis of the data is based on the same criterion for all observations and is not influenced by the subjective judgement of a specialized operator 5 computational details the results in this paper were obtained using r 3 4 1 with the lokern 1 1 8 package changepoint 2 2 2 package ismev 1 42 package stats 0 1 0 package mass 7 3 51 4 package ecp 3 1 1 package car 3 0 3 package robustbase 0 93 5 package and openair 2 6 5 package r itself and all the packages used are available from the comprehensive r archive network cran at https cran r project org declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this article was produced with the financial support of the ministry of transport of the czech republic within the programme of long term conceptual development of research institutions and with the support of the specific research project no fast s 22 7867 at brno university of technology a structure of the envoutliers package in this section we describe the syntax and structure of the functions implemented in the envoutliers package within each outlier detection function smoothing of the series is provided in the first step which is based on local bandwidth and kernel of order 2 subsequently smoothing residuals are inspected for the presence of outliers however the user may select either a local or the global bandwidth the order of the optimal kernel used for smoothing or choose if smoothing should be performed or not with that in mind notice that in the following section the code symbol x stands either for the underlying time series y 1 y n or for the sequence of residuals x 1 x n depending on the user s settings this applies with exception to the mrl plot and the stability plots which are conducted directly on the input data hence the smoothing must be performed in advance in order to perform the plots on the residuals for this reason the envoutliers package also includes a function for separate kernel smoothing in the event that the smoothing is not disabled and the bandwidth is not given by the input a data adaptive local plug in herrmann 1997 for local smoothing or data adaptive global plug in gasser et al 1991 for global smoothing bandwidth is used both bandwidth estimation and kernel smoothing are performed via the lokerns and glkerns functions from the lokern package herrmann and maechler 2013 a 1 method based on changepoint analysis the method for outlier detection based on changepoint analysis is accessed within the envoutliers package using the krdetect outliers changepoint function for a parametric changepoint analysis the cpt var and cpts functions from the changepoint package killick et al 2015 are employed non parametric changepoint analysis is performed via the e divisive function from the ecp package james and matteson 2014 and boxcox from the mass package venables and ripley 2002 is used for box cox transformation of the residuals function the tests performed within the function are performed via the levenetest function from the car package fox and weisberg 2011 and the mc function from the robustbase package todorov and filzmoser 2009 the function is structured as follows image 15 the arguments of this function are x data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo perform smoothing a logical value specifying if data smoothing is performed if true default data are smoothed perform cp analysis a logical value specifying if changepoint analysis is performed if true default smoothing residuals are partitioned into homogeneous segments bandwidth type a character string specifying the type of bandwidth possible options are local default to use local bandwidth global to use global bandwidth bandwidth value a local bandwidth array for bandwidth type local or global bandwidth value for bandwidth type global for kernel regression estimation if bandwidth value null default a data adaptive local plug in herrmann 1997 for bandwidth type local or data adaptive global plug in gasser et al 1991 for bandwidth type global bandwidth is used instead kernel order a nonnegative integer giving the order of the optimal kernel gasser et al 1985 used for smoothing possible options are kernel order 2 default kernel order 4 cp analysis type a character string specifying the type of changepoint analysis possible options are parametric default to perform changepoint analysis using pelt algorithm killick et al 2012 nonparametric to perform a nonparametric approach for multiple changepoints matteson and james 2014 pen value a character string giving the formula for manual penalty used in pelt algorithm only required for cp analysis type parametric default is pen value 5 log n alpha edivisive a numeric value giving the moment index used for determining the distance between and within segments in non parametric changepoint model default is alpha edivisive 0 3 min segment length a numeric value giving the minimal required number of observations on segments from changepoint analysis if a segment contains less than the min segment length observations and the variances of data on the segment and the previous one are supposed to be equal based on levene s test fox 2016 for the homogeneity of variances the segment is merged with the previous one analogous the first segment can be merged with the second one default is min segment length 30 segment length for merge a numeric value giving the minimal required number of observations on segments for performing the homogeneity test within changepoint split control a segment with less data than segment length for merge is merged with the previous one without testing the homogeneity of variances the first segment is merged with the second one default is min segment length for merge 15 method a character string specifying the method for identification of outlier residuals possible options are auto default for automatic selection based on the structure of the residuals grubbs test for grubbs test normal distribution for quantiles of normal distribution chebyshev inequality for chebyshev inequality prefer grubbs a logical variable specifing if grubbs test for identification of outlier residuals is preferred to quantiles of normal distribution true default means that grubbs test is preferred only required for method auto alpha default a numeric value from interval 0 1 of the parameter α determining the criterion for residual outlier detection the limits for outlier residuals on individual segments are set as α 2 quantile of normal distribution with parameters corresponding to residuals on studied segment sample standard deviation of residuals on corresponding segment if alpha default null default its value on individual segments is estimated using the modified algorithm a1 čampulová et al 2018 l default a numeric value of l parameter determining the criterion for outlier residual detection the limits for outlier residuals on individual segments are set as l sample standard deviation of residuals on corresponding segment if l default null default its value on individual segments is estimated using the algorithm a1 čampulová et al 2018 the choice of parameters alpha and l that define the criterion for outlier detection is crucial for the method these values can be specified by the user or estimated automatically using data driven algorithms čampulová et al 2018 the output of the method is a krdetect object which contains a list with elements method type a character string giving the type of method used for outlier identification x a numeric vector of observations index a numeric vector of index design points assigned to individual observations smoothed a numeric vector of estimates of the kernel regression function smoothed data changepoints an integer membership vector for individual segments normality results a data frame of normality results of residuals on individual segments detection method a character string giving the type of method used for identification of outlier residuals alpha a numeric vector of α parameters used for outlier identification on individual segments l a numeric vector of l parameters used for outlier identification on individual segments outlier a logical vector specifying the identified outliers true means that the corresponding observation from vector x is detected as an outlier a 2 method based on control charts the method for outlier detection based on control charts is in the envoutliers package accessed using the krdetect outliers controlchart function the function is structured as follows image 16 the arguments of the function are x data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo perform smoothing a logical value specifying if data smoothing is performed if true default data are smoothed bandwidth type a character string specifying the type of bandwidth possible options are local default to use local bandwidth global to use global bandwidth bandwidth value a local bandwidth array for bandwidth type local or global bandwidth value for bandwidth type global for kernel regression estimation if bandwidth value null default a data adaptive local plug in herrmann 1997 for bandwidth type local or data adaptive global plug in gasser et al 1991 for bandwidth type global bandwidth is used instead kernel order a nonnegative integer giving the order of the optimal kernel gasser et al 1985 used for smoothing possible options are kernel order 2 default kernel order 4 method a character string specifying the preferred estimate of the standard deviation parameter see čampulová et al 2017 possible options are range default for estimation based on sample ranges sd for estimation based on sample standard deviations group size x a positive integer giving the number of observations in individual segments used for computation of x chart control limits if the data cannot be equidistantly divided the first extra values will be excluded from the analysis default is group size x 3 group size r a positive integer giving the number of observations in individual segments used for computation of r chart control limits if the data cannot be equidistantly divided the first extra values will be excluded from the analysis default is group size r 3 group size s a positive integer giving the number of observations in individual segments used for computation of s chart control limits if the data cannot be equidistantly divided the first extra values will be excluded from the analysis default is group size s 3 l x a positive numeric value giving parameter l specifying the width of x chart control limits default is l x 3 l r a positive numeric value giving parameter l specifying the width of r chart control limits default is l r 3 l s a positive numeric value giving parameter l specifying the width of s chart control limits default is l s 3 the choice of parameters group size x group size s group size r l x l r and l s specifying the width of control limits is crucial for the method especially different values of parameters l x l r and l s determine different criteria for outlier detection and can significantly influence the results for more information and recommendations see čampulová et al 2017 the output of the method is a krdetect object which contains a list with elements method type a character string giving the type of method used for outlier identification x a numeric vector of observations index a numeric vector of index design points assigned to individual observations smoothed a numeric vector of estimates of the kernel regression function smoothed data outlier x a logical vector specifying the identified outliers based on the limits of control chart x true means that the corresponding observation from vector x is detected as an outlier outlier r a logical vector specifying the identified outliers based on the limits of control chart r true means that the corresponding observation from vector x is detected as an outlier outlier s a logical vector specifying the identified outliers based on the limits of control chart s true means that the corresponding observation from vector x is detected as an outlier outlier a logical vector specifying the identified outliers based on at least one type of control limits true means that the corresponding observation from vector x is detected as an outlier the description of other output parameters is accessed via command image 17 a 3 method based on extreme value theory the method for outlier detection based on extreme value theory is in the envoutliers package accessed using the krdetect outliers ev function the fit of smoothing residuals to gp distribution is done via the function gpd fit from the ismev package heffernan and stephenson 2016 for the estimation of the extremal index based on block maxima approach gomes 1993 the function gev fit from the same package is used the function is structured as follows image 18 the arguments of the function are x data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo perform smoothing a logical value specifying if data smoothing is performed if true default data are smoothed bandwidth type a character string specifying the type of bandwidth possible options are local default to use local bandwidth global to use global bandwidth bandwidth value a local bandwidth array for bandwidth type local or global bandwidth value for bandwidth type global for kernel regression estimation if bandwidth value null default a data adaptive local plug in herrmann 1997 for bandwidth type local or data adaptive global plug in gasser et al 1991 for bandwidth type global bandwidth is used instead kernel order a nonnegative integer giving the order of the optimal kernel gasser et al 1985 used for smoothing possible options are kernel order 2 default kernel order 4 gpd fit method a character string specifying the method used for the estimate of the scale and shape parameters of gp distribution possible options are mle default for maximum likelihood estimates coles 2001 moment for moment estimates de haan and ferreira 2006 threshold min a threshold value for residuals with low values that is used to find the maximum likelihood estimates of shape and scale parameters of gp distribution and selected types of extremal index estimates specifically intervals estimator ferro and segers 2003 censored estimator holešovský and fusek 2020 k gaps estimator süveges and davison 2010 runs estimator smith and weissman 1994 if threshold min null default threshold is estimated as 90 quantile of negated smoothing residuals threshold max a threshold value for residuals with high values that is used to find the maximum likelihood estimates of shape and scale parameters of gp distribution and selected types of extremal index estimates specifically intervals estimator ferro and segers 2003 censored estimator holešovský and fusek 2020 k gaps estimator süveges and davison 2010 runs estimator smith and weissman 1994 if threshold min null default threshold is estimated as 90 quantile of smoothing residuals k min a positive integer for residuals with low values giving the number of largest order statistics used to find the moment estimates de haan and ferreira 2006 of shape and scale parameters of gp distribution default is k min round length x 0 1 k max a positive integer for residuals with high values giving the number of largest order statistics used to find the moment estimates de haan and ferreira 2006 of shape and scale parameters of gp distribution default is k max round length x 0 1 extremal index min a numeric value giving the extremal index for the identification of outliers with extremely low value if extremal index max null default the extremal index is estimated using the method specified by the parameter extremal index type extremal index max a numeric value giving the extremal index for the identification of outliers with extremely high value if extremal index max null default the extremal index is estimated using the method specified by the parameter extremal index type extremal index type a character string specifying the type of extremal index estimate possible options are block maxima default for block maxima estimator gomes 1993 intervals for intervals estimator ferro and segers 2003 censored for censored estimator holešovský and fusek 2020 kgaps for k gaps estimator süveges and davison 2010 sliding blocks for sliding blocks estimator northrop 2015 runs for runs estimator smith and weissman 1994 block length min a numeric value for residuals with low values giving the length of blocks for estimation of extremal index only required for extremal index type block maxima and extremal index type sliding blocks default is block length min round sqrt length x block length max a numeric value for residuals with high values giving the length of blocks for estimation of extremal index only required for extremal index type block maxima and extremal index type sliding blocks default is block length max round sqrt length x d min a nonnegative integer for residuals with low values giving the value of d parameter used for censored extremal index estimate holešovský and fusek 2020 only required for extremal index type censored d max a nonnegative integer for residuals with high values giving the value of d parameter used for censored extremal index estimate holešovský and fusek 2020 only required for extremal index type censored k min a nonnegative integer for residuals with low values giving the value of k parameter used for k gaps extremal index estimate süveges and davison 2010 only required for extremal index type kgaps k max a nonnegative integer for residuals with high values giving the value of k parameter used for k gaps extremal index estimate süveges and davison 2010 only required for extremal index type kgaps r min a positive integer for residuals with low values giving the value of runs parameter of runs extremal index estimate smith and weissman 1994 only required for extremal index type runs r max a positive integer for residuals with high values giving the value of runs parameter of runs extremal index estimate smith and weissman 1994 only required for extremal index type runs return period a positive numeric value giving return period default is r 120 which means that observations whose values are exceeded on average once every 120 observations are detected as outliers the crucial thing for the method is the choice of return period r the parameter defining the criterion for outliers detection the outliers with extremely high values are detected as observations whose values are exceeded on average once every r observations likewise the outliers with extremely low values are identified for more information and recommendations see holešovský et al 2018 the output of the method is a krdetect object which contains a list with elements method type a character string giving the type of method used for outlier identification x a numeric vector of observations index a numeric vector of index design points assigned to individual observations smoothed a numeric vector of estimates of the kernel regression function smoothed data outlier min a logical vector specifying the identified outliers with extremely low value true means that the corresponding observation from vector x is detected as an outlier outlier max a logical vector specifying the identified outliers with extremely high value true means that the corresponding observation from vector x is detected as an outlier outlier a logical vector specifying the identified outliers with both extremely low and extremely high value true means that the corresponding observation from vector x is detected as an outlier the description of other output parameters is accessed via command image 19 a 4 plot and summary of outlier detection results the results of all three outlier detection methods can be visualized and summarized using the function plot and summary respectively the argument of those functions is the result obtained using previously described functions krdetect outliers changepoint krdetect outliers controlchart and krdetect outliers ev the description of input parameters is accessed via commands image 20 a 5 kernel smoothing the function for kernel smoothing is in the envoutliers package accessed using the smoothing function the function is structured as follows image 21 the arguments of the function are x a numeric vector of design points of the same length as y default is x c 1 length y y data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo bandwidth type a character string specifying the type of bandwidth possible options are local default to use local bandwidth global to use global bandwidth bandwidth value a local bandwidth array for bandwidth type local or global bandwidth value for bandwidth type global for kernel regression estimation if bandwidth type null default a data adaptive local plug in herrmann 1997 for bandwidth type local or data adaptive global plug in gasser et al 1991 for bandwidth type global bandwidth is used instead kernel order a nonnegative integer giving the order of the optimal kernel gasser et al 1985 used for smoothing possible options are kernel order 2 default kernel order 4 the function computes the estimate of kernel regression function using a local or global data adaptive plug in algorithm and optimal kernels gasser et al 1985 the output of the method is a list with the following elements data smoothed a numeric vector of estimates of the kernel regression function smoothed data residuals a numeric vector of smoothing residuals a 6 mean residual life plot the method for mean residual life plot is in the envoutliers package accessed using the mrl plot function the function is structured as follows image 22 the arguments of the function are x data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo umin the minimum threshold at which the mean residual life function is calculated based on maximum likelihood estimates default is umin quantile na omit x probs 0 8 umax the maximum threshold at which the mean residual life function is calculated based on maximum likelihood estimates default is umax quantile na omit x probs 0 95 kmin the minimum number of largest order statistics for which the mean residual life function is calculated based on moment estimates default is kmin round length na omit x 0 05 kmax the maximum number of largest order statistics for which the mean residual life function is calculated based on moment estimates default is kmax round length na omit x 0 2 nint the number of points at which the mean residual life function is calculated default is nint 100 conf the confidence coefficient for the confidence intervals depicted in the plot default is conf 0 95 est method a character string specifying the type of estimates for the scale and shape parameters of gp distribution possible options are mle default to use maximum likelihood estimates coles 2001 moment to use moment estimates de haan and ferreira 2006 u0 a numeric value giving the threshold meant for a gp approximation of the threshold exceedances default is u0 null k0 a numeric value giving the number k0 1 of largest observations meant for a gp approximation default is k0 null the function constructs mrl plot coles 2001 based on maximum likelihood or moment estimates for parameters of gp distribution if u0 or k0 respectively is given a gp mean threshold dependency line is plotted in addition to the mrl plot coles 2001 each of the lines provide the user an option to assess the suitability of u0 or k0 as a lower bound for the threshold exceedances for u0 or the number of upper order statistics for k0 to fit the gp distribution a 7 stability plots the stability plots of the gp parameter estimates are in the envoutliers package accessed using the stability plot function the function is structured as follows image 23 the arguments of the function are x data values supported data types a numeric vector a time series object ts a time series object xts a time series object zoo umin the minimum threshold at which the mean residual life function is calculated default is umin quantile na omit x probs 0 8 umax the maximum threshold at which the mean residual life function is calculated default is umax quantile na omit x probs 0 95 kmin the minimum number of largest order statistics for which the mean residual life function is calculated based on moment estimates default is kmin round length na omit x 0 05 kmax the maximum number of largest order statistics for which the mean residual life function is calculated based on moment estimates default is kmax round length na omit x 0 2 nint the number of points at which the mean residual life function is calculated default is nint 100 conf the confidence coefficient for the confidence intervals depicted in the plot default is conf 0 95 est method a character string specifying the type of estimates for the scale and shape parameters of gp distribution possible options are mle default to use maximum likelihood estimates coles 2001 moment to use moment estimates de haan and ferreira 2006 u0 a numeric value giving the threshold meant for a gp approximation of the threshold exceedances default is u0 null k0 a numeric value giving the number k0 1 of largest observations meant for a gp approximation default is k0 null the function estimates the gp parameters at a range of thresholds in case est method mle or a range of upper order statistics in case of est method moment and shows the sample paths of the estimates if u0 or k0 respectively is given a threshold dependency lines for the particular parameters are plotted in addition the lines provide the user an option to assess the suitability of u0 or k0 as a lower bound for the threshold exceedances for u0 or the number of upper order statistics for k0 to fit the gp distribution 
