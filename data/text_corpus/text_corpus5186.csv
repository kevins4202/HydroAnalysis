index,text
25930,despite the growing acknowledgment of reproducibility crisis in computational science there is still a lack of clarity around what exactly constitutes a reproducible or replicable study in many computational fields including environmental modelling to this end we put forth a taxonomy that defines an environmental modelling study as being either 1 repeatable 2 runnable 3 reproducible or 4 replicable we introduce these terms with illustrative examples from hydrology using a hydrologic modelling framework along with cyberinfrastructure aimed at fostering reproducibility using this taxonomy as a guide we argue that containerization is an important but lacking component needed to achieve the goal of computational reproducibility in hydrology and environmental modelling examples from hydrology are provided to demonstrate how new tools including a user friendly tool for containerization of computational analyses called sciunit can lower the barrier to reproducibility and replicability in the environmental modelling community keywords reproducibility replicability containers docker singularity 1 introduction emphasis on the importance of research reproducibility is steadily rising yet many studies are still not reproducible bell et al 2009 garijo et al 2013 hothorn and leisch 2011 stagge et al 2019b one study found that 70 of researchers tried but failed to reproduce another researcher s experiments baker 2016 this failure was largely due to lack of documentation and the omission of important details from the published article additionally numerous studies have concluded that scientific articles commonly leave out details essential for reproduction ioannidis et al 2009 nekrutenko and taylor 2012 stagge et al 2019b even when these details are in place complex computational studies can take hundreds of hours to reproduce especially for a scientist not involved in the original study baggerly and coombes 2009 reproducibility in such computational studies is particularly difficult because they rely on nontrivial software systems with multiple software dependencies as computational studies become more complex and dependent on a variety of software systems this difficulty leads to the reproducibility crisis being experienced across fields where scientists are unable to reproduce analyses from the information provided in articles and software documentation alone baker 2016 baker and penny 2016 some scientific disciplines including biology biostatistics and biomedicine have been discussing this reproducibility crisis for some time peng 2011 while in other fields such as hydrology and water resources the topic is still being brought to light hutton et al 2016 rosenberg et al 2020 stagge et al 2019a using hydrology and water resources as a case study this paper addresses reproducibility challenges applicable in the broader field of environmental modelling by offering a taxonomy to aid in the interdisciplinary communication around reproducibility as well as an approach involving containerization of computational experiments to enhance reproducibility one of the challenges in discussing the reproducibility crisis in computational modelling fields is the lack of a standard definition for reproducibility despite the fact that several scientists have made efforts to define reproducibility easterbrook 2014 goodman et al 2018 gorgolewski and poldrack 2016 ioannidis et al 2009 stodden et al 2013 the problem has recently resulted in a high level report by the united states us national academies in collaboration with the us national science foundation hereafter referred to as the national academies report national academies of sciences engineering and medicine 2019 the national academies report acknowledges the lack of a standard definition for computational reproducibility across scientific fields to address this problem it offers a standard definition for reproducibility along with a higher level concept named replicability the report defines computational reproducibility as obtaining consistent results using the same input data computational steps methods code and conditions of analysis and replicability as obtaining consistent results across studies aimed at answering the same scientific question each of which has obtained its own data national academies of sciences engineering and medicine 2019 we have adopted these definitions in this paper but argue that there are stages in environmental modelling and likely other computational modelling fields as well before one reaches reproducible as defined in the national academies report without consistent definitions of these earlier stages there will continue to be confusion on what exactly constitutes a reproducible computational study as an example of this confusion a common explanation for achieving reproducibility in computational fields is a study that has all code and data shared with the published article but this explanation is incomplete for computationally complex studies the idea is widely held as evidenced by many scientific journals now requiring authors to provide data availability statements providing digital object identifiers dois for data and models used in the analysis however this requirement alone will not ensure that a computational study is reproducible for example suppose a researcher publishes an article and shares the associated data and models through an open data repository if other researchers have the necessary digital resources and try to reproduce this research using the paper data and models they may be unable to do so if the first researcher did not share exact details of the conditions of analysis as the national academies report national academies of sciences engineering and medicine 2019 puts it used to complete the study the conditions of analysis speak to the computation environment used to conduct the experiment including the operating system used but also the exact versions of all software and software dependencies used in the study even in simple cases where only one software system is used for an analysis e g a single simulation model or single analysis software like python or r because software is being constantly updated and dependencies may change sharing the data and code for that simulation alone may not be sufficient for achieving reproducibility if not all software dependencies are shared too one needs a way to create a self contained package or container of the conditions of analysis that preserves these conditions that were present during experimentation so that even if the container is moved to different computational environments the experiment continues to re execute as originally designed an alternative way researchers have proposed to address the general confusion about different levels of reproducible research is by organizing studies into three categories low medium and high reproducible research tatman et al 2018 peng 2011 low reproducible research occurs when a well written article which may include a detailed methodology section does not provide the digital resources such as code data and computational environment necessary to reproduce that research when one of these resources typically the data associated with the article is published but the others are withheld this research is classified as being medium reproducibility tatman et al 2018 the highest level of reproducibility referred as the gold standard of reproducibility by peng 2011 occurs when all codes data and environments used in the research are both described in the published article and shared these three levels of defining reproducibility provide a useful way to classify studies but they tend to focus on the sharing of digital objects rather than on the functional aspects of reproducibility we argue that the sharing of digital objects is a prerequisite to reproducible research the real challenge and need for categorical distinctions along a reproducibility spectrum should target functional requirements showing the levels at which computational experiments can be rerun by the original authors and ultimately by others to reproduce and replicate the study results for studies that require only minimal software tools it seems feasible for researchers to achieve reproducibility without spending a significant amount of additional time and effort beyond gathering digital objects and setting up virtual environments for an analysis for example the concept of virtual environments in python that allow one to package the specific version of python and dependent libraries used in an analysis is becoming common place with tools like anaconda https www anaconda com however when an analysis makes use of more than just a single software system capturing the conditions of analysis national academies of sciences engineering and medicine 2019 or computational environment peng 2011 that includes all software dependencies becomes more complicated it can often be unclear even to the most sophisticated modeler exactly what software was used by a model or larger analysis workflow given the complex dependencies and sub dependencies with many scientific software systems including modelling systems container technologies offer a solution for this problem handigol et al 2012 knoth and n√ºst 2017 piccolo and frampton 2016 docker merkel 2014 and singularity kurtzer et al 2017 are the most common container technologies but these are programmable tools and for many modelers require a steep learning curve to work with which is often a function of the complexity of the software program being contained in addition because it is unclear exactly what software to include in a container to make an analysis reproducible scientists may overestimate the software needed making for bloated containers that become more like virtual machines thus losing one of the main benefits of container technologies for this reason there are efforts in scientific communities to create cyberinfrastructure technologies and approaches better suited for computational experiments brinckman et al 2019 marwick et al 2018 stodden et al 2015 including efforts to lower the barrier to container technology adoption kjeldgaard 2020 n√ºst et al 2017 n√ºst and hinz 2019 the geotrust project funded through the us national science foundation earthcube program that is advancing sciunit http sciunit run is one such effort chuah et al 2020 that et al 2017 yuan et al 2018 using sciunit a tool for tracing and encapsulating dependencies in a linux environment researchers can create virtual environments for computational analyses that only include the software dependencies used within the analysis that et al 2017 sciunit acts as a monitoring system recording what software dependencies were used by an analysis and then packaging these dependencies into a virtual environment in capturing dependencies during program execution sciunit is unlike docker which uses programmatic methods to capture dependencies sciunit also uses content based data de duplication yuan et al 2018 and is thus more efficient to use than docker meng et al 2015 in past work we have shown how geotrust s sciunit can be combined with hydroshare to improve reproducibility in computational analyses essawy et al 2018 here we extend on this idea to provide a more complete view of best practices for reproducible and replicable computational analyses first we present a taxonomy to capture the reproducibility spectrum for environmental modelling beginning with running an analysis on a single machine and ending with reproducing and replicating that study then we demonstrate the path along this spectrum with an analysis using the structure for unifying multiple modeling alternatives summa hydrology model clark et al 2015a 2015b as an example application as part of this example we demonstrate how the geotrust sciunit tool and open data repositories such as hydroshare can be leveraged to achieve both reproducibility and the higher level concept of replicability a discussion follows that first speaks to nuances in the pursuit of reproducible computational analyses namely the idea that reproducibility does not require an exact match in a computational environment or the results of the analysis but instead requires results that are as the national academies report states consistent this makes knowing when reproducibility has been achieved more challenging because quantifying an exact match is computationally simpler than quantifying a consistent match this section also outlines gaps that remain in achieving reproducibility of environmental modelling analyses that should be addressed through future research and development efforts finally we conclude with a summary of the contributions of this study which are mainly a clearer definition of a taxonomy for describing environmental modelling studies as repeatable runnable reproducible or replicable and an illustrative example for how to move analyses along this taxonomy so that modelers can be more confident that their studies are reproducible rather than being only repeatable or runnable 2 methodology 2 1 defining a taxonomy for reproducible environmental modelling the first objective of this study was to define an appropriate taxonomy for capturing the spectrum of reproducibility for computational environmental modelling to do this we reviewed and synthesized ideas from literature we also adopted the definitions provided by the national academies report as this is a high level report aimed at clarifying two concepts in particular reproducibility and replicability we expanded on the definitions provided in this report by adding additional explanation of these definitions and importantly the common steps in environmental modelling that come before the reproducibility step the result of this work is a four level taxonomy of repeatability runnability reproducibility and replicability described more fully in the results section of the paper and used to organize an example application where a hydrologic modelling analysis moves through the stages in the taxonomy each representing a step along the reproducibility spectrum in designing the taxonomy we had two main goals the first goal was to allow researchers to better distinguish the level of reproducibility of computational studies by providing clear definitions for placing a given study along a reproducibility spectrum while past researchers have also used the concept of a reproducibility spectrum peng 2011 designating studies as either reaching low medium or high reproducibility we designed the taxonomy to focus on functionality rather than data availability our approach views availability as a prerequisite for reproducibility rather than a distinguishing characteristic the second goal of the taxonomy was to not only place an analysis along the reproducibility spectrum but also understanding what additional steps are required to advance to the next level in the reproducibility spectrum through an example application presented in this paper we demonstrate how research can advance an analysis along the spectrum 2 2 steps for creating a reproducible analysis the second objective in this study was to design a general approach for advancing computational analyses common in environmental modelling along the reproducibility spectrum the obvious goal of any modelling study is to get a simulation running on the researcher s own machine once this has been achieved the next step should be to repeat this analysis on a new machine in repeating an analysis on a different machine the researcher will be forced to overcome potential difficulties that another researcher may face when attempting to replicate the study this step allows the researcher to document steps more clearly and completely by overcoming potential problems that another researcher may face when trying to setup and reproduce the study on a different machine while a researcher must share these resources reproducibility cannot be claimed until a third party has verified that it is possible to reproduce an analysis the more researchers able to verify that an analysis can be reproduced in their own computational environment the more confidence one can have that the work has achieved reproducibility while a researcher can make an analysis reproducible with sufficient documentation and sharing of digital resources for complex analyses doing so is time consuming challenging to do correctly and difficult to keep up to date fig 1 illustrates this point with a simple example suppose researcher a shares completed work in the form of a published article with associated code and data because the full computational environment capturing the conditions of analysis including all dependencies is not shared with this study researchers b c and d must try to recreate the computational environment and dependencies on their own researcher b has a different operating system researcher c has a different core software system for the analysis and researcher d has the wrong software dependencies as a result none of the researchers are able to reproduce the results of the analysis without detailed documentation on required software dependencies and the broader conditions of analysis it may not even be clear to researchers b c and d why they are unable to reproduce the results of researcher a the following methodology allows researcher a to better share her analysis in a way that can be more easily reproduced this method emphasizes the use of a container to encapsulate a virtual environment and open data repositories for sharing digital resources these are critical enabling technologies and tools for advancing reproducibility in a research context where the software is part of the experiment and the environmental modelling is conducted with research simulation models that do not have a standard release schedule and installation package the simulation models used by scientific teams for research within the scientific community are often developed in a bespoke fashion often advanced by different research teams and updated in a non coordinated manner thus unlike commercial simulation software that has more formalized release schedules continuous integration testing and installers simply getting some scientific software properly installed while also knowing what version of the software is being used is a nontrivial exercise for such bespoke software it is increasingly important to snapshot a working execution and make the snapshots reproducible for others in different environments the steps illustrated in fig 2 and described below in a general model agnostic way demonstrate this need the results section includes results when applying this methodology for a specific modelling analysis 1 researcher a creates the analysis on her local machine and repeats it several times to ensure the analysis is working correctly from here we can say repeatability is achieved 2 researcher a moves the analysis to another machine and repeats step 1 completing this step will allow researcher a to better understand and document the conditions of analysis so that anyone attempting to reproduce the study can also run the analysis on their own computational environment 3 researcher a acknowledging that it is time consuming to perfectly document the steps needed to recreate the conditions of the analysis based on performing step 2 creates a containerized virtual environment that packages the conditions of analysis as a reproducible unit this containerized virtual environment is shared with the community alongside the article in an open repository with its own digital object identifier doi and metadata as shown in fig 3 4 other researchers download this containerized virtual environment and required input data to their own machines to verify that they can obtain researcher a s results the more scientists able to reproduce the analysis in their own environment the more confidence researcher a has that her work has reached what peng 2011 calls the reproducibility gold standard 5 once researchers have been able to reproduce the analysis they may now wish to replicate it by using the same analysis but with their own data sets because they were first able to reproduce the study on their own they can be more confident in reusing the analysis to build from past work and advance their own research 2 3 leveraging advances in cyberinfrastructure to create reproducible workflows the third objective of this study was to effectively leverage cyberinfrastructure to reduce the burden scientists have in creating reproducible analyses achieving reproducibility remains time consuming and tedious due to a gap in technology recent advances in the broader information technology field can be used to assist researchers in better documenting and sharing code data metadata and building a virtual environment to achieve reproducibility some scientists have begun using these new tools and approaches with the aim of making their research more reproducible and they have discussed the reproducibility of their research in their published articles ivie and thain 2018 sadler et al 2018 woodson et al 2018 in these studies online data repositories such as github figshare zenodo and hydroshare are becoming more commonly used by scientists to describe and share their data and code many of these repositories provide the ability to publish digital resources through their system assigning a digital object identifier doi to the resource that can be used to uniquely identify that resource in perpetuity containers and virtual environments are still uncommon tools in scientific studies perhaps because of their steeper learning curve for use in computational modelling and analysis studies while there are a growing number of tools available to scientists to make their work more reproducible and replicable we focus on two specific systems 1 the sciunit tool http sciunit run and 2 the hydroshare online repository http www hydroshare org the sciunit tool is software that enables researchers to create virtual environments for reproducing their computational analyses with minimal user interaction sciunit advances the concept of a research object which is an automatic aggregation of digital artifacts such as code data scripts and temporary experiment results that together with any research paper provide an authoritative and far more complete record of a piece of research bechhofer et al 2013 hydroshare is an open repository for sharing hydrologic data and models as digital resources including detailed hydrologic specific resource metadata horsburgh et al 2015 tarboton and idaszak 2015 we showed in prior work how combining these two tools sciunit and hydroshare can improve reproducibility for computational analyses essawy et al 2018 we showed how sciunit can be used to create containerized virtual environments but lacks mechanisms for sharing them within a community of users hydroshare on the other hand allows for sharing codes data and descriptive metadata but it does not address the challenge of packaging virtual environments for reproducing complex computational analyses building from this research here we extend this work to the more complete taxonomy for describing repeatable runnable reproducible and replicable studies illustrating how an environmental modelling study advances along this taxonomy and how tools like sciunit and hydroshare enable studies to move from being repeatable to runnable then reproducible and finally replicable applying the general five step procedure outlined in the prior section the sciunit tool is used to capture encapsulate and make the model execution portable by creating sciunit containers while the hydroshare repository is used to share the resulting sciunits and other key digital resources in the analysis more specifically the sciunit tool is used to automatically containerize or package and share the virtual environment to hydroshare as a sciunit and then a hydroshare compute platform such as cuahsi juypterhub or cybergis jupyter for water is used to interact with the shared sciunit through a jupyter notebook fig 4 these jupyterhub implementations because of their ability to read and write data from and to hydroshare provide a powerful analysis and model execution environment bandaragoda et al 2019 the advantage of this methodology is 1 it reduces the time necessary to download install and run the workflow on local machines 2 scientists don t face dependency issues such as missing or out of date dependencies and 3 it avoids local computing restraints that scientists face such as limited memory or storage 2 4 example application using the summa hydrologic model an example application is presented to illustrate how the five step procedure presented in the prior section can be applied with sciunit and hydroshare for an environmental modelling analysis the example uses the summa hydrologic modelling framework to simulate different hydrologic hypotheses and show how to reproduce the model simulation process clark et al 2015a 2015b the summa structure enables users to implement different modelling approaches with controlled and systematic analysis and provide insight for the advanced unified modelling framework fig 5 shows the summa model construction and process flexibility the summa structure consists of a core solver with outer branches and produces a numerical solution using conservation equations for estimating water and energy flux and state variables the summa model provides flexibility to evaluate the interplay between the choice of model parameters and the choice of process parameterizations separating modelling decisions on process representation from their numerical implementation and providing capabilities to experiment with different numerical solvers building from four synthetic and nine field study test cases of summa described in clark et al 2015b we selected and automated two of the test cases 1 field data test case 5 snow interception at umpqua and 2 field data test case 7 sensitivity of evapotranspiration to the stomatal resistance parameterization aspen stand at reynolds mountain east these two test cases are described in clark et al 2015b and the digital resources needed to reproduce the paper are available online however the test cases were not reproducible as defined by the national academies report because the conditions of analysis were not completely described and documented we demonstrate in the results section how following the five step procedure described earlier and using sciunit and hydroshare one can move the summa analysis from being repeatable to being runnable and finally to being reproducible and replicable 3 results 3 1 taxonomy for reproducibility in environmental modelling the taxonomy resulting from this work organizes the reproducibility spectrum into four levels repeatability runnability reproducibility and replicability fig 6 these levels represent a progression where the base level repeatability is the first step to achieve followed by runnability reproducibility and finally replicability the terms reproducibility and replicability are consistent with those proposed in the national academies report we argue that the two lower level concepts repeatability and runnability are needed as precursors to correctly capture the steps along the path to reproducibility and replicability the four levels are defined below and illustrated more fully later in this section using the summa model as a case study repeatability is achieved upon obtaining consistent results using the same input data computational steps methods and code on the original researcher s machine this level is normally achieved in scientific papers the author is able to rerun the analysis on his or her own computer to obtain the analysis results however it does require detailed documentation and benefits greatly from automation of steps to make repeating the analysis in a consistent way possible runnability is achieved when the author of the research can obtain consistent results using the same input data computational steps methods code and conditions of analysis on a new machine achieving this level requires thought and care to document the conditions of the analysis meaning software dependencies and related details of the computational environment in such a way that the analysis can be repeated correctly in a new computing environment reproducibility is achieved when a new researcher not an original author of the analysis is able to reproduce the analysis in their own computational environment achieving this step shows that one is able to achieve the national academies report definition of reproducibility obtaining consistent results using the same input data computational steps methods and code and conditions of analysis replicability is the higher level concept defined in the national academies report as obtaining consistent results across studies aimed at answering the same scientific question each of which has obtained its own data replicability also allows scientists not involved in the original study to build from and expand on research once they are first able to reproduce that research note that the concept of results being consistent used in these definitions adopted from the national academy report is less rigid than exact numerical equivalence rather it involves some degree of evaluation as to how close is good enough acknowledging numerical differences that arise on different platforms this concept is addressed more fully in the discussion section of the paper 3 2 modelling example case study repeating the summa analysis in this section we present a modelling example case study that steps through the four levels of the taxonomy using the general methodology described in section 2 2 applied for the specific environmental modelling use case described in section 2 4 that leverages the summa model the first step is for the author to repeat an analysis on her own machine and obtain consistent results we did this for test case 7 sensitivity of evapotranspiration to the stomatal resistance parameterization aspen stand at reynolds mountain east described in clark et al 2015b for this test case we created an automated analysis using the summa python wrapper software pysumma choi et al 2018 to model the sensitivity of evapotranspiration to the stomatal resistance parameterization for the aspen stand at the reynolds mountain east study site we installed the summa model by following the steps in the summa installation documentation https summa readthedocs io en latest installation summa installation after we installed the summa model we developed a workflow using pysumma to model the impact of stomatal resistance parameterizations on total evapotranspiration fig 7 shows the script used to set up and run the test case 7 summa model simulation using pysumma and create a visualization of the model this script shows how a summa model can be initialized how the model configuration can be adjusted with single lines e g setting the stomata resistance function to jarvis how summa can be executed and how the results can be visualized all from a single python script while these steps can be completed without pysumma through manipulating input files for a summa model directly pysumma acts as a wrapper for these text files and as a means for documenting a model configuration for a specific experiment the result from running the script shown in fig 7 shows the plot generated from the analysis this can be compared to the plot that appears in clark et al 2015b to gauge if the results are consistent in this case consistent results were achieved and therefore we can conclude that the analysis has reached the repeatable stage we are assuming for the sake of this paper that we are playing the role of the original author in this study as the original paper did not provide details on repeatability 3 3 making the analysis runnable using sciunit and cuahsi hydroshare jupyterhub once the analysis was repeated the next step was to make the analysis runnable on a separate machine to do this we made use of the sciunit tool to package the analysis along with its dependencies and provenance metadata fig 8 shows the steps to package the workflow analysis using the sciunit tool these steps are 1 create a new sciunit myanalysis this will create a virtual directory which will include the captured execution of the computational workflow with all the dependencies and provenance metadata associated with it 2 open the myanalysis sciunit to begin working in the desired sciunit 3 execute the code required to be packaged as a virtual environment in order to repeat the analysis 4 show the details of the virtual environment encapsulated in a sciunit package 5 place the packaged sciunit on hydroshare as a digital resource and 6 test the runnability of the package by executing the sciunit on the cuahsi hydroshare jupyterhub app linked to hydroshare and configured to open and execute scripts acting on content from resources in hydroshare note to run a sciunit again requires the sciunit tool which is installed on cuahsi hydroshare jupyterhub the initial test of this execution by us playing the role of the original author on the separate jupyterhub app computer demonstrating runnability is detailed in the following paragraph once the sciunit is shared on hydroshare users of hydroshare can interact and execute the shared sciunit from hydroshare using jupyterhub thereby achieving reproducibility doing so would achieve the reproducibility gold standard peng 2011 the cuahsi hydroshare jupyterhub app is a deployment of jupyterhub hosted at http jupyter cuahsi org that is part of the overall hydroshare cyberinfrastructure linked to but separate from the hydroshare repository www hydroshare org jupyterhub enables literate programming an approach that describes the written program so that the user can understand it piccolo and frampton 2016 literate programing allows the researcher to split the code into fragments that can be executed independently and it enables the researcher to organize the fragments in a way that is easily understandable jupyter gives scientists the capability of combing data and visualizations in powerful ways when communicating their research p√©rez and granger 2007 scientists use jupyter notebooks as a supplement to the published manuscripts so that others can replicate their analysis and re generate the published results for example sadler et al 2018 took a step toward creating reproducible research in the field of hydrology by publishing all codes and data on hydroshare additionally sadler et al 2018 used the jupyter functionality within hydroshare to create notebooks for some of the scripts used in his study and he shared these notebooks with other researchers so they could interact with the notebooks using jupyter step 5 resulted in a hydroshare resource named myanalysis fig 9 choi 2020 to run this in jupyterhub the user would first click the open with button from the resource s landing page in hydroshare and select cuahsi jupyterhub this step will open an instance of jupyterhub where the user can reproduce the workflow saved in the sciunit package fig 10 shows these steps for reproducing the analysis with commands issued through the jupyter user interface the user must issue sciunit commands by using the expression in jupyter the sciunit open myanalysis command will open the sciunit the sciunit show command will show the latest packaged experiment within myanalysis sciunit while the sciunit list command will list the all the packaged experiments within myanalysis sciunit finally the sciunit repeat e1 command will rerun the experiment 1 e1 analysis on the host machine examining the plot generated validates that results are consistent with the original work and demonstrates runnability 3 4 reproducing the summa analysis using sciunit and hydroshare once runnability has been validated the original researcher can make the hydroshare resource holding the sciunit package public and accessible to other interested researchers it can also be permanently published and be given a citable doi a new researcher can then access this resource to reproduce the analysis one way the new researcher can do this is to also use the cuahsi hydroshare jupyterhub app essentially repeating the validation of runnability done by the original author above this is however only a weak demonstration of reproducibility because it uses the same environment cuahsi hydroshare jupyterhub where runnability was tested the following steps describe how other researchers can reproduce the analysis on their own computer using the sciunit package shared on hydroshare a stronger test for reproducibility these steps assume the new machine has sciunit installed but make no additional assumptions of software availability on the new machine 1 download the sciunit resource from hydroshare 2 navigate to the location where the sciunit resides 3 use the command line to unpack and initiate the selected sciunit using sciunit open 4 use the command sciunit list to see what packages are within the sciunit and which package will be used in the process since some sciunits may contain multiple packages for example sciunit may use the same datasets but employ different methods 5 repeat the e1 package to generate the analysis 6 once the analysis is repeated a directory will be created with a copy of the analysis and all associated files examining the plot generated validates that results are consistent with the original work and demonstrates reproducibility 3 5 replicating the summa analysis once the user achieves reproducibility following the steps outlined in the prior section the user is now in a position to replicate the work by modifying the sciunit package and running it using a new dataset or code aimed at answering the same research question using the different data or exploring whether a different model or approach to the analysis produces results that support the conclusions a different implementation of the model here may be new code but the same underlying equations or principles and this would serve to evaluate whether the code and solvers actually implement the equations properly while replicability could be achieved without first reproducing another s work we suggest that in many cases the path to replicability is best traveled by first being able to reproduce past work this puts a researcher in a better position to interpret any differences that may arise and at the same time be more efficient and take advantage of and reuse code from the original researchers the steps for achieving replicability within cuahsi hydroshare jupyterhub using sciunit build off of the steps for reproducibility once the five steps needed for reproducing an analysis have been completed the next steps are 1 navigate to the directory that includes the analysis created and edit the script to use different data or a different approach fig 11 and 2 the user would commit changes and repeat sciunit execution package e2 with the sciunit given command as shown in fig 12 after completing these steps the user has replicated the study by creating a copy of a past analysis changing that analysis s input data and rerunning the analysis using the new input file because all software dependencies are handled by the sciunit tool the analysis will be reproducible on any machine with a compatible os and one that has the sciunit client software installed this same process could be repeated outside of the cuahsi hydroshare jupyterhub environment following a similar procedure to the one described in section 3 4 4 discussion 4 1 defining and assuring consistent results the national academies report s definition for reproducibility includes the phrase consistent results acknowledging that achieving reproducibility does not require obtaining the exact same results as the original study very often in computational modelling numerical differences due to a variety of factors including rounding and precision mean that the output from an analysis does not have to be exactly the same as prior runs of the analysis but it should be close enough furthermore some computational modelling strategies are stochastic so achieving the exact result from a prior run is impossible obviously close enough is a relative measure that requires expert judgment based on the specific requirements of the study and the tolerance for differences in numerical results for this reason it is a more difficult standard in some ways that achieving the exact results from a prior analysis the consistent results requirement also points to the complicating factor that it is not necessary to have an exact recreation of the computational environment in order to achieve reproducibility thus while we made the claim at the start of this paper that software dependency differences could be the reason for an inability to reproduce a prior computational study see fig 1 this may not always be true for example consider the case illustrated in fig 13 which presents a more detailed view of the case described in fig 1 in this more detailed view two researchers a and b have slight differences in their computational environments yet runnability can still be achieved if the results from researcher b s setup different libraries and operating system can produce results that are close enough to researcher a s results there is risk here however just because the results are close enough for one run reproducibility there is no guarantee that a slight change in the input data could still produce results that are close enough for some other run replication however there is value in allowing for software inconsistencies because if setup b is close to the setup a setup b has migrated the model to later versions of dependency software there clearly needs to be the ability to move to new software versions while also achieving reproducibility along the way at the same time it is known that some software mitigations like the migration from gdal v2 to gdal v3 illustrated in fig 13 can cause breaking changes for example even though the coordinate system is basic functionality in geospatial software upgrading to gdal v3 changed the behavoir of the exporttoproj4 function which is the function used to specify the coordinate system for both raster and vector data to address and mitigate breaking changes such as this introduced by dependency updates it is important that specific dependency versions be well documented and that test data be available for a new researcher to ues to test for reproducibility in migrating to an updated computational environment it is also important for container technology such as sciunit to encapsulate the dependencies used by the original researcher to aid in migration and facilitate reproducibility by the new researcher 4 2 limitations and remaining challenges while computational resources data code and environment are important these alone are insufficient to ensure reproducibility the researcher must also include documentation and metadata about the software they use in order to run their programs correctly documentation in the journal article itself is often lacking and must be accompanied with user manuals or other resources including jupyter notebooks that more fully describe the computational analysis metadata is needed to uniquely identify computational resources within the growing ecosystem of data and software while metadata for data is well established recent efforts have been made towards capturing software metadata ontosoft is an example that provides an ontology and portal for addressing the challenge of capturing metadata for scientific software in a formal way gil et al 2016 2015 in hydrology the hydroshare system can be used to describe metadata for data and models hydroshare defines two key computational modelling concepts a model program and a model instance the model program is the software for executing the model and the model instance is the input files required for executing the model horsburgh et al 2015 morsy et al 2017 2014 tarboton et al 2014 this study focuses on studies where input data have already been prepared for an analysis and all required data for running the analysis are stored locally i e there are no references to external data or services once this is the case the remaining goal is to perform model simulations and analyze the model output as described in this paper following this work a next step needed to achieve reproducibility in complex computational studies is to have workflows that automate the end to end process of reproducing the analysis from raw data to publication ready figures and tables the general workflow should automate the steps to 1 obtain the raw data published in an online repository preferably from a published immutable external resource with a doi for reproducibility 2 run one or more scripts needed to prepare the raw data for the model 3 run the model using the prepared datasets and 4 post process the model output to generate publication ready figures and tables this four step workflow should be containerized into a virtual environment that can be repeated by other researchers fig 14 shows the proposed approach to achieve reproducible research some efforts have already been made to create generic processes to automate the larger workflow required for hydrologic modelling one example is work using the variable infiltration capacity vic hydrological model a macro scale hydrologic model that applies water and energy balances to simulate terrestrial hydrology at a regional level liang et al 1996 the vic model like many hydrologic models requires significant effort to prepare its input data billah et al 2016 demonstrated a single step process to create a workflow that automated the preparation of the input data for the vic while workflow software can help to better capture the provenance it is still important to have sufficient metadata for each step within the workflow essawy et al 2017 an example of workflows focused on metadata capture is work using modflow nwt as a demonstration model essawy et al 2018 the modflow nwt is a version of the united states geological survey s groundwater model modflow niswonger et al 2011 the work done for the modflow nwt because it leverages the sciunit concept provided through the geotrust project improves reproducibility in the way it guarantees the replicability of research because this workflow is automated the user is unable to change the parameters of the workflow execution and all data run through this workflow will be executed according to the same parameters specified by the automated workflow work is needed to merge these ideas and concepts into a complete end to end workflow for repeatable runnable reproducible and replicable environment modelling that stretches from data preparation to model execution to post processing of model outputs all containerized well documented with appropriate metadata easy to understand and reuse and discoverable through online data repository systems 5 conclusions this research has shown a path to achieving reproducibility and replicability of environmental modelling studies by first providing a more formal taxonomy that defines studies as being repeatable runnable reproducible or replicable after defining this taxonomy we focused on a methodology that uses this taxonomy in describe steps needed to move research along the spectrum from being repeatable to being replicable using a hydrologic modelling analysis as an example we demonstrate these steps and highlight the important role of containers and software tools that enable scientists to use containers in achieving reproducible and replicable studies the hydrologic modelling analysis demonstrating this process uses the summa modelling framework and different parameterizations of stomatal resistance to estimate total evapotranspiration for a study site we demonstrated how to achieve the steps along the taxonomy with existing cyberinfrastructure software tools as follows 1 repeating an analysis on the same computer with the same inputs as its original application 2 running the same analysis on another machine and getting consistent results by packaging it as a container using the sciunit software 3 reproducing the analysis by sharing it to hydroshare and allowing other researchers to re execute it through the literate programing jupyterhub 4 replicating the analysis by allowing researchers to easily change the shared sciunit package to use their own data or modelling ideas and then repeating the 1 4 sequence of steps while this work moves closer to achieving reproducible and replicable environmental modelling studies there are still remaining limitations and challenges to be addressed first it is difficult to know precisely when an analysis has been reproduced because achieving this goal does not require an exact match of the computational environment but rather requires a similar computational environment that is perhaps some dependences have been updated to new versions but it is essentially the same computational environment and set up therefore the results may not be exactly the same but are consistent with results in the prior analysis run creating automated tools for judging this potentially vague criteria of consistent results for reproducibility remains a challenge literate programming tools like jupyter are a major step forward in providing documentation alongside analysis but more work is needed to better document computational analyses in ways that foster reproducibility and replicability this is especially true for efforts to document and automate end to end workflows that operate from raw input data create model input files execute models and produce publication ready figures capturing these entire end to end workflows in a way that can be not only reproduced and replicated by others but also easy to understand and reuse remains an open challenge software availability the analysis example illustrated in this research is available free and open source under an mit license from github at https github com uva hydroinformatics pysumma sciunit declaration of competing interest the authors have no affiliation with any organization with a direct or indirect financial interest in the subject matter discussed in the manuscript acknowledgements we gratefully acknowledge the national science foundation for support of this work under awards icer 1639759 icer 1661918 icer 1540901 and oac 1664061 
25930,despite the growing acknowledgment of reproducibility crisis in computational science there is still a lack of clarity around what exactly constitutes a reproducible or replicable study in many computational fields including environmental modelling to this end we put forth a taxonomy that defines an environmental modelling study as being either 1 repeatable 2 runnable 3 reproducible or 4 replicable we introduce these terms with illustrative examples from hydrology using a hydrologic modelling framework along with cyberinfrastructure aimed at fostering reproducibility using this taxonomy as a guide we argue that containerization is an important but lacking component needed to achieve the goal of computational reproducibility in hydrology and environmental modelling examples from hydrology are provided to demonstrate how new tools including a user friendly tool for containerization of computational analyses called sciunit can lower the barrier to reproducibility and replicability in the environmental modelling community keywords reproducibility replicability containers docker singularity 1 introduction emphasis on the importance of research reproducibility is steadily rising yet many studies are still not reproducible bell et al 2009 garijo et al 2013 hothorn and leisch 2011 stagge et al 2019b one study found that 70 of researchers tried but failed to reproduce another researcher s experiments baker 2016 this failure was largely due to lack of documentation and the omission of important details from the published article additionally numerous studies have concluded that scientific articles commonly leave out details essential for reproduction ioannidis et al 2009 nekrutenko and taylor 2012 stagge et al 2019b even when these details are in place complex computational studies can take hundreds of hours to reproduce especially for a scientist not involved in the original study baggerly and coombes 2009 reproducibility in such computational studies is particularly difficult because they rely on nontrivial software systems with multiple software dependencies as computational studies become more complex and dependent on a variety of software systems this difficulty leads to the reproducibility crisis being experienced across fields where scientists are unable to reproduce analyses from the information provided in articles and software documentation alone baker 2016 baker and penny 2016 some scientific disciplines including biology biostatistics and biomedicine have been discussing this reproducibility crisis for some time peng 2011 while in other fields such as hydrology and water resources the topic is still being brought to light hutton et al 2016 rosenberg et al 2020 stagge et al 2019a using hydrology and water resources as a case study this paper addresses reproducibility challenges applicable in the broader field of environmental modelling by offering a taxonomy to aid in the interdisciplinary communication around reproducibility as well as an approach involving containerization of computational experiments to enhance reproducibility one of the challenges in discussing the reproducibility crisis in computational modelling fields is the lack of a standard definition for reproducibility despite the fact that several scientists have made efforts to define reproducibility easterbrook 2014 goodman et al 2018 gorgolewski and poldrack 2016 ioannidis et al 2009 stodden et al 2013 the problem has recently resulted in a high level report by the united states us national academies in collaboration with the us national science foundation hereafter referred to as the national academies report national academies of sciences engineering and medicine 2019 the national academies report acknowledges the lack of a standard definition for computational reproducibility across scientific fields to address this problem it offers a standard definition for reproducibility along with a higher level concept named replicability the report defines computational reproducibility as obtaining consistent results using the same input data computational steps methods code and conditions of analysis and replicability as obtaining consistent results across studies aimed at answering the same scientific question each of which has obtained its own data national academies of sciences engineering and medicine 2019 we have adopted these definitions in this paper but argue that there are stages in environmental modelling and likely other computational modelling fields as well before one reaches reproducible as defined in the national academies report without consistent definitions of these earlier stages there will continue to be confusion on what exactly constitutes a reproducible computational study as an example of this confusion a common explanation for achieving reproducibility in computational fields is a study that has all code and data shared with the published article but this explanation is incomplete for computationally complex studies the idea is widely held as evidenced by many scientific journals now requiring authors to provide data availability statements providing digital object identifiers dois for data and models used in the analysis however this requirement alone will not ensure that a computational study is reproducible for example suppose a researcher publishes an article and shares the associated data and models through an open data repository if other researchers have the necessary digital resources and try to reproduce this research using the paper data and models they may be unable to do so if the first researcher did not share exact details of the conditions of analysis as the national academies report national academies of sciences engineering and medicine 2019 puts it used to complete the study the conditions of analysis speak to the computation environment used to conduct the experiment including the operating system used but also the exact versions of all software and software dependencies used in the study even in simple cases where only one software system is used for an analysis e g a single simulation model or single analysis software like python or r because software is being constantly updated and dependencies may change sharing the data and code for that simulation alone may not be sufficient for achieving reproducibility if not all software dependencies are shared too one needs a way to create a self contained package or container of the conditions of analysis that preserves these conditions that were present during experimentation so that even if the container is moved to different computational environments the experiment continues to re execute as originally designed an alternative way researchers have proposed to address the general confusion about different levels of reproducible research is by organizing studies into three categories low medium and high reproducible research tatman et al 2018 peng 2011 low reproducible research occurs when a well written article which may include a detailed methodology section does not provide the digital resources such as code data and computational environment necessary to reproduce that research when one of these resources typically the data associated with the article is published but the others are withheld this research is classified as being medium reproducibility tatman et al 2018 the highest level of reproducibility referred as the gold standard of reproducibility by peng 2011 occurs when all codes data and environments used in the research are both described in the published article and shared these three levels of defining reproducibility provide a useful way to classify studies but they tend to focus on the sharing of digital objects rather than on the functional aspects of reproducibility we argue that the sharing of digital objects is a prerequisite to reproducible research the real challenge and need for categorical distinctions along a reproducibility spectrum should target functional requirements showing the levels at which computational experiments can be rerun by the original authors and ultimately by others to reproduce and replicate the study results for studies that require only minimal software tools it seems feasible for researchers to achieve reproducibility without spending a significant amount of additional time and effort beyond gathering digital objects and setting up virtual environments for an analysis for example the concept of virtual environments in python that allow one to package the specific version of python and dependent libraries used in an analysis is becoming common place with tools like anaconda https www anaconda com however when an analysis makes use of more than just a single software system capturing the conditions of analysis national academies of sciences engineering and medicine 2019 or computational environment peng 2011 that includes all software dependencies becomes more complicated it can often be unclear even to the most sophisticated modeler exactly what software was used by a model or larger analysis workflow given the complex dependencies and sub dependencies with many scientific software systems including modelling systems container technologies offer a solution for this problem handigol et al 2012 knoth and n√ºst 2017 piccolo and frampton 2016 docker merkel 2014 and singularity kurtzer et al 2017 are the most common container technologies but these are programmable tools and for many modelers require a steep learning curve to work with which is often a function of the complexity of the software program being contained in addition because it is unclear exactly what software to include in a container to make an analysis reproducible scientists may overestimate the software needed making for bloated containers that become more like virtual machines thus losing one of the main benefits of container technologies for this reason there are efforts in scientific communities to create cyberinfrastructure technologies and approaches better suited for computational experiments brinckman et al 2019 marwick et al 2018 stodden et al 2015 including efforts to lower the barrier to container technology adoption kjeldgaard 2020 n√ºst et al 2017 n√ºst and hinz 2019 the geotrust project funded through the us national science foundation earthcube program that is advancing sciunit http sciunit run is one such effort chuah et al 2020 that et al 2017 yuan et al 2018 using sciunit a tool for tracing and encapsulating dependencies in a linux environment researchers can create virtual environments for computational analyses that only include the software dependencies used within the analysis that et al 2017 sciunit acts as a monitoring system recording what software dependencies were used by an analysis and then packaging these dependencies into a virtual environment in capturing dependencies during program execution sciunit is unlike docker which uses programmatic methods to capture dependencies sciunit also uses content based data de duplication yuan et al 2018 and is thus more efficient to use than docker meng et al 2015 in past work we have shown how geotrust s sciunit can be combined with hydroshare to improve reproducibility in computational analyses essawy et al 2018 here we extend on this idea to provide a more complete view of best practices for reproducible and replicable computational analyses first we present a taxonomy to capture the reproducibility spectrum for environmental modelling beginning with running an analysis on a single machine and ending with reproducing and replicating that study then we demonstrate the path along this spectrum with an analysis using the structure for unifying multiple modeling alternatives summa hydrology model clark et al 2015a 2015b as an example application as part of this example we demonstrate how the geotrust sciunit tool and open data repositories such as hydroshare can be leveraged to achieve both reproducibility and the higher level concept of replicability a discussion follows that first speaks to nuances in the pursuit of reproducible computational analyses namely the idea that reproducibility does not require an exact match in a computational environment or the results of the analysis but instead requires results that are as the national academies report states consistent this makes knowing when reproducibility has been achieved more challenging because quantifying an exact match is computationally simpler than quantifying a consistent match this section also outlines gaps that remain in achieving reproducibility of environmental modelling analyses that should be addressed through future research and development efforts finally we conclude with a summary of the contributions of this study which are mainly a clearer definition of a taxonomy for describing environmental modelling studies as repeatable runnable reproducible or replicable and an illustrative example for how to move analyses along this taxonomy so that modelers can be more confident that their studies are reproducible rather than being only repeatable or runnable 2 methodology 2 1 defining a taxonomy for reproducible environmental modelling the first objective of this study was to define an appropriate taxonomy for capturing the spectrum of reproducibility for computational environmental modelling to do this we reviewed and synthesized ideas from literature we also adopted the definitions provided by the national academies report as this is a high level report aimed at clarifying two concepts in particular reproducibility and replicability we expanded on the definitions provided in this report by adding additional explanation of these definitions and importantly the common steps in environmental modelling that come before the reproducibility step the result of this work is a four level taxonomy of repeatability runnability reproducibility and replicability described more fully in the results section of the paper and used to organize an example application where a hydrologic modelling analysis moves through the stages in the taxonomy each representing a step along the reproducibility spectrum in designing the taxonomy we had two main goals the first goal was to allow researchers to better distinguish the level of reproducibility of computational studies by providing clear definitions for placing a given study along a reproducibility spectrum while past researchers have also used the concept of a reproducibility spectrum peng 2011 designating studies as either reaching low medium or high reproducibility we designed the taxonomy to focus on functionality rather than data availability our approach views availability as a prerequisite for reproducibility rather than a distinguishing characteristic the second goal of the taxonomy was to not only place an analysis along the reproducibility spectrum but also understanding what additional steps are required to advance to the next level in the reproducibility spectrum through an example application presented in this paper we demonstrate how research can advance an analysis along the spectrum 2 2 steps for creating a reproducible analysis the second objective in this study was to design a general approach for advancing computational analyses common in environmental modelling along the reproducibility spectrum the obvious goal of any modelling study is to get a simulation running on the researcher s own machine once this has been achieved the next step should be to repeat this analysis on a new machine in repeating an analysis on a different machine the researcher will be forced to overcome potential difficulties that another researcher may face when attempting to replicate the study this step allows the researcher to document steps more clearly and completely by overcoming potential problems that another researcher may face when trying to setup and reproduce the study on a different machine while a researcher must share these resources reproducibility cannot be claimed until a third party has verified that it is possible to reproduce an analysis the more researchers able to verify that an analysis can be reproduced in their own computational environment the more confidence one can have that the work has achieved reproducibility while a researcher can make an analysis reproducible with sufficient documentation and sharing of digital resources for complex analyses doing so is time consuming challenging to do correctly and difficult to keep up to date fig 1 illustrates this point with a simple example suppose researcher a shares completed work in the form of a published article with associated code and data because the full computational environment capturing the conditions of analysis including all dependencies is not shared with this study researchers b c and d must try to recreate the computational environment and dependencies on their own researcher b has a different operating system researcher c has a different core software system for the analysis and researcher d has the wrong software dependencies as a result none of the researchers are able to reproduce the results of the analysis without detailed documentation on required software dependencies and the broader conditions of analysis it may not even be clear to researchers b c and d why they are unable to reproduce the results of researcher a the following methodology allows researcher a to better share her analysis in a way that can be more easily reproduced this method emphasizes the use of a container to encapsulate a virtual environment and open data repositories for sharing digital resources these are critical enabling technologies and tools for advancing reproducibility in a research context where the software is part of the experiment and the environmental modelling is conducted with research simulation models that do not have a standard release schedule and installation package the simulation models used by scientific teams for research within the scientific community are often developed in a bespoke fashion often advanced by different research teams and updated in a non coordinated manner thus unlike commercial simulation software that has more formalized release schedules continuous integration testing and installers simply getting some scientific software properly installed while also knowing what version of the software is being used is a nontrivial exercise for such bespoke software it is increasingly important to snapshot a working execution and make the snapshots reproducible for others in different environments the steps illustrated in fig 2 and described below in a general model agnostic way demonstrate this need the results section includes results when applying this methodology for a specific modelling analysis 1 researcher a creates the analysis on her local machine and repeats it several times to ensure the analysis is working correctly from here we can say repeatability is achieved 2 researcher a moves the analysis to another machine and repeats step 1 completing this step will allow researcher a to better understand and document the conditions of analysis so that anyone attempting to reproduce the study can also run the analysis on their own computational environment 3 researcher a acknowledging that it is time consuming to perfectly document the steps needed to recreate the conditions of the analysis based on performing step 2 creates a containerized virtual environment that packages the conditions of analysis as a reproducible unit this containerized virtual environment is shared with the community alongside the article in an open repository with its own digital object identifier doi and metadata as shown in fig 3 4 other researchers download this containerized virtual environment and required input data to their own machines to verify that they can obtain researcher a s results the more scientists able to reproduce the analysis in their own environment the more confidence researcher a has that her work has reached what peng 2011 calls the reproducibility gold standard 5 once researchers have been able to reproduce the analysis they may now wish to replicate it by using the same analysis but with their own data sets because they were first able to reproduce the study on their own they can be more confident in reusing the analysis to build from past work and advance their own research 2 3 leveraging advances in cyberinfrastructure to create reproducible workflows the third objective of this study was to effectively leverage cyberinfrastructure to reduce the burden scientists have in creating reproducible analyses achieving reproducibility remains time consuming and tedious due to a gap in technology recent advances in the broader information technology field can be used to assist researchers in better documenting and sharing code data metadata and building a virtual environment to achieve reproducibility some scientists have begun using these new tools and approaches with the aim of making their research more reproducible and they have discussed the reproducibility of their research in their published articles ivie and thain 2018 sadler et al 2018 woodson et al 2018 in these studies online data repositories such as github figshare zenodo and hydroshare are becoming more commonly used by scientists to describe and share their data and code many of these repositories provide the ability to publish digital resources through their system assigning a digital object identifier doi to the resource that can be used to uniquely identify that resource in perpetuity containers and virtual environments are still uncommon tools in scientific studies perhaps because of their steeper learning curve for use in computational modelling and analysis studies while there are a growing number of tools available to scientists to make their work more reproducible and replicable we focus on two specific systems 1 the sciunit tool http sciunit run and 2 the hydroshare online repository http www hydroshare org the sciunit tool is software that enables researchers to create virtual environments for reproducing their computational analyses with minimal user interaction sciunit advances the concept of a research object which is an automatic aggregation of digital artifacts such as code data scripts and temporary experiment results that together with any research paper provide an authoritative and far more complete record of a piece of research bechhofer et al 2013 hydroshare is an open repository for sharing hydrologic data and models as digital resources including detailed hydrologic specific resource metadata horsburgh et al 2015 tarboton and idaszak 2015 we showed in prior work how combining these two tools sciunit and hydroshare can improve reproducibility for computational analyses essawy et al 2018 we showed how sciunit can be used to create containerized virtual environments but lacks mechanisms for sharing them within a community of users hydroshare on the other hand allows for sharing codes data and descriptive metadata but it does not address the challenge of packaging virtual environments for reproducing complex computational analyses building from this research here we extend this work to the more complete taxonomy for describing repeatable runnable reproducible and replicable studies illustrating how an environmental modelling study advances along this taxonomy and how tools like sciunit and hydroshare enable studies to move from being repeatable to runnable then reproducible and finally replicable applying the general five step procedure outlined in the prior section the sciunit tool is used to capture encapsulate and make the model execution portable by creating sciunit containers while the hydroshare repository is used to share the resulting sciunits and other key digital resources in the analysis more specifically the sciunit tool is used to automatically containerize or package and share the virtual environment to hydroshare as a sciunit and then a hydroshare compute platform such as cuahsi juypterhub or cybergis jupyter for water is used to interact with the shared sciunit through a jupyter notebook fig 4 these jupyterhub implementations because of their ability to read and write data from and to hydroshare provide a powerful analysis and model execution environment bandaragoda et al 2019 the advantage of this methodology is 1 it reduces the time necessary to download install and run the workflow on local machines 2 scientists don t face dependency issues such as missing or out of date dependencies and 3 it avoids local computing restraints that scientists face such as limited memory or storage 2 4 example application using the summa hydrologic model an example application is presented to illustrate how the five step procedure presented in the prior section can be applied with sciunit and hydroshare for an environmental modelling analysis the example uses the summa hydrologic modelling framework to simulate different hydrologic hypotheses and show how to reproduce the model simulation process clark et al 2015a 2015b the summa structure enables users to implement different modelling approaches with controlled and systematic analysis and provide insight for the advanced unified modelling framework fig 5 shows the summa model construction and process flexibility the summa structure consists of a core solver with outer branches and produces a numerical solution using conservation equations for estimating water and energy flux and state variables the summa model provides flexibility to evaluate the interplay between the choice of model parameters and the choice of process parameterizations separating modelling decisions on process representation from their numerical implementation and providing capabilities to experiment with different numerical solvers building from four synthetic and nine field study test cases of summa described in clark et al 2015b we selected and automated two of the test cases 1 field data test case 5 snow interception at umpqua and 2 field data test case 7 sensitivity of evapotranspiration to the stomatal resistance parameterization aspen stand at reynolds mountain east these two test cases are described in clark et al 2015b and the digital resources needed to reproduce the paper are available online however the test cases were not reproducible as defined by the national academies report because the conditions of analysis were not completely described and documented we demonstrate in the results section how following the five step procedure described earlier and using sciunit and hydroshare one can move the summa analysis from being repeatable to being runnable and finally to being reproducible and replicable 3 results 3 1 taxonomy for reproducibility in environmental modelling the taxonomy resulting from this work organizes the reproducibility spectrum into four levels repeatability runnability reproducibility and replicability fig 6 these levels represent a progression where the base level repeatability is the first step to achieve followed by runnability reproducibility and finally replicability the terms reproducibility and replicability are consistent with those proposed in the national academies report we argue that the two lower level concepts repeatability and runnability are needed as precursors to correctly capture the steps along the path to reproducibility and replicability the four levels are defined below and illustrated more fully later in this section using the summa model as a case study repeatability is achieved upon obtaining consistent results using the same input data computational steps methods and code on the original researcher s machine this level is normally achieved in scientific papers the author is able to rerun the analysis on his or her own computer to obtain the analysis results however it does require detailed documentation and benefits greatly from automation of steps to make repeating the analysis in a consistent way possible runnability is achieved when the author of the research can obtain consistent results using the same input data computational steps methods code and conditions of analysis on a new machine achieving this level requires thought and care to document the conditions of the analysis meaning software dependencies and related details of the computational environment in such a way that the analysis can be repeated correctly in a new computing environment reproducibility is achieved when a new researcher not an original author of the analysis is able to reproduce the analysis in their own computational environment achieving this step shows that one is able to achieve the national academies report definition of reproducibility obtaining consistent results using the same input data computational steps methods and code and conditions of analysis replicability is the higher level concept defined in the national academies report as obtaining consistent results across studies aimed at answering the same scientific question each of which has obtained its own data replicability also allows scientists not involved in the original study to build from and expand on research once they are first able to reproduce that research note that the concept of results being consistent used in these definitions adopted from the national academy report is less rigid than exact numerical equivalence rather it involves some degree of evaluation as to how close is good enough acknowledging numerical differences that arise on different platforms this concept is addressed more fully in the discussion section of the paper 3 2 modelling example case study repeating the summa analysis in this section we present a modelling example case study that steps through the four levels of the taxonomy using the general methodology described in section 2 2 applied for the specific environmental modelling use case described in section 2 4 that leverages the summa model the first step is for the author to repeat an analysis on her own machine and obtain consistent results we did this for test case 7 sensitivity of evapotranspiration to the stomatal resistance parameterization aspen stand at reynolds mountain east described in clark et al 2015b for this test case we created an automated analysis using the summa python wrapper software pysumma choi et al 2018 to model the sensitivity of evapotranspiration to the stomatal resistance parameterization for the aspen stand at the reynolds mountain east study site we installed the summa model by following the steps in the summa installation documentation https summa readthedocs io en latest installation summa installation after we installed the summa model we developed a workflow using pysumma to model the impact of stomatal resistance parameterizations on total evapotranspiration fig 7 shows the script used to set up and run the test case 7 summa model simulation using pysumma and create a visualization of the model this script shows how a summa model can be initialized how the model configuration can be adjusted with single lines e g setting the stomata resistance function to jarvis how summa can be executed and how the results can be visualized all from a single python script while these steps can be completed without pysumma through manipulating input files for a summa model directly pysumma acts as a wrapper for these text files and as a means for documenting a model configuration for a specific experiment the result from running the script shown in fig 7 shows the plot generated from the analysis this can be compared to the plot that appears in clark et al 2015b to gauge if the results are consistent in this case consistent results were achieved and therefore we can conclude that the analysis has reached the repeatable stage we are assuming for the sake of this paper that we are playing the role of the original author in this study as the original paper did not provide details on repeatability 3 3 making the analysis runnable using sciunit and cuahsi hydroshare jupyterhub once the analysis was repeated the next step was to make the analysis runnable on a separate machine to do this we made use of the sciunit tool to package the analysis along with its dependencies and provenance metadata fig 8 shows the steps to package the workflow analysis using the sciunit tool these steps are 1 create a new sciunit myanalysis this will create a virtual directory which will include the captured execution of the computational workflow with all the dependencies and provenance metadata associated with it 2 open the myanalysis sciunit to begin working in the desired sciunit 3 execute the code required to be packaged as a virtual environment in order to repeat the analysis 4 show the details of the virtual environment encapsulated in a sciunit package 5 place the packaged sciunit on hydroshare as a digital resource and 6 test the runnability of the package by executing the sciunit on the cuahsi hydroshare jupyterhub app linked to hydroshare and configured to open and execute scripts acting on content from resources in hydroshare note to run a sciunit again requires the sciunit tool which is installed on cuahsi hydroshare jupyterhub the initial test of this execution by us playing the role of the original author on the separate jupyterhub app computer demonstrating runnability is detailed in the following paragraph once the sciunit is shared on hydroshare users of hydroshare can interact and execute the shared sciunit from hydroshare using jupyterhub thereby achieving reproducibility doing so would achieve the reproducibility gold standard peng 2011 the cuahsi hydroshare jupyterhub app is a deployment of jupyterhub hosted at http jupyter cuahsi org that is part of the overall hydroshare cyberinfrastructure linked to but separate from the hydroshare repository www hydroshare org jupyterhub enables literate programming an approach that describes the written program so that the user can understand it piccolo and frampton 2016 literate programing allows the researcher to split the code into fragments that can be executed independently and it enables the researcher to organize the fragments in a way that is easily understandable jupyter gives scientists the capability of combing data and visualizations in powerful ways when communicating their research p√©rez and granger 2007 scientists use jupyter notebooks as a supplement to the published manuscripts so that others can replicate their analysis and re generate the published results for example sadler et al 2018 took a step toward creating reproducible research in the field of hydrology by publishing all codes and data on hydroshare additionally sadler et al 2018 used the jupyter functionality within hydroshare to create notebooks for some of the scripts used in his study and he shared these notebooks with other researchers so they could interact with the notebooks using jupyter step 5 resulted in a hydroshare resource named myanalysis fig 9 choi 2020 to run this in jupyterhub the user would first click the open with button from the resource s landing page in hydroshare and select cuahsi jupyterhub this step will open an instance of jupyterhub where the user can reproduce the workflow saved in the sciunit package fig 10 shows these steps for reproducing the analysis with commands issued through the jupyter user interface the user must issue sciunit commands by using the expression in jupyter the sciunit open myanalysis command will open the sciunit the sciunit show command will show the latest packaged experiment within myanalysis sciunit while the sciunit list command will list the all the packaged experiments within myanalysis sciunit finally the sciunit repeat e1 command will rerun the experiment 1 e1 analysis on the host machine examining the plot generated validates that results are consistent with the original work and demonstrates runnability 3 4 reproducing the summa analysis using sciunit and hydroshare once runnability has been validated the original researcher can make the hydroshare resource holding the sciunit package public and accessible to other interested researchers it can also be permanently published and be given a citable doi a new researcher can then access this resource to reproduce the analysis one way the new researcher can do this is to also use the cuahsi hydroshare jupyterhub app essentially repeating the validation of runnability done by the original author above this is however only a weak demonstration of reproducibility because it uses the same environment cuahsi hydroshare jupyterhub where runnability was tested the following steps describe how other researchers can reproduce the analysis on their own computer using the sciunit package shared on hydroshare a stronger test for reproducibility these steps assume the new machine has sciunit installed but make no additional assumptions of software availability on the new machine 1 download the sciunit resource from hydroshare 2 navigate to the location where the sciunit resides 3 use the command line to unpack and initiate the selected sciunit using sciunit open 4 use the command sciunit list to see what packages are within the sciunit and which package will be used in the process since some sciunits may contain multiple packages for example sciunit may use the same datasets but employ different methods 5 repeat the e1 package to generate the analysis 6 once the analysis is repeated a directory will be created with a copy of the analysis and all associated files examining the plot generated validates that results are consistent with the original work and demonstrates reproducibility 3 5 replicating the summa analysis once the user achieves reproducibility following the steps outlined in the prior section the user is now in a position to replicate the work by modifying the sciunit package and running it using a new dataset or code aimed at answering the same research question using the different data or exploring whether a different model or approach to the analysis produces results that support the conclusions a different implementation of the model here may be new code but the same underlying equations or principles and this would serve to evaluate whether the code and solvers actually implement the equations properly while replicability could be achieved without first reproducing another s work we suggest that in many cases the path to replicability is best traveled by first being able to reproduce past work this puts a researcher in a better position to interpret any differences that may arise and at the same time be more efficient and take advantage of and reuse code from the original researchers the steps for achieving replicability within cuahsi hydroshare jupyterhub using sciunit build off of the steps for reproducibility once the five steps needed for reproducing an analysis have been completed the next steps are 1 navigate to the directory that includes the analysis created and edit the script to use different data or a different approach fig 11 and 2 the user would commit changes and repeat sciunit execution package e2 with the sciunit given command as shown in fig 12 after completing these steps the user has replicated the study by creating a copy of a past analysis changing that analysis s input data and rerunning the analysis using the new input file because all software dependencies are handled by the sciunit tool the analysis will be reproducible on any machine with a compatible os and one that has the sciunit client software installed this same process could be repeated outside of the cuahsi hydroshare jupyterhub environment following a similar procedure to the one described in section 3 4 4 discussion 4 1 defining and assuring consistent results the national academies report s definition for reproducibility includes the phrase consistent results acknowledging that achieving reproducibility does not require obtaining the exact same results as the original study very often in computational modelling numerical differences due to a variety of factors including rounding and precision mean that the output from an analysis does not have to be exactly the same as prior runs of the analysis but it should be close enough furthermore some computational modelling strategies are stochastic so achieving the exact result from a prior run is impossible obviously close enough is a relative measure that requires expert judgment based on the specific requirements of the study and the tolerance for differences in numerical results for this reason it is a more difficult standard in some ways that achieving the exact results from a prior analysis the consistent results requirement also points to the complicating factor that it is not necessary to have an exact recreation of the computational environment in order to achieve reproducibility thus while we made the claim at the start of this paper that software dependency differences could be the reason for an inability to reproduce a prior computational study see fig 1 this may not always be true for example consider the case illustrated in fig 13 which presents a more detailed view of the case described in fig 1 in this more detailed view two researchers a and b have slight differences in their computational environments yet runnability can still be achieved if the results from researcher b s setup different libraries and operating system can produce results that are close enough to researcher a s results there is risk here however just because the results are close enough for one run reproducibility there is no guarantee that a slight change in the input data could still produce results that are close enough for some other run replication however there is value in allowing for software inconsistencies because if setup b is close to the setup a setup b has migrated the model to later versions of dependency software there clearly needs to be the ability to move to new software versions while also achieving reproducibility along the way at the same time it is known that some software mitigations like the migration from gdal v2 to gdal v3 illustrated in fig 13 can cause breaking changes for example even though the coordinate system is basic functionality in geospatial software upgrading to gdal v3 changed the behavoir of the exporttoproj4 function which is the function used to specify the coordinate system for both raster and vector data to address and mitigate breaking changes such as this introduced by dependency updates it is important that specific dependency versions be well documented and that test data be available for a new researcher to ues to test for reproducibility in migrating to an updated computational environment it is also important for container technology such as sciunit to encapsulate the dependencies used by the original researcher to aid in migration and facilitate reproducibility by the new researcher 4 2 limitations and remaining challenges while computational resources data code and environment are important these alone are insufficient to ensure reproducibility the researcher must also include documentation and metadata about the software they use in order to run their programs correctly documentation in the journal article itself is often lacking and must be accompanied with user manuals or other resources including jupyter notebooks that more fully describe the computational analysis metadata is needed to uniquely identify computational resources within the growing ecosystem of data and software while metadata for data is well established recent efforts have been made towards capturing software metadata ontosoft is an example that provides an ontology and portal for addressing the challenge of capturing metadata for scientific software in a formal way gil et al 2016 2015 in hydrology the hydroshare system can be used to describe metadata for data and models hydroshare defines two key computational modelling concepts a model program and a model instance the model program is the software for executing the model and the model instance is the input files required for executing the model horsburgh et al 2015 morsy et al 2017 2014 tarboton et al 2014 this study focuses on studies where input data have already been prepared for an analysis and all required data for running the analysis are stored locally i e there are no references to external data or services once this is the case the remaining goal is to perform model simulations and analyze the model output as described in this paper following this work a next step needed to achieve reproducibility in complex computational studies is to have workflows that automate the end to end process of reproducing the analysis from raw data to publication ready figures and tables the general workflow should automate the steps to 1 obtain the raw data published in an online repository preferably from a published immutable external resource with a doi for reproducibility 2 run one or more scripts needed to prepare the raw data for the model 3 run the model using the prepared datasets and 4 post process the model output to generate publication ready figures and tables this four step workflow should be containerized into a virtual environment that can be repeated by other researchers fig 14 shows the proposed approach to achieve reproducible research some efforts have already been made to create generic processes to automate the larger workflow required for hydrologic modelling one example is work using the variable infiltration capacity vic hydrological model a macro scale hydrologic model that applies water and energy balances to simulate terrestrial hydrology at a regional level liang et al 1996 the vic model like many hydrologic models requires significant effort to prepare its input data billah et al 2016 demonstrated a single step process to create a workflow that automated the preparation of the input data for the vic while workflow software can help to better capture the provenance it is still important to have sufficient metadata for each step within the workflow essawy et al 2017 an example of workflows focused on metadata capture is work using modflow nwt as a demonstration model essawy et al 2018 the modflow nwt is a version of the united states geological survey s groundwater model modflow niswonger et al 2011 the work done for the modflow nwt because it leverages the sciunit concept provided through the geotrust project improves reproducibility in the way it guarantees the replicability of research because this workflow is automated the user is unable to change the parameters of the workflow execution and all data run through this workflow will be executed according to the same parameters specified by the automated workflow work is needed to merge these ideas and concepts into a complete end to end workflow for repeatable runnable reproducible and replicable environment modelling that stretches from data preparation to model execution to post processing of model outputs all containerized well documented with appropriate metadata easy to understand and reuse and discoverable through online data repository systems 5 conclusions this research has shown a path to achieving reproducibility and replicability of environmental modelling studies by first providing a more formal taxonomy that defines studies as being repeatable runnable reproducible or replicable after defining this taxonomy we focused on a methodology that uses this taxonomy in describe steps needed to move research along the spectrum from being repeatable to being replicable using a hydrologic modelling analysis as an example we demonstrate these steps and highlight the important role of containers and software tools that enable scientists to use containers in achieving reproducible and replicable studies the hydrologic modelling analysis demonstrating this process uses the summa modelling framework and different parameterizations of stomatal resistance to estimate total evapotranspiration for a study site we demonstrated how to achieve the steps along the taxonomy with existing cyberinfrastructure software tools as follows 1 repeating an analysis on the same computer with the same inputs as its original application 2 running the same analysis on another machine and getting consistent results by packaging it as a container using the sciunit software 3 reproducing the analysis by sharing it to hydroshare and allowing other researchers to re execute it through the literate programing jupyterhub 4 replicating the analysis by allowing researchers to easily change the shared sciunit package to use their own data or modelling ideas and then repeating the 1 4 sequence of steps while this work moves closer to achieving reproducible and replicable environmental modelling studies there are still remaining limitations and challenges to be addressed first it is difficult to know precisely when an analysis has been reproduced because achieving this goal does not require an exact match of the computational environment but rather requires a similar computational environment that is perhaps some dependences have been updated to new versions but it is essentially the same computational environment and set up therefore the results may not be exactly the same but are consistent with results in the prior analysis run creating automated tools for judging this potentially vague criteria of consistent results for reproducibility remains a challenge literate programming tools like jupyter are a major step forward in providing documentation alongside analysis but more work is needed to better document computational analyses in ways that foster reproducibility and replicability this is especially true for efforts to document and automate end to end workflows that operate from raw input data create model input files execute models and produce publication ready figures capturing these entire end to end workflows in a way that can be not only reproduced and replicated by others but also easy to understand and reuse remains an open challenge software availability the analysis example illustrated in this research is available free and open source under an mit license from github at https github com uva hydroinformatics pysumma sciunit declaration of competing interest the authors have no affiliation with any organization with a direct or indirect financial interest in the subject matter discussed in the manuscript acknowledgements we gratefully acknowledge the national science foundation for support of this work under awards icer 1639759 icer 1661918 icer 1540901 and oac 1664061 
25931,increasing volumes of data allow environmental scientists to use machine learning to construct data driven models of phenomena these models can provide decision relevant predictions but confident decision making requires that the involved uncertainties are understood we argue that existing frameworks for characterizing uncertainties are not appropriate for data driven models because of their focus on distinct locations of uncertainty we propose a framework for uncertainty assessment that uses argument analysis to assess the justification of the assumption that the model is fit for the predictive purpose at hand its flexibility makes the framework applicable to data driven models the framework is illustrated using a case study from environmental science we show that data driven models can be subject to substantial second order uncertainty i e uncertainty in the assessment of the predictive uncertainty because they are often applied to ill understood problems we close by discussing the implications of the predictive uncertainties of data driven models for decision making keywords uncertainty data driven models argument analysis predictions decision making 1 introduction real world policy decisions are taken in light of great uncertainties several aspects of a decision situation can be uncertain including the problem framing the available options from which to choose the actual or potential states of the world and the values that decision makers attach to these states of the world see bradley and drechsler 2014 hirsch et al 2015 hansson and hirsch hadorn 2016 in this paper we are concerned with uncertainty about the actual or potential states of the world often this uncertainty is related to uncertainty of scientific information on which a decision is to be based this information relates to what the world will be like either in the form of a prediction of environmental conditions such as a severe weather forecast or in the form of a conditional prediction so called projection of environmental conditions in response to a policy measure such as climatic conditions in response to a certain socioeconomic pathway and associated greenhouse gas emissions in order to take decisions under uncertainty the uncertainty should be analyzed and if possible quantified an important part of the uncertainty analysis is the characterization of what kind of uncertainty emerges because of which features of the research process only such a thorough treatment of uncertainties can help to ensure that societal decisions are based on no more and no less than what one actually knows betz 2016a 138 recent years have seen large increases in data produced and stored this trend is also apparent in the sciences in general and in the environmental sciences in particular e g in climate sciences see overpeck et al 2011 the increase in the availability of data about environmental systems enables the use of machine learning to analyze the data and to construct data driven models of phenomena see gibert et al 2018a 2018b numerous applications of machine learning can be found in the environmental science literature see e g reichstein et al 2019 in this paper we will discuss applications of machine learning for the data driven modeling of a phenomenon in contrast to a process based model in which the relationships between variables are prescribed in the form of equations specified by an expert a data driven model is constructed by algorithmically inferring the relationships or parameters from a dataset using machine learning for a more detailed distinction of process based and data driven models see kn√ºsel and baumberger 2020 forthcoming if predictions from data driven models are reliable which seems to be the case at least under certain conditions see pietsch 2015 northcott 2019 these predictions are potentially useful for decision making related to the modeled phenomena one of the advantages of data driven models is that they can be constructed when the processes producing a phenomenon are not fully understood hence they might provide decision relevant information specifically about phenomena which are quantitatively not understood well enough to construct process based models however no conceptual tools to appropriately evaluate data driven models in terms of their uncertainties are available to date which reduces their usefulness for decision making here we introduce such a tool that can complement existing quantitative methods to explore the uncertainty of predictions made with data driven models the remainder of this paper is organized as follows in section 2 we argue that existing frameworks for the characterization of the uncertainties of model based predictions are not appropriate for predictions from data driven models we hence introduce a new more general approach in section 3 it focuses on the justification of the assumptions underlying a prediction that is possible based on the available data and background knowledge the assumptions that need to be justified include the fitness for purpose of the used model in sections 4 and 5 we demonstrate the application of this framework to a toy example and to a case study from environmental science in section 6 we discuss the implications of this framework for the quantification of uncertainties and for decision making more generally we conclude in section 7 before proceeding we note here that in the present paper we do not distinguish between pure predictions of environmental conditions such as a forecast of severe weather conditions and predictions of the environmental response conditional on a human intervention such as climatic conditions in response to a certain path of greenhouse gas emissions the approach presented here is fairly general and can be applied in both situations equally 2 existing frameworks existing frameworks for model based decision support and specifically uncertainty analysis have been developed for process based models an influential framework to analyze uncertainties of model based predictions is due to walker et al 2003 it distinguishes three different dimensions of uncertainty that are arranged as an uncertainty matrix namely the location where in the modeling complex does the uncertainty manifest itself the nature is the uncertainty due to the inherent variability of the phenomenon or due to imperfect knowledge of it and the level of uncertainty how severe is the uncertainty ranging from complete certainty to complete ignorance several variations of this matrix have been developed see e g refsgaard et al 2007 and versions of it have been applied in different contexts see kwakkel et al 2010 the locations of uncertainty introduced by walker et al 2003 are the context the model structure and the technical model the driving forces and the system data the parameters and finally the model outcomes which obtain their uncertainty from the preceding locations kwakkel walker and marchau 2010 have synthesized adaptations and criticism of the uncertainty matrix introduced by walker et al and have amongst other things suggested that some of the dimensions including the locations of uncertainty be rearranged similar locations of uncertainty have been discussed in specific disciplines for climate models for example knutti 2018 suggests that the relevant locations of uncertainty are model structural uncertainty numerical approximations parameterizations natural variability due to initial conditions the emission scenario boundary conditions and observational data uncertainty all of which have an analog in the locations discussed above similar locations of uncertainty have been discussed for climate models by other authors winsberg 2018b chap 7 what the approaches presented above have in common is that they suggest that different model related aspects the locations of uncertainty are investigated in order to characterize the uncertainty of the model results namely at each location aspects should be identified that are either intrinsically or epistemically uncertain i e uncertain due to system properties aleatory uncertainty or due to our imperfect understanding of the system epistemic uncertainty and the level of this uncertainty should be determined then this uncertainty is propagated in order to characterize the uncertainty of the processed model outputs thinking about uncertainty in terms of specific locations is not informative for data driven models for three reasons 1 1 we note that in a given application of a model for decision support an additional problem can be that there are many more locations of uncertainty which are not directly related to the model but rather to contextual factors such uncertainties are best addressed e g by stakeholder engagement for an extensive overview of this point for water resource management see badham et al 2019 as the focus of this paper lies on uncertainty that is directly model dependent we will not further discuss additional locations or sources of uncertainty we thank a reviewer of this journal for raising this issue the first reason is that for many data driven models the model structure cannot readily be tied to processes in the target system or be interpreted in terms of the target system yet model structure is one of the locations of uncertainty common to the established frameworks this is especially obvious for models based on neural networks or bagging approaches like random forest while neural networks have a model structure consisting of a number of neurons and layers this structure cannot be interpreted in terms of the target system in a straightforward way for bagging approaches like random forest it is unclear what the relevant model structure would be because they make predictions based on the average of multiple individual estimators due to these issues considering the model structure as a location of uncertainty is likely not helpful to assess the predictive uncertainty of data driven models for process based models structural model uncertainty arises because different model specifications might seem equally plausible and it is unclear how to best represent the target system for a specific purpose this representational uncertainty is related to predictive uncertainty see parker 2010 the best model setup can be more radically underdetermined in the case of data driven models since a very large number of model setups can be consistent with the available training data hence the best model setup for a specific purpose has to be chosen based on background knowledge as data driven models are often employed when processes are ill understood see kn√ºsel et al 2019 the model setup of data driven models and the resulting representational uncertainty of a target seem relevant for an analysis of predictive uncertainty thus even though the model structure if there is an accessible model structure cannot directly be tied to the target system an approach to assess the predictive uncertainty of data driven models needs to be able to consider the representational uncertainty of the model at issue however it is unclear how exactly representational uncertainty and predictive uncertainty are related for data driven models the second reason existing approaches are not informative for understanding the predictive uncertainty of data driven models is that similar to the model structure model parameters are not an obvious location of uncertainty either for many data driven models some machine learning approaches like random forest are non parametric and hence this location of uncertainty is simply not defined for them such non parametric approaches do have meta parameters e g the number of trees or the number of considered variables at every split in a random forest model but these again cannot readily be interpreted in terms of the target system other machine learning approaches like deep neural networks can have an overwhelming number of parameters as for the model structure many of the model parameters have no obvious interpretation in terms of the target system and the uncertainty in each parameter does not directly translate to predictive uncertainty in the same sense thus thinking about model parameters as a location of uncertainty is not helpful for data driven models either the third reason is that machine learning is at least sometimes preoccupied mainly with good predictions and less with the reasons for these predictions but as we will see below for some predictions the reasons for the predictions can matter too a framework for assessing predictive uncertainties of data driven models should thus reflect this purpose dependence of the representational character of the model hence existing frameworks for uncertainty assessments seem inadequate as conceptual tools for a discussion of the predictive uncertainty of data driven models while it can be helpful to think about uncertainty in terms of different sources whose uncertainty can be considered additive for data driven models new conceptual tools are needed to analyze relevant sources of uncertainty from the above discussion two requirements become clear for an analytic framework for the predictive uncertainties of data driven models first such a framework needs to do justice to the fact that under some but not all circumstances a data driven model has an instrumental character in the sense that modelers are preoccupied only with the predictive success of data driven models and not with the reasons for this success such cases are typically characterized by model evaluation that focuses solely on performance metrics such as the cross validation error second in cases in which a modeler is not only preoccupied with the predictions but also with the reasons for predictive success the representational accuracy of data driven models needs to be assessed this is so even though as we have argued above neither the model structure nor model parameters can readily be interpreted in terms of the target system at least not for models with many degrees of freedom in these cases model users will be forced to adopt a more realistic interpretation of the model meaning that they need to assume that the model behaves in a way that is consistent with the actual mechanisms of the target system even if the model structure and parameters cannot be interpreted in terms of the target system specifically this means that the framework needs to be able to assess to what extent the behavior as opposed to the structure or the parameters of the data driven model is coherent with background knowledge and to what extent it is possible to argue from the coherence with background knowledge to representational accuracy 2 2 we note here that evaluating model behavior in order to assess the skill and the uncertainty of models is not unique to data driven models it is also done for process based models this has been discussed for example for climate models by lloyd 2009 who emphasizes the importance of model behavior e g the simulation of the tropopause for establishing confidence in model results and for environmental modeling more generally by hamilton et al 2019 under the term stylized facts we thank a reviewer of this journal for raising this issue uncertainty has also been a topic in the computer science literature on machine learning to the best of our knowledge the literature on uncertainties in machine learning has mainly discussed the role of bayesian approaches for quantifying uncertainty see blundell et al 2015 ghahramani 2015 gal and ghahramani 2016 kendall and gal 2017 have explicitly distinguished between epistemic and aleatory uncertainty in the context of bayesian deep learning for computer vision a distinction we will take up in later sections these bayesian approaches are certainly useful for some contexts such as computer vision and for cases in which uncertainty can be characterized easily because it mainly comes from certain sources such as noisy data however for the present purposes they seem insufficient as a basis of the uncertainty assessment of data driven environmental models as they are unable to account for uncertainty not present in the available dataset which becomes important when a relatively realistic interpretation of the model is adopted however we will return to how these methods could complement the ideas presented in our framework in section 6 3 an argument based framework for uncertainty analysis above we have argued that a framework for analyzing the predictive uncertainty of data driven models needs to do justice to the fact that some but not all data driven models have a purely instrumental character hence the framework necessarily has to be more general than existing frameworks in order to allow for different representational characters in different contexts here we put forward such a framework the framework suggests to analyze uncertainties in three steps the first step consists in reconstructing what assumptions have to be made when using a model for a specific purpose and how these assumptions can be justified the second step consists in evaluating how well justified the assumptions are the third step consists in assessing the uncertainty based on the previous two steps the basic assumption that has to be justified in any modeling application is an assumption regarding the fitness for purpose of the model used as parker 2009 has argued for climate models the goal of model evaluation should not be to confirm the model itself generally but rather to confirm that a model is adequate for a specific purpose in this paper we use the term fitness for purpose which in contrast to adequacy for purpose admits of various degrees of fitness meaning that models are not just fit for purpose or not but fit for purpose to a larger or smaller degree see parker 2020 forthcoming we take a model to be maximally fit for a specific predictive purpose if it can predict the variable of interest reliably with errors always lying in some small range which is determined by the inherent variability of the target system on this way of thinking as the fitness for purpose increases the model skill converges toward the theoretical limit of predictability of the system as described by the model i e with a certain resolution etc an example of such a fitness for purpose assumption would be model m is fit for predicting the total precipitation amount of the next 24 h at location l with errors in some small range according to the argument based framework epistemic uncertainty arises due to a lack of epistemic justification thus if it can be conclusively justified that the model is maximally fit for purpose in this sense the model predictions exhibit no epistemic uncertainty and the remaining uncertainty of the predictions arising from the small range of errors is aleatory in nature specifically the above considerations reveal three ways in which model predictions can be epistemically uncertain namely according to the framework presented here epistemic uncertainty arises because of known factors that result in less than maximal fitness for purpose factors that make the justification non conclusive or factors that make it unclear what degree of fitness for purpose a model has essentially all of these three kinds of factors increase the range around model predictions in which the actual state of affairs is expected to lie thus they are sources of epistemic uncertainty this conceptualization of source of uncertainty is consistent with guillaume et al 2015 101 who define a source of uncertainty as the origin of an uncertainty in the process to acquire and use knowledge in the following we will introduce the three steps of the framework in more detail the first step is the reconstruction of the assumptions and their possible justifications in order to do this the following two questions have to be addressed first what modeling assumptions does a prediction rely on second how can these assumptions be justified as noted above the basic assumption is a fitness for purpose assumption concerning the model that was used to make a prediction depending on the circumstances the fitness of the model for the predictive task at hand will be justified differently if the justification is itself partly based on assumptions that require further justification the two questions above need to be addressed several times in order to assess how well the basic assumption is justified once this is done an argument map for an introduction to argument maps see betz 2016b can be drawn in which it becomes apparent which arguments and assumptions justify the fitness for purpose assumption and how different arguments and theses are related an argument map is shown in fig 1 which is discussed in the case study in section 5 and essential elements of argument maps are introduced in the appendix in the second step based on the argument map from the first step or in simpler cases based on a direct assessment of the arguments it is evaluated how well all the different assumptions and specifically the fitness for purpose assumption are justified and based on that what the actual degree of fitness for purpose is for this it is relevant to check whether the premises used to justify a conclusion are true 3 3 we note here that in many practical situations evaluating the premises in terms of approximate truth may suffice in such circumstances the premises and conclusions could be qualified to allow for approximate truth and whether they provide sufficiently good reason to accept the truth of the conclusion specifically the evaluation step may reveal that some of the premises are strictly speaking false in these cases it should be evaluated whether the premise may still be close enough to the truth to support the conclusion furthermore premises may be identified of which we do not know whether they are true finally the evaluation step highlights non conclusive justifications in the complex argumentation arising from non deductively correct arguments see below here the strength of the justification needs to be assessed based on domain specific background knowledge to what extent the premises provide good reasons to accept the truth of the conclusion depends on whether the argument that is being evaluated is deductively valid or non deductively correct whenever possible arguments should be reconstructed as deductively valid arguments i e arguments for which the following condition holds if all the premises are true then the conclusion must be true this way the assumptions become more explicit which also makes more explicit why these assumptions may act as sources of uncertainty as will be assessed in the third step an example of a deductively valid argument is the following this example is taken from baumberger et al 2017 7 if a model is adequate for projecting x for the far future then the model reliably indicates x for past and present model m does not reliably indicate x for past and present hence m is not adequate for projecting x for the far future some arguments cannot easily be reconstructed as deductively valid e g because it can be unclear which premises would be appropriate to obtain a plausible deductively valid argument in these cases the arguments can be reconstructed as non deductively correct i e as arguments that provide sufficiently strong but not conclusive reasons for the truth of the conclusion non deductively correct arguments are risky in the sense that even if all of their premises are true the truth of the conclusion is not guaranteed an example of a non deductive argument is the following this example too is taken from baumberger et al 2017 7 model m reliably indicates x and climate quantities upon which x depends for past and present so probably m is adequate for projecting x for the near future finally the third step consists in an assessment of the uncertainty based on the previous step of evaluating the justification of the fitness for purpose assumption by analyzing the possible epistemic justification of assumptions the focus of the framework presented here is on epistemic uncertainties 4 4 note that the justifications always have to rely on arguments that can be constructed from what is known hence the framework introduced here is not helpful to identify uncertainty arising from unknown unknowns the risk of such genuine surprises should especially be considered if a system is complex ill understood if scientists know that they are overconfident if surprises have occurred in the past or if the system is subject to new conditions as parker and risbey 2015 have argued in practical applications of the framework outlined here arguments for the fitness for purpose could be made that aim to rule out the risk of surprises by making explicit references to these five factors depending on how well they are justified they would be additional sources of uncertainty that explicitly account for the risk of surprises the framework distinguishes between two types of epistemic uncertainty that we refer to as first order uncertainty and second order uncertainty these two types of uncertainty are distinguished by their objects first order uncertainty is the epistemic uncertainty of the prediction consequently first order uncertainty is defined here as the extent to which the assumptions underlying a prediction are insufficiently justified thus epistemic first order uncertainty arises if it cannot be conclusively justified that the model is maximally fit for purpose second order uncertainty is defined here as the extent to which the assessment of first order uncertainty is impaired by context specific factors specifically second order uncertainty depends on difficulties in assessing to which extent the assumptions are justified e g because of a lack of evidence contradictory evidence or expert disagreement thus epistemic second order uncertainty arises if the degree of fitness for purpose cannot be conclusively determined the assessment of these two types of uncertainty is based on the evaluation of the argumentation from the second step as will be illustrated below based on the framework the expressions of first order and second order uncertainty will be purely qualitative however in section 6 below we address the questions of uncertainty quantification which is desired for many applications 5 5 we note here that uncertainties of higher orders could be defined in analogy to second order uncertainty for example third order uncertainty could be defined as the uncertainty of the assessment of second order uncertainty as the discussion below will highlight second order uncertainty is an important concept to better understand the epistemic uncertainty of model predictions uncertainties of higher order are likely irrelevant in practical applications before we proceed to illustrating the application of the framework three clarifications are in order first in practical applications the fitness for purpose of a model will not only depend on its predictive accuracy further considerations such as practical concerns can play a role for example ease of use or computational cost see e g haasnoot et al 2014 111 however in the present paper we are only concerned with the uncertainty of the predictions of a model and we will hence discuss fitness for purpose only as it relates to predictive accuracy second maximal fitness for purpose is defined in terms of model performance specifically we have defined a model as maximally fit for purpose if it has errors that always fall within a small range that is due to the internal variability of the target system in practice it can be difficult to specify how wide exactly this small range should be thus the specification of fitness for purpose can itself be a source of epistemic uncertainty if it is unclear how large the errors arising from variability of the target system are third note that if a model is found to be less than maximally fit for purpose this does not mean that the model is outright unfit for purpose whether a given degree of fitness for purpose is sufficient to consider a model outright fit or adequate for a specific purpose depends on the context for models that are evaluated only in terms of predictive performance a model might be considered outright fit for purpose if it provides information that was previously not available a model with a predictive skill that is far from perfect can still provide information that was previously not available thus it can be outright fit for purpose also when it is less than maximally fit for purpose see knutti 2018 for models whose fitness is evaluated in terms of criteria other than predictive skill alone there can be well known trade offs between these criteria for example a trade off exists for climate models between how many processes they represent and how intelligible they are for model users see held 2005 due to such trade offs a model that is maximally fit for purpose in this multidimensional sense cannot be constructed even in principle the identification and evaluation of how the assumptions are justified requires expertise from domain scientists concerned with the phenomenon at hand and from modelers and data scientists but also expertise in argument analysis an introduction to argument analysis can be found in brun and betz 2016 in the literature it has been recognized that an analysis of assumptions can be important for a better understanding of uncertainties see kloprogge van der sluijs and petersen 2011 however as will be shown below since the framework presented here is concerned with justifying the fitness for purpose it is not only concerned with assumptions that a modeler makes in the process of model construction instead it is concerned with assumptions that a model user is relying on at least implicitly when using a given model for a specific predictive purpose for example a model user may have to assume that certain processes are accurately represented in a data driven model however this assumption is not made explicitly in the process of building a data driven model as the relationships between variables are not explicitly prescribed see pietsch 2015 and no processes are therefore explicitly represented hence both choices made in the path cf lahtinen et al 2017 of model construction e g the choice of hidden layers in a neural network or of meta parameters in random forest and assumptions made when using the fully trained model e g regarding representational accuracy can be relevant for an account of the predictive uncertainty of a model 4 toy example for illustration in this section we discuss a simple toy example of a data driven model to illustrate how the framework can be applied our toy model is a random forest algorithm that is trained to predict the maximum daily air temperature at a given location with a lead time of one day for this the current air temperature and pressure at the location the season and an index on the general weather conditions are used as predictors imagine that the model predictions are successful and actually measured maximum daily air temperatures are always close to the predicted value within a small error range and we are able to repeatedly use the model and evaluate its performance we might generally wish to better understand the uncertainty of this kind of prediction in order to base decisions on them for example the prediction of an unusually cold or hot maximum temperature might be relevant for public health in order to obtain a better understanding of the uncertainty of the predictions we can apply the framework introduced in the previous section first the assumptions and their possible justifications need to be identified and graphically arranged as explained above the most fundamental assumption is a fitness for purpose assumption in this case the fitness for purpose assumption states that the model is fit for making predictions of the daily maximum temperature with a lead time of a day for a specific location up to some range how can this assumption be justified since we have used the model and evaluated the accuracy of its predictions repeatedly in the past this past performance can be used to justify the fitness of the model for the predictions at hand namely the model has predicted many past instances of maximum daily temperature accurately 6 6 implicitly this can be read as saying that the model does not make an extrapolation far outside the range of values for which it has been trained this justification can now be illustrated for example in a table as shown in table 1 this justification can be reconstructed as a deductively valid argument of the following form p1 if a model has predicted many past instances of tmax accurately and the conditions for the predictions remain sufficiently similar to the past instances m is fit for predicting tmax p2 m has predicted many past instances of tmax accurately with a lead time of one day p3 the conditions for the predictions remain sufficiently similar to past instances c m is fit for predicting tmax now that the argumentation is reconstructed we can proceed to the second step namely evaluating to what extent the fitness for purpose assumption is justified for the evaluation of the justification we need to assess whether the premises are true and to what extent they provide good reasons for the fitness for purpose assumption as the justification can be reconstructed as a deductively valid argument the conclusion must be true if the premises are true thus the evaluation consists in determining whether all premises are true p1 makes a conditional claim about the fitness for purpose of the model this premise we take it is uncontroversial the premise p2 is related to the evaluation of past predictions of the model as mentioned above the model has been extensively used in past cases and has been predictively successful thus p2 is true too 7 7 we note here that premises 1 and 2 contain vague expressions namely the terms many instances and accurately it may be unclear in practical applications whether sufficiently many instances have been considered in past model evaluation and whether the range in which errors fall is sufficiently small to consider model predictions accurately in real world applications quantitative thresholds may sometimes be difficult to set this is related to the difficulty we have acknowledged in section 3 regarding the range of errors of a model that is considered maximally fit for purpose we thank a reviewer for raising this point therefore the uncertainty of the predictions hinges on the truth of p3 the truth of p3 has to be justified based on domain specific background knowledge namely that the past cases are sufficiently representative to be confident about the models performance more generally for cases that are similar to the ones considered thus far the short lead time of the predictions makes it likely that p3 is true too for two reasons first on a practical level the short lead time allows for repeated evaluations of the predictions second and more fundamentally the short lead time increases the chance that the evaluated predictions of the past are representative of the current predictions because the system is less likely to have experienced large changes in boundary conditions over a short period of time 8 8 note that the argument could as well be reconstructed without premise p3 this would turn the argument into an inductive argument whose strength would have to be assessed based on the representativeness of the past cases we choose and recommend the deductive reconstruction as it makes the uncertainty more explicit hence the evaluation of the argumentation shows that the fitness for purpose assumption is justified by a deductively valid argument furthermore there are good reasons to assume that all of the premises of the argument are true hence the argument seems to be sound a sound argument is a deductively valid argument with true premises which means that there are good reasons to have confidence in the truth of the conclusion finally in the third step the epistemic uncertainty of the prediction can be assessed the first order uncertainty depends on the extent to which the fitness for purpose assumption is justified as we have seen the justification of the fitness for purpose here seems to be a sound argument meaning that the model can be considered close to maximally fit for purpose insofar as it has always predicted the temperatures to within a very small margin of error and to the extent that we are justified in having confidence in p3 this means that epistemic first order uncertainty is quite small or even absent in the toy example epistemic second order uncertainty is also small or even absent as it is straightforward to reconstruct and evaluate the argument for the fitness for purpose assumption there may be some second order uncertainty related to premise p3 if it is unclear to what extent the past performance is representative of future performance the justification of the truth of p3 fundamentally depends on the system understanding note that in this simple toy example the framework can also help to identify the aleatory uncertainty the reason for this is that the total uncertainty of the predictions can be estimated in a straightforward way based on the evaluated predictions since the epistemic uncertainty of the predictions can be reliably estimated as a result of the low level of second order uncertainty the aleatory uncertainty is simply the difference between total uncertainty and epistemic first order uncertainty in the present case aleatory uncertainty corresponds to the range that can be estimated from the small random errors of model predictions as kendall and gal 2017 have argued in machine learning aleatory uncertainty is best understood as uncertainty that cannot be reduced by collecting additional samples of data which is exactly the kind of uncertainty that remains here if sufficient volumes of data have been used for model construction and different modeling choices have been tested and none of the alternative choices was able to outperform the model setup in question then there are reasons to believe that the range of errors arises due to aleatory uncertainty thus to the extent that these conditions hold in the toy example the aleatory uncertainty can readily be quantified based on records of past model performance in sum all things considered both epistemic first order and second order uncertainty turn out to be very small in this example the reason for the small epistemic uncertainty is that the model predictions have been evaluated repeatedly for similar cases this allows model users to adopt a purely instrumental view of the model meaning that the focus of the model evaluation lies purely on its predictive success and not on the reasons for why the model makes certain predictions nor on its structure hence in such a case an evaluation of uncertainty in terms of model structure or model parameters would make little sense this highlights that the framework introduced here can deal with cases where model users have a purely instrumental view of their models 5 case study long term global selenium predictions in this section we present a case study from environmental science to demonstrate the application of the framework to a long term prediction to illustrate how the framework works in more complex situations than the toy example from the previous section namely we discuss the case of predictions of changes in global soil selenium content by jones et al 2017 the study used data driven models for three goals namely i to test hypothesized drivers of soil se selenium concentrations ii to predict global soil se selenium concentrations quantitatively and iii to quantify potential changes in soil se selenium concentrations resulting from climate change jones et al 2017 2848 for illustration we will discuss the last of these three goals the impact of climate change on soil selenium jones et al 2017 used data from over 30 000 samples worldwide to train three different models namely two artificial neural networks and a random forest model all three data driven models relate environmental variables to soil selenium concentrations the authors performed a variable selection procedure after which the seven most important predictors were retained and trained the three models using historical data the chosen predictors were the aridity index the clay content evapotranspiration lithology ph precipitation and soil organic carbon these trained models were then used to project changes in soil selenium concentrations due to climate change hereby using changes in precipitation and evapotranspiration from climate models for rcp6 0 and an accompanying scenario for the development of soil organic carbon based on the projections performed with these models jones et al 2017 estimated that average soil selenium concentrations will decline by 4 3 under the chosen boundary conditions selenium is an essential micronutrient which makes information on future selenium loss of this magnitude potentially decision relevant for example changes to farming practices might be required to counter the climate impacts to ensure nutritionally adequate crops such measures could include fertilization and relying on crops that can take up selenium from the soil even if soil selenium concentrations are lower however taking decisions about such measures requires confidence in the predictions which in turn requires an analysis of the uncertainties of the predictions for this the framework introduced in this paper can be applied again in a first step the assumptions underlying the predictions and their possible justification need to be reconstructed and graphically represented an overview of all of the assumptions and their justification is provided in table 2 the first assumption is the fitness for purpose assumption the fitness for purpose assumption states that the models constructed with the given set of drivers and historical data allows to project i e make a conditional prediction of future selenium concentrations 9 9 the reconstruction presented here assumes that the boundary conditions require no further justification and can just be regarded as given this is done for simplicity a different reconstruction would be possible in which the adequacy of the boundary conditions which depends on the internal consistency of the scenario and on how informative the scenario is for the purpose at hand could be included this assumption would then have to be justified based on an independent evaluation of the respective models that were used to create these scenarios as this point is not essential for the present purposes we will not engage with this discussion in more detail however in this case the fitness for purpose assumption can no longer be conclusively justified based on the repeated evaluation of the model predictions as it was in the toy example because it is unclear whether the future cases are sufficiently similar to past cases hence further ways of justifying the fitness for purpose are required besides past model performance rather than arguing for the similarity of past and future cases a plausible justification for the fitness for purpose assumption is that the modeled relationships can be assumed to be sufficiently constant over time argument 1 for this and all the other labeled arguments see appendix if this constancy assumption holds the model can be used for extrapolations into the far future see kn√ºsel et al 2019 however this assumption itself requires further justification a possible justification of the constancy assumption is that the model accurately represents the most relevant causal processes that drive selenium concentrations and that these mechanisms will not change in response to changing environmental conditions argument 2 10 10 we note here that a different justification for the constancy of the described relationship could also be that two factors whose relationship is modeled have a common cause instead of being directly causally related however the direct path is the more plausible one here while an accurate representation of important causal processes can be used to justify the constancy assumption it is not clear at first glance how strong this justification is the reason for this is that no mechanisms were explicitly included in the models as has been discussed in section 2 thus the assumption regarding the representational accuracy of important causal processes also needs to be justified furthermore even if the model represents important causal processes it may be unclear to what extent these mechanisms remain constant when extrapolated to changing environmental conditions this is relevant here because the model is used to make projections for values of the variables that are somewhat different from today s values because of climate change the justification of the assumption that most of the relevant causal processes are accurately represented leads to a third argumentation iteration and hence a third row in table 2 due to the lack of explicit representations of processes the assumption needs to be justified indirectly there are several ways of how this assumption could be justified some of these have been suggested as conditions for the adequacy of machine learning approaches for yielding reliable predictions or for yielding representationally accurate models in the philosophical literature for example pietsch 2015 has argued that models built with machine learning can extract the causal structure of a phenomenon if among other things 1 the most important variables are included in model construction and 2 data representative of the relevant configurations of the target system are considered kn√ºsel and baumberger 2020 forthcoming have further emphasized that 3 the use of sufficiently flexible machine learning algorithms 4 the empirical accuracy of the models 5 whether model behavior is consistent with background knowledge and 6 robustness arguments can provide non deductive arguments for the representational accuracy of data driven models hence these six points can be used to form arguments justifying the assumption that the most important causal mechanisms are accurately represented in the model the points 1 2 and 3 are best reconstructed as the premises of one argument argument 3 we note here that point 3 refers to the use of sufficiently flexible machine learning algorithms such flexible methods have to be accompanied by measures to avoid overfitting which should also be considered in the argument the points 4 5 and 6 are each best interpreted as premises of a separate argument point 4 states that the models are empirically accurate as assessed by a low cross validation error argument 4 point 5 states that model behavior as assessed through sensitivity analysis is consistent with background knowledge about the system argument 5 point 6 finally states that three different machine learning algorithms were used and largely agreed i e a robustness argument argument 6 assessing the justification provided by points 4 5 and 6 only requires a consideration of model outputs and a test dataset thus these points require no further justification this is not true of the points 1 2 and 3 because their truth cannot be directly assessed by considering model outputs hence they require further justification leading to a fourth iteration in order to justify them scientists have to rely on both background knowledge on the behavior of trace elements in the environment and on understanding of and experience with machine learning each of these three points needs to be justified individually resulting in three additional arguments arguments 7 8 and 9 with this the possible justification of the fitness for purpose has been identified for space reasons we do not provide an explicit reconstruction of the arguments here a reconstruction of the complex argumentation is provided in the appendix we note here that the arguments in the first two iterations can be reconstructed as deductively valid arguments this is not easily possible for the third iteration fig 1 is an argument map showing how the different arguments relate to each other note that the empirical accuracy of the model predictions now appears twice as a justification it is a necessary condition for considering the model fit for purpose in argument 1 but it also gives some indication that the relevant processes are represented accurately in the model in argument 4 in the second step the arguments for justifying the fitness for purpose have to be evaluated the truth of the individual premises see the reconstruction in the appendix has to be evaluated based on domain specific background knowledge the evaluation of the models with available data and the comparison of the behavior of the three individual models based on the model evaluation as discussed by jones et al 2017 all the premises of the arguments seem to be at least approximately true there is however one exception to this concerning the choice of variables as jones et al 2017 note their model lacks selenium sources which leads to an underprediction of global average selenium values the reason for this is that data on selenium sources like atmospheric deposition or biomass deposition was missing the problems emerging from the underprediction and from missing variables are displayed as two theses that attack some of the assumptions in fig 1 that there is an underprediction of average soil selenium concentrations attacks the inference from the empirical accuracy of the models to the conclusion that most relevant processes are accurately represented in the models that data on selenium sources was lacking attacks the premise that most relevant predictors were considered because the global underprediction shows that selenium sources are important for soil selenium concentrations this means that the strength of the argument from empirical accuracy is reduced somewhat and the argument about the most important variables being considered has a premise that is strictly speaking false these problems result in a less than maximal degree of fitness for purpose because the resulting bias increases the range around the model predictions in which observations should be expected to lie this case study is not only characterized by a less than maximal degree of fitness for purpose but also by a non conclusive justification of the actual degree of fitness for purpose the reason for this is that while several arguments can be provided to argue for the assumption that most relevant mechanisms are accurately represented in the model they neither individually nor jointly guarantee that the mechanisms are represented accurately this is because in the third iteration the arguments are not deductively valid but provide only more or less strong reasons for the truth of their conclusion hence also the preceding assumptions regarding the constancy of the identified relationships and regarding the fitness for purpose cannot be justified conclusively now in a third step we can proceed to assessing the uncertainty based on the argumentation reconstructed above as noted some of the provided premises are known to be false strictly speaking namely the assumption that all relevant variables were included this also somewhat affects the empirical accuracy of the models furthermore the justifications provided can neither individually nor jointly guarantee that the models are really fit for purpose the lack of good justification of some of the assumptions leads to a lower than maximal fitness for purpose which means that there is more epistemic first order uncertainty in this case study compared to the toy example from above the framework introduced here does not only highlight that the epistemic uncertainty of the prediction is comparatively large it also highlights which specific aspects of the justification are the source of this uncertainty second order uncertainty is also substantially larger than in the toy example above the reason for this is that it is not clear to what extent the modeling assumptions are justified by the provided evidence this has to do in part with the opacity of the models as it is not entirely clear what relations they actually represent and on what grounds 11 11 the reason for this is that the models do not provide an explicit equation or a set of rules that could be analyzed this is especially true of the models used by jones et al 2017 random forest and neural networks more importantly it has to do with the lack of background knowledge to judge to what degree the assumptions are justified by the provided arguments this lack of background knowledge makes it difficult to assess the strength of the non deductive arguments in the third and fourth iterations shown in table 2 thus all uncertainties considered we see that in this example both first order and second order epistemic uncertainty are present to a larger degree than in the toy example substantial first order uncertainty arises for two reasons namely because the justification of the degree of fitness for purpose is non conclusive and because of the low bias of the models that cannot be corrected for which reduces the actual degree of fitness for purpose the former source of uncertainty stems from non deductive arguments in the justification due to which the conclusion need not be true even if all the premises are true this means that the provided justification cannot guarantee that the model is generally fit for the kind of prediction of interest the latter source of uncertainty arises from an underprediction of global average selenium concentrations resulting from a lack of data on selenium sources this low bias in the predictions reduces the actual degree of fitness for purpose of the models the sources of this bias are apparent in the justification scheme as two theses attacking two of the arguments namely the lack of data on selenium sources attacks the argument stating that the most important variables were included in model construction argument 8 and the resulting underprediction attacks the argument from empirical accuracy to representational accuracy argument 4 note again here that even though the degree of fitness for purpose is less than maximal the models are not outright unfit for purpose second order uncertainty in this case study arises because of certain arguments whose strength is difficult to evaluate for example it is unclear whether the models really do capture important causal processes that are sufficiently constant under changing environmental conditions this is due to a lack of domain specific background knowledge also the arguments provided for the statement that the model represents important causal processes accurately are all non deductive these non deductive arguments introduce first order uncertainty because the justification of fitness for purpose becomes less conclusive they also introduce second order uncertainty because the strength of the justification is difficult to evaluate due to a lack of system understanding the arguments discussed above can also be found in the paper by jones et al 2017 who discussed them in order to understand the uncertainties of the inferences for example they provide a discussion of missing variables concerning 1 above and of the representativeness of the available samples concerning 2 above furthermore they discussed the empirical accuracy of the model in a cross validation setting concerning 4 above and conducted sensitivity analyses to assess whether model behavior is consistent with background knowledge concerning 5 above finally they only considered predictions for pixels where the three data driven models agreed on the sign of change hence they considered the robustness of the predictions concerning 6 above this shows that the arguments provided here were actively engaged with by the authors of the original study note however that the assumptions discussed are not explicitly made during the process of model construction by the modelers rather they are assumptions that modelers need to make once they apply the models for certain kinds of long term predictions as noted both first order and second order uncertainty were considerably larger in this case study than in the toy example above the reason for this is that the data driven models were constructed for the selenium prediction but due to the long lead time of the prediction and the lack of evaluation of the model predictions for the desired purpose model users had to adopt a more realistic interpretation of model behavior compared to the more instrumental view of the model in the toy example hence the reason for the increase in both types of epistemic uncertainty is not simply that data driven models were used but rather that data driven models were used in a context where the uncertainty cannot be estimated from the past performance of the model alone the conclusion of this discussion is likely to hold more generally in cases where background knowledge is insufficient to provide conclusive justification of the fitness for purpose assumption as kn√ºsel et al 2019 argue data driven models are often constructed when background knowledge is insufficient for constructing process based models hence the points about increasing second order uncertainty are likely to hold more generally not just for this case study the flexibility of the framework presented here might lead to different arguments being relevant in different contexts however the arguments highlighted in this case study are likely to show up in different contexts again specifically the six reasons provided for assuming that the model represents most of the relevant processes accurately in some examples it is well possible that further iterations are required to justify some of the six points raised above 6 implications for decision making one of the key reasons for better understanding the uncertainties of scientific inferences is that this understanding is required for epistemically confident decision making hence more needs to be said about how the kind of information provided by the framework presented here can be handled in decision making this is specifically important because the framework delivers information on two types of epistemic uncertainty first order and second order uncertainty and characterizes them in a purely qualitative form in this section we address how the information on uncertainty provided by our framework can be used effectively for decision making and point to areas where further research is necessary as walker et al 2003 8 state quantified statistical uncertainty should not be accorded as much attention as other levels of uncertainty in the uncertainty analysis if there are more severe levels of uncertainty present this means that uncertainties should only be quantified when researchers are in a position to do so confidently which is in line with the faithfulness requirement for uncertainty assessment discussed by parker and risbey 2015 as shown by guillaume et al 2017 researchers already effectively use various methods other than quantification to express uncertainty thus decision makers who are used to handling scientific information may have no problems in handling the qualitative information on uncertainty provided by the framework introduced in this paper nevertheless many formalized decision principles require that information on first order uncertainties be quantified this quantification first requires a good understanding of what the relevant sources of uncertainty are i e it requires an understanding of which assumptions lead to uncertainty in this sense the framework presented here can be used to build the groundwork for uncertainty quantification because it highlights where uncertainties come from and what the relevant uncertainties are approaches exist to quantify uncertainties from machine learning predictions some of these such as quantile regression forests directly provide probabilistic information by predicting not only the best estimate but also the quantiles of the probability distribution function which is learned from the data meinshausen 2006 there are also approaches for estimating uncertainty that are based on bayesian reasoning that account for the uncertainty of individual parameter values in deep learning see e g blundell et al 2015 gal and ghahramani 2016 these approaches are useful to quantify uncertainty that can directly be inferred from the available data we recognize that they yield valuable information and can provide a full account of uncertainties in some settings e g in image classification tasks however in cases such as the case study considered in this paper these approaches would not be able to quantify the full uncertainty namely as the argument based approach introduced here reveals some of the uncertainty cannot be inferred directly from the data for example this concern holds for the statement that the identified relationships remain sufficiently constant while this assumption leads to uncertainty it is not the kind of uncertainty that can be learned directly from the available data in examples like the case study presented in this paper reporting uncertainty purely based on such algorithmic approaches would result in uncertainty assessments that are neither faithful nor complete cf parker and risbey 2015 hence the case study discussed above illustrates the need for additional approaches for uncertainty quantification one promising approach might be to rely on structured expert elicitation in order to estimate quantitative information on uncertainties from the qualitative information that the framework presented here provides see morgan 2014 thompson et al 2016 oppenheimer et al 2016 as it will generally be difficult to create exact uncertainty estimates based on the framework experts will likely be inclined to provide imprecise probability estimates this would require experts to consider a graphical representation such as the argument map in fig 1 and assess the strength of the arguments provided for the fitness for purpose assumption at hand the aforementioned methods for uncertainty quantification based on the robustness of the results can provide a good starting point here based on the expert assessment the intervals obtained would have to be widened or narrowed accordingly the strength of the arguments discussed above should be assessed by domain experts for some of the factors leading to uncertainty it can suffice to specify plausible scenarios without quantitative information on their probability this would be scenario uncertainty in the matrix of walker et al 2003 this is for example the case for the information on boundary conditions regarding the changing climatic conditions in the case study introduced above the framework does not only provide information on first order but also on second order uncertainty when quantifying first order uncertainty second order uncertainty should be considered too a large second order uncertainty means that it is difficult to judge the degree of fitness for purpose of the model this means that first order uncertainty will be only weakly constrained if first order uncertainty is less well constrained a trade off emerges experts can either provide narrower estimates of first order uncertainty and be less confident about it i e they face more second order uncertainty or provide a wider estimate of first order uncertainty with more confidence see winsberg 2018b chap 7 balancing this trade off has to be based on what is perceived to be the most useful for decision makers winsberg 2018a research into the development of decision principles that can be used with the two tiered information on uncertainty discussed here is still needed winsberg 2018b chap 8 a candidate approach is the confidence approach that considers different models depending on decision makers risk attitude roussos bradley and frigg 2020 forthcoming a different approach is decision making with possibilistic information i e with information on what is and what is not consistent with our understanding of a system betz 2016a if model based information is handled with such a possibilistic mindset a greater second order uncertainty implies that it is more difficult to distinguish between outcomes that are consistent with our background knowledge outcomes that are inconsistent with our background knowledge and outcomes that cannot be put in either of these categories hence a possible outcome an event with some epistemic first order uncertainty with a large second order uncertainty might have to be considered by a risk averse decision maker even if its largest possible likelihood as estimated based on the first order uncertainty seems small the reason for this is that its first order uncertainty assessment is uncertain and might need to be revised in light of new information predictions are not the only way in which models can provide decision relevant information namely knowledge of causal connections and exploratory modeling can guide policy decisions weaver et al 2013 in such cases models are needed that represent the processes responsible for producing a phenomenon with sufficient accuracy data driven models can be fit for providing this kind of information too in these cases the evaluation of the models fitness is similar to the uncertainty analysis seen in section 5 see kn√ºsel and baumberger 2020 forthcoming 7 conclusions in this paper we have presented an argument based framework for assessing the uncertainties of model based predictions we hereby focused on features of data driven models and showed that the framework is able to analyze the uncertainty of predictions from data driven models based on a toy example and the extensive discussion of a case study from environmental science we highlighted how the application of the framework works in practice constructing data driven models is possible also when a phenomenon is comparatively ill understood however this lack of background knowledge and the opacity of data driven models can lead to substantial second order uncertainty as we have shown here we then discussed what the framework implies for the quantification of uncertainties and for decision making based on information from data driven models open questions remain specifically with respect to the quantification of uncertainties we encourage attempts at using structured expert elicitation as suggested here and further research into decision principles environmental scientists working with data driven models are often aware of the limitations and uncertainties of their models however the lack of conceptual tools for uncertainty assessments may inhibit a clear understanding of how large these uncertainties are thus there can potentially be overconfidence about results obtained with data driven models the lack of conceptual tools can also impair a better understanding of the factors that lead to the uncertainty understanding these factors can be useful for researchers e g to identify what steps they could take to reduce the impact of a specific factor that leads to uncertainty the framework presented here provides tools to perform such uncertainty assessments and communicate the uncertainty of predictions from data driven models more transparently hence we encourage researchers developing and working with data driven models to employ the framework provided here to assess the predictive uncertainties of their models being more explicit about uncertainties increases the usefulness of data driven models both for scientific and policy purposes at the same time explicitly discussing the representational function of models may reveal that data driven models are more skillful in some applications than one might have expected initially hence the argument based framework provided here can help to make good use of data driven models in environmental science kwakkel et al 2010 have emphasized the importance of using a common language in uncertainty assessments in order to provide information to decision makers that is easier for them to compare to other cases and contexts we agree with this view however in the case of data driven models it seems unlikely that locations of uncertainty similar to the ones from other frameworks can be defined that can be applied to data driven models generally and are informative of their predictive uncertainty for example it might be intuitive here to speak of model uncertainty and extrapolation uncertainty however as the discussion of the case study has shown how much uncertainty the extrapolation introduces directly depends on properties of the model hence these two terms would not refer to distinct locations of uncertainty however we encourage future work that aims to find a terminology for the information from our framework that can consistently be related to uncertainties from other frameworks future research should also address decision principles that can handle the kind of uncertainty that the framework presented here provides the considerations made in this paper are likely relevant beyond data driven models the framework discussed here is quite general it can hence be applied to other types of models too and could complement existing discussions of uncertainty of environmental models furthermore as the framework focuses on assumptions and how they are justified it can potentially reveal that some of the analyzed assumptions concern value judgments and hence highlight cases of value uncertainty funding this research was funded by the swiss national science foundation under the national research programme big data nrp75 project no 167215 declaration of competing interest the authors declare no conflicts of interest acknowledgments we thank richard bradley roman frigg gertrude hirsch hadorn david stainforth and lenny winkel for discussions and or feedback on earlier versions of this manuscript we further thank the participants of the workshop uncertainty in data driven environmental modeling at eth zurich in august 2019 appendix a reconstruction of the arguments for the case study here we present the individual arguments that can be made to justify the fitness for purpose in the case study presented in section 5 it is essentially an explicit reconstruction of the justifications shown in table 2 the method used to reconstruct the arguments and relate them to each other in the argument map is largely based on betz 2016b with the exception that we also consider non deductive arguments here for a general introduction into the analysis of practical arguments the reader is referred to brun and betz 2016 in this reconstruction the variable m denotes the model ensemble used by jones et al 2017 consisting of an ensemble of data driven models one of which was built using random forest and two of which were built using artificial neural networks the variable s refers to soil selenium concentrations the first argument corresponds to the first row of table 2 and directly concerns the fitness for purpose of the model and is similar to the one presented in the toy example argument 1 p1 1 if a model has predicted many past instances of a phenomenon accurately and the modeled relationships remain sufficiently constant that model is fit for predicting the phenomenon in the far future p1 2 m has predicted many past instances of s accurately p1 3 the modeled relationships in m remain sufficiently constant c1 m is fit for predicting s in the far future in argument 1 premise p1 3 requires further justification a possible justification is based on the fact that the relevant causal processes are represented in the model in a sufficiently accurate manner this argument can again be reconstructed as a deductively valid argument argument 2 p2 1 if a model represents the most important causal processes producing a phenomenon accurately and these processes are unaffected by changing environmental conditions the modeled relationships remain sufficiently constant p2 2 the causal processes represented in m are unaffected by changing environmental conditions p2 3 m accurately represents the most important causal processes that produce s c2 the modeled relationships in m remain sufficiently constant p1 3 while this argument is deductively valid it is not clear whether its premises are true premise p2 1 seems uncontroversial premise p2 2 requires some further justification this can for example be justified based on background knowledge e g if the processes represented are consistent with current scientific understanding and there is reason to believe that they are not dependent on current environmental conditions premise p2 3 in argument 2 also requires further justifications there are four arguments that can be made in favor of p2 3 all of which are non deductive hence in these arguments even if all the premises are true they neither individually nor jointly guarantee the truth of the conclusion the first of these arguments refers the reasons 1 2 and 3 presented in the main text and concerns how the machine learning algorithms were trained to construct the ensemble of data driven models argument 3 p3 1 m was constructed using data that represents sufficiently many configurations of s p3 2 m was constructed using the most important variables p3 3 m was constructed using sufficiently flexible methods and overfitting was avoided c3 m accurately represents the most important causal processes that produce s p2 3 in argument 3 the individual premises require further justification too this justification has to be made by referring to background knowledge the expertise of both domain scientists and data scientists is necessary who need to judge whether the considered samples are sufficiently diverse p3 1 whether relevant variables were omitted p3 2 and whether the used methods were sufficiently flexible p3 3 a second argument that can be made in favor of p2 3 refers to the empirical accuracy of the model argument 4 p4 1 m is empirically accurate with respect to the data from the past c4 m accurately represents the most important causal processes that produce s p2 3 note here that there is a thesis that attacks argument 4 namely m has a low bias and underpredicts global average soil selenium concentration this underprediction attacks p4 1 to some extent a third argument considers the consistency of the model with background knowledge the truth of p5 can be established by conducting sensitivity analyses of the models furthermore jones et al 2017 also use existing samples to show that the rate of change predicted by their models has historical precedents which also serves as evidence for the truth of p5 argument 5 p5 m behaves in consistency with background knowledge about s c5 m accurately represents the most important causal processes that produce s p2 3 finally a fourth argument can be made that refers to the robustness of the models because predictions were only considered for the regions in which all three machine learning algorithms agreed argument 6 p6 the predictions are only considered if the ensemble members of m agree on the sign of change of s c6 m accurately represents the most important causal processes that produce s p2 3 as mentioned above the premises of argument 3 all require further justification for each of these that justification has to come from background knowledge argument 7 p7 m was constructed using over 30 000 samples from different continents c7 m was constructed using data that represents sufficiently many configurations of s p3 1 argument 8 p8 1 m was constructed using seven variables chosen based on a variable selection procedure p8 2 most potentially relevant variables were included in the variable selection procedure c8 m was constructed using the most important variables p3 2 argument 9 p9 1 m was constructed using artificial neural networks and random forest p9 2 measures were taken to avoid overfitting c9 m was constructed using sufficiently flexible methods and overfitting was avoided p3 3 a problem emerges with respect to argument 8 namely as has been noted data on selenium sources was lacking this is what leads the models to underpredict global average selenium concentration see argument 4 above the low bias of the models shows that these sources of selenium are important for soil selenium concentrations hence that data on these sources was lacking directly attacks premise p8 2 which states that all potentially relevant variables were included in the variable selection procedure the measures mentioned in argument 9 which were taken to avoid overfitting include the restriction of the number of variables considered the model evaluation and that categorical variables were only considered if each class contained sufficiently many cases as explained in the supplementary material of the article by jones et al 2017 we also note here that a more detailed justification of the chosen algorithms could be made in which choices such as meta parameters for random forest or the number of hidden layers in neural networks would be explicitly discussed to justify that the methods are sufficiently flexible for reasons of simplicity we focus on the more general justification here all of these arguments can then be arranged in an argument map as shown in fig 1 in the article this map is created as introduced by betz 2016b white boxes refer to arguments and grey boxes to theses solid arrows denote that the content of one box be it an argument or a thesis supports the content of the other box dashed arrows denote that the content of one box attacks the content of the other box note that the solid arrows in fig 1 do not differentiate between deductive and non deductive arguments if an arrow goes from a thesis to an argument this means that the thesis is a premise of the argument support or that the thesis contradicts a premise of the argument attack if the arrow goes from an argument to a thesis this means that the thesis is the conclusion of the argument support or that the thesis is contradicted by the conclusion of the argument attack 
25931,increasing volumes of data allow environmental scientists to use machine learning to construct data driven models of phenomena these models can provide decision relevant predictions but confident decision making requires that the involved uncertainties are understood we argue that existing frameworks for characterizing uncertainties are not appropriate for data driven models because of their focus on distinct locations of uncertainty we propose a framework for uncertainty assessment that uses argument analysis to assess the justification of the assumption that the model is fit for the predictive purpose at hand its flexibility makes the framework applicable to data driven models the framework is illustrated using a case study from environmental science we show that data driven models can be subject to substantial second order uncertainty i e uncertainty in the assessment of the predictive uncertainty because they are often applied to ill understood problems we close by discussing the implications of the predictive uncertainties of data driven models for decision making keywords uncertainty data driven models argument analysis predictions decision making 1 introduction real world policy decisions are taken in light of great uncertainties several aspects of a decision situation can be uncertain including the problem framing the available options from which to choose the actual or potential states of the world and the values that decision makers attach to these states of the world see bradley and drechsler 2014 hirsch et al 2015 hansson and hirsch hadorn 2016 in this paper we are concerned with uncertainty about the actual or potential states of the world often this uncertainty is related to uncertainty of scientific information on which a decision is to be based this information relates to what the world will be like either in the form of a prediction of environmental conditions such as a severe weather forecast or in the form of a conditional prediction so called projection of environmental conditions in response to a policy measure such as climatic conditions in response to a certain socioeconomic pathway and associated greenhouse gas emissions in order to take decisions under uncertainty the uncertainty should be analyzed and if possible quantified an important part of the uncertainty analysis is the characterization of what kind of uncertainty emerges because of which features of the research process only such a thorough treatment of uncertainties can help to ensure that societal decisions are based on no more and no less than what one actually knows betz 2016a 138 recent years have seen large increases in data produced and stored this trend is also apparent in the sciences in general and in the environmental sciences in particular e g in climate sciences see overpeck et al 2011 the increase in the availability of data about environmental systems enables the use of machine learning to analyze the data and to construct data driven models of phenomena see gibert et al 2018a 2018b numerous applications of machine learning can be found in the environmental science literature see e g reichstein et al 2019 in this paper we will discuss applications of machine learning for the data driven modeling of a phenomenon in contrast to a process based model in which the relationships between variables are prescribed in the form of equations specified by an expert a data driven model is constructed by algorithmically inferring the relationships or parameters from a dataset using machine learning for a more detailed distinction of process based and data driven models see kn√ºsel and baumberger 2020 forthcoming if predictions from data driven models are reliable which seems to be the case at least under certain conditions see pietsch 2015 northcott 2019 these predictions are potentially useful for decision making related to the modeled phenomena one of the advantages of data driven models is that they can be constructed when the processes producing a phenomenon are not fully understood hence they might provide decision relevant information specifically about phenomena which are quantitatively not understood well enough to construct process based models however no conceptual tools to appropriately evaluate data driven models in terms of their uncertainties are available to date which reduces their usefulness for decision making here we introduce such a tool that can complement existing quantitative methods to explore the uncertainty of predictions made with data driven models the remainder of this paper is organized as follows in section 2 we argue that existing frameworks for the characterization of the uncertainties of model based predictions are not appropriate for predictions from data driven models we hence introduce a new more general approach in section 3 it focuses on the justification of the assumptions underlying a prediction that is possible based on the available data and background knowledge the assumptions that need to be justified include the fitness for purpose of the used model in sections 4 and 5 we demonstrate the application of this framework to a toy example and to a case study from environmental science in section 6 we discuss the implications of this framework for the quantification of uncertainties and for decision making more generally we conclude in section 7 before proceeding we note here that in the present paper we do not distinguish between pure predictions of environmental conditions such as a forecast of severe weather conditions and predictions of the environmental response conditional on a human intervention such as climatic conditions in response to a certain path of greenhouse gas emissions the approach presented here is fairly general and can be applied in both situations equally 2 existing frameworks existing frameworks for model based decision support and specifically uncertainty analysis have been developed for process based models an influential framework to analyze uncertainties of model based predictions is due to walker et al 2003 it distinguishes three different dimensions of uncertainty that are arranged as an uncertainty matrix namely the location where in the modeling complex does the uncertainty manifest itself the nature is the uncertainty due to the inherent variability of the phenomenon or due to imperfect knowledge of it and the level of uncertainty how severe is the uncertainty ranging from complete certainty to complete ignorance several variations of this matrix have been developed see e g refsgaard et al 2007 and versions of it have been applied in different contexts see kwakkel et al 2010 the locations of uncertainty introduced by walker et al 2003 are the context the model structure and the technical model the driving forces and the system data the parameters and finally the model outcomes which obtain their uncertainty from the preceding locations kwakkel walker and marchau 2010 have synthesized adaptations and criticism of the uncertainty matrix introduced by walker et al and have amongst other things suggested that some of the dimensions including the locations of uncertainty be rearranged similar locations of uncertainty have been discussed in specific disciplines for climate models for example knutti 2018 suggests that the relevant locations of uncertainty are model structural uncertainty numerical approximations parameterizations natural variability due to initial conditions the emission scenario boundary conditions and observational data uncertainty all of which have an analog in the locations discussed above similar locations of uncertainty have been discussed for climate models by other authors winsberg 2018b chap 7 what the approaches presented above have in common is that they suggest that different model related aspects the locations of uncertainty are investigated in order to characterize the uncertainty of the model results namely at each location aspects should be identified that are either intrinsically or epistemically uncertain i e uncertain due to system properties aleatory uncertainty or due to our imperfect understanding of the system epistemic uncertainty and the level of this uncertainty should be determined then this uncertainty is propagated in order to characterize the uncertainty of the processed model outputs thinking about uncertainty in terms of specific locations is not informative for data driven models for three reasons 1 1 we note that in a given application of a model for decision support an additional problem can be that there are many more locations of uncertainty which are not directly related to the model but rather to contextual factors such uncertainties are best addressed e g by stakeholder engagement for an extensive overview of this point for water resource management see badham et al 2019 as the focus of this paper lies on uncertainty that is directly model dependent we will not further discuss additional locations or sources of uncertainty we thank a reviewer of this journal for raising this issue the first reason is that for many data driven models the model structure cannot readily be tied to processes in the target system or be interpreted in terms of the target system yet model structure is one of the locations of uncertainty common to the established frameworks this is especially obvious for models based on neural networks or bagging approaches like random forest while neural networks have a model structure consisting of a number of neurons and layers this structure cannot be interpreted in terms of the target system in a straightforward way for bagging approaches like random forest it is unclear what the relevant model structure would be because they make predictions based on the average of multiple individual estimators due to these issues considering the model structure as a location of uncertainty is likely not helpful to assess the predictive uncertainty of data driven models for process based models structural model uncertainty arises because different model specifications might seem equally plausible and it is unclear how to best represent the target system for a specific purpose this representational uncertainty is related to predictive uncertainty see parker 2010 the best model setup can be more radically underdetermined in the case of data driven models since a very large number of model setups can be consistent with the available training data hence the best model setup for a specific purpose has to be chosen based on background knowledge as data driven models are often employed when processes are ill understood see kn√ºsel et al 2019 the model setup of data driven models and the resulting representational uncertainty of a target seem relevant for an analysis of predictive uncertainty thus even though the model structure if there is an accessible model structure cannot directly be tied to the target system an approach to assess the predictive uncertainty of data driven models needs to be able to consider the representational uncertainty of the model at issue however it is unclear how exactly representational uncertainty and predictive uncertainty are related for data driven models the second reason existing approaches are not informative for understanding the predictive uncertainty of data driven models is that similar to the model structure model parameters are not an obvious location of uncertainty either for many data driven models some machine learning approaches like random forest are non parametric and hence this location of uncertainty is simply not defined for them such non parametric approaches do have meta parameters e g the number of trees or the number of considered variables at every split in a random forest model but these again cannot readily be interpreted in terms of the target system other machine learning approaches like deep neural networks can have an overwhelming number of parameters as for the model structure many of the model parameters have no obvious interpretation in terms of the target system and the uncertainty in each parameter does not directly translate to predictive uncertainty in the same sense thus thinking about model parameters as a location of uncertainty is not helpful for data driven models either the third reason is that machine learning is at least sometimes preoccupied mainly with good predictions and less with the reasons for these predictions but as we will see below for some predictions the reasons for the predictions can matter too a framework for assessing predictive uncertainties of data driven models should thus reflect this purpose dependence of the representational character of the model hence existing frameworks for uncertainty assessments seem inadequate as conceptual tools for a discussion of the predictive uncertainty of data driven models while it can be helpful to think about uncertainty in terms of different sources whose uncertainty can be considered additive for data driven models new conceptual tools are needed to analyze relevant sources of uncertainty from the above discussion two requirements become clear for an analytic framework for the predictive uncertainties of data driven models first such a framework needs to do justice to the fact that under some but not all circumstances a data driven model has an instrumental character in the sense that modelers are preoccupied only with the predictive success of data driven models and not with the reasons for this success such cases are typically characterized by model evaluation that focuses solely on performance metrics such as the cross validation error second in cases in which a modeler is not only preoccupied with the predictions but also with the reasons for predictive success the representational accuracy of data driven models needs to be assessed this is so even though as we have argued above neither the model structure nor model parameters can readily be interpreted in terms of the target system at least not for models with many degrees of freedom in these cases model users will be forced to adopt a more realistic interpretation of the model meaning that they need to assume that the model behaves in a way that is consistent with the actual mechanisms of the target system even if the model structure and parameters cannot be interpreted in terms of the target system specifically this means that the framework needs to be able to assess to what extent the behavior as opposed to the structure or the parameters of the data driven model is coherent with background knowledge and to what extent it is possible to argue from the coherence with background knowledge to representational accuracy 2 2 we note here that evaluating model behavior in order to assess the skill and the uncertainty of models is not unique to data driven models it is also done for process based models this has been discussed for example for climate models by lloyd 2009 who emphasizes the importance of model behavior e g the simulation of the tropopause for establishing confidence in model results and for environmental modeling more generally by hamilton et al 2019 under the term stylized facts we thank a reviewer of this journal for raising this issue uncertainty has also been a topic in the computer science literature on machine learning to the best of our knowledge the literature on uncertainties in machine learning has mainly discussed the role of bayesian approaches for quantifying uncertainty see blundell et al 2015 ghahramani 2015 gal and ghahramani 2016 kendall and gal 2017 have explicitly distinguished between epistemic and aleatory uncertainty in the context of bayesian deep learning for computer vision a distinction we will take up in later sections these bayesian approaches are certainly useful for some contexts such as computer vision and for cases in which uncertainty can be characterized easily because it mainly comes from certain sources such as noisy data however for the present purposes they seem insufficient as a basis of the uncertainty assessment of data driven environmental models as they are unable to account for uncertainty not present in the available dataset which becomes important when a relatively realistic interpretation of the model is adopted however we will return to how these methods could complement the ideas presented in our framework in section 6 3 an argument based framework for uncertainty analysis above we have argued that a framework for analyzing the predictive uncertainty of data driven models needs to do justice to the fact that some but not all data driven models have a purely instrumental character hence the framework necessarily has to be more general than existing frameworks in order to allow for different representational characters in different contexts here we put forward such a framework the framework suggests to analyze uncertainties in three steps the first step consists in reconstructing what assumptions have to be made when using a model for a specific purpose and how these assumptions can be justified the second step consists in evaluating how well justified the assumptions are the third step consists in assessing the uncertainty based on the previous two steps the basic assumption that has to be justified in any modeling application is an assumption regarding the fitness for purpose of the model used as parker 2009 has argued for climate models the goal of model evaluation should not be to confirm the model itself generally but rather to confirm that a model is adequate for a specific purpose in this paper we use the term fitness for purpose which in contrast to adequacy for purpose admits of various degrees of fitness meaning that models are not just fit for purpose or not but fit for purpose to a larger or smaller degree see parker 2020 forthcoming we take a model to be maximally fit for a specific predictive purpose if it can predict the variable of interest reliably with errors always lying in some small range which is determined by the inherent variability of the target system on this way of thinking as the fitness for purpose increases the model skill converges toward the theoretical limit of predictability of the system as described by the model i e with a certain resolution etc an example of such a fitness for purpose assumption would be model m is fit for predicting the total precipitation amount of the next 24 h at location l with errors in some small range according to the argument based framework epistemic uncertainty arises due to a lack of epistemic justification thus if it can be conclusively justified that the model is maximally fit for purpose in this sense the model predictions exhibit no epistemic uncertainty and the remaining uncertainty of the predictions arising from the small range of errors is aleatory in nature specifically the above considerations reveal three ways in which model predictions can be epistemically uncertain namely according to the framework presented here epistemic uncertainty arises because of known factors that result in less than maximal fitness for purpose factors that make the justification non conclusive or factors that make it unclear what degree of fitness for purpose a model has essentially all of these three kinds of factors increase the range around model predictions in which the actual state of affairs is expected to lie thus they are sources of epistemic uncertainty this conceptualization of source of uncertainty is consistent with guillaume et al 2015 101 who define a source of uncertainty as the origin of an uncertainty in the process to acquire and use knowledge in the following we will introduce the three steps of the framework in more detail the first step is the reconstruction of the assumptions and their possible justifications in order to do this the following two questions have to be addressed first what modeling assumptions does a prediction rely on second how can these assumptions be justified as noted above the basic assumption is a fitness for purpose assumption concerning the model that was used to make a prediction depending on the circumstances the fitness of the model for the predictive task at hand will be justified differently if the justification is itself partly based on assumptions that require further justification the two questions above need to be addressed several times in order to assess how well the basic assumption is justified once this is done an argument map for an introduction to argument maps see betz 2016b can be drawn in which it becomes apparent which arguments and assumptions justify the fitness for purpose assumption and how different arguments and theses are related an argument map is shown in fig 1 which is discussed in the case study in section 5 and essential elements of argument maps are introduced in the appendix in the second step based on the argument map from the first step or in simpler cases based on a direct assessment of the arguments it is evaluated how well all the different assumptions and specifically the fitness for purpose assumption are justified and based on that what the actual degree of fitness for purpose is for this it is relevant to check whether the premises used to justify a conclusion are true 3 3 we note here that in many practical situations evaluating the premises in terms of approximate truth may suffice in such circumstances the premises and conclusions could be qualified to allow for approximate truth and whether they provide sufficiently good reason to accept the truth of the conclusion specifically the evaluation step may reveal that some of the premises are strictly speaking false in these cases it should be evaluated whether the premise may still be close enough to the truth to support the conclusion furthermore premises may be identified of which we do not know whether they are true finally the evaluation step highlights non conclusive justifications in the complex argumentation arising from non deductively correct arguments see below here the strength of the justification needs to be assessed based on domain specific background knowledge to what extent the premises provide good reasons to accept the truth of the conclusion depends on whether the argument that is being evaluated is deductively valid or non deductively correct whenever possible arguments should be reconstructed as deductively valid arguments i e arguments for which the following condition holds if all the premises are true then the conclusion must be true this way the assumptions become more explicit which also makes more explicit why these assumptions may act as sources of uncertainty as will be assessed in the third step an example of a deductively valid argument is the following this example is taken from baumberger et al 2017 7 if a model is adequate for projecting x for the far future then the model reliably indicates x for past and present model m does not reliably indicate x for past and present hence m is not adequate for projecting x for the far future some arguments cannot easily be reconstructed as deductively valid e g because it can be unclear which premises would be appropriate to obtain a plausible deductively valid argument in these cases the arguments can be reconstructed as non deductively correct i e as arguments that provide sufficiently strong but not conclusive reasons for the truth of the conclusion non deductively correct arguments are risky in the sense that even if all of their premises are true the truth of the conclusion is not guaranteed an example of a non deductive argument is the following this example too is taken from baumberger et al 2017 7 model m reliably indicates x and climate quantities upon which x depends for past and present so probably m is adequate for projecting x for the near future finally the third step consists in an assessment of the uncertainty based on the previous step of evaluating the justification of the fitness for purpose assumption by analyzing the possible epistemic justification of assumptions the focus of the framework presented here is on epistemic uncertainties 4 4 note that the justifications always have to rely on arguments that can be constructed from what is known hence the framework introduced here is not helpful to identify uncertainty arising from unknown unknowns the risk of such genuine surprises should especially be considered if a system is complex ill understood if scientists know that they are overconfident if surprises have occurred in the past or if the system is subject to new conditions as parker and risbey 2015 have argued in practical applications of the framework outlined here arguments for the fitness for purpose could be made that aim to rule out the risk of surprises by making explicit references to these five factors depending on how well they are justified they would be additional sources of uncertainty that explicitly account for the risk of surprises the framework distinguishes between two types of epistemic uncertainty that we refer to as first order uncertainty and second order uncertainty these two types of uncertainty are distinguished by their objects first order uncertainty is the epistemic uncertainty of the prediction consequently first order uncertainty is defined here as the extent to which the assumptions underlying a prediction are insufficiently justified thus epistemic first order uncertainty arises if it cannot be conclusively justified that the model is maximally fit for purpose second order uncertainty is defined here as the extent to which the assessment of first order uncertainty is impaired by context specific factors specifically second order uncertainty depends on difficulties in assessing to which extent the assumptions are justified e g because of a lack of evidence contradictory evidence or expert disagreement thus epistemic second order uncertainty arises if the degree of fitness for purpose cannot be conclusively determined the assessment of these two types of uncertainty is based on the evaluation of the argumentation from the second step as will be illustrated below based on the framework the expressions of first order and second order uncertainty will be purely qualitative however in section 6 below we address the questions of uncertainty quantification which is desired for many applications 5 5 we note here that uncertainties of higher orders could be defined in analogy to second order uncertainty for example third order uncertainty could be defined as the uncertainty of the assessment of second order uncertainty as the discussion below will highlight second order uncertainty is an important concept to better understand the epistemic uncertainty of model predictions uncertainties of higher order are likely irrelevant in practical applications before we proceed to illustrating the application of the framework three clarifications are in order first in practical applications the fitness for purpose of a model will not only depend on its predictive accuracy further considerations such as practical concerns can play a role for example ease of use or computational cost see e g haasnoot et al 2014 111 however in the present paper we are only concerned with the uncertainty of the predictions of a model and we will hence discuss fitness for purpose only as it relates to predictive accuracy second maximal fitness for purpose is defined in terms of model performance specifically we have defined a model as maximally fit for purpose if it has errors that always fall within a small range that is due to the internal variability of the target system in practice it can be difficult to specify how wide exactly this small range should be thus the specification of fitness for purpose can itself be a source of epistemic uncertainty if it is unclear how large the errors arising from variability of the target system are third note that if a model is found to be less than maximally fit for purpose this does not mean that the model is outright unfit for purpose whether a given degree of fitness for purpose is sufficient to consider a model outright fit or adequate for a specific purpose depends on the context for models that are evaluated only in terms of predictive performance a model might be considered outright fit for purpose if it provides information that was previously not available a model with a predictive skill that is far from perfect can still provide information that was previously not available thus it can be outright fit for purpose also when it is less than maximally fit for purpose see knutti 2018 for models whose fitness is evaluated in terms of criteria other than predictive skill alone there can be well known trade offs between these criteria for example a trade off exists for climate models between how many processes they represent and how intelligible they are for model users see held 2005 due to such trade offs a model that is maximally fit for purpose in this multidimensional sense cannot be constructed even in principle the identification and evaluation of how the assumptions are justified requires expertise from domain scientists concerned with the phenomenon at hand and from modelers and data scientists but also expertise in argument analysis an introduction to argument analysis can be found in brun and betz 2016 in the literature it has been recognized that an analysis of assumptions can be important for a better understanding of uncertainties see kloprogge van der sluijs and petersen 2011 however as will be shown below since the framework presented here is concerned with justifying the fitness for purpose it is not only concerned with assumptions that a modeler makes in the process of model construction instead it is concerned with assumptions that a model user is relying on at least implicitly when using a given model for a specific predictive purpose for example a model user may have to assume that certain processes are accurately represented in a data driven model however this assumption is not made explicitly in the process of building a data driven model as the relationships between variables are not explicitly prescribed see pietsch 2015 and no processes are therefore explicitly represented hence both choices made in the path cf lahtinen et al 2017 of model construction e g the choice of hidden layers in a neural network or of meta parameters in random forest and assumptions made when using the fully trained model e g regarding representational accuracy can be relevant for an account of the predictive uncertainty of a model 4 toy example for illustration in this section we discuss a simple toy example of a data driven model to illustrate how the framework can be applied our toy model is a random forest algorithm that is trained to predict the maximum daily air temperature at a given location with a lead time of one day for this the current air temperature and pressure at the location the season and an index on the general weather conditions are used as predictors imagine that the model predictions are successful and actually measured maximum daily air temperatures are always close to the predicted value within a small error range and we are able to repeatedly use the model and evaluate its performance we might generally wish to better understand the uncertainty of this kind of prediction in order to base decisions on them for example the prediction of an unusually cold or hot maximum temperature might be relevant for public health in order to obtain a better understanding of the uncertainty of the predictions we can apply the framework introduced in the previous section first the assumptions and their possible justifications need to be identified and graphically arranged as explained above the most fundamental assumption is a fitness for purpose assumption in this case the fitness for purpose assumption states that the model is fit for making predictions of the daily maximum temperature with a lead time of a day for a specific location up to some range how can this assumption be justified since we have used the model and evaluated the accuracy of its predictions repeatedly in the past this past performance can be used to justify the fitness of the model for the predictions at hand namely the model has predicted many past instances of maximum daily temperature accurately 6 6 implicitly this can be read as saying that the model does not make an extrapolation far outside the range of values for which it has been trained this justification can now be illustrated for example in a table as shown in table 1 this justification can be reconstructed as a deductively valid argument of the following form p1 if a model has predicted many past instances of tmax accurately and the conditions for the predictions remain sufficiently similar to the past instances m is fit for predicting tmax p2 m has predicted many past instances of tmax accurately with a lead time of one day p3 the conditions for the predictions remain sufficiently similar to past instances c m is fit for predicting tmax now that the argumentation is reconstructed we can proceed to the second step namely evaluating to what extent the fitness for purpose assumption is justified for the evaluation of the justification we need to assess whether the premises are true and to what extent they provide good reasons for the fitness for purpose assumption as the justification can be reconstructed as a deductively valid argument the conclusion must be true if the premises are true thus the evaluation consists in determining whether all premises are true p1 makes a conditional claim about the fitness for purpose of the model this premise we take it is uncontroversial the premise p2 is related to the evaluation of past predictions of the model as mentioned above the model has been extensively used in past cases and has been predictively successful thus p2 is true too 7 7 we note here that premises 1 and 2 contain vague expressions namely the terms many instances and accurately it may be unclear in practical applications whether sufficiently many instances have been considered in past model evaluation and whether the range in which errors fall is sufficiently small to consider model predictions accurately in real world applications quantitative thresholds may sometimes be difficult to set this is related to the difficulty we have acknowledged in section 3 regarding the range of errors of a model that is considered maximally fit for purpose we thank a reviewer for raising this point therefore the uncertainty of the predictions hinges on the truth of p3 the truth of p3 has to be justified based on domain specific background knowledge namely that the past cases are sufficiently representative to be confident about the models performance more generally for cases that are similar to the ones considered thus far the short lead time of the predictions makes it likely that p3 is true too for two reasons first on a practical level the short lead time allows for repeated evaluations of the predictions second and more fundamentally the short lead time increases the chance that the evaluated predictions of the past are representative of the current predictions because the system is less likely to have experienced large changes in boundary conditions over a short period of time 8 8 note that the argument could as well be reconstructed without premise p3 this would turn the argument into an inductive argument whose strength would have to be assessed based on the representativeness of the past cases we choose and recommend the deductive reconstruction as it makes the uncertainty more explicit hence the evaluation of the argumentation shows that the fitness for purpose assumption is justified by a deductively valid argument furthermore there are good reasons to assume that all of the premises of the argument are true hence the argument seems to be sound a sound argument is a deductively valid argument with true premises which means that there are good reasons to have confidence in the truth of the conclusion finally in the third step the epistemic uncertainty of the prediction can be assessed the first order uncertainty depends on the extent to which the fitness for purpose assumption is justified as we have seen the justification of the fitness for purpose here seems to be a sound argument meaning that the model can be considered close to maximally fit for purpose insofar as it has always predicted the temperatures to within a very small margin of error and to the extent that we are justified in having confidence in p3 this means that epistemic first order uncertainty is quite small or even absent in the toy example epistemic second order uncertainty is also small or even absent as it is straightforward to reconstruct and evaluate the argument for the fitness for purpose assumption there may be some second order uncertainty related to premise p3 if it is unclear to what extent the past performance is representative of future performance the justification of the truth of p3 fundamentally depends on the system understanding note that in this simple toy example the framework can also help to identify the aleatory uncertainty the reason for this is that the total uncertainty of the predictions can be estimated in a straightforward way based on the evaluated predictions since the epistemic uncertainty of the predictions can be reliably estimated as a result of the low level of second order uncertainty the aleatory uncertainty is simply the difference between total uncertainty and epistemic first order uncertainty in the present case aleatory uncertainty corresponds to the range that can be estimated from the small random errors of model predictions as kendall and gal 2017 have argued in machine learning aleatory uncertainty is best understood as uncertainty that cannot be reduced by collecting additional samples of data which is exactly the kind of uncertainty that remains here if sufficient volumes of data have been used for model construction and different modeling choices have been tested and none of the alternative choices was able to outperform the model setup in question then there are reasons to believe that the range of errors arises due to aleatory uncertainty thus to the extent that these conditions hold in the toy example the aleatory uncertainty can readily be quantified based on records of past model performance in sum all things considered both epistemic first order and second order uncertainty turn out to be very small in this example the reason for the small epistemic uncertainty is that the model predictions have been evaluated repeatedly for similar cases this allows model users to adopt a purely instrumental view of the model meaning that the focus of the model evaluation lies purely on its predictive success and not on the reasons for why the model makes certain predictions nor on its structure hence in such a case an evaluation of uncertainty in terms of model structure or model parameters would make little sense this highlights that the framework introduced here can deal with cases where model users have a purely instrumental view of their models 5 case study long term global selenium predictions in this section we present a case study from environmental science to demonstrate the application of the framework to a long term prediction to illustrate how the framework works in more complex situations than the toy example from the previous section namely we discuss the case of predictions of changes in global soil selenium content by jones et al 2017 the study used data driven models for three goals namely i to test hypothesized drivers of soil se selenium concentrations ii to predict global soil se selenium concentrations quantitatively and iii to quantify potential changes in soil se selenium concentrations resulting from climate change jones et al 2017 2848 for illustration we will discuss the last of these three goals the impact of climate change on soil selenium jones et al 2017 used data from over 30 000 samples worldwide to train three different models namely two artificial neural networks and a random forest model all three data driven models relate environmental variables to soil selenium concentrations the authors performed a variable selection procedure after which the seven most important predictors were retained and trained the three models using historical data the chosen predictors were the aridity index the clay content evapotranspiration lithology ph precipitation and soil organic carbon these trained models were then used to project changes in soil selenium concentrations due to climate change hereby using changes in precipitation and evapotranspiration from climate models for rcp6 0 and an accompanying scenario for the development of soil organic carbon based on the projections performed with these models jones et al 2017 estimated that average soil selenium concentrations will decline by 4 3 under the chosen boundary conditions selenium is an essential micronutrient which makes information on future selenium loss of this magnitude potentially decision relevant for example changes to farming practices might be required to counter the climate impacts to ensure nutritionally adequate crops such measures could include fertilization and relying on crops that can take up selenium from the soil even if soil selenium concentrations are lower however taking decisions about such measures requires confidence in the predictions which in turn requires an analysis of the uncertainties of the predictions for this the framework introduced in this paper can be applied again in a first step the assumptions underlying the predictions and their possible justification need to be reconstructed and graphically represented an overview of all of the assumptions and their justification is provided in table 2 the first assumption is the fitness for purpose assumption the fitness for purpose assumption states that the models constructed with the given set of drivers and historical data allows to project i e make a conditional prediction of future selenium concentrations 9 9 the reconstruction presented here assumes that the boundary conditions require no further justification and can just be regarded as given this is done for simplicity a different reconstruction would be possible in which the adequacy of the boundary conditions which depends on the internal consistency of the scenario and on how informative the scenario is for the purpose at hand could be included this assumption would then have to be justified based on an independent evaluation of the respective models that were used to create these scenarios as this point is not essential for the present purposes we will not engage with this discussion in more detail however in this case the fitness for purpose assumption can no longer be conclusively justified based on the repeated evaluation of the model predictions as it was in the toy example because it is unclear whether the future cases are sufficiently similar to past cases hence further ways of justifying the fitness for purpose are required besides past model performance rather than arguing for the similarity of past and future cases a plausible justification for the fitness for purpose assumption is that the modeled relationships can be assumed to be sufficiently constant over time argument 1 for this and all the other labeled arguments see appendix if this constancy assumption holds the model can be used for extrapolations into the far future see kn√ºsel et al 2019 however this assumption itself requires further justification a possible justification of the constancy assumption is that the model accurately represents the most relevant causal processes that drive selenium concentrations and that these mechanisms will not change in response to changing environmental conditions argument 2 10 10 we note here that a different justification for the constancy of the described relationship could also be that two factors whose relationship is modeled have a common cause instead of being directly causally related however the direct path is the more plausible one here while an accurate representation of important causal processes can be used to justify the constancy assumption it is not clear at first glance how strong this justification is the reason for this is that no mechanisms were explicitly included in the models as has been discussed in section 2 thus the assumption regarding the representational accuracy of important causal processes also needs to be justified furthermore even if the model represents important causal processes it may be unclear to what extent these mechanisms remain constant when extrapolated to changing environmental conditions this is relevant here because the model is used to make projections for values of the variables that are somewhat different from today s values because of climate change the justification of the assumption that most of the relevant causal processes are accurately represented leads to a third argumentation iteration and hence a third row in table 2 due to the lack of explicit representations of processes the assumption needs to be justified indirectly there are several ways of how this assumption could be justified some of these have been suggested as conditions for the adequacy of machine learning approaches for yielding reliable predictions or for yielding representationally accurate models in the philosophical literature for example pietsch 2015 has argued that models built with machine learning can extract the causal structure of a phenomenon if among other things 1 the most important variables are included in model construction and 2 data representative of the relevant configurations of the target system are considered kn√ºsel and baumberger 2020 forthcoming have further emphasized that 3 the use of sufficiently flexible machine learning algorithms 4 the empirical accuracy of the models 5 whether model behavior is consistent with background knowledge and 6 robustness arguments can provide non deductive arguments for the representational accuracy of data driven models hence these six points can be used to form arguments justifying the assumption that the most important causal mechanisms are accurately represented in the model the points 1 2 and 3 are best reconstructed as the premises of one argument argument 3 we note here that point 3 refers to the use of sufficiently flexible machine learning algorithms such flexible methods have to be accompanied by measures to avoid overfitting which should also be considered in the argument the points 4 5 and 6 are each best interpreted as premises of a separate argument point 4 states that the models are empirically accurate as assessed by a low cross validation error argument 4 point 5 states that model behavior as assessed through sensitivity analysis is consistent with background knowledge about the system argument 5 point 6 finally states that three different machine learning algorithms were used and largely agreed i e a robustness argument argument 6 assessing the justification provided by points 4 5 and 6 only requires a consideration of model outputs and a test dataset thus these points require no further justification this is not true of the points 1 2 and 3 because their truth cannot be directly assessed by considering model outputs hence they require further justification leading to a fourth iteration in order to justify them scientists have to rely on both background knowledge on the behavior of trace elements in the environment and on understanding of and experience with machine learning each of these three points needs to be justified individually resulting in three additional arguments arguments 7 8 and 9 with this the possible justification of the fitness for purpose has been identified for space reasons we do not provide an explicit reconstruction of the arguments here a reconstruction of the complex argumentation is provided in the appendix we note here that the arguments in the first two iterations can be reconstructed as deductively valid arguments this is not easily possible for the third iteration fig 1 is an argument map showing how the different arguments relate to each other note that the empirical accuracy of the model predictions now appears twice as a justification it is a necessary condition for considering the model fit for purpose in argument 1 but it also gives some indication that the relevant processes are represented accurately in the model in argument 4 in the second step the arguments for justifying the fitness for purpose have to be evaluated the truth of the individual premises see the reconstruction in the appendix has to be evaluated based on domain specific background knowledge the evaluation of the models with available data and the comparison of the behavior of the three individual models based on the model evaluation as discussed by jones et al 2017 all the premises of the arguments seem to be at least approximately true there is however one exception to this concerning the choice of variables as jones et al 2017 note their model lacks selenium sources which leads to an underprediction of global average selenium values the reason for this is that data on selenium sources like atmospheric deposition or biomass deposition was missing the problems emerging from the underprediction and from missing variables are displayed as two theses that attack some of the assumptions in fig 1 that there is an underprediction of average soil selenium concentrations attacks the inference from the empirical accuracy of the models to the conclusion that most relevant processes are accurately represented in the models that data on selenium sources was lacking attacks the premise that most relevant predictors were considered because the global underprediction shows that selenium sources are important for soil selenium concentrations this means that the strength of the argument from empirical accuracy is reduced somewhat and the argument about the most important variables being considered has a premise that is strictly speaking false these problems result in a less than maximal degree of fitness for purpose because the resulting bias increases the range around the model predictions in which observations should be expected to lie this case study is not only characterized by a less than maximal degree of fitness for purpose but also by a non conclusive justification of the actual degree of fitness for purpose the reason for this is that while several arguments can be provided to argue for the assumption that most relevant mechanisms are accurately represented in the model they neither individually nor jointly guarantee that the mechanisms are represented accurately this is because in the third iteration the arguments are not deductively valid but provide only more or less strong reasons for the truth of their conclusion hence also the preceding assumptions regarding the constancy of the identified relationships and regarding the fitness for purpose cannot be justified conclusively now in a third step we can proceed to assessing the uncertainty based on the argumentation reconstructed above as noted some of the provided premises are known to be false strictly speaking namely the assumption that all relevant variables were included this also somewhat affects the empirical accuracy of the models furthermore the justifications provided can neither individually nor jointly guarantee that the models are really fit for purpose the lack of good justification of some of the assumptions leads to a lower than maximal fitness for purpose which means that there is more epistemic first order uncertainty in this case study compared to the toy example from above the framework introduced here does not only highlight that the epistemic uncertainty of the prediction is comparatively large it also highlights which specific aspects of the justification are the source of this uncertainty second order uncertainty is also substantially larger than in the toy example above the reason for this is that it is not clear to what extent the modeling assumptions are justified by the provided evidence this has to do in part with the opacity of the models as it is not entirely clear what relations they actually represent and on what grounds 11 11 the reason for this is that the models do not provide an explicit equation or a set of rules that could be analyzed this is especially true of the models used by jones et al 2017 random forest and neural networks more importantly it has to do with the lack of background knowledge to judge to what degree the assumptions are justified by the provided arguments this lack of background knowledge makes it difficult to assess the strength of the non deductive arguments in the third and fourth iterations shown in table 2 thus all uncertainties considered we see that in this example both first order and second order epistemic uncertainty are present to a larger degree than in the toy example substantial first order uncertainty arises for two reasons namely because the justification of the degree of fitness for purpose is non conclusive and because of the low bias of the models that cannot be corrected for which reduces the actual degree of fitness for purpose the former source of uncertainty stems from non deductive arguments in the justification due to which the conclusion need not be true even if all the premises are true this means that the provided justification cannot guarantee that the model is generally fit for the kind of prediction of interest the latter source of uncertainty arises from an underprediction of global average selenium concentrations resulting from a lack of data on selenium sources this low bias in the predictions reduces the actual degree of fitness for purpose of the models the sources of this bias are apparent in the justification scheme as two theses attacking two of the arguments namely the lack of data on selenium sources attacks the argument stating that the most important variables were included in model construction argument 8 and the resulting underprediction attacks the argument from empirical accuracy to representational accuracy argument 4 note again here that even though the degree of fitness for purpose is less than maximal the models are not outright unfit for purpose second order uncertainty in this case study arises because of certain arguments whose strength is difficult to evaluate for example it is unclear whether the models really do capture important causal processes that are sufficiently constant under changing environmental conditions this is due to a lack of domain specific background knowledge also the arguments provided for the statement that the model represents important causal processes accurately are all non deductive these non deductive arguments introduce first order uncertainty because the justification of fitness for purpose becomes less conclusive they also introduce second order uncertainty because the strength of the justification is difficult to evaluate due to a lack of system understanding the arguments discussed above can also be found in the paper by jones et al 2017 who discussed them in order to understand the uncertainties of the inferences for example they provide a discussion of missing variables concerning 1 above and of the representativeness of the available samples concerning 2 above furthermore they discussed the empirical accuracy of the model in a cross validation setting concerning 4 above and conducted sensitivity analyses to assess whether model behavior is consistent with background knowledge concerning 5 above finally they only considered predictions for pixels where the three data driven models agreed on the sign of change hence they considered the robustness of the predictions concerning 6 above this shows that the arguments provided here were actively engaged with by the authors of the original study note however that the assumptions discussed are not explicitly made during the process of model construction by the modelers rather they are assumptions that modelers need to make once they apply the models for certain kinds of long term predictions as noted both first order and second order uncertainty were considerably larger in this case study than in the toy example above the reason for this is that the data driven models were constructed for the selenium prediction but due to the long lead time of the prediction and the lack of evaluation of the model predictions for the desired purpose model users had to adopt a more realistic interpretation of model behavior compared to the more instrumental view of the model in the toy example hence the reason for the increase in both types of epistemic uncertainty is not simply that data driven models were used but rather that data driven models were used in a context where the uncertainty cannot be estimated from the past performance of the model alone the conclusion of this discussion is likely to hold more generally in cases where background knowledge is insufficient to provide conclusive justification of the fitness for purpose assumption as kn√ºsel et al 2019 argue data driven models are often constructed when background knowledge is insufficient for constructing process based models hence the points about increasing second order uncertainty are likely to hold more generally not just for this case study the flexibility of the framework presented here might lead to different arguments being relevant in different contexts however the arguments highlighted in this case study are likely to show up in different contexts again specifically the six reasons provided for assuming that the model represents most of the relevant processes accurately in some examples it is well possible that further iterations are required to justify some of the six points raised above 6 implications for decision making one of the key reasons for better understanding the uncertainties of scientific inferences is that this understanding is required for epistemically confident decision making hence more needs to be said about how the kind of information provided by the framework presented here can be handled in decision making this is specifically important because the framework delivers information on two types of epistemic uncertainty first order and second order uncertainty and characterizes them in a purely qualitative form in this section we address how the information on uncertainty provided by our framework can be used effectively for decision making and point to areas where further research is necessary as walker et al 2003 8 state quantified statistical uncertainty should not be accorded as much attention as other levels of uncertainty in the uncertainty analysis if there are more severe levels of uncertainty present this means that uncertainties should only be quantified when researchers are in a position to do so confidently which is in line with the faithfulness requirement for uncertainty assessment discussed by parker and risbey 2015 as shown by guillaume et al 2017 researchers already effectively use various methods other than quantification to express uncertainty thus decision makers who are used to handling scientific information may have no problems in handling the qualitative information on uncertainty provided by the framework introduced in this paper nevertheless many formalized decision principles require that information on first order uncertainties be quantified this quantification first requires a good understanding of what the relevant sources of uncertainty are i e it requires an understanding of which assumptions lead to uncertainty in this sense the framework presented here can be used to build the groundwork for uncertainty quantification because it highlights where uncertainties come from and what the relevant uncertainties are approaches exist to quantify uncertainties from machine learning predictions some of these such as quantile regression forests directly provide probabilistic information by predicting not only the best estimate but also the quantiles of the probability distribution function which is learned from the data meinshausen 2006 there are also approaches for estimating uncertainty that are based on bayesian reasoning that account for the uncertainty of individual parameter values in deep learning see e g blundell et al 2015 gal and ghahramani 2016 these approaches are useful to quantify uncertainty that can directly be inferred from the available data we recognize that they yield valuable information and can provide a full account of uncertainties in some settings e g in image classification tasks however in cases such as the case study considered in this paper these approaches would not be able to quantify the full uncertainty namely as the argument based approach introduced here reveals some of the uncertainty cannot be inferred directly from the data for example this concern holds for the statement that the identified relationships remain sufficiently constant while this assumption leads to uncertainty it is not the kind of uncertainty that can be learned directly from the available data in examples like the case study presented in this paper reporting uncertainty purely based on such algorithmic approaches would result in uncertainty assessments that are neither faithful nor complete cf parker and risbey 2015 hence the case study discussed above illustrates the need for additional approaches for uncertainty quantification one promising approach might be to rely on structured expert elicitation in order to estimate quantitative information on uncertainties from the qualitative information that the framework presented here provides see morgan 2014 thompson et al 2016 oppenheimer et al 2016 as it will generally be difficult to create exact uncertainty estimates based on the framework experts will likely be inclined to provide imprecise probability estimates this would require experts to consider a graphical representation such as the argument map in fig 1 and assess the strength of the arguments provided for the fitness for purpose assumption at hand the aforementioned methods for uncertainty quantification based on the robustness of the results can provide a good starting point here based on the expert assessment the intervals obtained would have to be widened or narrowed accordingly the strength of the arguments discussed above should be assessed by domain experts for some of the factors leading to uncertainty it can suffice to specify plausible scenarios without quantitative information on their probability this would be scenario uncertainty in the matrix of walker et al 2003 this is for example the case for the information on boundary conditions regarding the changing climatic conditions in the case study introduced above the framework does not only provide information on first order but also on second order uncertainty when quantifying first order uncertainty second order uncertainty should be considered too a large second order uncertainty means that it is difficult to judge the degree of fitness for purpose of the model this means that first order uncertainty will be only weakly constrained if first order uncertainty is less well constrained a trade off emerges experts can either provide narrower estimates of first order uncertainty and be less confident about it i e they face more second order uncertainty or provide a wider estimate of first order uncertainty with more confidence see winsberg 2018b chap 7 balancing this trade off has to be based on what is perceived to be the most useful for decision makers winsberg 2018a research into the development of decision principles that can be used with the two tiered information on uncertainty discussed here is still needed winsberg 2018b chap 8 a candidate approach is the confidence approach that considers different models depending on decision makers risk attitude roussos bradley and frigg 2020 forthcoming a different approach is decision making with possibilistic information i e with information on what is and what is not consistent with our understanding of a system betz 2016a if model based information is handled with such a possibilistic mindset a greater second order uncertainty implies that it is more difficult to distinguish between outcomes that are consistent with our background knowledge outcomes that are inconsistent with our background knowledge and outcomes that cannot be put in either of these categories hence a possible outcome an event with some epistemic first order uncertainty with a large second order uncertainty might have to be considered by a risk averse decision maker even if its largest possible likelihood as estimated based on the first order uncertainty seems small the reason for this is that its first order uncertainty assessment is uncertain and might need to be revised in light of new information predictions are not the only way in which models can provide decision relevant information namely knowledge of causal connections and exploratory modeling can guide policy decisions weaver et al 2013 in such cases models are needed that represent the processes responsible for producing a phenomenon with sufficient accuracy data driven models can be fit for providing this kind of information too in these cases the evaluation of the models fitness is similar to the uncertainty analysis seen in section 5 see kn√ºsel and baumberger 2020 forthcoming 7 conclusions in this paper we have presented an argument based framework for assessing the uncertainties of model based predictions we hereby focused on features of data driven models and showed that the framework is able to analyze the uncertainty of predictions from data driven models based on a toy example and the extensive discussion of a case study from environmental science we highlighted how the application of the framework works in practice constructing data driven models is possible also when a phenomenon is comparatively ill understood however this lack of background knowledge and the opacity of data driven models can lead to substantial second order uncertainty as we have shown here we then discussed what the framework implies for the quantification of uncertainties and for decision making based on information from data driven models open questions remain specifically with respect to the quantification of uncertainties we encourage attempts at using structured expert elicitation as suggested here and further research into decision principles environmental scientists working with data driven models are often aware of the limitations and uncertainties of their models however the lack of conceptual tools for uncertainty assessments may inhibit a clear understanding of how large these uncertainties are thus there can potentially be overconfidence about results obtained with data driven models the lack of conceptual tools can also impair a better understanding of the factors that lead to the uncertainty understanding these factors can be useful for researchers e g to identify what steps they could take to reduce the impact of a specific factor that leads to uncertainty the framework presented here provides tools to perform such uncertainty assessments and communicate the uncertainty of predictions from data driven models more transparently hence we encourage researchers developing and working with data driven models to employ the framework provided here to assess the predictive uncertainties of their models being more explicit about uncertainties increases the usefulness of data driven models both for scientific and policy purposes at the same time explicitly discussing the representational function of models may reveal that data driven models are more skillful in some applications than one might have expected initially hence the argument based framework provided here can help to make good use of data driven models in environmental science kwakkel et al 2010 have emphasized the importance of using a common language in uncertainty assessments in order to provide information to decision makers that is easier for them to compare to other cases and contexts we agree with this view however in the case of data driven models it seems unlikely that locations of uncertainty similar to the ones from other frameworks can be defined that can be applied to data driven models generally and are informative of their predictive uncertainty for example it might be intuitive here to speak of model uncertainty and extrapolation uncertainty however as the discussion of the case study has shown how much uncertainty the extrapolation introduces directly depends on properties of the model hence these two terms would not refer to distinct locations of uncertainty however we encourage future work that aims to find a terminology for the information from our framework that can consistently be related to uncertainties from other frameworks future research should also address decision principles that can handle the kind of uncertainty that the framework presented here provides the considerations made in this paper are likely relevant beyond data driven models the framework discussed here is quite general it can hence be applied to other types of models too and could complement existing discussions of uncertainty of environmental models furthermore as the framework focuses on assumptions and how they are justified it can potentially reveal that some of the analyzed assumptions concern value judgments and hence highlight cases of value uncertainty funding this research was funded by the swiss national science foundation under the national research programme big data nrp75 project no 167215 declaration of competing interest the authors declare no conflicts of interest acknowledgments we thank richard bradley roman frigg gertrude hirsch hadorn david stainforth and lenny winkel for discussions and or feedback on earlier versions of this manuscript we further thank the participants of the workshop uncertainty in data driven environmental modeling at eth zurich in august 2019 appendix a reconstruction of the arguments for the case study here we present the individual arguments that can be made to justify the fitness for purpose in the case study presented in section 5 it is essentially an explicit reconstruction of the justifications shown in table 2 the method used to reconstruct the arguments and relate them to each other in the argument map is largely based on betz 2016b with the exception that we also consider non deductive arguments here for a general introduction into the analysis of practical arguments the reader is referred to brun and betz 2016 in this reconstruction the variable m denotes the model ensemble used by jones et al 2017 consisting of an ensemble of data driven models one of which was built using random forest and two of which were built using artificial neural networks the variable s refers to soil selenium concentrations the first argument corresponds to the first row of table 2 and directly concerns the fitness for purpose of the model and is similar to the one presented in the toy example argument 1 p1 1 if a model has predicted many past instances of a phenomenon accurately and the modeled relationships remain sufficiently constant that model is fit for predicting the phenomenon in the far future p1 2 m has predicted many past instances of s accurately p1 3 the modeled relationships in m remain sufficiently constant c1 m is fit for predicting s in the far future in argument 1 premise p1 3 requires further justification a possible justification is based on the fact that the relevant causal processes are represented in the model in a sufficiently accurate manner this argument can again be reconstructed as a deductively valid argument argument 2 p2 1 if a model represents the most important causal processes producing a phenomenon accurately and these processes are unaffected by changing environmental conditions the modeled relationships remain sufficiently constant p2 2 the causal processes represented in m are unaffected by changing environmental conditions p2 3 m accurately represents the most important causal processes that produce s c2 the modeled relationships in m remain sufficiently constant p1 3 while this argument is deductively valid it is not clear whether its premises are true premise p2 1 seems uncontroversial premise p2 2 requires some further justification this can for example be justified based on background knowledge e g if the processes represented are consistent with current scientific understanding and there is reason to believe that they are not dependent on current environmental conditions premise p2 3 in argument 2 also requires further justifications there are four arguments that can be made in favor of p2 3 all of which are non deductive hence in these arguments even if all the premises are true they neither individually nor jointly guarantee the truth of the conclusion the first of these arguments refers the reasons 1 2 and 3 presented in the main text and concerns how the machine learning algorithms were trained to construct the ensemble of data driven models argument 3 p3 1 m was constructed using data that represents sufficiently many configurations of s p3 2 m was constructed using the most important variables p3 3 m was constructed using sufficiently flexible methods and overfitting was avoided c3 m accurately represents the most important causal processes that produce s p2 3 in argument 3 the individual premises require further justification too this justification has to be made by referring to background knowledge the expertise of both domain scientists and data scientists is necessary who need to judge whether the considered samples are sufficiently diverse p3 1 whether relevant variables were omitted p3 2 and whether the used methods were sufficiently flexible p3 3 a second argument that can be made in favor of p2 3 refers to the empirical accuracy of the model argument 4 p4 1 m is empirically accurate with respect to the data from the past c4 m accurately represents the most important causal processes that produce s p2 3 note here that there is a thesis that attacks argument 4 namely m has a low bias and underpredicts global average soil selenium concentration this underprediction attacks p4 1 to some extent a third argument considers the consistency of the model with background knowledge the truth of p5 can be established by conducting sensitivity analyses of the models furthermore jones et al 2017 also use existing samples to show that the rate of change predicted by their models has historical precedents which also serves as evidence for the truth of p5 argument 5 p5 m behaves in consistency with background knowledge about s c5 m accurately represents the most important causal processes that produce s p2 3 finally a fourth argument can be made that refers to the robustness of the models because predictions were only considered for the regions in which all three machine learning algorithms agreed argument 6 p6 the predictions are only considered if the ensemble members of m agree on the sign of change of s c6 m accurately represents the most important causal processes that produce s p2 3 as mentioned above the premises of argument 3 all require further justification for each of these that justification has to come from background knowledge argument 7 p7 m was constructed using over 30 000 samples from different continents c7 m was constructed using data that represents sufficiently many configurations of s p3 1 argument 8 p8 1 m was constructed using seven variables chosen based on a variable selection procedure p8 2 most potentially relevant variables were included in the variable selection procedure c8 m was constructed using the most important variables p3 2 argument 9 p9 1 m was constructed using artificial neural networks and random forest p9 2 measures were taken to avoid overfitting c9 m was constructed using sufficiently flexible methods and overfitting was avoided p3 3 a problem emerges with respect to argument 8 namely as has been noted data on selenium sources was lacking this is what leads the models to underpredict global average selenium concentration see argument 4 above the low bias of the models shows that these sources of selenium are important for soil selenium concentrations hence that data on these sources was lacking directly attacks premise p8 2 which states that all potentially relevant variables were included in the variable selection procedure the measures mentioned in argument 9 which were taken to avoid overfitting include the restriction of the number of variables considered the model evaluation and that categorical variables were only considered if each class contained sufficiently many cases as explained in the supplementary material of the article by jones et al 2017 we also note here that a more detailed justification of the chosen algorithms could be made in which choices such as meta parameters for random forest or the number of hidden layers in neural networks would be explicitly discussed to justify that the methods are sufficiently flexible for reasons of simplicity we focus on the more general justification here all of these arguments can then be arranged in an argument map as shown in fig 1 in the article this map is created as introduced by betz 2016b white boxes refer to arguments and grey boxes to theses solid arrows denote that the content of one box be it an argument or a thesis supports the content of the other box dashed arrows denote that the content of one box attacks the content of the other box note that the solid arrows in fig 1 do not differentiate between deductive and non deductive arguments if an arrow goes from a thesis to an argument this means that the thesis is a premise of the argument support or that the thesis contradicts a premise of the argument attack if the arrow goes from an argument to a thesis this means that the thesis is the conclusion of the argument support or that the thesis is contradicted by the conclusion of the argument attack 
25932,large scale computational investigations of groundwater levels are proposed to accelerate science delivery through a workflow spanning database assembly statistics and information synthesis and packaging a water availability study of the mississippi river alluvial plain and particularly the mississippi river valley alluvial aquifer mrva is ongoing software visgwdbmrva has been released as part of the study that demonstrates groundwater informatics for the aquifer considerable water level data collected by multiple agencies over a seven state area exist 18 903 wells 287 272 measurements april 22 2019 data and metadata quality assurance methods basic statistics hydrograph visualization outlier identification hypothesis testing and time series modeling are described two approaches generalized additive models gams and support vector machines svms are used for data interpolation and extension to monthly water level estimates numerical congruence between gam and svm estimates will be useful to limit inclusion of monthly estimates from subsequent science activities keywords mississippi river valley alluvial aquifer water levels statistics generalized additive model support vector machine 1 introduction regional aquifer systems supporting intensive production agriculture or other water supply uses are economically vital resources for making water resources decisions maupin and barber 2005 aquifer systems may have state level records of tens of thousands of drilled wells the data sources can include various federal state and local entities intricate multi agency responsibilities often exist in groundwater data collection and data stewardship these complexities are partly caused by administrative and political boundaries and historical community level views of groundwater resources as local or regional rather than national assets ownership and stewardship of field data hard copy and digital and digital groundwater databases are thus distributed for these reasons data aggregation and database management for a comprehensive study of groundwater levels is complex and difficult one aquifer of national scale importance for production agriculture for which these reasons are acutely applicable is the mississippi river valley alluvial aquifer mrva located in the south central united states ackerman 1996 ausbrook and prior 2008 reba et al 2017 sahoo et al 2017 renken 1998 and is a focus of this study large scale computational investigation of water levels groundwater informatics involves comprehensive statistical data syntheses of discrete measurements and continuous records data syntheses and timely delivery of information are critical to support interpretive science and numerical hydrologic modeling including groundwater flow u s geological survey 2019a simulation models are used to assess water resource availability help identify information gaps and support recommendations for future data needs in april 2019 the u s geological survey usgs national water information system nwis database u s geological survey 2019b provided field measurements of groundwater water levels cunningham and schalk 2011 for nearly 890 000 wells and daily mean values for nearly 8 000 wells in the united states nwis also contains many metadata slots and codes about wells and measurements many of the wells and water levels in nwis are associated with other agencies beyond historical and contemporaneous usgs data activities many wells exist in other agencies inventories and within their own monitoring networks and databases these facts should be considered also within the context of the 20th century transition from purely discrete paper records analog recorder data and manual transcription to increasingly common digital field computers electronic telemetry and enterprise scale information delivery discrete measurements as a rule though represent the majority of spatially distributed data of water levels a general stakeholder need for cost effective and timely science requires interoperability between data hydrogeologic mapping and analyses and numerical simulation models potentiometric surface maps and water level change maps provide valuable visual syntheses of groundwater data and these maps are readily interpreted by scientific and non scientific audiences mcguire 2014 mcguire et al 2019 2020 numerical models can be either steady state or nonsteady state transient depending on science objectives and the needs of resource management clark et al 2011 p 40 for both model types copious quantities of groundwater level information are vital water level statistics reduce the cognitive burden to those with different science objectives such as data quality assurance potentiometric surface evaluation model calibration uncertainty assessments information gap identification or network analyses monthly time step nonsteady state simulations benefit from inclusion of continuous observed monthly water levels however sufficient amounts of continuous time series data to compute successive monthly means usually are lacking for many wells and such monthly means are the rare exception rather than the rule in most cases statistics can be used to create continuous monthly estimates from discrete water level measurements by statistically interpolating or extending observations a complete although estimated time series can be compared to output from a groundwater model at any simulation time statistical estimates of continuous monthly water levels in conjunction with projected uncertainties potentially accelerate and enhance numerical model development welter et al 2015 hughes et al 2017 statistical time series estimation of water levels has been documented in a previous study of the aquifer sahoo et al 2017 p 3 880 used an inverse distance weighting method to spatially predict and estimate discontinuous gaps in mrva water levels through machine learning both observational and statistical uncertainties are key to assign weights to input data or water level estimates in numerical modeling these weights when combined with hydrogeologic knowledge and judgment form the basis on which some model parameters are estimated and help to assess the value of both existing and potential new data for improving accuracy of model forecasts synthesizing the information content of groundwater level databases thus is an important preprocessing step ahead of numerical groundwater flow model calibration because of 1 continual changes in data availability in time and space as water level data for new and existing wells are acquired or as cross agency databases are merged reconciled and erroneous information reduced 2 continual changes in scientific understanding of aquifer features through acquisition of measured aquifer properties or enhanced constraints on the hydrogeologic setting aquifer bottom and other formation contacts through geophysical methods and 3 continual enhancements in numerical flow model capacity uncertainty analyses statistical simulation and parameter estimation capacities and inferred model calibrated aquifer properties for these reasons cost effective and timely science requires computation of representative annual and monthly observed water levels from discrete and continuous daily mean measurements the iterative and rigorous processing of observed water levels for estimation of continuous time series of monthly data also is needed both the computed and estimated monthly data should be structured for rapid update and subsequent insertion into data worth data importance and uncertainty assessments white 2017 the estimation of continuous monthly values for the 135 months october 2007 through december 2018 is of current 2020 interest by the groundwater flow modeling team of the map study in assessment of data worth data importance and uncertainty analyses as well as unsteady state groundwater flow model parameter estimation monthly estimates are useful even in areas of the map considered to have intensive data collection such as the delta region figs 1 and 3 water levels in the mrva are often just semi annual spring and fall seasons this necessitates use of statistical methods to interpolate and extend water levels from discontinuous real world data to continuous monthly information with attendant uncertainties statistical errors monthly water level estimation does not supplant but only augments numerical modeling because statistics do not provide the benefits of numerical modeling including flux estimates residence time computations and managed recharge sahoo et al 2017 p 3 892 1 1 software availability the results from this study were estimated using publicly available r software titled visgwdbmrva asquith et al 2019 and its associated preprocessing infogw2visgwdb software asquith and seanor 2019 basic operational instructions defaults and reference output examples that parallel the visual results are depicted in this paper further the suggested tag for downloading the version used at the completion of this paper is v1 0 4 the repository tags are accessible may 28 2020 at https code usgs gov map gw visgwdbmrva tags delivery of visgwdbmrva through a formal source code repository framework fosters scientific reproducibility and specific reference to files of the visgwdbmrva repository are made herein as needed 1 2 purpose and scope the purpose of this paper is to describe groundwater informatics through the visgwdbmrva software a review of features is made by demonstration of selected results towards an objective of estimates of monthly water level estimates based on april 22 2019 data for wells screen in the mrva in addition the software is used to identify potentially erroneous outlier data and justify use of statistics for quality assurance of the data the scope of this study is limited to the description of underlying and mrva data in nwis as of april 22 2019 although the software can be used to estimate water levels for all 18 903 wells in the nwis database seven wells that represent different data issues are used here as examples the time period for the water level estimates is october 2007 through december 2018 but water levels are shown on the hydrograph for the entire period of record for each well plus the estimation time period lastly the map boundary painter and westerman 2018 was used as a clipping extent to remove wells for which nwis had coded as mrva but are geographically plotted outside the map 1 3 paper organization the study area is reviewed in section 2 and data sources and background on methods are reviewed in section 3 and data forensics are described in section 3 1 basic statistical methods used to describe the information content of example water level databases follow in section 3 2 section 3 4 describes two approaches for time series modeling generalized additive models sec 3 6 and support vector machines sec 3 8 these models were used in tandem for continuous monthly water level estimation software settings applicable to both approaches are described in section 3 5 monthly estimates are presented in section 4 with detailed discussion for each of the seven wells in section 4 1 general discussion and interesting facts about monthly water level estimation are offered in section 4 2 conclusions and advice on groundwater database stewardship are then offered sec 5 a monospaced font denotes specific fields from the nwis database computational informatics by the visgwdbmrva software are demonstrated through selected results herein when deemed necessary for description of specific algorithmic details in sections 3 7 and 3 9 statistical functions within the r software r development core team 2019 or other functions from add on statistical libraries packages are identified 2 the mississippi river alluvial plain and mississippi river valley alluvial aquifer the usgs is involved in a multi year multi discipline regional water availability study for the mississippi river alluvial plain map more information on the study is available online accessed may 28 2020 at https www2 usgs gov water lowermississippigulf map index html for the map study the usgs in cooperation with more than 10 federal state and local stakeholders is comprehensively studying and modeling the mississippi river valley alluvial aquifer mrva and associated hydrogeologic units clark et al 2011 fig 10 underlying the map fig 1 the map is subdivided into seven generalized regions ladd and travers 2019 the mrva is the uppermost hydrogeologic unit that overlies the mississippi embayment aquifer system clark et al 2011 u s geological survey 2015a the aquifer is critical for irrigation reba et al 2017 and contributes to the map being a premier agricultural region water level declines in the mrva and in map streams have been shown to coincide with an increase in withdrawals for irrigation since the late 1980s barlow and clark 2011 the map is an extensive flat alluvial plain extending beyond the historic floodplain of the mississippi river and other proximal streams clark et al 2011 kleiss et al 2000 renken 1998 and is a major physiographic feature spanning parts of seven states the central and northern sections of the map mostly are used for intensive production agriculture and primary crops include cotton rice corn and soybeans the mrva fig 2 is the shallowest hydrogeologic unit underlying the map fig 1 and it extends southward from the head of the mississippi embayment aquifer system clark et al 2011 and merges with the coastal lowlands aquifer system martin and whiteman 1999 barlow and belitz 2016 u s geological survey 2015b the coastal lowlands aquifer system fig 1 is a band of sediments in low lying areas parallel to the gulf coast renken 1998 the extent of the mrva for simplicity is coincident with the map boundary painter and westerman 2018 which is the default of the infogw2visgwdb software asquith and seanor 2019 the aquifer spans about 53 000 square kilometers km2 about 33 000 square miles mi2 and ranges in width from about 120 km 75 mi to 190 km 120 mi sand gravel silt and minor clay deposits of quaternary age make up the aquifer renken 1998 schematics of conceptual horizontal and vertical hydrogeologic properties of the mrva are shown in fig 2 the hydrogeologic complexity of the mrva contributes to considerable variations in water level altitudes this observation remains even when ignoring other variations induced by extensive though site to macro scale specific pumping and surface water recharge and discharge histories barlow and clark 2011 mrva hydrology and hydrogeology are also greatly influenced by many streams and rivers that incise it figs 1 and 2 clark et al 2011 fig 10 when a stream incises the aquifer the stream may 1 provide recharge to the aquifer 2 receive discharge from the aquifer or 3 temporally do either the spatial distribution of areal recharge and its identification is complicated dyer et al 2015 large groundwater withdrawals have resulted in long term declines of water levels in some areas and have diminished aquifer discharge to some of the streams barlow and clark 2011 killian et al 2019 key parts of the map study are the systematic evaluation of water level data and well inventory information using statistical forensics mrva water level data collection history is long and complex which necessitates informatics to package key water level information for other science teams and stakeholders asquith et al 2018 asquith et al 2019 killian and asquith 2019 as of april 22 2019 there are 18 903 wells which plot within the map boundary painter and westerman 2018 with 287 272 water level measurements stored in nwis these nwis data are embedded in the visgwdbmrva software asquith et al 2019 to provide a degree of reproducibility of results specific to this paper the locations of all of the wells used in this process are depicted in fig 3 however the total reported in nwis is considerably smaller than the number of wells in existance for example a separate database not stored in nwis or reflected in the aforementioned counts of mrva well drillers reports in missouri maintained by the missouri department of natural resources modnr exists missouri department of natural resources 2018 as static water levels for more than 16 000 wells the modnr data are known to have data quality issues including excessive measurements of exactly zero smith et al 2019 and considerable evidence of excessive rounding of measurements favoring even numbers and those wells located only to the nearest 3 m 10 ft native units driller s reports such as the modnr database do contain valuable information but care is needed in how water level data are from these reports is used iterative reviews including use of visgwdbmrva can help screen wells with water level altitudes that seem consistent with other wells already in the usgs nwis database also on a well by well basis if a subject well differed by more than about 6 1 m 20 ft in water level altitude from statistical generalization of neighboring wells such a well was removed after further supervisory review mrva water level syntheses are complicated by temporally inconsistent data acquisition and management storage practices further factors conflating efficient study include the changes or differences in field practices between data collection agencies and follow on record keeping and in perhaps the last two decades the establishment and administration of enterprise scale databases non spreadsheet thus there are appreciable data completeness and consistency issues to overcome substantial changes in data acquisition cunningham and schalk 2011cunningham and schalk 2011 and storage practices ranging from analog to digital methods in the past century further complicates data syntheses data consistency issues and the presence of potentially erroneous outlier data can hinder timely and reliable science delivery and affect subsequent management decisions and political discernment by stakeholders there also exists for the mrva considerable 1 differences in spatial density of wells and disparate amounts of water level data over time 2 spatial variation in the horizontal and vertical hydrogeologic properties ackerman 1996 arthur 1994 and 3 variation in availability of well construction details screened open intervals these issues interplay with water use on short local spatial and temporal scales that produce nuanced patterns in water levels in space and time barlow and clark 2011 3 data sources and methods the visgwdbmrva software facilitates data preparation subsequent parameter estimation and ancillary study of the groundwater flow model rapid updates are important whenever the underlying database contains substantial new or revised data in reviewing a new forecast first modeling paradigm for groundwater simulation white 2017 p 663 states that by making the simulated forecasts more prevalent and accessible during each stage of the groundwater modeling analysis advances in appropriate modeling strategies will be facilitated the estimation of continuous time series of monthly water levels fits white s objectives by making groundwater level information syntheses more prevalent and accessible the software visgwdbmrva can efficiently update results as more data become available and improve quality assurance of both usgs and furnished data the software produces statistics of mrva water levels and for other aquifers in region and assembles groundwater level information the output is varied and extensive asquith et al 2019 file readmeoutput md the input data standard of the software is described by asquith et al 2019 file input gwcolumns gwcolumns pdf the data for this study are limited to water levels and associated metadata available in the nwis database u s geological survey 2019b during the period 2016 19 considerable efforts were made to incrementally expand the amount of water level data for the mrva in nwis by entering newly acquired usgs and newly acquired furnished and legacy data from other agencies efforts were also made to merge interlace daily values from continuous hourly recorders into discrete measurements the derived daily means are different from discrete measurements but they are fungible for this study and both are referred to as measurements including water level data from furnished sources involves exhaustive and iterative reconciliation of other agency well numbering schemes and legal descriptions of both well location and name reconciliation was needed because a common well identification scheme is not used by all agencies instances of lost duplicated or triplicated data could occur when separate databases are naively merged 3 1 statistical forensics for general groundwater database review statistical forensics intended to help identify potentially erroneous data water levels and metadata were made the identification of such data does not automatically mean that these data are rejected but provides guidance for validating database integrity some forensics include identification of water level measurements depth below land surface of zero or negative a water level at or above land surface in parts of the map for which such conditions were expected based on hydrogeologic context and data for nearby wells identification of water levels below the bottom of the well which physically is not logical however note that the reported depth of the well could be the cause and not the measurement value itself another possibility is that a well has been deepened and such information has not been tracked identification of water levels below the lowest opening in the well measurement in the sump of the well as opposed to formation waters this forensic is predicated on well construction data being available and accurate which might not be the general rule fig 2 identification of water level measurements at depths below best available estimates of the altitude of the base of the mrva torak and painter 2019 these could indicate measurement errors or wells erroneously labeled as screened only in the mrva identification of potential duplicate measurements an effective scheme for this is to make a hash table of a key value pair with keys being concatenation of a well identification number date stamp and water level measurement and review of the distribution of the first two significant figures nigrini 2011 of the measurements for example consider two measurements of 3 4 m and 34 m both are encoded for inspection based on the ideas of nigrini 2011 as 34 such a review by infogw2visgwdb software asquith and seanor 2019 permits assessment of rounding tendencies by different collection agencies data rounding could be motivated by natural human tendencies to favor even numbers or numbers involving trailing 5s or 10s decimal shifts as potential errors also can be identified for further scrutiny the r package benford analysis cinelli 2017 supported the review of significant figures an example review of the first two significant figures following other data quality assurance screening or filtering of the modnr well drillers report database smith et al 2019 of about 10 000 modnr wells each with a solitary measurement is shown in fig 4 in the infogw2visgwdb software this drillers report database is held external to the nwis database the distribution of the first two significant figures shows considerable rounding and biases towards even numbers including 5s and 10s decimal shifts could also be present but the left part of the figure shows no measurements of better resolution than 1 ft these data characteristics possibly originate from the field observation or measurement methods used for some of these water levels which lack the precision of other methods such as a calibrated steel tape cunningham and schalk 2011 as a general rule the reported measurements are from unknown methods the annotations in the figure are intended to clarify interpretive nuances such database review and comparisons to similar plots conditioned on agency source or other metadata help to inform data stewards about relative data quality for an example like this and from a perspective of groundwater informatics large sample sizes number of wells would yield unbiased statistics if the field data themselves have been observed or otherwise measured in unbiased ways although individual measurements could have large though unquantified error statistics are useful to assign relative uncertainty to solitary data measurements which is critical for pest assistance in flow model parameter estimation as of april 22 2019 and after clipping to the map boundary nwis contained 18 903 wells with collectively 287 272 measurements including daily values from continuous recorders u s geological survey 2019b the earliest measurement date is december 31 1899 but this measurement date in nwis likely is erroneous for the simple reason of occurrence at the end of a year the most recent measurement at the time of the writing of this paper was april 17 2019 which made available data through the 2018 map irrigation season the irrigation season depends on many factors but generally occurs between april and october the number of wells and measurements for arkansas louisiana mississippi missouri and tennessee are 4 751 164 104 3 588 15 224 9 626 102 738 904 5 091 and 34 115 respectively as of april 22 2019 there were no water level measurements in mrva wells in nwis for the parts of illinois or kentucky that lie within the map boundary there also are great measurement count discrepancies amongst the wells for example the minimum and median numbers of measurements are just 1 the arithmetic mean is 11 the third quartile is just 5 and the maximum is nearly 3 000 nearly 10 years of continuous data these counts support the assertion in section 1 that sufficient amounts of continuous time series data to compute successive monthly means usually are lacking for many wells and such monthly means are the rare exception rather than the rule which justifies the utility of estimation of monthly values by statistical methods described in this study 3 2 basic statistical methods for data quality assurance all the statistics and methods described in this report exist in output files of the visgwdbmrva software basic statistical methods are useful towards data quality assurance statistical summaries provide initial steps towards detection of potentially erroneous data and subsequent database reconciliation between data collection agencies for a given year range and or range in months for the year range statistics such as minimums medians means maximums and respective dates of minimums and maximums can be informative data features such as sample size number of unique years standard deviation and other variation and skewness measures l moments asquith 2011 2018 as well as decadal means also provide information packaging another useful statistic to subset the database for further scrutiny is the intra annual standard deviation year over year variation in the annual means more sophisticated statistics than these are useful for hypothesis testing in order to enhance review and achieve data quality assurance for example null hypotheses such as 1 the data have no monotonic trend or 2 differing agencies collect data with similar statistical properties the latter example includes null hypotheses such as 1 the data have a unimodal distribution and 2 the same median in absence of an overall trend with these hypotheses in mind processing all results not reported herein for a given well asquith et al 2019 include 1 kendall s tau statistical trend test helsel and hirsch 2002 of the water levels with time which assesses monotonic trends over time 2 hartigans dip test for unimodality hartigan and hartigan 1985 maechler 2016 of water levels conditional on metadata including agency source being a grouping variable and 3 kruskal wallis rank sum test helsel and hirsch 2002 for comparison of data medians also conditional on agency source these tests involve assessments of data compatibility between sources in other words the hartigans dip test and kruskal wallis rank sum test indicates that the data credited to different agencies or sources are statistically similar and hence compatible the statistical time series modeling also facilitates more elaborate database reviews asquith et al 2019 for example the water level residuals of a time series model that represent detrended information are used for another dip test of unimodality conditional on agency source or a levene s test from fox and weisberg 2011 for homogeneity of variance across agencies the kruskal wallis tests on the residuals conditional on the agency source again are used to assess data compatibility a well known estimator of slope trend in time related to kendall s tau is the slope from the nonparametric theil or theil sen line helsel and hirsch 2002 this slope is a simple estimate of overall changes in water levels and studying wells that have unusual slopes relative to others could be informative and useful in regional assessment of trends and hence detection of unusual data for a well 3 3 visualization techniques and example hydrographs for seven wells data visualization is important for communicating the information content of a database and statistical results comparing an individual well hydrograph with hydrographs for nearby wells and reporting any associated statistical results is an effective means of communication to local stakeholders the 18 903 wells in nwis is a large number and implies that comprehensive visualization is a prodigious effort thus the seven example wells and their corresponding hydrographs were selected to provide proof of concept demonstration of the visgwdbmrva software asquith et al 2019 only one of the seven wells selected for this study has just one measurement simultaneous depiction of measurements at nearby wells can be especially effective for data review for such wells the other six wells were chosen because of spatial distribution to illustrate a variety of data properties and statistical results in aggregate the seven wells show data features commonly found in large aquifer systems such as abundant neighboring well records data from a variety of sources and inconspicuous outliers sophisticated hydrograph visualization inherently is information dense and cognitively complex an expansive list exists of symbol sets and coloring pallets to depict the data for a given well along with neighboring well data a common explanation applicable throughout the hydrographs of the seven wells is shown in fig 5 the visgwdbmrva documentation contains highly complementary material to this paper by depicting hydrograph without statistical time series estimation focus of this paper for the same seven wells asquith et al 2019 file inst doc examplehydrographs examplehydrographs pdf the agencies involved in collecting data for a given aquifer vary in time and space as a result the visgwdbmrva software automatically expands or contracts its symbol pallet fig 5 as needed for a given input data set this is an important point because it means that an applicable explanation is itself mutable by the unique agencies encountered as data are loaded the color pallet conversely is fixed for the complete nwis level source code list a d l o r s u and z though these as letters are not drawn just the corresponding color is used 3 4 generalized additive model and support vector machine approaches two different modeling approaches are implemented in visgwdbmrva to depict the temporal variation in water levels generalized additive models gams and support vector machines svms the choice of these two approaches was influenced by a need to identify potentially erroneous well specific data and also provide both site specific and neighborhood water level estimates including monthly estimates considerable background for the topic of statistical prediction is found in hastie et al 2008 kuhn and johnson 2016 and wood 2017 though gams and svms differ from one another considerably in mathematical formulation and nuances of use in a loose conceptual sense these two approaches are analogous to well known least squares regression techniques faraway 2005 specifically within each a response variable can be estimated by one or more predictor variables gam and svm approaches are more sophisticated relative to least squares regression methods faraway 2005 which are available in the built in r function lm and local regression smoothing methods cleveland 1979 cleveland et al 1992 which are available in the built in r functions lowess and loess incidentally jones et al 2014 provide a combination of spreadsheet and r software gwsdat with a graphical user interface for plotting time series of groundwater solute concentrations based on local linear regression to depict trends and uncertainty bounds in solute concentrations both gam and svm approaches are flexible and complement each other the various settings and configurations of the gams and svms described herein asquith et al 2019 file include visgwdb control r were developed as part of recurring episodes of database review and erroneous data reconciliation some heuristic judgement is implicit and ad hoc criteria such as overall sample sizes evolved as the authors reviewed many thousands of hydrographs and statistical predictions 3 5 general settings for gam and svm construction two different styles of analyses were made first expanding on the hydrographs for the seven example wells the fits of a site specific gam and svm for each subject well were made second the wells in the local neighborhood of each target well were used for fitting regional gam and svms before the mathematical details of the gam and svms are described discussion of general settings used follows for the site specific analyses the minimum sample size number of water levels for the gam and svm could be independently controlled if insufficient water levels are available no fitting is made and thus no residuals are available for this study a minimum sample size of 10 water levels was chosen for both styles of analysis before the gam and svm were fit about 75 percent of the wells lack sufficient data less than 10 measurements for the site specific gam and svm the minimum of 10 is a small sample size but a performance review by the authors suggests that this is reasonable as a trade off between the ability to explore for outliers and provide monthly estimates with associated uncertainty and simultaneously useful in the same computational run for outlier identification for the regional analyses to create a gam and svm herein circular search from 0 to 8 km 0 5 mi radius was made with the models accepting an unbounded number of water levels from up to 300 nearest wells neighbors other settings were as described for the site specific analyses this same radius was used for the construction of the seven hydrographs sec 3 3 for this study outliers are jointly defined using a residual magnitude of at least 3 3 m 10 ft other choices could be made as well as being beyond the 1st or 99th percentile other limits could be chosen of the predicted error distribution for the estimation the definition of an outlier does not materially impact the construction of the time series models the visgwdbmrva software does not dynamically remove outliers it only flags them 3 6 generalized additive model estimation of mrva water levels gams are flexible and capable of mimicking curvilinear patterns in data hastie and tibshirani 1990 hastie et al 2008 wood 2017 gams model a response variable using an additive combination of various optional parametric terms and smooth functions of predictor variables the incorporation of smooth functions is an advantage over simpler multi linear least squares regression and similar methods because appropriately configured smooth functions can adapt to nonlinear relations the general form of a gam is 1 y i Œ≤ o x i Œ∏ f x i Œµ i where y i is the response variable untransformed herein that is the ith water level observation Œ≤ o is an intercept x i is a one row matrix for strictly parametric and suitably transformed predictor variables Œ∏ is a one column parameter matrix the f is a smooth function itself having arguments that is estimated automatically for the predictor variable x i the three dots represent additional smooth terms as needed and Œµ i are errors taken as independent and identically distributed with zero mean the x i Œ∏ term is the multi linear parametric regression faraway 2005 component of a gam for this study y i is water level altitude in the mrva and x i is a numerical representation of the date of water level measurement the parametric terms could include trigonometric operations used to express seasonality or land surface altitude only trigonometric operations thus far have been thoroughly studied a gam is fit by the defaults provided in wood 2019 using penalized iterative least squares wood 2017 p 180 182 and a generalized cross validation a type of leave one out score for the smoothness controls on the smooth functions wood 2017 p 171 a setting in visgwdbmrva software use of gams was a multiplier on the sample size set to eight controlling when simple first order seasonal terms are to be evaluated the multiplier was set to four if the number of measurements exceeded 40 4 10 four times the minimum sample size then trigonometric functions were conditionally used in a provisional gam to capture potential seasonality for this study the parametric terms potentially included were simply cos 2 œÄ d i and sin 2 œÄ d i in which d i is the day fraction into the year of the measurement date if both the p values are 0 005 then the two trigonometric terms are retained otherwise they are dropped and the original gam is used the fact that both p values are required to be 0 005 is a strict setting for example it would be justified to accept the two terms if only one of them was statistically significant helsel and hirsch 2002 p 342 but because of the scale of automated processing it was explicitly decided to have a tight threshold of maximum p value for both terms for this study the fact that considerable historical effort by others to collect semi annual data spring and fall in much of the mrva and the presence of a substantial underlying seasonal trend the two trigonometric functions were present in many of the regional gams and much less commonly in the site specific gams because of sample size limitations the initial form of the gam was 2 y ÀÜ i Œ≤ o a cos 2 œÄ d i b sin 2 œÄ d i f x i Œµ i where y ÀÜ i is a predicted water level Œ≤ o is an intercept a and b are regression coefficients d i is day fraction into the year and x i is time in decimal days from a time origin the two trigonometric terms represent a simple and symmetrical seasonal component which often capture anticipated timing of aquifer rebound or recovery in mrva water levels by about late march to mid april prior to or at the beginning of the next irrigation season more sophisticated seasonal terms were evaluated using higher order cos k œÄ d i and sin k œÄ d i terms for k 4 6 more trigonometric functionality higher orders of k œÄ could produce asymmetrical seasonal patterns compared to a simple symmetrical sinusoidal pattern that is mandated by 2 œÄ terms alone after exploratory analyses and for general application for the mrva deliberately it was decided to keep only the two terms involving 2 œÄ in many parts of the mrva most water levels are made in aprils and octobers with such sample timing consistency the data density to compute timing of peaks real world seasonal aquifer maxima and troughs real world aquifer minima of mrva water levels based solely on statistical analyses likely is lacking a final note is that gams by their nature can eventually extrapolate away from the global central tendency of the data in contrast to svms 3 7 generalized additive model implementations for the gams a gaussian family for the generalized linear model glm faraway 2006 was used to estimate the parametric terms and largely default parameters arguments of the gam function provided by the mgcv package wood 2019 for documentation purposes it is useful to described how the gam was implemented by the visgwdbmrva software and then contrast it with other tested versions the gam constructed by the gam function in the mgcv package wood 2019 was similar to the following gam1 gam y s x bs cr cosine sine weights wgts where gam1 is the fitted gam to the data y is a vector of mrva water level measurements s denotes a smooth x is the day month year of the measurement and is equivalent to the d i and the cosine and sine represent the cos 2 œÄ d i and sin 2 œÄ d i parametric terms representing the respective trigonometric operations of the day fraction into the year of the measurement date d i as previously described a gam can use observation weights the wgts is optional though used meaning actually turned on within visgwdbmrva for this study vector of weights which are inversely proportional with distance from the subject well to the neighboring wells the bs cr shows that a cubic regression spline basis function of the gam smooth f x i was used the trigonometric functions were potentially dropped in accordance with the previous p value discussion finally note that gam1 was used for both site specific and regional gams days as a time object in the r software are in seconds from the beginning of an epoch 1970 01 01 in visgwdbmrva conversion of the dates to years by division of the number of seconds in an average year 60 60 24 365 25 is necessary before initiating the gam function to avoid fatal numerical overflow errors in the gam algorithms wood 2017 the conversion means that the gam algorithms see a floating point number of years since the epoch beginning noting that spatial data were also available for study these other gams were evaluated for the regional modeling gam2 gam y s x bs cr sine cosine s e n bs tp gam3 gam y s x bs cr sine cosine s a bs tp gam4 gam y te x e n bs tp sine cosine the gam2 has an additional smooth term for spatial location specifically added are the e easting and n northing coordinates of an albers equal area conic coordinate projection of the latitude and longitude of the nearby wells and the subject of a 2 dimensional thin plate regression spline bs tp the gam3 has an alternative smooth term using the a land surface altitude and the gam4 is more complicated the prospect of gam2 as being preferable was a preliminary idea because the addition of a spatial trend term is intuitively promising however exploratory testing suggests that although there is enough variation in the joint spatial and temporal density of observations there appear to be insufficient data overall in the mrva for gam2 as a result the gam2 sometimes provides unreliable estimates for the available data exploratory testing indicates that gam3 does not have much of a performance gain because the map is topographically flat and the neighboring data are selected on a relatively small radial distances around any given subject well also land surface altitude as a predictor might be less attractive for confined aquifer systems such as the confined sparta sand fig 2 and finally in some hydrogeologic settings the altitude of the base of an aquifer could be useful the gam4 has the time and spatial coordinates modeled using a tensor smooth estimator te in three dimensions with a thin plate regression spline this tensor form of smoothing is needed because the times and spatial locations are measured on inherently different unit systems wood 2017 2019 similar to the gam2 testing also suggests that gam4 sometimes provides unreliable estimates for the available data these three gams have been experimentally used over many iterations some minor variation in control setting and basis functions supported by wood 2019 also were experimented with for this study future designs for model form and ancillary settings could be different as understanding improves or for aquifer systems other than the mrva finally precise implementation strategies for a generalization beyond the mrva of groundwater informatics using gams are difficult to propose without iterative study 3 8 support vector machine estimation of mrva water levels svms are a type of machine learning approach to prediction in which complex linear combinations of specific data points and attendant weights are used to define a hyperplane through the data entire data set contemporaneous study of mrva water levels using machine learning through a multilayer perceptron network has been made sahoo et al 2017 some salient remarks by those authors suggest that data driven and machine learning methods based on nonlinear interdependencies may be able to predict groundwater level change without deep knowledge of the underlying physical parameters sahoo et al 2017 p 3 878 behzad et al 2010 conducted a comparative study of svms and artificial neural networks anns for groundwater level estimation including monthly time scales and noted that svms outperformed anns and that svms have the potential to be a useful and practical tool for cases where less measured data are available for future prediction svms clifton et al 2014 kuhn and johnson 2016 hastie et al 2008 pimentel et al 2014 steinwart and christmann 2008 like gams also are flexible and capable of mimicking curvilinear patterns in the data and svm description is best setup as an analogy to linear regression svms are a type of robust regression in which squared residuals are not used for fitting as they are in least squares regression faraway 2005 as a result data points with large residuals have limited effects on the fitted svm kuhn and johnson 2016 p 153 a curious and identifying feature of svms is that the data points residing near predictions for the sample made by the svm will actually have no effect some control on how large residuals can be with no effect of the svm is influenced by a so called epsilon setting Œµ svm the Œµ svm in svm implementation karatzoglou et al 2018 controls width of the prediction tube which has an effect of influencing the inclusion of data near the hyperplane defining the regression this setting is a parameter that can be tuned our experience is that the default of epsilon 0 1 seems to work well enough to leave as is in visgwdbmrva the fit phase of an svm is based on minimization of an error function subject to a penalty as the model progresses towards overfitting following kuhn and johnson 2016 the coefficients Œ≤ j of a fitted svm minimize the f o objective function for a model having p number of predictors 3 f o y i c l Œµ c i 1 n l Œµ y i y ÀÜ i j 1 p Œ≤ j 2 where c is a cost parameter that is responsible for penalizing large residuals Œµ i y i y ÀÜ i for the n sample of y i and predictions y ÀÜ i the c is a multiplicative factor on an l Œµ epsilon insensitive function to increase or decrease the importance of errors the l Œµ has the aforementioned Œµ svm setting our experience is that the default c 1 seems to work well enough to leave as is in visgwdbmrva large Œ≤ j estimates in magnitude increase the objective function as do large Œµ i these two components in the objective function contrast to methods such as least squares that use the Œµ i only faraway 2005 the Œ≤ j parameters are chosen such that eq 3 is minimized in the regression analogy an estimate prediction of y i and vector of predictors x i x 1 i x 2 i x p i of size n uses a linear combination of model parameters coefficients Œ≤ j to the new sample of the p predictors u j for j 1 2 p the prediction y ÀÜ can be written with the intercept term Œ≤ 0 as 4 y ÀÜ Œ≤ 0 Œ≤ 1 u 1 Œ≤ p u p Œ≤ 0 j 1 p Œ≤ j u j the linear svm is similar when used for estimation but the Œ≤ j estimates importantly can be written as functions of a set of additional but unknown hyper parameters Œ± i and the sample data of size n denoted as x i j so that 5 y ÀÜ Œ≤ 0 Œ≤ 1 u 1 Œ≤ p u p Œ≤ 0 j 1 p Œ≤ j u j Œ≤ 0 j 1 p i 1 n Œ± i x i j u j Œ≤ 0 i 1 n Œ± i j 1 p x i j u j two major insights into the last equation are useful first the final form shows weighted linear combinations of the formula of training data and the new explanatory data of which to make predictions kuhn and johnson 2016 p 155 show that matrix algebra can rewrite the training data and new explanatory data as a linear kernel function with notational adjustment the function itself can be replaced with a nonlinear kernel thus extending svms into nonlinear regression kernels include linear kernel polynomial kernel gaussian radial basis functions rbfs and others the rbf represents a normalized euclidean distance metric bishop 2006 chap 6 second there are as many Œ± i parameters as there are data points compared to conventional regression the svm is over parameterized through the tenet of conventional regression that the model should have vastly fewer parameters than data points kuhn and johnson 2016 p 154 but the penalty or cost used for svm fit compensates for this undesirable situation and in practice many of the Œ± i 0 vanish go to zero finally when given new data on which to make predictions svms might be less utilitarian as extrapolation increases away from the data in contrast to gams as extrapolation is encountered by the svm it regresses to the global mean of the data 3 9 support vector machine implementations for implementation the ksvm function from the kernlab package karatzoglou et al 2018 was used details of svm support are found in hornik et al 2006 for the svms a radial basis function default representing the l Œµ was used and the default Œµ svm 0 1 for epsilon setting and the default c 1 for cost applied these default settings of karatzoglou et al 2018 appear suitable for large scale systematic and automated unsupervised estimation for the mrva water levels there is not a parallel feature with the ksvm function for the inclusion of weights into the svm as was done for the gam general review of the epsilon and cost parameter settings using non default values could be illuminating but is problematic to design given the purpose of visgwdbmrva to support full regional or small scale processing killian and asquith 2019 killian et al 2019 for the mrva processing of other aquifer systems of interest to the map study or other aquifers in the nation for the purposes of documentation it is informative to described how the svm was implemented for this study and contrasted with other evaluated versions the svm was evaluated and similar to the following svm1 ksvm y x svm2 ksvm y x sine cosine where the terms are the same as described for the gam implementations previously described y is water level x is decimal time however the cosine and sine terms using day of the year were ultimately dropped for all modeling just svm1 used other types of svms were evaluated and similar to the following svm3 ksvm y x e n svm4 ksvm y x e n a the svm3 shows use of the e easting and n northing coordinates of the nearby wells and svm4 adds the a land surface altitude review of general prediction reliability when the coordinates are included indicates that svm3 can sometimes provide unreliable estimates it is thought that there are insufficient joint spatial and temporal water level data continued review of svm3 and svm4 is suggested again as said for the gams when land surface altitude is investigated as a predictor it likely would not be useful for confined aquifer systems such as the confined sparta sand fig 2 experiments were conducted using trigonometric functions in the style of sec 3 6 to contribute seasonal terms and potentially enhance the model results not reported here indicate little benefit for reliable estimation and often a deleterious impact on estimation we speculate that in contrast to the parametric form with the trigonometric terms in the gam there is almost universally insufficient near continuous monthly data a measurement occurs at least every couple of months for the svm to adequately perform with trigonometric inputs beyond what it is able to do when just restricted to using time ability of an svm lacking trigonometric explanatory variables to mimic a sinusoidal function is shown by kuhn and johnson 2016 p 156 but sufficient data must exist finally exact implementation strategies for a generalization beyond the mrva of groundwater informatics using svms are difficult to propose 4 estimates of monthly water levels for october 2007 through december 2018 for october 2007 through december 2018 both gam and svms estimated monthly water levels for each of the seven example wells using 1 observed data for a given well the subject well in the site specific model and 2 observed data for a local region surrounding the subject well in the regional model four time series models for a well are thus potentially available site specific gam site specific svm regional gam and regional svm absence of the site specific gam and svm for a particular well is caused by a well having fewer than 10 measurements with no neighboring wells within 8 km 5 mi the monthly estimates are a type of information product needed by other scientists involved in the map study and these are the vertices used to construct the trend lines depicted in figs 7 13 estimates to earlier times than october 2007 are shown in the hydrographs herein if data exist prior to that date because the visualization algorithms in the visgwdbmrva software are designed to include all data however only the october 2007 through december 2018 period was exported to a computer file system as the output pozo csv files described as part of the embedded visgwdbmrva documentation asquith et al 2019 file readmeoutput md a common shared explanation of the line plotting styles and colors used to depict the gam and svms both site specific and regional types is shown in fig 6 the explanation fig 6 also distinguishes between the presence of the trigonometric seasonal terms dotted dash line style in either the site specific gam or regional gam the explanation also provides a clarifying note about the presence of a seasonal trigonometric terms in the gam for either the subject well data or for the neighboring data only one gam is shown for each gam input set site specific and regional and these gams may or may not contain seasonal terms 4 1 gam and svms for the seven example wells 4 1 1 the gam and svms for well ar008 354547090522001 all gams and svms are available for this well fig 7 the site specific gam and svms nearly overlap from 1992 through about 2000 and then diverge because the gam does not show seasonality which is confirmed by the dashed line plotting style the seasonal terms are absent the 2002 water level measurement pulls the gam away from the rest of the data resulting in a lack of congruence between the site specific gam and svm estimates after 2002 in this example the gam clearly has extrapolation problems the svm conversely appears to function satisfactorily to about 2002 but then swings towards the global mean after 2002 the regional gam and regional svm fitted only to the neighboring well data produce similar results throughout the period january 1992 through december 2018 the regional gam has a seasonal amplitude that does not change in time this is true because no provision is made in the model construction for the interaction of the trigonometric functions and hence amplitude changes as a function of year this means that the coefficients on the trigonometric functions from the gam are constant identification of outlier measurements is an important part of data quality assurance the 2002 measurement is identified by a red circle fig 7 see also fig 6 which is a visual cue that the site specific svm has identified an outlier the identification is automated and the concentric circle is drawn when the figure is rendered in this study outliers are jointly defined using a residual magnitude of at least 3 3 m 10 ft as well as being beyond the 1st or 99th percentile of the predicted error distribution for the estimation 4 1 2 the gam and svms for well usgs 320314091452201 only two time series models for this well fig 8 are available the site specific gam and svms could not be fit because the sample size is one which is less than the minimum of 10 measurements required thus only the regional gam and svm are shown the regional gam lacks the dash dot line plotting style seen previously for well ar008 354547090522001 fig 7 which means that seasonality is not statistically significant for the neighboring wells the regional gam appears to have a reasonable fit throughout most of the time period shown the regional svm is shown to regress towards the global mean within a year or two when water levels are not available the results fig 8 show that pursuing data quality assurance for wells having solitary measurements is possible large differences between the reported water level and the two estimates by the regional gam and svms could be an indication of potentially erroneous data much in the same way that outliers such as those shown in figs 7 and 9 or 10 are identified the residuals to the solitary measurements could provide an indirect measure of uncertainty a review such as this formed part of a screening review for a quality assurance study of the modnr well drillers report database smith et al 2019 the solitary measurement for this well is temporally isolated the regional gam and svm predict about 14 9 m 49 ft for the 1980 90 measurements compared to about 13 4 m 44 ft for the 1950 measurement it is suggested at the present and for the mrva that solitary measurements simultaneously within about 6 1 m 20 ft of the regional gam and svms provide a type of quality assurance inclusion of noisy databases such modnr well drillers database that is external to nwis into other science activities could be justifiable several filtered versions of the modnr well drillers database are published as a usgs data release smith et al 2019 these data could be joined for statistical purposes as needed and infogw2visgwdb software provides a useful preprocessor asquith and seanor 2019 inclusion of certain datasets into nwis is problematic partly because of inherent imprecision for some of the water levels if for no other reason than the rounding concerns fig 4 4 1 3 the gam and svms for well usgs 331101090294801 all four gams and svms for this well fig 9 are available the outlier in 1985 might be erroneous because it is a spike upward downward spikes are more readily explainable but local hydrogeologic or climatic considerations would be informative downward spikes might represent recent pumping effects a plausible explanation upward spikes conversely might represent substantial recharge if a sustained flow river is nearby nwis unfortunately provides no context of the measurement conditions because of missing metadata interestingly the neighboring wells have contemporaneous increases of similar magnitude this well was used to create the 2016 and 2018 potentiometric maps by mcguire et al 2019 and mcguire et al 2020 seasonal terms for both the site specific and regional gams were statistically significant as indicated by the undulation of both gam lines the regional gam almost uniformly predicts water levels higher than the site specific gam and water level observations by about 1 m 3 ft throughout the period of record notably the seasonal amplitude is similar between the two gams the two svms show more complicated relations between each other the data for this well and its neighbors also provide an opportunity to highlight examples of open interval comparison between the neighbors and the subject well it appears that the neighbors having generally higher water level altitudes than the subject well are completed above the subject well plausibly in a higher shallower stratum of the mrva conversely the neighboring wells with water levels generally lower than those of the subject well have openings below the subject well another shaded area shows opening overlaps these are generally higher than the subject well finally there are two or three nearby wells that also have opening overlaps and have similar water levels to the subject well 4 1 4 the gam and svms for well usgs 331745090260401 all four gams and svms for this well fig 10 are available the site specific gam and svm estimates are similar and both identify the spring 1986 measurement as an outlier even though most of the years have semi annual measurements there were no statistically significant seasonal terms in the site specific gam this well is an example for which the october 2007 through december 2018 monthly estimates are expected to be particularly useful for data worth data importance as well as groundwater flow model parameter estimation because the statistical models appear to have performed satisfactorily throughout the period of interest 4 1 5 the gam and svms for well usgs 331750090294501 all four gams and svms for this well fig 11 are available this well is an example of a location lacking any data for the period october 2007 through december 2018 extrapolation problems are likely with the site specific gam and svm models as evidenced by the lack of congruence between the results of these models ample neighboring well and regional data exist to make reliable regional predictions for the time period of interest and general similarities between the observed data 1981 89 in the neighboring wells to the estimates from the regional gam and svms are encouraging the site specific estimates however should be rejected after about 1995 because of a lack of site specific gam and svm congruence as well as the additional lack of absence of regional gam and svm congruence 4 1 6 the gam and svms for well usscs 343532091211601 all four gams and svms for this well fig 12 are available there are sufficient data throughout the time period and reasonable site specific gam and svms exist one outlier is identified as shown but by visual inspection the outlier does not appear as potentially erroneous the regional gam and svms are not based on large amounts of data only four neighboring wells the apparent lack of congruence between the site specific and regional models which is almost 6 m 20 ft in the last 15 years could be used to reject some or all of the monthly estimates in subsequent scientific inquiries this well was used to create the 2016 and 2018 potentiometric maps by mcguire et al 2019 and mcguire et al 2020 further this well is useful for another type of data review the gam trend line shows seasonality by the sinusoidal undulations and the dash dot line style the hartigans dip test for unimodality on the residuals of the site specific gam that has effectively detrended the data has a p value of 0 88 not statistically significant after about 1998 one agency usgs measured the well in the spring and another agency usscs nrcs measured the well in both the spring and fall the expected power of the test has limitations because alternatively the kruskal wallis rank sum test on the water levels site specific gam residuals and site specific svm residuals produce small p values for all three as 0 0001 which is an indication that the agency source is a statistically significant alpha 0 01 issue and that further supervisory review for this well is suggested 4 1 7 the gam and svms for well usscs 362118090313201 all four gams and svms for this well fig 13 are available nothing particularly remarkable about the trend lines is seen for this well systematic downward trends in the data and the neighboring data are evident and the four models clearly reproduce these trends his well was used to create the 2016 potentiometric map by mcguire et al 2019 4 2 discussion the seven examples out of 18 903 wells provide a representative range of situations for database review and time series modeling examples of outlier identification also are shown when copious data exist reliable monthly estimates from either site specific or regional models are likely to be robust when the data span or extend beyond the time period of estimation interest the period october 2007 through december 2018 for a subject well the site specific estimates would generally be preferred over the regional estimates the use of the neighboring data in contrast promises to synthesize more information at the price of heuristic judgement of algorithm settings the visgwdbmrva software uses a simple circular selection scheme for neighbors and no local scale spatial trend assessment thus the regional models often still are deemed to produce reliable results other aquifer systems might require recasting the neighborhood search into non circular shapes or adding spatial trend information such as inclusion of horizontal coordinates and land surface altitude or other reference altitudes secs 3 7 and 3 9 there are difficulties to developing settings for the statistical computations that are universally sufficient for the mrva such settings include the statistical answers being sought the choice of modeling approach minimum sample sizes search radii number of neighbors and structural form of the model iterative study is thus advised also the study of water levels in aquifers other than the mrva could lead to other setting choices metadata are required and highly informative when large scale computational methods are used the agency source lev agency cd level source lev src cd and level status lev status cd codes when available offer opportunities to conduct hypothesis testing on data compatibility and provide a means to identify wells for further supervisory review the level agency code was shown to be useful as part of database review as was shown for well usscs 343532091211601 fig 12 data stewards when using extant field records might be able to reconcile the apparent differences between agency sources example detections by hartigans dip tests and kruskal wallis tests were shown for this study level status codes also can be useful for assessing outliers because such codes document situations affecting the measurement such as recent pumping though such data are useful there is a general absence of level status codes which is unfortunate from the perspective of computational informatics the codes can be useful for efficient supervisory screening of outliers for example downward spikes identified to be outliers could be associated with documented or undocumented pumping in the database of the well or nearby wells whereas upward spikes tend to be more difficult to explain the level status code of r meaning the water level is affected by recent pumping is shown only once within the time series of two of the seven example wells wells usgs 331101090294801 fig 9 and usgs 331750090294501 fig 11 metadata about individual measurements fig 6 is useful not commonly provided nwis metadata and possibly databases of other agencies recognize data rejection codes for example a level aging code lev age cd not otherwise mentioned herein in nwis keeps individual water level measurements from appearing in public retrievals from the internet but such data can be retrieved by database administrators at public request statistical modeling can produce reliable and continuous estimates of monthly water levels but the analyst needs ways to detect unreliable estimates congruence between gam and svm estimates is one way such could be achieved with time series modeling of water levels there is undoubtedly information in using groundwater extraction data as predictor variables but such information is simply not available at well specific resolution for the techniques described here we explicitly refrain from determining a single preferable modeling approach gam or svm based but highlight how the two model types can be used in tandem for instance lack of model congruence can be an indicator diagnostic that insufficient information exists numerical congruence between the estimates by the gam and svms could be used by other scientists as a method to quantify data extension into situations of extrapolation and thus used to limit later use of monthly estimates asquith 2020 considers the relative performance of gam and svms at conditions of extrapolation in detail for example if site specific or regional gam and svms differ in absolute value by about 6 1 m 20 ft native units or even as tight as 3 05 m 10 ft then the estimate pairs perhaps should be rejected this is the criteria contained in the check delta between models variable in asquith et al 2019 see file visgwdb control r and file output pozo aheader txt in the visgwdbmrva software for the seven wells in this study as extrapolation progresses a gam curving away from the global mean and the svm regressing towards returning to the global mean is a complementary benefit of using gam and svm approaches in tandem congruence in the estimates as judged by some exceedance threshold could contribute to information weighting within pest welter et al 2015 where more certainty when the estimates are similar and less certainty when the estimates are not additionally identification of data gaps in the mrva could also be associated when gam and svm congruence is lacking and could contribute to network analysis asquith 2020 5 conclusions groundwater flow modeling scientists require copious amounts of water level information for the mrva and other aquifer systems in the region state boundaries and regional interests naturally have resulted in complex data histories and challenges remain for the accurate and consistent revision of data and metadata with provable error s to all instances of those data amongst the various databases where they exist although seemingly abundant data exist in nwis as of april 22 2019 18 903 wells and collectively 287 272 measurements data availability is highly variable in time and space interpolation and extension are needed as part of data syntheses computational informatics are appealing because visualization and statistical estimation can simultaneously document inherent uncertainties about water levels in space and time the tandem use of gams and svms for site specific and local neighborhood modeling can produce useful monthly estimates along with uncertainty estimates and outlier detection this study shows selected monthly estimates from computational informatics using the visgwdbmrva software asquith et al 2019 using the output of the preprocessor infogw2visgwdb software asquith and seanor 2019 for purposes of data quality assurance visgwdbmrva also supports database review sophisticated visualization outlier identification and techniques for review of wells having solitary measurements the vast majority of observed measured water levels in the mrva by the usgs and other agencies presently stored in the nwis database are self consistent intra well consistency or are consistent with data for neighboring wells short distances away inter well consistency declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank many colleagues associated with the water availability study for the mississippi river alluvial plain other studies and usgs database administration these colleagues have helped identify needs of the scientific community that have predicated the informatics described jeannie barlow brian clark mike fienen courtney killian steve peterson d charlie smith d sam wallace toby welborn and jeremy white the authors would like to thank howard reeves jason ramage and elena crowley ornelas for usgs peer review along with the anonymous peer reviewers these reviewers collectively have improved the manuscript any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government this study funded by the u s geological survey usgs water availability and use science program wausp appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104758 
25932,large scale computational investigations of groundwater levels are proposed to accelerate science delivery through a workflow spanning database assembly statistics and information synthesis and packaging a water availability study of the mississippi river alluvial plain and particularly the mississippi river valley alluvial aquifer mrva is ongoing software visgwdbmrva has been released as part of the study that demonstrates groundwater informatics for the aquifer considerable water level data collected by multiple agencies over a seven state area exist 18 903 wells 287 272 measurements april 22 2019 data and metadata quality assurance methods basic statistics hydrograph visualization outlier identification hypothesis testing and time series modeling are described two approaches generalized additive models gams and support vector machines svms are used for data interpolation and extension to monthly water level estimates numerical congruence between gam and svm estimates will be useful to limit inclusion of monthly estimates from subsequent science activities keywords mississippi river valley alluvial aquifer water levels statistics generalized additive model support vector machine 1 introduction regional aquifer systems supporting intensive production agriculture or other water supply uses are economically vital resources for making water resources decisions maupin and barber 2005 aquifer systems may have state level records of tens of thousands of drilled wells the data sources can include various federal state and local entities intricate multi agency responsibilities often exist in groundwater data collection and data stewardship these complexities are partly caused by administrative and political boundaries and historical community level views of groundwater resources as local or regional rather than national assets ownership and stewardship of field data hard copy and digital and digital groundwater databases are thus distributed for these reasons data aggregation and database management for a comprehensive study of groundwater levels is complex and difficult one aquifer of national scale importance for production agriculture for which these reasons are acutely applicable is the mississippi river valley alluvial aquifer mrva located in the south central united states ackerman 1996 ausbrook and prior 2008 reba et al 2017 sahoo et al 2017 renken 1998 and is a focus of this study large scale computational investigation of water levels groundwater informatics involves comprehensive statistical data syntheses of discrete measurements and continuous records data syntheses and timely delivery of information are critical to support interpretive science and numerical hydrologic modeling including groundwater flow u s geological survey 2019a simulation models are used to assess water resource availability help identify information gaps and support recommendations for future data needs in april 2019 the u s geological survey usgs national water information system nwis database u s geological survey 2019b provided field measurements of groundwater water levels cunningham and schalk 2011 for nearly 890 000 wells and daily mean values for nearly 8 000 wells in the united states nwis also contains many metadata slots and codes about wells and measurements many of the wells and water levels in nwis are associated with other agencies beyond historical and contemporaneous usgs data activities many wells exist in other agencies inventories and within their own monitoring networks and databases these facts should be considered also within the context of the 20th century transition from purely discrete paper records analog recorder data and manual transcription to increasingly common digital field computers electronic telemetry and enterprise scale information delivery discrete measurements as a rule though represent the majority of spatially distributed data of water levels a general stakeholder need for cost effective and timely science requires interoperability between data hydrogeologic mapping and analyses and numerical simulation models potentiometric surface maps and water level change maps provide valuable visual syntheses of groundwater data and these maps are readily interpreted by scientific and non scientific audiences mcguire 2014 mcguire et al 2019 2020 numerical models can be either steady state or nonsteady state transient depending on science objectives and the needs of resource management clark et al 2011 p 40 for both model types copious quantities of groundwater level information are vital water level statistics reduce the cognitive burden to those with different science objectives such as data quality assurance potentiometric surface evaluation model calibration uncertainty assessments information gap identification or network analyses monthly time step nonsteady state simulations benefit from inclusion of continuous observed monthly water levels however sufficient amounts of continuous time series data to compute successive monthly means usually are lacking for many wells and such monthly means are the rare exception rather than the rule in most cases statistics can be used to create continuous monthly estimates from discrete water level measurements by statistically interpolating or extending observations a complete although estimated time series can be compared to output from a groundwater model at any simulation time statistical estimates of continuous monthly water levels in conjunction with projected uncertainties potentially accelerate and enhance numerical model development welter et al 2015 hughes et al 2017 statistical time series estimation of water levels has been documented in a previous study of the aquifer sahoo et al 2017 p 3 880 used an inverse distance weighting method to spatially predict and estimate discontinuous gaps in mrva water levels through machine learning both observational and statistical uncertainties are key to assign weights to input data or water level estimates in numerical modeling these weights when combined with hydrogeologic knowledge and judgment form the basis on which some model parameters are estimated and help to assess the value of both existing and potential new data for improving accuracy of model forecasts synthesizing the information content of groundwater level databases thus is an important preprocessing step ahead of numerical groundwater flow model calibration because of 1 continual changes in data availability in time and space as water level data for new and existing wells are acquired or as cross agency databases are merged reconciled and erroneous information reduced 2 continual changes in scientific understanding of aquifer features through acquisition of measured aquifer properties or enhanced constraints on the hydrogeologic setting aquifer bottom and other formation contacts through geophysical methods and 3 continual enhancements in numerical flow model capacity uncertainty analyses statistical simulation and parameter estimation capacities and inferred model calibrated aquifer properties for these reasons cost effective and timely science requires computation of representative annual and monthly observed water levels from discrete and continuous daily mean measurements the iterative and rigorous processing of observed water levels for estimation of continuous time series of monthly data also is needed both the computed and estimated monthly data should be structured for rapid update and subsequent insertion into data worth data importance and uncertainty assessments white 2017 the estimation of continuous monthly values for the 135 months october 2007 through december 2018 is of current 2020 interest by the groundwater flow modeling team of the map study in assessment of data worth data importance and uncertainty analyses as well as unsteady state groundwater flow model parameter estimation monthly estimates are useful even in areas of the map considered to have intensive data collection such as the delta region figs 1 and 3 water levels in the mrva are often just semi annual spring and fall seasons this necessitates use of statistical methods to interpolate and extend water levels from discontinuous real world data to continuous monthly information with attendant uncertainties statistical errors monthly water level estimation does not supplant but only augments numerical modeling because statistics do not provide the benefits of numerical modeling including flux estimates residence time computations and managed recharge sahoo et al 2017 p 3 892 1 1 software availability the results from this study were estimated using publicly available r software titled visgwdbmrva asquith et al 2019 and its associated preprocessing infogw2visgwdb software asquith and seanor 2019 basic operational instructions defaults and reference output examples that parallel the visual results are depicted in this paper further the suggested tag for downloading the version used at the completion of this paper is v1 0 4 the repository tags are accessible may 28 2020 at https code usgs gov map gw visgwdbmrva tags delivery of visgwdbmrva through a formal source code repository framework fosters scientific reproducibility and specific reference to files of the visgwdbmrva repository are made herein as needed 1 2 purpose and scope the purpose of this paper is to describe groundwater informatics through the visgwdbmrva software a review of features is made by demonstration of selected results towards an objective of estimates of monthly water level estimates based on april 22 2019 data for wells screen in the mrva in addition the software is used to identify potentially erroneous outlier data and justify use of statistics for quality assurance of the data the scope of this study is limited to the description of underlying and mrva data in nwis as of april 22 2019 although the software can be used to estimate water levels for all 18 903 wells in the nwis database seven wells that represent different data issues are used here as examples the time period for the water level estimates is october 2007 through december 2018 but water levels are shown on the hydrograph for the entire period of record for each well plus the estimation time period lastly the map boundary painter and westerman 2018 was used as a clipping extent to remove wells for which nwis had coded as mrva but are geographically plotted outside the map 1 3 paper organization the study area is reviewed in section 2 and data sources and background on methods are reviewed in section 3 and data forensics are described in section 3 1 basic statistical methods used to describe the information content of example water level databases follow in section 3 2 section 3 4 describes two approaches for time series modeling generalized additive models sec 3 6 and support vector machines sec 3 8 these models were used in tandem for continuous monthly water level estimation software settings applicable to both approaches are described in section 3 5 monthly estimates are presented in section 4 with detailed discussion for each of the seven wells in section 4 1 general discussion and interesting facts about monthly water level estimation are offered in section 4 2 conclusions and advice on groundwater database stewardship are then offered sec 5 a monospaced font denotes specific fields from the nwis database computational informatics by the visgwdbmrva software are demonstrated through selected results herein when deemed necessary for description of specific algorithmic details in sections 3 7 and 3 9 statistical functions within the r software r development core team 2019 or other functions from add on statistical libraries packages are identified 2 the mississippi river alluvial plain and mississippi river valley alluvial aquifer the usgs is involved in a multi year multi discipline regional water availability study for the mississippi river alluvial plain map more information on the study is available online accessed may 28 2020 at https www2 usgs gov water lowermississippigulf map index html for the map study the usgs in cooperation with more than 10 federal state and local stakeholders is comprehensively studying and modeling the mississippi river valley alluvial aquifer mrva and associated hydrogeologic units clark et al 2011 fig 10 underlying the map fig 1 the map is subdivided into seven generalized regions ladd and travers 2019 the mrva is the uppermost hydrogeologic unit that overlies the mississippi embayment aquifer system clark et al 2011 u s geological survey 2015a the aquifer is critical for irrigation reba et al 2017 and contributes to the map being a premier agricultural region water level declines in the mrva and in map streams have been shown to coincide with an increase in withdrawals for irrigation since the late 1980s barlow and clark 2011 the map is an extensive flat alluvial plain extending beyond the historic floodplain of the mississippi river and other proximal streams clark et al 2011 kleiss et al 2000 renken 1998 and is a major physiographic feature spanning parts of seven states the central and northern sections of the map mostly are used for intensive production agriculture and primary crops include cotton rice corn and soybeans the mrva fig 2 is the shallowest hydrogeologic unit underlying the map fig 1 and it extends southward from the head of the mississippi embayment aquifer system clark et al 2011 and merges with the coastal lowlands aquifer system martin and whiteman 1999 barlow and belitz 2016 u s geological survey 2015b the coastal lowlands aquifer system fig 1 is a band of sediments in low lying areas parallel to the gulf coast renken 1998 the extent of the mrva for simplicity is coincident with the map boundary painter and westerman 2018 which is the default of the infogw2visgwdb software asquith and seanor 2019 the aquifer spans about 53 000 square kilometers km2 about 33 000 square miles mi2 and ranges in width from about 120 km 75 mi to 190 km 120 mi sand gravel silt and minor clay deposits of quaternary age make up the aquifer renken 1998 schematics of conceptual horizontal and vertical hydrogeologic properties of the mrva are shown in fig 2 the hydrogeologic complexity of the mrva contributes to considerable variations in water level altitudes this observation remains even when ignoring other variations induced by extensive though site to macro scale specific pumping and surface water recharge and discharge histories barlow and clark 2011 mrva hydrology and hydrogeology are also greatly influenced by many streams and rivers that incise it figs 1 and 2 clark et al 2011 fig 10 when a stream incises the aquifer the stream may 1 provide recharge to the aquifer 2 receive discharge from the aquifer or 3 temporally do either the spatial distribution of areal recharge and its identification is complicated dyer et al 2015 large groundwater withdrawals have resulted in long term declines of water levels in some areas and have diminished aquifer discharge to some of the streams barlow and clark 2011 killian et al 2019 key parts of the map study are the systematic evaluation of water level data and well inventory information using statistical forensics mrva water level data collection history is long and complex which necessitates informatics to package key water level information for other science teams and stakeholders asquith et al 2018 asquith et al 2019 killian and asquith 2019 as of april 22 2019 there are 18 903 wells which plot within the map boundary painter and westerman 2018 with 287 272 water level measurements stored in nwis these nwis data are embedded in the visgwdbmrva software asquith et al 2019 to provide a degree of reproducibility of results specific to this paper the locations of all of the wells used in this process are depicted in fig 3 however the total reported in nwis is considerably smaller than the number of wells in existance for example a separate database not stored in nwis or reflected in the aforementioned counts of mrva well drillers reports in missouri maintained by the missouri department of natural resources modnr exists missouri department of natural resources 2018 as static water levels for more than 16 000 wells the modnr data are known to have data quality issues including excessive measurements of exactly zero smith et al 2019 and considerable evidence of excessive rounding of measurements favoring even numbers and those wells located only to the nearest 3 m 10 ft native units driller s reports such as the modnr database do contain valuable information but care is needed in how water level data are from these reports is used iterative reviews including use of visgwdbmrva can help screen wells with water level altitudes that seem consistent with other wells already in the usgs nwis database also on a well by well basis if a subject well differed by more than about 6 1 m 20 ft in water level altitude from statistical generalization of neighboring wells such a well was removed after further supervisory review mrva water level syntheses are complicated by temporally inconsistent data acquisition and management storage practices further factors conflating efficient study include the changes or differences in field practices between data collection agencies and follow on record keeping and in perhaps the last two decades the establishment and administration of enterprise scale databases non spreadsheet thus there are appreciable data completeness and consistency issues to overcome substantial changes in data acquisition cunningham and schalk 2011cunningham and schalk 2011 and storage practices ranging from analog to digital methods in the past century further complicates data syntheses data consistency issues and the presence of potentially erroneous outlier data can hinder timely and reliable science delivery and affect subsequent management decisions and political discernment by stakeholders there also exists for the mrva considerable 1 differences in spatial density of wells and disparate amounts of water level data over time 2 spatial variation in the horizontal and vertical hydrogeologic properties ackerman 1996 arthur 1994 and 3 variation in availability of well construction details screened open intervals these issues interplay with water use on short local spatial and temporal scales that produce nuanced patterns in water levels in space and time barlow and clark 2011 3 data sources and methods the visgwdbmrva software facilitates data preparation subsequent parameter estimation and ancillary study of the groundwater flow model rapid updates are important whenever the underlying database contains substantial new or revised data in reviewing a new forecast first modeling paradigm for groundwater simulation white 2017 p 663 states that by making the simulated forecasts more prevalent and accessible during each stage of the groundwater modeling analysis advances in appropriate modeling strategies will be facilitated the estimation of continuous time series of monthly water levels fits white s objectives by making groundwater level information syntheses more prevalent and accessible the software visgwdbmrva can efficiently update results as more data become available and improve quality assurance of both usgs and furnished data the software produces statistics of mrva water levels and for other aquifers in region and assembles groundwater level information the output is varied and extensive asquith et al 2019 file readmeoutput md the input data standard of the software is described by asquith et al 2019 file input gwcolumns gwcolumns pdf the data for this study are limited to water levels and associated metadata available in the nwis database u s geological survey 2019b during the period 2016 19 considerable efforts were made to incrementally expand the amount of water level data for the mrva in nwis by entering newly acquired usgs and newly acquired furnished and legacy data from other agencies efforts were also made to merge interlace daily values from continuous hourly recorders into discrete measurements the derived daily means are different from discrete measurements but they are fungible for this study and both are referred to as measurements including water level data from furnished sources involves exhaustive and iterative reconciliation of other agency well numbering schemes and legal descriptions of both well location and name reconciliation was needed because a common well identification scheme is not used by all agencies instances of lost duplicated or triplicated data could occur when separate databases are naively merged 3 1 statistical forensics for general groundwater database review statistical forensics intended to help identify potentially erroneous data water levels and metadata were made the identification of such data does not automatically mean that these data are rejected but provides guidance for validating database integrity some forensics include identification of water level measurements depth below land surface of zero or negative a water level at or above land surface in parts of the map for which such conditions were expected based on hydrogeologic context and data for nearby wells identification of water levels below the bottom of the well which physically is not logical however note that the reported depth of the well could be the cause and not the measurement value itself another possibility is that a well has been deepened and such information has not been tracked identification of water levels below the lowest opening in the well measurement in the sump of the well as opposed to formation waters this forensic is predicated on well construction data being available and accurate which might not be the general rule fig 2 identification of water level measurements at depths below best available estimates of the altitude of the base of the mrva torak and painter 2019 these could indicate measurement errors or wells erroneously labeled as screened only in the mrva identification of potential duplicate measurements an effective scheme for this is to make a hash table of a key value pair with keys being concatenation of a well identification number date stamp and water level measurement and review of the distribution of the first two significant figures nigrini 2011 of the measurements for example consider two measurements of 3 4 m and 34 m both are encoded for inspection based on the ideas of nigrini 2011 as 34 such a review by infogw2visgwdb software asquith and seanor 2019 permits assessment of rounding tendencies by different collection agencies data rounding could be motivated by natural human tendencies to favor even numbers or numbers involving trailing 5s or 10s decimal shifts as potential errors also can be identified for further scrutiny the r package benford analysis cinelli 2017 supported the review of significant figures an example review of the first two significant figures following other data quality assurance screening or filtering of the modnr well drillers report database smith et al 2019 of about 10 000 modnr wells each with a solitary measurement is shown in fig 4 in the infogw2visgwdb software this drillers report database is held external to the nwis database the distribution of the first two significant figures shows considerable rounding and biases towards even numbers including 5s and 10s decimal shifts could also be present but the left part of the figure shows no measurements of better resolution than 1 ft these data characteristics possibly originate from the field observation or measurement methods used for some of these water levels which lack the precision of other methods such as a calibrated steel tape cunningham and schalk 2011 as a general rule the reported measurements are from unknown methods the annotations in the figure are intended to clarify interpretive nuances such database review and comparisons to similar plots conditioned on agency source or other metadata help to inform data stewards about relative data quality for an example like this and from a perspective of groundwater informatics large sample sizes number of wells would yield unbiased statistics if the field data themselves have been observed or otherwise measured in unbiased ways although individual measurements could have large though unquantified error statistics are useful to assign relative uncertainty to solitary data measurements which is critical for pest assistance in flow model parameter estimation as of april 22 2019 and after clipping to the map boundary nwis contained 18 903 wells with collectively 287 272 measurements including daily values from continuous recorders u s geological survey 2019b the earliest measurement date is december 31 1899 but this measurement date in nwis likely is erroneous for the simple reason of occurrence at the end of a year the most recent measurement at the time of the writing of this paper was april 17 2019 which made available data through the 2018 map irrigation season the irrigation season depends on many factors but generally occurs between april and october the number of wells and measurements for arkansas louisiana mississippi missouri and tennessee are 4 751 164 104 3 588 15 224 9 626 102 738 904 5 091 and 34 115 respectively as of april 22 2019 there were no water level measurements in mrva wells in nwis for the parts of illinois or kentucky that lie within the map boundary there also are great measurement count discrepancies amongst the wells for example the minimum and median numbers of measurements are just 1 the arithmetic mean is 11 the third quartile is just 5 and the maximum is nearly 3 000 nearly 10 years of continuous data these counts support the assertion in section 1 that sufficient amounts of continuous time series data to compute successive monthly means usually are lacking for many wells and such monthly means are the rare exception rather than the rule which justifies the utility of estimation of monthly values by statistical methods described in this study 3 2 basic statistical methods for data quality assurance all the statistics and methods described in this report exist in output files of the visgwdbmrva software basic statistical methods are useful towards data quality assurance statistical summaries provide initial steps towards detection of potentially erroneous data and subsequent database reconciliation between data collection agencies for a given year range and or range in months for the year range statistics such as minimums medians means maximums and respective dates of minimums and maximums can be informative data features such as sample size number of unique years standard deviation and other variation and skewness measures l moments asquith 2011 2018 as well as decadal means also provide information packaging another useful statistic to subset the database for further scrutiny is the intra annual standard deviation year over year variation in the annual means more sophisticated statistics than these are useful for hypothesis testing in order to enhance review and achieve data quality assurance for example null hypotheses such as 1 the data have no monotonic trend or 2 differing agencies collect data with similar statistical properties the latter example includes null hypotheses such as 1 the data have a unimodal distribution and 2 the same median in absence of an overall trend with these hypotheses in mind processing all results not reported herein for a given well asquith et al 2019 include 1 kendall s tau statistical trend test helsel and hirsch 2002 of the water levels with time which assesses monotonic trends over time 2 hartigans dip test for unimodality hartigan and hartigan 1985 maechler 2016 of water levels conditional on metadata including agency source being a grouping variable and 3 kruskal wallis rank sum test helsel and hirsch 2002 for comparison of data medians also conditional on agency source these tests involve assessments of data compatibility between sources in other words the hartigans dip test and kruskal wallis rank sum test indicates that the data credited to different agencies or sources are statistically similar and hence compatible the statistical time series modeling also facilitates more elaborate database reviews asquith et al 2019 for example the water level residuals of a time series model that represent detrended information are used for another dip test of unimodality conditional on agency source or a levene s test from fox and weisberg 2011 for homogeneity of variance across agencies the kruskal wallis tests on the residuals conditional on the agency source again are used to assess data compatibility a well known estimator of slope trend in time related to kendall s tau is the slope from the nonparametric theil or theil sen line helsel and hirsch 2002 this slope is a simple estimate of overall changes in water levels and studying wells that have unusual slopes relative to others could be informative and useful in regional assessment of trends and hence detection of unusual data for a well 3 3 visualization techniques and example hydrographs for seven wells data visualization is important for communicating the information content of a database and statistical results comparing an individual well hydrograph with hydrographs for nearby wells and reporting any associated statistical results is an effective means of communication to local stakeholders the 18 903 wells in nwis is a large number and implies that comprehensive visualization is a prodigious effort thus the seven example wells and their corresponding hydrographs were selected to provide proof of concept demonstration of the visgwdbmrva software asquith et al 2019 only one of the seven wells selected for this study has just one measurement simultaneous depiction of measurements at nearby wells can be especially effective for data review for such wells the other six wells were chosen because of spatial distribution to illustrate a variety of data properties and statistical results in aggregate the seven wells show data features commonly found in large aquifer systems such as abundant neighboring well records data from a variety of sources and inconspicuous outliers sophisticated hydrograph visualization inherently is information dense and cognitively complex an expansive list exists of symbol sets and coloring pallets to depict the data for a given well along with neighboring well data a common explanation applicable throughout the hydrographs of the seven wells is shown in fig 5 the visgwdbmrva documentation contains highly complementary material to this paper by depicting hydrograph without statistical time series estimation focus of this paper for the same seven wells asquith et al 2019 file inst doc examplehydrographs examplehydrographs pdf the agencies involved in collecting data for a given aquifer vary in time and space as a result the visgwdbmrva software automatically expands or contracts its symbol pallet fig 5 as needed for a given input data set this is an important point because it means that an applicable explanation is itself mutable by the unique agencies encountered as data are loaded the color pallet conversely is fixed for the complete nwis level source code list a d l o r s u and z though these as letters are not drawn just the corresponding color is used 3 4 generalized additive model and support vector machine approaches two different modeling approaches are implemented in visgwdbmrva to depict the temporal variation in water levels generalized additive models gams and support vector machines svms the choice of these two approaches was influenced by a need to identify potentially erroneous well specific data and also provide both site specific and neighborhood water level estimates including monthly estimates considerable background for the topic of statistical prediction is found in hastie et al 2008 kuhn and johnson 2016 and wood 2017 though gams and svms differ from one another considerably in mathematical formulation and nuances of use in a loose conceptual sense these two approaches are analogous to well known least squares regression techniques faraway 2005 specifically within each a response variable can be estimated by one or more predictor variables gam and svm approaches are more sophisticated relative to least squares regression methods faraway 2005 which are available in the built in r function lm and local regression smoothing methods cleveland 1979 cleveland et al 1992 which are available in the built in r functions lowess and loess incidentally jones et al 2014 provide a combination of spreadsheet and r software gwsdat with a graphical user interface for plotting time series of groundwater solute concentrations based on local linear regression to depict trends and uncertainty bounds in solute concentrations both gam and svm approaches are flexible and complement each other the various settings and configurations of the gams and svms described herein asquith et al 2019 file include visgwdb control r were developed as part of recurring episodes of database review and erroneous data reconciliation some heuristic judgement is implicit and ad hoc criteria such as overall sample sizes evolved as the authors reviewed many thousands of hydrographs and statistical predictions 3 5 general settings for gam and svm construction two different styles of analyses were made first expanding on the hydrographs for the seven example wells the fits of a site specific gam and svm for each subject well were made second the wells in the local neighborhood of each target well were used for fitting regional gam and svms before the mathematical details of the gam and svms are described discussion of general settings used follows for the site specific analyses the minimum sample size number of water levels for the gam and svm could be independently controlled if insufficient water levels are available no fitting is made and thus no residuals are available for this study a minimum sample size of 10 water levels was chosen for both styles of analysis before the gam and svm were fit about 75 percent of the wells lack sufficient data less than 10 measurements for the site specific gam and svm the minimum of 10 is a small sample size but a performance review by the authors suggests that this is reasonable as a trade off between the ability to explore for outliers and provide monthly estimates with associated uncertainty and simultaneously useful in the same computational run for outlier identification for the regional analyses to create a gam and svm herein circular search from 0 to 8 km 0 5 mi radius was made with the models accepting an unbounded number of water levels from up to 300 nearest wells neighbors other settings were as described for the site specific analyses this same radius was used for the construction of the seven hydrographs sec 3 3 for this study outliers are jointly defined using a residual magnitude of at least 3 3 m 10 ft other choices could be made as well as being beyond the 1st or 99th percentile other limits could be chosen of the predicted error distribution for the estimation the definition of an outlier does not materially impact the construction of the time series models the visgwdbmrva software does not dynamically remove outliers it only flags them 3 6 generalized additive model estimation of mrva water levels gams are flexible and capable of mimicking curvilinear patterns in data hastie and tibshirani 1990 hastie et al 2008 wood 2017 gams model a response variable using an additive combination of various optional parametric terms and smooth functions of predictor variables the incorporation of smooth functions is an advantage over simpler multi linear least squares regression and similar methods because appropriately configured smooth functions can adapt to nonlinear relations the general form of a gam is 1 y i Œ≤ o x i Œ∏ f x i Œµ i where y i is the response variable untransformed herein that is the ith water level observation Œ≤ o is an intercept x i is a one row matrix for strictly parametric and suitably transformed predictor variables Œ∏ is a one column parameter matrix the f is a smooth function itself having arguments that is estimated automatically for the predictor variable x i the three dots represent additional smooth terms as needed and Œµ i are errors taken as independent and identically distributed with zero mean the x i Œ∏ term is the multi linear parametric regression faraway 2005 component of a gam for this study y i is water level altitude in the mrva and x i is a numerical representation of the date of water level measurement the parametric terms could include trigonometric operations used to express seasonality or land surface altitude only trigonometric operations thus far have been thoroughly studied a gam is fit by the defaults provided in wood 2019 using penalized iterative least squares wood 2017 p 180 182 and a generalized cross validation a type of leave one out score for the smoothness controls on the smooth functions wood 2017 p 171 a setting in visgwdbmrva software use of gams was a multiplier on the sample size set to eight controlling when simple first order seasonal terms are to be evaluated the multiplier was set to four if the number of measurements exceeded 40 4 10 four times the minimum sample size then trigonometric functions were conditionally used in a provisional gam to capture potential seasonality for this study the parametric terms potentially included were simply cos 2 œÄ d i and sin 2 œÄ d i in which d i is the day fraction into the year of the measurement date if both the p values are 0 005 then the two trigonometric terms are retained otherwise they are dropped and the original gam is used the fact that both p values are required to be 0 005 is a strict setting for example it would be justified to accept the two terms if only one of them was statistically significant helsel and hirsch 2002 p 342 but because of the scale of automated processing it was explicitly decided to have a tight threshold of maximum p value for both terms for this study the fact that considerable historical effort by others to collect semi annual data spring and fall in much of the mrva and the presence of a substantial underlying seasonal trend the two trigonometric functions were present in many of the regional gams and much less commonly in the site specific gams because of sample size limitations the initial form of the gam was 2 y ÀÜ i Œ≤ o a cos 2 œÄ d i b sin 2 œÄ d i f x i Œµ i where y ÀÜ i is a predicted water level Œ≤ o is an intercept a and b are regression coefficients d i is day fraction into the year and x i is time in decimal days from a time origin the two trigonometric terms represent a simple and symmetrical seasonal component which often capture anticipated timing of aquifer rebound or recovery in mrva water levels by about late march to mid april prior to or at the beginning of the next irrigation season more sophisticated seasonal terms were evaluated using higher order cos k œÄ d i and sin k œÄ d i terms for k 4 6 more trigonometric functionality higher orders of k œÄ could produce asymmetrical seasonal patterns compared to a simple symmetrical sinusoidal pattern that is mandated by 2 œÄ terms alone after exploratory analyses and for general application for the mrva deliberately it was decided to keep only the two terms involving 2 œÄ in many parts of the mrva most water levels are made in aprils and octobers with such sample timing consistency the data density to compute timing of peaks real world seasonal aquifer maxima and troughs real world aquifer minima of mrva water levels based solely on statistical analyses likely is lacking a final note is that gams by their nature can eventually extrapolate away from the global central tendency of the data in contrast to svms 3 7 generalized additive model implementations for the gams a gaussian family for the generalized linear model glm faraway 2006 was used to estimate the parametric terms and largely default parameters arguments of the gam function provided by the mgcv package wood 2019 for documentation purposes it is useful to described how the gam was implemented by the visgwdbmrva software and then contrast it with other tested versions the gam constructed by the gam function in the mgcv package wood 2019 was similar to the following gam1 gam y s x bs cr cosine sine weights wgts where gam1 is the fitted gam to the data y is a vector of mrva water level measurements s denotes a smooth x is the day month year of the measurement and is equivalent to the d i and the cosine and sine represent the cos 2 œÄ d i and sin 2 œÄ d i parametric terms representing the respective trigonometric operations of the day fraction into the year of the measurement date d i as previously described a gam can use observation weights the wgts is optional though used meaning actually turned on within visgwdbmrva for this study vector of weights which are inversely proportional with distance from the subject well to the neighboring wells the bs cr shows that a cubic regression spline basis function of the gam smooth f x i was used the trigonometric functions were potentially dropped in accordance with the previous p value discussion finally note that gam1 was used for both site specific and regional gams days as a time object in the r software are in seconds from the beginning of an epoch 1970 01 01 in visgwdbmrva conversion of the dates to years by division of the number of seconds in an average year 60 60 24 365 25 is necessary before initiating the gam function to avoid fatal numerical overflow errors in the gam algorithms wood 2017 the conversion means that the gam algorithms see a floating point number of years since the epoch beginning noting that spatial data were also available for study these other gams were evaluated for the regional modeling gam2 gam y s x bs cr sine cosine s e n bs tp gam3 gam y s x bs cr sine cosine s a bs tp gam4 gam y te x e n bs tp sine cosine the gam2 has an additional smooth term for spatial location specifically added are the e easting and n northing coordinates of an albers equal area conic coordinate projection of the latitude and longitude of the nearby wells and the subject of a 2 dimensional thin plate regression spline bs tp the gam3 has an alternative smooth term using the a land surface altitude and the gam4 is more complicated the prospect of gam2 as being preferable was a preliminary idea because the addition of a spatial trend term is intuitively promising however exploratory testing suggests that although there is enough variation in the joint spatial and temporal density of observations there appear to be insufficient data overall in the mrva for gam2 as a result the gam2 sometimes provides unreliable estimates for the available data exploratory testing indicates that gam3 does not have much of a performance gain because the map is topographically flat and the neighboring data are selected on a relatively small radial distances around any given subject well also land surface altitude as a predictor might be less attractive for confined aquifer systems such as the confined sparta sand fig 2 and finally in some hydrogeologic settings the altitude of the base of an aquifer could be useful the gam4 has the time and spatial coordinates modeled using a tensor smooth estimator te in three dimensions with a thin plate regression spline this tensor form of smoothing is needed because the times and spatial locations are measured on inherently different unit systems wood 2017 2019 similar to the gam2 testing also suggests that gam4 sometimes provides unreliable estimates for the available data these three gams have been experimentally used over many iterations some minor variation in control setting and basis functions supported by wood 2019 also were experimented with for this study future designs for model form and ancillary settings could be different as understanding improves or for aquifer systems other than the mrva finally precise implementation strategies for a generalization beyond the mrva of groundwater informatics using gams are difficult to propose without iterative study 3 8 support vector machine estimation of mrva water levels svms are a type of machine learning approach to prediction in which complex linear combinations of specific data points and attendant weights are used to define a hyperplane through the data entire data set contemporaneous study of mrva water levels using machine learning through a multilayer perceptron network has been made sahoo et al 2017 some salient remarks by those authors suggest that data driven and machine learning methods based on nonlinear interdependencies may be able to predict groundwater level change without deep knowledge of the underlying physical parameters sahoo et al 2017 p 3 878 behzad et al 2010 conducted a comparative study of svms and artificial neural networks anns for groundwater level estimation including monthly time scales and noted that svms outperformed anns and that svms have the potential to be a useful and practical tool for cases where less measured data are available for future prediction svms clifton et al 2014 kuhn and johnson 2016 hastie et al 2008 pimentel et al 2014 steinwart and christmann 2008 like gams also are flexible and capable of mimicking curvilinear patterns in the data and svm description is best setup as an analogy to linear regression svms are a type of robust regression in which squared residuals are not used for fitting as they are in least squares regression faraway 2005 as a result data points with large residuals have limited effects on the fitted svm kuhn and johnson 2016 p 153 a curious and identifying feature of svms is that the data points residing near predictions for the sample made by the svm will actually have no effect some control on how large residuals can be with no effect of the svm is influenced by a so called epsilon setting Œµ svm the Œµ svm in svm implementation karatzoglou et al 2018 controls width of the prediction tube which has an effect of influencing the inclusion of data near the hyperplane defining the regression this setting is a parameter that can be tuned our experience is that the default of epsilon 0 1 seems to work well enough to leave as is in visgwdbmrva the fit phase of an svm is based on minimization of an error function subject to a penalty as the model progresses towards overfitting following kuhn and johnson 2016 the coefficients Œ≤ j of a fitted svm minimize the f o objective function for a model having p number of predictors 3 f o y i c l Œµ c i 1 n l Œµ y i y ÀÜ i j 1 p Œ≤ j 2 where c is a cost parameter that is responsible for penalizing large residuals Œµ i y i y ÀÜ i for the n sample of y i and predictions y ÀÜ i the c is a multiplicative factor on an l Œµ epsilon insensitive function to increase or decrease the importance of errors the l Œµ has the aforementioned Œµ svm setting our experience is that the default c 1 seems to work well enough to leave as is in visgwdbmrva large Œ≤ j estimates in magnitude increase the objective function as do large Œµ i these two components in the objective function contrast to methods such as least squares that use the Œµ i only faraway 2005 the Œ≤ j parameters are chosen such that eq 3 is minimized in the regression analogy an estimate prediction of y i and vector of predictors x i x 1 i x 2 i x p i of size n uses a linear combination of model parameters coefficients Œ≤ j to the new sample of the p predictors u j for j 1 2 p the prediction y ÀÜ can be written with the intercept term Œ≤ 0 as 4 y ÀÜ Œ≤ 0 Œ≤ 1 u 1 Œ≤ p u p Œ≤ 0 j 1 p Œ≤ j u j the linear svm is similar when used for estimation but the Œ≤ j estimates importantly can be written as functions of a set of additional but unknown hyper parameters Œ± i and the sample data of size n denoted as x i j so that 5 y ÀÜ Œ≤ 0 Œ≤ 1 u 1 Œ≤ p u p Œ≤ 0 j 1 p Œ≤ j u j Œ≤ 0 j 1 p i 1 n Œ± i x i j u j Œ≤ 0 i 1 n Œ± i j 1 p x i j u j two major insights into the last equation are useful first the final form shows weighted linear combinations of the formula of training data and the new explanatory data of which to make predictions kuhn and johnson 2016 p 155 show that matrix algebra can rewrite the training data and new explanatory data as a linear kernel function with notational adjustment the function itself can be replaced with a nonlinear kernel thus extending svms into nonlinear regression kernels include linear kernel polynomial kernel gaussian radial basis functions rbfs and others the rbf represents a normalized euclidean distance metric bishop 2006 chap 6 second there are as many Œ± i parameters as there are data points compared to conventional regression the svm is over parameterized through the tenet of conventional regression that the model should have vastly fewer parameters than data points kuhn and johnson 2016 p 154 but the penalty or cost used for svm fit compensates for this undesirable situation and in practice many of the Œ± i 0 vanish go to zero finally when given new data on which to make predictions svms might be less utilitarian as extrapolation increases away from the data in contrast to gams as extrapolation is encountered by the svm it regresses to the global mean of the data 3 9 support vector machine implementations for implementation the ksvm function from the kernlab package karatzoglou et al 2018 was used details of svm support are found in hornik et al 2006 for the svms a radial basis function default representing the l Œµ was used and the default Œµ svm 0 1 for epsilon setting and the default c 1 for cost applied these default settings of karatzoglou et al 2018 appear suitable for large scale systematic and automated unsupervised estimation for the mrva water levels there is not a parallel feature with the ksvm function for the inclusion of weights into the svm as was done for the gam general review of the epsilon and cost parameter settings using non default values could be illuminating but is problematic to design given the purpose of visgwdbmrva to support full regional or small scale processing killian and asquith 2019 killian et al 2019 for the mrva processing of other aquifer systems of interest to the map study or other aquifers in the nation for the purposes of documentation it is informative to described how the svm was implemented for this study and contrasted with other evaluated versions the svm was evaluated and similar to the following svm1 ksvm y x svm2 ksvm y x sine cosine where the terms are the same as described for the gam implementations previously described y is water level x is decimal time however the cosine and sine terms using day of the year were ultimately dropped for all modeling just svm1 used other types of svms were evaluated and similar to the following svm3 ksvm y x e n svm4 ksvm y x e n a the svm3 shows use of the e easting and n northing coordinates of the nearby wells and svm4 adds the a land surface altitude review of general prediction reliability when the coordinates are included indicates that svm3 can sometimes provide unreliable estimates it is thought that there are insufficient joint spatial and temporal water level data continued review of svm3 and svm4 is suggested again as said for the gams when land surface altitude is investigated as a predictor it likely would not be useful for confined aquifer systems such as the confined sparta sand fig 2 experiments were conducted using trigonometric functions in the style of sec 3 6 to contribute seasonal terms and potentially enhance the model results not reported here indicate little benefit for reliable estimation and often a deleterious impact on estimation we speculate that in contrast to the parametric form with the trigonometric terms in the gam there is almost universally insufficient near continuous monthly data a measurement occurs at least every couple of months for the svm to adequately perform with trigonometric inputs beyond what it is able to do when just restricted to using time ability of an svm lacking trigonometric explanatory variables to mimic a sinusoidal function is shown by kuhn and johnson 2016 p 156 but sufficient data must exist finally exact implementation strategies for a generalization beyond the mrva of groundwater informatics using svms are difficult to propose 4 estimates of monthly water levels for october 2007 through december 2018 for october 2007 through december 2018 both gam and svms estimated monthly water levels for each of the seven example wells using 1 observed data for a given well the subject well in the site specific model and 2 observed data for a local region surrounding the subject well in the regional model four time series models for a well are thus potentially available site specific gam site specific svm regional gam and regional svm absence of the site specific gam and svm for a particular well is caused by a well having fewer than 10 measurements with no neighboring wells within 8 km 5 mi the monthly estimates are a type of information product needed by other scientists involved in the map study and these are the vertices used to construct the trend lines depicted in figs 7 13 estimates to earlier times than october 2007 are shown in the hydrographs herein if data exist prior to that date because the visualization algorithms in the visgwdbmrva software are designed to include all data however only the october 2007 through december 2018 period was exported to a computer file system as the output pozo csv files described as part of the embedded visgwdbmrva documentation asquith et al 2019 file readmeoutput md a common shared explanation of the line plotting styles and colors used to depict the gam and svms both site specific and regional types is shown in fig 6 the explanation fig 6 also distinguishes between the presence of the trigonometric seasonal terms dotted dash line style in either the site specific gam or regional gam the explanation also provides a clarifying note about the presence of a seasonal trigonometric terms in the gam for either the subject well data or for the neighboring data only one gam is shown for each gam input set site specific and regional and these gams may or may not contain seasonal terms 4 1 gam and svms for the seven example wells 4 1 1 the gam and svms for well ar008 354547090522001 all gams and svms are available for this well fig 7 the site specific gam and svms nearly overlap from 1992 through about 2000 and then diverge because the gam does not show seasonality which is confirmed by the dashed line plotting style the seasonal terms are absent the 2002 water level measurement pulls the gam away from the rest of the data resulting in a lack of congruence between the site specific gam and svm estimates after 2002 in this example the gam clearly has extrapolation problems the svm conversely appears to function satisfactorily to about 2002 but then swings towards the global mean after 2002 the regional gam and regional svm fitted only to the neighboring well data produce similar results throughout the period january 1992 through december 2018 the regional gam has a seasonal amplitude that does not change in time this is true because no provision is made in the model construction for the interaction of the trigonometric functions and hence amplitude changes as a function of year this means that the coefficients on the trigonometric functions from the gam are constant identification of outlier measurements is an important part of data quality assurance the 2002 measurement is identified by a red circle fig 7 see also fig 6 which is a visual cue that the site specific svm has identified an outlier the identification is automated and the concentric circle is drawn when the figure is rendered in this study outliers are jointly defined using a residual magnitude of at least 3 3 m 10 ft as well as being beyond the 1st or 99th percentile of the predicted error distribution for the estimation 4 1 2 the gam and svms for well usgs 320314091452201 only two time series models for this well fig 8 are available the site specific gam and svms could not be fit because the sample size is one which is less than the minimum of 10 measurements required thus only the regional gam and svm are shown the regional gam lacks the dash dot line plotting style seen previously for well ar008 354547090522001 fig 7 which means that seasonality is not statistically significant for the neighboring wells the regional gam appears to have a reasonable fit throughout most of the time period shown the regional svm is shown to regress towards the global mean within a year or two when water levels are not available the results fig 8 show that pursuing data quality assurance for wells having solitary measurements is possible large differences between the reported water level and the two estimates by the regional gam and svms could be an indication of potentially erroneous data much in the same way that outliers such as those shown in figs 7 and 9 or 10 are identified the residuals to the solitary measurements could provide an indirect measure of uncertainty a review such as this formed part of a screening review for a quality assurance study of the modnr well drillers report database smith et al 2019 the solitary measurement for this well is temporally isolated the regional gam and svm predict about 14 9 m 49 ft for the 1980 90 measurements compared to about 13 4 m 44 ft for the 1950 measurement it is suggested at the present and for the mrva that solitary measurements simultaneously within about 6 1 m 20 ft of the regional gam and svms provide a type of quality assurance inclusion of noisy databases such modnr well drillers database that is external to nwis into other science activities could be justifiable several filtered versions of the modnr well drillers database are published as a usgs data release smith et al 2019 these data could be joined for statistical purposes as needed and infogw2visgwdb software provides a useful preprocessor asquith and seanor 2019 inclusion of certain datasets into nwis is problematic partly because of inherent imprecision for some of the water levels if for no other reason than the rounding concerns fig 4 4 1 3 the gam and svms for well usgs 331101090294801 all four gams and svms for this well fig 9 are available the outlier in 1985 might be erroneous because it is a spike upward downward spikes are more readily explainable but local hydrogeologic or climatic considerations would be informative downward spikes might represent recent pumping effects a plausible explanation upward spikes conversely might represent substantial recharge if a sustained flow river is nearby nwis unfortunately provides no context of the measurement conditions because of missing metadata interestingly the neighboring wells have contemporaneous increases of similar magnitude this well was used to create the 2016 and 2018 potentiometric maps by mcguire et al 2019 and mcguire et al 2020 seasonal terms for both the site specific and regional gams were statistically significant as indicated by the undulation of both gam lines the regional gam almost uniformly predicts water levels higher than the site specific gam and water level observations by about 1 m 3 ft throughout the period of record notably the seasonal amplitude is similar between the two gams the two svms show more complicated relations between each other the data for this well and its neighbors also provide an opportunity to highlight examples of open interval comparison between the neighbors and the subject well it appears that the neighbors having generally higher water level altitudes than the subject well are completed above the subject well plausibly in a higher shallower stratum of the mrva conversely the neighboring wells with water levels generally lower than those of the subject well have openings below the subject well another shaded area shows opening overlaps these are generally higher than the subject well finally there are two or three nearby wells that also have opening overlaps and have similar water levels to the subject well 4 1 4 the gam and svms for well usgs 331745090260401 all four gams and svms for this well fig 10 are available the site specific gam and svm estimates are similar and both identify the spring 1986 measurement as an outlier even though most of the years have semi annual measurements there were no statistically significant seasonal terms in the site specific gam this well is an example for which the october 2007 through december 2018 monthly estimates are expected to be particularly useful for data worth data importance as well as groundwater flow model parameter estimation because the statistical models appear to have performed satisfactorily throughout the period of interest 4 1 5 the gam and svms for well usgs 331750090294501 all four gams and svms for this well fig 11 are available this well is an example of a location lacking any data for the period october 2007 through december 2018 extrapolation problems are likely with the site specific gam and svm models as evidenced by the lack of congruence between the results of these models ample neighboring well and regional data exist to make reliable regional predictions for the time period of interest and general similarities between the observed data 1981 89 in the neighboring wells to the estimates from the regional gam and svms are encouraging the site specific estimates however should be rejected after about 1995 because of a lack of site specific gam and svm congruence as well as the additional lack of absence of regional gam and svm congruence 4 1 6 the gam and svms for well usscs 343532091211601 all four gams and svms for this well fig 12 are available there are sufficient data throughout the time period and reasonable site specific gam and svms exist one outlier is identified as shown but by visual inspection the outlier does not appear as potentially erroneous the regional gam and svms are not based on large amounts of data only four neighboring wells the apparent lack of congruence between the site specific and regional models which is almost 6 m 20 ft in the last 15 years could be used to reject some or all of the monthly estimates in subsequent scientific inquiries this well was used to create the 2016 and 2018 potentiometric maps by mcguire et al 2019 and mcguire et al 2020 further this well is useful for another type of data review the gam trend line shows seasonality by the sinusoidal undulations and the dash dot line style the hartigans dip test for unimodality on the residuals of the site specific gam that has effectively detrended the data has a p value of 0 88 not statistically significant after about 1998 one agency usgs measured the well in the spring and another agency usscs nrcs measured the well in both the spring and fall the expected power of the test has limitations because alternatively the kruskal wallis rank sum test on the water levels site specific gam residuals and site specific svm residuals produce small p values for all three as 0 0001 which is an indication that the agency source is a statistically significant alpha 0 01 issue and that further supervisory review for this well is suggested 4 1 7 the gam and svms for well usscs 362118090313201 all four gams and svms for this well fig 13 are available nothing particularly remarkable about the trend lines is seen for this well systematic downward trends in the data and the neighboring data are evident and the four models clearly reproduce these trends his well was used to create the 2016 potentiometric map by mcguire et al 2019 4 2 discussion the seven examples out of 18 903 wells provide a representative range of situations for database review and time series modeling examples of outlier identification also are shown when copious data exist reliable monthly estimates from either site specific or regional models are likely to be robust when the data span or extend beyond the time period of estimation interest the period october 2007 through december 2018 for a subject well the site specific estimates would generally be preferred over the regional estimates the use of the neighboring data in contrast promises to synthesize more information at the price of heuristic judgement of algorithm settings the visgwdbmrva software uses a simple circular selection scheme for neighbors and no local scale spatial trend assessment thus the regional models often still are deemed to produce reliable results other aquifer systems might require recasting the neighborhood search into non circular shapes or adding spatial trend information such as inclusion of horizontal coordinates and land surface altitude or other reference altitudes secs 3 7 and 3 9 there are difficulties to developing settings for the statistical computations that are universally sufficient for the mrva such settings include the statistical answers being sought the choice of modeling approach minimum sample sizes search radii number of neighbors and structural form of the model iterative study is thus advised also the study of water levels in aquifers other than the mrva could lead to other setting choices metadata are required and highly informative when large scale computational methods are used the agency source lev agency cd level source lev src cd and level status lev status cd codes when available offer opportunities to conduct hypothesis testing on data compatibility and provide a means to identify wells for further supervisory review the level agency code was shown to be useful as part of database review as was shown for well usscs 343532091211601 fig 12 data stewards when using extant field records might be able to reconcile the apparent differences between agency sources example detections by hartigans dip tests and kruskal wallis tests were shown for this study level status codes also can be useful for assessing outliers because such codes document situations affecting the measurement such as recent pumping though such data are useful there is a general absence of level status codes which is unfortunate from the perspective of computational informatics the codes can be useful for efficient supervisory screening of outliers for example downward spikes identified to be outliers could be associated with documented or undocumented pumping in the database of the well or nearby wells whereas upward spikes tend to be more difficult to explain the level status code of r meaning the water level is affected by recent pumping is shown only once within the time series of two of the seven example wells wells usgs 331101090294801 fig 9 and usgs 331750090294501 fig 11 metadata about individual measurements fig 6 is useful not commonly provided nwis metadata and possibly databases of other agencies recognize data rejection codes for example a level aging code lev age cd not otherwise mentioned herein in nwis keeps individual water level measurements from appearing in public retrievals from the internet but such data can be retrieved by database administrators at public request statistical modeling can produce reliable and continuous estimates of monthly water levels but the analyst needs ways to detect unreliable estimates congruence between gam and svm estimates is one way such could be achieved with time series modeling of water levels there is undoubtedly information in using groundwater extraction data as predictor variables but such information is simply not available at well specific resolution for the techniques described here we explicitly refrain from determining a single preferable modeling approach gam or svm based but highlight how the two model types can be used in tandem for instance lack of model congruence can be an indicator diagnostic that insufficient information exists numerical congruence between the estimates by the gam and svms could be used by other scientists as a method to quantify data extension into situations of extrapolation and thus used to limit later use of monthly estimates asquith 2020 considers the relative performance of gam and svms at conditions of extrapolation in detail for example if site specific or regional gam and svms differ in absolute value by about 6 1 m 20 ft native units or even as tight as 3 05 m 10 ft then the estimate pairs perhaps should be rejected this is the criteria contained in the check delta between models variable in asquith et al 2019 see file visgwdb control r and file output pozo aheader txt in the visgwdbmrva software for the seven wells in this study as extrapolation progresses a gam curving away from the global mean and the svm regressing towards returning to the global mean is a complementary benefit of using gam and svm approaches in tandem congruence in the estimates as judged by some exceedance threshold could contribute to information weighting within pest welter et al 2015 where more certainty when the estimates are similar and less certainty when the estimates are not additionally identification of data gaps in the mrva could also be associated when gam and svm congruence is lacking and could contribute to network analysis asquith 2020 5 conclusions groundwater flow modeling scientists require copious amounts of water level information for the mrva and other aquifer systems in the region state boundaries and regional interests naturally have resulted in complex data histories and challenges remain for the accurate and consistent revision of data and metadata with provable error s to all instances of those data amongst the various databases where they exist although seemingly abundant data exist in nwis as of april 22 2019 18 903 wells and collectively 287 272 measurements data availability is highly variable in time and space interpolation and extension are needed as part of data syntheses computational informatics are appealing because visualization and statistical estimation can simultaneously document inherent uncertainties about water levels in space and time the tandem use of gams and svms for site specific and local neighborhood modeling can produce useful monthly estimates along with uncertainty estimates and outlier detection this study shows selected monthly estimates from computational informatics using the visgwdbmrva software asquith et al 2019 using the output of the preprocessor infogw2visgwdb software asquith and seanor 2019 for purposes of data quality assurance visgwdbmrva also supports database review sophisticated visualization outlier identification and techniques for review of wells having solitary measurements the vast majority of observed measured water levels in the mrva by the usgs and other agencies presently stored in the nwis database are self consistent intra well consistency or are consistent with data for neighboring wells short distances away inter well consistency declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank many colleagues associated with the water availability study for the mississippi river alluvial plain other studies and usgs database administration these colleagues have helped identify needs of the scientific community that have predicated the informatics described jeannie barlow brian clark mike fienen courtney killian steve peterson d charlie smith d sam wallace toby welborn and jeremy white the authors would like to thank howard reeves jason ramage and elena crowley ornelas for usgs peer review along with the anonymous peer reviewers these reviewers collectively have improved the manuscript any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government this study funded by the u s geological survey usgs water availability and use science program wausp appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104758 
25933,many aspects of land use management and policy making require information regarding how and where land cover and land uses will change in the future in this research we propose a method for modeling and predicting developed land expansion using the idea of pixel wise semantic segmentation through deep convolutional neural networks this analysis is done on a watershed scale with a focus on where developed lands are predicted to expand we introduce a method to construct data cubes of the land patches which represent important information related to diverse characteristics of the area under consideration we model the developed land expansion using an encoder decoder network and then perform prediction using a simple sigmoid layer our results indicate a performance accuracy of 98 on the test data the proposed technique could thus play an important role in improving our understanding mapping and modeling of spatially explicit landscape changes and in facilitating land use decision making keywords developed land expansion pixel wise segmentation convolutional neural network 1 introduction land use and land cover change can affect global and local environmental conditions lambin et al 2000 urban and rural land developments are forms of land change which substantially impact the environment these anthropogenic changes have subtle impacts on the global climate change they influence the hydrological system soil air energy cycle and land resources wilson and chakraborty 2013 moreover the exposure of these lands to the natural hazards and their vulnerability in the event of natural catastrophes is a matter of concern for risk management modelers ntelekos et al 2010 a variety of urban growth and land transformation models have been applied to project the process of land development pijanowski et al 2009 jin et al 2013 kamusoko and gamba 2015 these models are applicable in natural catastrophe risk modeling and management climate change modeling and assessment of interactions of urban rural land development ntelekos et al 2010 rizeei et al 2018 pijanowski et al 2001 predictive models of urbanization are also deployed to mitigate the impact of urban sprawl and unplanned urbanization improvement of such models is beneficial to a broad spectrum of scientists policy makers and environmental modelers recent advancements in machine learning and deep learning have led to improvements in predictive land transformations models kamusoko and gamba 2015 pijanowski et al 2002 2014 neural networks as universal function approximators have been widely applied in land change prediction land cover classification and object detection pijanowski et al 2001 hu et al 2015 castelluccio et al 2015 one of the popular neural network models used for land change prediction is land transformation model ltm pijanowski et al 2002 ltm is a multi layer perceptron mlp network which predicts a change likelihood value for each cell of the raster map the neural network gets several inputs including the number of transformed cells called principle index driver pid to make predictions with ltm first pids should be computed based on population and then these indices should be passed as one of the inputs to the network pijanowski et al 2002 in remote sensing various architectures of neural network models have been deployed to perform object detection land cover classification and semantic segmentation of the land kampffmeyer et al 2016 sherrah 2016 hu et al 2015 and castelluccio et al 2015 applied several architectures such as alexnet vgg from visual geometry group caffe and placenet to label patches of land images and compare the accuracy of models these efforts are good examples of the use of advanced well known deep learning designs in analyzing geographic data of the land kampffmeyer et al 2016 and sherrah 2016 used the convolutional neural network cnn and the fully connected network fcn to perform patch based and pixel to pixel segmentation they applied deep learning for segmentation of true ortho photos top images into 6 classes namely impervious surfaces building low vegetation tree car and clutter background kampffmeyer et al 2016 in their research they show that imbalanced data impacts the accuracy of the segmentation in their work performance of patch based and pixel to pixel segmentation varied for different classes overall performance of the models showed that fcn for patch based segmentation had a higher accuracy score in comparison to pixel to pixel segmentation in image processing and bio medical disciplines segnet badrinarayanan et al 2017 and u net ronneberger et al 2015 are well known deep learning models for pixelwise semantic segmentation more details on model architecture for u net and segnet are provided in badrinarayanan et al 2017 and ronneberger et al 2015 respectively these models utilize the contextual information of the images to perform segmentation both u net and segnet apply deep convolutional encoder decoder neural networks to find the state of each pixel the encoder maps the input data into a different feature representation and the decoder uses the encoded representation and maps it back as an image these models map and re map sets of variables such that the output is an image representing the desired segmentation deep learning has been well practiced for scene labeling and land classification and it has been shown that these models outperform many other machine learning techniques however deep learning is rarely applied in land change prediction ltm uses mlp network but its accuracy depends on another model to predict the number of transformed cells pids determining pids with a separate model brings more uncertainty to the land change prediction because pids are not counted in the final model assessment in this study we introduce deepland a deep learning model which is powerful enough to predict the number of transforming cells as well as the location of changes building on the successful application of convolutional encoder decoder models in image processing and bio medical fields deepland applies convolutional encoder decoder in land change prediction the convolutional encoder decoder models are usually morphologically symmetric although this is not a requirement the encoder includes convolutional layers poolings and batch normalization or dropouts the decoder encompasses deconvolutional layers with convolutional layers and upsamplings kampffmeyer et al 2016 the major contributions of this work are 1 application of deep convolutional encoder decoder segmentation for land change modeling and prediction and 2 data manipulation through application of a novel method based on the concept of multispectral imagery which facilitates computational operations and improves model performance the architectures presented in u net and segnet are used as baselines and deepland uses enhanced architectures to improve the accuracy of the land change prediction we also use an mlp model with an architecture similar to the ltm as another baseline and compared the model results with ours however the mlp model does not require pids as an input and consequently it is not an exact reproduction of the ltm model 2 data the focus area for this study is monongahela river watershed that includes parts of west virginia pennsylvania and maryland from the appalachian region of the united states fig 1 the region covered by the monongahela watershed has an abundance of natural energy and anthropogenic resources which creates additional complexity given the scope of our research and the focus area it is difficult to take into account all the explanatory factors of land development we identify the explanatory variables by considering the landscape characteristics of the study area and the key influential factors noted in other published literature verburg 2006 pijanowski et al 2009 wu et al 2009 the variables used in this analysis fit into the broad categories of terrain based policy related socio economic and distance density features the source features for distance and density related variables were defined for specific traits of the region as place based characteristics lambin et al 2000 for example because of the rich oil and gas resources in the region we included a kernel density of oil and gas wells within 10 km of each point after geo referencing and co registering the vector data raster layers were created at a spatial resolution of 30 m 30 m we used 32 cell by 32 cell patches as sample data and there are a total of 52455 patches cunstructed for the whole study area table 1 provides a description of all the variables we considered in this study in table 2 we include summary statistics of the variables and information on the nature and data type of the variables this table indicates that the values of each variable vary according to the nature of the variable for the base year of 2001 hence we standardize the continuous variables in the range of 0 and 1 to improve neural network stability and modeling performance for the categorical variables we introduce dummy variables for each category 2 1 data representation in this study we conducted data manipulation process to resemble hyperspectral images using convolutional neural networks cnns cnn has proven to perform well in the computer vision field and suits tasks such as classification object detection and object localization badrinarayanan et al 2017 the explanatory variables are depicted and ordered in a way similar to hyperspectral images we adopted spectral cubes of map layers for data manipulation in hyperspectral imaging and remote sensing each image can represent different bands each of which captures specific wavelength ranges and all of them point to the same geographic area the different wavelength representations can be viewed as spectral cubes for each object if different portrayals of a geographic region are considered as different variables stacking the variables on top of each other resembles the idea of hyperspectral images this creates a data cube of each patch of data the advantage of using such data structure is its convenience for applying convolutional kernels in deep learning models these kernels are capable of leveraging the contextual information from adjacent layers of variables ronneberger et al 2015 therefore representation of data as data cubes facilitates dealing with the complexity and correcting the inter variable correlation accordingly a cross section of data cubes each with a dimension of 32 32 71 1 1 the patch size for this study is 32 32 and after changing the categorical variables to binary dummy variables the dimensions reaches to 71 are generated from the study area see fig 2 each data cube representing a three dimensional patch is considered as one sample of data the first and second dimension of the cubes are the size of each patch and the third dimension represents the stacked variables pointed in table 1 the target variable is a binary variable indicating whether land development has occurred between 2001 and 2011 in each cell for training the models we use the national land cover dataset of 2001 jin et al 2013 and 2011 homer et al 2015 we randomly partitioned the data into training and test 66 of the patches are used for training and the remaining 33 are used as test data gis raster data and python scripts are used for the data processing and for building the models keras library which is provides a high level api for neural networks running on top of tensorflow is used for building the models 3 modeling we predict future developed land expansion using semantic segmentation in which we attempt to partition the raster image into semantically meaningful parts and to classify each part into one of the binary classes developed and non developed since we use raster data in which each cell is a pixel we used the term pixel wise segmentation our work is based on the idea that with the provided dataset the model should be able to semantically evaluate the role of each pixel 30m by 30m piece of land in the image or the 32 cells by 32 cells patch we propose deepland a convolutional deep neural network for predicting developed land use expansion using the multi dimensional data cubes for 10 years time steps in particular we build on segnet badrinarayanan et al 2017 and u net ronneberger et al 2015 two existing convolutional encoder decoder models used respectively in general computer vision and in biomedical imaging in deepland we enhance segnet and u net which we denote as deepland s and deepland u respectively to improve their accuracy in land change prediction in deepland model the first two convolutional layers include 64 filters of size 3 3 followed by average pooling using a 2 2 filter two next convolutional layers include 128 filters and they are also followed by maxpoolings the convolutional layers illustrated in fig 3 and fig 4 have different number of filters in convolutional layers in both models all the filters in the convolutional layers are of size 3 3 and all the poolings use 2 2 filters since the input spatial data the data cubes are highly correlated with the neighboring data cubes wu et al 2009 in the first pooling we apply an average pooling to accelerate the training and improve the model results we use dropout at the final convolutional layer of the encoder the decoder has more convolutional layers with more upsamplings in all the layers except the first two layers leakyrelu which is a version of rectified linear unit relu function is used the first two layers use relu as activation function figs 3 and 4 show the model architecture for deepland u and deepland s activation functions we use two activation functions 1 l e a k y r e l u which is applied after each convolutional layer and 2 s i g m o i d function that is used at the last layer of each model l e a k y r e l u function minimize the effect of the class imbalance data and is defined in equation 1 1 f x x if x 0 Œ± x if x 0 where x is the value of the input for the function from the convolutional layer and Œ± is a coefficient for negative values of x we use a value of 0 05 for Œ± f x is the output of the activation function which is passed to the next layer at the last layer s i g m o i d function equation 2 is used as the activation function since our output in binary s i g m o i d returns a probability value in the range of 0 1 per cell 2 s x i 1 1 e x i where s x i is the probability of the i th cell x i is a given cell value optimizer we use the adaptive moment estimation adam optimizer which calculates the adaptive learning rates for each parameter ruder 2016 loss function the loss function for this model was binary cross entropy which computes the loss value based on equation 3 3 l y y ÀÜ 1 n i 1 n y i l o g y ÀÜ i 1 y i l o g 1 y ÀÜ i where y ÀÜ is the estimated value y is the ground truth value and n is the number of samples and each sample is denoted as i we use patches of size 32 32 because the model best responds to this patch size the values at cell pixel location in the output image denotes the probability value for the developed land expansion expansion at the given cell the probability of 0 50 or higher represent class value of 1 developed land and lower values denote a class value of 0 non developed land 4 evaluation and discussions to evaluate the performance of the proposed models we performed computational experiments on predicting developed land expansion in the identified region 4 1 evaluation metrics the data for the analysis was imbalanced to avoid the effect of class imbalance in accuracy measurement we used the area under the curve of the receiver operating characteristic auc roc davis and goadrich 2006 roc measures the ability of a binary classifier system the higher the roc value the larger the difference between true positive rate tpr and false positive rate fpr 4 2 baselines for comparison we evaluated the performance of the two baseline deep learning models namely segnet and u net we also used an mlp model as baseline with an architecture similar to ltm the mlp baseline unlike ltm predicts both configuration and composition of the transforming cells and does not require the number of changing cells as an input therefore the mlp model is not an exact reproduction of ltm 4 3 results the models were implemented on the nvidia tesla k80 graphics processing units gpus at the pittsburgh supercomputing center psc to access the psc we used xsede virtual system that facilitated interactive sharing of computer resources towns et al 2014 fig 5 shows the trend of the binary cross entropy loss for deepland u at every iteration for both training set and validation set we can observe that as expected the error falls rapidly after the first few epochs and stabilizes after epoch 105 the changes in the auc roc metric at each iteration for different models are shown in fig 6 the results indicate that auc roc of the models increased at each iteration except for segnet the accuracy seemed stabilized after about 30 epochs overall best performance was produced using the proposed deepland u at 98 accuracy followed by using u net we can also observe significant improvement of the proposed deepland s over segnet while segnet required a lot of iterations before convergence the deepland s model converged relatively quickly after about 120 epochs the results also show that model prediction on the test dataset using deepland u or deepland s resulted in an improved prediction accuracy over the popular approach using ltm fig 7 shows a visualization of the ground truth heat maps and deepland u predictions for three selected patches with different levels of developed land heat maps of the selected patches in the middle row show the score predicted by the model which ranges between 0 and 1 the predicted label of each pixel the right column in fig 7 are determined according to equation 4 where l x i is the class of the i th cell in each patch of land and s x i is the score of the i th cell in a sample patch the three sample patches in fig 7 are 32 by 32 samples of the study area and are mainly located in the regions with higher ratio of developed lands consequently the model results in single patch level might not show the accuracy of the model over the whole test region fig 8 shows the ground truth and model predictions of the whole study area we magnified a part of pittsburgh pa metropolitan area to make it easier to visually compare the ground truth and the model prediction results imply that the model predictions are considerably accurate in comparison to the ground truth of nlcd 2011 data in this figure the areas that are masked with white layer are training samples we excluded these areas from the visualization computational efficiency in terms of both memory and time becomes a major consideration in developed land analysis which often include large data sets to alleviate the computational challenges we performed the experiments on high performance computing facilities at the pittsburgh supercomputing center and observed how the required computational time changed with the execution of the proposed deep convolutional neural networks we recorded the time for running each of the models on a dual processor pc with 3 47 gh cpus and 48 gb ram and on the gpus available on psc we used 4 nvidia tesla k80 gpu nodes each of which holds two nvidia k80 gpu cards with 24 gb of gpu memory 48gb node table 3 shows a comparison of the time per step and time per each epoch for each model first we can obverse the improvement in computational time using the proposed deepland when compared with the base models segnet and u net this is significant especially in the light of the superior performance of deepland models in the problem at hand predicting impervious land expansion further we can also notice how the deepland models also performed better than mlp model that is similar to ltm in terms of both prediction accuracy and computational time we can observe that segnet is generally slower that u net our proposed deepland u was also faster that deepland s whereas both deepland s and deepland u are faster than segnet and u net the time difference in the model performance is resulted from the structure of the models the number of parameters which get fine tuned in deepland u is 10 772 997 whereas there are 17 631 969 trainable parameters in deepland s number of trainable parameters are 30 764 805 in u net and 31 929 885 in segnet this difference in the structure of the model and number of trainable parameters essentially impact the required time for training the models finally we can observe the significant impact of using the high performance computing resources resulting in almost a factor of 20 speed up we were unable to run mlp on the gpus at psc due to time constraints in the access to the machine however the substantial difference in the running times using a cpu and the psc underlines the significance of computational resources in overcoming the huge computational resource requirements for models such as deepland 5 conclusion numerous machine learning methods have been developed to predict the complex and dynamic process of land development however the highly complex processes of land development is very challenging to model in order to predict the future of land change in this work we examined the applicability of pixel wise classification based image segmentation models for use in studying land change deep deconvolutional neural networks for predicting developed land expansion namely deepland has been introduced in this work these models are built up on top of segnet and u net two known deep learning models for semantic pixelwise segmentation to apply these image based models we introduced the idea of representing different land characteristics in the form of a multidimensional multispectral data cube where each spectrum or data layer corresponds to a different characteristic and each cube captures data in a different patch or location within the region under study performance of the experiments using the proposed deepland models along with comparison of the prediction results against those from the baseline deep convolutional networks and against mlp show that deepland is able to learn the transition rules of the land change more accurately among the five compared networks of u net segnet mlp our implementation of ltm deepland u and deepland s the suggested deepland models required less parameters to be fine tuned and also resulted in improved prediction accuracy in particular deepland u resulted in the best overall prediction performance when compared to our implementation of ltm segnet u net and deepland s the differences in the architecture of the models has considerable impact on the time efficiency of the models all in all the findings of this research determine how the structure of a model along with application of computational facilities can lead to reduction in the temporal costs and enhancement in the accuracy of the model results in this work we showed that some models might not have been developed to explain the land transformation for instance u net was developed for biomedical image analysis however from a big picture perspective effectively addressing the problem may require a cross disciplinary approach with a good understanding of the problem it becomes possible to apply other models that have been successful and accurate on similar problems in other fields declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this paper is based upon work supported by the national science foundation under cooperative agreement number oia 1458952 this work used the extreme science and engineering discovery environment xsede which is supported by national science foundation grant number aci 1548562 specifically it used the bridges system which is supported by nsf award number aci 1445606 at the pittsburgh supercomputing center psc any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the national science foundation the work was also supported by the usda national institute of food and agriculture hatch project and the west virginia agricultural and forestry experiment station appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104751 
25933,many aspects of land use management and policy making require information regarding how and where land cover and land uses will change in the future in this research we propose a method for modeling and predicting developed land expansion using the idea of pixel wise semantic segmentation through deep convolutional neural networks this analysis is done on a watershed scale with a focus on where developed lands are predicted to expand we introduce a method to construct data cubes of the land patches which represent important information related to diverse characteristics of the area under consideration we model the developed land expansion using an encoder decoder network and then perform prediction using a simple sigmoid layer our results indicate a performance accuracy of 98 on the test data the proposed technique could thus play an important role in improving our understanding mapping and modeling of spatially explicit landscape changes and in facilitating land use decision making keywords developed land expansion pixel wise segmentation convolutional neural network 1 introduction land use and land cover change can affect global and local environmental conditions lambin et al 2000 urban and rural land developments are forms of land change which substantially impact the environment these anthropogenic changes have subtle impacts on the global climate change they influence the hydrological system soil air energy cycle and land resources wilson and chakraborty 2013 moreover the exposure of these lands to the natural hazards and their vulnerability in the event of natural catastrophes is a matter of concern for risk management modelers ntelekos et al 2010 a variety of urban growth and land transformation models have been applied to project the process of land development pijanowski et al 2009 jin et al 2013 kamusoko and gamba 2015 these models are applicable in natural catastrophe risk modeling and management climate change modeling and assessment of interactions of urban rural land development ntelekos et al 2010 rizeei et al 2018 pijanowski et al 2001 predictive models of urbanization are also deployed to mitigate the impact of urban sprawl and unplanned urbanization improvement of such models is beneficial to a broad spectrum of scientists policy makers and environmental modelers recent advancements in machine learning and deep learning have led to improvements in predictive land transformations models kamusoko and gamba 2015 pijanowski et al 2002 2014 neural networks as universal function approximators have been widely applied in land change prediction land cover classification and object detection pijanowski et al 2001 hu et al 2015 castelluccio et al 2015 one of the popular neural network models used for land change prediction is land transformation model ltm pijanowski et al 2002 ltm is a multi layer perceptron mlp network which predicts a change likelihood value for each cell of the raster map the neural network gets several inputs including the number of transformed cells called principle index driver pid to make predictions with ltm first pids should be computed based on population and then these indices should be passed as one of the inputs to the network pijanowski et al 2002 in remote sensing various architectures of neural network models have been deployed to perform object detection land cover classification and semantic segmentation of the land kampffmeyer et al 2016 sherrah 2016 hu et al 2015 and castelluccio et al 2015 applied several architectures such as alexnet vgg from visual geometry group caffe and placenet to label patches of land images and compare the accuracy of models these efforts are good examples of the use of advanced well known deep learning designs in analyzing geographic data of the land kampffmeyer et al 2016 and sherrah 2016 used the convolutional neural network cnn and the fully connected network fcn to perform patch based and pixel to pixel segmentation they applied deep learning for segmentation of true ortho photos top images into 6 classes namely impervious surfaces building low vegetation tree car and clutter background kampffmeyer et al 2016 in their research they show that imbalanced data impacts the accuracy of the segmentation in their work performance of patch based and pixel to pixel segmentation varied for different classes overall performance of the models showed that fcn for patch based segmentation had a higher accuracy score in comparison to pixel to pixel segmentation in image processing and bio medical disciplines segnet badrinarayanan et al 2017 and u net ronneberger et al 2015 are well known deep learning models for pixelwise semantic segmentation more details on model architecture for u net and segnet are provided in badrinarayanan et al 2017 and ronneberger et al 2015 respectively these models utilize the contextual information of the images to perform segmentation both u net and segnet apply deep convolutional encoder decoder neural networks to find the state of each pixel the encoder maps the input data into a different feature representation and the decoder uses the encoded representation and maps it back as an image these models map and re map sets of variables such that the output is an image representing the desired segmentation deep learning has been well practiced for scene labeling and land classification and it has been shown that these models outperform many other machine learning techniques however deep learning is rarely applied in land change prediction ltm uses mlp network but its accuracy depends on another model to predict the number of transformed cells pids determining pids with a separate model brings more uncertainty to the land change prediction because pids are not counted in the final model assessment in this study we introduce deepland a deep learning model which is powerful enough to predict the number of transforming cells as well as the location of changes building on the successful application of convolutional encoder decoder models in image processing and bio medical fields deepland applies convolutional encoder decoder in land change prediction the convolutional encoder decoder models are usually morphologically symmetric although this is not a requirement the encoder includes convolutional layers poolings and batch normalization or dropouts the decoder encompasses deconvolutional layers with convolutional layers and upsamplings kampffmeyer et al 2016 the major contributions of this work are 1 application of deep convolutional encoder decoder segmentation for land change modeling and prediction and 2 data manipulation through application of a novel method based on the concept of multispectral imagery which facilitates computational operations and improves model performance the architectures presented in u net and segnet are used as baselines and deepland uses enhanced architectures to improve the accuracy of the land change prediction we also use an mlp model with an architecture similar to the ltm as another baseline and compared the model results with ours however the mlp model does not require pids as an input and consequently it is not an exact reproduction of the ltm model 2 data the focus area for this study is monongahela river watershed that includes parts of west virginia pennsylvania and maryland from the appalachian region of the united states fig 1 the region covered by the monongahela watershed has an abundance of natural energy and anthropogenic resources which creates additional complexity given the scope of our research and the focus area it is difficult to take into account all the explanatory factors of land development we identify the explanatory variables by considering the landscape characteristics of the study area and the key influential factors noted in other published literature verburg 2006 pijanowski et al 2009 wu et al 2009 the variables used in this analysis fit into the broad categories of terrain based policy related socio economic and distance density features the source features for distance and density related variables were defined for specific traits of the region as place based characteristics lambin et al 2000 for example because of the rich oil and gas resources in the region we included a kernel density of oil and gas wells within 10 km of each point after geo referencing and co registering the vector data raster layers were created at a spatial resolution of 30 m 30 m we used 32 cell by 32 cell patches as sample data and there are a total of 52455 patches cunstructed for the whole study area table 1 provides a description of all the variables we considered in this study in table 2 we include summary statistics of the variables and information on the nature and data type of the variables this table indicates that the values of each variable vary according to the nature of the variable for the base year of 2001 hence we standardize the continuous variables in the range of 0 and 1 to improve neural network stability and modeling performance for the categorical variables we introduce dummy variables for each category 2 1 data representation in this study we conducted data manipulation process to resemble hyperspectral images using convolutional neural networks cnns cnn has proven to perform well in the computer vision field and suits tasks such as classification object detection and object localization badrinarayanan et al 2017 the explanatory variables are depicted and ordered in a way similar to hyperspectral images we adopted spectral cubes of map layers for data manipulation in hyperspectral imaging and remote sensing each image can represent different bands each of which captures specific wavelength ranges and all of them point to the same geographic area the different wavelength representations can be viewed as spectral cubes for each object if different portrayals of a geographic region are considered as different variables stacking the variables on top of each other resembles the idea of hyperspectral images this creates a data cube of each patch of data the advantage of using such data structure is its convenience for applying convolutional kernels in deep learning models these kernels are capable of leveraging the contextual information from adjacent layers of variables ronneberger et al 2015 therefore representation of data as data cubes facilitates dealing with the complexity and correcting the inter variable correlation accordingly a cross section of data cubes each with a dimension of 32 32 71 1 1 the patch size for this study is 32 32 and after changing the categorical variables to binary dummy variables the dimensions reaches to 71 are generated from the study area see fig 2 each data cube representing a three dimensional patch is considered as one sample of data the first and second dimension of the cubes are the size of each patch and the third dimension represents the stacked variables pointed in table 1 the target variable is a binary variable indicating whether land development has occurred between 2001 and 2011 in each cell for training the models we use the national land cover dataset of 2001 jin et al 2013 and 2011 homer et al 2015 we randomly partitioned the data into training and test 66 of the patches are used for training and the remaining 33 are used as test data gis raster data and python scripts are used for the data processing and for building the models keras library which is provides a high level api for neural networks running on top of tensorflow is used for building the models 3 modeling we predict future developed land expansion using semantic segmentation in which we attempt to partition the raster image into semantically meaningful parts and to classify each part into one of the binary classes developed and non developed since we use raster data in which each cell is a pixel we used the term pixel wise segmentation our work is based on the idea that with the provided dataset the model should be able to semantically evaluate the role of each pixel 30m by 30m piece of land in the image or the 32 cells by 32 cells patch we propose deepland a convolutional deep neural network for predicting developed land use expansion using the multi dimensional data cubes for 10 years time steps in particular we build on segnet badrinarayanan et al 2017 and u net ronneberger et al 2015 two existing convolutional encoder decoder models used respectively in general computer vision and in biomedical imaging in deepland we enhance segnet and u net which we denote as deepland s and deepland u respectively to improve their accuracy in land change prediction in deepland model the first two convolutional layers include 64 filters of size 3 3 followed by average pooling using a 2 2 filter two next convolutional layers include 128 filters and they are also followed by maxpoolings the convolutional layers illustrated in fig 3 and fig 4 have different number of filters in convolutional layers in both models all the filters in the convolutional layers are of size 3 3 and all the poolings use 2 2 filters since the input spatial data the data cubes are highly correlated with the neighboring data cubes wu et al 2009 in the first pooling we apply an average pooling to accelerate the training and improve the model results we use dropout at the final convolutional layer of the encoder the decoder has more convolutional layers with more upsamplings in all the layers except the first two layers leakyrelu which is a version of rectified linear unit relu function is used the first two layers use relu as activation function figs 3 and 4 show the model architecture for deepland u and deepland s activation functions we use two activation functions 1 l e a k y r e l u which is applied after each convolutional layer and 2 s i g m o i d function that is used at the last layer of each model l e a k y r e l u function minimize the effect of the class imbalance data and is defined in equation 1 1 f x x if x 0 Œ± x if x 0 where x is the value of the input for the function from the convolutional layer and Œ± is a coefficient for negative values of x we use a value of 0 05 for Œ± f x is the output of the activation function which is passed to the next layer at the last layer s i g m o i d function equation 2 is used as the activation function since our output in binary s i g m o i d returns a probability value in the range of 0 1 per cell 2 s x i 1 1 e x i where s x i is the probability of the i th cell x i is a given cell value optimizer we use the adaptive moment estimation adam optimizer which calculates the adaptive learning rates for each parameter ruder 2016 loss function the loss function for this model was binary cross entropy which computes the loss value based on equation 3 3 l y y ÀÜ 1 n i 1 n y i l o g y ÀÜ i 1 y i l o g 1 y ÀÜ i where y ÀÜ is the estimated value y is the ground truth value and n is the number of samples and each sample is denoted as i we use patches of size 32 32 because the model best responds to this patch size the values at cell pixel location in the output image denotes the probability value for the developed land expansion expansion at the given cell the probability of 0 50 or higher represent class value of 1 developed land and lower values denote a class value of 0 non developed land 4 evaluation and discussions to evaluate the performance of the proposed models we performed computational experiments on predicting developed land expansion in the identified region 4 1 evaluation metrics the data for the analysis was imbalanced to avoid the effect of class imbalance in accuracy measurement we used the area under the curve of the receiver operating characteristic auc roc davis and goadrich 2006 roc measures the ability of a binary classifier system the higher the roc value the larger the difference between true positive rate tpr and false positive rate fpr 4 2 baselines for comparison we evaluated the performance of the two baseline deep learning models namely segnet and u net we also used an mlp model as baseline with an architecture similar to ltm the mlp baseline unlike ltm predicts both configuration and composition of the transforming cells and does not require the number of changing cells as an input therefore the mlp model is not an exact reproduction of ltm 4 3 results the models were implemented on the nvidia tesla k80 graphics processing units gpus at the pittsburgh supercomputing center psc to access the psc we used xsede virtual system that facilitated interactive sharing of computer resources towns et al 2014 fig 5 shows the trend of the binary cross entropy loss for deepland u at every iteration for both training set and validation set we can observe that as expected the error falls rapidly after the first few epochs and stabilizes after epoch 105 the changes in the auc roc metric at each iteration for different models are shown in fig 6 the results indicate that auc roc of the models increased at each iteration except for segnet the accuracy seemed stabilized after about 30 epochs overall best performance was produced using the proposed deepland u at 98 accuracy followed by using u net we can also observe significant improvement of the proposed deepland s over segnet while segnet required a lot of iterations before convergence the deepland s model converged relatively quickly after about 120 epochs the results also show that model prediction on the test dataset using deepland u or deepland s resulted in an improved prediction accuracy over the popular approach using ltm fig 7 shows a visualization of the ground truth heat maps and deepland u predictions for three selected patches with different levels of developed land heat maps of the selected patches in the middle row show the score predicted by the model which ranges between 0 and 1 the predicted label of each pixel the right column in fig 7 are determined according to equation 4 where l x i is the class of the i th cell in each patch of land and s x i is the score of the i th cell in a sample patch the three sample patches in fig 7 are 32 by 32 samples of the study area and are mainly located in the regions with higher ratio of developed lands consequently the model results in single patch level might not show the accuracy of the model over the whole test region fig 8 shows the ground truth and model predictions of the whole study area we magnified a part of pittsburgh pa metropolitan area to make it easier to visually compare the ground truth and the model prediction results imply that the model predictions are considerably accurate in comparison to the ground truth of nlcd 2011 data in this figure the areas that are masked with white layer are training samples we excluded these areas from the visualization computational efficiency in terms of both memory and time becomes a major consideration in developed land analysis which often include large data sets to alleviate the computational challenges we performed the experiments on high performance computing facilities at the pittsburgh supercomputing center and observed how the required computational time changed with the execution of the proposed deep convolutional neural networks we recorded the time for running each of the models on a dual processor pc with 3 47 gh cpus and 48 gb ram and on the gpus available on psc we used 4 nvidia tesla k80 gpu nodes each of which holds two nvidia k80 gpu cards with 24 gb of gpu memory 48gb node table 3 shows a comparison of the time per step and time per each epoch for each model first we can obverse the improvement in computational time using the proposed deepland when compared with the base models segnet and u net this is significant especially in the light of the superior performance of deepland models in the problem at hand predicting impervious land expansion further we can also notice how the deepland models also performed better than mlp model that is similar to ltm in terms of both prediction accuracy and computational time we can observe that segnet is generally slower that u net our proposed deepland u was also faster that deepland s whereas both deepland s and deepland u are faster than segnet and u net the time difference in the model performance is resulted from the structure of the models the number of parameters which get fine tuned in deepland u is 10 772 997 whereas there are 17 631 969 trainable parameters in deepland s number of trainable parameters are 30 764 805 in u net and 31 929 885 in segnet this difference in the structure of the model and number of trainable parameters essentially impact the required time for training the models finally we can observe the significant impact of using the high performance computing resources resulting in almost a factor of 20 speed up we were unable to run mlp on the gpus at psc due to time constraints in the access to the machine however the substantial difference in the running times using a cpu and the psc underlines the significance of computational resources in overcoming the huge computational resource requirements for models such as deepland 5 conclusion numerous machine learning methods have been developed to predict the complex and dynamic process of land development however the highly complex processes of land development is very challenging to model in order to predict the future of land change in this work we examined the applicability of pixel wise classification based image segmentation models for use in studying land change deep deconvolutional neural networks for predicting developed land expansion namely deepland has been introduced in this work these models are built up on top of segnet and u net two known deep learning models for semantic pixelwise segmentation to apply these image based models we introduced the idea of representing different land characteristics in the form of a multidimensional multispectral data cube where each spectrum or data layer corresponds to a different characteristic and each cube captures data in a different patch or location within the region under study performance of the experiments using the proposed deepland models along with comparison of the prediction results against those from the baseline deep convolutional networks and against mlp show that deepland is able to learn the transition rules of the land change more accurately among the five compared networks of u net segnet mlp our implementation of ltm deepland u and deepland s the suggested deepland models required less parameters to be fine tuned and also resulted in improved prediction accuracy in particular deepland u resulted in the best overall prediction performance when compared to our implementation of ltm segnet u net and deepland s the differences in the architecture of the models has considerable impact on the time efficiency of the models all in all the findings of this research determine how the structure of a model along with application of computational facilities can lead to reduction in the temporal costs and enhancement in the accuracy of the model results in this work we showed that some models might not have been developed to explain the land transformation for instance u net was developed for biomedical image analysis however from a big picture perspective effectively addressing the problem may require a cross disciplinary approach with a good understanding of the problem it becomes possible to apply other models that have been successful and accurate on similar problems in other fields declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this paper is based upon work supported by the national science foundation under cooperative agreement number oia 1458952 this work used the extreme science and engineering discovery environment xsede which is supported by national science foundation grant number aci 1548562 specifically it used the bridges system which is supported by nsf award number aci 1445606 at the pittsburgh supercomputing center psc any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the national science foundation the work was also supported by the usda national institute of food and agriculture hatch project and the west virginia agricultural and forestry experiment station appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104751 
25934,the scarcity of long time series of water temperature for tropical reservoirs usually available at a low frequency of measurements represents a serious constraint for the understanding of their thermal behavior hydrodynamic models become a powerful tool for that understanding through a qualitative approach in this study the one dimensional general lake model glm was applied to hindcast multiannual temperature profiles of a cascade system of six reservoirs located in southeast brazil over a period of eight years the model parameters were calibrated and validated by field data for four reservoirs to overcome the challenge of exploring hydrodynamics in two reservoirs with a lack of measured temperatures a parameterization strategy was applied to accomplish a better simulation by the estimation of potential sensitive parameters through regression curves it is expected that this approach can be extended to other tropical reservoirs whenever field data is not available for the calibration procedure keywords thermal dynamics stratification one dimensional vertical model glm low latitude reservoirs 1 introduction the number of man made lakes has increased continuously mainly after the 1950s and currently accounts for 6 of the total inland water surface area downing et al 2006 the growing need for water has resulted in the construction of reservoirs beyond europe and the united states in several low latitude countries according to the international commission on large dams there are 59 000 reservoirs of more than 15 m depth and more than half of them 53 are located in china india brazil and south africa icold 2019 nowadays reservoirs are highly valued as they serve multiple purposes and provide numerous ecosystem services such as fishery flood control recreation nutrient uptake sediment stabilization and navigation schallenberg et al 2013 beyond the historical uses for water supply and hydropower generation they make a significant contribution to the efficient management of finite water resources that are subject to large seasonal fluctuations compared to lakes reservoirs have been poorly documented in the tropics it is in part because they are located far from the most active research centers causing a corresponding gap in limnological investigations as a result 41 of the 54 most large low latitude reservoirs have not yet been subject to basic limnological studies winton et al 2019 low latitude reservoirs often lack long term series of temperature profiles an easily measurable in situ data many reservoirs including the current study site only have reliable data sets for irregular periods in such context hydrodynamic models are fundamental tools to support the comprehension of lake s dynamics since they can simulate and predict multiannual cycles of water temperature providing additional information to field measurements on mixing extent and stratification periods they simulate the vertical thermal dynamics which have important implications for the biogeochemical and ecological cycles including renewal of dissolved oxygen in the hypolimnion the release of nutrients from the sediments as well as the control of timing magnitude and composition of phytoplankton blooms cao et al 2016 therefore given the paramount importance of the thermal structure for lentic aquatic ecosystems for both field biologists interested in ecosystem ecology and engineers charged with the management of reservoirs hydrodynamic models have become more abundant and sophisticated since the early models developed in the 1970s imberger 1985 they have been used for various purposes such as to evaluate the response to different wind conditions mi et al 2018 to evaluate the impacts of storm water runoff on lake ecosystems silva et al 2019 to select the withdrawal depth kerimoglu and rinke 2013 and to forecast the mixing regime under climate change scenarios fenocchi et al 2018 it is expected that as the population grows simultaneous competing water uses in addition to increased nutrient loading will intensify anthropogenic pressures on low latitude reservoirs increasing their vulnerability globally folke et al 2004 hence the understanding of their thermal behavior is essential to assist in planning mitigation and adaptation strategies in this context the main objective of this study was to deepen the knowledge on the thermal behavior across six reservoirs using them as representative of the sub tropical ones by applying the well established 1 d hydrodynamic model glm over an 8 year period 2009 2016 the specific aims were as follows 1 to simulate multiannual cycles of the thermal behavior 2 to perform a sensitivity analysis of parameters and variables influencing water level and water temperature 3 to calibrate and validate the model for simulating both the water temperature and the water balance and 4 to provide an experimental calibration strategy for two reservoirs lacking water temperature data to illustrate how hydrodynamic modelling can provide surrogates for missing data we expect that the results provide useful guidance for modelling efforts of poor monitored reservoirs especially in low latitude countries 2 materials and methods 2 1 study site the study site is located in southeast brazil and comprises a sub tropical cascade system along the tiet√™ river composed by the following six reservoirs from upstream to downstream barra bonita bb bariri ba ibitinga ib promiss√£o pr nova avanhandava na and tr√™s irm√£os ti the reservoirs were built from 1964 to 1991 and they were hydrologically and operationally connected through the cascade for attending the common goal of electricity generation taking advantage of the cumulative effect of flow regulation to attend country demand nowadays the reservoirs are intended for multiple uses see table 1 in the supplementary material this cascade system was chosen because it is composed of reservoirs representing many others of tropical regions where water temperature is warm throughout the year in addition the reservoirs have a varied range of morphometric features such as mean depth from 8 6 m to 17 2 m surface area from 45 km2 to 654 km2 and volume from 0 61 109 m3 to 11 46 109 m3 table 1 high resolution meteorological stations are situated near them gauged inflows and outflows are available and water quality stations are available for some reservoirs fig 1 land use in their catchment basin is characterized by urban areas with more than 24 million inhabitants ibge 2019 and intensive agriculture mainly sugarcane camara and caldarelli 2016 the climate in the region is sub tropical k√∂ppen 1948 with a rainy season from october to march and a dry season from april to september the mean annual rainfall from 1981 to 2010 was 1406 mm the mean air temperature was 22 6 c with a minimum of 14 0 c in july and a maximum of 30 4 c in october inmet 2019 2 2 hydrodynamic model description the hydrodynamic one dimensional general lake model glm v 2 2 0 hipsey et al 2019 was applied to predict the thermal structure of each reservoir glm was selected mainly because of its good performance documented in several investigations all around the world e g bueche et al 2017 bruce et al 2018 the model assumes that horizontal gradients are negligible and it computes the vertical profiles of temperature salinity and density over time in lentic water bodies such as lakes reservoirs mining pit lakes wastewater ponds and urban wetlands whether shallow and well mixed or deep and stratified glm simulations are based on parameterizations of mixing processes surface dynamics energy budget and water balance the algorithms are based on equations defined by hamilton and schladow 1997 and by imberger and patterson 1981 the occurrence of thermal stratification and vertical mixing processes is based on the energy balance between the kinetic energy and the internal potential energy of the water body the model assumes the occurrence of mixing whenever available kinetic energy exceeds the potential energy required to keep stratification the available kinetic energy is calculated from wind stirring shear production between layers convective overturn and kelvin helmholtz billowing the mixing process ends when the internal kinetic energy is insufficient to overcome the stored potential energy then the thermal stratification process begins hipsey et al 2019 2 3 required input data for modelling glm requires the following site information as input data morphometry meteorological time series and hydrological time series obtaining and processing of input data required by glm is described in the following paragraphs i the hypsographic curve of each reservoir was provided by bathymetric surveys from previous studies a comparative plot of the hypsographic curves for all reservoirs shows diversity in reservoir size and volume fig 2 ii high resolution meteorological data from terrestrial based stations were available hourly measurements of air temperature wind speed solar radiation air relative humidity and rainfall were obtained from five monitoring stations operated by the national institute of meteorology inmet 2020 red markers in fig 1 table 2 in the supplementary material the meteorological variables were measured as required by glm air temperature air relative humidity and wind speed were measured 10 m above ground solar radiation and rainfall were measured at the ground wind direction although not required by glm is presented in fig 1 in the supplementary material for pr and na reservoirs meteorological data were obtained from one station the closest one to both reservoirs the missing values table 3 in the supplementary material of air temperature solar radiation and air relative humidity were filled by linear regression with data from the meteorological station with the highest correlation coefficient table 4 in the supplementary material when data was not available for both stations remaining data gaps were filled by the linear regression with the station of the following highest correlation coefficient and then successively in face of the weak linear correlation between measured data in different meteorological stations regarding wind speed r 0 54 and rainfall r 0 27 the missing data were filled by linear interpolation when data gaps were longer than ten consecutive hours instead of using the linear interpolation method gaps were filled by the mean monthly value cloud cover data was not available and it was estimated by glm based on the bird clear sky insolation model bird 1984 which compares the measured and theoretical clear sky solar irradiance in order to approximate the cloud cover fraction iii daily hydrological data has been provided by the national water agency ana 2018 gaps in daily inflow and outflow volume were rare less than 0 6 of total data and they were filled by linear interpolation inflow temperature data was measured every two months by the environmental company of the state of s√£o paulo cetesb 2019 for all reservoirs yellow circles in fig 1 except for ib for this reservoir data from ba was adopted inflow temperature was then converted into daily frequency through linear regression between bi monthly inflow temperature and minimum air temperature bb ba pr and ti mean air temperature ib and maximum shortwave radiation na the meteorological variable used in the linear regression was selected from the highest correlation coefficient with bi monthly inflow temperatures r values between 0 56 and 0 91 see fig 2 in the supplementary material as inflow salinity is not monitored it was estimated from electrical conductivity data due to its low value below 0 2 g kg 1 a constant value was assumed for the entire simulation period fig 3 in the supplementary material presents monthly means and range of input data for the six reservoirs the description of the applied input data is summarized in table 2 2 4 required in situ data for modelling the vertical profile of temperature and salinity in the reservoir as well as secchi depth and water level are also required for parameterization setting initial conditions and performing model assessment i the vertical profile of water temperature in the middle of the reservoirs bodies was measured every 2 months as follows see purple triangles in fig 1 a for bb it was measured by cetesb 2019 in 1 m depth steps down to the bottom b for ib it was measured by the national institute of space research cairo 2015 continuously from surface until 2 m above the bottom from november 2013 to september 2014 and c for pr and ti it was measured only at the surface by cetesb ii the electrical conductivity was measured every two months only at surface for bb pr and ti and along the water column for ib reservoir iii secchi depth was measured every two months for bb cetesb 2019 and ib cairo 2015 and twice a year in 2008 and 2009 for pr and ti santos 2010 iv the water level of all six reservoirs was daily gauged by ana 2018 for ba and na reservoirs there was no field data available for the simulation period the description of the field data applied for initial conditions and modelling assessment during calibration and validation is summarized in table 3 2 5 modelling setup and baseline simulations the glm simulations were conducted at an hourly time step a baseline simulation was run for nine years from 2008 to 2016 and a generic model parameterization table 4 was set according to hipsey et al 2019 the period of simulation was run from the first day with field measurements for bb pr and ti reservoirs for the other reservoirs the first day of simulation was set as january 1 2008 the initial temperature and salinity profiles in the reservoir were taken as follows for bb the vertical profile was taken from field measurements in the first simulation day for pr and ti the initial condition was set only at the surface for ba ib and na both temperature and salinity were set only at the surface as the mean value computed from the measured values in the other reservoirs the light extinction coefficient k w was determined for bb ib pr and ti reservoirs from the mean value of measured secchi depth s d and assuming k w 1 7 s d according to poole and atkins 1929 for ba and na the light extinction coefficient was set as 0 2 m 1 table 4 the initial conditions for each reservoir are summarized in table 5 the software r 3 4 0 r core team 2017 was applied to simulate and analyze modelling results using the packages glmr hipsey et al 2014 and glmtools read et al 2014 2 6 sensitivity analysis a sensitivity analysis was applied for the baseline simulation to reduce the effort of the model calibration process the one at a time method was adopted through which the model was first run with the initial parameter values table 4 and then by successive runs in which every parameter value was either increased or decreased by 10 of initial values while holding constant the remaining parameters the sensitivity of water level to ten parameters presented in table 4 was evaluated for all reservoirs and the sensitivity of water temperature was evaluated only for those with available data bb ib pr and ti at all available depths given the uncertainties in wind speed and hydrological data wind speed factor was included in the sensitivity analysis with a range of 30 and inflow and outflow factors were included with a range of 10 the objective was to account for the distance between meteorological station and reservoirs as well as wind sheltering effects and to account for groundwater inflows smaller tributaries or seepage the normalized sensitivity index si was calculated for each selected parameter according to 1 si Œ¥ y y Œ¥ x x where y is the model performance for the reference value x of the parameter which has a variation Œ¥ x generating a model performance variation Œ¥ y the model performance was assessed through the root mean square error rmse 2 r m s e i 1 n s i o i 2 n where n is the number of observations o i and s i are the ith observed and simulated data respectively 2 7 calibration and validation a calibration procedure was performed to simultaneously optimize both water level and temperature for the reservoirs with measured field data bb ib pr and ti for bb pr and ti reservoirs a 5 year calibration period was determined from the first day of the baseline simulation to december 31 2012 for ib reservoir the simulation was calibrated from november 1 2013 to september 18 2014 for ba and na due to the lack of measured field data the model was calibrated only regarding water level for the period from january 1 2008 to december 31 2012 the initial conditions were set as the same adopted for the baseline simulation the first year of simulation 2008 was considered as a warm up period to avoid uncertainties related to the initial conditions thus it was not included in the performance assessment the automatic calibration procedure was executed using the package glmgui poschlod et al 2018 which implements the brute force method for all possible combinations and permutations of the selected parameters to gradually improve them until the simulation results reached the lowest deviations between simulated and measured water level and water temperature the selection of parameters to be calibrated was based on the sensitivity analysis parameters with si 0 01 were calibrated to optimize both water level and temperature simultaneously the initial values of mixing parameters were tested for a percentage range of 50 considering that those values can be significantly different in a tropical region from the initial ones usually estimated in a temperate zone table 4 the surface transfer coefficients were calibrated for ranges experimentally determined in previous studies brutsaert 1982 xiao et al 2013 0 7 1 7 10 3 for the latent heat transfer coefficient ce 0 7 2 02 10 3 for the sensible heat transfer coefficient ch and 0 6 3 4 10 3 for the momentum coefficient cd table 4 the calibration range for wind factor was set between 0 7 and 1 3 30 and for inflow and outflow factors was set between 0 9 and 1 1 10 the best fitted model of water level and temperature was validated using the calibrated parameter values and an independent set of data for a 4 year period from the first day with field measurements in 2013 to december 31 2016 for bb pr and ti reservoirs the initial conditions in these reservoirs were taken from field measurements in the first simulation day the validation of water temperature was not performed for ib reservoir due to the absence of data beyond the dataset applied for the calibration in this manner for ba ib and na the model was validated only regarding water level for the period from january 1 2013 to december 31 2016 a summary of the periods applied for calibration and validation for each reservoir is presented in table 6 the objective function used for both calibration and validation procedures was the rmse the model performance was also assessed through the linear correlation coefficient r computed from simulated and measured water level and water temperature and the rmse normalized by the data range nrmse in order to standardize comparisons among reservoirs the abovementioned error metrics were calculated as follows 3 r i 1 n s i s o i o i 1 n s i s 2 i 1 n o i o 2 1 2 4 n r m s e r m s e o m a x o m i n where n is the number of observations o i and s i are the ith observed and simulated data o and s are the mean observed and simulated data and o m a x and o m i n are the maximum and minimum observed data respectively 2 8 parameterization strategy for non monitored reservoirs since there is no field data for the calibration procedure of water temperature for ba and na linear regression curves were constructed aiming at the estimation of the most sensitive parameters as indicated in the sensitivity analysis a linear correlation analysis was performed between the calibrated parameter values for bb ib pr and ti and their morphometric and meteorological variables the following variables were selected based on their physical relationship with model parameters as reported in previous studies i mean wind speed computed from 2008 to 2016 considering the calibrated wind factor m s 1 ii fetch f km calculated from surface area a s as f a s œÄ hipsey et al 2019 iii mean depth m iv surface area km2 and v perimeter km as measure of reservoir s size wind speed data and reservoir morphometric characteristics such as mean depth surface area perimeter and fetch are in general easily obtained and are available for the reservoirs of this study furthermore their relationship with hydrodynamic processes in lentic systems was extensively presented in previous studies harbeck 1962 strub and powell 1987 kwan and taylor 1994 ven√§l√§inen et al 1998 atakt√ºrk and katsaros 1999 heikinheimo et al 1999 schertzer et al 2003 xiao et al 2013 based on the curves with the highest correlation in an attempt to minimize uncertainties regarding the adoption of parameter values suggested in the literature the best fitted linear regression was obtained for estimating the parameter values for ba and na the reservoirs without measured field data 2 9 hydrodynamic metrics hydrodynamic metrics were calculated based on the modelling results using the package rlakeanalyzer winslow et al 2018 in this study mixing was assumed when the temperature difference Œ¥t between the surface and the bottom layers was smaller than 2 0 c lewis 2000 otherwise it was the stratification condition the onset of stratification was the first day when Œ¥t 2 c for a period longer than 96 h epilimnion and hypolimnion were defined respectively as the region above and below the thermocline where the density gradient was lower than 0 1 kg m 3 per meter winslow et al 2018 the schmidt s stability index s t j m 2 indicates the intensity of stratification i e the dynamic stability of the system by calculating the resistance to mechanical mixing due to the potential energy inherent in the stratification of the water column idso 1973 s t values represent the amount of work required to mix the entire water body to a uniform density in this manner the higher is s t the higher the strength of stratification is schmidt s stability index is calculated as 5 s t g a s 0 z d z z v œÅ i œÅ z a z z where g is the acceleration due to gravity m s 2 a s is the reservoir surface area m2 a z is the area of the reservoir at depth z m2 z d is the maximum depth of the reservoir m z v is the depth of the volume center of the reservoir m œÅ i is the mean density value kg m 3 and œÅ z is the water density at depth z kg m 3 the wedderburn number w describes the likelihood of water upwelling under stratified conditions w was applied as an index of 1 d suitability as it is a relative measure between the vertical stratification strength against the horizontal wind energy at surface the 1 d assumption is suitable for w 1 when the mixed layer deepens slowly imberger and patterson 1990 otherwise w 1 there is a high potential for mixing events predicting wind induced internal waves the wedderburn number can be written as 6 w g z e 2 u 2 l s where g g Œ¥ œÅ œÅ h is the reduced gravity m s 2 due to the difference Œ¥ œÅ between the hypolimnion density œÅ h and epilimnion density z e is the depth of the mixed layer m l s is the reservoir fetch length m and u is the water friction velocity due to wind stress m s 1 3 results 3 1 sensitivity analysis the sensitivity analysis highlighted some variability across the ten parameters and three input data factors as expected the water level had the highest sensitivity to perturbations in inflow and outflow factors fig 3 a additionally there was a high level of sensitivity for the latent heat transfer coefficient ce and the wind factor the sensitivity analysis regarding the water temperature revealed a consistent level of sensitivity for all reservoirs to the three coefficients related to surface heat exchange latent heat transfer coefficient ce sensible heat transfer coefficient ch and momentum coefficient cd and to three input data factors wind inflow and outflow factors fig 3b for both water level and water temperature there was little sensitivity to perturbations in mixing parameters 3 2 model performance the calibrated inflow and outflow factors that achieved the lowest discrepancies between simulated and measured water levels in the reservoirs ranged from 1 00 to 1 06 table 7 the optimization of those factors enabled a fair estimate of the water balance and the model succeeded in the prediction of the water level the seasonal variations were simulated correctly supporting the model s ability to compute the water balance glm tended to produce positive bias regarding the water level of ba and ti overestimating the total volume the rmse varied from 0 12 m to 1 56 m n 2922 over the simulation period see fig 4 in the supplementary material and r varied from 0 49 to 0 99 see fig 5 and table 5 in the supplementary material for bb and pr reservoirs three parameters sensible heat transfer coefficient ch latent heat transfer coefficient ce and momentum coefficient cd and wind factor were calibrated instead of ten according to the sensitivity analysis for ib reservoir ce cd wind factor and the light extinction coefficient kw were calibrated and for ti reservoir mixing efficiency for wind stirring cw and mixing efficiency for unsteady turbulence ct were calibrated in addition to the aforementioned ones table 7 presents the optimized values that reached the best simulation results water temperatures were simulated accurately for the calibration period mean rmse of 1 49 c as well as for the validation period mean rmse of 1 65 c see table 6 in the supplementary material positive and negative deviations were observed for daily temperatures but they were generally small see fig 6 in the supplementary material model efficiency for water temperature was similar across the four reservoirs and correlations were quite high fig 4 indicating a robust fit for glm 3 3 parameterization strategy for non monitored reservoirs applying regression curves for ba and na reservoirs the lack of field data prevented the calibration procedure for the simulation of water temperature assuming that the parameters related to surface heat exchanges latent heat transfer coefficient ce sensible heat transfer coefficient ch and momentum coefficient cd and the wind factor would present a high sensitivity index for these reservoirs as they presented for the other ones their values were linear correlated to known morphometric and meteorological variables although the light extinction coefficient kw showed a high si for ib and ti reservoirs it was not included in the parameterization strategy due to the unavailability of water quality data potentially related to kw such as suspended solids concentration phytoplankton biovolume or chlorophyll a concentration inflow and outflow factors were not included because water level data was available for their calibration in ba and na reservoirs linear correlation analysis revealed that surface heat exchange parameters were significantly correlated to mean wind speed reservoir s mean depth perimeter and fetch table 8 the correlation of the three parameters with the surface area was not significant the latent heat transfer coefficient ce showed the highest linear correlation to the mean depth the sensible heat transfer coefficient ch was mostly correlated to the reservoir s fetch and the momentum coefficient cd presented the highest linear correlation with wind speed from the best fitted relationships the following regression equations were obtained 7 c e 10 3 0 117055 h 0 462997 8 c h 10 3 0 075702 f 1 035025 9 c d 10 3 0 181924 u 10 0 726093 where h is the mean depth m f is the fetch km and u 10 is the mean wind speed the equations among the parameters and the other variables are presented in table 7 in the supplementary material the surface transfer coefficients calculated from the regression equations were adopted for the parameterization of ba and na reservoirs fig 5 linear regressions are valid for variable ranges that result in parameter values inside the pre defined ranges according to literature the value obtained for the latent heat transfer coefficient for ba reservoir ce 0 0005 was slightly lower than the pre defined range according to literature in this case ce coefficient was adopted as the minimum value in the range 0 0007 linear correlations of wind speed factor were significant for reservoir s mean depth perimeter and fetch table 8 however the correlation coefficients were small revealing that the available data did not provide a sound basis enough for the parameterization procedure in this manner wind speed factors for ba and na reservoirs were maintained at their initial values the estimated parameter values for ba and na reservoirs are presented in table 7 3 4 thermal regime glm simulated the thermal structure of the reservoirs over an 8 years period different thermal conditions in the water column were represented by the simulation a bb and ba presented a polymictic behavior with full mixing events alternating with short periods around 5 days of thermal stratification throughout the year and b ib pr na and ti showed a monomictic regime characterized by a periodic annual mixing event during the winter with a nearly uniform profile at 21 6 c mostly recorded from april to september and the stratification was characterized by temperature gradients from 2 0 to 13 6 c with a well established thermocline between october and march fig 6 mean surface temperature varied from 22 3 to 26 3 c and mean bottom temperature varied from 19 7 to 22 9 c table 8 in the supplementary material the warmest mean temperature in surface 27 4 2 1 c was recorded in february during stratification events the epilimnion depth varied from 1 3 to 18 3 m the mean thermocline depth ranged from 2 0 to 19 3 m and the metalimnion thickness varied between 0 7 and 4 2 m schmidt s stability index was calculated using simulated temperatures the mean stratification intensity varied from 4 5 j m 2 in ba to 668 441 j m 2 in ti table 8 in the supplementary material with peaks mostly in september and october at the onset of the stratification period and then gradually losing stability with lowest values from may to july the wedderburn number reached the maximum values during the stratification period simulated mean values were much higher than one varying between 14 and 5143 w lower than one occurred in few occasions over the simulation period from 0 to 4 of the time except for pr reservoir which experienced more mixing events 9 of the time induced by wind 4 discussion 4 1 sensitivity analysis the 1 d hydrodynamic model glm was used for investigating time changes in thermal structure across six sub tropical reservoirs with different physical characteristics the sensitivity analysis revealed that water level simulation was highly sensitive to inflow and outflow factors wind speed factor and to the bulk aerodynamic coefficient for latent heat transfer ce the expected high sensitivity to the inflow and outflow factors is due to their direct contribution to the reservoir s volume the high sensitivity of both wind speed factor and ce on water level is likely related to their influence on the calculation of evaporation rate in the simulated reservoirs evaporation rates varied from up to 2 6 mm day 1 in na to up to 10 2 mm day 1 in ti revealing the importance of evaporation as a component of water loses in subtropical lentic systems where its rate is high in face of the high water temperatures ji 2007 p 70 regarding water temperature the reservoir models were sensitive to surface energy exchanges parameters ce ch cd in addition to the factors related to input data inflow outflow and wind speed rather than by mixing efficiency inflow and outflow discharges are related not only to water level surface area and volume but also to the heat budget as an input of kinetic energy wind is usually a major source of energy in lakes by supplying kinetic energy for the deepening of the surface mixed layer and for turbulent processes through internal waves its high sensitivity index is consistent with the literature indeed several studies reported wind speed as a driver of the mixing behavior as in lake iseo italy valerio et al 2015 in oneida lake united states hetherington et al 2015 and in rappbode reservoir germany mi et al 2018 the latent heat transfer coefficient ce is related to heat loss due to evaporation the sensible heat transfer coefficient ch accounts for heat gains and losses due to the heat flux between the surface water and the atmosphere when they have different temperatures the momentum coefficient cd is related to heat losses through the action of the wind across the lake surface hipsey et al 2019 the high sensitivity to surface energy exchange parameters was also reported for lake baratz italy and lake ammersee germany bueche et al 2020 considering that all reservoirs have water temperature measured at surface but not along the water column for two of them it was expected a higher degree of sensitivity to changes in ce epilimnion temperatures are more sensitive to changes in ce while hypolimnion temperatures are more sensitive to changes in cd bruce et al 2018 indeed ib reservoir was the one with the deepest measured temperatures and the highest sensitivity to cd in particular ti reservoir was also sensitive to mixing efficiency for wind stirring cw and for unsteady turbulence ct these parameters are directly related to the transfer of wind energy to the mixing process therefore deep lakes with long residence time are generally sensitive to them since they require greater efficiency in transferring energy until the bottom of the water column the lower sensitivity to the parameterization of the other mixing coefficients indicates the dominance of surface boundary conditions in the thermal budget associated with the lack of measured temperatures in the hypolimnion for pr and ti reservoirs the sensitivity analysis highlighted the importance of generating reliable hydrologic input data and the need for in lake wind measurement a further outcome of the sensitivity analysis was to identify the candidate parameters for the calibration exercise that future applications in tropical reservoirs can refer to 4 2 model performance since the model was sensitive to a set of parameters the calibration process was recommended for the four reservoirs with available field data water level calibration resulted in good reproduction of the water balance differences between the simulated and observed water level were minimal through most of the reservoirs rmse between 0 12 m and 0 21 m except for bb rmse of 0 90 m and ti rmse of 1 56 m however considering their water level variation 5 8 m in bb and 9 4 m in ti their normalized error nrmse was in accordance with the others rmse values found in this study were in agreement with literature rmse from 0 15 to 0 20 m for hsinshan reservoir chang et al 2015 rmse of 0 12 m for grosse dhuenn reservoir weber et al 2017 and rmse from 0 36 to 0 73 m for karaoun reservoir fadel et al 2017 deviations between simulated and observed water levels could be attributed to non measured withdrawals in the catchment basins inaccurate knowledge of the reservoirs geometry and imprecise discharge measurements the simulation of the thermal structure performed well across the four reservoirs during the 8 years simulation period simulations of water temperature were satisfactory with an average overall rmse of 1 48 c in the range of previous studies applying glm 1 17 c in lake ammersee and 1 30 c in lake baratz bueche et al 2020 1 33 2 06 c in serra azul reservoir soares et al 2019 0 89 c in rappbode reservoir mi et al 2018 1 34 c in 32 lakes worldwide bruce et al 2018 0 19 1 18 c in lake ammersee bueche et al 2017 1 23 c in grosse dhuenn reservoir weber et al 2017 0 87 1 49 c in lake maggiore fenocchi et al 2017 and 2 78 c in 2368 lakes in the united states read et al 2014 the results of the model validation confirmed the robustness of the calibrated model the discrepancies between measured and simulated temperature may be attributed to one or more of the following sources of uncertainty 1 assumption of horizontal homogeneity 1 d models are not capable to fully account for three dimensional processes which can be important for vertical transport of heat below the epilimnion mi et al 2018 2 assumption of a single average light extinction coefficient kw over eight annual seasonal cycles particularly for lakes with high light extinction coefficient kw 0 5 m 1 which were the case of four reservoirs bb ib pr and ti the lack of seasonal variation of kw may exert a negative influence on the simulation accuracy bueche et al 2017 and lakes with kw 0 5 m 1 recorded the greatest error in the prediction of temperatures in a previous study bruce et al 2018 the incorporation of intra annual effects of light extinction by providing kw as forcing input to the model or coupling glm to a water quality model with explicit light extinction feedback properties could help to improve model performance 3 inaccuracies of measured field data the meteorological data especially wind speed was not recorded in situ not being representative of actual meteorological conditions over the lake and 4 the simple method for predicting daily inflow temperatures through linear regression with minimum and mean air temperature and solar radiation may provide inaccurate predictions arismendi et al 2014 although previous studies have also applied this strategy with satisfactory results e g hornung 2002 bueche and vetter 2015 this uncertainty could be reduced in future studies by applying a more sophisticated method for estimating inflow temperatures such as the tool air2stream which is based on daily air temperature and flow discharge and has obtained satisfactory results in a wide range of different rivers and conditions toffolon and piccolroaz 2015 despite those uncertainties the model was capable to meaningfully relate the hydro meteorological forcing to the seasonal stratification dynamics capturing the thermal behavior of each reservoir in this sense the model system was considered to be adequate for the purposes of the current study 4 3 1 d suitability the fundamental assumption of 1 d models is that the mixing within the lake can be constrained by processes acting in the vertical and that gradients in the horizontal plane such as the degree of upwelling of the thermocline are much lower having minimal impact on vertical transport this assumption is usually valid for stratified lakes but can be disrupted by intense wind events inducing upwelling of cold water mean values of simulated wedderburn number were much higher than one w 1 for all reservoirs corroborating the suitability of the 1 d approach upwelling occurred over short periods up to 4 of the total simulation period except for pr reservoir 9 of the total simulation period in this case the model probably overmixed the water column and thus underestimated lake stability kerimoglu and rinke 2013 4 4 thermal regime this work has advanced compared to previous studies by filling the gaps in the understanding of the reservoirs hydrodynamic processes overall the hydrodynamics simulation successfully reproduced most aspects of the thermal structure and was able to replicate the intra annual variability of the water temperature in all reservoirs bb and ba presented a typical polymictic behavior of shallow lakes with intermittent mixing throughout the year halted by rapid stratification events the thermal behavior of ib pr na and ti was comparable to what has been documented about tropical reservoirs which are fundamentally warm monomictic with a mixing season during winter lewis 2000 from october stratification becomes stronger and the thermal structure remains stable until march the variation of schmidt s stability values throughout the year corroborated the simulated thermal behaviour 4 5 parameterization strategy for non monitored reservoirs applying regression curves vertical transfer of heat and momentum between surface water and atmosphere is an important process for the energy budget of lentic systems which is widely simulated in hydrodynamic models through empirical bulk aerodynamic formulae see strub and powell 1987 for further information on bulk formulae in these formulae surface transfer coefficients i e coefficients of sensible heat latent heat and momentum represent the efficiency of turbulent transport between surface water and a reference level usually 10 m above the surface heikinheimo et al 1999 since bulk aerodynamic formulae are an empirical approach estimation and calibration of surface transfer coefficient values in hydrodynamic models such as glm is an important step for thermal processes simulation especially for seasonal time scales fischer et al 1979 brutsaert 1982 p 203 the surface transfer coefficient values in reservoirs and lakes may depend on the temporal scale of interest atmospheric stability wind speed reservoir size depth fetch and exposure to wind harbeck 1962 hicks 1972 strub and powell 1987 ji 2007 p 31 in this work linear regression curves between calibrated surface transfer coefficients of monitored reservoirs and their morphometric features and wind speed were used in order to estimate parameters values for the non monitored ba and na reservoirs the latent heat transfer coefficient ce showed a positive correlation with reservoir s mean depth which is likely related to the greater capacity of deep lakes to store internal energy compared to shallow lakes ven√§l√§inen et al 1998 as ce depends on the available energy in the lentic system to maintain evaporation as well as on surface temperature ce value tends to be higher in deep lakes than in shallow ones the positive correlation between the sensible heat transfer coefficient ch and the reservoir s fetch is probably related to the increasing of wind speed with the fetch over the lake theoretically a higher fetch enhances the mixing of the air and the local friction velocity intensifying the efficiency of heat transport through turbulence the momentum coefficient cd increases with the mean wind speed wind action over water generates waves currents and turbulence macintyre 1983 resulting in a rougher surface which enhances the vertical transfer of horizontal momentum from atmosphere to water ji 2007 p 169 the wind speed factor should be estimated in face of its high sensitivity regarding both water level and temperature however the parameterization strategy could not be applied for this factor in face of the poor correlation coefficient with morphometric features of the reservoirs instead for study sites where land use and cover are available both for the reservoir s surroundings and near the meteorological station a correlation analysis could provide insights for its value estimation as an empirical method the validity of the linear regressions is dependent on the morphometric features being within the range of this work and parameter values within the range recommended in the literature see table 4 the approach applied in this study could be extended to lakes and reservoirs with mean depth between 10 m and 18 m for ce estimation fetch between 0 and 13 km for ch estimation and mean wind speed between 0 and 14 m s 1 for cd estimation a wide spectrum of lakes and reservoirs in poorly monitored areas could benefit from this parameterization strategy which is facilitated by the available and open source module chain of packages glmr hipsey et al 2014 glmtools read et al 2014 glmgui poschlod et al 2018 rlakeanalyzer winslow et al 2018 the authors are aware of the limitations imposed by the few number of reservoirs used to construct the empirical relationships since the results presented here are based on measurements made on four reservoirs however the proposed approach represents an initial effort to estimate sensitive parameters for regional reservoirs which have different morphometric characteristics it may be a conceptual strategy to enable modelling studies with better predictive capacity in the context of reservoirs with a lack of field data for calibration which is quite frequent in tropical countries otherwise the lack of field measurements would keep impairing modelling in regions with sparse data availability this concept of borrowing strength from well studied lakes to overcome problems of insufficient local data was introduced by bruce et al 2018 remote sensing data may also contribute to improve knowledge about the thermal behaviour of non monitored reservoirs calamita et al 2019 for instance alc√¢ntara et al 2010 applied satellite images to improve understanding of spatiotemporal variations in a brazilian hydroelectric reservoir as a complement to the expensive and time consuming conventional water quality monitoring crosman and horel 2009 proved the utility of using remote sensing data to monitor water temperature of lakes where in situ measurements are rarely available and nouchi et al 2019 demonstrated the benefit of combining in situ water analysis hydrodynamic modelling and remote sensing for investigating biogeochemical processes to the knowledge of the authors this is the first study aiming to optimize parameters for tropical lakes with no measurements of the temperature profile since the relationships are entirely empirical further systematic investigations with other frequently monitored reservoirs are strongly recommended to increase the representativeness and the fit of curves for the non monitored reservoirs that are exposed to the same regional climate 5 conclusions this study presented the hydrodynamic simulation of a cascade system composed of six reservoirs in s√£o paulo brazil although the study sites have been the subject of numerous investigations as the authors are aware the thermal behavior of these reservoirs was reproduced by a hydrodynamic model for the first time hindcast simulations applying glm reconstituted the thermal structure over 2009 2016 and was able to provide the temperature profiles as well as stratification patterns the model accurately predicted water levels and the thermal structure demonstrated good agreement based on the efficiency criteria rmse between simulated and observed temperatures the model revealed that the four reservoirs presented high sensitivity to surface energy exchange parameters the coefficients for latent heat transfer sensible heat transfer and momentum ce ch and cd respectively in addition to wind speed this pattern could be used in future studies to identify the potential parameters for the calibration procedure besides the sensitivity analysis further highlighted the importance of in situ measurements of wind speed this modelling study enhanced the current knowledge of hydrodynamics for bb ib pr and ti reservoirs since the water temperature was recorded in a scarce frequency every two months a first representation of the thermal characteristics of ba and na reservoirs whose water temperature has never been monitored was also provided through a parameterization strategy that benefited from the calibration of surface heat transfer parameters in the monitored reservoirs modelling results of the later reservoirs should be interpreted with care since there is no available data for model performance assessment and errors may always arise indeed further work to test the parameterization curves for other reservoirs is strongly recommended to critically frame the obtained results nevertheless it was considered that such simulations were useful for integrating the existing knowledge of thermal dynamics since without this modelling approach the thermal behavior of such reservoirs would remain unclear it was provided a methodological insight for estimation of potential sensitive parameters that could be highly transferable to other reservoirs with hydrological and meteorological monitoring but with a lack of extended temperature records declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national council for scientific and technological development cnpq additionally we are grateful to the national institute of meteorology inmet the national water agency ana and the environmental company of the state of s√£o paulo cetesb for the available data appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104803 
25934,the scarcity of long time series of water temperature for tropical reservoirs usually available at a low frequency of measurements represents a serious constraint for the understanding of their thermal behavior hydrodynamic models become a powerful tool for that understanding through a qualitative approach in this study the one dimensional general lake model glm was applied to hindcast multiannual temperature profiles of a cascade system of six reservoirs located in southeast brazil over a period of eight years the model parameters were calibrated and validated by field data for four reservoirs to overcome the challenge of exploring hydrodynamics in two reservoirs with a lack of measured temperatures a parameterization strategy was applied to accomplish a better simulation by the estimation of potential sensitive parameters through regression curves it is expected that this approach can be extended to other tropical reservoirs whenever field data is not available for the calibration procedure keywords thermal dynamics stratification one dimensional vertical model glm low latitude reservoirs 1 introduction the number of man made lakes has increased continuously mainly after the 1950s and currently accounts for 6 of the total inland water surface area downing et al 2006 the growing need for water has resulted in the construction of reservoirs beyond europe and the united states in several low latitude countries according to the international commission on large dams there are 59 000 reservoirs of more than 15 m depth and more than half of them 53 are located in china india brazil and south africa icold 2019 nowadays reservoirs are highly valued as they serve multiple purposes and provide numerous ecosystem services such as fishery flood control recreation nutrient uptake sediment stabilization and navigation schallenberg et al 2013 beyond the historical uses for water supply and hydropower generation they make a significant contribution to the efficient management of finite water resources that are subject to large seasonal fluctuations compared to lakes reservoirs have been poorly documented in the tropics it is in part because they are located far from the most active research centers causing a corresponding gap in limnological investigations as a result 41 of the 54 most large low latitude reservoirs have not yet been subject to basic limnological studies winton et al 2019 low latitude reservoirs often lack long term series of temperature profiles an easily measurable in situ data many reservoirs including the current study site only have reliable data sets for irregular periods in such context hydrodynamic models are fundamental tools to support the comprehension of lake s dynamics since they can simulate and predict multiannual cycles of water temperature providing additional information to field measurements on mixing extent and stratification periods they simulate the vertical thermal dynamics which have important implications for the biogeochemical and ecological cycles including renewal of dissolved oxygen in the hypolimnion the release of nutrients from the sediments as well as the control of timing magnitude and composition of phytoplankton blooms cao et al 2016 therefore given the paramount importance of the thermal structure for lentic aquatic ecosystems for both field biologists interested in ecosystem ecology and engineers charged with the management of reservoirs hydrodynamic models have become more abundant and sophisticated since the early models developed in the 1970s imberger 1985 they have been used for various purposes such as to evaluate the response to different wind conditions mi et al 2018 to evaluate the impacts of storm water runoff on lake ecosystems silva et al 2019 to select the withdrawal depth kerimoglu and rinke 2013 and to forecast the mixing regime under climate change scenarios fenocchi et al 2018 it is expected that as the population grows simultaneous competing water uses in addition to increased nutrient loading will intensify anthropogenic pressures on low latitude reservoirs increasing their vulnerability globally folke et al 2004 hence the understanding of their thermal behavior is essential to assist in planning mitigation and adaptation strategies in this context the main objective of this study was to deepen the knowledge on the thermal behavior across six reservoirs using them as representative of the sub tropical ones by applying the well established 1 d hydrodynamic model glm over an 8 year period 2009 2016 the specific aims were as follows 1 to simulate multiannual cycles of the thermal behavior 2 to perform a sensitivity analysis of parameters and variables influencing water level and water temperature 3 to calibrate and validate the model for simulating both the water temperature and the water balance and 4 to provide an experimental calibration strategy for two reservoirs lacking water temperature data to illustrate how hydrodynamic modelling can provide surrogates for missing data we expect that the results provide useful guidance for modelling efforts of poor monitored reservoirs especially in low latitude countries 2 materials and methods 2 1 study site the study site is located in southeast brazil and comprises a sub tropical cascade system along the tiet√™ river composed by the following six reservoirs from upstream to downstream barra bonita bb bariri ba ibitinga ib promiss√£o pr nova avanhandava na and tr√™s irm√£os ti the reservoirs were built from 1964 to 1991 and they were hydrologically and operationally connected through the cascade for attending the common goal of electricity generation taking advantage of the cumulative effect of flow regulation to attend country demand nowadays the reservoirs are intended for multiple uses see table 1 in the supplementary material this cascade system was chosen because it is composed of reservoirs representing many others of tropical regions where water temperature is warm throughout the year in addition the reservoirs have a varied range of morphometric features such as mean depth from 8 6 m to 17 2 m surface area from 45 km2 to 654 km2 and volume from 0 61 109 m3 to 11 46 109 m3 table 1 high resolution meteorological stations are situated near them gauged inflows and outflows are available and water quality stations are available for some reservoirs fig 1 land use in their catchment basin is characterized by urban areas with more than 24 million inhabitants ibge 2019 and intensive agriculture mainly sugarcane camara and caldarelli 2016 the climate in the region is sub tropical k√∂ppen 1948 with a rainy season from october to march and a dry season from april to september the mean annual rainfall from 1981 to 2010 was 1406 mm the mean air temperature was 22 6 c with a minimum of 14 0 c in july and a maximum of 30 4 c in october inmet 2019 2 2 hydrodynamic model description the hydrodynamic one dimensional general lake model glm v 2 2 0 hipsey et al 2019 was applied to predict the thermal structure of each reservoir glm was selected mainly because of its good performance documented in several investigations all around the world e g bueche et al 2017 bruce et al 2018 the model assumes that horizontal gradients are negligible and it computes the vertical profiles of temperature salinity and density over time in lentic water bodies such as lakes reservoirs mining pit lakes wastewater ponds and urban wetlands whether shallow and well mixed or deep and stratified glm simulations are based on parameterizations of mixing processes surface dynamics energy budget and water balance the algorithms are based on equations defined by hamilton and schladow 1997 and by imberger and patterson 1981 the occurrence of thermal stratification and vertical mixing processes is based on the energy balance between the kinetic energy and the internal potential energy of the water body the model assumes the occurrence of mixing whenever available kinetic energy exceeds the potential energy required to keep stratification the available kinetic energy is calculated from wind stirring shear production between layers convective overturn and kelvin helmholtz billowing the mixing process ends when the internal kinetic energy is insufficient to overcome the stored potential energy then the thermal stratification process begins hipsey et al 2019 2 3 required input data for modelling glm requires the following site information as input data morphometry meteorological time series and hydrological time series obtaining and processing of input data required by glm is described in the following paragraphs i the hypsographic curve of each reservoir was provided by bathymetric surveys from previous studies a comparative plot of the hypsographic curves for all reservoirs shows diversity in reservoir size and volume fig 2 ii high resolution meteorological data from terrestrial based stations were available hourly measurements of air temperature wind speed solar radiation air relative humidity and rainfall were obtained from five monitoring stations operated by the national institute of meteorology inmet 2020 red markers in fig 1 table 2 in the supplementary material the meteorological variables were measured as required by glm air temperature air relative humidity and wind speed were measured 10 m above ground solar radiation and rainfall were measured at the ground wind direction although not required by glm is presented in fig 1 in the supplementary material for pr and na reservoirs meteorological data were obtained from one station the closest one to both reservoirs the missing values table 3 in the supplementary material of air temperature solar radiation and air relative humidity were filled by linear regression with data from the meteorological station with the highest correlation coefficient table 4 in the supplementary material when data was not available for both stations remaining data gaps were filled by the linear regression with the station of the following highest correlation coefficient and then successively in face of the weak linear correlation between measured data in different meteorological stations regarding wind speed r 0 54 and rainfall r 0 27 the missing data were filled by linear interpolation when data gaps were longer than ten consecutive hours instead of using the linear interpolation method gaps were filled by the mean monthly value cloud cover data was not available and it was estimated by glm based on the bird clear sky insolation model bird 1984 which compares the measured and theoretical clear sky solar irradiance in order to approximate the cloud cover fraction iii daily hydrological data has been provided by the national water agency ana 2018 gaps in daily inflow and outflow volume were rare less than 0 6 of total data and they were filled by linear interpolation inflow temperature data was measured every two months by the environmental company of the state of s√£o paulo cetesb 2019 for all reservoirs yellow circles in fig 1 except for ib for this reservoir data from ba was adopted inflow temperature was then converted into daily frequency through linear regression between bi monthly inflow temperature and minimum air temperature bb ba pr and ti mean air temperature ib and maximum shortwave radiation na the meteorological variable used in the linear regression was selected from the highest correlation coefficient with bi monthly inflow temperatures r values between 0 56 and 0 91 see fig 2 in the supplementary material as inflow salinity is not monitored it was estimated from electrical conductivity data due to its low value below 0 2 g kg 1 a constant value was assumed for the entire simulation period fig 3 in the supplementary material presents monthly means and range of input data for the six reservoirs the description of the applied input data is summarized in table 2 2 4 required in situ data for modelling the vertical profile of temperature and salinity in the reservoir as well as secchi depth and water level are also required for parameterization setting initial conditions and performing model assessment i the vertical profile of water temperature in the middle of the reservoirs bodies was measured every 2 months as follows see purple triangles in fig 1 a for bb it was measured by cetesb 2019 in 1 m depth steps down to the bottom b for ib it was measured by the national institute of space research cairo 2015 continuously from surface until 2 m above the bottom from november 2013 to september 2014 and c for pr and ti it was measured only at the surface by cetesb ii the electrical conductivity was measured every two months only at surface for bb pr and ti and along the water column for ib reservoir iii secchi depth was measured every two months for bb cetesb 2019 and ib cairo 2015 and twice a year in 2008 and 2009 for pr and ti santos 2010 iv the water level of all six reservoirs was daily gauged by ana 2018 for ba and na reservoirs there was no field data available for the simulation period the description of the field data applied for initial conditions and modelling assessment during calibration and validation is summarized in table 3 2 5 modelling setup and baseline simulations the glm simulations were conducted at an hourly time step a baseline simulation was run for nine years from 2008 to 2016 and a generic model parameterization table 4 was set according to hipsey et al 2019 the period of simulation was run from the first day with field measurements for bb pr and ti reservoirs for the other reservoirs the first day of simulation was set as january 1 2008 the initial temperature and salinity profiles in the reservoir were taken as follows for bb the vertical profile was taken from field measurements in the first simulation day for pr and ti the initial condition was set only at the surface for ba ib and na both temperature and salinity were set only at the surface as the mean value computed from the measured values in the other reservoirs the light extinction coefficient k w was determined for bb ib pr and ti reservoirs from the mean value of measured secchi depth s d and assuming k w 1 7 s d according to poole and atkins 1929 for ba and na the light extinction coefficient was set as 0 2 m 1 table 4 the initial conditions for each reservoir are summarized in table 5 the software r 3 4 0 r core team 2017 was applied to simulate and analyze modelling results using the packages glmr hipsey et al 2014 and glmtools read et al 2014 2 6 sensitivity analysis a sensitivity analysis was applied for the baseline simulation to reduce the effort of the model calibration process the one at a time method was adopted through which the model was first run with the initial parameter values table 4 and then by successive runs in which every parameter value was either increased or decreased by 10 of initial values while holding constant the remaining parameters the sensitivity of water level to ten parameters presented in table 4 was evaluated for all reservoirs and the sensitivity of water temperature was evaluated only for those with available data bb ib pr and ti at all available depths given the uncertainties in wind speed and hydrological data wind speed factor was included in the sensitivity analysis with a range of 30 and inflow and outflow factors were included with a range of 10 the objective was to account for the distance between meteorological station and reservoirs as well as wind sheltering effects and to account for groundwater inflows smaller tributaries or seepage the normalized sensitivity index si was calculated for each selected parameter according to 1 si Œ¥ y y Œ¥ x x where y is the model performance for the reference value x of the parameter which has a variation Œ¥ x generating a model performance variation Œ¥ y the model performance was assessed through the root mean square error rmse 2 r m s e i 1 n s i o i 2 n where n is the number of observations o i and s i are the ith observed and simulated data respectively 2 7 calibration and validation a calibration procedure was performed to simultaneously optimize both water level and temperature for the reservoirs with measured field data bb ib pr and ti for bb pr and ti reservoirs a 5 year calibration period was determined from the first day of the baseline simulation to december 31 2012 for ib reservoir the simulation was calibrated from november 1 2013 to september 18 2014 for ba and na due to the lack of measured field data the model was calibrated only regarding water level for the period from january 1 2008 to december 31 2012 the initial conditions were set as the same adopted for the baseline simulation the first year of simulation 2008 was considered as a warm up period to avoid uncertainties related to the initial conditions thus it was not included in the performance assessment the automatic calibration procedure was executed using the package glmgui poschlod et al 2018 which implements the brute force method for all possible combinations and permutations of the selected parameters to gradually improve them until the simulation results reached the lowest deviations between simulated and measured water level and water temperature the selection of parameters to be calibrated was based on the sensitivity analysis parameters with si 0 01 were calibrated to optimize both water level and temperature simultaneously the initial values of mixing parameters were tested for a percentage range of 50 considering that those values can be significantly different in a tropical region from the initial ones usually estimated in a temperate zone table 4 the surface transfer coefficients were calibrated for ranges experimentally determined in previous studies brutsaert 1982 xiao et al 2013 0 7 1 7 10 3 for the latent heat transfer coefficient ce 0 7 2 02 10 3 for the sensible heat transfer coefficient ch and 0 6 3 4 10 3 for the momentum coefficient cd table 4 the calibration range for wind factor was set between 0 7 and 1 3 30 and for inflow and outflow factors was set between 0 9 and 1 1 10 the best fitted model of water level and temperature was validated using the calibrated parameter values and an independent set of data for a 4 year period from the first day with field measurements in 2013 to december 31 2016 for bb pr and ti reservoirs the initial conditions in these reservoirs were taken from field measurements in the first simulation day the validation of water temperature was not performed for ib reservoir due to the absence of data beyond the dataset applied for the calibration in this manner for ba ib and na the model was validated only regarding water level for the period from january 1 2013 to december 31 2016 a summary of the periods applied for calibration and validation for each reservoir is presented in table 6 the objective function used for both calibration and validation procedures was the rmse the model performance was also assessed through the linear correlation coefficient r computed from simulated and measured water level and water temperature and the rmse normalized by the data range nrmse in order to standardize comparisons among reservoirs the abovementioned error metrics were calculated as follows 3 r i 1 n s i s o i o i 1 n s i s 2 i 1 n o i o 2 1 2 4 n r m s e r m s e o m a x o m i n where n is the number of observations o i and s i are the ith observed and simulated data o and s are the mean observed and simulated data and o m a x and o m i n are the maximum and minimum observed data respectively 2 8 parameterization strategy for non monitored reservoirs since there is no field data for the calibration procedure of water temperature for ba and na linear regression curves were constructed aiming at the estimation of the most sensitive parameters as indicated in the sensitivity analysis a linear correlation analysis was performed between the calibrated parameter values for bb ib pr and ti and their morphometric and meteorological variables the following variables were selected based on their physical relationship with model parameters as reported in previous studies i mean wind speed computed from 2008 to 2016 considering the calibrated wind factor m s 1 ii fetch f km calculated from surface area a s as f a s œÄ hipsey et al 2019 iii mean depth m iv surface area km2 and v perimeter km as measure of reservoir s size wind speed data and reservoir morphometric characteristics such as mean depth surface area perimeter and fetch are in general easily obtained and are available for the reservoirs of this study furthermore their relationship with hydrodynamic processes in lentic systems was extensively presented in previous studies harbeck 1962 strub and powell 1987 kwan and taylor 1994 ven√§l√§inen et al 1998 atakt√ºrk and katsaros 1999 heikinheimo et al 1999 schertzer et al 2003 xiao et al 2013 based on the curves with the highest correlation in an attempt to minimize uncertainties regarding the adoption of parameter values suggested in the literature the best fitted linear regression was obtained for estimating the parameter values for ba and na the reservoirs without measured field data 2 9 hydrodynamic metrics hydrodynamic metrics were calculated based on the modelling results using the package rlakeanalyzer winslow et al 2018 in this study mixing was assumed when the temperature difference Œ¥t between the surface and the bottom layers was smaller than 2 0 c lewis 2000 otherwise it was the stratification condition the onset of stratification was the first day when Œ¥t 2 c for a period longer than 96 h epilimnion and hypolimnion were defined respectively as the region above and below the thermocline where the density gradient was lower than 0 1 kg m 3 per meter winslow et al 2018 the schmidt s stability index s t j m 2 indicates the intensity of stratification i e the dynamic stability of the system by calculating the resistance to mechanical mixing due to the potential energy inherent in the stratification of the water column idso 1973 s t values represent the amount of work required to mix the entire water body to a uniform density in this manner the higher is s t the higher the strength of stratification is schmidt s stability index is calculated as 5 s t g a s 0 z d z z v œÅ i œÅ z a z z where g is the acceleration due to gravity m s 2 a s is the reservoir surface area m2 a z is the area of the reservoir at depth z m2 z d is the maximum depth of the reservoir m z v is the depth of the volume center of the reservoir m œÅ i is the mean density value kg m 3 and œÅ z is the water density at depth z kg m 3 the wedderburn number w describes the likelihood of water upwelling under stratified conditions w was applied as an index of 1 d suitability as it is a relative measure between the vertical stratification strength against the horizontal wind energy at surface the 1 d assumption is suitable for w 1 when the mixed layer deepens slowly imberger and patterson 1990 otherwise w 1 there is a high potential for mixing events predicting wind induced internal waves the wedderburn number can be written as 6 w g z e 2 u 2 l s where g g Œ¥ œÅ œÅ h is the reduced gravity m s 2 due to the difference Œ¥ œÅ between the hypolimnion density œÅ h and epilimnion density z e is the depth of the mixed layer m l s is the reservoir fetch length m and u is the water friction velocity due to wind stress m s 1 3 results 3 1 sensitivity analysis the sensitivity analysis highlighted some variability across the ten parameters and three input data factors as expected the water level had the highest sensitivity to perturbations in inflow and outflow factors fig 3 a additionally there was a high level of sensitivity for the latent heat transfer coefficient ce and the wind factor the sensitivity analysis regarding the water temperature revealed a consistent level of sensitivity for all reservoirs to the three coefficients related to surface heat exchange latent heat transfer coefficient ce sensible heat transfer coefficient ch and momentum coefficient cd and to three input data factors wind inflow and outflow factors fig 3b for both water level and water temperature there was little sensitivity to perturbations in mixing parameters 3 2 model performance the calibrated inflow and outflow factors that achieved the lowest discrepancies between simulated and measured water levels in the reservoirs ranged from 1 00 to 1 06 table 7 the optimization of those factors enabled a fair estimate of the water balance and the model succeeded in the prediction of the water level the seasonal variations were simulated correctly supporting the model s ability to compute the water balance glm tended to produce positive bias regarding the water level of ba and ti overestimating the total volume the rmse varied from 0 12 m to 1 56 m n 2922 over the simulation period see fig 4 in the supplementary material and r varied from 0 49 to 0 99 see fig 5 and table 5 in the supplementary material for bb and pr reservoirs three parameters sensible heat transfer coefficient ch latent heat transfer coefficient ce and momentum coefficient cd and wind factor were calibrated instead of ten according to the sensitivity analysis for ib reservoir ce cd wind factor and the light extinction coefficient kw were calibrated and for ti reservoir mixing efficiency for wind stirring cw and mixing efficiency for unsteady turbulence ct were calibrated in addition to the aforementioned ones table 7 presents the optimized values that reached the best simulation results water temperatures were simulated accurately for the calibration period mean rmse of 1 49 c as well as for the validation period mean rmse of 1 65 c see table 6 in the supplementary material positive and negative deviations were observed for daily temperatures but they were generally small see fig 6 in the supplementary material model efficiency for water temperature was similar across the four reservoirs and correlations were quite high fig 4 indicating a robust fit for glm 3 3 parameterization strategy for non monitored reservoirs applying regression curves for ba and na reservoirs the lack of field data prevented the calibration procedure for the simulation of water temperature assuming that the parameters related to surface heat exchanges latent heat transfer coefficient ce sensible heat transfer coefficient ch and momentum coefficient cd and the wind factor would present a high sensitivity index for these reservoirs as they presented for the other ones their values were linear correlated to known morphometric and meteorological variables although the light extinction coefficient kw showed a high si for ib and ti reservoirs it was not included in the parameterization strategy due to the unavailability of water quality data potentially related to kw such as suspended solids concentration phytoplankton biovolume or chlorophyll a concentration inflow and outflow factors were not included because water level data was available for their calibration in ba and na reservoirs linear correlation analysis revealed that surface heat exchange parameters were significantly correlated to mean wind speed reservoir s mean depth perimeter and fetch table 8 the correlation of the three parameters with the surface area was not significant the latent heat transfer coefficient ce showed the highest linear correlation to the mean depth the sensible heat transfer coefficient ch was mostly correlated to the reservoir s fetch and the momentum coefficient cd presented the highest linear correlation with wind speed from the best fitted relationships the following regression equations were obtained 7 c e 10 3 0 117055 h 0 462997 8 c h 10 3 0 075702 f 1 035025 9 c d 10 3 0 181924 u 10 0 726093 where h is the mean depth m f is the fetch km and u 10 is the mean wind speed the equations among the parameters and the other variables are presented in table 7 in the supplementary material the surface transfer coefficients calculated from the regression equations were adopted for the parameterization of ba and na reservoirs fig 5 linear regressions are valid for variable ranges that result in parameter values inside the pre defined ranges according to literature the value obtained for the latent heat transfer coefficient for ba reservoir ce 0 0005 was slightly lower than the pre defined range according to literature in this case ce coefficient was adopted as the minimum value in the range 0 0007 linear correlations of wind speed factor were significant for reservoir s mean depth perimeter and fetch table 8 however the correlation coefficients were small revealing that the available data did not provide a sound basis enough for the parameterization procedure in this manner wind speed factors for ba and na reservoirs were maintained at their initial values the estimated parameter values for ba and na reservoirs are presented in table 7 3 4 thermal regime glm simulated the thermal structure of the reservoirs over an 8 years period different thermal conditions in the water column were represented by the simulation a bb and ba presented a polymictic behavior with full mixing events alternating with short periods around 5 days of thermal stratification throughout the year and b ib pr na and ti showed a monomictic regime characterized by a periodic annual mixing event during the winter with a nearly uniform profile at 21 6 c mostly recorded from april to september and the stratification was characterized by temperature gradients from 2 0 to 13 6 c with a well established thermocline between october and march fig 6 mean surface temperature varied from 22 3 to 26 3 c and mean bottom temperature varied from 19 7 to 22 9 c table 8 in the supplementary material the warmest mean temperature in surface 27 4 2 1 c was recorded in february during stratification events the epilimnion depth varied from 1 3 to 18 3 m the mean thermocline depth ranged from 2 0 to 19 3 m and the metalimnion thickness varied between 0 7 and 4 2 m schmidt s stability index was calculated using simulated temperatures the mean stratification intensity varied from 4 5 j m 2 in ba to 668 441 j m 2 in ti table 8 in the supplementary material with peaks mostly in september and october at the onset of the stratification period and then gradually losing stability with lowest values from may to july the wedderburn number reached the maximum values during the stratification period simulated mean values were much higher than one varying between 14 and 5143 w lower than one occurred in few occasions over the simulation period from 0 to 4 of the time except for pr reservoir which experienced more mixing events 9 of the time induced by wind 4 discussion 4 1 sensitivity analysis the 1 d hydrodynamic model glm was used for investigating time changes in thermal structure across six sub tropical reservoirs with different physical characteristics the sensitivity analysis revealed that water level simulation was highly sensitive to inflow and outflow factors wind speed factor and to the bulk aerodynamic coefficient for latent heat transfer ce the expected high sensitivity to the inflow and outflow factors is due to their direct contribution to the reservoir s volume the high sensitivity of both wind speed factor and ce on water level is likely related to their influence on the calculation of evaporation rate in the simulated reservoirs evaporation rates varied from up to 2 6 mm day 1 in na to up to 10 2 mm day 1 in ti revealing the importance of evaporation as a component of water loses in subtropical lentic systems where its rate is high in face of the high water temperatures ji 2007 p 70 regarding water temperature the reservoir models were sensitive to surface energy exchanges parameters ce ch cd in addition to the factors related to input data inflow outflow and wind speed rather than by mixing efficiency inflow and outflow discharges are related not only to water level surface area and volume but also to the heat budget as an input of kinetic energy wind is usually a major source of energy in lakes by supplying kinetic energy for the deepening of the surface mixed layer and for turbulent processes through internal waves its high sensitivity index is consistent with the literature indeed several studies reported wind speed as a driver of the mixing behavior as in lake iseo italy valerio et al 2015 in oneida lake united states hetherington et al 2015 and in rappbode reservoir germany mi et al 2018 the latent heat transfer coefficient ce is related to heat loss due to evaporation the sensible heat transfer coefficient ch accounts for heat gains and losses due to the heat flux between the surface water and the atmosphere when they have different temperatures the momentum coefficient cd is related to heat losses through the action of the wind across the lake surface hipsey et al 2019 the high sensitivity to surface energy exchange parameters was also reported for lake baratz italy and lake ammersee germany bueche et al 2020 considering that all reservoirs have water temperature measured at surface but not along the water column for two of them it was expected a higher degree of sensitivity to changes in ce epilimnion temperatures are more sensitive to changes in ce while hypolimnion temperatures are more sensitive to changes in cd bruce et al 2018 indeed ib reservoir was the one with the deepest measured temperatures and the highest sensitivity to cd in particular ti reservoir was also sensitive to mixing efficiency for wind stirring cw and for unsteady turbulence ct these parameters are directly related to the transfer of wind energy to the mixing process therefore deep lakes with long residence time are generally sensitive to them since they require greater efficiency in transferring energy until the bottom of the water column the lower sensitivity to the parameterization of the other mixing coefficients indicates the dominance of surface boundary conditions in the thermal budget associated with the lack of measured temperatures in the hypolimnion for pr and ti reservoirs the sensitivity analysis highlighted the importance of generating reliable hydrologic input data and the need for in lake wind measurement a further outcome of the sensitivity analysis was to identify the candidate parameters for the calibration exercise that future applications in tropical reservoirs can refer to 4 2 model performance since the model was sensitive to a set of parameters the calibration process was recommended for the four reservoirs with available field data water level calibration resulted in good reproduction of the water balance differences between the simulated and observed water level were minimal through most of the reservoirs rmse between 0 12 m and 0 21 m except for bb rmse of 0 90 m and ti rmse of 1 56 m however considering their water level variation 5 8 m in bb and 9 4 m in ti their normalized error nrmse was in accordance with the others rmse values found in this study were in agreement with literature rmse from 0 15 to 0 20 m for hsinshan reservoir chang et al 2015 rmse of 0 12 m for grosse dhuenn reservoir weber et al 2017 and rmse from 0 36 to 0 73 m for karaoun reservoir fadel et al 2017 deviations between simulated and observed water levels could be attributed to non measured withdrawals in the catchment basins inaccurate knowledge of the reservoirs geometry and imprecise discharge measurements the simulation of the thermal structure performed well across the four reservoirs during the 8 years simulation period simulations of water temperature were satisfactory with an average overall rmse of 1 48 c in the range of previous studies applying glm 1 17 c in lake ammersee and 1 30 c in lake baratz bueche et al 2020 1 33 2 06 c in serra azul reservoir soares et al 2019 0 89 c in rappbode reservoir mi et al 2018 1 34 c in 32 lakes worldwide bruce et al 2018 0 19 1 18 c in lake ammersee bueche et al 2017 1 23 c in grosse dhuenn reservoir weber et al 2017 0 87 1 49 c in lake maggiore fenocchi et al 2017 and 2 78 c in 2368 lakes in the united states read et al 2014 the results of the model validation confirmed the robustness of the calibrated model the discrepancies between measured and simulated temperature may be attributed to one or more of the following sources of uncertainty 1 assumption of horizontal homogeneity 1 d models are not capable to fully account for three dimensional processes which can be important for vertical transport of heat below the epilimnion mi et al 2018 2 assumption of a single average light extinction coefficient kw over eight annual seasonal cycles particularly for lakes with high light extinction coefficient kw 0 5 m 1 which were the case of four reservoirs bb ib pr and ti the lack of seasonal variation of kw may exert a negative influence on the simulation accuracy bueche et al 2017 and lakes with kw 0 5 m 1 recorded the greatest error in the prediction of temperatures in a previous study bruce et al 2018 the incorporation of intra annual effects of light extinction by providing kw as forcing input to the model or coupling glm to a water quality model with explicit light extinction feedback properties could help to improve model performance 3 inaccuracies of measured field data the meteorological data especially wind speed was not recorded in situ not being representative of actual meteorological conditions over the lake and 4 the simple method for predicting daily inflow temperatures through linear regression with minimum and mean air temperature and solar radiation may provide inaccurate predictions arismendi et al 2014 although previous studies have also applied this strategy with satisfactory results e g hornung 2002 bueche and vetter 2015 this uncertainty could be reduced in future studies by applying a more sophisticated method for estimating inflow temperatures such as the tool air2stream which is based on daily air temperature and flow discharge and has obtained satisfactory results in a wide range of different rivers and conditions toffolon and piccolroaz 2015 despite those uncertainties the model was capable to meaningfully relate the hydro meteorological forcing to the seasonal stratification dynamics capturing the thermal behavior of each reservoir in this sense the model system was considered to be adequate for the purposes of the current study 4 3 1 d suitability the fundamental assumption of 1 d models is that the mixing within the lake can be constrained by processes acting in the vertical and that gradients in the horizontal plane such as the degree of upwelling of the thermocline are much lower having minimal impact on vertical transport this assumption is usually valid for stratified lakes but can be disrupted by intense wind events inducing upwelling of cold water mean values of simulated wedderburn number were much higher than one w 1 for all reservoirs corroborating the suitability of the 1 d approach upwelling occurred over short periods up to 4 of the total simulation period except for pr reservoir 9 of the total simulation period in this case the model probably overmixed the water column and thus underestimated lake stability kerimoglu and rinke 2013 4 4 thermal regime this work has advanced compared to previous studies by filling the gaps in the understanding of the reservoirs hydrodynamic processes overall the hydrodynamics simulation successfully reproduced most aspects of the thermal structure and was able to replicate the intra annual variability of the water temperature in all reservoirs bb and ba presented a typical polymictic behavior of shallow lakes with intermittent mixing throughout the year halted by rapid stratification events the thermal behavior of ib pr na and ti was comparable to what has been documented about tropical reservoirs which are fundamentally warm monomictic with a mixing season during winter lewis 2000 from october stratification becomes stronger and the thermal structure remains stable until march the variation of schmidt s stability values throughout the year corroborated the simulated thermal behaviour 4 5 parameterization strategy for non monitored reservoirs applying regression curves vertical transfer of heat and momentum between surface water and atmosphere is an important process for the energy budget of lentic systems which is widely simulated in hydrodynamic models through empirical bulk aerodynamic formulae see strub and powell 1987 for further information on bulk formulae in these formulae surface transfer coefficients i e coefficients of sensible heat latent heat and momentum represent the efficiency of turbulent transport between surface water and a reference level usually 10 m above the surface heikinheimo et al 1999 since bulk aerodynamic formulae are an empirical approach estimation and calibration of surface transfer coefficient values in hydrodynamic models such as glm is an important step for thermal processes simulation especially for seasonal time scales fischer et al 1979 brutsaert 1982 p 203 the surface transfer coefficient values in reservoirs and lakes may depend on the temporal scale of interest atmospheric stability wind speed reservoir size depth fetch and exposure to wind harbeck 1962 hicks 1972 strub and powell 1987 ji 2007 p 31 in this work linear regression curves between calibrated surface transfer coefficients of monitored reservoirs and their morphometric features and wind speed were used in order to estimate parameters values for the non monitored ba and na reservoirs the latent heat transfer coefficient ce showed a positive correlation with reservoir s mean depth which is likely related to the greater capacity of deep lakes to store internal energy compared to shallow lakes ven√§l√§inen et al 1998 as ce depends on the available energy in the lentic system to maintain evaporation as well as on surface temperature ce value tends to be higher in deep lakes than in shallow ones the positive correlation between the sensible heat transfer coefficient ch and the reservoir s fetch is probably related to the increasing of wind speed with the fetch over the lake theoretically a higher fetch enhances the mixing of the air and the local friction velocity intensifying the efficiency of heat transport through turbulence the momentum coefficient cd increases with the mean wind speed wind action over water generates waves currents and turbulence macintyre 1983 resulting in a rougher surface which enhances the vertical transfer of horizontal momentum from atmosphere to water ji 2007 p 169 the wind speed factor should be estimated in face of its high sensitivity regarding both water level and temperature however the parameterization strategy could not be applied for this factor in face of the poor correlation coefficient with morphometric features of the reservoirs instead for study sites where land use and cover are available both for the reservoir s surroundings and near the meteorological station a correlation analysis could provide insights for its value estimation as an empirical method the validity of the linear regressions is dependent on the morphometric features being within the range of this work and parameter values within the range recommended in the literature see table 4 the approach applied in this study could be extended to lakes and reservoirs with mean depth between 10 m and 18 m for ce estimation fetch between 0 and 13 km for ch estimation and mean wind speed between 0 and 14 m s 1 for cd estimation a wide spectrum of lakes and reservoirs in poorly monitored areas could benefit from this parameterization strategy which is facilitated by the available and open source module chain of packages glmr hipsey et al 2014 glmtools read et al 2014 glmgui poschlod et al 2018 rlakeanalyzer winslow et al 2018 the authors are aware of the limitations imposed by the few number of reservoirs used to construct the empirical relationships since the results presented here are based on measurements made on four reservoirs however the proposed approach represents an initial effort to estimate sensitive parameters for regional reservoirs which have different morphometric characteristics it may be a conceptual strategy to enable modelling studies with better predictive capacity in the context of reservoirs with a lack of field data for calibration which is quite frequent in tropical countries otherwise the lack of field measurements would keep impairing modelling in regions with sparse data availability this concept of borrowing strength from well studied lakes to overcome problems of insufficient local data was introduced by bruce et al 2018 remote sensing data may also contribute to improve knowledge about the thermal behaviour of non monitored reservoirs calamita et al 2019 for instance alc√¢ntara et al 2010 applied satellite images to improve understanding of spatiotemporal variations in a brazilian hydroelectric reservoir as a complement to the expensive and time consuming conventional water quality monitoring crosman and horel 2009 proved the utility of using remote sensing data to monitor water temperature of lakes where in situ measurements are rarely available and nouchi et al 2019 demonstrated the benefit of combining in situ water analysis hydrodynamic modelling and remote sensing for investigating biogeochemical processes to the knowledge of the authors this is the first study aiming to optimize parameters for tropical lakes with no measurements of the temperature profile since the relationships are entirely empirical further systematic investigations with other frequently monitored reservoirs are strongly recommended to increase the representativeness and the fit of curves for the non monitored reservoirs that are exposed to the same regional climate 5 conclusions this study presented the hydrodynamic simulation of a cascade system composed of six reservoirs in s√£o paulo brazil although the study sites have been the subject of numerous investigations as the authors are aware the thermal behavior of these reservoirs was reproduced by a hydrodynamic model for the first time hindcast simulations applying glm reconstituted the thermal structure over 2009 2016 and was able to provide the temperature profiles as well as stratification patterns the model accurately predicted water levels and the thermal structure demonstrated good agreement based on the efficiency criteria rmse between simulated and observed temperatures the model revealed that the four reservoirs presented high sensitivity to surface energy exchange parameters the coefficients for latent heat transfer sensible heat transfer and momentum ce ch and cd respectively in addition to wind speed this pattern could be used in future studies to identify the potential parameters for the calibration procedure besides the sensitivity analysis further highlighted the importance of in situ measurements of wind speed this modelling study enhanced the current knowledge of hydrodynamics for bb ib pr and ti reservoirs since the water temperature was recorded in a scarce frequency every two months a first representation of the thermal characteristics of ba and na reservoirs whose water temperature has never been monitored was also provided through a parameterization strategy that benefited from the calibration of surface heat transfer parameters in the monitored reservoirs modelling results of the later reservoirs should be interpreted with care since there is no available data for model performance assessment and errors may always arise indeed further work to test the parameterization curves for other reservoirs is strongly recommended to critically frame the obtained results nevertheless it was considered that such simulations were useful for integrating the existing knowledge of thermal dynamics since without this modelling approach the thermal behavior of such reservoirs would remain unclear it was provided a methodological insight for estimation of potential sensitive parameters that could be highly transferable to other reservoirs with hydrological and meteorological monitoring but with a lack of extended temperature records declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national council for scientific and technological development cnpq additionally we are grateful to the national institute of meteorology inmet the national water agency ana and the environmental company of the state of s√£o paulo cetesb for the available data appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104803 
