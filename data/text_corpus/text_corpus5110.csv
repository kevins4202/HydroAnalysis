index,text
25550,models are widely used for investigating cause effect relationships in complex systems however often different models yield diverging causal claims about specific phenomena therefore critical reflection is needed on causal insights derived from modeling as an example we here compare ecological models dealing with the dynamics and collapse of cod in the baltic sea the models addressed different specific questions but also vary widely in system conceptualization and complexity with each model certain ecological factors and mechanisms were analyzed in detail while others were included but remained unchanged or were excluded model based causal analyses of the same system are thus inherently constrained by diverse implicit assumptions about possible determinants of causation in developing recommendations for human action awareness is needed of this strong context dependence of causal claims which is often not entirely clear model comparisons can be supplemented by integrating findings from multiple models and confronting models with multiple observed patterns keywords causation ecological models social ecological systems context dependence model comparison data availability no data was used for the research described in the article 1 introduction ecosystem functions and services are at risk because of overexploitation habitat and biodiversity loss pollution and in general unsustainable management transformation to sustainability is therefore mandatory as summarized in the un sustainable development goals messerli et al 2019 however due to the complexity of many ecosystems or social ecological systems clear cause effect relationships are hard to identify often we do not know the causes of a certain feature or behavior of a system and usually we cannot fully foresee the effects of certain interventions e g policy management harvesting be they desired or undesired effects however without understanding at least the most important causes underlying ecosystem dynamics successful transformation to sustainability will be hard if not impossible traditionally observation and experiments are used to discover causal relationships if sufficient data are available correlations can be identified and suggest certain causal pathways which then can be tested in targeted experiments with many ecological and especially social ecological systems though data never can be comprehensive as those systems are too complex and large likewise experiments under controlled conditions are not possible modeling is therefore widely used to study such systems assuming that a model despite its simplifications reflects reality sufficiently well for its purpose models seem to be an ideal tool for understanding causation they can be complex and cover large scales one can collect various virtual data of the modeled system and perform any kind of controlled experiments to identify causes still modeling of complex systems has its own challenges whether or not a model is realistic enough for its purpose often remains uncertain also because usually there are not enough actual data to reduce uncertainty moreover even for realistic and robust models fully understanding cause effect relationships is hard to achieve for the same reasons that real systems are hard to understand the large number of direct and indirect effects nonlinearities delayed effects complex and nested networks of interactions and the heterogeneity of the actors and factors involved in system dynamics nevertheless inferences are made from models but often the justifications and constraints of claimed cause effect relationships are implicit and therefore not transparent and clear most puzzling though is that different models addressing the same system and similar questions often vary widely in their structure the way they are analyzed and hence the respective conclusions this variation is partly inevitable as scientific explanations always depend on presuppositions of those seeking explanations e g van fraassen 1980 and it also has benefits as it means to look at the same problem from different angles however the diversity of models complicates establishing a generally accepted level of understanding of cause effect relationships that could support successful interventions and transformation while numerous reviews of models of certain ecological or social ecological systems exist so far there seems to be no attempt to review models with a focus on their causal claims and how those might be constrained not only explicitly as stated by their developers but also implicitly by decisions and presuppositions underlying the models design we therefore reviewed ecological models addressing the collapse of the baltic cod or its general population dynamics with a focus on collapse or recovery we chose this subject mainly because the number of such models is limited so that we had the chance to zoom into details of the causal claims but also because understanding collapses in general and of the baltic cod in particular is of high general interest cod is ecologically important as a top predator in a food web including its main prey species sprat and herring as well as benthic zooplankton and phytoplankton species this means that changes in cod abundance can cause and be caused by considerable changes in the abundance of other species in the food web moreover the species populations respond to environmental conditions such as water temperature salinity and oxygen content e g lindegren et al 2009 from an anthropocentric perspective cod is economically important as a fishing resource in the baltic sea cod used to be harvested in large amounts especially during periods of consistently high abundance throughout the 20th century e g österblom et al 2007 starting in the 1980s however the cod populations severely declined e g casini et al 2008 möllmann et al 2009 and have so far not recovered to consistently high abundance the baltic cod consist of two main populations the eastern and the western baltic cod stocks while there have been slight indications of recovery for both stocks in the recent decade eero et al 2012a 2014 hüssy et al 2016 sguotti et al 2019 both stocks still have critically low abundances today ices 2021a 2021b additionally the baltic cod average body size has decreased which is considered a further hindrance to recovery eero et al 2019 niiranen et al 2019 we found 15 relevant models and analyzed their corresponding publications as to which causal explanations of baltic cod dynamics were derived after inspecting the studies with respect to criteria such as the addressed research questions the model types system conceptualizations and additional assumptions we focused on the question which causal claims were made how were the claims derived and justified and how were they shaped and constrained by the model assumptions we show that multiple explicit and implicit choices are part of the context of causal claims derived from ecological models which is crucial for fully comprehending the meaning and scope of these claims 2 methods we selected 15 ecological models from 19 published studies table 1 to represent how baltic cod dynamics have been studied with a particular focus on ecological aspects involved all these models were used for studying causes of baltic cod dynamics they either represented a period including the collapse in the late 1980s and early 1990s or they were used to generally investigate the possibilities of cod collapse or recovery we did not take into account studies in which the modeling primarily focused not on the causes but on the consequences of the baltic cod collapse e g casini et al 2008 2009 to analyze how the ecological models were used to develop causal knowledge about cod dynamics we assessed the following criteria 1 which main research questions were addressed we distinguish between questions aiming at a certain degree of explanatory understanding kuorikoski and ylikoski 2015 and questions aiming at predictions this criterion is of utmost importance because models do not represent systems per se but systems with respect to a specific question understanding evaluating and comparing models thus requires awareness of the specific questions and the overall purposes of the models grimm et al 2020a 2 which model type was used various categorizations of ecological models exist e g hilborn and mangel 1997 otto and day 2007 evans et al 2013 deangelis and grimm 2014 we distinguished between statistical models and dynamic simulation models as the boundaries between these types are blurred e g dormann et al 2012 oberpriller et al 2021 we interpreted them broadly for instance we considered an artificial neural network belonging to the statistical models and any model representing system dynamics through state variables and processes that change them as dynamic simulation models 3 how was the social ecological system conceptualized in the models this comprises which entities and which relationships between them were represented in a model and how and whether entities were exogenous i e only affecting other entities or endogenous i e also being affected by other entities 4 which additional assumptions were made to define the model and how were they justified 5 which data were incorporated and how 6 which answers to the research questions were given we reviewed the causal claims that were made and differences among claims on similar subjects from the different modeling studies in particular we examined how the models were used to derive and support these causal claims we stress that all our assessments of the model characteristics and analyses were done to the best of our understanding of the publications but may be not fully precise and correct in all details unambiguously obtaining the specific information for a review of for example fish and fisheries models is not always possible cf nielsen et al 2018 3 results 3 1 research questions we found that the causal analyses based on the models always met at least one of the general research purposes of explanatory understanding or prediction cf 2 methods the identified key research questions related to some measure of cod dynamics one to three questions per study broadly fall in the following three categories of causal inquiry c causes which factors affect cod dynamics m mechanisms which mechanisms drive cod dynamics p predictions which consequences would certain environmental or management scenarios have for cod dynamics categories c and m aim at explanatory understanding but the studies varied in terms of what exactly was intended to be explained cf kuorikoski and ylikoski 2015 in category m not only the causes but also the underlying mechanisms shall be uncovered we understand a mechanism as a causal explanation that explicitly often stepwise links cause s and effect s mechanisms comprise processes that involve certain entities and that together bring about the phenomena of interest cf e g hedström and ylikoski 2010 cabral et al 2017 connolly et al 2017 heger 2022 for alternatives and discussions on the definition of mechanisms in ecology and sociology category p aims at prediction of the effects of environmental change or management decisions we rephrased the research questions to keep them brief and facilitate comparison see table 1 for all questions and their categorization for the 19 studies reviewed we found 11 posing a research question of category c causes 9 posing a question of category m mechanisms and 9 posing a question of category p predictions some questions had overlaps so they were assigned to two categories table 1 in several cases mechanisms were asked for and also provided in the studies but mainly through interpretation of the causal factors identified with the models rather than through dynamic simulations table 1 see also 3 6 causal claims table 4 it also occurred that one model was used to address several questions of different categories either in one or in different studies for example the model gadget was developed and used to detect causes of cod diet dynamics by kulatska et al 2019 and used to predict cod fishery profits under various scenarios by bauer et al 2019 although the question categories are general the particular research questions in each study often focused on specific elements of the ecological system for example which factors drive baltic cod recruitment category c krekoukiotis et al 2016 which mechanisms explain the lack of recovery of top predator cod populations when including an intermediate life stage feeding on benthos category m van leeuwen et al 2013 which eastern baltic cod exploitation level can rebuild the cod population and sustain future fishery category p jonzén et al 2002 these clear foci of analyses do not reveal which additional elements of the complex ecosystem were included in the models and if so how however such system conceptualizations provided decisive context for answering the questions and may have considerably influenced the research findings we therefore examined which entities were taken into account and linked in each of the models cf 3 3 system conceptualizations 3 2 model types given our broad categorization cf 2 methods most models could be assigned to either statistical or dynamic simulation models table 1 the model balmar lindegren et al 2009 2010 was assigned to both types since a statistical multivariate autoregressive model was fitted to time series data and the obtained food web model was then used for stochastic simulations of cod population dynamics under different scenarios all statistical models were used to answer a research question of category c causes table 1 if they were used to address additional questions of another category this happened through discussion and mechanistic interpretation of the statistical model results by contrast the dynamical models were used to answer questions of all three categories c m and p table 1 the multispecies virtual population analysis msvpa e g sparholt 1991 1994 is a very prominent dynamic model used as a standard by the international council for the exploration of the sea ices this model provides time series of interacting baltic fish stocks separately for the western ices subdivisions 22 24 and eastern ices subdivisions 25 32 baltic sea it is established to use these msvpa output time series like input data in other models as it was done in several of the reviewed studies cf 3 5 data use 3 3 system conceptualizations we observed that a large number of different species several environmental factors and fishing were taken into account in the 15 different models fig 1 this led to a huge variety of conceptualizations of the ecological or social ecological system among the models even more so as not only the entities considered vary but also which of them were related and how fig 2 in addition different sets of variables have been used to characterize the entities on top of these differences relationships between the same entities and in the same direction have been defined and parameterized differently in the different models while the cod population was included in all 15 models the models differ as to which characteristic of the cod population was the outcome of interest that is the measure of the cod dynamics for which causes should be investigated fig 3 for instance cod recruitment could be the outcome of interest and thus to be explained by a model which then also included cod abundance as one explanatory variable e g baltic cod gam möllmann et al 2008 fishann krekoukiotis et al 2016 but in other models cod abundance was to be explained and a measure of cod recruitment used as an explanatory variable instead e g msvpa sparholt 1994 mspm horbowy 2005 the cod s main prey species sprat and herring were considered in several but not all models 10 models for sprat 8 for herring in several models the fish populations were divided into age groups e g chs model heikinheimo 2011 size groups e g gadget bauer et al 2019 kulatska et al 2019 or life stage groups e g mspm horbowy 2005 also here it became obvious that even when considering the same entities models may have incorporated different state variables and different relationships between them one typical example relationship is sprat predation by cod in some models its effect on the sprat population was considered but no effect on the cod population e g chs model heikinheimo 2011 other modelers did the opposite taking into account the effect of the sprat population on the cod population but not vice versa e g margonski et al 2010 in their stock recruitment model in yet other models both directions of this causal relationship were included e g empirical baltic sea model lade et al 2015 additional biotic populations such as zooplankton zoobenthos and other species were generally considered in fewer models fig 1 for specifying species interactions in dynamic simulation models the parameter values for functional relationships representing interactions were often determined by fitting model output to data e g sparholt 1991 tomczak et al 2012 thereby invariable functional relationships were commonly assumed however interactions do vary in reality for example when abundances of preferred or non preferred prey species vary niiranen et al 2019 or in response to varying abiotic conditions köster et al 2005 hence more complex functional relationships reflecting these variations would be desirable for realistically representing actual causal relationships between species sub populations correctly defining and parametrizing such functional relationships remains a challenge plagányi et al 2014 peck et al 2018 generally unavailable or incomplete data on relevant species precluded taking into account complex species interactions in some of the models e g mentioned by möllmann et al 2008 tomczak et al 2012 wikström et al 2016 the baltic sea environment was conceptualized in the models through including various abiotic factors cod reproductive volume rv was the most common abiotic factor included in 7 models rv is a standard measure of the suitable water volume for cod egg development defined as the water volume with a suitable combination of salinity 11 psμ and oxygen 2 ml 1 conditions mackenzie et al 2000 other factors such as salinity or oxygen alone temperature or climatic indices were included in several models but less often fig 1 the environmental factors were generally considered as exogenous entities without feedback in the models they had an effect on but were not affected by other entities here an exception is detritus in the ecopath with ecosim ewe baltproweb model where this dead organic material was affected by zooplankton and zoobenthos populations niiranen et al 2012 tomczak et al 2012 in most but not all models 12 out of 15 was fishing explicitly considered the reasons for the three exceptions are as follows the baltic cod gam by casini et al 2016 aimed at explaining cod condition the stock recruitment model by margonski et al 2010 at explaining cod recruitment in both models cod population abundance was used as one explanatory variable cf fig 3 which obviously was affected by fishing and thus fishing was implicitly taken into account the baltic sea cod sprat model van leeuwen et al 2013 was used to mechanistically study the effects of cod prey switching on cod abundance cf fig 3 and on food web dynamics in general thus this model had a purely ecological focus disregarding fishing for the purpose of this analysis we have taken explicit inclusion of fishing in the models as fishers or other fishing related actors being conceptualized as an entity fig 1 this was usually an exogenous entity affecting ecological dynamics without feedback apart from sometimes monitoring catch profit as an output variable fishing was applied to cod and sometimes also to sprat and herring one model gadget had separate fishing fleets but this was not much discussed in the respective studies bauer et al 2019 kulatska et al 2019 balmar had a simple bioeconomic submodel with different incl one adaptive management strategies lindegren et al 2009 the empirical baltic sea model had a sophisticated social submodel and also other social ecological relationships than fishers affecting fish populations namely perceptions and regulations depending on cod abundance and catches lade et al 2015 however we included it in our selection of ecological models because lade et al 2015 also analyzed an ecological model version in which the ecological subsystem was decoupled from social processes i e with constant fishing effort with respect to how space and spatial heterogeneity were conceptualized it was common in the modeling studies to assume that certain areas or locations can be regarded as representative or most important for environmental effects on cod dynamics that mobile species are generally able to follow favorable conditions or prey species that fishers follow the target species or that species distributions and spatial overlap do not change and can therefore be captured by modeling invariant interactions of whole populations however it was also commonly mentioned that such assumptions were a potential limitation for correctly identifying complex causal relationships to name just a few examples tomczak et al 2012 wrote ewe models have a problem with highly migratory species or stocks patchiness christensen and walters 2004 in our case herring and sprat according to one hypothesis sprat aggregations could migrate stepputtis 2006 dependent on the cod biomass or temperature at a certain habitat also herring shows high intra species differences local spawning groups and spawning migration to coastal areas and it is difficult to apply a single environmental factor that determines herring recruitment krekoukiotis et al 2016 wrote we assumed for our data that mechanisms across baltic subdivisions are roughly the same since the integrated abundance of all spawning areas should be unaffected by transport between spawning areas köster et al 2005 we kept this spatial homogeneity within the central baltic similar to studies by sparholt 1996 and jarre teichmann et al 2000 we can t be sure to what extent this decision affects the interpretations of complex non linear relationships for recruitment kulatska et al 2019 wrote negative switching is when the proportion of a prey type in a predator s diet decreases slower than its proportion in total prey abundance this has been suggested to cause destabilization of prey populations as the impact of a predator on a prey with decreasing abundance is over proportionally high chesson 1984 however this phenomenon can also be observed as an artifact of variable predator prey spatial overlap neuenfeldt and beyer 2006 which was not accounted for in this study regarding the alternative option of spatially explicit models a critical issue would be the use of appropriate data to define and fit a model cf 3 5 data use some of the reviewed studies show that spatially resolved data e g at the resolution of ices subdivisions would be available for several environmental factors and fish populations margonski et al 2010 casini et al 2016 see also casini et al 2011 the two respective models did not explicitly include fishing cf figs 1 2 however also fishing data can be obtained at the resolution of ices subdivisions eero et al 2012b hence it seems that data availability was not the major limitation preventing spatially explicit modeling at least at a low resolution casini et al 2016 did initially account for spatial heterogeneity at the resolution of ices subdivisions 25 28 but then decided to use averages over this whole area of the central baltic sea instead because they did not observe significant variation among the subdivisions the study by margonski et al 2010 is an example where an environmental factor cod rv from different spatial locations was initially taken into account bornholm basin gdańsk basin gotland basin the three main cod spawning areas in the central baltic sea eventually one best fitting location was selected for the final statistical model 3 4 further model assumptions the system conceptualizations include the tacit assumption that the entities and relationships considered in each model are the most relevant for the specific aspect of cod dynamics to be modeled that is for answering the research questions cf table 1 similarly the selected model type and the particular definition of relationships e g functional forms parameter values include the tacit assumption that these choices are the most appropriate to adequately represent the actual ecological dynamics in addition a variety of specific assumptions have been made and stated in the studies presenting the models we commonly observed that model assumptions were explicitly based on previously published literature for example candidate causal variables of the cod dynamics in statistical models were often selected defined and justified in this way for example we used 1 ml l as threshold for hypoxia because the baltic cod has been shown to avoid oxygen concentrations below this value 46 casini et al 2016 we consider habitat driven variables proposed by köster et al 2005 that potentially explain changes in recruitment regimes krekoukiotis et al 2016 also assumptions on specific forms of causal relationships in dynamic simulation models were obtained from earlier studies for instance the cod stock recruitment equation in the chs model heikinheimo 2011 was taken from heikinheimo 2008 the threshold cod lengths for ontogenetic shifts in the baltic sea cod sprat model van leeuwen et al 2013 were obtained from hüssy et al 1997 the assumption of multiple age groups to represent ontogenetic shifts in ewe baltproweb tomczak et al 2012 was made according to another ewe version for a different ecosystem walters et al 2008 we also found model assumptions based on prior knowledge that were not directly related to literature for example in the 1980s and 1990s a marked decline in the growth rate of baltic herring was observed this phenomenon was simulated in the model by presenting the anabolism coefficient h as a function of time linearly decreasing until 1998 mspm horbowy 2005 such assumptions were apparently justified through presupposing sufficient expertise in other cases expert knowledge was explicitly referred to for example a team of baltic sea experts from the natural and social sciences collaboratively developed a conceptual model of the key ecological and social quantities and processes that contributed to cod stock dynamics empirical baltic sea model lade et al 2015 other assumptions were not made to fully reflect the knowledge of the complex ecosystem but to keep the model simple this simplification could occur implicitly through stating assumptions that obviously simplify the actual ecological setting such as the model is based on the assumptions that the distribution of predators and prey is spatially uniform predator and prey populations are each made up of identical individuals with identical life history parameters and the growth and reproductive rates of cod do not depend on consumption rates chs model heikinheimo 2011 or it was made explicit like we have made fewer assumptions and used a simple model structure and this should enhance the robustness of our conclusions single species model jonzén et al 2002 we also found simplifying assumptions that were based on the model s lack of sensitivity to changes in these assumptions for the chs model sensitivity analysis did not indicate any marked effect of an age based preference coefficient and for simplicity this was not taken into account heikinheimo 2011 for ewe baltproweb of many vulnerability parameters that define predator prey interactions only those that the modeled dynamics were known to be most sensitive to were included in the calibration to data while a default value was assumed for others niiranen et al 2012 tomczak et al 2012 available data have often influenced and constrained model assumptions this was explicitly stated in many studies for example all in all deep water salinity data was not included due to lack of sufficient availability of the data set fishann krekoukiotis et al 2016 due to available data we based our model on the swedish cod fishery empirical baltic sea model lade et al 2015 the human impact was indicated by using one fishing fleet for each fish species due to the lack of historical fleet structure and effort data ewe baltproweb tomczak et al 2012 the production models presented here are developed in such a way that the demand for age based data is reduced to a minimum mspm horbowy 2005 because of limitations in the availability of the best data we used time series covering the period 1974 2005 in our analyses baltic cod gam möllmann et al 2008 3 5 data use almost all reviewed modeling studies referred to time series data of some of the variables representing biotic or abiotic entities or fishing cf figs 1 2 there was also considerable overlap in the actual datasets used among the studies cf gårdmark et al 2013 these observational data were used directly to define the temporal dynamics of exogenous entities or indirectly to fit modeled time series to the data common biotic time series data were the abundance or biomass of cod sprat herring and other species populations sometimes these time series were stratified by age particularly for cod additional attributes e g length weight age life history events e g mortalities recruitment or diet compositions stomach contents were used too typical fishing data were time series of fishing mortality or catch amounts sometimes including age stratification e g msvpa sparholt 1991 1994 in some studies the fish and or fishing time series data used as model input were output data produced with the msvpa model e g horbowy 2005 möllmann et al 2008 heikinheimo 2011 such msvpa output data were obtained from reports of different baltic sea working groups of the ices abiotic time series data used in the models either covered single environmental factors e g salinity oxygen water temperature or specific measures obtained from these factors e g cod rv combining oxygen and salinity cf 3 3 system conceptualizations baltic sea index winter north atlantic index we also found cases where abiotic or biotic data used represent only specific times e g cod rv in august fishann krekoukiotis et al 2016 or specific locations e g multiple locations in ices subdivisions 25 32 which were then combined to represent the whole baltic sea as a single area kulatska et al 2019 or specific times and locations in the study area e g cod rv in may in the gotland basin stock recruitment model margonski et al 2010 summer temperature at 0 10m depth in the bornholm basin spring abundance of zooplankton species in the gotland basin balmar lindegren et al 2009 such specific constraints and decisions on data were presumably based on prior knowledge which was sometimes directly stated for instance in ewe baltproweb tomczak et al 2012 sea surface temperature at 0 10 m depth in august was used because it had been found to be clearly and significantly related with sprat recruitment in published studies in several models abiotic time series data were used with a specific delay lagged time series as input data this was done for example to take into account that once recruitment began under certain abiotic conditions the cod had to reach a certain age which was considered to be decisive e g balmar lindegren et al 2009 fishann krekoukiotis et al 2016 beyond time series examples of additional quantitative or qualitative data used were information on species characteristics such as on their diet e g food selection consumption rates baltic cod gam möllmann et al 2008 development e g maturation time empirical baltic sea model lade et al 2015 diet shifts throughout the life history baltic sea cod sprat model van leeuwen et al 2013 or mortality e g natural and fishing mortality chs model heikinheimo 2011 these data were typically taken from previously published studies for the empirical baltic sea model lade et al 2015 also used data on the social subsystem such as on fishing fleet composition cod supply from outside the study region or subsidy policy these data were obtained from literature or from expert knowledge fisher interviews 3 6 causal claims the models were used to derive and support a huge variety of explicit claims on the causes of baltic cod dynamics this variety is partly due to the heterogeneity among the studies different specific research questions cf table 1 we therefore identified and grouped the causal claims according to the same three categories of causal inquiry as the research questions c causes of cod dynamics table 3 m mechanisms driving cod dynamics table 4 and p predictions of cod dynamics table 5 in addition to claims that directly addressed the main research questions cf table 1 we found many more causal claims that fit into the same categories and added several examples to the overviews tables 3 5 these claims did not always relate to the main focus of the respective studies but they were based on the models presented and illustrate the overall diversity of the types of causal claims by and large the strategies for deriving causal claims were very diverse and hard to categorize table 2 inspecting the causal claims reveals that summarizing them and formulating claims that are supported by several modeling studies is only possible at a very broad level like unfavorable conditions and fishing pressure caused the baltic cod collapse however whether really both these factors caused the collapse and if so which was more important is already much more ambiguous some of the claims even seem conflicting making the formulation of generalizations challenging examining the claims more specifically shows that they are often highly context dependent this was partly acknowledged together with the claims directly more important a lot of context dependence stemmed from the respective choices of model type system conceptualization and further assumptions cf above and how the model was analyzed we illustrate this high degree of context dependence by three example topics one from each category of causal inquiry c m and p cf above on which we found various causal claims in the modeling studies 3 6 1 c causes fishing and or environmental conditions as causes of cod population dynamics several of the causal claims of category c identified either fishing pressure f or environmental conditions e or both as main causes of cod population dynamics in particular too high fishing pressure and or periods of unfavorable environmental conditions and sometimes specifically their interplay were claimed as causes of the cod collapse and remaining low abundance table 3 of the claims that include both causes f and e some explicitly ranked one cause as more important f e or e f table 3 one broadly shared causal finding among the studies is that when e were claimed as a main cause of cod population dynamics the specific environmental conditions considered in the model did include water salinity either salinity directly or via the cod rv which combines salinity and oxygen cf above by contrast several other claims did not explicitly mention e however some of those referred to recruitment r as another important cause of cod dynamics of course this r can depend on e and thus mediate effects of e this was explicitly mentioned in one causal claim cf table 3 but also when these potential causal relationships from e to r were not mentioned perhaps also not considered in the model they may still have been present in the recruitment data and affected the cod dynamics thus the modelers selected focus for formulating a claim as well as for designing a model strongly shaped the causal findings apparent differences get less conflicting upon closer examination and the causal claims should be considered in parallel rather than in opposition to each other however this makes it a big challenge to adequately summarize them into specific statements except at a very general level 3 6 2 m mechanisms the cultivation depensation hypothesis this example topic concerns mechanistic explanations for why in many marine ecosystems such as the baltic sea either large predatory fish species such as cod or their prey species such as sprat tend to be highly abundant but not both species together the hypothesis is that there are two stable states e g cod dominance or sprat dominance and each state reinforces itself either by cultivation or depensation walters and kitchell 2001 the cultivation mechanism is supposed to operate when cod is abundant many adult cods that feed on sprats keep sprat abundance low thus the cod adults reinforce favorable survival conditions for cod juveniles because sprats have a negative effect on cod juveniles survival sprats compete with cod larvae for zooplankton mainly pseudocalanus acuspes and sprats feed on cod eggs walters and kitchell 2001 möllmann et al 2009 by contrast the depensation mechanism is supposed to operate when cod abundance is low then sprats experience much less predation pressure and can achieve high abundance it follows that their negative effects on cod juveniles survival increase and thus reinforce unfavorable cod recruitment conditions the latter means low reproductive success due to reduced fitness at low density for cod also known as allee effect allee et al 1949 stephens et al 1999 roos and persson 2002 as a result from the mechanisms of cultivation and depensation cod and sprat could not simultaneously be highly abundant and a shift from high to low cod abundance irrespective of what has caused it would mean that a new and self reinforcing state of sprat dominance could establish and prevent the shift back to cod dominance therefore these mechanisms have been suggested as causal explanations of the 1980s baltic cod collapse and poor recovery e g möllmann et al 2008 casini et al 2009 while the suggested mechanisms are intuitive and appealing there are several complications for confirming or rejecting that they underlie cod dynamics in the baltic sea assuming that they are true only one of the two mechanisms would operate at a time either cultivation for cod dominance or depensation for sprat dominance the spatial areas occupied by both species can differ and their spatial geographical and vertical overlap vary the depensation mechanism with its negative effect of sprat on cod juveniles contains two pathways predation as well as competition for food depensation of cod can be difficult to attribute to high sprat abundance or unfavorable environmental conditions or fishing cf claims on fishing and or environmental conditions as causes of cod population dynamics above or interactions of these potential causes if they were simultaneously present also herring the second important prey species for cod feeds on cod eggs and zooplankton e g köster and möllmann 2000 since all three species are subject to fishing this certainly had considerable effects on their abundance apart from cultivation and depensation effects finally showing that model results are in line with the cultivation depensation hypothesis cdh does not necessarily show that the mechanisms operate the modeling studies we analyzed include diverse findings on the validity and importance of the cdh particularly of the depensation mechanism for the baltic cod collapse or poor recovery among the mechanistic claims on cod dynamics we found relations to the cdh in five studies three using dynamic models and two using statistical models table 4 for the latter the claims were located in the discussion section the statistically detected findings were mechanistically interpreted as confirming that the depensation mechanism operates möllmann et al 2008 or not margonski et al 2010 cf table 4 using a dynamic model van leeuwen et al 2013 explicitly focused on whether alternative stable states of either cod or sprat dominance in the baltic sea emerge when accounting for realistic diet shifts throughout cod life history the model included an intermediate cod life stage during which cods exclusively consume benthic organisms and the feedback effect that consumption has on the benthic organisms availability they showed that this causes cod growth to the adult stage being limited by intraspecific resource competition in this intermediate life stage instead of by interspecific competition with sprats in the juvenile life stage i e not limited by depensation section claims on cod recruitment dynamics in table 4 the model did not include cod egg predation by sprats strict criteria were applied for testing bistability under equal conditions differences between the persistence and invasion boundaries indicate a region of bistability where persistence is possible for combinations of resource productivity that do not allow for invasion van leeuwen et al 2013 hence model analyses led to claims that the complex yet purely ecological dynamics without fishing fig 2 were not in full agreement with the cdh section claims on baltic ecosystem dynamics stability in table 4 in the empirical baltic sea model by lade et al 2015 an intermediate cod life stage was included too both the intermediate and the adult cod life stage feed on benthic organisms but not exclusively as both feed on sprat and herring as well thus the food web interactions of both life stages were fundamentally the same in this generalized model a dynamical systems model in which processes are represented only with abstract placeholder functions lade et al 2015 herring several environmental factors and fishers were included in the full version with a complex social submodel but also in the decoupled ecological version cf 3 3 system conceptualizations fig 2 a specific feedback loop analysis was applied to detect main causes of cod collapse the identified destabilizing feedback loops were found to be in line with the depensation mechanism table 4 claims for the decoupled ecological and for the full model version for ewe baltproweb after fitting the dynamic model to empirical time series tomczak et al 2012 investigated how much the different interactions in the complex food web were realized in the actual simulations and over time quantifying energy flows through pathways in the food web the model included many species environmental factors and fishing fig 2 pathways that would represent depensation were found to be not intensely used from zooplankton to cod larvae from cod larvae to sprats cod larvae used as a proxy for cod eggs which were not modeled separately this led to a claim suggesting that depensation was not important for sprat dominance table 4 whether the low flow of energy from zooplankton to cod larvae could be an effect of the competition with sprat for this resource was not explicitly discussed it was mentioned that food web interactions with benthic organisms and herring could also play a significant role for shifts between different ecosystem states tomczak et al 2012 however in line with not claiming that the cdh prevents a shift back to high cod abundance in one of several other studies applying the ewe model to the baltic sea österblom et al 2007 found no indication that sprat and herring predation of cod eggs and larvae and competition for zooplankton are the mechanisms behind poor cod recovery noteworthy the model analyses and claims by lade et al 2015 and tomczak et al 2012 refer to empirical sets of population dynamics environmental conditions and fishing for periods including the baltic cod collapse the claims relate to the depensation mechanism with these empirical approaches it was not tested whether alternative counterfactual system states e g cod instead of sprat dominance for selected time points and under exactly the same conditions would have been possible such strict testing of bistability was done with the more theoretical and purely ecological model by van leeuwen et al 2013 hence the considerable differences with regard to system conceptualizations fig 2 model assumptions and analyses can partly explain the apparently controversial claims regarding the cdh that were derived with the three dynamic models table 4 similar explanations likely apply to statements in additional modeling studies that relate to the cdh but not directly to causes of cod dynamics and were therefore not included in table 4 e g the simulation results indicate that predation by cod is able to control the abundance of sprat and depress the stock to a low level in line with the cultivation mechanism heikinheimo 2011 3 6 3 p predictions cod recovery the predictive claims on cod recovery were all derived from dynamic models except for the baltic cod gam by möllmann et al 2008 in this study the causal claim was formulated in the discussion of statistical model results the claims show considerable variation and some appear controversial section claims on cod population dynamics recovery in table 5 this can partly be explained by different time frames and criteria for recovery moreover the simulated scenarios varied induced by the different research questions addressed table 1 thus in each of the studies environmental conditions fishing or both were varied and explicitly denoted in the claims as causes behind the predictions table 5 and the claims were influenced by the underlying system conceptualizations assumptions and methods for analysis as well for these reasons the predictions derived are especially context dependent and should be regarded together with all these influences 4 discussion 4 1 an analysis of causal explanations derived with ecological models models are an established means for studying causation in complex ecological systems yet it is well known not only in ecology that for any system usually a multitude of models exist often leading to diverging explanations about the causes of certain phenomena or dynamics spence et al 2018 shea et al 2020 while many useful reviews of ecological models for certain systems and questions exist they mostly focus on differences in model structure and model output but not specifically on the corresponding causal claims how those claims were derived and justified and how they were constrained by model design and assumptions we thus reviewed models from the perspective of the causal claims made as a case we used models addressing the dynamics of the baltic cod with a focus on the collapse around the late 1980s or the general potential for cod collapse or recovery 4 2 model based causal claims are highly context dependent a variety of models have been applied to find causal explanations for baltic cod dynamics over recent decades including its 1980s collapse comparing these models we found broad agreement on very general causal explanations such as that intense fishing and unfavorable environmental conditions have been causes of the collapse and poor recovery since however regarding more nuanced claims for example on the relative importance or interactions of different causes less agreement can be found tables 3 5 most importantly our review shows that ecological models for the same system were developed and analyzed in quite different ways which may reflect different research questions table 1 data availability but also for example differences in knowledge and opinion of experts krueger et al 2012 consequently the causal claims derived are highly context dependent thus the scope of these claims can be more specific than it seems and certain claims that may appear conflicting should rather be regarded as complementary while most researchers are in principle aware of such context dependence its critical impact on causal insights derived from specific models can easily be underestimated it is important to note that two different kinds of context can be distinguished first there are the explicit circumstances under which a certain causal claim holds once a model is created and specified the interdependencies and the complexity of the virtual system as it is represented by the model often necessitate formulating context dependent causation these are claims of the kind for favorable environmental conditions the cod stock would recover irrespective of fishing pressure but for unfavorable conditions only low fishing pressure would allow recovery the causal relationship from fishing pressure to cod recovery depends on the environmental context similarly when comparing studies different contexts applied in the different models e g different environmental conditions or fishing scenarios need to be considered and can clarify why causal claims diverge such dependence on the context that is explicitly represented in the models is often in the focus of causal explanations which is natural as it corresponds to how we also think about and try to find causal explanations for complex phenomena in the real world 4 3 another kind of context the system conceptualization and further model assumptions second the model based causal findings depend on a lot of additional context which is usually less explicit but at least as important it can be summarized as what goes in a model and how it is analyzed predefine what can come out this second kind of context includes the chosen focal elements of a modeling study for example the dynamics or phenomenon that shall be the model output fig 3 the candidate causes for this output or the processes and relationships that shall be varied to investigate their impact on the output it goes on with all further elements included in the system conceptualization and the relationships between them which we found differ considerably among models addressing similar questions figs 1 2 moreover the context comprises implicit assumptions for example global interactions implying that populations can always meet and interact in the same way or the omission of certain aspects in the system conceptualization implying that these are not considered relevant for causation of the studied phenomenon or perhaps simply not known a key advantage of modeling is that the second kind of context determined by the system conceptualization and additional assumptions need not be fixed but can also be systematically varied to understand its role for causation the sensitivity and robustness of causal findings in response to this context can be thoroughly tested saltelli 2004 thiele et al 2014 grimm and berger 2016 peck et al 2018 drechsler et al 2022 this means analyzing different model versions that exclude or include certain entities follow different assumptions on causal relationships between them or different definitions and parameterizations of processes representing these relationships such analyses can add to the insights gained from showing that a certain version of a dynamic model satisfactorily reproduces the data and inspecting the mechanisms at work in this specific model particularly very complex models aimed to reflect the complexity of the real ecosystem tend to have many degrees of freedom for adjusting them to data this bears the risk that different model versions can reproduce a phenomenon similarly well but the underlying causal relationships and mechanisms that lead to the phenomenon in the model differ strongly equifinality this was shown in a comparison of many versions of the same model ewe baltproweb with similar goodness of fit to the data by niiranen et al 2012 cf table 4 it might apply also to other studies such that alternative equifinal model versions would exist that perform similarly well but differ in terms of the explanations and predictions they provide hence model equifinality may have contributed to the heterogeneity of causal claims that we observed tables 3 5 4 4 key ecosystem features when studying causation with models species interactions and spatial heterogeneity one example belonging to the second kind of context that can considerably affect model results and thus causal findings is how species interactions were represented although the marine food web in the baltic sea comprises comparably few species e g bagge and thurow 1994 österblom et al 2007 a variety of potential interactions are known this required modelers choices about which species and which relationships among them to include in the system conceptualizations and these choices varied widely cf 3 3 system conceptualizations for instance separating species into subpopulations age size or life stage groups with different inter and intraspecific interactions can be decisive for emergent cod dynamics e g van leeuwen et al 2013 in addition even when modeling the same biotic entities and the same interactions between them the actual types and parametrization of functional relationships can still vary and strongly affect the overall simulation dynamics gårdmark et al 2013 möllmann et al 2014 muelder and filatova 2018 another aspect was treated similarly in all models but is an important part of the second kind of context the causal claims depend on spatial heterogeneity was rarely explicitly represented cf 3 3 system conceptualizations populations of fish and other species in the baltic sea are not homogeneously distributed and are highly mobile and environmental factors and also fishing vary in space and time obviously this heterogeneity has high potential for affecting the causal relationships between these entities and the resulting overall model output e g fulton et al 2004 travers et al 2007 for example varying spatial overlap between species both horizontal and or vertical e g köster et al 2005 should lead to variation in the strengths of their interactions moreover spatial variation in environmental factors means spatial variation in their impact on species and this in turn may change their overall impact things get more complex as such causal relationships modified by spatial heterogeneity are often interdependent one example are areas with low oxygen concentration which do not only hamper cod egg development but also reduce the productivity of benthic species and thus the food availability for cod however benthic organisms can also tolerate low oxygen concentrations better than cod and therefore partly escape the cod predators in these areas this reduction in spatial overlap additionally reduces the availability of benthic organisms as a resource for cod and modifies their interaction casini et al 2016 while in principle spatially explicit models could better account for the additional complexities that emerge from spatial heterogeneity filatova et al 2013 there are also reasons for the non spatial approach favored by most of the modeling studies spatially explicit models of baltic cod dynamics are certainly much more complicated to develop and analyze studies that explored taking spatial heterogeneity explicitly into account used ices subdivisions or the main cod spawning areas as spatial units margonski et al 2010 casini et al 2016 these rather large areas might not be an adequate resolution for capturing the spatial variation in local interactions between environmental factors fish stocks and fishers further difficult questions would remain such as how movement of entities should be correctly represented in spatially explicit models here scarcity of data for assessing fish movement has been identified as a major hindrance of spatially explicit models plagányi et al 2014 nonetheless addressing the complexity of spatial heterogeneity in the baltic sea at smaller scales is possible with models and may become more frequent recent examples include a spatially explicit version of the ewe model with a grid cell side length of 0 25 bauer et al 2018 and an individual based baltic cod model with time varying vertical layers of the represented water volume pierce et al 2017 4 5 from ecological to social ecological models modeling cultures within a discipline such as ecology can tend to simplify or ignore important components of studied phenomena because these components include entities and processes belonging to other scientific domains ecological dynamics in the baltic sea are not isolated but connected to human action especially through fishing fishers were considered as an exogenous driver in most of the models cf 3 3 system conceptualizations however fishery comprises social and economic processes individual collective and political decisions which are causally related to each other and to ecological factors in various ways e g österblom et al 2007 nielsen et al 2018 for taking these manifold interactions into account just linking two independent models one social and one ecological would be insufficient rather they should be represented in one combined social ecological system including the various strongly intertwined feedbacks between social and ecological processes folke et al 2016 preiser et al 2021 this shift from ecological to social ecological models will often add complexity and make developing models and understanding the causes of modeled phenomena particularly challenging filatova et al 2016 will et al 2021 for instance selecting the entities and relationships to include in the system conceptualization or obtaining and incorporating adequate data is more difficult than for each subsystem separately lacking empirical information has been identified as a reason for not including social or economic processes in fisheries ecosystem models plagányi et al 2014 yet models are an indispensable tool for determining causal relationships also in complex intertwined social ecological systems schulze et al 2017 schlüter et al 2019 the coupled social ecological version of the empirical baltic sea model lade et al 2015 is one example several others have been developed for marine social ecological systems some of them also for the baltic sea plagányi 2007 nielsen et al 2018 in general we think that our observations for ecological models apply to social ecological models too given the complexity and more ambiguous boundaries of the actual systems to be represented a high diversity of models for similar phenomena can be expected as well as a high dependence of causal findings on constraints that follow from various explicit and implicit assumptions in these models 4 6 future directions for using models to understand causes of ecosystem dynamics aside from social ecological modeling we see also other ways for making future models more useful for understanding causal relationships and supporting successful policy and management towards sustainability incorporating essential ecological system features such as spatial heterogeneity or multiple and changing species interactions and investigating their effects on the dynamics of interest is possible in addition comprehensive communication of models and their context is required grimm et al 2014 2020b this should make models more user friendly and the causal claims understandable to persons who have not developed them especially the stakeholders who have to make decisions grimm et al 2020a nielsen et al 2018 will et al 2021 a recommendable option to decrease the uncertainty in causal explanations is to apply ensembles of different models to the same system and questions e g jones and cheung 2015 anderson et al 2017 carlson et al 2018 however this needs to be taken with care since different models have rarely been developed to address exactly the same question table 1 given the models substantial differences at various levels modifying them to serve as consistent members of an ensemble will often be very laborious perhaps almost impossible without involving experts for each model included gårdmark et al 2013 bauer et al 2019 a suggested solution is to use a statistical framework to combine information from ecological models that differ in system conceptualization further assumptions and the kind of model output they provide spence et al 2018 still different models contributing to outcomes of ensemble analyses may show substantial variation in their results or may yield similar results but via different underlying mechanisms due to model equifinality both can be problematic for deriving precise but generally valid causal claims therefore another recent recommendation is to complement the use of multiple models with a formal and structured discussion among the modelers shea et al 2020 this shall elicit the models similarities and differences allow assessment why models disagree and thus enhance the causal insights on the dynamics addressed another strategy dubbed pattern oriented modeling grimm 2005 grimm and railsback 2012 could be used for scrutinizing causal explanations and also predictions derived from models the strategy is to confront model results with observed patterns ideally characterizing the system at different scales and organizational levels while it can be relatively easy to make a model reproduce a single observed pattern for example cyclic population dynamics or certain ranges of abundance reproducing multiple patterns simultaneously is more challenging each pattern serves as a filter to reject unrealistic parameter values or functional relationships regarding ensembles the different models could be ranked by their ability to reproduce multiple patterns or even predict new patterns not used for model development and calibration for example heine et al 2005 used this approach to evaluate and rank the realism of different economic models representing the same phenomenon so far it has to our knowledge not been tried to systematically relate the factors and processes represented in models to their potential of reproducing patterns but this might be a promising quantitative complement to the structured discourse on multiple models shea et al 2020 cf above the outcome could be an integrated model that incorporates all relevant factors also those that become relevant only under certain circumstances or an evaluation under which conditions each of the models evaluated is considered relevant enough for its purpose if at all 5 conclusion models are indeed an indispensable tool for exploring causal relationships in complex systems however the causal findings coming out of modeling studies are to a considerable degree determined or at least constrained by what has been put in the models and how they were analyzed this means that in addition to the explicit scope of causal claims various other aspects belong to the context of model based causal explanations this context is not always entirely clear but it is a strong source for heterogeneity in causal claims derived especially with regard to diverse specific statements on similar subjects made in different modeling studies the multi faceted context dependence needs to be considered when generalizing findings on the causes and potential manipulations of ecosystem dynamics as a basis for policy and management decisions that enhance sustainability software availability not applicable declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the swedish research council grant no 2018 06139 
25550,models are widely used for investigating cause effect relationships in complex systems however often different models yield diverging causal claims about specific phenomena therefore critical reflection is needed on causal insights derived from modeling as an example we here compare ecological models dealing with the dynamics and collapse of cod in the baltic sea the models addressed different specific questions but also vary widely in system conceptualization and complexity with each model certain ecological factors and mechanisms were analyzed in detail while others were included but remained unchanged or were excluded model based causal analyses of the same system are thus inherently constrained by diverse implicit assumptions about possible determinants of causation in developing recommendations for human action awareness is needed of this strong context dependence of causal claims which is often not entirely clear model comparisons can be supplemented by integrating findings from multiple models and confronting models with multiple observed patterns keywords causation ecological models social ecological systems context dependence model comparison data availability no data was used for the research described in the article 1 introduction ecosystem functions and services are at risk because of overexploitation habitat and biodiversity loss pollution and in general unsustainable management transformation to sustainability is therefore mandatory as summarized in the un sustainable development goals messerli et al 2019 however due to the complexity of many ecosystems or social ecological systems clear cause effect relationships are hard to identify often we do not know the causes of a certain feature or behavior of a system and usually we cannot fully foresee the effects of certain interventions e g policy management harvesting be they desired or undesired effects however without understanding at least the most important causes underlying ecosystem dynamics successful transformation to sustainability will be hard if not impossible traditionally observation and experiments are used to discover causal relationships if sufficient data are available correlations can be identified and suggest certain causal pathways which then can be tested in targeted experiments with many ecological and especially social ecological systems though data never can be comprehensive as those systems are too complex and large likewise experiments under controlled conditions are not possible modeling is therefore widely used to study such systems assuming that a model despite its simplifications reflects reality sufficiently well for its purpose models seem to be an ideal tool for understanding causation they can be complex and cover large scales one can collect various virtual data of the modeled system and perform any kind of controlled experiments to identify causes still modeling of complex systems has its own challenges whether or not a model is realistic enough for its purpose often remains uncertain also because usually there are not enough actual data to reduce uncertainty moreover even for realistic and robust models fully understanding cause effect relationships is hard to achieve for the same reasons that real systems are hard to understand the large number of direct and indirect effects nonlinearities delayed effects complex and nested networks of interactions and the heterogeneity of the actors and factors involved in system dynamics nevertheless inferences are made from models but often the justifications and constraints of claimed cause effect relationships are implicit and therefore not transparent and clear most puzzling though is that different models addressing the same system and similar questions often vary widely in their structure the way they are analyzed and hence the respective conclusions this variation is partly inevitable as scientific explanations always depend on presuppositions of those seeking explanations e g van fraassen 1980 and it also has benefits as it means to look at the same problem from different angles however the diversity of models complicates establishing a generally accepted level of understanding of cause effect relationships that could support successful interventions and transformation while numerous reviews of models of certain ecological or social ecological systems exist so far there seems to be no attempt to review models with a focus on their causal claims and how those might be constrained not only explicitly as stated by their developers but also implicitly by decisions and presuppositions underlying the models design we therefore reviewed ecological models addressing the collapse of the baltic cod or its general population dynamics with a focus on collapse or recovery we chose this subject mainly because the number of such models is limited so that we had the chance to zoom into details of the causal claims but also because understanding collapses in general and of the baltic cod in particular is of high general interest cod is ecologically important as a top predator in a food web including its main prey species sprat and herring as well as benthic zooplankton and phytoplankton species this means that changes in cod abundance can cause and be caused by considerable changes in the abundance of other species in the food web moreover the species populations respond to environmental conditions such as water temperature salinity and oxygen content e g lindegren et al 2009 from an anthropocentric perspective cod is economically important as a fishing resource in the baltic sea cod used to be harvested in large amounts especially during periods of consistently high abundance throughout the 20th century e g österblom et al 2007 starting in the 1980s however the cod populations severely declined e g casini et al 2008 möllmann et al 2009 and have so far not recovered to consistently high abundance the baltic cod consist of two main populations the eastern and the western baltic cod stocks while there have been slight indications of recovery for both stocks in the recent decade eero et al 2012a 2014 hüssy et al 2016 sguotti et al 2019 both stocks still have critically low abundances today ices 2021a 2021b additionally the baltic cod average body size has decreased which is considered a further hindrance to recovery eero et al 2019 niiranen et al 2019 we found 15 relevant models and analyzed their corresponding publications as to which causal explanations of baltic cod dynamics were derived after inspecting the studies with respect to criteria such as the addressed research questions the model types system conceptualizations and additional assumptions we focused on the question which causal claims were made how were the claims derived and justified and how were they shaped and constrained by the model assumptions we show that multiple explicit and implicit choices are part of the context of causal claims derived from ecological models which is crucial for fully comprehending the meaning and scope of these claims 2 methods we selected 15 ecological models from 19 published studies table 1 to represent how baltic cod dynamics have been studied with a particular focus on ecological aspects involved all these models were used for studying causes of baltic cod dynamics they either represented a period including the collapse in the late 1980s and early 1990s or they were used to generally investigate the possibilities of cod collapse or recovery we did not take into account studies in which the modeling primarily focused not on the causes but on the consequences of the baltic cod collapse e g casini et al 2008 2009 to analyze how the ecological models were used to develop causal knowledge about cod dynamics we assessed the following criteria 1 which main research questions were addressed we distinguish between questions aiming at a certain degree of explanatory understanding kuorikoski and ylikoski 2015 and questions aiming at predictions this criterion is of utmost importance because models do not represent systems per se but systems with respect to a specific question understanding evaluating and comparing models thus requires awareness of the specific questions and the overall purposes of the models grimm et al 2020a 2 which model type was used various categorizations of ecological models exist e g hilborn and mangel 1997 otto and day 2007 evans et al 2013 deangelis and grimm 2014 we distinguished between statistical models and dynamic simulation models as the boundaries between these types are blurred e g dormann et al 2012 oberpriller et al 2021 we interpreted them broadly for instance we considered an artificial neural network belonging to the statistical models and any model representing system dynamics through state variables and processes that change them as dynamic simulation models 3 how was the social ecological system conceptualized in the models this comprises which entities and which relationships between them were represented in a model and how and whether entities were exogenous i e only affecting other entities or endogenous i e also being affected by other entities 4 which additional assumptions were made to define the model and how were they justified 5 which data were incorporated and how 6 which answers to the research questions were given we reviewed the causal claims that were made and differences among claims on similar subjects from the different modeling studies in particular we examined how the models were used to derive and support these causal claims we stress that all our assessments of the model characteristics and analyses were done to the best of our understanding of the publications but may be not fully precise and correct in all details unambiguously obtaining the specific information for a review of for example fish and fisheries models is not always possible cf nielsen et al 2018 3 results 3 1 research questions we found that the causal analyses based on the models always met at least one of the general research purposes of explanatory understanding or prediction cf 2 methods the identified key research questions related to some measure of cod dynamics one to three questions per study broadly fall in the following three categories of causal inquiry c causes which factors affect cod dynamics m mechanisms which mechanisms drive cod dynamics p predictions which consequences would certain environmental or management scenarios have for cod dynamics categories c and m aim at explanatory understanding but the studies varied in terms of what exactly was intended to be explained cf kuorikoski and ylikoski 2015 in category m not only the causes but also the underlying mechanisms shall be uncovered we understand a mechanism as a causal explanation that explicitly often stepwise links cause s and effect s mechanisms comprise processes that involve certain entities and that together bring about the phenomena of interest cf e g hedström and ylikoski 2010 cabral et al 2017 connolly et al 2017 heger 2022 for alternatives and discussions on the definition of mechanisms in ecology and sociology category p aims at prediction of the effects of environmental change or management decisions we rephrased the research questions to keep them brief and facilitate comparison see table 1 for all questions and their categorization for the 19 studies reviewed we found 11 posing a research question of category c causes 9 posing a question of category m mechanisms and 9 posing a question of category p predictions some questions had overlaps so they were assigned to two categories table 1 in several cases mechanisms were asked for and also provided in the studies but mainly through interpretation of the causal factors identified with the models rather than through dynamic simulations table 1 see also 3 6 causal claims table 4 it also occurred that one model was used to address several questions of different categories either in one or in different studies for example the model gadget was developed and used to detect causes of cod diet dynamics by kulatska et al 2019 and used to predict cod fishery profits under various scenarios by bauer et al 2019 although the question categories are general the particular research questions in each study often focused on specific elements of the ecological system for example which factors drive baltic cod recruitment category c krekoukiotis et al 2016 which mechanisms explain the lack of recovery of top predator cod populations when including an intermediate life stage feeding on benthos category m van leeuwen et al 2013 which eastern baltic cod exploitation level can rebuild the cod population and sustain future fishery category p jonzén et al 2002 these clear foci of analyses do not reveal which additional elements of the complex ecosystem were included in the models and if so how however such system conceptualizations provided decisive context for answering the questions and may have considerably influenced the research findings we therefore examined which entities were taken into account and linked in each of the models cf 3 3 system conceptualizations 3 2 model types given our broad categorization cf 2 methods most models could be assigned to either statistical or dynamic simulation models table 1 the model balmar lindegren et al 2009 2010 was assigned to both types since a statistical multivariate autoregressive model was fitted to time series data and the obtained food web model was then used for stochastic simulations of cod population dynamics under different scenarios all statistical models were used to answer a research question of category c causes table 1 if they were used to address additional questions of another category this happened through discussion and mechanistic interpretation of the statistical model results by contrast the dynamical models were used to answer questions of all three categories c m and p table 1 the multispecies virtual population analysis msvpa e g sparholt 1991 1994 is a very prominent dynamic model used as a standard by the international council for the exploration of the sea ices this model provides time series of interacting baltic fish stocks separately for the western ices subdivisions 22 24 and eastern ices subdivisions 25 32 baltic sea it is established to use these msvpa output time series like input data in other models as it was done in several of the reviewed studies cf 3 5 data use 3 3 system conceptualizations we observed that a large number of different species several environmental factors and fishing were taken into account in the 15 different models fig 1 this led to a huge variety of conceptualizations of the ecological or social ecological system among the models even more so as not only the entities considered vary but also which of them were related and how fig 2 in addition different sets of variables have been used to characterize the entities on top of these differences relationships between the same entities and in the same direction have been defined and parameterized differently in the different models while the cod population was included in all 15 models the models differ as to which characteristic of the cod population was the outcome of interest that is the measure of the cod dynamics for which causes should be investigated fig 3 for instance cod recruitment could be the outcome of interest and thus to be explained by a model which then also included cod abundance as one explanatory variable e g baltic cod gam möllmann et al 2008 fishann krekoukiotis et al 2016 but in other models cod abundance was to be explained and a measure of cod recruitment used as an explanatory variable instead e g msvpa sparholt 1994 mspm horbowy 2005 the cod s main prey species sprat and herring were considered in several but not all models 10 models for sprat 8 for herring in several models the fish populations were divided into age groups e g chs model heikinheimo 2011 size groups e g gadget bauer et al 2019 kulatska et al 2019 or life stage groups e g mspm horbowy 2005 also here it became obvious that even when considering the same entities models may have incorporated different state variables and different relationships between them one typical example relationship is sprat predation by cod in some models its effect on the sprat population was considered but no effect on the cod population e g chs model heikinheimo 2011 other modelers did the opposite taking into account the effect of the sprat population on the cod population but not vice versa e g margonski et al 2010 in their stock recruitment model in yet other models both directions of this causal relationship were included e g empirical baltic sea model lade et al 2015 additional biotic populations such as zooplankton zoobenthos and other species were generally considered in fewer models fig 1 for specifying species interactions in dynamic simulation models the parameter values for functional relationships representing interactions were often determined by fitting model output to data e g sparholt 1991 tomczak et al 2012 thereby invariable functional relationships were commonly assumed however interactions do vary in reality for example when abundances of preferred or non preferred prey species vary niiranen et al 2019 or in response to varying abiotic conditions köster et al 2005 hence more complex functional relationships reflecting these variations would be desirable for realistically representing actual causal relationships between species sub populations correctly defining and parametrizing such functional relationships remains a challenge plagányi et al 2014 peck et al 2018 generally unavailable or incomplete data on relevant species precluded taking into account complex species interactions in some of the models e g mentioned by möllmann et al 2008 tomczak et al 2012 wikström et al 2016 the baltic sea environment was conceptualized in the models through including various abiotic factors cod reproductive volume rv was the most common abiotic factor included in 7 models rv is a standard measure of the suitable water volume for cod egg development defined as the water volume with a suitable combination of salinity 11 psμ and oxygen 2 ml 1 conditions mackenzie et al 2000 other factors such as salinity or oxygen alone temperature or climatic indices were included in several models but less often fig 1 the environmental factors were generally considered as exogenous entities without feedback in the models they had an effect on but were not affected by other entities here an exception is detritus in the ecopath with ecosim ewe baltproweb model where this dead organic material was affected by zooplankton and zoobenthos populations niiranen et al 2012 tomczak et al 2012 in most but not all models 12 out of 15 was fishing explicitly considered the reasons for the three exceptions are as follows the baltic cod gam by casini et al 2016 aimed at explaining cod condition the stock recruitment model by margonski et al 2010 at explaining cod recruitment in both models cod population abundance was used as one explanatory variable cf fig 3 which obviously was affected by fishing and thus fishing was implicitly taken into account the baltic sea cod sprat model van leeuwen et al 2013 was used to mechanistically study the effects of cod prey switching on cod abundance cf fig 3 and on food web dynamics in general thus this model had a purely ecological focus disregarding fishing for the purpose of this analysis we have taken explicit inclusion of fishing in the models as fishers or other fishing related actors being conceptualized as an entity fig 1 this was usually an exogenous entity affecting ecological dynamics without feedback apart from sometimes monitoring catch profit as an output variable fishing was applied to cod and sometimes also to sprat and herring one model gadget had separate fishing fleets but this was not much discussed in the respective studies bauer et al 2019 kulatska et al 2019 balmar had a simple bioeconomic submodel with different incl one adaptive management strategies lindegren et al 2009 the empirical baltic sea model had a sophisticated social submodel and also other social ecological relationships than fishers affecting fish populations namely perceptions and regulations depending on cod abundance and catches lade et al 2015 however we included it in our selection of ecological models because lade et al 2015 also analyzed an ecological model version in which the ecological subsystem was decoupled from social processes i e with constant fishing effort with respect to how space and spatial heterogeneity were conceptualized it was common in the modeling studies to assume that certain areas or locations can be regarded as representative or most important for environmental effects on cod dynamics that mobile species are generally able to follow favorable conditions or prey species that fishers follow the target species or that species distributions and spatial overlap do not change and can therefore be captured by modeling invariant interactions of whole populations however it was also commonly mentioned that such assumptions were a potential limitation for correctly identifying complex causal relationships to name just a few examples tomczak et al 2012 wrote ewe models have a problem with highly migratory species or stocks patchiness christensen and walters 2004 in our case herring and sprat according to one hypothesis sprat aggregations could migrate stepputtis 2006 dependent on the cod biomass or temperature at a certain habitat also herring shows high intra species differences local spawning groups and spawning migration to coastal areas and it is difficult to apply a single environmental factor that determines herring recruitment krekoukiotis et al 2016 wrote we assumed for our data that mechanisms across baltic subdivisions are roughly the same since the integrated abundance of all spawning areas should be unaffected by transport between spawning areas köster et al 2005 we kept this spatial homogeneity within the central baltic similar to studies by sparholt 1996 and jarre teichmann et al 2000 we can t be sure to what extent this decision affects the interpretations of complex non linear relationships for recruitment kulatska et al 2019 wrote negative switching is when the proportion of a prey type in a predator s diet decreases slower than its proportion in total prey abundance this has been suggested to cause destabilization of prey populations as the impact of a predator on a prey with decreasing abundance is over proportionally high chesson 1984 however this phenomenon can also be observed as an artifact of variable predator prey spatial overlap neuenfeldt and beyer 2006 which was not accounted for in this study regarding the alternative option of spatially explicit models a critical issue would be the use of appropriate data to define and fit a model cf 3 5 data use some of the reviewed studies show that spatially resolved data e g at the resolution of ices subdivisions would be available for several environmental factors and fish populations margonski et al 2010 casini et al 2016 see also casini et al 2011 the two respective models did not explicitly include fishing cf figs 1 2 however also fishing data can be obtained at the resolution of ices subdivisions eero et al 2012b hence it seems that data availability was not the major limitation preventing spatially explicit modeling at least at a low resolution casini et al 2016 did initially account for spatial heterogeneity at the resolution of ices subdivisions 25 28 but then decided to use averages over this whole area of the central baltic sea instead because they did not observe significant variation among the subdivisions the study by margonski et al 2010 is an example where an environmental factor cod rv from different spatial locations was initially taken into account bornholm basin gdańsk basin gotland basin the three main cod spawning areas in the central baltic sea eventually one best fitting location was selected for the final statistical model 3 4 further model assumptions the system conceptualizations include the tacit assumption that the entities and relationships considered in each model are the most relevant for the specific aspect of cod dynamics to be modeled that is for answering the research questions cf table 1 similarly the selected model type and the particular definition of relationships e g functional forms parameter values include the tacit assumption that these choices are the most appropriate to adequately represent the actual ecological dynamics in addition a variety of specific assumptions have been made and stated in the studies presenting the models we commonly observed that model assumptions were explicitly based on previously published literature for example candidate causal variables of the cod dynamics in statistical models were often selected defined and justified in this way for example we used 1 ml l as threshold for hypoxia because the baltic cod has been shown to avoid oxygen concentrations below this value 46 casini et al 2016 we consider habitat driven variables proposed by köster et al 2005 that potentially explain changes in recruitment regimes krekoukiotis et al 2016 also assumptions on specific forms of causal relationships in dynamic simulation models were obtained from earlier studies for instance the cod stock recruitment equation in the chs model heikinheimo 2011 was taken from heikinheimo 2008 the threshold cod lengths for ontogenetic shifts in the baltic sea cod sprat model van leeuwen et al 2013 were obtained from hüssy et al 1997 the assumption of multiple age groups to represent ontogenetic shifts in ewe baltproweb tomczak et al 2012 was made according to another ewe version for a different ecosystem walters et al 2008 we also found model assumptions based on prior knowledge that were not directly related to literature for example in the 1980s and 1990s a marked decline in the growth rate of baltic herring was observed this phenomenon was simulated in the model by presenting the anabolism coefficient h as a function of time linearly decreasing until 1998 mspm horbowy 2005 such assumptions were apparently justified through presupposing sufficient expertise in other cases expert knowledge was explicitly referred to for example a team of baltic sea experts from the natural and social sciences collaboratively developed a conceptual model of the key ecological and social quantities and processes that contributed to cod stock dynamics empirical baltic sea model lade et al 2015 other assumptions were not made to fully reflect the knowledge of the complex ecosystem but to keep the model simple this simplification could occur implicitly through stating assumptions that obviously simplify the actual ecological setting such as the model is based on the assumptions that the distribution of predators and prey is spatially uniform predator and prey populations are each made up of identical individuals with identical life history parameters and the growth and reproductive rates of cod do not depend on consumption rates chs model heikinheimo 2011 or it was made explicit like we have made fewer assumptions and used a simple model structure and this should enhance the robustness of our conclusions single species model jonzén et al 2002 we also found simplifying assumptions that were based on the model s lack of sensitivity to changes in these assumptions for the chs model sensitivity analysis did not indicate any marked effect of an age based preference coefficient and for simplicity this was not taken into account heikinheimo 2011 for ewe baltproweb of many vulnerability parameters that define predator prey interactions only those that the modeled dynamics were known to be most sensitive to were included in the calibration to data while a default value was assumed for others niiranen et al 2012 tomczak et al 2012 available data have often influenced and constrained model assumptions this was explicitly stated in many studies for example all in all deep water salinity data was not included due to lack of sufficient availability of the data set fishann krekoukiotis et al 2016 due to available data we based our model on the swedish cod fishery empirical baltic sea model lade et al 2015 the human impact was indicated by using one fishing fleet for each fish species due to the lack of historical fleet structure and effort data ewe baltproweb tomczak et al 2012 the production models presented here are developed in such a way that the demand for age based data is reduced to a minimum mspm horbowy 2005 because of limitations in the availability of the best data we used time series covering the period 1974 2005 in our analyses baltic cod gam möllmann et al 2008 3 5 data use almost all reviewed modeling studies referred to time series data of some of the variables representing biotic or abiotic entities or fishing cf figs 1 2 there was also considerable overlap in the actual datasets used among the studies cf gårdmark et al 2013 these observational data were used directly to define the temporal dynamics of exogenous entities or indirectly to fit modeled time series to the data common biotic time series data were the abundance or biomass of cod sprat herring and other species populations sometimes these time series were stratified by age particularly for cod additional attributes e g length weight age life history events e g mortalities recruitment or diet compositions stomach contents were used too typical fishing data were time series of fishing mortality or catch amounts sometimes including age stratification e g msvpa sparholt 1991 1994 in some studies the fish and or fishing time series data used as model input were output data produced with the msvpa model e g horbowy 2005 möllmann et al 2008 heikinheimo 2011 such msvpa output data were obtained from reports of different baltic sea working groups of the ices abiotic time series data used in the models either covered single environmental factors e g salinity oxygen water temperature or specific measures obtained from these factors e g cod rv combining oxygen and salinity cf 3 3 system conceptualizations baltic sea index winter north atlantic index we also found cases where abiotic or biotic data used represent only specific times e g cod rv in august fishann krekoukiotis et al 2016 or specific locations e g multiple locations in ices subdivisions 25 32 which were then combined to represent the whole baltic sea as a single area kulatska et al 2019 or specific times and locations in the study area e g cod rv in may in the gotland basin stock recruitment model margonski et al 2010 summer temperature at 0 10m depth in the bornholm basin spring abundance of zooplankton species in the gotland basin balmar lindegren et al 2009 such specific constraints and decisions on data were presumably based on prior knowledge which was sometimes directly stated for instance in ewe baltproweb tomczak et al 2012 sea surface temperature at 0 10 m depth in august was used because it had been found to be clearly and significantly related with sprat recruitment in published studies in several models abiotic time series data were used with a specific delay lagged time series as input data this was done for example to take into account that once recruitment began under certain abiotic conditions the cod had to reach a certain age which was considered to be decisive e g balmar lindegren et al 2009 fishann krekoukiotis et al 2016 beyond time series examples of additional quantitative or qualitative data used were information on species characteristics such as on their diet e g food selection consumption rates baltic cod gam möllmann et al 2008 development e g maturation time empirical baltic sea model lade et al 2015 diet shifts throughout the life history baltic sea cod sprat model van leeuwen et al 2013 or mortality e g natural and fishing mortality chs model heikinheimo 2011 these data were typically taken from previously published studies for the empirical baltic sea model lade et al 2015 also used data on the social subsystem such as on fishing fleet composition cod supply from outside the study region or subsidy policy these data were obtained from literature or from expert knowledge fisher interviews 3 6 causal claims the models were used to derive and support a huge variety of explicit claims on the causes of baltic cod dynamics this variety is partly due to the heterogeneity among the studies different specific research questions cf table 1 we therefore identified and grouped the causal claims according to the same three categories of causal inquiry as the research questions c causes of cod dynamics table 3 m mechanisms driving cod dynamics table 4 and p predictions of cod dynamics table 5 in addition to claims that directly addressed the main research questions cf table 1 we found many more causal claims that fit into the same categories and added several examples to the overviews tables 3 5 these claims did not always relate to the main focus of the respective studies but they were based on the models presented and illustrate the overall diversity of the types of causal claims by and large the strategies for deriving causal claims were very diverse and hard to categorize table 2 inspecting the causal claims reveals that summarizing them and formulating claims that are supported by several modeling studies is only possible at a very broad level like unfavorable conditions and fishing pressure caused the baltic cod collapse however whether really both these factors caused the collapse and if so which was more important is already much more ambiguous some of the claims even seem conflicting making the formulation of generalizations challenging examining the claims more specifically shows that they are often highly context dependent this was partly acknowledged together with the claims directly more important a lot of context dependence stemmed from the respective choices of model type system conceptualization and further assumptions cf above and how the model was analyzed we illustrate this high degree of context dependence by three example topics one from each category of causal inquiry c m and p cf above on which we found various causal claims in the modeling studies 3 6 1 c causes fishing and or environmental conditions as causes of cod population dynamics several of the causal claims of category c identified either fishing pressure f or environmental conditions e or both as main causes of cod population dynamics in particular too high fishing pressure and or periods of unfavorable environmental conditions and sometimes specifically their interplay were claimed as causes of the cod collapse and remaining low abundance table 3 of the claims that include both causes f and e some explicitly ranked one cause as more important f e or e f table 3 one broadly shared causal finding among the studies is that when e were claimed as a main cause of cod population dynamics the specific environmental conditions considered in the model did include water salinity either salinity directly or via the cod rv which combines salinity and oxygen cf above by contrast several other claims did not explicitly mention e however some of those referred to recruitment r as another important cause of cod dynamics of course this r can depend on e and thus mediate effects of e this was explicitly mentioned in one causal claim cf table 3 but also when these potential causal relationships from e to r were not mentioned perhaps also not considered in the model they may still have been present in the recruitment data and affected the cod dynamics thus the modelers selected focus for formulating a claim as well as for designing a model strongly shaped the causal findings apparent differences get less conflicting upon closer examination and the causal claims should be considered in parallel rather than in opposition to each other however this makes it a big challenge to adequately summarize them into specific statements except at a very general level 3 6 2 m mechanisms the cultivation depensation hypothesis this example topic concerns mechanistic explanations for why in many marine ecosystems such as the baltic sea either large predatory fish species such as cod or their prey species such as sprat tend to be highly abundant but not both species together the hypothesis is that there are two stable states e g cod dominance or sprat dominance and each state reinforces itself either by cultivation or depensation walters and kitchell 2001 the cultivation mechanism is supposed to operate when cod is abundant many adult cods that feed on sprats keep sprat abundance low thus the cod adults reinforce favorable survival conditions for cod juveniles because sprats have a negative effect on cod juveniles survival sprats compete with cod larvae for zooplankton mainly pseudocalanus acuspes and sprats feed on cod eggs walters and kitchell 2001 möllmann et al 2009 by contrast the depensation mechanism is supposed to operate when cod abundance is low then sprats experience much less predation pressure and can achieve high abundance it follows that their negative effects on cod juveniles survival increase and thus reinforce unfavorable cod recruitment conditions the latter means low reproductive success due to reduced fitness at low density for cod also known as allee effect allee et al 1949 stephens et al 1999 roos and persson 2002 as a result from the mechanisms of cultivation and depensation cod and sprat could not simultaneously be highly abundant and a shift from high to low cod abundance irrespective of what has caused it would mean that a new and self reinforcing state of sprat dominance could establish and prevent the shift back to cod dominance therefore these mechanisms have been suggested as causal explanations of the 1980s baltic cod collapse and poor recovery e g möllmann et al 2008 casini et al 2009 while the suggested mechanisms are intuitive and appealing there are several complications for confirming or rejecting that they underlie cod dynamics in the baltic sea assuming that they are true only one of the two mechanisms would operate at a time either cultivation for cod dominance or depensation for sprat dominance the spatial areas occupied by both species can differ and their spatial geographical and vertical overlap vary the depensation mechanism with its negative effect of sprat on cod juveniles contains two pathways predation as well as competition for food depensation of cod can be difficult to attribute to high sprat abundance or unfavorable environmental conditions or fishing cf claims on fishing and or environmental conditions as causes of cod population dynamics above or interactions of these potential causes if they were simultaneously present also herring the second important prey species for cod feeds on cod eggs and zooplankton e g köster and möllmann 2000 since all three species are subject to fishing this certainly had considerable effects on their abundance apart from cultivation and depensation effects finally showing that model results are in line with the cultivation depensation hypothesis cdh does not necessarily show that the mechanisms operate the modeling studies we analyzed include diverse findings on the validity and importance of the cdh particularly of the depensation mechanism for the baltic cod collapse or poor recovery among the mechanistic claims on cod dynamics we found relations to the cdh in five studies three using dynamic models and two using statistical models table 4 for the latter the claims were located in the discussion section the statistically detected findings were mechanistically interpreted as confirming that the depensation mechanism operates möllmann et al 2008 or not margonski et al 2010 cf table 4 using a dynamic model van leeuwen et al 2013 explicitly focused on whether alternative stable states of either cod or sprat dominance in the baltic sea emerge when accounting for realistic diet shifts throughout cod life history the model included an intermediate cod life stage during which cods exclusively consume benthic organisms and the feedback effect that consumption has on the benthic organisms availability they showed that this causes cod growth to the adult stage being limited by intraspecific resource competition in this intermediate life stage instead of by interspecific competition with sprats in the juvenile life stage i e not limited by depensation section claims on cod recruitment dynamics in table 4 the model did not include cod egg predation by sprats strict criteria were applied for testing bistability under equal conditions differences between the persistence and invasion boundaries indicate a region of bistability where persistence is possible for combinations of resource productivity that do not allow for invasion van leeuwen et al 2013 hence model analyses led to claims that the complex yet purely ecological dynamics without fishing fig 2 were not in full agreement with the cdh section claims on baltic ecosystem dynamics stability in table 4 in the empirical baltic sea model by lade et al 2015 an intermediate cod life stage was included too both the intermediate and the adult cod life stage feed on benthic organisms but not exclusively as both feed on sprat and herring as well thus the food web interactions of both life stages were fundamentally the same in this generalized model a dynamical systems model in which processes are represented only with abstract placeholder functions lade et al 2015 herring several environmental factors and fishers were included in the full version with a complex social submodel but also in the decoupled ecological version cf 3 3 system conceptualizations fig 2 a specific feedback loop analysis was applied to detect main causes of cod collapse the identified destabilizing feedback loops were found to be in line with the depensation mechanism table 4 claims for the decoupled ecological and for the full model version for ewe baltproweb after fitting the dynamic model to empirical time series tomczak et al 2012 investigated how much the different interactions in the complex food web were realized in the actual simulations and over time quantifying energy flows through pathways in the food web the model included many species environmental factors and fishing fig 2 pathways that would represent depensation were found to be not intensely used from zooplankton to cod larvae from cod larvae to sprats cod larvae used as a proxy for cod eggs which were not modeled separately this led to a claim suggesting that depensation was not important for sprat dominance table 4 whether the low flow of energy from zooplankton to cod larvae could be an effect of the competition with sprat for this resource was not explicitly discussed it was mentioned that food web interactions with benthic organisms and herring could also play a significant role for shifts between different ecosystem states tomczak et al 2012 however in line with not claiming that the cdh prevents a shift back to high cod abundance in one of several other studies applying the ewe model to the baltic sea österblom et al 2007 found no indication that sprat and herring predation of cod eggs and larvae and competition for zooplankton are the mechanisms behind poor cod recovery noteworthy the model analyses and claims by lade et al 2015 and tomczak et al 2012 refer to empirical sets of population dynamics environmental conditions and fishing for periods including the baltic cod collapse the claims relate to the depensation mechanism with these empirical approaches it was not tested whether alternative counterfactual system states e g cod instead of sprat dominance for selected time points and under exactly the same conditions would have been possible such strict testing of bistability was done with the more theoretical and purely ecological model by van leeuwen et al 2013 hence the considerable differences with regard to system conceptualizations fig 2 model assumptions and analyses can partly explain the apparently controversial claims regarding the cdh that were derived with the three dynamic models table 4 similar explanations likely apply to statements in additional modeling studies that relate to the cdh but not directly to causes of cod dynamics and were therefore not included in table 4 e g the simulation results indicate that predation by cod is able to control the abundance of sprat and depress the stock to a low level in line with the cultivation mechanism heikinheimo 2011 3 6 3 p predictions cod recovery the predictive claims on cod recovery were all derived from dynamic models except for the baltic cod gam by möllmann et al 2008 in this study the causal claim was formulated in the discussion of statistical model results the claims show considerable variation and some appear controversial section claims on cod population dynamics recovery in table 5 this can partly be explained by different time frames and criteria for recovery moreover the simulated scenarios varied induced by the different research questions addressed table 1 thus in each of the studies environmental conditions fishing or both were varied and explicitly denoted in the claims as causes behind the predictions table 5 and the claims were influenced by the underlying system conceptualizations assumptions and methods for analysis as well for these reasons the predictions derived are especially context dependent and should be regarded together with all these influences 4 discussion 4 1 an analysis of causal explanations derived with ecological models models are an established means for studying causation in complex ecological systems yet it is well known not only in ecology that for any system usually a multitude of models exist often leading to diverging explanations about the causes of certain phenomena or dynamics spence et al 2018 shea et al 2020 while many useful reviews of ecological models for certain systems and questions exist they mostly focus on differences in model structure and model output but not specifically on the corresponding causal claims how those claims were derived and justified and how they were constrained by model design and assumptions we thus reviewed models from the perspective of the causal claims made as a case we used models addressing the dynamics of the baltic cod with a focus on the collapse around the late 1980s or the general potential for cod collapse or recovery 4 2 model based causal claims are highly context dependent a variety of models have been applied to find causal explanations for baltic cod dynamics over recent decades including its 1980s collapse comparing these models we found broad agreement on very general causal explanations such as that intense fishing and unfavorable environmental conditions have been causes of the collapse and poor recovery since however regarding more nuanced claims for example on the relative importance or interactions of different causes less agreement can be found tables 3 5 most importantly our review shows that ecological models for the same system were developed and analyzed in quite different ways which may reflect different research questions table 1 data availability but also for example differences in knowledge and opinion of experts krueger et al 2012 consequently the causal claims derived are highly context dependent thus the scope of these claims can be more specific than it seems and certain claims that may appear conflicting should rather be regarded as complementary while most researchers are in principle aware of such context dependence its critical impact on causal insights derived from specific models can easily be underestimated it is important to note that two different kinds of context can be distinguished first there are the explicit circumstances under which a certain causal claim holds once a model is created and specified the interdependencies and the complexity of the virtual system as it is represented by the model often necessitate formulating context dependent causation these are claims of the kind for favorable environmental conditions the cod stock would recover irrespective of fishing pressure but for unfavorable conditions only low fishing pressure would allow recovery the causal relationship from fishing pressure to cod recovery depends on the environmental context similarly when comparing studies different contexts applied in the different models e g different environmental conditions or fishing scenarios need to be considered and can clarify why causal claims diverge such dependence on the context that is explicitly represented in the models is often in the focus of causal explanations which is natural as it corresponds to how we also think about and try to find causal explanations for complex phenomena in the real world 4 3 another kind of context the system conceptualization and further model assumptions second the model based causal findings depend on a lot of additional context which is usually less explicit but at least as important it can be summarized as what goes in a model and how it is analyzed predefine what can come out this second kind of context includes the chosen focal elements of a modeling study for example the dynamics or phenomenon that shall be the model output fig 3 the candidate causes for this output or the processes and relationships that shall be varied to investigate their impact on the output it goes on with all further elements included in the system conceptualization and the relationships between them which we found differ considerably among models addressing similar questions figs 1 2 moreover the context comprises implicit assumptions for example global interactions implying that populations can always meet and interact in the same way or the omission of certain aspects in the system conceptualization implying that these are not considered relevant for causation of the studied phenomenon or perhaps simply not known a key advantage of modeling is that the second kind of context determined by the system conceptualization and additional assumptions need not be fixed but can also be systematically varied to understand its role for causation the sensitivity and robustness of causal findings in response to this context can be thoroughly tested saltelli 2004 thiele et al 2014 grimm and berger 2016 peck et al 2018 drechsler et al 2022 this means analyzing different model versions that exclude or include certain entities follow different assumptions on causal relationships between them or different definitions and parameterizations of processes representing these relationships such analyses can add to the insights gained from showing that a certain version of a dynamic model satisfactorily reproduces the data and inspecting the mechanisms at work in this specific model particularly very complex models aimed to reflect the complexity of the real ecosystem tend to have many degrees of freedom for adjusting them to data this bears the risk that different model versions can reproduce a phenomenon similarly well but the underlying causal relationships and mechanisms that lead to the phenomenon in the model differ strongly equifinality this was shown in a comparison of many versions of the same model ewe baltproweb with similar goodness of fit to the data by niiranen et al 2012 cf table 4 it might apply also to other studies such that alternative equifinal model versions would exist that perform similarly well but differ in terms of the explanations and predictions they provide hence model equifinality may have contributed to the heterogeneity of causal claims that we observed tables 3 5 4 4 key ecosystem features when studying causation with models species interactions and spatial heterogeneity one example belonging to the second kind of context that can considerably affect model results and thus causal findings is how species interactions were represented although the marine food web in the baltic sea comprises comparably few species e g bagge and thurow 1994 österblom et al 2007 a variety of potential interactions are known this required modelers choices about which species and which relationships among them to include in the system conceptualizations and these choices varied widely cf 3 3 system conceptualizations for instance separating species into subpopulations age size or life stage groups with different inter and intraspecific interactions can be decisive for emergent cod dynamics e g van leeuwen et al 2013 in addition even when modeling the same biotic entities and the same interactions between them the actual types and parametrization of functional relationships can still vary and strongly affect the overall simulation dynamics gårdmark et al 2013 möllmann et al 2014 muelder and filatova 2018 another aspect was treated similarly in all models but is an important part of the second kind of context the causal claims depend on spatial heterogeneity was rarely explicitly represented cf 3 3 system conceptualizations populations of fish and other species in the baltic sea are not homogeneously distributed and are highly mobile and environmental factors and also fishing vary in space and time obviously this heterogeneity has high potential for affecting the causal relationships between these entities and the resulting overall model output e g fulton et al 2004 travers et al 2007 for example varying spatial overlap between species both horizontal and or vertical e g köster et al 2005 should lead to variation in the strengths of their interactions moreover spatial variation in environmental factors means spatial variation in their impact on species and this in turn may change their overall impact things get more complex as such causal relationships modified by spatial heterogeneity are often interdependent one example are areas with low oxygen concentration which do not only hamper cod egg development but also reduce the productivity of benthic species and thus the food availability for cod however benthic organisms can also tolerate low oxygen concentrations better than cod and therefore partly escape the cod predators in these areas this reduction in spatial overlap additionally reduces the availability of benthic organisms as a resource for cod and modifies their interaction casini et al 2016 while in principle spatially explicit models could better account for the additional complexities that emerge from spatial heterogeneity filatova et al 2013 there are also reasons for the non spatial approach favored by most of the modeling studies spatially explicit models of baltic cod dynamics are certainly much more complicated to develop and analyze studies that explored taking spatial heterogeneity explicitly into account used ices subdivisions or the main cod spawning areas as spatial units margonski et al 2010 casini et al 2016 these rather large areas might not be an adequate resolution for capturing the spatial variation in local interactions between environmental factors fish stocks and fishers further difficult questions would remain such as how movement of entities should be correctly represented in spatially explicit models here scarcity of data for assessing fish movement has been identified as a major hindrance of spatially explicit models plagányi et al 2014 nonetheless addressing the complexity of spatial heterogeneity in the baltic sea at smaller scales is possible with models and may become more frequent recent examples include a spatially explicit version of the ewe model with a grid cell side length of 0 25 bauer et al 2018 and an individual based baltic cod model with time varying vertical layers of the represented water volume pierce et al 2017 4 5 from ecological to social ecological models modeling cultures within a discipline such as ecology can tend to simplify or ignore important components of studied phenomena because these components include entities and processes belonging to other scientific domains ecological dynamics in the baltic sea are not isolated but connected to human action especially through fishing fishers were considered as an exogenous driver in most of the models cf 3 3 system conceptualizations however fishery comprises social and economic processes individual collective and political decisions which are causally related to each other and to ecological factors in various ways e g österblom et al 2007 nielsen et al 2018 for taking these manifold interactions into account just linking two independent models one social and one ecological would be insufficient rather they should be represented in one combined social ecological system including the various strongly intertwined feedbacks between social and ecological processes folke et al 2016 preiser et al 2021 this shift from ecological to social ecological models will often add complexity and make developing models and understanding the causes of modeled phenomena particularly challenging filatova et al 2016 will et al 2021 for instance selecting the entities and relationships to include in the system conceptualization or obtaining and incorporating adequate data is more difficult than for each subsystem separately lacking empirical information has been identified as a reason for not including social or economic processes in fisheries ecosystem models plagányi et al 2014 yet models are an indispensable tool for determining causal relationships also in complex intertwined social ecological systems schulze et al 2017 schlüter et al 2019 the coupled social ecological version of the empirical baltic sea model lade et al 2015 is one example several others have been developed for marine social ecological systems some of them also for the baltic sea plagányi 2007 nielsen et al 2018 in general we think that our observations for ecological models apply to social ecological models too given the complexity and more ambiguous boundaries of the actual systems to be represented a high diversity of models for similar phenomena can be expected as well as a high dependence of causal findings on constraints that follow from various explicit and implicit assumptions in these models 4 6 future directions for using models to understand causes of ecosystem dynamics aside from social ecological modeling we see also other ways for making future models more useful for understanding causal relationships and supporting successful policy and management towards sustainability incorporating essential ecological system features such as spatial heterogeneity or multiple and changing species interactions and investigating their effects on the dynamics of interest is possible in addition comprehensive communication of models and their context is required grimm et al 2014 2020b this should make models more user friendly and the causal claims understandable to persons who have not developed them especially the stakeholders who have to make decisions grimm et al 2020a nielsen et al 2018 will et al 2021 a recommendable option to decrease the uncertainty in causal explanations is to apply ensembles of different models to the same system and questions e g jones and cheung 2015 anderson et al 2017 carlson et al 2018 however this needs to be taken with care since different models have rarely been developed to address exactly the same question table 1 given the models substantial differences at various levels modifying them to serve as consistent members of an ensemble will often be very laborious perhaps almost impossible without involving experts for each model included gårdmark et al 2013 bauer et al 2019 a suggested solution is to use a statistical framework to combine information from ecological models that differ in system conceptualization further assumptions and the kind of model output they provide spence et al 2018 still different models contributing to outcomes of ensemble analyses may show substantial variation in their results or may yield similar results but via different underlying mechanisms due to model equifinality both can be problematic for deriving precise but generally valid causal claims therefore another recent recommendation is to complement the use of multiple models with a formal and structured discussion among the modelers shea et al 2020 this shall elicit the models similarities and differences allow assessment why models disagree and thus enhance the causal insights on the dynamics addressed another strategy dubbed pattern oriented modeling grimm 2005 grimm and railsback 2012 could be used for scrutinizing causal explanations and also predictions derived from models the strategy is to confront model results with observed patterns ideally characterizing the system at different scales and organizational levels while it can be relatively easy to make a model reproduce a single observed pattern for example cyclic population dynamics or certain ranges of abundance reproducing multiple patterns simultaneously is more challenging each pattern serves as a filter to reject unrealistic parameter values or functional relationships regarding ensembles the different models could be ranked by their ability to reproduce multiple patterns or even predict new patterns not used for model development and calibration for example heine et al 2005 used this approach to evaluate and rank the realism of different economic models representing the same phenomenon so far it has to our knowledge not been tried to systematically relate the factors and processes represented in models to their potential of reproducing patterns but this might be a promising quantitative complement to the structured discourse on multiple models shea et al 2020 cf above the outcome could be an integrated model that incorporates all relevant factors also those that become relevant only under certain circumstances or an evaluation under which conditions each of the models evaluated is considered relevant enough for its purpose if at all 5 conclusion models are indeed an indispensable tool for exploring causal relationships in complex systems however the causal findings coming out of modeling studies are to a considerable degree determined or at least constrained by what has been put in the models and how they were analyzed this means that in addition to the explicit scope of causal claims various other aspects belong to the context of model based causal explanations this context is not always entirely clear but it is a strong source for heterogeneity in causal claims derived especially with regard to diverse specific statements on similar subjects made in different modeling studies the multi faceted context dependence needs to be considered when generalizing findings on the causes and potential manipulations of ecosystem dynamics as a basis for policy and management decisions that enhance sustainability software availability not applicable declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the swedish research council grant no 2018 06139 
25551,prediction of wildfire propagation plays a crucial role in reducing the impacts of such events various machine learning ml approaches namely support vector regression svr gaussian process regression gpr regression tree and neural networks nn were used to understand their applicability in developing models to predict the rate of spread of grassfires a dataset from both wildfires and experimental fires comprising 283 records with 7 features was compiled and utilized to develop and evaluate ml based models these models produced excellent fits to the model development dataset model fit against the evaluation dataset resulted in higher errors with some of the models that yielded the lowest error against the model development dataset producing the highest errors against the evaluation dataset the predictive performance of the best ml based models against that of operational models was evaluated the shap visualization tool was used to determine the most influential variables in the best performing models keywords artificial intelligence bushfire and wildfire machine learning rate of fire spread remote regions shap sensitivity analysis data availability the data that has been used is confidential 1 introduction wildfires are a natural hazard in remote regions and of worldwide significance vilar et al 2021 with their occurrence and adverse impacts anticipated to rise in the future ellis et al 2022 knowledge of wildfire behaviour and growth potential is necessary to support fire managers decision making and reduce the undesirable impacts of wildfires cruz et al 2015a forward fire rate of spread fros is one of the fire behaviour characteristics that is important to predict for fire suppression decision support alexander 2000 accurate prediction of fire fros assists suppression planning actions and be used to release timely warnings to the general population inaccurate or lack of predictions can lead to disastrous consequences price and bedward 2019 storey et al 2021 given the widespread nature of grasslands cruz et al 2015a groves 1994 and their associated potential to support fast spreading wildfires cruz and alexander 2019 noble 1991 predicting their spread is an essential component of emergency response past studies developed several fire spread models applicable to grasslands over a broad range of modelling approaches namely empirical cheney et al 1998 cruz et al 2018 mcarthur 1966 1977 noble et al 1980 semi empirical rothermel 1972 and physical linn and cunningham 2005 mell et al 2007 past ros models have provided significant contributions to fire management organizations machine learning ml methods have not yet been used to develop models that can be used operationally to forecast grass fire propagation these methods are seen as a powerful modelling approach with possible application in the wildfire modelling space namely in modelling the spread of free spreading fires ml uses input data for the purpose of learning and then predicts new situations based on what was learned improvement in data gathering procedures and computational power has opened the range of applications for ml models alsharif et al 2022 ml techniques are being utilized widely for developing predictive models in environmental applications zumwald et al 2021 past studies have employed various ml approaches such as support vector regression pesantez et al 2020 gaussian process regression cui et al 2021 regression tree jaxa rozen and kwakkel 2018 and neural networks arashpour et al 2022 wadhwani et al 2021 ml has been suggested as an appropriate tool for addressing wildfire problems pais et al 2021 an ml based fire spread simulation model was developed by using an extreme learning machine zheng et al 2017 comparing the results of ml based model prediction and five historic fires shows that the ml based model accuracy was above the acceptable threshold zheng et al 2017 a deep convolutional inverse graphics network as an ml approach was utilized in order to replicate wildfire simulations hodges and lattimer 2019 carried out by the fire growth simulation model finney 1987 hodges and lattimer 2019 shows the ml based model was capable of replicating fire propagation simulated by a different model some ml approaches such as neural networks have the capability to model a process without any implicit assumptions as shown by wadhwani et al 2021 in modelling the spread rate of fire in shrublands this is an advantage over traditional regression based models that require implicit assumptions on the model form the review of published studies emphasizes two significant aspects 1 the quality of ml approaches is heavily dependent on the quality and quantity of input data hence data must be considered a valuable resource during wildfire disasters 2 practitioners generally suffer from the low interpretability of ml methods thus the interpretability of these methods should be increased jain et al 2020 ml models generally behave as a black box lyngdoh et al 2022 thus the interpretation of the ml results through other methods is necessary to understand model behaviour better the visualization tool shapely additive explanations shap can be used to understand the sensitivity or influence of input variables in ml models and internal input output interdependencies lundberg et al 2020 game theory is the basis of shapely value for discovering the importance of input variables sundararajan and najmi 2020 this study aims to analyze ml approaches to modelling fire ros in grasslands three objectives and related research questions rq were considered rq1 what is the best ml method to describe and simulate wildfire ros in grasslands 16 different ml methods were tested including four support vector regressions four gaussian process regressions four regression trees and four neural networks the prediction performance of ml based ros models was compared against the model development dataset and an independent evaluation dataset rq2 how does the prediction performance of the best ml based models compare with established empirically based ros models for grassland fuel types the prediction accuracy of the best ml based model within each family was contrasted against that of empirical based ros models applied operationally in australia to predict wildfire propagation this comparison was based on a dataset that was independent of the formulation of any of the models tested rq3 what are the most influential variables for predicting the spread rate of fires in grassland fuels considering the dependencies among input variables the influence of each input variable on the model output was compared to answer the third research question shap was utilized for this purpose 2 methods the structure of this research study is shown in fig 1 the first step in developing a valid ml model is to gather a reliable input dataset the dataset used comprises the environmental and fire variables typically collected in fire behaviour studies categorized as weather fuel and fire behaviour the data was divided into development training and evaluation datasets the second step builds the ml based prediction models this was followed by an assessment of model predictive capacity against a fully independent dataset the best performing models were selected based on the model accuracy i e lower error and bias answering rq1 then the prediction capacity of these models was compared against an empirical based model used operationally answering rq2 finally the shap technique was utilized to interpret the effect of the various input variables in the selected ml model answering rq3 2 1 dataset selection the published literature has been reviewed in order to collect a dataset of grassfire data for consistency of the analysis the focus was placed on data from australia where previous experimental burn programs and data collected in wildfires in remote regions yielded a consistent dataset covering a broad range of burning conditions cheney et al 1998 cruz et al 2018 2020 harris et al 2011 the assembled dataset table 1 contains 283 data records of australian grassfires the dataset was divided into development d and evaluation e subsets the development dataset to be used in the model training process comprises 238 data records arising from different studies and comprised of both experimental fires and wildfires table 1 the evaluation dataset comprised 45 wildfire runs taken from harris et al 2011 and kilinc et al 2012 it is worth noting that the dataset is limited to grassfires in flat or gently undulating terrain where the slope was not a relevant variable determining fire propagation see table 2 2 2 dataset summaries error reference source not found summarises the statistics of the variables in the model development and evaluation datasets according to the fact that cheney et al 1998 ros model is one of the most practical grassland ros models in australia the selected variables and their units in this study are consistent with cheney et al 1998 to ensure consistency in the experimental and wildfire dataset where fuel moisture is not measured but estimated fine dead fuel moisture content was estimated based on air temperature and relative humidity using the cheney et al 1989 parameterization of mcarthur 1960 fuel moisture tables this also ensures consistency when applying a model in a predictive situation where dead fuel moisture content cannot be measured but is estimated fuel condition was characterized by grass condition as per cheney et al 1998 as available in the wildfire datasets fuel load although present in the experimental data was not present in the wildfire data and hence was not used in the analysis it should be noted that fuel load can be estimated from grass fuel conditions cheney et al 1993 gill et al 2010 although large uncertainty exists for the unharvested and harvested or cut conditions cruz et al 2018 2 3 ml based ros model development various ml types were used in this study namely support vector regression svr gaussian process regression gpr regression trees and neural networks nn regression learner application in matlab software matlab 2021 was used for developing the ml models these methods can be summarized as follows support vector regression svr is an ml method used in many previous studies as a prediction tool arashpour et al 2021 xiao et al 2019 the empirical performance of svr expands its popularity wei 2015 the mathematical formulation of svr is mentioned in table 3 svr is based on convex optimization to minimize predicted and real data errors all svr models considered in this study and their descriptions are available in table 3 as mentioned in table 3 the predicted value should fall within the support vector for all modelled data the epsilon that is half of the support vector is calculated based on eq 1 qadri et al 2020 1a e p s i l o n 0 1 i f i q r y 0 i q r y 13 49 o t h e r w i s e where y is the output of the regression analysis iqr y computes the interquartile range of y gaussian process regression gpr is recognized as a non parametric regression technique rasmussen 2003 the general form of regression is provided in table 3 in gpr each new input data updates the regression means and covariance matrix the covariance matrix is developed by covariance function or kernel function cui et al 2021 this study employed various kernel functions for gpr as given in table 3 regression trees trees are reviewed as a straightforward ml approach jaxa rozen and kwakkel 2018 if then linguistic rules are the basis of decision trees to provide a relationship between input variables and the response variable bockstaller et al 2017 in each iteration the tree split the parent node into two child nodes according to specific input data jafino and kwakkel 2021 regression decision trees find the optimal binary split in view of all possible binary splits reaching the greatest reduction in mean square error mse is a must for the optimal binary split based on the equation in table 3 in addition to normal decision trees appreciating the improvement in the performance of an ensemble of decision trees belitz and stackelberg 2021 bagged trees and boosted trees are reviewed by this study as an ensemble of decision trees bagged trees develop a bootstrap sample of training data which is operated in place of the entire training data john lu 2010 boosted trees combine numerous models to improve the model accuracy silva et al 2021 table 3 exhibits the tree models examined by this study and their designated model options neural networks the human neural system in the brain was the source of inspiration for artificial neural networks nn nn is capable of predicting fire ros wadhwani et al 2021 the multilayer perceptron mlp is a suitable approach for regression analysis golafshani et al 2022 this study employed a feed forward mlp containing three layers input hidden and output the layers include several artificial neurons connected by weights and biases which are set iteratively using the training dataset to minimize the loss function based on the equation in table 3 rodriguez delgado et al 2019 during the training process the k fold cross validation technique was selected to prevent the overfitting problem and increase the model generalization this technique divides the training dataset into k numbers of groups evenly a group among k groups known as the validation group is hidden from the training process in each iteration after the model development based on the remaining dataset training dataset model performance will be analysed using the validation dataset the average performance of the k developed models in the validation groups is reported as the performance of the ml technique the k is assumed 5 in this study the ml methods employed in this study were selected as they are among the most popular machine learning methods camastra et al 2022 golafshani et al 2022 jaxa rozen and kwakkel 2018 rasmussen 2003 2 4 evaluation statistics assessment of models performance and selecting the most accurate model is the final step to answering the first research question accordingly root mean square error rmse mean absolute error mae and mean bias error mbe are performance metrics that can assess the quality of prediction models hofman et al 2022 rmse mae and mbe are mathematically formulated as given in eqs 1 3 respectively sadeghi et al 2020 1b r m s e 1 n i 1 n r o s p r o s o 2 2 m a e 1 n i 1 n r o s p r o s o 3 m b e 1 n i 1 n r o s p r o s o where n is the number of data records rosp is the predicted value of fire ros roso is the observed fire ros in rmse formulation the difference between predicted and observed values raised to the second power hence the influence of outliers is more considerable than mae the mbe selection is consistent with its capability to discover the model overestimation mbe 0 or underestimation mbe 0 he et al 2019 2 5 comparison against empirical based ros models the ml models with the best performance were compared against the cheney et al 1998 suite of models for the rate of spread of grassfires the three models developed by cheney et al 1998 each applicable to a grass fuel condition i e undisturbed harvested or cut and eaten out are currently used in the operational prediction of grassfire propagation in australia cruz et al 2015b they have as inputs the 10 m open wind speed fine dead fuel moisture content and the degree of curing the comparison relied on the use of the various models to predict the fire spread rates of the harris et al 2011 and kilinc et al 2012 datasets table 1 this dataset is comprised solely of wildfire data and was not used in the development of any of the models used making this evaluation truly independent of the model development datasets to contrast the predictive capacity of the various models first a comparison between the obtained models fit statistics e g rmse mae and mbe was conducted second inspected graphic outputs such as the scatterplots of predicted versus observed rates of fire spread and residual distributions were used to further understand the various models predictions 2 6 interpretation with shap shapely additive explanations shap allow us to understand the sensitivity or influence of input variables in ml models shap mathematical formulation of lundberg et al 2020 is utilized in this research shapely values have the capability to demonstrate the contribution of each input variable and detect the importance and positive or negative impact of the input variables on the output shap values usually are approximated by various methods such as kernel shap deep shap and tree shap kannangara et al 2022 the selection of approximation methods is dependent on the machine learning method 3 results 3 1 predictive performance of ml based ros models the ml based approach to model ros resulted in a relatively short range of rmse mae and mbe when contrasted with the model development dataset respectively 0 39 1 69 km h 1 0 15 1 18 km h 1 and 0 20 to 0 00 km h 1 table 4 it is worth to note that the smaller value of rmse and mae means that the model prediction performance is better in addition mbe reveals the model overestimation mbe 0 or underestimation mbe 0 the smaller rmse mae and mbe value for the development dataset in comparison with the evaluation dataset are related to the fact that models are trained by the development dataset although the evaluation dataset is unseen for the models for this dataset the lowest rmse and mae values were obtained by the nn methods and the higher errors were obtained by the tree methods nonetheless the nn models produced the highest errors when using the evaluation dataset table 4 on average the nn models mae were ten times larger for the model evaluation dataset than for the model development dataset on average the gpr and tree modelling approaches produced the lowest mae against the model evaluation dataset respectively 2 80 and 2 94 km h 1 to further analyze the prediction performance of ml based ros models scatterplots of predicted versus observed values of all svr gpr tree and nn models are depicted in figs 2 5 respectively visual interpretation of the model predictions shows that although some models yield good overall fit statistics their behaviour is not adequate for example the bagged and coarse tree models produce relatively low mae and mbe values although examination of their predictions shows the models to predict ros within a narrow range fig 4b the best models were selected based on the fit statistics for the model evaluation dataset a visual analysis of the errors depicted in figs 2 5 and associated residuals the linear svr exponential gpr boosted trees and bilayered nn models were selected as the best models in each model family table 4 overall the selected models produced an mae varying between 2 62 and 2 92 km h 1 and an mbe between 0 22 and 2 05 km h 1 it is worth noting that gaussian svr and coarse tree models although producing low error statistics were not selected the gaussian svr model had the lowest mae and the second lowest mbe but under predicted all fires spreading faster than 9 km h 1 fig 2a the maximum predicted ros by this model was 10 km h 1 which would limit its application in predictive situations similarly the coarse tree model predictions were not satisfactory although this model yielded a low mbe and mae for the evaluation dataset it predicted only two different values 5 8 and 7 5 km h 1 with 90 of the predictions being the latter fig 4c 3 2 comparison of the predictive performance of ml models against cheney et al 1998 models against the evaluation dataset the cheney et al 1998 models yielded an overall rmse of 3 80 km h 1 an mae of 2 93 km h 1 and an mbe of 2 65 km h 1 fig 6 compares the rmse mae and mbe of the linear svr exponential gpr boosted tree and bilayered nn models as the best ml based models against cheney et al 1998 ros model each of the ml based models produced lower errors than the cheney et al 1998 models fig 6 and has a better prediction performance in addition it is worth to note that the ml models predictions have a statistically significant difference with cheney et al 1998 model based on p value analysis using the t test in addition the results of akaike information criterion aic and bayesian information criterion bic methods for the evaluation dataset as shown in table 5 are consistent and show a better performance of four ml models than cheney et al 1998 model as shown in fig 7 the performance of boosted trees in scatterplots is not appropriate for fast fires although the performance of linear svr exponential gpr and the bilayered nn are appropriate across the full spectrum of rates of fire spread the linear svr exponential gpr and the bilayered nn models yielded mae values of 2 65 2 62 and 2 92 km h 1 this amounts to a 10 reduction in average error for the linear svr exponential gpr models whereas the bilayered nn error is virtually identical to the results of the cheney et al 1998 model nonetheless the mbe obtained by these models respectively 2 05 1 94 and 0 22 km h 1 was much lower than obtained by the cheney et al 1998 model to further explore the performance of the models the error statistics for these three ml based models were calculated table 6 considering the grassed or cut condition the most common during the fire season in australia the three ml based models produced mae values 5 17 and 30 lower than the cheney et al 1998 models for the eaten out condition which typically reflects the end of fire season during drought years the ml based models yielded high mae values 70 16 and 16 higher than those obtained by the cheney et al 1998 model and a larger bias 3 3 interpretation of the top 3 ml based models shap approximation was employed to evaluate the effect of the different variables in the predicted rate of fire spread for the top 3 models ml based models namely linear svr exponential gpr and bilayered nn fig 8 displays the violin plot of the shap values for inputs for each input the shap value in the x axis indicates the effect of the selected input on ros for a given fire run the colour gradient represents the magnitude of the input variable for example for u10 blue coloured points on the left reveal low wind speed and red coloured points on the right demonstrate high wind speed correspondingly a shap value of 10 for the rightmost point indicates that for a particular fire run a high wind speed increased ros by 10 km h 1 from the mean value fig 8a fig 8 also reveals u10 is the most influential parameter in forecasting the fire ros for top 3 models m follows u10 as the second most influential variable for two of the models but not for the linear svr model the effect of m with the red higher fuel moisture content dots associated with negative shap values indicate the expected reduction in ros with an increase in m pasture condition p a surrogate of fuel arrangement and quantity was found to be the second most important variable for the linear svr model considering the shap values of c this variable was found to have a low impact on ros a result of the high curing levels average curing 97 and low variability of this variable in the dataset varying between 85 and 100 table 3 in brief fig 8 shows that wind speed and fuel moisture are the two main variables influencing the rate of spread 4 discussion 4 1 application of ml methods to develop ros models ml methods are typically used to develop predictive models in the context of fire behaviour prediction ml based models may have application in the operational prediction of fire propagation one of the advantages of ml models over other modeling approaches namely regression based ones e g sullivan 2009 is that it allows models to incorporate the effect of variables for which the effect on fire behavior is poorly understood wind gustiness vertical wind profile fire atmosphere interactions and feedback mechanisms are just a few components of the fire environment in rural and remote regions that are expected to affect grassfire propagation but are not quantified in existing operational models the datasets used in this study only described the basic variables that are used in existent models precluding the use of the full potential of ml methods testing a number of different ml methods allowed us to understand the wide variation in possible results and the contrast between the fit statistics obtained for the model development and evaluation datasets results demonstrate that a few ml based models such as wide neural networks had an astonishing performance for the development dataset however their prediction performance for the evaluation dataset was not suitable with average mean absolute errors one order of magnitude higher than obtained for the development dataset this issue is the result of ml based models overfitting the training dataset which has been reported in the literature for distinct applications lyngdoh et al 2022 williams et al 2020 these results highlighted the importance of testing the model against an unseen fully independent dataset have the models been evaluated against a dataset that was a randomly selected subsample of the overall data used in the study the subsample would likely be an image of the overall dataset and it would be possible to artificially improve the fit statistics of some of the poorly rated models the broad range of results observed highlighted the need to test different ml approaches when developing models for the rate of spread of wildfires in rural and remote regions testing the same array of ml based approaches to other fuel types such as shrublands or forests will allow us to better understand which ones are most adequate for fire spread modelling there might also be other approaches not tested in the present study e g extreme gradient boosting xgboost chen and guestrin 2016 that might also be adequate for the prediction of fire propagation the analysis showed that model selection based on average fit statistics such as mae and mbe might fail to lead to the selection of the best model the models that produced the lowest error statistics against the evaluation dataset e g exponential gpr and coarse tree were found to have limitations that would preclude their use in a predictive sense use of the average fit statistics with a judicious analysis of residuals and model behaviour was required to better understand model behaviour and limitations 4 2 contrasting ml based models against cheney et al 1998 models the results showed one requires a careful analysis to select the best ml based approach to produce models applicable for predicting the spread of wildfires the three best ml based models linear svr exponential gpr and bilayered nn produced mean errors that were on average 10 lower than the cheney et al 1998 ros models when compared to the wildfire model evaluation dataset bias reduction by the ml based models was significant and an advantage of these models it is worth noting that the lack of variability in the curing data limited the predictive power of the ml based models for lower curing values namely in situations characteristic of early fire season wildfires the results also showed the ml based models accuracy was low for fires in eaten out pastures cheney et al 1998 model was a more accurate one for predicting ros in this fuel condition this limitation can again be attributed to the lack of adequate data on eaten out pastures in the model development dataset ml based models are data dependent and if the quantity and the quality of data are not satisfactory the model performance will be limited cabaneros et al 2019 4 3 using ml based models to understand the effect of environmental variables determining the most influential variables and their weight in fire propagation can lead to an improvement in understanding of fire dynamics shap visualization was used to give insights into how the best ml based models incorporate the effect of the environmental variables based on shap results the wind speed was recognized as the most influential predictor of ros independent of the model these results are consistent with the literature cheney et al 1993 kucuk et al 2012 rossa and fernandes 2018 the magnitude of the wind speed effect in rural and remote regions was several times higher than the effect of other variables dead fuel moisture content was estimated as the second most influential predictor of ros albeit its influence was much lower than wind speed this result also arises from the fact that the fire data was characterized by low fuel moisture contents i e not covering a broad range the ml based models also found pasture conditions to have a strong effect curing level was identified by shap to have a very small effect on the rate of spread this result is inconsistent with empirical observations cheney et al 1998 cruz et al 2015c the results obtained for curing level are likely to be constrained by the small range of the curing level variables in the dataset varying over a very restricted range 85 100 where it is considered that the curing level effect is small cruz et al 2015c the use of a broader dataset will be required to expand the ml analysis to better capture this effect 4 4 data limitation according to the fact that data gathering in the middle of a disaster is not the priority wildfire data records may have lower reliability than experimental fire records however experimental fires generally are conducted under specific controlled conditions with a low fire spread rate in light of this concept the data summaries section shows that the ros means for the evaluation dataset wildfires is more than the evaluation dataset mostly experimental fires gathering more fast wildfire data records will increase the accuracy of the ml models in addition the impact of topography and climate change can be considered by more data records with more detailed features 5 conclusion the most important contribution of this study is related to the application of ml based ros models a large field based dataset n 283 was collected from australian grassfires in remote regions sixteen machine learning approaches from various machine learning types namely support vector regression gaussian process regression regression tree and neural networks were developed and their performances were reported the four ml models provide a significantly better fit to the data compared to the cheney et al 1998 model from a predictive performance point of view it was found that the selected machine learning models had some advantages including more accurate prediction and revealing the most influential variables over the empirically based formulations currently used to predict fire spread in grasslands in rural and remote regions of australia operationally overall this paper indicates that machine learning based models have a solid potential to improve existing models and develop new models for the rate of fire spread applying these methods to other fuel types will be necessary to identify the most suitable ml approaches to describe the problem further refinement of the ml based models namely through more comprehensive datasets is necessary to improve the models to handle better the effect of curing level and possibly the inclusions of other variables that have not yet been used in models such as wind gustiness and atmospheric stability moreover there is a future research opportunity to evaluate the impacts of climate change on behaviour of bushfires by using a comprehensive dataset declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors are grateful to musa kilinc for allowing the use of his data in the present analysis they are also grateful for support from the australian research council arc through the linkage project lp180101080 
25551,prediction of wildfire propagation plays a crucial role in reducing the impacts of such events various machine learning ml approaches namely support vector regression svr gaussian process regression gpr regression tree and neural networks nn were used to understand their applicability in developing models to predict the rate of spread of grassfires a dataset from both wildfires and experimental fires comprising 283 records with 7 features was compiled and utilized to develop and evaluate ml based models these models produced excellent fits to the model development dataset model fit against the evaluation dataset resulted in higher errors with some of the models that yielded the lowest error against the model development dataset producing the highest errors against the evaluation dataset the predictive performance of the best ml based models against that of operational models was evaluated the shap visualization tool was used to determine the most influential variables in the best performing models keywords artificial intelligence bushfire and wildfire machine learning rate of fire spread remote regions shap sensitivity analysis data availability the data that has been used is confidential 1 introduction wildfires are a natural hazard in remote regions and of worldwide significance vilar et al 2021 with their occurrence and adverse impacts anticipated to rise in the future ellis et al 2022 knowledge of wildfire behaviour and growth potential is necessary to support fire managers decision making and reduce the undesirable impacts of wildfires cruz et al 2015a forward fire rate of spread fros is one of the fire behaviour characteristics that is important to predict for fire suppression decision support alexander 2000 accurate prediction of fire fros assists suppression planning actions and be used to release timely warnings to the general population inaccurate or lack of predictions can lead to disastrous consequences price and bedward 2019 storey et al 2021 given the widespread nature of grasslands cruz et al 2015a groves 1994 and their associated potential to support fast spreading wildfires cruz and alexander 2019 noble 1991 predicting their spread is an essential component of emergency response past studies developed several fire spread models applicable to grasslands over a broad range of modelling approaches namely empirical cheney et al 1998 cruz et al 2018 mcarthur 1966 1977 noble et al 1980 semi empirical rothermel 1972 and physical linn and cunningham 2005 mell et al 2007 past ros models have provided significant contributions to fire management organizations machine learning ml methods have not yet been used to develop models that can be used operationally to forecast grass fire propagation these methods are seen as a powerful modelling approach with possible application in the wildfire modelling space namely in modelling the spread of free spreading fires ml uses input data for the purpose of learning and then predicts new situations based on what was learned improvement in data gathering procedures and computational power has opened the range of applications for ml models alsharif et al 2022 ml techniques are being utilized widely for developing predictive models in environmental applications zumwald et al 2021 past studies have employed various ml approaches such as support vector regression pesantez et al 2020 gaussian process regression cui et al 2021 regression tree jaxa rozen and kwakkel 2018 and neural networks arashpour et al 2022 wadhwani et al 2021 ml has been suggested as an appropriate tool for addressing wildfire problems pais et al 2021 an ml based fire spread simulation model was developed by using an extreme learning machine zheng et al 2017 comparing the results of ml based model prediction and five historic fires shows that the ml based model accuracy was above the acceptable threshold zheng et al 2017 a deep convolutional inverse graphics network as an ml approach was utilized in order to replicate wildfire simulations hodges and lattimer 2019 carried out by the fire growth simulation model finney 1987 hodges and lattimer 2019 shows the ml based model was capable of replicating fire propagation simulated by a different model some ml approaches such as neural networks have the capability to model a process without any implicit assumptions as shown by wadhwani et al 2021 in modelling the spread rate of fire in shrublands this is an advantage over traditional regression based models that require implicit assumptions on the model form the review of published studies emphasizes two significant aspects 1 the quality of ml approaches is heavily dependent on the quality and quantity of input data hence data must be considered a valuable resource during wildfire disasters 2 practitioners generally suffer from the low interpretability of ml methods thus the interpretability of these methods should be increased jain et al 2020 ml models generally behave as a black box lyngdoh et al 2022 thus the interpretation of the ml results through other methods is necessary to understand model behaviour better the visualization tool shapely additive explanations shap can be used to understand the sensitivity or influence of input variables in ml models and internal input output interdependencies lundberg et al 2020 game theory is the basis of shapely value for discovering the importance of input variables sundararajan and najmi 2020 this study aims to analyze ml approaches to modelling fire ros in grasslands three objectives and related research questions rq were considered rq1 what is the best ml method to describe and simulate wildfire ros in grasslands 16 different ml methods were tested including four support vector regressions four gaussian process regressions four regression trees and four neural networks the prediction performance of ml based ros models was compared against the model development dataset and an independent evaluation dataset rq2 how does the prediction performance of the best ml based models compare with established empirically based ros models for grassland fuel types the prediction accuracy of the best ml based model within each family was contrasted against that of empirical based ros models applied operationally in australia to predict wildfire propagation this comparison was based on a dataset that was independent of the formulation of any of the models tested rq3 what are the most influential variables for predicting the spread rate of fires in grassland fuels considering the dependencies among input variables the influence of each input variable on the model output was compared to answer the third research question shap was utilized for this purpose 2 methods the structure of this research study is shown in fig 1 the first step in developing a valid ml model is to gather a reliable input dataset the dataset used comprises the environmental and fire variables typically collected in fire behaviour studies categorized as weather fuel and fire behaviour the data was divided into development training and evaluation datasets the second step builds the ml based prediction models this was followed by an assessment of model predictive capacity against a fully independent dataset the best performing models were selected based on the model accuracy i e lower error and bias answering rq1 then the prediction capacity of these models was compared against an empirical based model used operationally answering rq2 finally the shap technique was utilized to interpret the effect of the various input variables in the selected ml model answering rq3 2 1 dataset selection the published literature has been reviewed in order to collect a dataset of grassfire data for consistency of the analysis the focus was placed on data from australia where previous experimental burn programs and data collected in wildfires in remote regions yielded a consistent dataset covering a broad range of burning conditions cheney et al 1998 cruz et al 2018 2020 harris et al 2011 the assembled dataset table 1 contains 283 data records of australian grassfires the dataset was divided into development d and evaluation e subsets the development dataset to be used in the model training process comprises 238 data records arising from different studies and comprised of both experimental fires and wildfires table 1 the evaluation dataset comprised 45 wildfire runs taken from harris et al 2011 and kilinc et al 2012 it is worth noting that the dataset is limited to grassfires in flat or gently undulating terrain where the slope was not a relevant variable determining fire propagation see table 2 2 2 dataset summaries error reference source not found summarises the statistics of the variables in the model development and evaluation datasets according to the fact that cheney et al 1998 ros model is one of the most practical grassland ros models in australia the selected variables and their units in this study are consistent with cheney et al 1998 to ensure consistency in the experimental and wildfire dataset where fuel moisture is not measured but estimated fine dead fuel moisture content was estimated based on air temperature and relative humidity using the cheney et al 1989 parameterization of mcarthur 1960 fuel moisture tables this also ensures consistency when applying a model in a predictive situation where dead fuel moisture content cannot be measured but is estimated fuel condition was characterized by grass condition as per cheney et al 1998 as available in the wildfire datasets fuel load although present in the experimental data was not present in the wildfire data and hence was not used in the analysis it should be noted that fuel load can be estimated from grass fuel conditions cheney et al 1993 gill et al 2010 although large uncertainty exists for the unharvested and harvested or cut conditions cruz et al 2018 2 3 ml based ros model development various ml types were used in this study namely support vector regression svr gaussian process regression gpr regression trees and neural networks nn regression learner application in matlab software matlab 2021 was used for developing the ml models these methods can be summarized as follows support vector regression svr is an ml method used in many previous studies as a prediction tool arashpour et al 2021 xiao et al 2019 the empirical performance of svr expands its popularity wei 2015 the mathematical formulation of svr is mentioned in table 3 svr is based on convex optimization to minimize predicted and real data errors all svr models considered in this study and their descriptions are available in table 3 as mentioned in table 3 the predicted value should fall within the support vector for all modelled data the epsilon that is half of the support vector is calculated based on eq 1 qadri et al 2020 1a e p s i l o n 0 1 i f i q r y 0 i q r y 13 49 o t h e r w i s e where y is the output of the regression analysis iqr y computes the interquartile range of y gaussian process regression gpr is recognized as a non parametric regression technique rasmussen 2003 the general form of regression is provided in table 3 in gpr each new input data updates the regression means and covariance matrix the covariance matrix is developed by covariance function or kernel function cui et al 2021 this study employed various kernel functions for gpr as given in table 3 regression trees trees are reviewed as a straightforward ml approach jaxa rozen and kwakkel 2018 if then linguistic rules are the basis of decision trees to provide a relationship between input variables and the response variable bockstaller et al 2017 in each iteration the tree split the parent node into two child nodes according to specific input data jafino and kwakkel 2021 regression decision trees find the optimal binary split in view of all possible binary splits reaching the greatest reduction in mean square error mse is a must for the optimal binary split based on the equation in table 3 in addition to normal decision trees appreciating the improvement in the performance of an ensemble of decision trees belitz and stackelberg 2021 bagged trees and boosted trees are reviewed by this study as an ensemble of decision trees bagged trees develop a bootstrap sample of training data which is operated in place of the entire training data john lu 2010 boosted trees combine numerous models to improve the model accuracy silva et al 2021 table 3 exhibits the tree models examined by this study and their designated model options neural networks the human neural system in the brain was the source of inspiration for artificial neural networks nn nn is capable of predicting fire ros wadhwani et al 2021 the multilayer perceptron mlp is a suitable approach for regression analysis golafshani et al 2022 this study employed a feed forward mlp containing three layers input hidden and output the layers include several artificial neurons connected by weights and biases which are set iteratively using the training dataset to minimize the loss function based on the equation in table 3 rodriguez delgado et al 2019 during the training process the k fold cross validation technique was selected to prevent the overfitting problem and increase the model generalization this technique divides the training dataset into k numbers of groups evenly a group among k groups known as the validation group is hidden from the training process in each iteration after the model development based on the remaining dataset training dataset model performance will be analysed using the validation dataset the average performance of the k developed models in the validation groups is reported as the performance of the ml technique the k is assumed 5 in this study the ml methods employed in this study were selected as they are among the most popular machine learning methods camastra et al 2022 golafshani et al 2022 jaxa rozen and kwakkel 2018 rasmussen 2003 2 4 evaluation statistics assessment of models performance and selecting the most accurate model is the final step to answering the first research question accordingly root mean square error rmse mean absolute error mae and mean bias error mbe are performance metrics that can assess the quality of prediction models hofman et al 2022 rmse mae and mbe are mathematically formulated as given in eqs 1 3 respectively sadeghi et al 2020 1b r m s e 1 n i 1 n r o s p r o s o 2 2 m a e 1 n i 1 n r o s p r o s o 3 m b e 1 n i 1 n r o s p r o s o where n is the number of data records rosp is the predicted value of fire ros roso is the observed fire ros in rmse formulation the difference between predicted and observed values raised to the second power hence the influence of outliers is more considerable than mae the mbe selection is consistent with its capability to discover the model overestimation mbe 0 or underestimation mbe 0 he et al 2019 2 5 comparison against empirical based ros models the ml models with the best performance were compared against the cheney et al 1998 suite of models for the rate of spread of grassfires the three models developed by cheney et al 1998 each applicable to a grass fuel condition i e undisturbed harvested or cut and eaten out are currently used in the operational prediction of grassfire propagation in australia cruz et al 2015b they have as inputs the 10 m open wind speed fine dead fuel moisture content and the degree of curing the comparison relied on the use of the various models to predict the fire spread rates of the harris et al 2011 and kilinc et al 2012 datasets table 1 this dataset is comprised solely of wildfire data and was not used in the development of any of the models used making this evaluation truly independent of the model development datasets to contrast the predictive capacity of the various models first a comparison between the obtained models fit statistics e g rmse mae and mbe was conducted second inspected graphic outputs such as the scatterplots of predicted versus observed rates of fire spread and residual distributions were used to further understand the various models predictions 2 6 interpretation with shap shapely additive explanations shap allow us to understand the sensitivity or influence of input variables in ml models shap mathematical formulation of lundberg et al 2020 is utilized in this research shapely values have the capability to demonstrate the contribution of each input variable and detect the importance and positive or negative impact of the input variables on the output shap values usually are approximated by various methods such as kernel shap deep shap and tree shap kannangara et al 2022 the selection of approximation methods is dependent on the machine learning method 3 results 3 1 predictive performance of ml based ros models the ml based approach to model ros resulted in a relatively short range of rmse mae and mbe when contrasted with the model development dataset respectively 0 39 1 69 km h 1 0 15 1 18 km h 1 and 0 20 to 0 00 km h 1 table 4 it is worth to note that the smaller value of rmse and mae means that the model prediction performance is better in addition mbe reveals the model overestimation mbe 0 or underestimation mbe 0 the smaller rmse mae and mbe value for the development dataset in comparison with the evaluation dataset are related to the fact that models are trained by the development dataset although the evaluation dataset is unseen for the models for this dataset the lowest rmse and mae values were obtained by the nn methods and the higher errors were obtained by the tree methods nonetheless the nn models produced the highest errors when using the evaluation dataset table 4 on average the nn models mae were ten times larger for the model evaluation dataset than for the model development dataset on average the gpr and tree modelling approaches produced the lowest mae against the model evaluation dataset respectively 2 80 and 2 94 km h 1 to further analyze the prediction performance of ml based ros models scatterplots of predicted versus observed values of all svr gpr tree and nn models are depicted in figs 2 5 respectively visual interpretation of the model predictions shows that although some models yield good overall fit statistics their behaviour is not adequate for example the bagged and coarse tree models produce relatively low mae and mbe values although examination of their predictions shows the models to predict ros within a narrow range fig 4b the best models were selected based on the fit statistics for the model evaluation dataset a visual analysis of the errors depicted in figs 2 5 and associated residuals the linear svr exponential gpr boosted trees and bilayered nn models were selected as the best models in each model family table 4 overall the selected models produced an mae varying between 2 62 and 2 92 km h 1 and an mbe between 0 22 and 2 05 km h 1 it is worth noting that gaussian svr and coarse tree models although producing low error statistics were not selected the gaussian svr model had the lowest mae and the second lowest mbe but under predicted all fires spreading faster than 9 km h 1 fig 2a the maximum predicted ros by this model was 10 km h 1 which would limit its application in predictive situations similarly the coarse tree model predictions were not satisfactory although this model yielded a low mbe and mae for the evaluation dataset it predicted only two different values 5 8 and 7 5 km h 1 with 90 of the predictions being the latter fig 4c 3 2 comparison of the predictive performance of ml models against cheney et al 1998 models against the evaluation dataset the cheney et al 1998 models yielded an overall rmse of 3 80 km h 1 an mae of 2 93 km h 1 and an mbe of 2 65 km h 1 fig 6 compares the rmse mae and mbe of the linear svr exponential gpr boosted tree and bilayered nn models as the best ml based models against cheney et al 1998 ros model each of the ml based models produced lower errors than the cheney et al 1998 models fig 6 and has a better prediction performance in addition it is worth to note that the ml models predictions have a statistically significant difference with cheney et al 1998 model based on p value analysis using the t test in addition the results of akaike information criterion aic and bayesian information criterion bic methods for the evaluation dataset as shown in table 5 are consistent and show a better performance of four ml models than cheney et al 1998 model as shown in fig 7 the performance of boosted trees in scatterplots is not appropriate for fast fires although the performance of linear svr exponential gpr and the bilayered nn are appropriate across the full spectrum of rates of fire spread the linear svr exponential gpr and the bilayered nn models yielded mae values of 2 65 2 62 and 2 92 km h 1 this amounts to a 10 reduction in average error for the linear svr exponential gpr models whereas the bilayered nn error is virtually identical to the results of the cheney et al 1998 model nonetheless the mbe obtained by these models respectively 2 05 1 94 and 0 22 km h 1 was much lower than obtained by the cheney et al 1998 model to further explore the performance of the models the error statistics for these three ml based models were calculated table 6 considering the grassed or cut condition the most common during the fire season in australia the three ml based models produced mae values 5 17 and 30 lower than the cheney et al 1998 models for the eaten out condition which typically reflects the end of fire season during drought years the ml based models yielded high mae values 70 16 and 16 higher than those obtained by the cheney et al 1998 model and a larger bias 3 3 interpretation of the top 3 ml based models shap approximation was employed to evaluate the effect of the different variables in the predicted rate of fire spread for the top 3 models ml based models namely linear svr exponential gpr and bilayered nn fig 8 displays the violin plot of the shap values for inputs for each input the shap value in the x axis indicates the effect of the selected input on ros for a given fire run the colour gradient represents the magnitude of the input variable for example for u10 blue coloured points on the left reveal low wind speed and red coloured points on the right demonstrate high wind speed correspondingly a shap value of 10 for the rightmost point indicates that for a particular fire run a high wind speed increased ros by 10 km h 1 from the mean value fig 8a fig 8 also reveals u10 is the most influential parameter in forecasting the fire ros for top 3 models m follows u10 as the second most influential variable for two of the models but not for the linear svr model the effect of m with the red higher fuel moisture content dots associated with negative shap values indicate the expected reduction in ros with an increase in m pasture condition p a surrogate of fuel arrangement and quantity was found to be the second most important variable for the linear svr model considering the shap values of c this variable was found to have a low impact on ros a result of the high curing levels average curing 97 and low variability of this variable in the dataset varying between 85 and 100 table 3 in brief fig 8 shows that wind speed and fuel moisture are the two main variables influencing the rate of spread 4 discussion 4 1 application of ml methods to develop ros models ml methods are typically used to develop predictive models in the context of fire behaviour prediction ml based models may have application in the operational prediction of fire propagation one of the advantages of ml models over other modeling approaches namely regression based ones e g sullivan 2009 is that it allows models to incorporate the effect of variables for which the effect on fire behavior is poorly understood wind gustiness vertical wind profile fire atmosphere interactions and feedback mechanisms are just a few components of the fire environment in rural and remote regions that are expected to affect grassfire propagation but are not quantified in existing operational models the datasets used in this study only described the basic variables that are used in existent models precluding the use of the full potential of ml methods testing a number of different ml methods allowed us to understand the wide variation in possible results and the contrast between the fit statistics obtained for the model development and evaluation datasets results demonstrate that a few ml based models such as wide neural networks had an astonishing performance for the development dataset however their prediction performance for the evaluation dataset was not suitable with average mean absolute errors one order of magnitude higher than obtained for the development dataset this issue is the result of ml based models overfitting the training dataset which has been reported in the literature for distinct applications lyngdoh et al 2022 williams et al 2020 these results highlighted the importance of testing the model against an unseen fully independent dataset have the models been evaluated against a dataset that was a randomly selected subsample of the overall data used in the study the subsample would likely be an image of the overall dataset and it would be possible to artificially improve the fit statistics of some of the poorly rated models the broad range of results observed highlighted the need to test different ml approaches when developing models for the rate of spread of wildfires in rural and remote regions testing the same array of ml based approaches to other fuel types such as shrublands or forests will allow us to better understand which ones are most adequate for fire spread modelling there might also be other approaches not tested in the present study e g extreme gradient boosting xgboost chen and guestrin 2016 that might also be adequate for the prediction of fire propagation the analysis showed that model selection based on average fit statistics such as mae and mbe might fail to lead to the selection of the best model the models that produced the lowest error statistics against the evaluation dataset e g exponential gpr and coarse tree were found to have limitations that would preclude their use in a predictive sense use of the average fit statistics with a judicious analysis of residuals and model behaviour was required to better understand model behaviour and limitations 4 2 contrasting ml based models against cheney et al 1998 models the results showed one requires a careful analysis to select the best ml based approach to produce models applicable for predicting the spread of wildfires the three best ml based models linear svr exponential gpr and bilayered nn produced mean errors that were on average 10 lower than the cheney et al 1998 ros models when compared to the wildfire model evaluation dataset bias reduction by the ml based models was significant and an advantage of these models it is worth noting that the lack of variability in the curing data limited the predictive power of the ml based models for lower curing values namely in situations characteristic of early fire season wildfires the results also showed the ml based models accuracy was low for fires in eaten out pastures cheney et al 1998 model was a more accurate one for predicting ros in this fuel condition this limitation can again be attributed to the lack of adequate data on eaten out pastures in the model development dataset ml based models are data dependent and if the quantity and the quality of data are not satisfactory the model performance will be limited cabaneros et al 2019 4 3 using ml based models to understand the effect of environmental variables determining the most influential variables and their weight in fire propagation can lead to an improvement in understanding of fire dynamics shap visualization was used to give insights into how the best ml based models incorporate the effect of the environmental variables based on shap results the wind speed was recognized as the most influential predictor of ros independent of the model these results are consistent with the literature cheney et al 1993 kucuk et al 2012 rossa and fernandes 2018 the magnitude of the wind speed effect in rural and remote regions was several times higher than the effect of other variables dead fuel moisture content was estimated as the second most influential predictor of ros albeit its influence was much lower than wind speed this result also arises from the fact that the fire data was characterized by low fuel moisture contents i e not covering a broad range the ml based models also found pasture conditions to have a strong effect curing level was identified by shap to have a very small effect on the rate of spread this result is inconsistent with empirical observations cheney et al 1998 cruz et al 2015c the results obtained for curing level are likely to be constrained by the small range of the curing level variables in the dataset varying over a very restricted range 85 100 where it is considered that the curing level effect is small cruz et al 2015c the use of a broader dataset will be required to expand the ml analysis to better capture this effect 4 4 data limitation according to the fact that data gathering in the middle of a disaster is not the priority wildfire data records may have lower reliability than experimental fire records however experimental fires generally are conducted under specific controlled conditions with a low fire spread rate in light of this concept the data summaries section shows that the ros means for the evaluation dataset wildfires is more than the evaluation dataset mostly experimental fires gathering more fast wildfire data records will increase the accuracy of the ml models in addition the impact of topography and climate change can be considered by more data records with more detailed features 5 conclusion the most important contribution of this study is related to the application of ml based ros models a large field based dataset n 283 was collected from australian grassfires in remote regions sixteen machine learning approaches from various machine learning types namely support vector regression gaussian process regression regression tree and neural networks were developed and their performances were reported the four ml models provide a significantly better fit to the data compared to the cheney et al 1998 model from a predictive performance point of view it was found that the selected machine learning models had some advantages including more accurate prediction and revealing the most influential variables over the empirically based formulations currently used to predict fire spread in grasslands in rural and remote regions of australia operationally overall this paper indicates that machine learning based models have a solid potential to improve existing models and develop new models for the rate of fire spread applying these methods to other fuel types will be necessary to identify the most suitable ml approaches to describe the problem further refinement of the ml based models namely through more comprehensive datasets is necessary to improve the models to handle better the effect of curing level and possibly the inclusions of other variables that have not yet been used in models such as wind gustiness and atmospheric stability moreover there is a future research opportunity to evaluate the impacts of climate change on behaviour of bushfires by using a comprehensive dataset declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors are grateful to musa kilinc for allowing the use of his data in the present analysis they are also grateful for support from the australian research council arc through the linkage project lp180101080 
25552,concerns about water resource availability and preservation have grown over recent decades due to increased levels of pollution stemming mainly from economic activities and uncontrolled population growth in this context the present study seeks to identify and quantify point pollution sources released from river margins a two dimensional transient advection diffusion equation is proposed to model the physical problem while the inverse problem for the identification and quantification of pollution sources is investigated through the definition of an objective function considering or not the availability of prior information the objective function then is minimized by applying the differential evolution algorithm simulated cases are presented and discussed for distinct release scenarios including single and multiple point locations the results obtained were found to be satisfactory even for the most difficult cases illustrating that the computational approaches presented in this work may yield valuable tools in the management of surface water resources keywords inverse problem differential evolution finite difference pollutant point sources data availability data will be made available on request 1 introduction the pollution of water resources is a significant problem for contemporary society given that water is a basic component not only for the maintenance of life but also for the majority of human activities the deterioration of this important resource is driven mainly by economic and population growth that leads to adding impurities to surface water bodies making impossible its direct use by supply systems drinking water and industrial applications that require high quality standards panagopoulos and haralambous 2020 zeunert and meon 2020 faria et al 2020 jiang et al 2021 united nations 2021 panagopoulos 2021 2022 according to the united nations world water development report approximately 80 of all wastewater industrial or urban is released into the environment without preliminary treatment which can cause a range of damaging effects for human health and the ecosystem united nations 2017 in addition to wastewater in recent decades thousands of accidental and illegal spills of pollutants have occurred around the world to be able to identify those responsible for these spills and to outline more effective plans to respond to emergencies it is essential to determine certain properties of the source to predict the behavior of the pollution plume zeunert and meon 2020 jiang et al 2021 moghaddam et al 2021 if these properties cannot be determined or if there is elevated uncertainty regarding their value mitigation policies will present reduced efficacy therefore the formulation and solution of inverse problems for the identification and quantification of pollution sources would be an important tool to aid in decision making for environmental management agencies this type of problem is a highly challenging topic given the ill posedness and specific complexities amongst which are the identification of multi point sources heterogeneous transport media and the availability of sparse and noisy data moghaddam et al 2021 mazaheri et al 2015 yan et al 2019 generally the investigation of pollutant spills in surface or subterranean water bodies seeks to estimate the location duration and or intensity of a certain number of sources the literature classifies the main estimation methods for pollution sources in water resources as simulation optimization probabilistic and direct or mathematical amiri et al 2019 this study is focused on optimization based approaches which are the most commonly employed as pointed out by moghaddam et al 2021 in this context we may cite the work of wang 1997 who employed a genetic algorithm ga to calibrate a conceptual rainfall runoff model obtaining good fits even for models with nine parameters khorsandi et al 2015 used the ce qual w2 model assuming synthetic data with three levels of error the authors compared an artificial neural network ann with the genetic algorithm and the pattern search ps with the best adjustments being subsidized by the optimization methods zhang and xin 2017 estimated pollutant sources in rivers investigating single and multiple spills the variables of interest were satisfactorily recovered with relative errors smaller than 5 modified versions of the genetic algorithms also have shown good results including the shuffled complex evolution sce ua the quantum genetic algorithm qga and the alternating direction genetic algorithm adga huang et al 2018 zhao et al 2020 2021 other evolutionary heuristics such as the differential evolution de have also been successfully employed within this subject gurarslan and karahan 2015 determined the number of spills and the source parameters the simulations performed yielded results with lower normalized errors than other works that addressed the same problem wang et al 2018 developed a new approach which is based on the combination of the backward probability method bpm with a linear regression model which was then solved via optimization with the aid of differential evolution the simulated annealing sa has been employed in the characterization of sources and mass of pollutant inserted in water bodies for instance faria et al 2020 performed both offline estimation using sa and online estimation using particle filter pf the second approach is interesting because it provides a way to carry out the identification process in real time which can be a helpful tool in monitoring programs requiring greater control levels similarly rodrigues et al 2013 solved an inverse problem employing a dispersion dead zone model ddzm the estimates found led to a better fit when compared to other classical one dimensional transport models yeh et al 2016 reconstructed the release history of pollutants in groundwater assuming their known location aquifers with different configurations for point surface and volumetric sources were modeled heuristics based on swarm intelligence have also been demonstrated successful tools mategaonkar and eldho 2012 studied the quality of water in an unconfined aquifer by monitoring the amount of total dissolved solids tds in which particle swarm optimization pso was used to find an optimal pump rate configuration for two discharge ponds as well as the costs involved in the remediation project parolin et al 2015 characterized the location and intensity of a hypothetical point source in an estuary presenting a comparison between the luus jaakola lj particle collision pca and ant colony aco methods due to the relevance of optimization approaches within this subject several other strategies have also been proposed in the literature such as ayvaz 2010 who used harmony search hs amiri et al 2019 who retrieved the location of the history for multiple sources from a system of linear algebraic equations with tikhonov regularization for the treatment of the ill posed feature of the inverse problem li et al 2016 who proposed a new objective function taking into account in addition to the residuals the governing equation and its respective boundary and initial conditions built in optimization routines from commercial solvers were also employed such as in jamshidi et al 2020 in which it was evaluated two different case studies using the fmincon solver and in zeunert and meon 2020 in which it was investigated a two dimensional problem using the fminsearchbnd routine it should be highlighted that in many of these studies among others it is reported non uniqueness issues in the solution of the inverse problem for the cases of multi point spills for instance one may see zeunert and meon 2020 mazaheri et al 2015 huang et al 2018 wang et al 2018 ayvaz 2010 according to moghaddam et al 2021 this difficulty occurs due to the ill posedness commonly present in this type of problem with this limitation normally being overcome through regularization schemes the installation of more monitoring stations or by using additional information based on this historical background the present work is aimed at the formulation and solution of inverse problems to estimate stationary pollutant sources single or multiple point which occur on the margins of a hypothetical stretch of river in different release configurations it is worth noting that these types of spills even though very common in practice are somehow overlooked in the literature especially regarding further analysis of non uniqueness issues in the identification of multiple contaminant sources the proposed modeling may be particularly useful to environmental management agencies whether in planning remediation actions in licensing processes and in identifying those responsible for spills that do not comply with current environmental legislation the physical problem addressed in this work is represented through the two dimensional transient advection diffusion equation and solved using the finite difference method the inverse problem is formulated implicitly as an optimization problem to be minimized with the differential evolution algorithm the solution to this minimization problem is carried out with a python package recently developed by the authors coined as ipsimpy inverse problem simple modeling the first study is aimed at the calibration of the advective and dispersive parameters of the transport equation based on available experimental data related to an instantaneous release of a saline tracer in an experiment performed by rodrigues et al 2013 at river são pedro brazil the quantification of characteristics of pollution sources such as load and location is carried out through numerical simulations in which three distinct cases are considered and the non uniqueness of the inverse problem solution associated with multi point spills is further discussed 2 formulation and solution of the direct problem 2 1 transport equation consider the release of a conservative substance into a stretch of river of length l x and width l y with fully developed laminar flow and constant dispersion coefficients as depicted in fig 1 the time evolution of the pollution plume can be modeled using the following partial differential equation fischer et al 1979 chapra 2008 1a c x y t t u y c x y t x e x 2 c x y t x 2 e y 2 c x y t y 2 t 0 1b c 0 y t 0 and c x x l x 0 1c e y c y y 0 ϕ 2 x t and e y c y y l y ϕ 1 x t 1d c x y 0 c 0 x y 0 where c is the concentration g m 3 u y is the velocity m s e x and e y are the longitudinal and transversal dispersion coefficients m 2 s respectively ϕ 1 x t and ϕ 2 x t are the pollutant sources g m 2 s located on the left and right river margins and c 0 x y is the initial concentration distribution g m 3 considering a fully developed laminar flow with no slip at the boundaries and neglecting the velocity component in the transversal direction in this work it is assumed a parabolic velocity profile is given as 2 u y u m a x 1 y 0 5 l y 0 5 l y 2 with u m a x indicating the maximum velocity m s which occurs at y l y 2 m that is at the center of the river 2 2 finite difference solution the solution of problem 1 is carried out with an explicit finite difference formulation through the forward time centered space ftcs scheme by approximating the spatial and time derivatives as advanced and centered finite difference formulas respectively fletcher and srinivas 1991 anderson et al 1997 hoffmann and chiang 2004 for a generic point x i y j within the discretized domain at an arbitrary time instant t n i and j indicate the spatial indices of the point x i y j and n indicates the time instant t n as follows 3 c x i y j t n c i j n employing these approximations after some algebraic manipulation it is achieved the following expression for the internal nodes of the computational mesh 4 c i j n 1 s y c i j 1 n s x r j 2 c i 1 j n 1 2 s x 2 s y c i j n 5 s x r j 2 c i 1 j n s y c i j 1 n with 6 r j u y j δ t δ x u j δ t δ x s x e x δ t δ x 2 s y e y δ t δ y 2 where δ x and δ y represent the grid spacing in the x and y directions respectively δ t represents the time discretization step r j s x and s y are coefficients resulting from the discretization procedure the boundary conditions are handled with fictitious nodes fletcher and srinivas 1991 anderson et al 1997 noye 1984 ozisik 1994 for the boundary at y 0 the following expression is obtained 7 c i 0 n 1 s x r 0 2 c i 1 0 n 1 2 s x 2 s y c i 0 n s x r 0 2 c i 1 0 n 2 s y c i 1 n 2 s y δ y e y ϕ 2 i δ x n δ t for the boundary at y l y 8 c i n y n 1 2 s y c i n y 1 n s x r n y 2 c i 1 n y n 1 2 s x 2 s y c i n y n s x r n y 2 c i 1 n y n 2 s y δ y e y ϕ 1 i δ x n δ t for the boundary at x l x 9 c n x j n 1 s y c n x j 1 n 2 s x c n x 1 j n 1 2 s x 2 s y c n x j n s y c n x j 1 n it is important to highlight that the nodes at the corners of the spatial mesh at n x 0 and n x n y as depicted in fig 2 may receive special treatment as they must simultaneously satisfy two boundary conditions one in the x direction and the other in the y direction for the corner located at x l x and y 0 it is employed the following expression 10 c n x 0 n 1 2 s x c n x 1 0 n 1 2 s x 2 s y c n x 0 n 2 s y c n x 1 n 2 s y δ y e y ϕ 2 n x δ x n δ t whereas for the corner at x l x and y l y it is achieved c n x n y n 1 2 s y c n x n y 1 n 2 s x c n x 1 n y n 1 2 s x 2 s y c n x n y n 11 2 s y δ y e y ϕ 1 n x δ x n δ t for the corners of the spatial mesh located at 0 0 and 0 n y it is not necessary to use this procedure since at x 0 a dirichlet boundary condition is present the expressions provided in eqs 4 11 can be used to calculate the concentration field within the computational mesh at successive time steps thus representing the solution of the direct problem 3 formulation and solution of the inverse problem in the present work the inverse problem is formulated implicitly as a minimization problem aiming at estimating the parameters vector of interest p hence an objective function s p may be defined to express a measure of the adherence between the calculated concentrations and the available experimental measurements assuming that the experimental errors are additive non correlated and follow a normal distribution with zero mean and constant standard deviation σ e x p the following likelihood function can be employed beck and arnold 1977 orlande et al 2011 ozisik 2000 which expresses the occurrence probability of the observed data y given the values of the parameters p 12 l p y 1 2 π σ e x p 2 n d exp 1 2 σ e x p 2 y c p y c p where p is the parameter vector with n p components y is the vector containing n d experimental data and c p is the vector containing the predicted concentrations as calculated from the solution of eq 1 employing the set of parameters p at the same positions and time instants of the measurements vector according to the maximum likelihood approach the choice of p should be made to maximize the probability of occurrence of the experimental set orlande et al 2011 this happens when the argument of the exponent of eq 12 is minimized therefore the metric which represents the adherence of the calculated concentrations to the available measurements in this case can be expressed by eq 13 13 s p i 1 n d y i c i p 2 y c p y c p if additional information is available regarding the values of the sought parameters the use of bayes theorem is very attractive given that through eq 14 this a priori information can be easily incorporated into the formulation of the inverse problem kaipio and somersalo 2004 as follows 14 π p y π p r p π y p π y where π p y is the posterior probability density π p r p the prior probability density π y p the likelihood function and π y is a normalization constant if p can be modeled by a normal distribution with mean μ p r and covariance matrix ω the prior distribution can be represented as beck and arnold 1977 aster et al 2018 15 π p r p 1 2 π n p 1 ω exp 1 2 μ p r p ω 1 μ p r p combining the prior information and the likelihood according to bayes theorem the maximum a posteriori map objective function can be defined as 16 s map p y c p y c p μ p r p ω 1 μ p r p which can be minimized in order to provide point estimates for the sought parameters 3 1 sensitivity analysis the sensitivity analysis can provide important information and anticipate difficulties for the solution of inverse problems and is an important tool for the experimental design by analyzing the sensitivity coefficients it is possible to evaluate the behavior of the observable quantity with respect to the sought parameters the scaled sensitivity coefficients can be calculated as follows beck and arnold 1977 beck et al 1985 17 x p j p j c i p p j i 1 2 n d and j 1 2 n p where x p j is the scaled sensitivity coefficient concerning the parameter p j it is desired that the sensitivity coefficients are relatively high and when two or more unknowns are simultaneously estimated their sensitivity coefficients must be linearly independent which graphically means that they should not present the same slope in absolute value in this work as the derivatives cannot be analytically obtained the sensitivity coefficients have been calculated using the central finite difference formula 18 c i p p j c p 1 p 2 p j δ p j p n p c p 1 p 2 p j δ p j p n p 2 δ p j important information can also be obtained through the analysis of the sensitivity matrix j ozisik 2000 as defined below 19 j p c p p c 1 p 1 c 1 p 2 c 1 p n p c 2 p 1 c 2 p 2 c 2 p n p c n d p 1 c n d p 2 c n d p n p if the sensitivity coefficients are too small or if they present a high level of linear dependence with each other it is expected that det j j 0 and the problem becomes ill conditioned ozisik 2000 hence the analysis of det j j for different scenarios may be relevant to assessing the inverse problem solution regarding non uniqueness issues 3 2 differential evolution algorithm the differential evolution de is a metaheuristic optimization method originally proposed by storn and price 1997 the method is inspired by the genetic algorithm simulating mutation and crossing operations in a given population to achieve the global optimum of an objective function it is a gradient free algorithm designed to identify the best decision variables in a continuous n dimensional search space presenting with good convergence properties in several tested optimization problems holland 1975 simon 2013 yang 2014 the iterative procedure of de can be synthesized according to the following steps definition of the initial population and updating the population with mutation operators crossing and selection until a given stop criterion is satisfied in order to detail the method consider n p sought parameters in p and a population with n p o p individuals at any given generation g the k ith individual of the population can be represented as x k g x k 1 x k 2 x k j x k n p 20 k 1 2 n p o p and j 1 2 n p hence a population of n p o p individuals at the g ith generation can be expressed as x g x 1 g x 2 g x k g x n p o p g x 1 1 x 1 2 x 1 j x 1 n p x 2 1 x 2 2 x 2 j x 2 n p x k 1 x k 2 x k j x k n p x n p o p 1 x n p o p 2 x n p o p j x n p o p n p the procedure is initialized with the definition of an initial population i e g 1 this population is randomly generated and yang 2014 affirms that the most applied technique for this randomization consists in using a uniformly distributed random variable to sample the search space that is for g 1 21 x k j x j l r a n d x j u x j l k 1 2 n p o p and j 1 2 n p where r a n d is a random number from a uniform distribution in 0 1 and x j l and x j u represent respectively the lower and upper limits of the sought parameters this initial population may then evolve throughout successive generations the first step of this iterative procedure is the application of the mutation operator for each vector x k g of the current generation another three distinct individuals x r 1 g x r 2 g and x r 3 g are randomly chosen to make up a mutant vector by performing the following vectorial operation 22 v k x r 1 g f x r 2 g x r 3 g k 1 2 n p o p with f 0 2 being the rate or factor of mutation and v k the k ith mutant vector it is worth noting that f is responsible for weighting the vectorial difference present in eq 22 then the crossing operator must be employed a test vector u k is constructed as follows 23 u k j v k j if r a n d c r or j j r a n d x k j otherwise in this expression c r 0 1 is the crossover rate which controls the components that will be transmitted to the test vector and j r a n d is an element randomly chosen from the set 1 2 n p with the goal of guaranteeing that u k is not an exact copy of the mutant vector fig 3 illustrates the mutation mechanism for a parameter vector with n p 7 components the updating of the population for the next generation is then performed using the selection operator if the test vector is better fitted i e if it yields a lower value in the objective function than the original individual then it substitutes the original individual in the next generation this operation can be written as follows 24 x k g 1 u k if s u k s x k g x k g otherwise k 1 2 n p o p after selecting the best individuals in the population the mutation crossing and selection mechanisms are performed until the stop criterion is satisfied the most commonly employed stop criteria are the number of generations the magnitude of the improvement of the solution the number of evaluations of the objective function and the execution time bozorg et al 2017 the flowchart of the de algorithm with the main fundamental steps is schematically presented in fig 4 the de algorithm as here described is readily available in the ipsimpy package for python it can be used with the following command desolution ipsimpy differentialevolution of npop lc options where of is the objective function to be optimized npop is the population size lc are the search intervals for the sought parameters of the problem options contains the de parameters f c r g m a x t o l besides the option summary the options vector can be set with all or part of its elements the omitted ones are assumed as the default values in the table 1 the variable desolution stores the results returning the estimated parameters as well as the convergence the value of the objective function of the best solution and the computational time spent in seconds if summary is true at the end of the minimization process all desolution information will be printed more details on the ipsimpy package including other optimization methods available can be obtained in the public project on github at https github com brunolugao ipsimpy git 4 results and discussion the river characteristics in the study cases examined in this work are motivated by an actual experimental setup by rodrigues et al 2013 performed in the river são pedro located in the mountains of rio de janeiro state southeast brazil in that work a one dimensional model was considered and the dispersion and advection coefficients were estimated via an inverse problem approach in the present work this experimental setup is first revisited to estimate the model parameters of the two dimensional transport equation here employed eqs 1a 1d then once the model considered is adequately calibrated some numerical experiments are examined for the identification of polluting sources released from the river margins both simple and multi point as schematically shown in fig 5 as highlighted by bozorg et al 2017 for stochastic optimization methods it is important to analyze several runs during the solution of a problem as due to the stochastic nature of the method in each run a slightly different nearly optimal solution may be obtained thus in this work the solution of the inverse problem is carried out in all cases considered for a set of at least 20 independent executions of the differential evolution algorithm adopting the number of generations as the stopping criterion the analysis of the results was based on the mean the standard deviation the coefficient of variation the worst and best estimates observed the results were also evaluated in terms of the fit between the calculated and the experimental concentrations real or simulated data through the root mean square error rmse the determination coefficient r 2 and the relative error of the pollutant mass recovered ϵ m as given below 25 rmse 1 n d i 1 n d y i c i p 2 26 r 2 i 1 n d y i y c i p c p i 1 n d y i y 2 i 1 n d c i p c p 2 2 27 ϵ m m t r u e m c a l c m t r u e 100 where y is the mean of experimental data c p is the mean of the calculated concentrations m t r u e is the true mass and m c a l c is the calculated mass obtained from the expression 28 m c a l c w t w where t w the duration of the constant load w when the load is not known assuming that the pollutant is completely mixed along the cross section m c a l c can be computed at position x e x p as 29 m c a l c q 0 t f c x e x p t d t where q represents the flow rate and t f the final simulation time all the codes were implemented in python language except the solution of the direct problem which was implemented in fortran 90 and converted into a python module using the f2py routine harris et al 2020 it is worth noting that the whole inverse analysis was carried out using the ipsimpy package 4 1 model calibration and validation table 2 presents the characteristics of the injection section of the problem considered the stretch in which the experiment took place is shallow enough to justify a vertically integrated approach on the other hand as the hypothetical monitoring takes place in a region where the complete lateral homogenization might not have been achieved two dimensional modeling as adopted in this work may be relevant the experiment consisted of the release of 2000 grams of sodium chloride nacl diluted in a bucket with approximately 15 liters of water which was instantaneously released into the central flow line samples were taken at two points respectively at 50 m and 100 m from the release point at x r e l e a s e 50 m every 15 s in the time interval from t 90 s to t 285 s using a portable conductivity meter model ec 214 this conductivity meter has a resolution of 0 1 μ scm 1 and an accuracy of 1 more details regarding the sampling process can be found in rodrigues et al 2013 fig 6 depicts the passage of the plume as a function of time in x e x p 150 m and y e x p 1 2 m it was considered a stretch of river of length l x 200 m with the injection of the tracer occurring at position x r e l e a s e 50 m and y r e l e a s e 1 2 m margin releases do not take place in this case and therefore ϕ 1 ϕ 2 0 in eq 1c in order to model the instantaneous release into the river the following initial condition was adopted 30 c x y 0 c 0 x y c 0 m h δ x x 0 δ y y 0 where h is the average depth m is the pollutant mass c 0 is the white background concentration present in the river and δ is the dirac delta function in order to illustrate the convergence of the numerical solution of the two dimensional transport equation here considered as well as provide verification of the code implementation table 3 presents the calculated concentrations of the saline tracer considering different spatial meshes for δ t 0 04 s evaluated at times t 100 200 and 400 s comparing to an independent numerical solution obtained with the built in routine ndsolve available in the wolfram mathematica software running in its default configuration under automatic absolute and relative error control in these results it is assumed u m a x 1 5 u and the dispersion coefficients were calculated with the empirical expressions developed by fischer et al 1979 yielding e x 0 2128 m 2 s and e y 0 0075 m 2 s the mesh chosen to be employed within the inverse problem solution procedure is the one with 201 61 nodes which presented a good compromise between accuracy and computational time it should be highlighted that a stability analysis was performed using the von neumann method and it was verified that for the range of the parameters values considered in this work this mesh ensured the non amplification of numerical errors i e stability of the numerical solution fig 7 further illustrates the numerical solution obtained by showing the evolution of the plume along the domain from t 15 s to t 350 s note that the complete mixing zone occurs around 100 m downstream from the release site from 300 s so from this point on it would be feasible to employ one dimensional models it should be noted that a different configuration of the advection and dispersion parameters can anticipate or delay the homogenization along the cross section the inverse problem in the model calibration step considers the minimization of the maximum likelihood objective function in order to estimate the advection and dispersion coefficients of the proposed two dimensional transport equation that is p u m a x e x e y the following parameters were considered within the differential evolution algorithm f 0 75 c r 0 8 n p o p 15 g m a x 100 the lower and upper search bounds for each parameter are presented in table 4 fig 8 shows the scaled sensitivity coefficients concerning the sought parameters where one may observe a high sensitivity to the advection coefficient but also feasible sensitivities to the dispersion coefficients moreover the curves are not linearly dependent with respect to each other at least in part of the time range from t 100 to t 400 s table 5 summarizes the 20 independent executions of the differential evolution algorithm for the model calibration one may observe a small difference between the best the worst and the average solutions which highlights the robustness of the minimization procedure the standard deviation of the results is remarkably low demonstrating a small spread around the average in all cases a good fitting with the experimental data is achieved with relative errors for the recovered mass lower than 0 27 fig 9 a presents the evolution of the objective function value during the differential evolution algorithm for the cases regarding p w o r s t and p b e s t besides the average fig 9 b shows the experimental measurements to be compared with the two dimensional transport model tdtm given by eq 1 and those obtained by rodrigues et al 2013 using the one dimensional advection dispersion adm and dispersion dead zone ddzm models the ddzm presented the best results with a root mean square error of 1 244 followed by tdtm and adm with 3 078 and 3 265 respectively this fitting difference can be mainly explained by the ability of the dispersion dead zone model in reproducing asymmetric concentration curves unlike other methodologies that obey fick s laws it is noted that p b e s t and p w o r s t for the two dimensional transport model were essentially coincident even at the highest concentrations evidencing the robustness of the minimization procedure here employed moreover even for this scenario in which a 1d model is feasible the 2d model proposed in this work is clearly able to provide a better fitting to the experimental data 4 2 identification of pollutant releases some numerical results are now presented regarding the estimation of pollutant releases at specific locations of the river margins the pollutant releases are characterized by the pollutant loads and the release locations three cases are addressed in case 1 a single point of release is considered in case 2 two points of release are investigated both at the same margin whereas in case 3 also two points of release are considered but one at each margin of the river in these cases the parameter vector comprises information about the pollutant source load w and spill position x with the advective and dispersive parameters u m a x e x and e y being the same as those found in the model calibration as real experimental data were not available synthetic data simulated from the solution of the direct problem were used considering a vector of exact parameters p e x a c t by adding gaussian noise with zero mean and constant standard deviation σ e x p in the order of 2 of the maximum concentration of the pollutant at the monitoring site as follows 31 y i c i p e x a c t e i e i n 0 σ e x p i 1 2 n d where e i are random errors drawn from the normal distribution n 0 σ e x p the measurements are considered to be taken at a monitoring site located at x e x p 150 m and y e x p 1 2 m where data are measured from t 15 s to t 540 s at 15 s interval yielding a total of n d 36 measurements it is worth mentioning that mesh convergence and numerical verification of the solution was carried out for the release scenarios of cases 1 2 and 3 similarly to section 4 1 yielding satisfactory results which are here omitted for the sake of brevity case 1 case 1 involves a single point of release in the right margin of the stretch of river considered the boundary conditions at y 0 and y l y in eq 1 may then be written as follows 32a c y y 0 ϕ 2 x t 0 32b e y c y y l y ϕ 1 x t w 1 h δ x x 1 where w 1 and x 1 are respectively the load and the release location and δ is the dirac delta function fig 10 shows the evolution of concentrations in the spatial domain between time instants t 50 s and t 400 s using the boundary conditions given by eqs 31 a b it can be seen that unlike the scenario with an instantaneous release where the pollutant was inserted in the center of the river course the complete mixing zone still does not occur at 100 m from x 1 this fact was already expected since according to the studies by fischer et al 1979 and thomann and mueller 1987 the distance required for complete homogenization is greater here it becomes evident the importance of adopting the proposed 2d model in the release scenarios studied the time evolution of the scaled sensitivity coefficients for the sought parameters p w 1 x 1 as calculated at the measurement site are presented in fig 11 one may observe the curves are linearly independent in the interval between t 100 s and t 400 s and have relatively high sensitivities to both parameters in this case the uniqueness of the inverse problem solution is expected only the maximum likelihood objective function is employed eq 13 which is minimized using the differential evolution algorithm with f 0 75 c r 0 8 n p o p 10 and stopping criterion g m a x 20 the exact values as well as the search intervals considered for the parameters are presented in table 6 the results of the 20 executions of the inverse problem solution with de are presented in table 7 one may observe that the worst solution presents the objective function value approximately 3 times greater than the mean value nevertheless good estimates are still achieved for both parameters w 1 and x 1 very close to the exact expected values of w 1 50 g s and x 1 5 m the average of the 20 independent solutions yielded relative errors of 0 27 and 0 53 for the load w 1 and the release location x 1 respectively fig 12 further illustrates the robustness of the inverse problem solution presenting the evolution of the objective function value along with the generations of de in fig 12 a and the comparison of the calculated concentrations with the estimated parameters against the simulated measurements in fig 12 b in these results one may observe general good behavior even for the worst ranked solution case 2 case 2 involves two points of release in the right margin of the stretch of river considered the boundary conditions at y 0 and y l y in eq 1 now may be written as follows 33a c y y 0 ϕ 2 x t 0 33b e y c y y l y ϕ 1 x t w 1 h δ x x 1 w 2 h δ x x 2 where w 1 and w 2 are the loads at the release locations x 1 and x 2 respectively yielding the sought parameters p w 1 x 1 w 2 x 2 of the inverse problem it should be noted that x 1 w 1 represents the first source and x 2 w 2 the second as already illustrated in fig 5 fig 13 shows the evolution of the plume at some time instants using the boundary conditions given by eqs 32a b it is interesting to note that at the time instant t 100 s the pollutant does not reach the right margin of the river course yet furthermore the complete mixing zone is further away since the second source is located at the position x 80 m once again this clearly illustrates the importance of the 2d model adopted in this work for dealing with the pollutant sources released from the margins of the river the scaled sensitivity coefficients for the sought parameters as calculated at the measurement site i e at x e x p 150 m and y e x p 1 2 m are presented in fig 14 one may observe that all curves show reasonable sensitivities regarding all parameters the simple observation of fig 14 a may lead to the impression that the curves present linear independence among them but the linear combination of two or more curves can yield a significant correlation with other curves which is not obvious from simple observation in fact to better address the uniqueness of the inverse problem solution one may observe fig 14 b which present the evolution of the determinant of j j for three different groups of parameters it should be noted that if both pollutant releases are estimated simultaneously the order of det j j drops from 1 0 3 to 10 6 clearly suggesting that uniqueness of the inverse problem solution is not expected in this case for p w 1 x 1 w 2 x 2 in fact by inspection of the boundary condition given by eq 33b it may be clear that interchanging the values of w 1 and x 1 with w 2 and x 2 leads to the same mathematical problem yielding the non uniqueness of this inverse problem solution in order to examine the inverse problem solution without prior information about the releases the maximum likelihood objective function eq 13 is first employed minimized using the differential evolution algorithm with f 0 75 c r 0 8 n p o p 20 and stopping criterion g m a x 60 the exact values as well as the search intervals considered for the parameters are presented in table 8 fig 15 graphically summarizes the solutions of 100 independent executions of de and clearly illustrates the non uniqueness behavior previously discussed in the dispersion graphs and histograms of the estimates for the location and load one may clearly observe that both solutions are identified the estimation of the two releases presents solutions grouped around two main minima of the objective function i e both w 1 x 1 and w 2 x 2 can interchangeably be 5 20 or 2 5 80 these results demonstrate that even in the absence of any prior information and despite the issues regarding non uniqueness of the inverse problem solution the inverse analysis through the minimization of the objective function with de may yield relevant information regarding the possible locations and loads of the releases now we examine the solution to this inverse problem within the bayesian framework i e assuming prior information may be available regarding the location of the pollutant releases for instance this information could be retrieved from any previous knowledge of most critical pollutant release locations within a river segment or from the solutions previously obtained and depicted in fig 15 where the two locations are clearly identified hence besides the information provided in table 8 the prior information was modeled by a normal distribution as provided in eq 15 with prior mean for x 1 and x 2 as 20 m and 80 m respectively and a standard deviation of 5 m table 9 presents the results of 20 independent executions of the de algorithm for the minimization of the map objective function with g m a x 60 in this case demonstrating that the prior information considered regarding the location of the releases may avoid the non uniqueness of the inverse problem solution one may observe consistent estimates with the worst result yielding only 0 77 error in the recovered mass ϵ m the metrics related to the adherence to the experimental set rmse and r 2 also showed significant similarity in all the estimates fig 16 further illustrates the robustness of the inverse problem solution presenting the evolution of the objective function value along with the generations of de in fig 16 a and the comparison of the calculated concentrations with the estimated parameters against the simulated measurements in fig 16 b in these results general good behavior is observed even for the worst ranked solution case 3 in case 3 once again the case of two points of release is investigated but now each release location is assumed at different margins of the stretch of river considered the boundary conditions at y 0 and y l y in eq 1 now may be written as follows 34a e y c y y 0 ϕ 2 x t w 2 h δ x x 2 34b e y c y y l y ϕ 1 x t w 1 h δ x x 1 where w 1 and w 2 are the loads and x 1 and x 2 are the respective locations yielding the sought parameters p w 1 x 1 w 2 x 2 of the inverse problem one may observe that w 1 x 1 represents the first source and w 2 x 2 the second as already shown in fig 5 fig 17 shows the evolution of the concentrations calculated at some selected time instants with the boundary conditions given by eqs 33 a b one may clearly observe the arrangement of the fonts on opposite margins in the positions x 1 20 m and x 2 80 m again it is worth noting that at t 100 s the two plumes begin to combine approximately at y 0 9 m and a 2d model is necessary similar to case 2 the time evolution of the scaled sensitivity coefficients for the sought parameters p w 1 x 1 w 2 x 2 as calculated at the measurement site i e at x e x p 150 m and y e x p 1 2 m are presented in fig 18 for case 3 the curves show reasonable sensitivities with respect to all parameters but the simple observation of fig 18 a can be misleading regarding the linear dependence of the sensitivity coefficients to better address the uniqueness of the inverse problem solution one may observe fig 18 b which present the evolution of the determinant of j j for three different groups of parameters it should be noted that if both pollutant releases are estimated simultaneously the order of det j j drops from 1 0 4 to 10 5 clearly suggesting that uniqueness of the inverse problem solution is not expected also in this case for p w 1 x 1 w 2 x 2 once again by inspection we can observe that interchanging the loads at the releases locations yields calculated concentrations at the monitoring site that are very similar to each other as illustrated in fig 19 the inverse problem solution without prior information regarding the releases i e the solution obtained with the minimization of the maximum likelihood objective function eq 13 is investigated with the de algorithm set with f 0 75 c r 0 8 n p o p 20 and stopping criterion g m a x 60 the exact values of this test case as well as the search intervals considered for the parameters are presented in table 10 fig 20 graphically summarizes the solutions of 100 independent executions of de presenting the dispersion graph and histograms regarding the locations and loads these results clearly illustrate the non uniqueness behavior one may observe that the estimation of the two releases presents solutions grouped around two main minima of the objective function i e both w 1 x 1 and w 2 x 2 can interchangeably be 5 20 or 2 5 80 these results demonstrate also for this case that even in the absence of any prior information and despite the issues regarding non uniqueness of the inverse problem solution the inverse analysis through the minimization of the objective function with de is capable of bringing relevant information regarding the locations and loads of the releases the analysis is now addressed towards the consideration of prior information available concerning the location of the releases again this information could be retrieved from any previous knowledge of most critical pollutant release locations within a river segment or from the solutions previously obtained and depicted in fig 20 where the two locations are clearly identified hence besides the information provided in table 10 the prior information was modeled by a normal distribution as provided in eq 15 with prior mean for x 1 and x 2 as 20 m and 80 m respectively and a standard deviation of 5 m the map objective function eq 16 is then minimized with de assuming f 0 75 c r 0 8 n p o p 20 e g m a x 60 table 11 presents the results of 20 independent executions of de for the inverse problem solution while fig 21 shows the convergence of the differential evolution algorithm as well as the comparison between calculated and measured simulated concentrations the results demonstrate that the prior information considered is capable of avoiding the non uniqueness of the solution with the coefficient of variation of the 20 execution yielding only cv 2 28 in the worst case for the load w 2 the metrics related to the adherence to the experimental set rmse and r 2 also shows significant similarity in all the estimates 5 concluding remarks the present study addressed the identification and quantification of point pollution sources both simple and multi point released from river margins the direct problem was modeled by the two dimensional advection diffusion equation and solved with an explicit finite difference scheme the inverse problem was formulated implicitly and treated as a minimization problem and then solved with the differential evolution algorithm as available in the python package developed by the authors named ipsimpy the results first illustrated the estimation of the advective and dispersive parameters of the 2d direct model proposed for set of real experimental data providing the calibration and validation of the model besides the verification of the numerical solution employed some numerical experiments were then performed aiming at evaluating the estimation of simple and multi point pollution sources in the following scenarios i case 1 a single release point ii case 2 two release points on the same margin and iii case 3 two release points release at different margins all cases showed good adherence to the simulated experimental data yielding coefficients of determination above 0 99 in addition the relative error of the pollutant mass recovered remained below 2 in all solutions for multiple releases limitations associated with the non uniqueness of the inverse problem solution were observed nonetheless even without considering any prior knowledge regarding spills positions the proposed methodology was able to obtain relevant information on source identification such as the clear identification of the two release locations in cases 2 and 3 it was also shown that this information modeled as prior information within the bayesian context through the map objective function was able to overcome the non uniqueness issues in light of the constantly increasing challenges for managing surface water resources the computational approaches developed in this work can provide feasible decision making tools to assist environmental agencies either in the analysis of critical scenarios in the environmental licensing process in the quantification of polluting substances or for the identification of probable spills in violation of environmental legislation guidelines this work may now evolve towards the simultaneous analysis of the hydrodynamics of the flow which would extend the applicability of this study to more complex geometries furthermore the formulation and solution of an inverse problem for function estimation aiming to identify transient point sources may also be considered declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was financed in part by capes coordenação de aperfeiçoamento de pessoal de nível superior brasil finance code 001 by cnpq conselho nacional de desenvolvimento científico e tecnológico award number 305086 2021 9 and by faperj fundação carlos chagas filho de amparo à pesquisa do estado do rio de janeiro award number e 26 202 746 2019 software availability name of the package ipsimpy availability https github com brunolugao ipsimpy git developer bruno carlos lugão year first available 2022 software required python version 3 x operation systems windows linux programming language python license gpl 3 0 
25552,concerns about water resource availability and preservation have grown over recent decades due to increased levels of pollution stemming mainly from economic activities and uncontrolled population growth in this context the present study seeks to identify and quantify point pollution sources released from river margins a two dimensional transient advection diffusion equation is proposed to model the physical problem while the inverse problem for the identification and quantification of pollution sources is investigated through the definition of an objective function considering or not the availability of prior information the objective function then is minimized by applying the differential evolution algorithm simulated cases are presented and discussed for distinct release scenarios including single and multiple point locations the results obtained were found to be satisfactory even for the most difficult cases illustrating that the computational approaches presented in this work may yield valuable tools in the management of surface water resources keywords inverse problem differential evolution finite difference pollutant point sources data availability data will be made available on request 1 introduction the pollution of water resources is a significant problem for contemporary society given that water is a basic component not only for the maintenance of life but also for the majority of human activities the deterioration of this important resource is driven mainly by economic and population growth that leads to adding impurities to surface water bodies making impossible its direct use by supply systems drinking water and industrial applications that require high quality standards panagopoulos and haralambous 2020 zeunert and meon 2020 faria et al 2020 jiang et al 2021 united nations 2021 panagopoulos 2021 2022 according to the united nations world water development report approximately 80 of all wastewater industrial or urban is released into the environment without preliminary treatment which can cause a range of damaging effects for human health and the ecosystem united nations 2017 in addition to wastewater in recent decades thousands of accidental and illegal spills of pollutants have occurred around the world to be able to identify those responsible for these spills and to outline more effective plans to respond to emergencies it is essential to determine certain properties of the source to predict the behavior of the pollution plume zeunert and meon 2020 jiang et al 2021 moghaddam et al 2021 if these properties cannot be determined or if there is elevated uncertainty regarding their value mitigation policies will present reduced efficacy therefore the formulation and solution of inverse problems for the identification and quantification of pollution sources would be an important tool to aid in decision making for environmental management agencies this type of problem is a highly challenging topic given the ill posedness and specific complexities amongst which are the identification of multi point sources heterogeneous transport media and the availability of sparse and noisy data moghaddam et al 2021 mazaheri et al 2015 yan et al 2019 generally the investigation of pollutant spills in surface or subterranean water bodies seeks to estimate the location duration and or intensity of a certain number of sources the literature classifies the main estimation methods for pollution sources in water resources as simulation optimization probabilistic and direct or mathematical amiri et al 2019 this study is focused on optimization based approaches which are the most commonly employed as pointed out by moghaddam et al 2021 in this context we may cite the work of wang 1997 who employed a genetic algorithm ga to calibrate a conceptual rainfall runoff model obtaining good fits even for models with nine parameters khorsandi et al 2015 used the ce qual w2 model assuming synthetic data with three levels of error the authors compared an artificial neural network ann with the genetic algorithm and the pattern search ps with the best adjustments being subsidized by the optimization methods zhang and xin 2017 estimated pollutant sources in rivers investigating single and multiple spills the variables of interest were satisfactorily recovered with relative errors smaller than 5 modified versions of the genetic algorithms also have shown good results including the shuffled complex evolution sce ua the quantum genetic algorithm qga and the alternating direction genetic algorithm adga huang et al 2018 zhao et al 2020 2021 other evolutionary heuristics such as the differential evolution de have also been successfully employed within this subject gurarslan and karahan 2015 determined the number of spills and the source parameters the simulations performed yielded results with lower normalized errors than other works that addressed the same problem wang et al 2018 developed a new approach which is based on the combination of the backward probability method bpm with a linear regression model which was then solved via optimization with the aid of differential evolution the simulated annealing sa has been employed in the characterization of sources and mass of pollutant inserted in water bodies for instance faria et al 2020 performed both offline estimation using sa and online estimation using particle filter pf the second approach is interesting because it provides a way to carry out the identification process in real time which can be a helpful tool in monitoring programs requiring greater control levels similarly rodrigues et al 2013 solved an inverse problem employing a dispersion dead zone model ddzm the estimates found led to a better fit when compared to other classical one dimensional transport models yeh et al 2016 reconstructed the release history of pollutants in groundwater assuming their known location aquifers with different configurations for point surface and volumetric sources were modeled heuristics based on swarm intelligence have also been demonstrated successful tools mategaonkar and eldho 2012 studied the quality of water in an unconfined aquifer by monitoring the amount of total dissolved solids tds in which particle swarm optimization pso was used to find an optimal pump rate configuration for two discharge ponds as well as the costs involved in the remediation project parolin et al 2015 characterized the location and intensity of a hypothetical point source in an estuary presenting a comparison between the luus jaakola lj particle collision pca and ant colony aco methods due to the relevance of optimization approaches within this subject several other strategies have also been proposed in the literature such as ayvaz 2010 who used harmony search hs amiri et al 2019 who retrieved the location of the history for multiple sources from a system of linear algebraic equations with tikhonov regularization for the treatment of the ill posed feature of the inverse problem li et al 2016 who proposed a new objective function taking into account in addition to the residuals the governing equation and its respective boundary and initial conditions built in optimization routines from commercial solvers were also employed such as in jamshidi et al 2020 in which it was evaluated two different case studies using the fmincon solver and in zeunert and meon 2020 in which it was investigated a two dimensional problem using the fminsearchbnd routine it should be highlighted that in many of these studies among others it is reported non uniqueness issues in the solution of the inverse problem for the cases of multi point spills for instance one may see zeunert and meon 2020 mazaheri et al 2015 huang et al 2018 wang et al 2018 ayvaz 2010 according to moghaddam et al 2021 this difficulty occurs due to the ill posedness commonly present in this type of problem with this limitation normally being overcome through regularization schemes the installation of more monitoring stations or by using additional information based on this historical background the present work is aimed at the formulation and solution of inverse problems to estimate stationary pollutant sources single or multiple point which occur on the margins of a hypothetical stretch of river in different release configurations it is worth noting that these types of spills even though very common in practice are somehow overlooked in the literature especially regarding further analysis of non uniqueness issues in the identification of multiple contaminant sources the proposed modeling may be particularly useful to environmental management agencies whether in planning remediation actions in licensing processes and in identifying those responsible for spills that do not comply with current environmental legislation the physical problem addressed in this work is represented through the two dimensional transient advection diffusion equation and solved using the finite difference method the inverse problem is formulated implicitly as an optimization problem to be minimized with the differential evolution algorithm the solution to this minimization problem is carried out with a python package recently developed by the authors coined as ipsimpy inverse problem simple modeling the first study is aimed at the calibration of the advective and dispersive parameters of the transport equation based on available experimental data related to an instantaneous release of a saline tracer in an experiment performed by rodrigues et al 2013 at river são pedro brazil the quantification of characteristics of pollution sources such as load and location is carried out through numerical simulations in which three distinct cases are considered and the non uniqueness of the inverse problem solution associated with multi point spills is further discussed 2 formulation and solution of the direct problem 2 1 transport equation consider the release of a conservative substance into a stretch of river of length l x and width l y with fully developed laminar flow and constant dispersion coefficients as depicted in fig 1 the time evolution of the pollution plume can be modeled using the following partial differential equation fischer et al 1979 chapra 2008 1a c x y t t u y c x y t x e x 2 c x y t x 2 e y 2 c x y t y 2 t 0 1b c 0 y t 0 and c x x l x 0 1c e y c y y 0 ϕ 2 x t and e y c y y l y ϕ 1 x t 1d c x y 0 c 0 x y 0 where c is the concentration g m 3 u y is the velocity m s e x and e y are the longitudinal and transversal dispersion coefficients m 2 s respectively ϕ 1 x t and ϕ 2 x t are the pollutant sources g m 2 s located on the left and right river margins and c 0 x y is the initial concentration distribution g m 3 considering a fully developed laminar flow with no slip at the boundaries and neglecting the velocity component in the transversal direction in this work it is assumed a parabolic velocity profile is given as 2 u y u m a x 1 y 0 5 l y 0 5 l y 2 with u m a x indicating the maximum velocity m s which occurs at y l y 2 m that is at the center of the river 2 2 finite difference solution the solution of problem 1 is carried out with an explicit finite difference formulation through the forward time centered space ftcs scheme by approximating the spatial and time derivatives as advanced and centered finite difference formulas respectively fletcher and srinivas 1991 anderson et al 1997 hoffmann and chiang 2004 for a generic point x i y j within the discretized domain at an arbitrary time instant t n i and j indicate the spatial indices of the point x i y j and n indicates the time instant t n as follows 3 c x i y j t n c i j n employing these approximations after some algebraic manipulation it is achieved the following expression for the internal nodes of the computational mesh 4 c i j n 1 s y c i j 1 n s x r j 2 c i 1 j n 1 2 s x 2 s y c i j n 5 s x r j 2 c i 1 j n s y c i j 1 n with 6 r j u y j δ t δ x u j δ t δ x s x e x δ t δ x 2 s y e y δ t δ y 2 where δ x and δ y represent the grid spacing in the x and y directions respectively δ t represents the time discretization step r j s x and s y are coefficients resulting from the discretization procedure the boundary conditions are handled with fictitious nodes fletcher and srinivas 1991 anderson et al 1997 noye 1984 ozisik 1994 for the boundary at y 0 the following expression is obtained 7 c i 0 n 1 s x r 0 2 c i 1 0 n 1 2 s x 2 s y c i 0 n s x r 0 2 c i 1 0 n 2 s y c i 1 n 2 s y δ y e y ϕ 2 i δ x n δ t for the boundary at y l y 8 c i n y n 1 2 s y c i n y 1 n s x r n y 2 c i 1 n y n 1 2 s x 2 s y c i n y n s x r n y 2 c i 1 n y n 2 s y δ y e y ϕ 1 i δ x n δ t for the boundary at x l x 9 c n x j n 1 s y c n x j 1 n 2 s x c n x 1 j n 1 2 s x 2 s y c n x j n s y c n x j 1 n it is important to highlight that the nodes at the corners of the spatial mesh at n x 0 and n x n y as depicted in fig 2 may receive special treatment as they must simultaneously satisfy two boundary conditions one in the x direction and the other in the y direction for the corner located at x l x and y 0 it is employed the following expression 10 c n x 0 n 1 2 s x c n x 1 0 n 1 2 s x 2 s y c n x 0 n 2 s y c n x 1 n 2 s y δ y e y ϕ 2 n x δ x n δ t whereas for the corner at x l x and y l y it is achieved c n x n y n 1 2 s y c n x n y 1 n 2 s x c n x 1 n y n 1 2 s x 2 s y c n x n y n 11 2 s y δ y e y ϕ 1 n x δ x n δ t for the corners of the spatial mesh located at 0 0 and 0 n y it is not necessary to use this procedure since at x 0 a dirichlet boundary condition is present the expressions provided in eqs 4 11 can be used to calculate the concentration field within the computational mesh at successive time steps thus representing the solution of the direct problem 3 formulation and solution of the inverse problem in the present work the inverse problem is formulated implicitly as a minimization problem aiming at estimating the parameters vector of interest p hence an objective function s p may be defined to express a measure of the adherence between the calculated concentrations and the available experimental measurements assuming that the experimental errors are additive non correlated and follow a normal distribution with zero mean and constant standard deviation σ e x p the following likelihood function can be employed beck and arnold 1977 orlande et al 2011 ozisik 2000 which expresses the occurrence probability of the observed data y given the values of the parameters p 12 l p y 1 2 π σ e x p 2 n d exp 1 2 σ e x p 2 y c p y c p where p is the parameter vector with n p components y is the vector containing n d experimental data and c p is the vector containing the predicted concentrations as calculated from the solution of eq 1 employing the set of parameters p at the same positions and time instants of the measurements vector according to the maximum likelihood approach the choice of p should be made to maximize the probability of occurrence of the experimental set orlande et al 2011 this happens when the argument of the exponent of eq 12 is minimized therefore the metric which represents the adherence of the calculated concentrations to the available measurements in this case can be expressed by eq 13 13 s p i 1 n d y i c i p 2 y c p y c p if additional information is available regarding the values of the sought parameters the use of bayes theorem is very attractive given that through eq 14 this a priori information can be easily incorporated into the formulation of the inverse problem kaipio and somersalo 2004 as follows 14 π p y π p r p π y p π y where π p y is the posterior probability density π p r p the prior probability density π y p the likelihood function and π y is a normalization constant if p can be modeled by a normal distribution with mean μ p r and covariance matrix ω the prior distribution can be represented as beck and arnold 1977 aster et al 2018 15 π p r p 1 2 π n p 1 ω exp 1 2 μ p r p ω 1 μ p r p combining the prior information and the likelihood according to bayes theorem the maximum a posteriori map objective function can be defined as 16 s map p y c p y c p μ p r p ω 1 μ p r p which can be minimized in order to provide point estimates for the sought parameters 3 1 sensitivity analysis the sensitivity analysis can provide important information and anticipate difficulties for the solution of inverse problems and is an important tool for the experimental design by analyzing the sensitivity coefficients it is possible to evaluate the behavior of the observable quantity with respect to the sought parameters the scaled sensitivity coefficients can be calculated as follows beck and arnold 1977 beck et al 1985 17 x p j p j c i p p j i 1 2 n d and j 1 2 n p where x p j is the scaled sensitivity coefficient concerning the parameter p j it is desired that the sensitivity coefficients are relatively high and when two or more unknowns are simultaneously estimated their sensitivity coefficients must be linearly independent which graphically means that they should not present the same slope in absolute value in this work as the derivatives cannot be analytically obtained the sensitivity coefficients have been calculated using the central finite difference formula 18 c i p p j c p 1 p 2 p j δ p j p n p c p 1 p 2 p j δ p j p n p 2 δ p j important information can also be obtained through the analysis of the sensitivity matrix j ozisik 2000 as defined below 19 j p c p p c 1 p 1 c 1 p 2 c 1 p n p c 2 p 1 c 2 p 2 c 2 p n p c n d p 1 c n d p 2 c n d p n p if the sensitivity coefficients are too small or if they present a high level of linear dependence with each other it is expected that det j j 0 and the problem becomes ill conditioned ozisik 2000 hence the analysis of det j j for different scenarios may be relevant to assessing the inverse problem solution regarding non uniqueness issues 3 2 differential evolution algorithm the differential evolution de is a metaheuristic optimization method originally proposed by storn and price 1997 the method is inspired by the genetic algorithm simulating mutation and crossing operations in a given population to achieve the global optimum of an objective function it is a gradient free algorithm designed to identify the best decision variables in a continuous n dimensional search space presenting with good convergence properties in several tested optimization problems holland 1975 simon 2013 yang 2014 the iterative procedure of de can be synthesized according to the following steps definition of the initial population and updating the population with mutation operators crossing and selection until a given stop criterion is satisfied in order to detail the method consider n p sought parameters in p and a population with n p o p individuals at any given generation g the k ith individual of the population can be represented as x k g x k 1 x k 2 x k j x k n p 20 k 1 2 n p o p and j 1 2 n p hence a population of n p o p individuals at the g ith generation can be expressed as x g x 1 g x 2 g x k g x n p o p g x 1 1 x 1 2 x 1 j x 1 n p x 2 1 x 2 2 x 2 j x 2 n p x k 1 x k 2 x k j x k n p x n p o p 1 x n p o p 2 x n p o p j x n p o p n p the procedure is initialized with the definition of an initial population i e g 1 this population is randomly generated and yang 2014 affirms that the most applied technique for this randomization consists in using a uniformly distributed random variable to sample the search space that is for g 1 21 x k j x j l r a n d x j u x j l k 1 2 n p o p and j 1 2 n p where r a n d is a random number from a uniform distribution in 0 1 and x j l and x j u represent respectively the lower and upper limits of the sought parameters this initial population may then evolve throughout successive generations the first step of this iterative procedure is the application of the mutation operator for each vector x k g of the current generation another three distinct individuals x r 1 g x r 2 g and x r 3 g are randomly chosen to make up a mutant vector by performing the following vectorial operation 22 v k x r 1 g f x r 2 g x r 3 g k 1 2 n p o p with f 0 2 being the rate or factor of mutation and v k the k ith mutant vector it is worth noting that f is responsible for weighting the vectorial difference present in eq 22 then the crossing operator must be employed a test vector u k is constructed as follows 23 u k j v k j if r a n d c r or j j r a n d x k j otherwise in this expression c r 0 1 is the crossover rate which controls the components that will be transmitted to the test vector and j r a n d is an element randomly chosen from the set 1 2 n p with the goal of guaranteeing that u k is not an exact copy of the mutant vector fig 3 illustrates the mutation mechanism for a parameter vector with n p 7 components the updating of the population for the next generation is then performed using the selection operator if the test vector is better fitted i e if it yields a lower value in the objective function than the original individual then it substitutes the original individual in the next generation this operation can be written as follows 24 x k g 1 u k if s u k s x k g x k g otherwise k 1 2 n p o p after selecting the best individuals in the population the mutation crossing and selection mechanisms are performed until the stop criterion is satisfied the most commonly employed stop criteria are the number of generations the magnitude of the improvement of the solution the number of evaluations of the objective function and the execution time bozorg et al 2017 the flowchart of the de algorithm with the main fundamental steps is schematically presented in fig 4 the de algorithm as here described is readily available in the ipsimpy package for python it can be used with the following command desolution ipsimpy differentialevolution of npop lc options where of is the objective function to be optimized npop is the population size lc are the search intervals for the sought parameters of the problem options contains the de parameters f c r g m a x t o l besides the option summary the options vector can be set with all or part of its elements the omitted ones are assumed as the default values in the table 1 the variable desolution stores the results returning the estimated parameters as well as the convergence the value of the objective function of the best solution and the computational time spent in seconds if summary is true at the end of the minimization process all desolution information will be printed more details on the ipsimpy package including other optimization methods available can be obtained in the public project on github at https github com brunolugao ipsimpy git 4 results and discussion the river characteristics in the study cases examined in this work are motivated by an actual experimental setup by rodrigues et al 2013 performed in the river são pedro located in the mountains of rio de janeiro state southeast brazil in that work a one dimensional model was considered and the dispersion and advection coefficients were estimated via an inverse problem approach in the present work this experimental setup is first revisited to estimate the model parameters of the two dimensional transport equation here employed eqs 1a 1d then once the model considered is adequately calibrated some numerical experiments are examined for the identification of polluting sources released from the river margins both simple and multi point as schematically shown in fig 5 as highlighted by bozorg et al 2017 for stochastic optimization methods it is important to analyze several runs during the solution of a problem as due to the stochastic nature of the method in each run a slightly different nearly optimal solution may be obtained thus in this work the solution of the inverse problem is carried out in all cases considered for a set of at least 20 independent executions of the differential evolution algorithm adopting the number of generations as the stopping criterion the analysis of the results was based on the mean the standard deviation the coefficient of variation the worst and best estimates observed the results were also evaluated in terms of the fit between the calculated and the experimental concentrations real or simulated data through the root mean square error rmse the determination coefficient r 2 and the relative error of the pollutant mass recovered ϵ m as given below 25 rmse 1 n d i 1 n d y i c i p 2 26 r 2 i 1 n d y i y c i p c p i 1 n d y i y 2 i 1 n d c i p c p 2 2 27 ϵ m m t r u e m c a l c m t r u e 100 where y is the mean of experimental data c p is the mean of the calculated concentrations m t r u e is the true mass and m c a l c is the calculated mass obtained from the expression 28 m c a l c w t w where t w the duration of the constant load w when the load is not known assuming that the pollutant is completely mixed along the cross section m c a l c can be computed at position x e x p as 29 m c a l c q 0 t f c x e x p t d t where q represents the flow rate and t f the final simulation time all the codes were implemented in python language except the solution of the direct problem which was implemented in fortran 90 and converted into a python module using the f2py routine harris et al 2020 it is worth noting that the whole inverse analysis was carried out using the ipsimpy package 4 1 model calibration and validation table 2 presents the characteristics of the injection section of the problem considered the stretch in which the experiment took place is shallow enough to justify a vertically integrated approach on the other hand as the hypothetical monitoring takes place in a region where the complete lateral homogenization might not have been achieved two dimensional modeling as adopted in this work may be relevant the experiment consisted of the release of 2000 grams of sodium chloride nacl diluted in a bucket with approximately 15 liters of water which was instantaneously released into the central flow line samples were taken at two points respectively at 50 m and 100 m from the release point at x r e l e a s e 50 m every 15 s in the time interval from t 90 s to t 285 s using a portable conductivity meter model ec 214 this conductivity meter has a resolution of 0 1 μ scm 1 and an accuracy of 1 more details regarding the sampling process can be found in rodrigues et al 2013 fig 6 depicts the passage of the plume as a function of time in x e x p 150 m and y e x p 1 2 m it was considered a stretch of river of length l x 200 m with the injection of the tracer occurring at position x r e l e a s e 50 m and y r e l e a s e 1 2 m margin releases do not take place in this case and therefore ϕ 1 ϕ 2 0 in eq 1c in order to model the instantaneous release into the river the following initial condition was adopted 30 c x y 0 c 0 x y c 0 m h δ x x 0 δ y y 0 where h is the average depth m is the pollutant mass c 0 is the white background concentration present in the river and δ is the dirac delta function in order to illustrate the convergence of the numerical solution of the two dimensional transport equation here considered as well as provide verification of the code implementation table 3 presents the calculated concentrations of the saline tracer considering different spatial meshes for δ t 0 04 s evaluated at times t 100 200 and 400 s comparing to an independent numerical solution obtained with the built in routine ndsolve available in the wolfram mathematica software running in its default configuration under automatic absolute and relative error control in these results it is assumed u m a x 1 5 u and the dispersion coefficients were calculated with the empirical expressions developed by fischer et al 1979 yielding e x 0 2128 m 2 s and e y 0 0075 m 2 s the mesh chosen to be employed within the inverse problem solution procedure is the one with 201 61 nodes which presented a good compromise between accuracy and computational time it should be highlighted that a stability analysis was performed using the von neumann method and it was verified that for the range of the parameters values considered in this work this mesh ensured the non amplification of numerical errors i e stability of the numerical solution fig 7 further illustrates the numerical solution obtained by showing the evolution of the plume along the domain from t 15 s to t 350 s note that the complete mixing zone occurs around 100 m downstream from the release site from 300 s so from this point on it would be feasible to employ one dimensional models it should be noted that a different configuration of the advection and dispersion parameters can anticipate or delay the homogenization along the cross section the inverse problem in the model calibration step considers the minimization of the maximum likelihood objective function in order to estimate the advection and dispersion coefficients of the proposed two dimensional transport equation that is p u m a x e x e y the following parameters were considered within the differential evolution algorithm f 0 75 c r 0 8 n p o p 15 g m a x 100 the lower and upper search bounds for each parameter are presented in table 4 fig 8 shows the scaled sensitivity coefficients concerning the sought parameters where one may observe a high sensitivity to the advection coefficient but also feasible sensitivities to the dispersion coefficients moreover the curves are not linearly dependent with respect to each other at least in part of the time range from t 100 to t 400 s table 5 summarizes the 20 independent executions of the differential evolution algorithm for the model calibration one may observe a small difference between the best the worst and the average solutions which highlights the robustness of the minimization procedure the standard deviation of the results is remarkably low demonstrating a small spread around the average in all cases a good fitting with the experimental data is achieved with relative errors for the recovered mass lower than 0 27 fig 9 a presents the evolution of the objective function value during the differential evolution algorithm for the cases regarding p w o r s t and p b e s t besides the average fig 9 b shows the experimental measurements to be compared with the two dimensional transport model tdtm given by eq 1 and those obtained by rodrigues et al 2013 using the one dimensional advection dispersion adm and dispersion dead zone ddzm models the ddzm presented the best results with a root mean square error of 1 244 followed by tdtm and adm with 3 078 and 3 265 respectively this fitting difference can be mainly explained by the ability of the dispersion dead zone model in reproducing asymmetric concentration curves unlike other methodologies that obey fick s laws it is noted that p b e s t and p w o r s t for the two dimensional transport model were essentially coincident even at the highest concentrations evidencing the robustness of the minimization procedure here employed moreover even for this scenario in which a 1d model is feasible the 2d model proposed in this work is clearly able to provide a better fitting to the experimental data 4 2 identification of pollutant releases some numerical results are now presented regarding the estimation of pollutant releases at specific locations of the river margins the pollutant releases are characterized by the pollutant loads and the release locations three cases are addressed in case 1 a single point of release is considered in case 2 two points of release are investigated both at the same margin whereas in case 3 also two points of release are considered but one at each margin of the river in these cases the parameter vector comprises information about the pollutant source load w and spill position x with the advective and dispersive parameters u m a x e x and e y being the same as those found in the model calibration as real experimental data were not available synthetic data simulated from the solution of the direct problem were used considering a vector of exact parameters p e x a c t by adding gaussian noise with zero mean and constant standard deviation σ e x p in the order of 2 of the maximum concentration of the pollutant at the monitoring site as follows 31 y i c i p e x a c t e i e i n 0 σ e x p i 1 2 n d where e i are random errors drawn from the normal distribution n 0 σ e x p the measurements are considered to be taken at a monitoring site located at x e x p 150 m and y e x p 1 2 m where data are measured from t 15 s to t 540 s at 15 s interval yielding a total of n d 36 measurements it is worth mentioning that mesh convergence and numerical verification of the solution was carried out for the release scenarios of cases 1 2 and 3 similarly to section 4 1 yielding satisfactory results which are here omitted for the sake of brevity case 1 case 1 involves a single point of release in the right margin of the stretch of river considered the boundary conditions at y 0 and y l y in eq 1 may then be written as follows 32a c y y 0 ϕ 2 x t 0 32b e y c y y l y ϕ 1 x t w 1 h δ x x 1 where w 1 and x 1 are respectively the load and the release location and δ is the dirac delta function fig 10 shows the evolution of concentrations in the spatial domain between time instants t 50 s and t 400 s using the boundary conditions given by eqs 31 a b it can be seen that unlike the scenario with an instantaneous release where the pollutant was inserted in the center of the river course the complete mixing zone still does not occur at 100 m from x 1 this fact was already expected since according to the studies by fischer et al 1979 and thomann and mueller 1987 the distance required for complete homogenization is greater here it becomes evident the importance of adopting the proposed 2d model in the release scenarios studied the time evolution of the scaled sensitivity coefficients for the sought parameters p w 1 x 1 as calculated at the measurement site are presented in fig 11 one may observe the curves are linearly independent in the interval between t 100 s and t 400 s and have relatively high sensitivities to both parameters in this case the uniqueness of the inverse problem solution is expected only the maximum likelihood objective function is employed eq 13 which is minimized using the differential evolution algorithm with f 0 75 c r 0 8 n p o p 10 and stopping criterion g m a x 20 the exact values as well as the search intervals considered for the parameters are presented in table 6 the results of the 20 executions of the inverse problem solution with de are presented in table 7 one may observe that the worst solution presents the objective function value approximately 3 times greater than the mean value nevertheless good estimates are still achieved for both parameters w 1 and x 1 very close to the exact expected values of w 1 50 g s and x 1 5 m the average of the 20 independent solutions yielded relative errors of 0 27 and 0 53 for the load w 1 and the release location x 1 respectively fig 12 further illustrates the robustness of the inverse problem solution presenting the evolution of the objective function value along with the generations of de in fig 12 a and the comparison of the calculated concentrations with the estimated parameters against the simulated measurements in fig 12 b in these results one may observe general good behavior even for the worst ranked solution case 2 case 2 involves two points of release in the right margin of the stretch of river considered the boundary conditions at y 0 and y l y in eq 1 now may be written as follows 33a c y y 0 ϕ 2 x t 0 33b e y c y y l y ϕ 1 x t w 1 h δ x x 1 w 2 h δ x x 2 where w 1 and w 2 are the loads at the release locations x 1 and x 2 respectively yielding the sought parameters p w 1 x 1 w 2 x 2 of the inverse problem it should be noted that x 1 w 1 represents the first source and x 2 w 2 the second as already illustrated in fig 5 fig 13 shows the evolution of the plume at some time instants using the boundary conditions given by eqs 32a b it is interesting to note that at the time instant t 100 s the pollutant does not reach the right margin of the river course yet furthermore the complete mixing zone is further away since the second source is located at the position x 80 m once again this clearly illustrates the importance of the 2d model adopted in this work for dealing with the pollutant sources released from the margins of the river the scaled sensitivity coefficients for the sought parameters as calculated at the measurement site i e at x e x p 150 m and y e x p 1 2 m are presented in fig 14 one may observe that all curves show reasonable sensitivities regarding all parameters the simple observation of fig 14 a may lead to the impression that the curves present linear independence among them but the linear combination of two or more curves can yield a significant correlation with other curves which is not obvious from simple observation in fact to better address the uniqueness of the inverse problem solution one may observe fig 14 b which present the evolution of the determinant of j j for three different groups of parameters it should be noted that if both pollutant releases are estimated simultaneously the order of det j j drops from 1 0 3 to 10 6 clearly suggesting that uniqueness of the inverse problem solution is not expected in this case for p w 1 x 1 w 2 x 2 in fact by inspection of the boundary condition given by eq 33b it may be clear that interchanging the values of w 1 and x 1 with w 2 and x 2 leads to the same mathematical problem yielding the non uniqueness of this inverse problem solution in order to examine the inverse problem solution without prior information about the releases the maximum likelihood objective function eq 13 is first employed minimized using the differential evolution algorithm with f 0 75 c r 0 8 n p o p 20 and stopping criterion g m a x 60 the exact values as well as the search intervals considered for the parameters are presented in table 8 fig 15 graphically summarizes the solutions of 100 independent executions of de and clearly illustrates the non uniqueness behavior previously discussed in the dispersion graphs and histograms of the estimates for the location and load one may clearly observe that both solutions are identified the estimation of the two releases presents solutions grouped around two main minima of the objective function i e both w 1 x 1 and w 2 x 2 can interchangeably be 5 20 or 2 5 80 these results demonstrate that even in the absence of any prior information and despite the issues regarding non uniqueness of the inverse problem solution the inverse analysis through the minimization of the objective function with de may yield relevant information regarding the possible locations and loads of the releases now we examine the solution to this inverse problem within the bayesian framework i e assuming prior information may be available regarding the location of the pollutant releases for instance this information could be retrieved from any previous knowledge of most critical pollutant release locations within a river segment or from the solutions previously obtained and depicted in fig 15 where the two locations are clearly identified hence besides the information provided in table 8 the prior information was modeled by a normal distribution as provided in eq 15 with prior mean for x 1 and x 2 as 20 m and 80 m respectively and a standard deviation of 5 m table 9 presents the results of 20 independent executions of the de algorithm for the minimization of the map objective function with g m a x 60 in this case demonstrating that the prior information considered regarding the location of the releases may avoid the non uniqueness of the inverse problem solution one may observe consistent estimates with the worst result yielding only 0 77 error in the recovered mass ϵ m the metrics related to the adherence to the experimental set rmse and r 2 also showed significant similarity in all the estimates fig 16 further illustrates the robustness of the inverse problem solution presenting the evolution of the objective function value along with the generations of de in fig 16 a and the comparison of the calculated concentrations with the estimated parameters against the simulated measurements in fig 16 b in these results general good behavior is observed even for the worst ranked solution case 3 in case 3 once again the case of two points of release is investigated but now each release location is assumed at different margins of the stretch of river considered the boundary conditions at y 0 and y l y in eq 1 now may be written as follows 34a e y c y y 0 ϕ 2 x t w 2 h δ x x 2 34b e y c y y l y ϕ 1 x t w 1 h δ x x 1 where w 1 and w 2 are the loads and x 1 and x 2 are the respective locations yielding the sought parameters p w 1 x 1 w 2 x 2 of the inverse problem one may observe that w 1 x 1 represents the first source and w 2 x 2 the second as already shown in fig 5 fig 17 shows the evolution of the concentrations calculated at some selected time instants with the boundary conditions given by eqs 33 a b one may clearly observe the arrangement of the fonts on opposite margins in the positions x 1 20 m and x 2 80 m again it is worth noting that at t 100 s the two plumes begin to combine approximately at y 0 9 m and a 2d model is necessary similar to case 2 the time evolution of the scaled sensitivity coefficients for the sought parameters p w 1 x 1 w 2 x 2 as calculated at the measurement site i e at x e x p 150 m and y e x p 1 2 m are presented in fig 18 for case 3 the curves show reasonable sensitivities with respect to all parameters but the simple observation of fig 18 a can be misleading regarding the linear dependence of the sensitivity coefficients to better address the uniqueness of the inverse problem solution one may observe fig 18 b which present the evolution of the determinant of j j for three different groups of parameters it should be noted that if both pollutant releases are estimated simultaneously the order of det j j drops from 1 0 4 to 10 5 clearly suggesting that uniqueness of the inverse problem solution is not expected also in this case for p w 1 x 1 w 2 x 2 once again by inspection we can observe that interchanging the loads at the releases locations yields calculated concentrations at the monitoring site that are very similar to each other as illustrated in fig 19 the inverse problem solution without prior information regarding the releases i e the solution obtained with the minimization of the maximum likelihood objective function eq 13 is investigated with the de algorithm set with f 0 75 c r 0 8 n p o p 20 and stopping criterion g m a x 60 the exact values of this test case as well as the search intervals considered for the parameters are presented in table 10 fig 20 graphically summarizes the solutions of 100 independent executions of de presenting the dispersion graph and histograms regarding the locations and loads these results clearly illustrate the non uniqueness behavior one may observe that the estimation of the two releases presents solutions grouped around two main minima of the objective function i e both w 1 x 1 and w 2 x 2 can interchangeably be 5 20 or 2 5 80 these results demonstrate also for this case that even in the absence of any prior information and despite the issues regarding non uniqueness of the inverse problem solution the inverse analysis through the minimization of the objective function with de is capable of bringing relevant information regarding the locations and loads of the releases the analysis is now addressed towards the consideration of prior information available concerning the location of the releases again this information could be retrieved from any previous knowledge of most critical pollutant release locations within a river segment or from the solutions previously obtained and depicted in fig 20 where the two locations are clearly identified hence besides the information provided in table 10 the prior information was modeled by a normal distribution as provided in eq 15 with prior mean for x 1 and x 2 as 20 m and 80 m respectively and a standard deviation of 5 m the map objective function eq 16 is then minimized with de assuming f 0 75 c r 0 8 n p o p 20 e g m a x 60 table 11 presents the results of 20 independent executions of de for the inverse problem solution while fig 21 shows the convergence of the differential evolution algorithm as well as the comparison between calculated and measured simulated concentrations the results demonstrate that the prior information considered is capable of avoiding the non uniqueness of the solution with the coefficient of variation of the 20 execution yielding only cv 2 28 in the worst case for the load w 2 the metrics related to the adherence to the experimental set rmse and r 2 also shows significant similarity in all the estimates 5 concluding remarks the present study addressed the identification and quantification of point pollution sources both simple and multi point released from river margins the direct problem was modeled by the two dimensional advection diffusion equation and solved with an explicit finite difference scheme the inverse problem was formulated implicitly and treated as a minimization problem and then solved with the differential evolution algorithm as available in the python package developed by the authors named ipsimpy the results first illustrated the estimation of the advective and dispersive parameters of the 2d direct model proposed for set of real experimental data providing the calibration and validation of the model besides the verification of the numerical solution employed some numerical experiments were then performed aiming at evaluating the estimation of simple and multi point pollution sources in the following scenarios i case 1 a single release point ii case 2 two release points on the same margin and iii case 3 two release points release at different margins all cases showed good adherence to the simulated experimental data yielding coefficients of determination above 0 99 in addition the relative error of the pollutant mass recovered remained below 2 in all solutions for multiple releases limitations associated with the non uniqueness of the inverse problem solution were observed nonetheless even without considering any prior knowledge regarding spills positions the proposed methodology was able to obtain relevant information on source identification such as the clear identification of the two release locations in cases 2 and 3 it was also shown that this information modeled as prior information within the bayesian context through the map objective function was able to overcome the non uniqueness issues in light of the constantly increasing challenges for managing surface water resources the computational approaches developed in this work can provide feasible decision making tools to assist environmental agencies either in the analysis of critical scenarios in the environmental licensing process in the quantification of polluting substances or for the identification of probable spills in violation of environmental legislation guidelines this work may now evolve towards the simultaneous analysis of the hydrodynamics of the flow which would extend the applicability of this study to more complex geometries furthermore the formulation and solution of an inverse problem for function estimation aiming to identify transient point sources may also be considered declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was financed in part by capes coordenação de aperfeiçoamento de pessoal de nível superior brasil finance code 001 by cnpq conselho nacional de desenvolvimento científico e tecnológico award number 305086 2021 9 and by faperj fundação carlos chagas filho de amparo à pesquisa do estado do rio de janeiro award number e 26 202 746 2019 software availability name of the package ipsimpy availability https github com brunolugao ipsimpy git developer bruno carlos lugão year first available 2022 software required python version 3 x operation systems windows linux programming language python license gpl 3 0 
25553,groundwater model data assimilation da aims to reduce uncertainty in simulated outcomes of interest to resource management while minimizing the potential for predictive bias sequential da which can estimate model states along with properties and stresses dynamically in time offers a potentially powerful alternative to batch da i e history matching for reducing bias in decision relevant predictions in the presence of incorrect model structure and or processes this study evaluates the ability of batch and sequential da approaches to history match and forecast simulated quantities in the presence of model error using a novel ensemble based paired complex simple approach that enables the incorporation of stochastic uncertainty and a statistical evaluation of predictive bias our findings have implications for groundwater decision support modeling as they underscore the pitfalls of fixing parameters and forcing variables a priori and present a proof of concept for using adjustable model states to cope with model error keywords groundwater decision support modeling data assimilation model error data availability following donoho et al 2008 and stodden 2010 and in line with the fair principles the authors have endeavoured to provide all relevant python code and model files for cross platform reproducibility of the complete analysis which is available at https github com kmarkovich intera freyberg da releases tag v2 0 0 the software tool applied herein is available within the open source pest code suite available at https github com usgs pestpp as a c code named pestpp da pre compiled statically linked e g stand alone binaries for windows linux and macos are available as are cmake make and visual studio solutions this analysis also relied heavily on python modules for environmental model uncertainty analyses pyemu an open source python package available through pypi and at https github com pypest pyemu 1 introduction groundwater models are by necessity and by design a simplified representation of the infinitely complex real world hydrogeological systems this simplification and abstraction may arise from issues of spatial or temporal scale where the discretization of a modeled process is too coarse to capture fine scale governing dynamics or the upscaling of properties smooths out temporal and or spatial heterogeneity or the simplification and abstraction may refer to mathematical representation of physical processes whether they be physically motivated or empirical schemes the cumulative effect of these well documented limitations in groundwater models is referred to as model error or model misspecification and this discrepancy gives rise to structural noise in the simulated results doherty and welter 2010 many workers have sought to address the degree to which this abstraction and simplification produces deleterious effects on predictions see gupta et al 2012 white et al 2014 and references therein the primary consequences of using simplified models for making predictions are parameter compensation clark and vrugt 2006a doherty and christensen 2011 and resulting potential for predictive bias parameter compensation describes the tendency for parameters or parameter components which are not sensitive to the information content of the observations i e null space parameters to be erroneously adjusted during model calibration doherty and christensen 2011 watson et al 2013 this occurs because the model structure including discretization and process representation is not sufficient to represent the information contained in the assimilated observations and signal components corresponding to this insufficiency in the observations are incorrectly assimilated in many cases the propensity for parameter compensation may be visible as prior data conflict evans and moshonov 2006 prior data conflict arises from the simple model s inability to simulate the proper range and or characteristics of the hydrologic state at a given observation location and or time for predictions of management interest that are sensitive to these erroneously adjusted parameters parameter compensation leads to predictive bias which compromises the model s fidelity for decision making despite undergoing rigorous calibration and uncertainty analysis particularly concerning is that parameter compensation is often invisible to the modeler white et al 2014 knowling et al 2019 a recent evolution in how best to undertake data assimilation da for decision support modeling is the idea that models should strive for an appropriate balance of simplicity complexity doherty and moore 2020 hunt et al 2007 that is they should be simple enough to be computational cheap and numerically stable yet possess appropriately high dimensional and diverse parameterization to receive information contained in observations and or to incorporate appropriate stochasticity and uncertainty in decision relevant predictions this push for a increased level of parameterization has been enabled by the advent of ensemble based methods for inverse problems which effectively eliminate the computational constraint on free parameters in estimating the jacobian chen and oliver 2013 white 2018 white et al 2021 with enough spatial coverage and diverse types of parameters the relative influence of null space components on any one type of parameter or location in the model is minimized while the ability for various parameter types distributed across the domain to receive information from observations is enhanced the success of this model simplification approach defined here as computational savings and uncertainty reduction achieved with minimal predictive consequences depends on the type of decision relevant prediction and form of simplification for example knowling et al 2019 showed that a reduction in horizontal parameter resolution produced generally similar first and second moments of the flow and spatially integrated transport predictions compared to a highly parameterized model they found that this did not hold for concentration predictions which are known to be sensitive to local scale properties and stresses similarly white et al 2020c and knowling et al 2020 showed that reducing resolution in the vertical direction results in acceptable flow model predictions but pronounced biases in transport predictions moore and doherty 2021 explored the consequences of only using steady state calibration to reduce predictive uncertainty and found that it effectively reduced uncertainty for heads predictions via conditioning of the hydraulic conductivity field near data assimilation locations but that artificiality of the steady state condition and error incurred through observation averaging led to substantial predictive biases elsewhere in the model domain that the conclusion of each of these studies is it depends suggests that we are far from a generalizable da approach for balancing model complexity uncertainty reduction and computational efficiency across the range of decision support modeling contexts to date the majority of groundwater modeling studies that perform da for parameter estimation and uncertainty analysis assimilate data in a batch fashion that is the groundwater model is run to completion through the entire historic period and all observations are assimilated at once post hoc for the purposes of this study we refer to this approach as batch da but note that it is identical in practice to history matching and calibration the inherent assumption and algorithmic requirement of batch da is that the model structure and processes are correct and the only sources of model input uncertainty or receptacles for information are the unknown properties and stresses a major advantage of batch da is the ability to assimilate long term low frequency information since all observations are assimilated at once however only allowing properties and stresses to be uncertain introduces the potential for parameter compensation in batch da through the assimilation of structural noise generated from model error an alternative to batch da and potential strategy for coping with error in groundwater modeling decision support exists sequential da evensen 1994 sequential da most commonly implemented in hydrogeologic applications as kalman filtering methods allows for a more flexible and dynamic approach to assimilation and forecast frequency ranging from near real time hendricks franssen and kinzelbach 2008 hendricks franssen et al 2011 to seasonal assimilation and forecast cycles huang et al 2017 a key functionality provided by sequential da methods is the ability to estimate system state along with properties and stresses at user defined temporal frequencies within a sequential da framework the da algorithm takes control of the simulated states to advance the model through time whereas in batch da the model itself controls the simulated states and advances through time implicit in state estimation is that states are uncertain and this state uncertainty is a more flexible and appropriate receptacle for structural noise contained in observations i e simulated water levels are a more appropriate receptacle for missing signal components in water level observations than hydraulic conductivity or porosity sequential da methods can therefore reduce the burden of structural noise on static properties by assimilating this noise into the states through the sequential da process clark and vrugt 2006b kim et al 2021 put bluntly batch da may never be completely free of predictive bias due to being tied to an imperfect model structure regardless of the level of complexity sequential da offers a potentially powerful strategy for reducing bias in decision relevant predictions in the presence of model error in this work we evaluate the outcomes of using batch and sequential da approaches in a hypothetical case where a simple model structure is in error with respect to the more complex truth model from which the observations available for assimilation are generated this evaluation of batch and sequential da approaches in the presence of model error is an important and novel contribution to the practice of groundwater decision support modeling additionally this work advances the powerful paired complex simple analysis of doherty and christensen 2011 into an empirical paired complex simple analysis to enable the incorporation of stochastic uncertainty and a statistical evaluation of predictive bias the model error in this empirical paired complex simple analysis is explicitly represented by two scenarios one where the simple model is of coarser spatial both vertical and horizontal and temporal resolution compared to the complex model the other scenario builds on the reduced resolution and includes the additive effect of model process error by treating historic groundwater pumping as a fixed known quantity rather than an uncertain one performance of the two da approaches are evaluated through a comparison of simple models ability to history match and forecast decision relevant predictions we hypothesize that the joint parameter state estimation ability of sequential da is able to better cope with model error by subsuming structural noise that would otherwise be absorbed by static properties in batch da 2 methods we evaluate the sequential and batch da strategies using pestpp da alzraiee et al 2021 a newly developed generic da tool to join the suite of modeling decision support and uncertainty analysis programs in pest white et al 2021 full details on the theory implementation and capabilities of pestpp da can be found in alzraiee et al 2021 and we provide a brief overview here pestpp da can assimilate data in both batch and sequential modes where for sequential da the model simulation is divided into user defined cycles or a period of time for which applied forcings i e boundary conditions and parameters are constant and or discrete observations are available for assimilation the modflow stress period concept which naturally lends itself to the cycle concept in pestpp da allows for a high level of flexibility for designing the assimilation cycle length and frequency pestpp da can be run in three modes state estimation parameter estimation and the focus of this study joint state parameter estimation in joint state parameter estimation mode model states e g heads are considered uncertain along with model properties and forcings e g hydrogeologic properties and boundary condition fluxes each cycle has an estimated initial state and a simulated final state where the final state becomes the initial state for the subsequent cycle importantly during each cycle the model is run using the estimated initial state prior to data assimilation and the simulated output of that initial run is referred to as a one step ahead one step ahead forecast this forecast is significant because it reveals the near term predictive power of sequential da where the posterior state from the previous assimilation cycle combined with model physics through a forward run can provide a simulated quantity that closely predicts observed quantities prior to any data assimilation as with other programs of the pest suite pestpp da follows a bayesian framework where a prior distribution is defined for all uncertain parameters based on field measurements expert knowledge and the literature within the solution scheme of pestpp da the likelihood is proportional to the inverse of a weighted l 2 norm of innovations e g the objective function which represents the degree of agreement between observations and their simulated equivalents for each assimilation cycle pestpp da updates all parameters and states if doing joint state parameter estimation with the option of using a standard kalman update an iterative kalman update known as multiple data assimilation mda emerick and reynolds 2013 or an iterative ensemble smoother based on the gauss levenberg marquardt scheme chen and oliver 2013 white 2018 the posterior ensembles of parameters and predictions represent uncertainty in these model components after da see alzraiee et al 2021 for a complete discussion of the pestpp da solution schemes importantly both iterative solution schemes involve the use of ensembles to approximate the cross covariance between parameters and states and simulated equivalents to observations worth adding however is that the sequential nature of pestpp da if running in sequential mode means that the iterative solution schemes are applied for each user defined assimilation cycle increasing the computational cost compared to batch assimilation using iterative ensemble methods though still more efficient than the traditional finite difference jacobian of gauss levenberg marquardt glm algorithms doherty 2003 white et al 2020b for high dimensional inverse problems 3 model description we perform the paired complex simple model analysis using a version of the freyberg freyberg 1988 hunt et al 2020 model originally published in white et al 2020b the model which is implemented in modflow 6 langevin et al 2017 hughes et al 2017 is a transient multi layer model that includes groundwater and surface water processes model error is explicitly represented as resolution error by assimilating output from a finer spatial resolution daily stress period version of the freyberg model into a coarser resolution monthly stress period version of the freyberg model following white et al 2021 this is not unlike the situation of all groundwater models which are invariably a simplification of the resolution and structure of real world processes that generate our observations the lower resolution simple model has 40 rows 20 columns 1 layer and 25 stress periods the first stress period is 10 000 days in length and represents a pre development condition there is not groundwater extraction during the first stress period the subsequent 24 stress periods are discretized into monthly periods and include time varying boundary conditions representing cyclical recharge and pumping over the two year period observations for assimilation are available during the first year e g stress periods 2 13 and include two deep and one shallow groundwater level observation locations and one surface water gage location fig 1 the last 12 stress periods represent an unmeasured forecast period groundwater extraction occurs in six wells scattered throughout the domain for the entire simulation period and a seventh extraction well new well in fig 1 is added in stress period 11 to impart a nonstationary trend in the forecast period the higher resolution complex model mimics the lower resolution model in properties and boundary conditions differing only in spatial and temporal resolution it has 120 rows 60 columns and 3 layers three times as many of each as the lower resolution model and with the exception of the same initial 10 000 day pre development stress period as in the simple model the complex model simulates daily stress periods over the two year simulation length for a total of 731 stress periods the location of groundwater extraction wells sfr reaches and state observations were made to correspond between the higher and lower resolution models four simulated quantities from the complex truth model were assimilated during the first 12 months of the 2 year simulation yielding 48 historic state observations for conditioning 1 one shallow i e layer 1 in the complex model groundwater location gw 1 in fig 1 2 two deep i e layer 3 in the complex model groundwater locations gw 2 and gw 3 in fig 1 3 one surface water location at the stream outlet sw 1 in fig 1 three simulated quantities of interest were defined shown in fig 1 1 an unmeasured groundwater level location in the upgradient hydrologic divide area of the model domain gw forecast in fig 1 2 aggregated surface water groundwater exchange for sfr headwater reaches in rows 1 20 green cells in fig 1 reaches 1 60 in the higher resolution model 3 aggregated surface water groundwater exchange for sfr tailwater reaches in rows 21 40 teal cells in fig 1 reaches 61 120 in the higher resolution model simulated outputs of these quantities were selected to represent common quantities of interest for decision making in applied groundwater modeling these quantities also represent a range of dependence on the historic state observations used for assimilation or put another way these quantities have varying null space dependence moore and doherty 2005 this variability will in test different aspects of the sequential and batch da approaches to cope with model error and resulting parameter compensation and potential predictive bias two parameterization scenarios were developed to evaluate the predictive power of sequential versus batch da the first parameterization scenario assumes all static properties as well as all boundary conditions are uncertain this scenario herein referred to as the coarse scenario makes use of the greatest number and variation of possible receptacles to receive information during the da process and allows for the isolation of model resolution error in comparing the two da approaches we note the complex model used to generate replicates of observations in the paired model analysis also treated all of these quantities as uncertain representing applied groundwater modeling settings the second parameterization scenario inspired by a common approach among practitioners assumes that all static properties and boundary conditions are uncertain except for historic groundwater extraction this scenario herein referred to as the fixed pumping scenario allows for evaluating the effect of model process error along with resolution error as it collapses the uncertainty of groundwater extraction estimates to zero potentially transferring the uncertainty in historic groundwater extraction to the other uncertain model inputs in both scenarios static properties such as horizontal and vertical hydraulic conductivity and confined and unconfined storage were parameterized with a multiscale approach e g mckenna et al 2019 and white et al 2020a using both pilot point e g doherty 2003 and grid scale parameters the recharge and groundwater extraction boundary conditions were parameterized to express both spatial and temporal uncertainty in these model inputs property grid based pilot point and recharge grid based parameters were assigned distance based prior correlations with an exponential variogram with range 500 2000 and 1000 m respectively the sill was set proportional to the variance of the parameter type the prior parameter ensemble used in both the sequential and batch da analyses was generated in pyemu white et al 2016 2021 using spectral simulation see white et al 2020b for more details importantly both the sequential and batch da approaches used the same prior parameter ensemble to undertake sequential da with the above described freyberg model we constructed a single stress period transient modflow 6 model and treated the initial groundwater level for each active model cell for each assimilation cycle as dynamic states that are estimated along with the other uncertain parameters pestpp da was used to advance the sequential assimilation process across the 25 cycles one cycle for each stress period in the batch da approach of the simple model note that only cycles 2 13 have four state observations each to assimilate while cycles 14 25 are open loop cycles where the prior ensemble is evaluated forward in time 4 paired complex simple workflow we extend the deterministic paired complex simple analysis of doherty and christensen 2011 to statistically evaluate the performance of batch and sequential da approaches to cope with model error briefly the paired complex simple analysis begins by completing a monte carlo with the complex model using realizations drawn from the complex model prior ensemble each set of the realized simulation outputs from this monte carlo are then treated as observations for history matching in the simple model herein we refer to these realized complex model simulation results as replicates and we generate 50 replicates for this analysis we extend the paired complex simple analysis to include ensemble based simple model results so that a more complete stochastic understanding of the simple model s behavior can be analyzed in both a prior and posterior stance that is for each of the 50 replicates an ensemble of 50 simple model realizations are propagated through the batch and sequential da algorithms yielding a corresponding prior and posterior probability density function pdf for each simple model output compared to a single value of simple model output in the original paired complex simple analysis by attempting to reproduce each replicate s simulated outputs at locations and times of state observations and also having the corresponding replicate e g correct value for the quantities of interest the empirical paired complex simple analysis allows rigorous statistical insights into how the simple model is able to cope with model error in the context of simulating the quantities of interest the novelty of this extended approach is the inclusion of the stochastic effect of model input uncertainty on predictions enabling a probabilistic rather than deterministic evaluation of bias at only a modest increase in computational cost we present the majority of paired complex simple analysis results in the form of s versus s plots where the 50 complex model replicate simulated quantities correspond to a single value s each on the y axis and the simple model simulated quantities s correspond to 50 realized e g ensemble based x values per complex replicate y value as discussed in detail in doherty and christensen 2011 s versus s plots herein referred to as s plots are useful model error diagnostic plots where a bias in simple model predictions can be clearly detected as deviations from a 1 1 line we plot the simple model simulated values as a point representing the mean and a line extending from the 5 to the 95 confidence intervals of the values to further aid in interpretation we show the results for cycle 13 and 25 both corresponding to the management relevant low flow period and encompassing an assimilation cycle and open loop cycle respectively 5 results 5 1 coarse scenario the coarse scenario involves a structurally simple model with a high dimensional parameterization 3500 adjustable parameters where the source of model error arises from a coarser horizontal and vertical resolution compared to the complex model s plots for the posterior assimilated gw 1 head observation show no appreciable difference in performance between batch and sequential da in the assimilation cycle fig 2a b the sequential da one step ahead forecasts for cycle 13 shown in gray in fig 2b are almost indistinguishable from the posterior values highlighting the capability of sequential da for near term prediction in the open loop cycle the batch da simple model first moments cluster along the 1 1 line fig 2c while the sequential da first moments show a trend with higher simple model head predictions at lower complex replicate values fig 2d this suggests that the sequential algorithm struggled to assimilate observations from simple model replicates with more pronounced declining trends a symptom that becomes exacerbated over time in the open loop cycles the two da approaches perform similarly in their ability to reproduce replicate surface water sw 1 flux predictions during the assimilation period though in cycle 13 the batch da simple model first moments cluster along the 1 1 line and the sequential da posterior first moments are slightly shifted to the right fig 3a b the one step ahead forecast predicts the general slope but overpredicts the magnitude of posterior first moments as with the groundwater assimilation location fig 2d this overprediction is even more pronounced in open loop cycle 25 fig 3d the introduction of a new pumping well in cycle 11 produces a declining trend that slightly degrades the one step ahead predictive ability which is ameliorated by history matching and allowing for model head states to be adjusted fig 3b as the declining trend continues throughout the forecast period so does the predictive performance of sequential da fig 3d with the batch da approach the simple model first moments in the forecast period plot along a moderately steeper slope than the 1 1 line fig 3c however the complex replicate value is almost always contained within the simple model ensemble values predictions for the forecast locations shed light on whether the assimilation process led to erroneous parameter conditioning in space during the assimilation period and space and time during the forecast period for the groundwater forecast location batch and sequential da show similar performance during both the assimilation and open loop cycles fig 4 this result is surprising given the biased open loop predictions made with the sequential approach at the gw 1 assimilation location fig 2d for the headwater sw gw exchange flux forecast location the simple model first moments of both da approaches closely predict the complex replicate value during the assimilation period clustering along the 1 1 line fig 5a b similar to the gw 1 and sw 1 assimilation locations however the sequential approach systematically overpredicts the flux from gw to sw compared to the complex replicate values in the forecast cycle fig 5d note that more negative indicates a higher flux from gw to sw plots of observed versus simulated quantities can shed further light on the history matching and forecast performance of the two da approaches in the coarse scenario we feature the outputs from one replicate fig 6 and refer readers to the supporting information for the observed versus simulated plots for additional replicates and all assimilation and forecast locations fig 6 reinforces the similar performance of batch and sequential da during the history matching period where the posterior ensemble blue lines and dots encompass the complex replicate streamflow at sw 1 in the forecast period second half of the simulation the batch da posterior ensemble encompasses the truth model output while the sequential da prior ensemble generally overpredicts the complex replicate flows compared to batch da overall the coarse scenario results reveal minimal difference in the ability of batch and sequential da to reproduce historical observations and modest differences in long term predictive outcomes particularly with surface water and sw gw exchange fluxes in the presence of model resolution error with respect to short term predictive performance the sequential da one step ahead forecasts closely predict the posterior simulated head quantities at both assimilation and forecast locations with slightly worse predictive performance for streamflow and sw gw exchange quantities 5 2 fixed pumping scenario in the fixed pumping scenario model error arises from both a coarser resolution in the simple compared to the complex model as well as misspecified pumping rates as the complex prior monte carlo varies pumping rates for each replicate and the simple model treats the pumping as a fixed quantity this scenario was designed to reflect a common practice in applied groundwater modeling where pumping is treated as known and not adjustable during data assimilation introducing model process error the two da approaches diverge in their ability to reproduce historical observations in the fixed well scenario for the shallow groundwater observation location a noticeable deviation in slope from the 1 1 line can be seen for batch da fig 7a here the simple model batch da results overestimate lower heads from the complex replicates and underestimate higher heads from the complex replicates reflecting an inability of batch da to reproduce the range in complex replicate head observations for this same observation location sequential da shows a similar bias in the lower replicate head values but is otherwise able to reproduce the majority of replicate head values without bias fig 7b a similar albeit more pronounced pattern can be seen in the open loop cycle fig 7c d notably the batch da simple model results show much larger variance than the sequential da results and variance in the sequential da simple model results follows a decreasing trend with decreasing complex replicate value fig 7c d the larger variance in the batch results occurs because the complex replicate values are in conflict with i e outside the range of the simple model prior ensemble predictions encouraging the batch algorithm to explore more extreme parameter values and combinations to match the observations the sequential approach on the other hand is able to overcome the prior data conflict through updating the head states but is potentially over conditioning the parameters that influence simulated head at gw 1 leading to smaller variances in both the assimilation fig 7b and open loop cycles fig 7d this over conditioning is not consistent across the replicates but increases with decreasing complex replicate value since the lower replicate gw 1 head values are likely driven by higher pumping rates in the complex model these replicate values may reflect a higher degree of model process error between the complex and simple models and as a consequence a higher degree of over conditioning by the sequential algorithm the bias in reproducing historical observations amidst model resolution error and fixed well pumping is more pronounced for the streamflow assimilation location fig 8 the posterior first moments of the simple model realizations differ strongly between the two da approaches with batch da plotting along a steeper regression line driven by an inability to reproduce the upper and lower range of complex replicate streamflow values fig 8a meanwhile sequential da shows an impressive ability to reproduce the complex replicate streamflow quantity fig 8b with first moments falling along the 1 1 line here uncertain head states in the sequential da approach subsume the noise introduced by variable pumping rates in the complex model replicates relative to the fixed pumping amounts in the simple model realizations minimizing bias that would affect the head dependent streamflow fluxes similar to the coarse scenario the sequential approach completely overpredicts streamflow in the open loop cycle fig 8d by not assimilating the long term trend from introducing a new well in cycle 11 the batch da results despite showing strong bias in the slope of the simple model first moments are better able to reproduce the long term trend evidenced by the complex replicate value falling within the wide 95 confidence intervals of the simple model ensembles fig 8d despite the fixed well scenario containing process error along with model resolution error the two approaches again perform equally well for the gw forecast location in both the assimilation and open loop cycles fig 9 one explanation for this is an implicit localizing of parameter compensation through a high degree of spatial parameter resolution such as with grid or pilot point based parameterization here the use of grid scale and pilot point adjustable parameters in the simple models means that parameter compensation from assimilating observations that contain structural noise in one model location does not necessarily permeate the entire domain in other words data assimilation with observations that contained structural noise arising from missing parameters i e variable pumping in the complex model at gw 1 gw 2 and gw 3 did not negatively impact predictions at gw forecast because no data was assimilated in that region of the model fig 1 an alternative yet not mutually exclusive explanation is that the groundwater forecast location is not impacted by the variable pumping in the complex replicates owing to its distant location from the extraction wells making the replicate gw forecast simulated value less marred by the model process error batch da shows substantial predictive bias at the sw gw exchange forecast locations fig 10a c that is the complex replicate simulated value varies widely but the simple model realizations show only small changes in the ensemble first moments leading to steeper slopes in the s plots this is not surprising given that batch da struggled to reproduce historical head fig 7 and sw flux 8 observations in the fixed well scenario as the sw gw exchanges are head dependent fluxes sequential da on the other hand matches the 1 1 slope with both the one step ahead and posterior simulated quantities during the assimilation period despite no sw gw flux observations being assimilated fig 10b this enhanced forecast performance begins to break down in the first open loop cycle and remains that way for the remainder of the simulation fig 10d this is also not surprising given that the sequential da approach updates head predictions with each assimilation cycle since the sw gw exchange flux is a head dependent process close reproduction of heads in the assimilation locations results in close reproduction of the sw gw exchange fluxes but only during the assimilation cycles when head states are being estimated in essence the head assimilation keeps the sequential da process on track in the presence of otherwise corrupting model error the observed versus simulated timeseries results provide an alternate vantage point of the relative performance of batch and sequential da approaches in the assimilation and forecast period using the sw 1 streamflow assimilation location and replicate 12 again as an example see supporting information for additional observed versus simulated replicate plots sequential da outperforms batch da during the assimilation period fig 11b the batch da posterior realizations overpredict the low flows and underpredict the high flows during the assimilation period reflecting the limitation of fixed pumping rates in the simple model for the forecast period of the simulation however sequential da completely overpredicts streamflow compared to the replicate value missing the long term declining trend present in the assimilation period the batch da approach despite the biases in reproducing historical observations is better able to reproduce the long term declining trend with the posterior simple model ensemble predictions encompassing the complex replicate value fig 11a this trade off in performance between short term history matching of sequential da and long term forecasting of batch da can be seen in other replicates with strong nonstationary trends e g figures s158 s165 and s172 in the supporting information 6 discussion our results demonstrate that both da approaches can successfully reproduce historical flow model quantities in the presence of model resolution error when a high dimensional parameterization of uncertain model properties and stresses is considered this is the case despite employing two distinct da algorithms batch da where all observations are assimilated at once and the only source of uncertainty is in the stresses and properties and sequential da where a subset of the observations are assimilated during discrete assimilation cycles and model states along with properties and stresses are considered uncertain and thus adjustable this does not suggest that sequential da does not have greater flexibility to cope with model error but rather that the additional flexibility of state estimation may not be needed when the model error is limited to resolution error and all model properties and stresses are treated as uncertain and adjustable this is evidenced by the infrequent occurrence of prior data conflict in the batch da realizations where a high parameter dimensionality and sufficiently wide uncertainty bounds of adjustable parameters provided enough bandwidth to encompass the range of outputs from each complex replicate the fixed well scenario revealed important limitations to reproducing historical observations when some uncertain model inputs are treated as fixed quantities during the inversion this amounts to model resolution error combined with process error where the complex replicates varied pumping and the simple realizations did not the effect is that the complex replicate simulated quantities contain information related to variable pumping rates that can only be assimilated into the wrong parameters in the simple models oliver and alfonzo 2018 here the state estimation capability of sequential da afforded much better performance over batch da in reproducing historical observations see plot b in figs 7 8 and 10 the lack of adjustable pumping and head states in batch da resulted in numerous occurrences of the complex replicate values falling outside of the simple model ensemble of simulated equivalents particularly at the upper and lower tails of the replicate values it is clear that for operational modeling settings where a groundwater model is used to make one step ahead forecasts the sequential da approach is ideal in that it is tuned to make high frequency forecasts where the initial states dominate predictive accuracy see gray points in plot b of figs 7 8 and 10 in this setting the sequential da approach affords the ability to assimilate recent observations into both states and parameters simultaneously towards making an optimal near term potentially one cycle ahead forecast for future system conditions however the state estimation capability of sequential da does not necessarily translate to better long term forecast performance as demonstrated with the sw gw exchange forecasts sequential da resulted in simple model predictions falling close to the 1 1 line during the assimilation cycle fig 10b but showed near vertical slopes in the open loop cycles fig 10d the promising performance in short term despite no sw gw exchange fluxes being assimilated occurs as a side effect of the state estimation where adjusting the head states to match the complex replicate values resulted in improved head dependent fluxes this benefit is afforded during active assimilation cycles and degrades quickly after the first open loop cycle poor performance in long term forecasting using sequential da is the result of assimilating observations from one monthly cycle at a time the adjustable head states in sequential da treat the time varying observations as independent between assimilation cycles missing the variable amplitude trends present in the complex model outputs unsurprisingly this forecast limitation is exacerbated in the fixed well scenario since the pumping rates are varied in the complex model replicates the resulting water level streamflow and sw gw exchange quantities have varying amplitudes and trends the result is that sequential da often substantially overpredicts heads streamflows and sw gw exchange fluxes in the second year open loop period of the simulation in the simple model realizations fig 11b batch da despite giving poor performance in reproducing historical observations in the fixed well scenario tended to better capture the long term trends fig 11a this long term predictive limitation of sequential da is less pronounced in the coarse scenario where all properties and stresses are considered uncertain fig 6b in that scenario the sequential approach is better able to assimilate the long term trend induced by pumping in the complex model replicate into the adjustable pumping rates in the simple model though the simple model ensembles still overpredict the complex replicate value in the forecast period put simply when the sequential algorithm is able to adjust the pumping rate parameter to reproduce history it is better able to impart a physically based long term trend in the forecast period thus the limitation of only assimilating a subset of observations is mitigated by the parameter conditioning process when all parameters are adjustable 7 implications these results have important implications for groundwater decision support modeling given the ever present trade off between model complexity i e computationally expensive and model error i e computationally cheap hunt and zheng 1999 simmons and hunt 2012 hill 2006 haitjema 2015 too often discussions around groundwater model complexity are derailed by parameterization hesitancy reducing the specter of complexity to number of free parameters as the hallmark of an overly complex model our results suggest the opposite to be true even in a relatively high dimensional free parameter example the effect of removing parameters representing uncertainty in pumping from the inverse analysis noticeably degraded history matching and forecast performance the adjustable head states of the sequential da approach helped to cope with this model error during the history matching and short term forecast period but our results show that representing all sources of model input uncertainty in space and time using a batch da approach yields superior history matching and forecast performance in both the short and long term if constructing a simple decision support model for a groundwater system using a high dimensional parameterization and batch da approach results in an unacceptable amount of model error i e numerous instances of prior data conflict sequential da may prove a useful tool to cope with that error in history matching and near term prediction future work to this end could explore adjusting prior uncertainty bounds on the head states as a form of regularization to capture structural noise while extracting maximum information from the set of observations into model properties and stresses finally while our results point to the potential pitfalls of assimilating a subset of data in predicting nonstationary quantities they also show strong promise for pestpp da to enable one step ahead forecasting in a decision support context the two da approaches in this study represent two ends of an assimilation window spectrum the two year period of batch da at the wide end and the monthly stress period assimilation cycles of sequential da at the narrow end if the decision support objective involves forecasting near to medium term quantities in the presence of nonstationarity using a wider assimilation window e g seasonal with sequential da may provide an optimal compromise for enabling one step ahead prediction while also assimilating low frequency e g trend information in the set of observations future work should explore wider assimilation cycles as a strategy for improving near term predictive performance 8 conclusions this study presents an enhanced paired complex simple approach through the use of ensembles to empirically evaluate the ability of batch and sequential da approaches to history match and forecast in the presence of model resolution error and model resolution and process error the empirical paired complex simple approach and comparative evaluation comprise novel contributions to the practice of groundwater decision support modeling overall we find that both approaches perform well in both history matching and forecasting when employing a high dimensional parameterization stance that is treating all properties and stresses as uncertain and adjustable during the inversion process when uncertain parameters are removed from the inversion process the data assimilation process is degraded in different ways for batch and sequential approaches sequential da is able to effectively reproduce historical observations by subsuming structural noise into the adjustable head states but fails to reproduce the interannual nonstationary trends in the model forecasts due to only assimilating observations one cycle month at a time batch da is less able to match history for the groundwater and streamflow assimilation locations which more or less propagates through to biased forecasts despite this batch da is better able to reproduce long term trends in the assimilation and forecast quantities owing to the wider assimilation window these results have implications for groundwater decision support modeling as they underscore the pitfalls of fixing parameters a priori such as with pumping and present a proof of concept for using adjustable model states to cope with model error in decision support modeling contexts 9 software and data availability following donoho et al 2008 and stodden 2010 and in line with the fair principles the authors have endeavoured to provide all relevant python code and model files for cross platform reproducibility of the complete analysis which is available at https github com kmarkovich intera freyberg da releases tag v2 0 0 the software tool applied herein is available within the open source pest code suite available at https github com usgs pestpp as a c code named pestpp da pre compiled statically linked e g stand alone binaries for windows linux and macos are available as are cmake make and visual studio solutions this analysis also relied heavily on python modules for environmental model uncertainty analyses pyemu an open source python package available through pypi and at https github com pypest pyemu credit authorship contribution statement katherine h markovich conceptualization methodology formal analysis writing visualization jeremy t white conceptualization methodology formal analysis writing visualization matthew j knowling conceptualization methodology writing declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests katherine markovich reports financial support was provided by intera incorporated jeremy white reports financial support was provided by intera incorporated acknowledgments authors khm and jtw were funded by intera incorporated appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2022 105498 appendix a supplementary data the following is the supplementary material related to this article mmc s1 observed vs simulated plots for all replicates at assimilation and forecast locations for both model error scenarios 
25553,groundwater model data assimilation da aims to reduce uncertainty in simulated outcomes of interest to resource management while minimizing the potential for predictive bias sequential da which can estimate model states along with properties and stresses dynamically in time offers a potentially powerful alternative to batch da i e history matching for reducing bias in decision relevant predictions in the presence of incorrect model structure and or processes this study evaluates the ability of batch and sequential da approaches to history match and forecast simulated quantities in the presence of model error using a novel ensemble based paired complex simple approach that enables the incorporation of stochastic uncertainty and a statistical evaluation of predictive bias our findings have implications for groundwater decision support modeling as they underscore the pitfalls of fixing parameters and forcing variables a priori and present a proof of concept for using adjustable model states to cope with model error keywords groundwater decision support modeling data assimilation model error data availability following donoho et al 2008 and stodden 2010 and in line with the fair principles the authors have endeavoured to provide all relevant python code and model files for cross platform reproducibility of the complete analysis which is available at https github com kmarkovich intera freyberg da releases tag v2 0 0 the software tool applied herein is available within the open source pest code suite available at https github com usgs pestpp as a c code named pestpp da pre compiled statically linked e g stand alone binaries for windows linux and macos are available as are cmake make and visual studio solutions this analysis also relied heavily on python modules for environmental model uncertainty analyses pyemu an open source python package available through pypi and at https github com pypest pyemu 1 introduction groundwater models are by necessity and by design a simplified representation of the infinitely complex real world hydrogeological systems this simplification and abstraction may arise from issues of spatial or temporal scale where the discretization of a modeled process is too coarse to capture fine scale governing dynamics or the upscaling of properties smooths out temporal and or spatial heterogeneity or the simplification and abstraction may refer to mathematical representation of physical processes whether they be physically motivated or empirical schemes the cumulative effect of these well documented limitations in groundwater models is referred to as model error or model misspecification and this discrepancy gives rise to structural noise in the simulated results doherty and welter 2010 many workers have sought to address the degree to which this abstraction and simplification produces deleterious effects on predictions see gupta et al 2012 white et al 2014 and references therein the primary consequences of using simplified models for making predictions are parameter compensation clark and vrugt 2006a doherty and christensen 2011 and resulting potential for predictive bias parameter compensation describes the tendency for parameters or parameter components which are not sensitive to the information content of the observations i e null space parameters to be erroneously adjusted during model calibration doherty and christensen 2011 watson et al 2013 this occurs because the model structure including discretization and process representation is not sufficient to represent the information contained in the assimilated observations and signal components corresponding to this insufficiency in the observations are incorrectly assimilated in many cases the propensity for parameter compensation may be visible as prior data conflict evans and moshonov 2006 prior data conflict arises from the simple model s inability to simulate the proper range and or characteristics of the hydrologic state at a given observation location and or time for predictions of management interest that are sensitive to these erroneously adjusted parameters parameter compensation leads to predictive bias which compromises the model s fidelity for decision making despite undergoing rigorous calibration and uncertainty analysis particularly concerning is that parameter compensation is often invisible to the modeler white et al 2014 knowling et al 2019 a recent evolution in how best to undertake data assimilation da for decision support modeling is the idea that models should strive for an appropriate balance of simplicity complexity doherty and moore 2020 hunt et al 2007 that is they should be simple enough to be computational cheap and numerically stable yet possess appropriately high dimensional and diverse parameterization to receive information contained in observations and or to incorporate appropriate stochasticity and uncertainty in decision relevant predictions this push for a increased level of parameterization has been enabled by the advent of ensemble based methods for inverse problems which effectively eliminate the computational constraint on free parameters in estimating the jacobian chen and oliver 2013 white 2018 white et al 2021 with enough spatial coverage and diverse types of parameters the relative influence of null space components on any one type of parameter or location in the model is minimized while the ability for various parameter types distributed across the domain to receive information from observations is enhanced the success of this model simplification approach defined here as computational savings and uncertainty reduction achieved with minimal predictive consequences depends on the type of decision relevant prediction and form of simplification for example knowling et al 2019 showed that a reduction in horizontal parameter resolution produced generally similar first and second moments of the flow and spatially integrated transport predictions compared to a highly parameterized model they found that this did not hold for concentration predictions which are known to be sensitive to local scale properties and stresses similarly white et al 2020c and knowling et al 2020 showed that reducing resolution in the vertical direction results in acceptable flow model predictions but pronounced biases in transport predictions moore and doherty 2021 explored the consequences of only using steady state calibration to reduce predictive uncertainty and found that it effectively reduced uncertainty for heads predictions via conditioning of the hydraulic conductivity field near data assimilation locations but that artificiality of the steady state condition and error incurred through observation averaging led to substantial predictive biases elsewhere in the model domain that the conclusion of each of these studies is it depends suggests that we are far from a generalizable da approach for balancing model complexity uncertainty reduction and computational efficiency across the range of decision support modeling contexts to date the majority of groundwater modeling studies that perform da for parameter estimation and uncertainty analysis assimilate data in a batch fashion that is the groundwater model is run to completion through the entire historic period and all observations are assimilated at once post hoc for the purposes of this study we refer to this approach as batch da but note that it is identical in practice to history matching and calibration the inherent assumption and algorithmic requirement of batch da is that the model structure and processes are correct and the only sources of model input uncertainty or receptacles for information are the unknown properties and stresses a major advantage of batch da is the ability to assimilate long term low frequency information since all observations are assimilated at once however only allowing properties and stresses to be uncertain introduces the potential for parameter compensation in batch da through the assimilation of structural noise generated from model error an alternative to batch da and potential strategy for coping with error in groundwater modeling decision support exists sequential da evensen 1994 sequential da most commonly implemented in hydrogeologic applications as kalman filtering methods allows for a more flexible and dynamic approach to assimilation and forecast frequency ranging from near real time hendricks franssen and kinzelbach 2008 hendricks franssen et al 2011 to seasonal assimilation and forecast cycles huang et al 2017 a key functionality provided by sequential da methods is the ability to estimate system state along with properties and stresses at user defined temporal frequencies within a sequential da framework the da algorithm takes control of the simulated states to advance the model through time whereas in batch da the model itself controls the simulated states and advances through time implicit in state estimation is that states are uncertain and this state uncertainty is a more flexible and appropriate receptacle for structural noise contained in observations i e simulated water levels are a more appropriate receptacle for missing signal components in water level observations than hydraulic conductivity or porosity sequential da methods can therefore reduce the burden of structural noise on static properties by assimilating this noise into the states through the sequential da process clark and vrugt 2006b kim et al 2021 put bluntly batch da may never be completely free of predictive bias due to being tied to an imperfect model structure regardless of the level of complexity sequential da offers a potentially powerful strategy for reducing bias in decision relevant predictions in the presence of model error in this work we evaluate the outcomes of using batch and sequential da approaches in a hypothetical case where a simple model structure is in error with respect to the more complex truth model from which the observations available for assimilation are generated this evaluation of batch and sequential da approaches in the presence of model error is an important and novel contribution to the practice of groundwater decision support modeling additionally this work advances the powerful paired complex simple analysis of doherty and christensen 2011 into an empirical paired complex simple analysis to enable the incorporation of stochastic uncertainty and a statistical evaluation of predictive bias the model error in this empirical paired complex simple analysis is explicitly represented by two scenarios one where the simple model is of coarser spatial both vertical and horizontal and temporal resolution compared to the complex model the other scenario builds on the reduced resolution and includes the additive effect of model process error by treating historic groundwater pumping as a fixed known quantity rather than an uncertain one performance of the two da approaches are evaluated through a comparison of simple models ability to history match and forecast decision relevant predictions we hypothesize that the joint parameter state estimation ability of sequential da is able to better cope with model error by subsuming structural noise that would otherwise be absorbed by static properties in batch da 2 methods we evaluate the sequential and batch da strategies using pestpp da alzraiee et al 2021 a newly developed generic da tool to join the suite of modeling decision support and uncertainty analysis programs in pest white et al 2021 full details on the theory implementation and capabilities of pestpp da can be found in alzraiee et al 2021 and we provide a brief overview here pestpp da can assimilate data in both batch and sequential modes where for sequential da the model simulation is divided into user defined cycles or a period of time for which applied forcings i e boundary conditions and parameters are constant and or discrete observations are available for assimilation the modflow stress period concept which naturally lends itself to the cycle concept in pestpp da allows for a high level of flexibility for designing the assimilation cycle length and frequency pestpp da can be run in three modes state estimation parameter estimation and the focus of this study joint state parameter estimation in joint state parameter estimation mode model states e g heads are considered uncertain along with model properties and forcings e g hydrogeologic properties and boundary condition fluxes each cycle has an estimated initial state and a simulated final state where the final state becomes the initial state for the subsequent cycle importantly during each cycle the model is run using the estimated initial state prior to data assimilation and the simulated output of that initial run is referred to as a one step ahead one step ahead forecast this forecast is significant because it reveals the near term predictive power of sequential da where the posterior state from the previous assimilation cycle combined with model physics through a forward run can provide a simulated quantity that closely predicts observed quantities prior to any data assimilation as with other programs of the pest suite pestpp da follows a bayesian framework where a prior distribution is defined for all uncertain parameters based on field measurements expert knowledge and the literature within the solution scheme of pestpp da the likelihood is proportional to the inverse of a weighted l 2 norm of innovations e g the objective function which represents the degree of agreement between observations and their simulated equivalents for each assimilation cycle pestpp da updates all parameters and states if doing joint state parameter estimation with the option of using a standard kalman update an iterative kalman update known as multiple data assimilation mda emerick and reynolds 2013 or an iterative ensemble smoother based on the gauss levenberg marquardt scheme chen and oliver 2013 white 2018 the posterior ensembles of parameters and predictions represent uncertainty in these model components after da see alzraiee et al 2021 for a complete discussion of the pestpp da solution schemes importantly both iterative solution schemes involve the use of ensembles to approximate the cross covariance between parameters and states and simulated equivalents to observations worth adding however is that the sequential nature of pestpp da if running in sequential mode means that the iterative solution schemes are applied for each user defined assimilation cycle increasing the computational cost compared to batch assimilation using iterative ensemble methods though still more efficient than the traditional finite difference jacobian of gauss levenberg marquardt glm algorithms doherty 2003 white et al 2020b for high dimensional inverse problems 3 model description we perform the paired complex simple model analysis using a version of the freyberg freyberg 1988 hunt et al 2020 model originally published in white et al 2020b the model which is implemented in modflow 6 langevin et al 2017 hughes et al 2017 is a transient multi layer model that includes groundwater and surface water processes model error is explicitly represented as resolution error by assimilating output from a finer spatial resolution daily stress period version of the freyberg model into a coarser resolution monthly stress period version of the freyberg model following white et al 2021 this is not unlike the situation of all groundwater models which are invariably a simplification of the resolution and structure of real world processes that generate our observations the lower resolution simple model has 40 rows 20 columns 1 layer and 25 stress periods the first stress period is 10 000 days in length and represents a pre development condition there is not groundwater extraction during the first stress period the subsequent 24 stress periods are discretized into monthly periods and include time varying boundary conditions representing cyclical recharge and pumping over the two year period observations for assimilation are available during the first year e g stress periods 2 13 and include two deep and one shallow groundwater level observation locations and one surface water gage location fig 1 the last 12 stress periods represent an unmeasured forecast period groundwater extraction occurs in six wells scattered throughout the domain for the entire simulation period and a seventh extraction well new well in fig 1 is added in stress period 11 to impart a nonstationary trend in the forecast period the higher resolution complex model mimics the lower resolution model in properties and boundary conditions differing only in spatial and temporal resolution it has 120 rows 60 columns and 3 layers three times as many of each as the lower resolution model and with the exception of the same initial 10 000 day pre development stress period as in the simple model the complex model simulates daily stress periods over the two year simulation length for a total of 731 stress periods the location of groundwater extraction wells sfr reaches and state observations were made to correspond between the higher and lower resolution models four simulated quantities from the complex truth model were assimilated during the first 12 months of the 2 year simulation yielding 48 historic state observations for conditioning 1 one shallow i e layer 1 in the complex model groundwater location gw 1 in fig 1 2 two deep i e layer 3 in the complex model groundwater locations gw 2 and gw 3 in fig 1 3 one surface water location at the stream outlet sw 1 in fig 1 three simulated quantities of interest were defined shown in fig 1 1 an unmeasured groundwater level location in the upgradient hydrologic divide area of the model domain gw forecast in fig 1 2 aggregated surface water groundwater exchange for sfr headwater reaches in rows 1 20 green cells in fig 1 reaches 1 60 in the higher resolution model 3 aggregated surface water groundwater exchange for sfr tailwater reaches in rows 21 40 teal cells in fig 1 reaches 61 120 in the higher resolution model simulated outputs of these quantities were selected to represent common quantities of interest for decision making in applied groundwater modeling these quantities also represent a range of dependence on the historic state observations used for assimilation or put another way these quantities have varying null space dependence moore and doherty 2005 this variability will in test different aspects of the sequential and batch da approaches to cope with model error and resulting parameter compensation and potential predictive bias two parameterization scenarios were developed to evaluate the predictive power of sequential versus batch da the first parameterization scenario assumes all static properties as well as all boundary conditions are uncertain this scenario herein referred to as the coarse scenario makes use of the greatest number and variation of possible receptacles to receive information during the da process and allows for the isolation of model resolution error in comparing the two da approaches we note the complex model used to generate replicates of observations in the paired model analysis also treated all of these quantities as uncertain representing applied groundwater modeling settings the second parameterization scenario inspired by a common approach among practitioners assumes that all static properties and boundary conditions are uncertain except for historic groundwater extraction this scenario herein referred to as the fixed pumping scenario allows for evaluating the effect of model process error along with resolution error as it collapses the uncertainty of groundwater extraction estimates to zero potentially transferring the uncertainty in historic groundwater extraction to the other uncertain model inputs in both scenarios static properties such as horizontal and vertical hydraulic conductivity and confined and unconfined storage were parameterized with a multiscale approach e g mckenna et al 2019 and white et al 2020a using both pilot point e g doherty 2003 and grid scale parameters the recharge and groundwater extraction boundary conditions were parameterized to express both spatial and temporal uncertainty in these model inputs property grid based pilot point and recharge grid based parameters were assigned distance based prior correlations with an exponential variogram with range 500 2000 and 1000 m respectively the sill was set proportional to the variance of the parameter type the prior parameter ensemble used in both the sequential and batch da analyses was generated in pyemu white et al 2016 2021 using spectral simulation see white et al 2020b for more details importantly both the sequential and batch da approaches used the same prior parameter ensemble to undertake sequential da with the above described freyberg model we constructed a single stress period transient modflow 6 model and treated the initial groundwater level for each active model cell for each assimilation cycle as dynamic states that are estimated along with the other uncertain parameters pestpp da was used to advance the sequential assimilation process across the 25 cycles one cycle for each stress period in the batch da approach of the simple model note that only cycles 2 13 have four state observations each to assimilate while cycles 14 25 are open loop cycles where the prior ensemble is evaluated forward in time 4 paired complex simple workflow we extend the deterministic paired complex simple analysis of doherty and christensen 2011 to statistically evaluate the performance of batch and sequential da approaches to cope with model error briefly the paired complex simple analysis begins by completing a monte carlo with the complex model using realizations drawn from the complex model prior ensemble each set of the realized simulation outputs from this monte carlo are then treated as observations for history matching in the simple model herein we refer to these realized complex model simulation results as replicates and we generate 50 replicates for this analysis we extend the paired complex simple analysis to include ensemble based simple model results so that a more complete stochastic understanding of the simple model s behavior can be analyzed in both a prior and posterior stance that is for each of the 50 replicates an ensemble of 50 simple model realizations are propagated through the batch and sequential da algorithms yielding a corresponding prior and posterior probability density function pdf for each simple model output compared to a single value of simple model output in the original paired complex simple analysis by attempting to reproduce each replicate s simulated outputs at locations and times of state observations and also having the corresponding replicate e g correct value for the quantities of interest the empirical paired complex simple analysis allows rigorous statistical insights into how the simple model is able to cope with model error in the context of simulating the quantities of interest the novelty of this extended approach is the inclusion of the stochastic effect of model input uncertainty on predictions enabling a probabilistic rather than deterministic evaluation of bias at only a modest increase in computational cost we present the majority of paired complex simple analysis results in the form of s versus s plots where the 50 complex model replicate simulated quantities correspond to a single value s each on the y axis and the simple model simulated quantities s correspond to 50 realized e g ensemble based x values per complex replicate y value as discussed in detail in doherty and christensen 2011 s versus s plots herein referred to as s plots are useful model error diagnostic plots where a bias in simple model predictions can be clearly detected as deviations from a 1 1 line we plot the simple model simulated values as a point representing the mean and a line extending from the 5 to the 95 confidence intervals of the values to further aid in interpretation we show the results for cycle 13 and 25 both corresponding to the management relevant low flow period and encompassing an assimilation cycle and open loop cycle respectively 5 results 5 1 coarse scenario the coarse scenario involves a structurally simple model with a high dimensional parameterization 3500 adjustable parameters where the source of model error arises from a coarser horizontal and vertical resolution compared to the complex model s plots for the posterior assimilated gw 1 head observation show no appreciable difference in performance between batch and sequential da in the assimilation cycle fig 2a b the sequential da one step ahead forecasts for cycle 13 shown in gray in fig 2b are almost indistinguishable from the posterior values highlighting the capability of sequential da for near term prediction in the open loop cycle the batch da simple model first moments cluster along the 1 1 line fig 2c while the sequential da first moments show a trend with higher simple model head predictions at lower complex replicate values fig 2d this suggests that the sequential algorithm struggled to assimilate observations from simple model replicates with more pronounced declining trends a symptom that becomes exacerbated over time in the open loop cycles the two da approaches perform similarly in their ability to reproduce replicate surface water sw 1 flux predictions during the assimilation period though in cycle 13 the batch da simple model first moments cluster along the 1 1 line and the sequential da posterior first moments are slightly shifted to the right fig 3a b the one step ahead forecast predicts the general slope but overpredicts the magnitude of posterior first moments as with the groundwater assimilation location fig 2d this overprediction is even more pronounced in open loop cycle 25 fig 3d the introduction of a new pumping well in cycle 11 produces a declining trend that slightly degrades the one step ahead predictive ability which is ameliorated by history matching and allowing for model head states to be adjusted fig 3b as the declining trend continues throughout the forecast period so does the predictive performance of sequential da fig 3d with the batch da approach the simple model first moments in the forecast period plot along a moderately steeper slope than the 1 1 line fig 3c however the complex replicate value is almost always contained within the simple model ensemble values predictions for the forecast locations shed light on whether the assimilation process led to erroneous parameter conditioning in space during the assimilation period and space and time during the forecast period for the groundwater forecast location batch and sequential da show similar performance during both the assimilation and open loop cycles fig 4 this result is surprising given the biased open loop predictions made with the sequential approach at the gw 1 assimilation location fig 2d for the headwater sw gw exchange flux forecast location the simple model first moments of both da approaches closely predict the complex replicate value during the assimilation period clustering along the 1 1 line fig 5a b similar to the gw 1 and sw 1 assimilation locations however the sequential approach systematically overpredicts the flux from gw to sw compared to the complex replicate values in the forecast cycle fig 5d note that more negative indicates a higher flux from gw to sw plots of observed versus simulated quantities can shed further light on the history matching and forecast performance of the two da approaches in the coarse scenario we feature the outputs from one replicate fig 6 and refer readers to the supporting information for the observed versus simulated plots for additional replicates and all assimilation and forecast locations fig 6 reinforces the similar performance of batch and sequential da during the history matching period where the posterior ensemble blue lines and dots encompass the complex replicate streamflow at sw 1 in the forecast period second half of the simulation the batch da posterior ensemble encompasses the truth model output while the sequential da prior ensemble generally overpredicts the complex replicate flows compared to batch da overall the coarse scenario results reveal minimal difference in the ability of batch and sequential da to reproduce historical observations and modest differences in long term predictive outcomes particularly with surface water and sw gw exchange fluxes in the presence of model resolution error with respect to short term predictive performance the sequential da one step ahead forecasts closely predict the posterior simulated head quantities at both assimilation and forecast locations with slightly worse predictive performance for streamflow and sw gw exchange quantities 5 2 fixed pumping scenario in the fixed pumping scenario model error arises from both a coarser resolution in the simple compared to the complex model as well as misspecified pumping rates as the complex prior monte carlo varies pumping rates for each replicate and the simple model treats the pumping as a fixed quantity this scenario was designed to reflect a common practice in applied groundwater modeling where pumping is treated as known and not adjustable during data assimilation introducing model process error the two da approaches diverge in their ability to reproduce historical observations in the fixed well scenario for the shallow groundwater observation location a noticeable deviation in slope from the 1 1 line can be seen for batch da fig 7a here the simple model batch da results overestimate lower heads from the complex replicates and underestimate higher heads from the complex replicates reflecting an inability of batch da to reproduce the range in complex replicate head observations for this same observation location sequential da shows a similar bias in the lower replicate head values but is otherwise able to reproduce the majority of replicate head values without bias fig 7b a similar albeit more pronounced pattern can be seen in the open loop cycle fig 7c d notably the batch da simple model results show much larger variance than the sequential da results and variance in the sequential da simple model results follows a decreasing trend with decreasing complex replicate value fig 7c d the larger variance in the batch results occurs because the complex replicate values are in conflict with i e outside the range of the simple model prior ensemble predictions encouraging the batch algorithm to explore more extreme parameter values and combinations to match the observations the sequential approach on the other hand is able to overcome the prior data conflict through updating the head states but is potentially over conditioning the parameters that influence simulated head at gw 1 leading to smaller variances in both the assimilation fig 7b and open loop cycles fig 7d this over conditioning is not consistent across the replicates but increases with decreasing complex replicate value since the lower replicate gw 1 head values are likely driven by higher pumping rates in the complex model these replicate values may reflect a higher degree of model process error between the complex and simple models and as a consequence a higher degree of over conditioning by the sequential algorithm the bias in reproducing historical observations amidst model resolution error and fixed well pumping is more pronounced for the streamflow assimilation location fig 8 the posterior first moments of the simple model realizations differ strongly between the two da approaches with batch da plotting along a steeper regression line driven by an inability to reproduce the upper and lower range of complex replicate streamflow values fig 8a meanwhile sequential da shows an impressive ability to reproduce the complex replicate streamflow quantity fig 8b with first moments falling along the 1 1 line here uncertain head states in the sequential da approach subsume the noise introduced by variable pumping rates in the complex model replicates relative to the fixed pumping amounts in the simple model realizations minimizing bias that would affect the head dependent streamflow fluxes similar to the coarse scenario the sequential approach completely overpredicts streamflow in the open loop cycle fig 8d by not assimilating the long term trend from introducing a new well in cycle 11 the batch da results despite showing strong bias in the slope of the simple model first moments are better able to reproduce the long term trend evidenced by the complex replicate value falling within the wide 95 confidence intervals of the simple model ensembles fig 8d despite the fixed well scenario containing process error along with model resolution error the two approaches again perform equally well for the gw forecast location in both the assimilation and open loop cycles fig 9 one explanation for this is an implicit localizing of parameter compensation through a high degree of spatial parameter resolution such as with grid or pilot point based parameterization here the use of grid scale and pilot point adjustable parameters in the simple models means that parameter compensation from assimilating observations that contain structural noise in one model location does not necessarily permeate the entire domain in other words data assimilation with observations that contained structural noise arising from missing parameters i e variable pumping in the complex model at gw 1 gw 2 and gw 3 did not negatively impact predictions at gw forecast because no data was assimilated in that region of the model fig 1 an alternative yet not mutually exclusive explanation is that the groundwater forecast location is not impacted by the variable pumping in the complex replicates owing to its distant location from the extraction wells making the replicate gw forecast simulated value less marred by the model process error batch da shows substantial predictive bias at the sw gw exchange forecast locations fig 10a c that is the complex replicate simulated value varies widely but the simple model realizations show only small changes in the ensemble first moments leading to steeper slopes in the s plots this is not surprising given that batch da struggled to reproduce historical head fig 7 and sw flux 8 observations in the fixed well scenario as the sw gw exchanges are head dependent fluxes sequential da on the other hand matches the 1 1 slope with both the one step ahead and posterior simulated quantities during the assimilation period despite no sw gw flux observations being assimilated fig 10b this enhanced forecast performance begins to break down in the first open loop cycle and remains that way for the remainder of the simulation fig 10d this is also not surprising given that the sequential da approach updates head predictions with each assimilation cycle since the sw gw exchange flux is a head dependent process close reproduction of heads in the assimilation locations results in close reproduction of the sw gw exchange fluxes but only during the assimilation cycles when head states are being estimated in essence the head assimilation keeps the sequential da process on track in the presence of otherwise corrupting model error the observed versus simulated timeseries results provide an alternate vantage point of the relative performance of batch and sequential da approaches in the assimilation and forecast period using the sw 1 streamflow assimilation location and replicate 12 again as an example see supporting information for additional observed versus simulated replicate plots sequential da outperforms batch da during the assimilation period fig 11b the batch da posterior realizations overpredict the low flows and underpredict the high flows during the assimilation period reflecting the limitation of fixed pumping rates in the simple model for the forecast period of the simulation however sequential da completely overpredicts streamflow compared to the replicate value missing the long term declining trend present in the assimilation period the batch da approach despite the biases in reproducing historical observations is better able to reproduce the long term declining trend with the posterior simple model ensemble predictions encompassing the complex replicate value fig 11a this trade off in performance between short term history matching of sequential da and long term forecasting of batch da can be seen in other replicates with strong nonstationary trends e g figures s158 s165 and s172 in the supporting information 6 discussion our results demonstrate that both da approaches can successfully reproduce historical flow model quantities in the presence of model resolution error when a high dimensional parameterization of uncertain model properties and stresses is considered this is the case despite employing two distinct da algorithms batch da where all observations are assimilated at once and the only source of uncertainty is in the stresses and properties and sequential da where a subset of the observations are assimilated during discrete assimilation cycles and model states along with properties and stresses are considered uncertain and thus adjustable this does not suggest that sequential da does not have greater flexibility to cope with model error but rather that the additional flexibility of state estimation may not be needed when the model error is limited to resolution error and all model properties and stresses are treated as uncertain and adjustable this is evidenced by the infrequent occurrence of prior data conflict in the batch da realizations where a high parameter dimensionality and sufficiently wide uncertainty bounds of adjustable parameters provided enough bandwidth to encompass the range of outputs from each complex replicate the fixed well scenario revealed important limitations to reproducing historical observations when some uncertain model inputs are treated as fixed quantities during the inversion this amounts to model resolution error combined with process error where the complex replicates varied pumping and the simple realizations did not the effect is that the complex replicate simulated quantities contain information related to variable pumping rates that can only be assimilated into the wrong parameters in the simple models oliver and alfonzo 2018 here the state estimation capability of sequential da afforded much better performance over batch da in reproducing historical observations see plot b in figs 7 8 and 10 the lack of adjustable pumping and head states in batch da resulted in numerous occurrences of the complex replicate values falling outside of the simple model ensemble of simulated equivalents particularly at the upper and lower tails of the replicate values it is clear that for operational modeling settings where a groundwater model is used to make one step ahead forecasts the sequential da approach is ideal in that it is tuned to make high frequency forecasts where the initial states dominate predictive accuracy see gray points in plot b of figs 7 8 and 10 in this setting the sequential da approach affords the ability to assimilate recent observations into both states and parameters simultaneously towards making an optimal near term potentially one cycle ahead forecast for future system conditions however the state estimation capability of sequential da does not necessarily translate to better long term forecast performance as demonstrated with the sw gw exchange forecasts sequential da resulted in simple model predictions falling close to the 1 1 line during the assimilation cycle fig 10b but showed near vertical slopes in the open loop cycles fig 10d the promising performance in short term despite no sw gw exchange fluxes being assimilated occurs as a side effect of the state estimation where adjusting the head states to match the complex replicate values resulted in improved head dependent fluxes this benefit is afforded during active assimilation cycles and degrades quickly after the first open loop cycle poor performance in long term forecasting using sequential da is the result of assimilating observations from one monthly cycle at a time the adjustable head states in sequential da treat the time varying observations as independent between assimilation cycles missing the variable amplitude trends present in the complex model outputs unsurprisingly this forecast limitation is exacerbated in the fixed well scenario since the pumping rates are varied in the complex model replicates the resulting water level streamflow and sw gw exchange quantities have varying amplitudes and trends the result is that sequential da often substantially overpredicts heads streamflows and sw gw exchange fluxes in the second year open loop period of the simulation in the simple model realizations fig 11b batch da despite giving poor performance in reproducing historical observations in the fixed well scenario tended to better capture the long term trends fig 11a this long term predictive limitation of sequential da is less pronounced in the coarse scenario where all properties and stresses are considered uncertain fig 6b in that scenario the sequential approach is better able to assimilate the long term trend induced by pumping in the complex model replicate into the adjustable pumping rates in the simple model though the simple model ensembles still overpredict the complex replicate value in the forecast period put simply when the sequential algorithm is able to adjust the pumping rate parameter to reproduce history it is better able to impart a physically based long term trend in the forecast period thus the limitation of only assimilating a subset of observations is mitigated by the parameter conditioning process when all parameters are adjustable 7 implications these results have important implications for groundwater decision support modeling given the ever present trade off between model complexity i e computationally expensive and model error i e computationally cheap hunt and zheng 1999 simmons and hunt 2012 hill 2006 haitjema 2015 too often discussions around groundwater model complexity are derailed by parameterization hesitancy reducing the specter of complexity to number of free parameters as the hallmark of an overly complex model our results suggest the opposite to be true even in a relatively high dimensional free parameter example the effect of removing parameters representing uncertainty in pumping from the inverse analysis noticeably degraded history matching and forecast performance the adjustable head states of the sequential da approach helped to cope with this model error during the history matching and short term forecast period but our results show that representing all sources of model input uncertainty in space and time using a batch da approach yields superior history matching and forecast performance in both the short and long term if constructing a simple decision support model for a groundwater system using a high dimensional parameterization and batch da approach results in an unacceptable amount of model error i e numerous instances of prior data conflict sequential da may prove a useful tool to cope with that error in history matching and near term prediction future work to this end could explore adjusting prior uncertainty bounds on the head states as a form of regularization to capture structural noise while extracting maximum information from the set of observations into model properties and stresses finally while our results point to the potential pitfalls of assimilating a subset of data in predicting nonstationary quantities they also show strong promise for pestpp da to enable one step ahead forecasting in a decision support context the two da approaches in this study represent two ends of an assimilation window spectrum the two year period of batch da at the wide end and the monthly stress period assimilation cycles of sequential da at the narrow end if the decision support objective involves forecasting near to medium term quantities in the presence of nonstationarity using a wider assimilation window e g seasonal with sequential da may provide an optimal compromise for enabling one step ahead prediction while also assimilating low frequency e g trend information in the set of observations future work should explore wider assimilation cycles as a strategy for improving near term predictive performance 8 conclusions this study presents an enhanced paired complex simple approach through the use of ensembles to empirically evaluate the ability of batch and sequential da approaches to history match and forecast in the presence of model resolution error and model resolution and process error the empirical paired complex simple approach and comparative evaluation comprise novel contributions to the practice of groundwater decision support modeling overall we find that both approaches perform well in both history matching and forecasting when employing a high dimensional parameterization stance that is treating all properties and stresses as uncertain and adjustable during the inversion process when uncertain parameters are removed from the inversion process the data assimilation process is degraded in different ways for batch and sequential approaches sequential da is able to effectively reproduce historical observations by subsuming structural noise into the adjustable head states but fails to reproduce the interannual nonstationary trends in the model forecasts due to only assimilating observations one cycle month at a time batch da is less able to match history for the groundwater and streamflow assimilation locations which more or less propagates through to biased forecasts despite this batch da is better able to reproduce long term trends in the assimilation and forecast quantities owing to the wider assimilation window these results have implications for groundwater decision support modeling as they underscore the pitfalls of fixing parameters a priori such as with pumping and present a proof of concept for using adjustable model states to cope with model error in decision support modeling contexts 9 software and data availability following donoho et al 2008 and stodden 2010 and in line with the fair principles the authors have endeavoured to provide all relevant python code and model files for cross platform reproducibility of the complete analysis which is available at https github com kmarkovich intera freyberg da releases tag v2 0 0 the software tool applied herein is available within the open source pest code suite available at https github com usgs pestpp as a c code named pestpp da pre compiled statically linked e g stand alone binaries for windows linux and macos are available as are cmake make and visual studio solutions this analysis also relied heavily on python modules for environmental model uncertainty analyses pyemu an open source python package available through pypi and at https github com pypest pyemu credit authorship contribution statement katherine h markovich conceptualization methodology formal analysis writing visualization jeremy t white conceptualization methodology formal analysis writing visualization matthew j knowling conceptualization methodology writing declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests katherine markovich reports financial support was provided by intera incorporated jeremy white reports financial support was provided by intera incorporated acknowledgments authors khm and jtw were funded by intera incorporated appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2022 105498 appendix a supplementary data the following is the supplementary material related to this article mmc s1 observed vs simulated plots for all replicates at assimilation and forecast locations for both model error scenarios 
25554,despite their frequent use there are few simple and readily accessible tools to help guide the logistical planning of tracer injections in streams and rivers we combined the widely used advection dispersion reaction equation peak concentration estimates based on a meta analysis of hundreds of tracer injections carried out in streams and rivers and simple mass balances in a dynamic excel workbook to 1 help users decide how much tracer mass should be added to achieve a specific dynamic concentration range that reduces known issues associated with breakthrough curve tail truncation and 2 generate tables and graphs that can be readily used to plan the deployment of resources our tracer injection planning tool tipt handles instantaneous and continuous tracer injections and assumes steady state and uniform flow conditions as well as first order decay or production while those assumptions do not strictly apply to natural streams and rivers they help simplify the planning of tracer injections with a predictive ability that is disproportionally favorable with respect to the few inputs required tipt is a versatile user friendly and graphical tool that can help design tracer injections and solute transport experiments that are more easily replicated within and across sites thus tipt contributes directly to advancing integrated coordinated open and networked icon principles similarly tipt can help generate datasets that more closely follow findable accessible interoperable and reusable fair principles we demonstrate the use of tipt through two case studies featuring 1 a continuous injection in a 2nd order stream and 2 an instantaneous injection in a 7th order stream keywords solute transport tracer injection streams rivers instantaneous injection continuous injection data availability no data was used for the research described in the article 1 introduction tracer injections in streams and rivers are commonly used to characterize physical and biochemical processes undergone by solutes and micro to nano particles transported in fluvial networks covino et al 2010 foppen et al 2011 gonzález pinzón et al 2015 drummond et al 2017 knapp et al 2018 these techniques help us quantify and visualize a variety of processes controlling the fate and transport of solutes physical processes including advection dispersion and transient storage biological processes including uptake decay and production and chemical processes including sorption decay and production stream solute workshop 1990 abbott et al 2016 knapp et al 2017 despite their wide use in research and consulting in hydrology environmental engineering and earth and aquatic sciences there are few simple and readily accessible tools to help guide the logistical planning of tracer injections in streams and rivers consequently tracer studies are not easily replicated i e different users might end up choosing considerably different injection and sampling durations target concentrations sampling frequencies and longitudinal extent even if they have a similar goal gonzález pinzón et al 2015 schmadel et al 2016 moreover the lack of optimal sampling frequencies and the anticipation of arrival and passage times may lead to suboptimal decisions about personnel and equipment supply logistics which ultimately reduce the quality and quantity of the information retrieved from tracer injections paradoxically the perfect logistical planning of a tracer injection would require carrying it out first and repeating it under the exact same physical and biochemical conditions harvey and gooseff 2015 harvey et al 1996 which is impossible here we propose to combine 1 the widely used advection dispersion reaction equation 2 a meta analysis of hundreds of tracer injections carried out in streams and rivers spanning four orders of magnitude in discharge and two in size which was conducted by jobson 1996 and 3 simple mass balances to more objectively plan the logistics of tracer injections in streams and rivers we programmed an excel workbook to offer a readily accessible product that can be used on multiple operating systems and devices using basic information about the study site supplemental information 1 the tracer injection planning tool tipt handles instantaneous and continuous tracer injections to 1 help users decide how much tracer mass should be added to achieve a specific dynamic concentration range i e the ratio between the maximum and minimum tracer concentrations measured that reduces known issues associated with breakthrough curve tail truncation drummond et al 2012 and 2 generate tables and graphs that can be readily used to plan the deployment of resources 2 methods 2 1 the advection dispersion equation in tipt we use the advection dispersion equation ade with first order decay or production to guide the logistical planning of tracer injections because it has analytical solutions for instantaneous and continuous injections of solutes chapra 2008 these analytical equations generate exact solutions that are free of numerical dispersion and stability issues chapra 2008 and are instantly computed using low memory and power requirements moreover the ade is the backbone of the most common solute transport models used by ecologists and hydrologists in environmental studies e g gonzález pinzón et al 2013 runkel 2007 stream solute workshop 1990 1 d c d t u d c d x d d 2 c d x 2 λ c where c m l 3 is the concentration of the reactive solute at a cross section located downstream of the solute injection site u l t 1 is the mean streamflow velocity d l t 2 is the dispersion coefficient λ t 1 is the first order rate coefficient λ 0 for a non reactive tracer x l is longitudinal distance and t t is time 2 1 1 instantaneous vs continuous tracer injections an instantaneous aka pulse gulp slug injection is one in which known tracer masses conservative and reactive are dissolved in a volume of water and poured all at once into the stream or river featuring a duration of the injection t i n j t effectively equal to zero continuous injections on the other hand feature t i n j 0 i e the release of the dissolved tracer masses into the stream occurs over periods typically ranging from hours to weeks note however that continuous injections do not guarantee reaching steady state aka plateau concentrations downstream for that to happen the solute injected must be enough to label and saturate all of the flowpaths upstream of a given observation point which depends on multiple stream characteristics such as discharge dispersion streambed permeability ambient concentrations and tracer solubility as well as variables associated with the design of the tracer injection such as its duration the injection rate and the location of sampling points researchers and practitioners use instantaneous and continuous in stream tracer injections to answer questions and test hypotheses associated with the fate and transport of solutes that are naturally present or become available through specific events such as storms spills leaks or day to day operations of wastewater treatment plants hyer 2007 leibundgut et al 2009 williams et al 2013 those injections can also be used to estimate river flow e g kilpatrick and cobb 1984 groundwater surface water interactions e g bencala and walters 1983 gonzález pinzón et al 2015 triska et al 1989 stream atmosphere interactions e g kilpatrick et al 1987 and travel and residence times kilpatrick and wilson 1989 instantaneous in stream tracer injections are ideal tools for understanding the fate and transport of solutes instantly added kilpatrick and wilson 1989 because they represent many real world situations where a solute becomes suddenly and briefly available e g truck spills and accidental pipe leaks that were quickly resolved and because they have low costs and simpler logistics also the results from instantaneous injections and principles of superposition could be used with to estimate experimental results for varying injection durations through convolution i e as done in s curve analyses used in other hydrology applications such as the unit hydrograph recent experimental efforts to discern the main differences in results from using instantaneous vs continuous injections suggest that longer tracer injections label wider deeper and thus longer subsurface flow paths affecting our in stream based interpretations of reach scale residence time distributions assessment of which compartments contribute more to transient storage and perhaps more importantly the existing relationships between physical transport and biochemical reactivity mainly because of differences in exposure times the sampling of biomass diversity and functioning and the concentration ranges and their influence in reaction kinetics harvey et al 1996 gooseff et al 2008 navel et al 2010 drummond et al 2012 knapp et al 2017 2 1 2 ade solution for instantaneous tracer injections the analytical solution for instantaneous tracer injections is chapra 2008 2 c x t m i n j 2 a π d t e x u t 2 4 d t λ t where m i n j m is the mass injected and a l2 is the stream cross sectional area ade solution for continuous tracer injections the analytical solution for step or continuous tracer injections is given in two parts chapra 2008 for t t i n j 3a c x t c t a r g e t m a x 2 e u x 1 γ 2 d e r f c x u t γ 2 d t e u x 1 γ 2 d e r f c x u t γ 2 d t for t t i n j 3b c x t c t a r g e t m a x 2 e u x 1 γ 2 d e r f c x u t γ 2 d t erfc x u t t i n j γ 2 d t t i n j e u x 1 γ 2 d e r f c x u t γ 2 d t erfc x u t t i n j γ 2 d t t i n j 3c γ 1 4 η 3d η λ d u 2 3e c t a r g e t m a x q p u m p q c t a r g e t i n j e c t a t e where q p u m p l3 t 1 is the continuous injection rate q l3 t 1 is the mean river discharge and c t a r g e t i n j e c t a t e m l 3 is the concentration of the target tracer i e the chemical compound quantified by analytical techniques or sensors in the injectate 2 2 estimation of tracer injection masses 2 2 1 instantaneous tracer injection we implemented two methods in tipt to estimate the mass of a commercially available tracer salt referred to as commercial tracer from here on that needs to be added to generate a user specified maximum target tracer concentration at the most downstream sampling location for example when nacl is used as the commercial tracer cl is the target tracer for this we assume that the target tracer is available with 100 purity 2 2 1 1 jobson s unit concentration method we used the unit concentration concept introduced by jobson 1996 to estimate the tracer mass needed to generate a maximum target concentration from an instantaneous injection briefly the unit concentration c u t 1 standardizes tracer concentrations by the mass of tracer injected tracer losses due to dilution and differences in stream discharge 4 c u 1 x 10 6 c r r q m i n j where c m l 3 is an observed tracer concentration r r is the ratio of the mass added to the total mass retrieved during the experiment at the sampling location i e r r m i n j 0 t c q d t note that 1 c u t 1 represents the solute mass flow rate mass per time per unit of mass injected 2 the 1 x 10 6 factor is used to bring c u close to a unit value regardless of the system of units chosen and 3 discharge must be expressed in units that are consistent with the denominator of the concentration and the injected mass must be in the same units as the numerator of the concentration an analysis of data from 422 experimental observations in 60 different rivers in the united states table 1 showed that peak c u values c u p correlate with increasing times to peak concentrations t p the predicting equation for c u p 1 s and t p h is 5 c u p 1105 t p 0 817 according to jobson 1996 this equation had a root mean square error rmse of 0 502 natural logarithmic units the coefficient of variation of the regression was 0 112 and the coefficient of determination r2 was 0 893 the standard error of estimate of the coefficient was 4 9 and the standard error of estimate for the exponent is 1 7 in tipt we combined equations 4 and 5 to estimate m i n j m from user specified values of q l3 t 1 c t a r g e t m a x m l 3 c o m l 3 and ade derived estimates of t p values t p a d e t to estimate t p a d e t we use user specified values of q l3 t 1 u l t 1 d l2 t 1 λ t 1 and x l and set m ˆ i n j 1 temporarily which does not affect the timing of t p a d e only the amplitude of the concentrations then we estimate the tracer injection mass using 6 m i n j j o b s o n s q c t a r g e t m a x c o max a t p ade b where a and b represent the unit consistent coefficient and exponent from equation 5 and max a t p ade b represents the time to peak at the most distal sampling location where c t a r g e t m a x will occur to help the user select a value for d tipt uses a simplified version of the following equation proposed by fisher et al 1979 7 d 0 011 u 2 w 2 h u where h l is the stream depth and is approximated as h q u w assuming a rectangular cross section u h g s l t 1 is the shear velocity g l t 2 is the gravitational acceleration and s is the longitudinal slope of the stream however we note that the estimation of the longitudinal dispersion coefficient is a critical step in the application of the ade as recently demonstrated by peruzzi et al 2021 and that there are many other empirical equations available fischer et al 1979 chapra 2008 finally we use the molar masses of the commercial and target tracers m m m mol 1 to estimate the mass of the commercial tracer to be added m c o m m e r c i a l m and the commercial tracer solubility c c o m m e r c i a l s o l m l 3 to estimate the volume v i n s t i n j l3 needed to dissolve m c o m m e r c i a l 8 m c o m m e r c i a l j o b s o n s m a d d j o b s o n s m m c o m m e r c i a l m m t a r g e t 9 v i n s t i n j m c o m m e r c i a l j o b s o n s c c o m m e r c i a l s o l 2 2 1 2 ade based method in tipt we also propose to estimate the mass needed in an instantaneous tracer injection using the analytical solution of the ade for the peak concentration c t a r g e t m a x and the times to peak concentration at the most distal sampling location t p a d e like in jobson s method we first need to estimate t p a d e t and then solve for m i n j m in equation 2 using user specified values for the most downstream sampling location x d i s t a l l and the maximum concentration wanted there at the time to peak c t a r g e t m a x x d i s t a l t p a d e m l 3 10 m i n j a d e c t a r g e t m a x x d i s t a l t p a d e c o 2 a π d t p a d e e x d i s t a l u t p a d e 2 4 d t λ t p a d e 11 m c o m m e r c i a l a d e m i n j a d e m m c o m m e r c i a l m m t a r g e t 12 v i n s t i n j m c o m m e r c i a l a d e c c o m m e r c i a l s o l 2 2 2 continuous tracer injection we used mass balance equations to guide the user through the planning of a continuous tracer injection i e those for which the injection time t i n j 0 the mass estimation consists of a trial and verification process i e once the user enters their feasible pump injection rate q p u m p l3 t 1 and the volume of the injectate v i n j e c t a t e l3 based on equipment and power availability simple mass balance equations help verify the maximum pump injection rate q p u m p m a x l3 t 1 the maximum injection time t i n j m a x t the concentration of the target tracer in the injectate assuming a 100 purity c t a r g e t i n j e c t a t e m l 3 the mass of commercial tracer to add to the injectate m c o m m e r c i a l i n j e c t a t e m and the percent saturation of the commercial tracer in the carboy s a t c o m m e r c i a l 13 q p u m p m a x v i n j e c t a t e t i n j 14 t i n j m a x v i n j e c t a t e q p u m p 15 c t a r g e t i n j e c t a t e q c t a r g e t m a x c o q p u m p 16 m c o m m e r c i a l i n j e c t a t e c t a r g e t i n j e c t a t e v i n j e c t a t e m m c o m m e r c i a l m m t a r g e t 17 s a t c o m m e r c i a l c t a r g e t i n j e c t a t e c c o m m e r c i a l s o l m m c o m m e r c i a l m m t a r g e t 2 3 additional considerations tipt accounts for uncertainty in the estimation of the velocity of the flow u and reports three btcs at each sampling location using the factors 0 8 u 1 0 u and 1 2 u in the analytical solutions of the ade tipt estimates the length required to attain complete lateral mixing for a tracer injection near the center of the channel this mixing length l m i x is fischer et al 1979 18 l m i x 0 1 w 2 u 0 6 h u note that if the tracer is released from the side of the channel the user should quadruple l m i x chapra 2008 tipt provides the option to enter the minimum travel time that the user expects is needed to see significant reactions after releasing a reactive tracer t m i n and uses it to estimate the minimum distance required between the injection and the first sampling location as 19 l m i n 1 2 u t m i n when the user enters a first sampling location at a distance smaller than l m i x or l m i n tipt flags that input to alert the user 2 4 user interaction with tipt the excel workbook tipt has multiple sheets where equations 1 17 are used to compute results and generate graphs however the user only inputs information in the main sheet yellow cells in workbook see table 2 tipt requires basic information about the study site simulation parameters and the distance from the tracer injection site to up to five sampling locations where breakthrough curve simulations will be computed and graphed fig 1 tipt also asks for information about the commercial and target tracers to compute stoichiometric relationships and estimate masses to be added and allows the user to select between the jobson s or ade based methods to estimate masses in instantaneous tracer experiments fig 2 for continuous injections i e when t i n j 0 tipt requests information about the pumping rate at which the tracer can be added and the volume of the carboy available to check mass balances and help the user minimize the masses and injectate volume needed for the experiment fig 3 we set the color convention described in table 2 to guide the user interaction with tipt to complete the planning of tracer experiments the users must follow these steps in the main sheet enter values for all cells in yellow cells in light purple are optional but may become yellow when switching between instantaneous and continuous injections enter values for cells b12 t i n j b13 d t or b14 t e n d to trigger the calculation of new tracer breakthrough curves note that t i n j 0 triggers results for instantaneous injections and any positive value triggers results for continuous injections cells f10 h10 allow the user to enter decay rate coefficients for the tracer that the user wants to graph in the tracer breakthrough curves if none is entered the graphs correspond to the conservative tracer for instantaneous injections select between jobson s or ade based methods to estimate tracer masses by default tipt uses the ade based method read cells f13 h14 to extract masses and volumes needed based on the information given in cells f4 h8 for continuous injections make sure that the values entered in cells k10 m11 do not result in red cells or numbers in the range k13 m17 if the results yield red cells or numbers in the range k13 m17 adjust values in k10 m11 until the cells turn green which indicates mass balance compliance keep in mind that the idea is to minimize the mass injected and the volume needed the rest of the sheets in tipt present graphs at different sampling locations and using both arithmetic and semi log scales the latter allowing the users to focus more on the timing of the low concentrations so as to avoid sampling truncation errors drummond et al 2012 gonzález pinzón et al 2013 within the known advantages and limitations of using the ade kirchner 2000 zarnetske et al 2012 bardini et al 2013 3 case studies 3 1 instantaneous injection gonzález pinzón et al 2019 conducted an instantaneous tracer injection in the rio grande river near albuquerque new mexico usa along a 7th order river reach the team used tipt to estimate the masses of nabr and nano3 as conservative and reactive tracers the location of two sampling stations featuring well mixed conditions and the time coverage required to sample the tracer breakthrough curves there on the day of the experiment the team measured the mean discharge velocity and river width using a flowtracker handheld adv sontek san diego usa and used google maps to estimate the longitudinal slope of the reach that information was entered in tipt as shown in figs 1 and 2 using the basic river information collected on site and setting nabr c o 30 μg l as informed by previous work in the reach tipt suggested an injection of 33 3 kg of nabr to get a desired nabr c t a r g e t m a x 1600 μg l at the station located 5 665 km downstream of the injection site fig 4 shows the predictions from tipt and the actual concentrations measured from the tracer experiment after adding 31 1 kg of nabr 2 2 kg less due to commercial tracer availability note that the observed concentrations fell within the time windows predicted by tipt set as 0 8x and 1 2x the mean velocity entered to account for uncertainties in the user specified parameters note that we limited the y axis in the semi log plot to show the instrument specific limit of detection for br samples i e 1 1 10 1 μg l as any lower concentration would have been undetectable with the dionex ics 1000 ion chromatograph with as23 ag23 analytical and guard columns that were used to read the field samples the day of the experiment using the results from tipt we organized two teams the first team with five members and one vehicle oversaw the mixing and injection of the tracer at the injection site and then moved to the second station to begin their sampling tasks about 1 7 h after the instantaneous tracer injection the second team with four members and a vehicle was asked to prepare the two sampling stations i e set up working tables and prepare sampling and labeling gear and be ready to begin sampling at the upstream site from the time of tracer injection the next 6 h so they could finish their labeling and filtration tasks work on picking up their gear and then drive to the second sampling station to support the rest of the crew and organize the retreat from the field both teams returned samples to the laboratory soon after the last field sample was collected 9 h after the instantaneous tracer injection given the size of the reach which required covering long distances and significant moving times between locations its proximity to a metropolitan area with 1 million inhabitants which limited the use of dyes or other observable tracers due to public concerns and the low sensitivity that could be achieved with conductivity sensors which forced us to use nabr as a conservative tracer tipt s predictions of the tracer arrival peak and passage times simplified the field logistics and allowed us to make objective decisions on who was doing what and where tipt was used to guide our grab sampling in real time because we could neither see the tracer plume nor measure br concentrations with a sensor finally tipt accurately guided us to select the amount of tracer needed to avoid under or overestimations that could result in data and time losses or environmental toxicity respectively 3 2 continuous injections in the summer of 2018 we conducted a continuous injection of resazurin in como creek colorado usa a steep 2nd order stream surrounded by 20 alpine meadow tundra and 80 conifer forest ries iii et al 2017 the night before the injection we used pre verified information from a weir located at the study site to estimate discharge as 0 02 m3 s for the next experimental day in tipt we entered this and other relevant site and tracer information as shown in figs 3 and 5 note that we set the first order rate coefficient of resazurin equal to zero despite knowing that microbial metabolism transforms it to its daughter product resorufin gonzález pinzón et al 2012 2014 knapp et al 2018 because we did not have a priori site specific information about the extent of the transformation of resazurin with these assumptions tipt suggested the addition of 29 g of resazurin dissolved in a volume of 0 135 m3 135 l to guarantee a 4 h injection using a pump injecting the injectate at a rate of 558 ml min out of practicality we added three pre weighed 10g bags with resazurin since the saturation level of the tracer was below 12 fig 3 during the experiment we took 20 ml aliquots from the stream over 36 h and adopted our sampling frequency during the rising and falling limbs to capture the dynamic range of the tracer breakthrough curve following the predictions visualized through tipt for this our injection time of 10 00 a m represented the 0h timestamp in tipt all samples were filtered immediately after collection using a 0 7 μm gf f filter sigma aldrich samples were kept frozen at 4 c to avoid ex situ transformation we followed the protocol presented in the supplementary information of knapp et al 2018 to estimate the resazurin concentrations in the laboratory using a varian carry eclipse spectrofluorometer with limit of detection of 1 0 10 2 μg l in fig 6 we overlapped the predictions from tipt that we used to guide our experimental set up and sampling the resazurin concentrations read in the laboratory and a post injection set of simulations from tipt showing the results that we would have gotten if we had set the transformation rate coefficient of resazurin to λ 2 10 4 s 1 this exercise shows that tipt s timing was accurate regardless of the assumptions made about λ and that the experimental λ at the study site was on the high end of values previously reported from tracer injections in stream ecosystems haggerty et al 2008 knapp et al 2017 generating experimental concentrations that were one order of magnitude smaller than what was expected under the assumption of negligible transformation i e conservative transport from this study case we emphasize here that any assumption about the likely behavior of a reactive solute that undergoes decay must be made judiciously considering that 1 by assuming conservative transport or negligible decay the resulting mass injected may generate small or undetectable dynamic concentration ranges due to unanticipated upstream uptake retention or transformation and 2 by assuming higher decay or transformation rate coefficients the resulting concentrations may inadvertently overload the stream and generate maximum concentrations beyond those considered safe for its ecological functioning and increase the cost of tracer salts unnecessarily 4 limitations and advantages of tipt 4 1 limitations tipt assumes steady state and uniform flow conditions i e the physical chemical and biological characteristics of a stream captured by the user inputs are assumed to remain constant over space and time while these assumptions let us use analytical solutions for the ade natural streams and rivers are dynamic and non uniform we developed tipt because we have found that assuming steady state and uniform conditions for planning the logistics of a tracer injection is practical and useful i e any alternative based on parameterizing dynamic and non uniform models without formally carrying out a tracer injection can be equally or more uncertain and laborious tipt assumes that the biochemical reactions undergone by a reactive solute can be described using first order decay or production λ therefore tipt cannot anticipate the effects of kinetics based reactions associated with limitations and co limitations among many other possibilities also predicting λ values before a tracer injection may be difficult so we recommend the use of experimental values reported in studies with similar characteristics as we noted before severely overpredicting λ can result in the introduction of too much mass to the stream ecosystem and unnecessary expenses associated with tracer supplies while severely underpredicting λ can result in low to undetectable concentrations at downstream sites the current version of tipt does not handle transient storage processes lateral inflows or outflows transient storage in the surface or subsurface of the stream increases the residence times of solutes which is manifested in longer btc tails haggerty et al 2002 gomez et al 2012 jackson et al 2013 accordingly tipt would underpredict the time that it takes to recover the tracer mass in streams and rivers with extensive recirculation zones or hyporheic exchange also while lateral outflows do not impact solute concentrations in well mixed streams and rivers but do impact mass balances lateral inflows impact concentrations through dilution but do not impact mass balances therefore we recommend using tipt in stream or river segments without significant inflows 4 2 advantages tipt is a first approximation tool that can be used to plan tracer injections and design the logistics of experiments involving solute transport processes in streams and rivers tipt offers a predictive ability that is disproportionally favorable with respect to the few inputs required and uses excel which is readily available software accessible to those without coding experience tipt is a versatile user friendly and graphical tool that can help the hydrologic ecologic and engineering communities design tracer injections and solute transport experiments that are more easily replicated within and across sites in doing so tipt contributes directly to advancing icon principles which call for efforts to become more integrated across disciplines coordinated with consistent protocols open across the entire research lifecycle and networked whereby a broad range of stakeholders design implement and benefit from the work goldman et al 2022 similarly tipt can help generate datasets that more closely follow fair principles i e they are findable accessible interoperable and reusable both icon and fair are pillars of the open watershed science by design approach promoted by the us department of energy stegen et al 2019 software availability the tracer injection planning tool has been uploaded to hydroshare and will be maintained there gonzalez pinzon r j dorley j singley k singha m gooseff t covino 2022 tipt the tracer injection planning tool hydroshare http www hydroshare org resource 6cc58a01c5b7463d97622bb225b73cca declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments ricardo gonzález pinzón thanks james fluke for support validating early versions of tipt the national science foundation provided funding support through grants nsf ear 1642399 nsf ear 1642368 nsf ear 1642402 and nsf ear 1642403 this material is also based upon work supported by the u s department of energy office of science office of biological environmental research under award number de sc0019424 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105504 
25554,despite their frequent use there are few simple and readily accessible tools to help guide the logistical planning of tracer injections in streams and rivers we combined the widely used advection dispersion reaction equation peak concentration estimates based on a meta analysis of hundreds of tracer injections carried out in streams and rivers and simple mass balances in a dynamic excel workbook to 1 help users decide how much tracer mass should be added to achieve a specific dynamic concentration range that reduces known issues associated with breakthrough curve tail truncation and 2 generate tables and graphs that can be readily used to plan the deployment of resources our tracer injection planning tool tipt handles instantaneous and continuous tracer injections and assumes steady state and uniform flow conditions as well as first order decay or production while those assumptions do not strictly apply to natural streams and rivers they help simplify the planning of tracer injections with a predictive ability that is disproportionally favorable with respect to the few inputs required tipt is a versatile user friendly and graphical tool that can help design tracer injections and solute transport experiments that are more easily replicated within and across sites thus tipt contributes directly to advancing integrated coordinated open and networked icon principles similarly tipt can help generate datasets that more closely follow findable accessible interoperable and reusable fair principles we demonstrate the use of tipt through two case studies featuring 1 a continuous injection in a 2nd order stream and 2 an instantaneous injection in a 7th order stream keywords solute transport tracer injection streams rivers instantaneous injection continuous injection data availability no data was used for the research described in the article 1 introduction tracer injections in streams and rivers are commonly used to characterize physical and biochemical processes undergone by solutes and micro to nano particles transported in fluvial networks covino et al 2010 foppen et al 2011 gonzález pinzón et al 2015 drummond et al 2017 knapp et al 2018 these techniques help us quantify and visualize a variety of processes controlling the fate and transport of solutes physical processes including advection dispersion and transient storage biological processes including uptake decay and production and chemical processes including sorption decay and production stream solute workshop 1990 abbott et al 2016 knapp et al 2017 despite their wide use in research and consulting in hydrology environmental engineering and earth and aquatic sciences there are few simple and readily accessible tools to help guide the logistical planning of tracer injections in streams and rivers consequently tracer studies are not easily replicated i e different users might end up choosing considerably different injection and sampling durations target concentrations sampling frequencies and longitudinal extent even if they have a similar goal gonzález pinzón et al 2015 schmadel et al 2016 moreover the lack of optimal sampling frequencies and the anticipation of arrival and passage times may lead to suboptimal decisions about personnel and equipment supply logistics which ultimately reduce the quality and quantity of the information retrieved from tracer injections paradoxically the perfect logistical planning of a tracer injection would require carrying it out first and repeating it under the exact same physical and biochemical conditions harvey and gooseff 2015 harvey et al 1996 which is impossible here we propose to combine 1 the widely used advection dispersion reaction equation 2 a meta analysis of hundreds of tracer injections carried out in streams and rivers spanning four orders of magnitude in discharge and two in size which was conducted by jobson 1996 and 3 simple mass balances to more objectively plan the logistics of tracer injections in streams and rivers we programmed an excel workbook to offer a readily accessible product that can be used on multiple operating systems and devices using basic information about the study site supplemental information 1 the tracer injection planning tool tipt handles instantaneous and continuous tracer injections to 1 help users decide how much tracer mass should be added to achieve a specific dynamic concentration range i e the ratio between the maximum and minimum tracer concentrations measured that reduces known issues associated with breakthrough curve tail truncation drummond et al 2012 and 2 generate tables and graphs that can be readily used to plan the deployment of resources 2 methods 2 1 the advection dispersion equation in tipt we use the advection dispersion equation ade with first order decay or production to guide the logistical planning of tracer injections because it has analytical solutions for instantaneous and continuous injections of solutes chapra 2008 these analytical equations generate exact solutions that are free of numerical dispersion and stability issues chapra 2008 and are instantly computed using low memory and power requirements moreover the ade is the backbone of the most common solute transport models used by ecologists and hydrologists in environmental studies e g gonzález pinzón et al 2013 runkel 2007 stream solute workshop 1990 1 d c d t u d c d x d d 2 c d x 2 λ c where c m l 3 is the concentration of the reactive solute at a cross section located downstream of the solute injection site u l t 1 is the mean streamflow velocity d l t 2 is the dispersion coefficient λ t 1 is the first order rate coefficient λ 0 for a non reactive tracer x l is longitudinal distance and t t is time 2 1 1 instantaneous vs continuous tracer injections an instantaneous aka pulse gulp slug injection is one in which known tracer masses conservative and reactive are dissolved in a volume of water and poured all at once into the stream or river featuring a duration of the injection t i n j t effectively equal to zero continuous injections on the other hand feature t i n j 0 i e the release of the dissolved tracer masses into the stream occurs over periods typically ranging from hours to weeks note however that continuous injections do not guarantee reaching steady state aka plateau concentrations downstream for that to happen the solute injected must be enough to label and saturate all of the flowpaths upstream of a given observation point which depends on multiple stream characteristics such as discharge dispersion streambed permeability ambient concentrations and tracer solubility as well as variables associated with the design of the tracer injection such as its duration the injection rate and the location of sampling points researchers and practitioners use instantaneous and continuous in stream tracer injections to answer questions and test hypotheses associated with the fate and transport of solutes that are naturally present or become available through specific events such as storms spills leaks or day to day operations of wastewater treatment plants hyer 2007 leibundgut et al 2009 williams et al 2013 those injections can also be used to estimate river flow e g kilpatrick and cobb 1984 groundwater surface water interactions e g bencala and walters 1983 gonzález pinzón et al 2015 triska et al 1989 stream atmosphere interactions e g kilpatrick et al 1987 and travel and residence times kilpatrick and wilson 1989 instantaneous in stream tracer injections are ideal tools for understanding the fate and transport of solutes instantly added kilpatrick and wilson 1989 because they represent many real world situations where a solute becomes suddenly and briefly available e g truck spills and accidental pipe leaks that were quickly resolved and because they have low costs and simpler logistics also the results from instantaneous injections and principles of superposition could be used with to estimate experimental results for varying injection durations through convolution i e as done in s curve analyses used in other hydrology applications such as the unit hydrograph recent experimental efforts to discern the main differences in results from using instantaneous vs continuous injections suggest that longer tracer injections label wider deeper and thus longer subsurface flow paths affecting our in stream based interpretations of reach scale residence time distributions assessment of which compartments contribute more to transient storage and perhaps more importantly the existing relationships between physical transport and biochemical reactivity mainly because of differences in exposure times the sampling of biomass diversity and functioning and the concentration ranges and their influence in reaction kinetics harvey et al 1996 gooseff et al 2008 navel et al 2010 drummond et al 2012 knapp et al 2017 2 1 2 ade solution for instantaneous tracer injections the analytical solution for instantaneous tracer injections is chapra 2008 2 c x t m i n j 2 a π d t e x u t 2 4 d t λ t where m i n j m is the mass injected and a l2 is the stream cross sectional area ade solution for continuous tracer injections the analytical solution for step or continuous tracer injections is given in two parts chapra 2008 for t t i n j 3a c x t c t a r g e t m a x 2 e u x 1 γ 2 d e r f c x u t γ 2 d t e u x 1 γ 2 d e r f c x u t γ 2 d t for t t i n j 3b c x t c t a r g e t m a x 2 e u x 1 γ 2 d e r f c x u t γ 2 d t erfc x u t t i n j γ 2 d t t i n j e u x 1 γ 2 d e r f c x u t γ 2 d t erfc x u t t i n j γ 2 d t t i n j 3c γ 1 4 η 3d η λ d u 2 3e c t a r g e t m a x q p u m p q c t a r g e t i n j e c t a t e where q p u m p l3 t 1 is the continuous injection rate q l3 t 1 is the mean river discharge and c t a r g e t i n j e c t a t e m l 3 is the concentration of the target tracer i e the chemical compound quantified by analytical techniques or sensors in the injectate 2 2 estimation of tracer injection masses 2 2 1 instantaneous tracer injection we implemented two methods in tipt to estimate the mass of a commercially available tracer salt referred to as commercial tracer from here on that needs to be added to generate a user specified maximum target tracer concentration at the most downstream sampling location for example when nacl is used as the commercial tracer cl is the target tracer for this we assume that the target tracer is available with 100 purity 2 2 1 1 jobson s unit concentration method we used the unit concentration concept introduced by jobson 1996 to estimate the tracer mass needed to generate a maximum target concentration from an instantaneous injection briefly the unit concentration c u t 1 standardizes tracer concentrations by the mass of tracer injected tracer losses due to dilution and differences in stream discharge 4 c u 1 x 10 6 c r r q m i n j where c m l 3 is an observed tracer concentration r r is the ratio of the mass added to the total mass retrieved during the experiment at the sampling location i e r r m i n j 0 t c q d t note that 1 c u t 1 represents the solute mass flow rate mass per time per unit of mass injected 2 the 1 x 10 6 factor is used to bring c u close to a unit value regardless of the system of units chosen and 3 discharge must be expressed in units that are consistent with the denominator of the concentration and the injected mass must be in the same units as the numerator of the concentration an analysis of data from 422 experimental observations in 60 different rivers in the united states table 1 showed that peak c u values c u p correlate with increasing times to peak concentrations t p the predicting equation for c u p 1 s and t p h is 5 c u p 1105 t p 0 817 according to jobson 1996 this equation had a root mean square error rmse of 0 502 natural logarithmic units the coefficient of variation of the regression was 0 112 and the coefficient of determination r2 was 0 893 the standard error of estimate of the coefficient was 4 9 and the standard error of estimate for the exponent is 1 7 in tipt we combined equations 4 and 5 to estimate m i n j m from user specified values of q l3 t 1 c t a r g e t m a x m l 3 c o m l 3 and ade derived estimates of t p values t p a d e t to estimate t p a d e t we use user specified values of q l3 t 1 u l t 1 d l2 t 1 λ t 1 and x l and set m ˆ i n j 1 temporarily which does not affect the timing of t p a d e only the amplitude of the concentrations then we estimate the tracer injection mass using 6 m i n j j o b s o n s q c t a r g e t m a x c o max a t p ade b where a and b represent the unit consistent coefficient and exponent from equation 5 and max a t p ade b represents the time to peak at the most distal sampling location where c t a r g e t m a x will occur to help the user select a value for d tipt uses a simplified version of the following equation proposed by fisher et al 1979 7 d 0 011 u 2 w 2 h u where h l is the stream depth and is approximated as h q u w assuming a rectangular cross section u h g s l t 1 is the shear velocity g l t 2 is the gravitational acceleration and s is the longitudinal slope of the stream however we note that the estimation of the longitudinal dispersion coefficient is a critical step in the application of the ade as recently demonstrated by peruzzi et al 2021 and that there are many other empirical equations available fischer et al 1979 chapra 2008 finally we use the molar masses of the commercial and target tracers m m m mol 1 to estimate the mass of the commercial tracer to be added m c o m m e r c i a l m and the commercial tracer solubility c c o m m e r c i a l s o l m l 3 to estimate the volume v i n s t i n j l3 needed to dissolve m c o m m e r c i a l 8 m c o m m e r c i a l j o b s o n s m a d d j o b s o n s m m c o m m e r c i a l m m t a r g e t 9 v i n s t i n j m c o m m e r c i a l j o b s o n s c c o m m e r c i a l s o l 2 2 1 2 ade based method in tipt we also propose to estimate the mass needed in an instantaneous tracer injection using the analytical solution of the ade for the peak concentration c t a r g e t m a x and the times to peak concentration at the most distal sampling location t p a d e like in jobson s method we first need to estimate t p a d e t and then solve for m i n j m in equation 2 using user specified values for the most downstream sampling location x d i s t a l l and the maximum concentration wanted there at the time to peak c t a r g e t m a x x d i s t a l t p a d e m l 3 10 m i n j a d e c t a r g e t m a x x d i s t a l t p a d e c o 2 a π d t p a d e e x d i s t a l u t p a d e 2 4 d t λ t p a d e 11 m c o m m e r c i a l a d e m i n j a d e m m c o m m e r c i a l m m t a r g e t 12 v i n s t i n j m c o m m e r c i a l a d e c c o m m e r c i a l s o l 2 2 2 continuous tracer injection we used mass balance equations to guide the user through the planning of a continuous tracer injection i e those for which the injection time t i n j 0 the mass estimation consists of a trial and verification process i e once the user enters their feasible pump injection rate q p u m p l3 t 1 and the volume of the injectate v i n j e c t a t e l3 based on equipment and power availability simple mass balance equations help verify the maximum pump injection rate q p u m p m a x l3 t 1 the maximum injection time t i n j m a x t the concentration of the target tracer in the injectate assuming a 100 purity c t a r g e t i n j e c t a t e m l 3 the mass of commercial tracer to add to the injectate m c o m m e r c i a l i n j e c t a t e m and the percent saturation of the commercial tracer in the carboy s a t c o m m e r c i a l 13 q p u m p m a x v i n j e c t a t e t i n j 14 t i n j m a x v i n j e c t a t e q p u m p 15 c t a r g e t i n j e c t a t e q c t a r g e t m a x c o q p u m p 16 m c o m m e r c i a l i n j e c t a t e c t a r g e t i n j e c t a t e v i n j e c t a t e m m c o m m e r c i a l m m t a r g e t 17 s a t c o m m e r c i a l c t a r g e t i n j e c t a t e c c o m m e r c i a l s o l m m c o m m e r c i a l m m t a r g e t 2 3 additional considerations tipt accounts for uncertainty in the estimation of the velocity of the flow u and reports three btcs at each sampling location using the factors 0 8 u 1 0 u and 1 2 u in the analytical solutions of the ade tipt estimates the length required to attain complete lateral mixing for a tracer injection near the center of the channel this mixing length l m i x is fischer et al 1979 18 l m i x 0 1 w 2 u 0 6 h u note that if the tracer is released from the side of the channel the user should quadruple l m i x chapra 2008 tipt provides the option to enter the minimum travel time that the user expects is needed to see significant reactions after releasing a reactive tracer t m i n and uses it to estimate the minimum distance required between the injection and the first sampling location as 19 l m i n 1 2 u t m i n when the user enters a first sampling location at a distance smaller than l m i x or l m i n tipt flags that input to alert the user 2 4 user interaction with tipt the excel workbook tipt has multiple sheets where equations 1 17 are used to compute results and generate graphs however the user only inputs information in the main sheet yellow cells in workbook see table 2 tipt requires basic information about the study site simulation parameters and the distance from the tracer injection site to up to five sampling locations where breakthrough curve simulations will be computed and graphed fig 1 tipt also asks for information about the commercial and target tracers to compute stoichiometric relationships and estimate masses to be added and allows the user to select between the jobson s or ade based methods to estimate masses in instantaneous tracer experiments fig 2 for continuous injections i e when t i n j 0 tipt requests information about the pumping rate at which the tracer can be added and the volume of the carboy available to check mass balances and help the user minimize the masses and injectate volume needed for the experiment fig 3 we set the color convention described in table 2 to guide the user interaction with tipt to complete the planning of tracer experiments the users must follow these steps in the main sheet enter values for all cells in yellow cells in light purple are optional but may become yellow when switching between instantaneous and continuous injections enter values for cells b12 t i n j b13 d t or b14 t e n d to trigger the calculation of new tracer breakthrough curves note that t i n j 0 triggers results for instantaneous injections and any positive value triggers results for continuous injections cells f10 h10 allow the user to enter decay rate coefficients for the tracer that the user wants to graph in the tracer breakthrough curves if none is entered the graphs correspond to the conservative tracer for instantaneous injections select between jobson s or ade based methods to estimate tracer masses by default tipt uses the ade based method read cells f13 h14 to extract masses and volumes needed based on the information given in cells f4 h8 for continuous injections make sure that the values entered in cells k10 m11 do not result in red cells or numbers in the range k13 m17 if the results yield red cells or numbers in the range k13 m17 adjust values in k10 m11 until the cells turn green which indicates mass balance compliance keep in mind that the idea is to minimize the mass injected and the volume needed the rest of the sheets in tipt present graphs at different sampling locations and using both arithmetic and semi log scales the latter allowing the users to focus more on the timing of the low concentrations so as to avoid sampling truncation errors drummond et al 2012 gonzález pinzón et al 2013 within the known advantages and limitations of using the ade kirchner 2000 zarnetske et al 2012 bardini et al 2013 3 case studies 3 1 instantaneous injection gonzález pinzón et al 2019 conducted an instantaneous tracer injection in the rio grande river near albuquerque new mexico usa along a 7th order river reach the team used tipt to estimate the masses of nabr and nano3 as conservative and reactive tracers the location of two sampling stations featuring well mixed conditions and the time coverage required to sample the tracer breakthrough curves there on the day of the experiment the team measured the mean discharge velocity and river width using a flowtracker handheld adv sontek san diego usa and used google maps to estimate the longitudinal slope of the reach that information was entered in tipt as shown in figs 1 and 2 using the basic river information collected on site and setting nabr c o 30 μg l as informed by previous work in the reach tipt suggested an injection of 33 3 kg of nabr to get a desired nabr c t a r g e t m a x 1600 μg l at the station located 5 665 km downstream of the injection site fig 4 shows the predictions from tipt and the actual concentrations measured from the tracer experiment after adding 31 1 kg of nabr 2 2 kg less due to commercial tracer availability note that the observed concentrations fell within the time windows predicted by tipt set as 0 8x and 1 2x the mean velocity entered to account for uncertainties in the user specified parameters note that we limited the y axis in the semi log plot to show the instrument specific limit of detection for br samples i e 1 1 10 1 μg l as any lower concentration would have been undetectable with the dionex ics 1000 ion chromatograph with as23 ag23 analytical and guard columns that were used to read the field samples the day of the experiment using the results from tipt we organized two teams the first team with five members and one vehicle oversaw the mixing and injection of the tracer at the injection site and then moved to the second station to begin their sampling tasks about 1 7 h after the instantaneous tracer injection the second team with four members and a vehicle was asked to prepare the two sampling stations i e set up working tables and prepare sampling and labeling gear and be ready to begin sampling at the upstream site from the time of tracer injection the next 6 h so they could finish their labeling and filtration tasks work on picking up their gear and then drive to the second sampling station to support the rest of the crew and organize the retreat from the field both teams returned samples to the laboratory soon after the last field sample was collected 9 h after the instantaneous tracer injection given the size of the reach which required covering long distances and significant moving times between locations its proximity to a metropolitan area with 1 million inhabitants which limited the use of dyes or other observable tracers due to public concerns and the low sensitivity that could be achieved with conductivity sensors which forced us to use nabr as a conservative tracer tipt s predictions of the tracer arrival peak and passage times simplified the field logistics and allowed us to make objective decisions on who was doing what and where tipt was used to guide our grab sampling in real time because we could neither see the tracer plume nor measure br concentrations with a sensor finally tipt accurately guided us to select the amount of tracer needed to avoid under or overestimations that could result in data and time losses or environmental toxicity respectively 3 2 continuous injections in the summer of 2018 we conducted a continuous injection of resazurin in como creek colorado usa a steep 2nd order stream surrounded by 20 alpine meadow tundra and 80 conifer forest ries iii et al 2017 the night before the injection we used pre verified information from a weir located at the study site to estimate discharge as 0 02 m3 s for the next experimental day in tipt we entered this and other relevant site and tracer information as shown in figs 3 and 5 note that we set the first order rate coefficient of resazurin equal to zero despite knowing that microbial metabolism transforms it to its daughter product resorufin gonzález pinzón et al 2012 2014 knapp et al 2018 because we did not have a priori site specific information about the extent of the transformation of resazurin with these assumptions tipt suggested the addition of 29 g of resazurin dissolved in a volume of 0 135 m3 135 l to guarantee a 4 h injection using a pump injecting the injectate at a rate of 558 ml min out of practicality we added three pre weighed 10g bags with resazurin since the saturation level of the tracer was below 12 fig 3 during the experiment we took 20 ml aliquots from the stream over 36 h and adopted our sampling frequency during the rising and falling limbs to capture the dynamic range of the tracer breakthrough curve following the predictions visualized through tipt for this our injection time of 10 00 a m represented the 0h timestamp in tipt all samples were filtered immediately after collection using a 0 7 μm gf f filter sigma aldrich samples were kept frozen at 4 c to avoid ex situ transformation we followed the protocol presented in the supplementary information of knapp et al 2018 to estimate the resazurin concentrations in the laboratory using a varian carry eclipse spectrofluorometer with limit of detection of 1 0 10 2 μg l in fig 6 we overlapped the predictions from tipt that we used to guide our experimental set up and sampling the resazurin concentrations read in the laboratory and a post injection set of simulations from tipt showing the results that we would have gotten if we had set the transformation rate coefficient of resazurin to λ 2 10 4 s 1 this exercise shows that tipt s timing was accurate regardless of the assumptions made about λ and that the experimental λ at the study site was on the high end of values previously reported from tracer injections in stream ecosystems haggerty et al 2008 knapp et al 2017 generating experimental concentrations that were one order of magnitude smaller than what was expected under the assumption of negligible transformation i e conservative transport from this study case we emphasize here that any assumption about the likely behavior of a reactive solute that undergoes decay must be made judiciously considering that 1 by assuming conservative transport or negligible decay the resulting mass injected may generate small or undetectable dynamic concentration ranges due to unanticipated upstream uptake retention or transformation and 2 by assuming higher decay or transformation rate coefficients the resulting concentrations may inadvertently overload the stream and generate maximum concentrations beyond those considered safe for its ecological functioning and increase the cost of tracer salts unnecessarily 4 limitations and advantages of tipt 4 1 limitations tipt assumes steady state and uniform flow conditions i e the physical chemical and biological characteristics of a stream captured by the user inputs are assumed to remain constant over space and time while these assumptions let us use analytical solutions for the ade natural streams and rivers are dynamic and non uniform we developed tipt because we have found that assuming steady state and uniform conditions for planning the logistics of a tracer injection is practical and useful i e any alternative based on parameterizing dynamic and non uniform models without formally carrying out a tracer injection can be equally or more uncertain and laborious tipt assumes that the biochemical reactions undergone by a reactive solute can be described using first order decay or production λ therefore tipt cannot anticipate the effects of kinetics based reactions associated with limitations and co limitations among many other possibilities also predicting λ values before a tracer injection may be difficult so we recommend the use of experimental values reported in studies with similar characteristics as we noted before severely overpredicting λ can result in the introduction of too much mass to the stream ecosystem and unnecessary expenses associated with tracer supplies while severely underpredicting λ can result in low to undetectable concentrations at downstream sites the current version of tipt does not handle transient storage processes lateral inflows or outflows transient storage in the surface or subsurface of the stream increases the residence times of solutes which is manifested in longer btc tails haggerty et al 2002 gomez et al 2012 jackson et al 2013 accordingly tipt would underpredict the time that it takes to recover the tracer mass in streams and rivers with extensive recirculation zones or hyporheic exchange also while lateral outflows do not impact solute concentrations in well mixed streams and rivers but do impact mass balances lateral inflows impact concentrations through dilution but do not impact mass balances therefore we recommend using tipt in stream or river segments without significant inflows 4 2 advantages tipt is a first approximation tool that can be used to plan tracer injections and design the logistics of experiments involving solute transport processes in streams and rivers tipt offers a predictive ability that is disproportionally favorable with respect to the few inputs required and uses excel which is readily available software accessible to those without coding experience tipt is a versatile user friendly and graphical tool that can help the hydrologic ecologic and engineering communities design tracer injections and solute transport experiments that are more easily replicated within and across sites in doing so tipt contributes directly to advancing icon principles which call for efforts to become more integrated across disciplines coordinated with consistent protocols open across the entire research lifecycle and networked whereby a broad range of stakeholders design implement and benefit from the work goldman et al 2022 similarly tipt can help generate datasets that more closely follow fair principles i e they are findable accessible interoperable and reusable both icon and fair are pillars of the open watershed science by design approach promoted by the us department of energy stegen et al 2019 software availability the tracer injection planning tool has been uploaded to hydroshare and will be maintained there gonzalez pinzon r j dorley j singley k singha m gooseff t covino 2022 tipt the tracer injection planning tool hydroshare http www hydroshare org resource 6cc58a01c5b7463d97622bb225b73cca declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments ricardo gonzález pinzón thanks james fluke for support validating early versions of tipt the national science foundation provided funding support through grants nsf ear 1642399 nsf ear 1642368 nsf ear 1642402 and nsf ear 1642403 this material is also based upon work supported by the u s department of energy office of science office of biological environmental research under award number de sc0019424 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105504 
