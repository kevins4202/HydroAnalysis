index,text
26430,it is crucial for sustainable planning to consider broad environmental and social dimensions and systemic implications of new infrastructure to build more resilient societies reduce poverty improve human well being mitigate climate change and address other global change processes this article presents resilience io 2 a platform to evaluate new infrastructure projects by assessing their design and effectiveness in meeting growing resource demands simulated using agent based modelling due to socio economic population changes we then use mixed integer linear programming to optimise a multi objective function to find cost optimal solutions inclusive of environmental metrics such as greenhouse gas emissions the solutions in space and time provide planning guidance for conventional and novel technology selection changes in network topology system costs and can incorporate any material waste energy labour or emissions flow as an application a use case is provided for the water sanitation and hygiene wash sector for a four million people city region in ghana keywords integrated environmental modelling iem decision support sub saharan africa wash agent based modelling mathematical programming 2010 msc 97m10 92d40 90c11 91b69 93a30 93b40 software data availability the resilience io wash model was built by charalampos p triantafyllidis xiaonan wang koen h van dam kamal kuriyan and nilay shah from imperial college london with data and technical support by rembrandt koppelaar zoltan kis and hannes kunz from the institute of integrated economic research the platform is designed for open source future release by using only open source software a beta version of the platform is available upon request from the authors including the data set to reproduce the presented use case the platform is of approximately 800mb in size on a windows based machine further details concerning the hardware software used can be found in table 3 1 introduction several modelling techniques have been proposed to gain insights in the delivery of sustainable goals in a scientific manner recently water quality performance assessment has been under the microscope massoudieh et al 2017 furthermore in alhamwi et al 2017 a gis based platform was introduced to facilitate storage and flexibilisation of technologies in urban areas in cominola et al 2015 the need for models that describe exogenous drivers affecting water and demand management was highlighted to inform strategic planning and policy formation while various platforms were also presented deoreo et al 1996 kowalski and marshallsay 2003 froehlich et al 2009 beal et al 2010 in hu et al 2015 multi threaded programming with hadoop based cloud computing was used to implement a multi agent system for environmental modelling furthermore in berglund 2015 agent based models abms are reviewed in the context of water management planning decisions the benefit of planning based on advanced system models lies in the ability to rapidly carry out project evaluation within a complex city systems perspective such models can remove the divide between high level master planning and isolated low level project planning by enabling a link of feedback between the two the exploration of more options and trade offs and enhanced visibility on positive and negative impacts in multiple dimensions thus lowering the risk and cost of planning key is not to rely on a pure techno economic planning approach e g reductionist infrastructure cost estimates since sustainable development social parameters and cost and benefits are inextricably bound together white and lee 2009 concerning the presented use case application models focusing on specific components of the water cycle are various and include agent based water demand models with attitudes norms and behavioral control towards water use koutiva and makropoulos 2016 linear optimisation lo models for water supply pipe networks sarbu and ostafe 2016 and sewer pipe networks safavi and geranmehr 2016 mixed integer non linear programming minlp approaches for pipe network optimisation moeini and afshar 2013 afshar et al 2015 groundwater and rainwater storage and network minlp chung et al 2009 mixed integer linear programming milp model based water treatment technology planning including energy use estimations alqattan et al 2015 and water reuse potential estimations using a milp architecture liu et al 2015 complete water system models are less common a class referred to as integrated urban water cycle models iuwcms when including all water cycle aspects and integrated urban water system models iuwsms that also integrate social environmental economic and other resource flows like energy bach et al 2014 based on a literature review from 1990 to 2015 a total of fourteen existing iuwsms were documented peña guzmán et al 2017 although some upon evaluation appear to be misclassified given their too narrow usability such as the commercial aquacycle tool that focuses on rain and storm water modelling sharma et al 2008 or the commercial mike urban software package for hydrology and flood modelling berggren et al 2012 social elements were covered in one out of fourteen models de haan et al 2013 and five allowed for energy and emissions the two most complete iuwsm models were found to be watermet2 behzadian et al 2014b and urban water optioneering tool uwot rozos and makropoulos 2013 watermet2 is an open access difference differential equation simulation model that integrates both natural and human water and waste water systems inclusive of four different scales indoor local catchment and city areas these include the ability to model water demand and supply balances pipeline fluxes energy requirements and greenhouse gas emissions behzadian et al 2014a uwot is an available on request spatially fixed fine grained minute to hour appliance household demand model with aggregation to neighborhood and city scale supply is spatially solved using genetic algorithm optimisation of a pre defined treatment plant and pipe network for water flow allocation the platform is built in simulink matlab linked to a technology and pipeline network library database rozos and makropoulos 2013 the main limitation of these tools is their rigidity as their goal is to analyse the performance of a user designed wash technology network system inclusive of any future interventions this as opposed to starting with performance criteria as constraints on the system including economic environmental and social requirements and using the tool to explore urban water system design solutions that fall within the desired performance to meet future urban water cycle needs the gap we attempt to bridge with this platform is therefore the following since technological innovation and planning is arguably crucial to achieve sustainable development anadon et al 2016 how can technical tools assist in i exploring planning solutions and providing the quantitative evidence on meeting the broader economic social and environmental requirements for implementing technological investments on the ground and ii bring learning and engagement of stakeholders in the full development cycle of such tools the first part is materialised by a systems modelling scheme we embrace the assemblage approach by combining polished and well established modelling formalisms including agent based modelling and mathematical programming abms are becoming more and more popular in environmental modelling sun et al 2016 and at the same time mathematical programming has seen enormous progress in the development of polished consistent and accurate solvers across the latest two decades while also being one of the most widely used tools to inform transparent decision making the intention of this approach is as analysed in voinov and shugart 2013 to allow models to exchange communication on run time as an integrated suite which outputs meaningful results and can potentially answer an array of questions that decision makers are interested in the second part is effectively achieved by the open access development cycle of the platform a major degree of involvement by potential users and stakeholders across a variety of disciplines took place to elaborate on the optimal design implementation and usefulness of the platform while not over complicating the models by conducting a series of communication exchanges with local experts in ghana from various institutions e g ghana water company university of ghana etc and delivery of an early version for testing we tried to shape the platform to best serve the user needs to assess policy implications so as to reach sustainability targets in the most realistic manner this article is organised as follows in section 2 we present the modelling techniques built in to the platform in section 3 we describe the link between the models and the input data and the associated mechanisms as well as the major implementation details section 4 demonstrates a series of questions that can be potentially answered by using resilience io section 5 presents the wash sector use case as a first application as well as the context upon which the analysis was built results follow in section 6 a comparison of the developed platform with the watermet2 and uwot models as well as limitations and advantages of our approach are given in the discussion section 7 we give our conclusions in section 8 finally the data set for the milp optimisation and examples of automated plots from the platform can be found in the appendix 2 methods as a novel implementation of the concept to combine technical planning with socio economic evaluation we built a set of models of infrastructure systems at a fine grain spatial and temporal scale within a socio economic context we start with human behavior and decisions by first defining population variability bentsi enchill et al 2010 ghana statistical service 2010 and simulating how it affects daily activities and location from which needs emerge for instance water demand as well as environmental impacts generation of waste water we therefore apply predictive inductive modelling in the sense that we used predefined sets of variables population characteristics and their projected change into the future that are coupled with demand requirements we then connect these demands to a supply flow optimisation model which links technology inputs and outputs in any dimension energy materials emissions wastes labour to find multi objective solutions to technology investment and operation including cost and sustainability criteria we follow the meticulous review in hamilton et al 2015 concerning the dimensions upon which software based tools which facilitate integrated environmental modelling iem should be designed and developed we also employ data sets as the intermediate tool between modular modelling methodologies presented herein so as to follow the discipline presented in laniak et al 2013 to this respect we treat the whole data set package as a scenario which not only defines the model output but enables to present different renditions based on user defined actions without altering the model structure and to establish a link to the next scenario run by outputting data sets which can then be used as inputs the description of our framework is following the sequentially implemented mechanisms 1 demographics calculation as the fingerprint of population and social economic conditions 2 agent based modelling abm as the simulation module to estimate demands 3 resource technology network modelling rtn as the decision support module to meet demands from the supply side a series of different software tools following the flow of design execution were used into the making of this platform yaml yaml 2017 human friendly data serialisation language for input output of parameter data java 8 java 2016 to code the demographics abm and rtn modules glpk glpk 2016 as the mixed integer linear programming milp open source optimisation solver and r r project 2016 for post processing and visualising outputs for abm and rtn all of the required packages and libraries used are bundled within the suite itself there is no need for extra installations from the user side to run the platform we will now analyse each of the modelling components separately 2 1 demographics calculation for the specified application sector the following characteristics articulate the agent variability district gender age group work force income access to drinking and non drinking water infrastructure rationing policy and toilet type the combination of these characteristics forms a unique combination which we call an agent combination archetype aca we also define a simpler set of company agents based on their type and number in each sector in relation to water usage we included four attributes for this module district sector water use on business basis m 3 day the changes in demographics are calculated on the basis of an initial master table with all 3 700 acas compiled for the baseline year using several databases the number of people and households belonging to each aca changes over time based on proportionally transposing demographic rates δ to each aca as such a unique master table for each future year is generated that can be used as an input for the abm demand component the influence of urban dynamics such as population migration within the modelled area and migration from to the outside world was not specifically modelled but included on an exogenous scenario basis future extension of the resilience io model to endogenously include this feature or linkages to existing models of urban dynamics galan et al 2009 is envisioned the rate parameter values are logged into a structured yaml file that is read by the demographics module the parameters and variables included in the demographics update of the master table are as follows including naming in light gray colour on the right side as for the rest of our model outline image 7 the first update to the socio demographic master table is carried out as follows to update for aging birth death and migration p t p t 1 δ a d t p t 1 δ d b t p t 1 δ d i t p t 1 δ i e t p t 1 δ e p t 1 p t b t d t m t next socio economic changes in employment are calculated to update the master table based on an assumed decline in unemployment the implementation uses a logistics curve as a simplified approximation of change in a growing economy such that the speed of change can easily be adjusted and a maximum unemployment rate is maintained as δ e m t 1 0 05 e r t 1 e r t max e m e m t 1 e m t δ e m t 1 δ u e t 1 0 05 1 e r t 1 1 e r t max e u e t 1 u e t 1 δ u e t 1 subsequently a set of socio economic calculations are made to update income level changes in the master table the calculation takes into account low medium and high income levels per population category as initiated from income estimates which were inferred from wage estimates and their distribution the calculation allows for either downward or upward migration from to income categories signalling income increase or decrease depending on the required simulation the calculation is carried out as follows l i t 1 l i t m i t l m m i t 1 m i t m i t l m m i t m h h i t 1 h i t m i t m h the number of households related to the population were proportionally estimated using a fixed ratio finally the adjustment for the companies in relation to employment change in a proportional manner is carried out as c c t 1 c c t 1 δ e m 2 2 agent based modelling for the agent based model bonabeau 2002 we used java based repast simphony repast simphony 2016 which is a free agent based simulation toolkit specifically designed for the systematic study of complex system behaviors a description of the abm in the overview design concepts detail odd protocol format grimm et al 2010 is given in table 2 first a synthetic population is generated which represents the actual population of the city visually shown in fig 1 some details about the agents and their attributes are shown in table 1 a population master table which contains over 3700 possible agent combination archetypes acas approximately 250 per district based on nine characteristic combinations as per table 1 is used by the abm to draw a random sample of agents to simulate using the number of people represented by an aca as the probability the result of this process is an agent population with a distribution of agent properties that closely resembles the actual population as represented acas cover 90 of real population variation the final simulation outcome based on individual agents is scaled up by their proportion to obtain results for the whole population of greater accra metropolitan area gama in this agent generation process each agent is allocated to a home district but in addition the model chooses a work location if applicable depending on the work force status this work location is typically near home for people on low income approximately 2 usd per day but can be in other parts of the city region for those with a higher income able to afford public or private vehicle transport the steps to form a demand profile for each district in the study include 1 estimate water use behavior and amount for each category of agent combination archetypes on discrete time intervals e g typically daily or hourly expressed as d i τ τ n n 1 where d i τ is the water demand of the i th archetype at discrete time interval τ and represents each time interval that composes a full day 2 select the proper regression functions using the data to obtain a time dependent formula that can describe at a fine grained 5 min interval the tendency and value range of water demand taking into account aca characteristics such as a polynomial or sum of sine regression models for example d i t j 1 n a j sin b j c j the choice of regression function depends on the water demand pattern that is simulated whereas in principle non linear functions are applicable a polynomial function was tested as having the best fit to the water demand data r 0 98 3 run the agent activity models that links location to water demands using the regression functions to obtain a five minute interval location specific water demand for each aca 4 write csv output files containing the temporal and spatial specific demand data scaled up from the sample simulated agents to the entire population 5 visualise the demand data see fig 5 and input the csv data set to the optimisation model rtn for further planning and operational purpose we initially assumed a fixed 80 of total water demand to be converted to waste water to be treated based on standard loss assumptions tchobanoglous et al 2014 this is to simply link the amount of waste water to total water usage however the user can define a different percentage or absolute values per district for the waste water generated inside the yaml data file the amount of waste water can also be obtained from abm based on agents activities if a more specific study is desired 2 3 resource technology network modelling to enable the open source and open access character of the platform the optimisation module should be able to basically reproduce the functionality of a modelling language such as the general algebraic modelling system gams with as possible minimum loss of generality this means that the java code should be capable of building the model based on the inputs from scratch and pass the formulated optimisation problem to the solver as any modelling language would do furthermore in order to cross validate consistency and check the integrity of the generated models from the rtn component we compared the equivalent model written in gams and performed element to element comparison of the compiled mps files respectively in matlab the resource technology network rtn is a mixed integer linear programming milp model bertsimas and tsitsiklis 1997 derived from a retrofit version of the original technologies and urban networks turn model kierstead and shah 2013 it calculates the flows and inter conversion of resources given specific technological infrastructure via a spatial and temporal approach the run results are suggested investments in technologies or transport network expansion following a cost minimisation approach so as to meet the projected water and sanitation demands generated from the abm component a distinct degree of flexibility is given throughout the whole model including the capability to introduce new technologies and any input output flow vector materials wastes energy labour and emissions allowing imports and exports restricting flows and defining upper bounds investment costs and leaks of transport links as well as changing maps of the initial transport network and setting where cell to cell flow expansion is admissible feasible also the optimisation uses a flexible weighted multi objective that can in the current version be tailored to incorporate economic financial cost environmental waste and emissions and social aspects labour in the use case implementation a joint financial cost and greenhouse gas emission minimisation objective was used a more detailed discussion about the way the user can craft input data to answer different questions is also given in section 4 the following is the description of the rtn model we used image 8 a set of constraints is defined to upper bound production rates the allocation of the units of technologies in relation to investments as well as to simulate how the resource balance is calculated as a result of production imports and bidirectional flows and leaks on the transport network so as to satisfy demands in potable water the weighted sum of metrics is the objective function to be minimised key performance indicators kpis serve as the metrics here as per the use case opex capex and emissions of ghg in c o 2 image 9 in terms of linear programming lp we will show how the model represents an milp problem consider the following primal linear programming problem in standard format m i n i m i s e c t x s u b j e c t t o a x b x 0 where x are the decision variables a r m n b r m c x r n t denotes transposition and r a n k a m 1 m n it is now easy to link the model components with the strict mathematical form of the milp problem coefficient matrix a is the set of constraints row by row as production limitations flow constraints mass balance etc c is the objective function weighted sum of metrics and b is the right hand side r h s which reflects to the specific resource demands in this case variable vector x is the combined sets of variables column by column in the problem p n q i n v r s l e a k s y 1 y 2 i m v m positivity of some variables is required as shown n i n v need to be integers and y 1 y 2 are binary variables thus converting the original lp to milp this set of variables is to be determined in such a way that all constraints are satisfied and at the same time the objective function is optimised this includes the production values p the number of units n for each different type of technology as well as the investments the production surplus r s in each cell and the flow amount q from to each cell for each resource being able to flow imports i m define resource inputs without the need of implemented technologies to produce them and the weighted sum of metrics is forming the objective function as a dot product of capex opex and c o 2 with their respective weights for the transport network topology and the associated flow constraints we use binary variables which stand for whether the network is already built 1 or not 0 between all the possible combinations of the cells the feasibility of constructing a pipe or technology in a cell is also set based on a binary variable approach to rule out impossible placements e g seawater desalination plants inland 3 data implementation details one of the most important structures in the platform is the demand for specific resources such as potable clean water which is generated by the abm module the demand is a function of resources cells and minor and major periods of time this enables flexibility in customising demand values differently within the same day over a year s length for instance also to simulate seasonal variation besides having the ability to read the abm demands using the generated csv file the user can also run the demands as manually defined exogenous inputs by defining them inside a structured yaml file this is to allow modular execution of the rtn component without the need to execute the abm module first a full list of components inside the yaml input data file are given in table 4 a yaml object is linked with a java class object in the sense that all of the yaml components are actually attributes of a single java class instance in this way maintenance and upgrade of the data input of this platform is straightforward clean and easy to implement a scheme showing how single read output yaml commands can access all of the scenario attributes at once for a single run is shown in fig 3 additionally no yml is a yaml file generated by the platform which contains the optimal infrastructure with investments so it can be used as a pre allocation status for a future run finally a series of sanity checks are performed upon reading the yaml data file to ensure consistency of the expected sizes and content of the components some fundamental technical attributes of the technological infrastructure are required in the yaml data file for instance capacity factors and nameplate capacities play a role in defining the maximum production rates of the technological units a series of tables are used to summarise conversion relationships between resources and technologies efficiency coefficients and flow import investments costs we also use a 2 d coordinate system for the centroids of the cells to calculate their relational distance in kilometers in addition the extra component in the yaml file c o o r d s serves as a way of changing the visual representation geolocalisation of the cells without affecting the calculations of resource flow cost which is handled by the x c y c attributes besides the coordinate requirement for visual representation this is to additionally scale the operational cost of flows or pipe expansions based on the distance of the corresponding flow path the link between the abm generated demands and the milp optimisation problem is clearly plugged in at the right hand side rhs of the milp problem and specifically in the mass balance constraint rows as the abm finishes calculating projected demands the results are stored in a csv file which can then be sequentially read to initialise demands for the rtn component the yaml file is read to provide parameter values for the milp model which are logged inside java via the glpk interface when the milp problem is built and passed to the optimisation solver to acquire the optimal solution if such exists although the model is feasible in nature investments can be used to further extend production output so as to meet demands some modifications can affect feasibility these include bounding the investments in specific cells in combination with the absence and limited expansion availability defined by the user of pipe network connectivity which if not restricted would allow for surplus production to flow so as to meet demands the platform has been designed and optimised using memory pre allocation sparse matrix handling with the ujmp java class package ujmp 2016 being essential to allow scaling of the model for very large dimensions using small sizes of ram fine tuning of the solver for the specific formulation etc for use with medium specifications laptops therefore it is lightweight in the required computational power and produces results within very reasonable cpu time for single scenario runs usually a few minutes or even less depending on the complexity of the problem the whole platform consists of the three modules demographics abm and rtn the required java run time environment jre and r programming language packages all bridged together upon execution it outputs automated plots inside categorised investments network flows infrastructure etc pdf files the graphical user interface gui shown at the initial execution of the platform is shown in fig 2 the ability to solve the formulated milp optimisation problem with commercial solvers such as cplex is also possible as the platform exports the mps problem format via the glpk interface before applying the glpk milp solver the overall flow of the implementation is shown in fig 4 the computational environment we used for the use case is shown in table 3 the total run time for all scenarios was around 23 min 1400 s most of the milps formulated under the rtn model are extremely sparse for instance for the 2030 sdg s scenario the milp is of 2 996 4 068 size rows columns or constraints variables and 12 700 non zeros the number of non zero entries in the coefficient matrix a of the milp problem which accounts for 12 700 2 996 4 068 0 1 density this enables wide applicability and accelerated converge with the latest mixed integer linear programming solvers at our disposal 4 functionality a series of specific questions can be answered via the optimisation module what kind of investments do we need to meet growing demands and sustainable development targets what capacity of which type and where to place the required technologies which changes to the resource flows transport network between cells e g city districts are needed what is the proposed topology of the pipe network how large is the capex associated with pipe expansions what is the operational cost per year and per capita how can the total output of c o 2 for the simulated period per year be reduced or managed what happens if a distinct connection or node e g transmission pipe facility becomes non functional for instance after an environmental calamity and what would a resilient network topology be that still allows for meeting demands how much of the flow resource is lost in the transport network e g pipes grids due to leaks losses and how does this affect results e g opex capex co2 which novel technologies are promising to use in terms of technical specifications costs and emissions what is the impact on system performance metrics of existing infrastructure plans if implemented what if we only substitute specific technologies and their numbers in specific districts or limit the expansion of some in specific and focus on different investments how is the network topology altered if we use transport links with different flow rates volumes in order to deliver insights into these questions the user can craft the input data so as to fashion the desirable simulation the analytical scenario is tailored by imposing limitations on resources technologies the flow network and policies specified by the user inside the yaml data text file the pre defined yaml structure as a series of tables and lists is used for this purpose such as to inject bounds to the expansion of the pipe network investments flows import of specific resource inputs needed e g raw water electricity etc define the weights of the metrics in the objective function so as to for instance give higher priority in a solution which is more emissions friendly experiment with different percentages of the abm demands pre allocate infrastructure and pipe network as well as upper bound each of those as desired specify a percentage of leaks on the existing pipe network as well as defining an available budget for the total cost among other settings 5 use case envisioning outcomes of ongoing wash projects and steps to meet wash targets the need to advance infrastructure planning capacity in sub sahara african countries is evident from their uneven progress as reported in the un sustainable development agenda 2030 united nations general assemply 2015 the capital city of ghana accra and fourteen neighboring administrative districts form the greater accra metropolitan area gama the geographic definition of gama as a city region was defined by local stakeholders using the locally defined metropolitan and municipal district assembly mmda structures in the country ghana statistical service 2014 from here on referred to as districts model calculations are carried out at the district level as a network to enable output matching to local planning targets set in district government medium term development plans dmtdp and are aggregated to the gama level in the results presented here gama is a rapidly growing metropolitan region where efforts to improve the water sanitation and hygiene wash situation have yielded mixed results household access to improved piped water grew on average by 81 to 83 from 2000 to 2010 and access to public and private improved toilet facilities increased from 58 to 81 ghana statistical service 2005 bentsi enchill et al 2010 however the percentage of waste water including waters which contain human excreta which was treated declined from around 10 to near zero between 2000 and 2010 whilst the population grew from three to four million people the city in recent years expanded potable water treatment at the kpone site by 190 000 m 3 day and opened a 60 000 m 3 day desalination plant resulting in an aggregate 54 treatment capacity increase in terms of waste water treatment the situation has deteriorated significantly as two existing large scale treatment plants the ama jamestown conventional treatment plant 16 000 m 3 day capacity and the tema community 3 lagoon treatment plant 20 000 m 3 day capacity broke down in 2004 and 2000 respectively the majority of waste water now ends up untreated in the environment directly or after collection and disposal the situation is not improving as the capacity of recently opened facilities is quite small which include a lagoon treatment plant at the university of ghana legon 6 400 m 3 day capacity and a human excreta de watering and sludge drying plant at the lavender hill beach site 800 m 3 day efforts are underway to rehabilitate the jamestown treatment plant but have met financing difficulties this use case serves to demonstrate how resilience io can provide knowledge support to implement macro planning targets for gama to improve the wash situation relative to the baseline year 2010 selected targets include those in the sustainable development goals sdgs and the ghana water sector strategic development plan wssdp ministry of water resources works and housing accra 2014 for 2012 to 2025 as developed by the ministry for water resources works and housing mwrwh the overarching urban objectives in the wssdp plan are to increase urban water and sanitation coverage to 100 in 2025 the use case calculates what combined projects infrastructure change and financing needs are required to meet these goals under different scenarios also taking into account the impact of currently ongoing projects on wash service access once completed the incorporated ongoing projects the world bank 2013 that we included in our simulation are accra sewerage improvement project asip gh gama sanitation and water project gh gama danida lavender hill sludge treatment slamson ghana korle lagoon cesspit treatment jamestown korle lagoon sewerage plant rehabilitation and the mudor faecal treatment plant also already completed projects were added if finished after the 2010 baseline and include teshie nungua desalination plant kpong china gezhouba and kpong tahal water treatment expansion we analysed the requirements from 2010 to 2030 to meet wash goals across different scenarios i wssdp decentralised districts where no pipe extensions are allowed with requirements to reach national 2025 wash targets focusing on district level infrastructure ii sdgs centralised with flexible pipe extensions with requirements to reach 2030 wash targets for the sustainable development goals based on city wide infrastructure and iii sdgs centralised and leakage reduction similar to the city wide systems scenario except that leakage rates from water and waste water pipe systems are set to 17 from the original 27 in the system from 2020 onward 6 results the use case input data was based on the 2010 ghana census among other private and public sources a series of technical characteristics were incorporated including pre allocation of existing infrastructure pipe network maps from to districts potable and waste water calibrated admissible transmission pipe and infrastructure expansions district to district transmission pipe leakage local tariff regulations on water use and waste water treatment and fifteen wash technology data sets including operational material and energy inputs and outputs operational labour inputs and financial investment and operational cost a summary of key indicators provided by the model as outputs is shown in table 5 population figures in gama grow between 2015 and 2030 from 4 4 to 6 5 million based on the demographics calculations we performed which included birth death and migration rates from the ghana census and were calibrated to un urban population projections to have a measure of comparison for our results we use the 2000 2015 estimates of the who for the per person costs to reach wash targets in sub saharan africa summarised in their report who 2004 the prediction was that the annual cost per person receiving interventions specifically intervention 5 is more closely linked to sdg scenario as presented here in table 14 would be around usd25 4 in a twenty years projection for 2010 2030 here this would account for nearly usd762 our calculations predict a cost of usd607 in the sdg 27 leaks scenario which is of the same order of magnitude 6 1 potable water potable water production from capacity extensions in the decentralised case rises from 0 54 to 0 85 million m 3 day between 2015 and 2030 in contrast in the sdgs infrastructure scenarios with leaks of 27 and 17 demand grows from 0 54 in 2015 to 0 87 and 0 76 million m 3 day in 2030 respectively for 2030 pipe leakage was evaluated at i wssdp decentralised 0 26 million ii sdgs centralised 0 27 million and iii sdgs centralised and leakage reduction 0 16 million m 3 day potable water demands without leaks were 0 58 million m 3 day in 2030 the lower sdg scenarios values arise from an optimised supply demand infrastructure location and pipe network resulting in lower pipe water flows and thus leakage by allowing pipe network extensions the total investment cost in new treatment capacity by 2030 to meet 100 potable water demands is 1 5 1 4 and 1 0 billion usd in the wssdp decentralised sdgs centralised and sdgs centralised and leakage reduction scenarios respectively investments were also calculated for potable and waste water pipe network expansions at 0 11 billion usd 2010 2030 for both sdgs centralised without and with leakage reduction respectively 6 2 waste water treatment the waste water evaluation yielded a required 0 47 million m 3 day in treatment capacity in 2030 in all three scenarios concerning leaks in 2030 2 085 m 3 day untreated waste water pipe leakage was established for the sdg centralised scenario leakage is so low because waste water treatment is found to be most cost effective using local technologies including aerated lagoons activated sludge and anaerobic digestion systems in each district thereby limiting the use of district to district waste water pipe networks capital expenditure for waste water treatment capacity from 2010 to 2030 for the three scenarios was 0 33 0 33 and 0 35 billion usd total operational cost for 2030 including potable water treatment can be found in table 5 6 3 emissions jobs the environmental impacts were captured by calculating total ghg emissions of the city region wash system since a small portion of waste water is currently treated ghg emissions from treatment infrastructure rises substantially in all scenarios in 2015 total system emissions were 3 7 thousand tonnes which increases to 107 thousand tonnes by 2030 in the three scenarios primarily due to waste water treatment growth from near 0 100 waste water treatment is fairly ghg intensive due to micro organisms turning sludge into either carbon dioxide or methane which ends up into the environment the calculations do not take into account what happens with untreated waste water that ends up in the environment as a baseline since a portion will naturally decompose aerobically or anaerobically in the environment by micro organisms the total system ghg emissions in k g c o 2 e q u i v a l e n t m 3 of treated water and waste water in 2030 are 81 80 and 87 thousands of tonnes correspondingly total jobs for all water and waste water treatment and distribution grow from 2 743 in 2015 to 3 253 4 380 and 3 862 jobs by 2030 in the wssdp decentralised sdgs centralised and sdgs centralised and leakage reduction scenarios respectively 7 discussion the functionality in the presented resilience io platform use case can be compared to iuwcm and iuwsm models since resilience io covers the water and waste water demand and supply cycle is able to incorporate other resource and pollutant vectors labour requirements and allows milp optimisation over both economic and environmental indicators in the use case using greenhouse gas emissions our optimisation approach allows exploration of wash system design by setting minimum performance criteria this is different from previous approaches like in watermet2 and uwot which calculate the performance of user defined designs and interventions particular differences of watermet2 to resilience io are i the approach is to test human designed plans as opposed to finding optimised supply resource technology system configurations ii a fixed set of treatment technologies with input output vectors as opposed to flexible introduction of new technologies in resilience io based on matrix insertion and iii a limit to daily time steps to settle supply and demand instead of hourly particular differences of uwot to resilience io are i the focus is on potable water supply and does not include waste water treatment ii the approach is to test the optimal operation of a user input based pipeline reservoir treatment system given a set of simulation demands as opposed to allowing for finding optimised entirely new network technology configurations iii uwot allows for appliance fixed household location demand as opposed to more flexible spatially mobile agents activity based demands the distinct advantages of our approach are that the platform is built for release as non commercial software we use behavioral modelling in a multi agent system to generate demands which enables simulations that can exploit fine grained variations in spatially defined variables of a population bonabeau 2002 bousquet and page 2004 multi user type variation as regarded effective in this context rixon et al 2007 we provide decision support for supply solutions via mathematical programming optimisation guignard spielberg and spielberg 2005 that can be tailored to specific sustainability needs for technology investment and operation due to flexible input output dimensions for each technology and a flexible multi criteria objective function our implementation is data driven and neither system sector nor region bounded and can be utilised in any city region for any demand supply flow context with the possibility of system specific functionality expansions the main limitations of the current state of the platform are that we do not yet cover all wash system features in particular water flow quality characterisation natural water reservoir stock dynamics and atmospheric water flows like storm water and flooding we do not provide automated functionality to rapidly evaluate the impacts of parameter uncertainties on wash system design scenario outcomes the platform is mainly usable by expert users due to the high level of technical detail involved combined with a text line data entry implementation 8 conclusions the resilience io platform results showed at a practical application level the possibilities of fine grained modelling of population characteristics technological facilities and associated resource and waste flows in space and time to provide insights for more resilient city region systems planning within a broader scope of economic social and environmental metrics in particular support was demonstrated for evaluation of future infrastructure needs for population and economic scenarios provisioning of total cost estimates to meet national and sdg targets visibility on environmental and social decision criteria such as jobs and ghg emission impacts and evaluation and prioritisation of planning options by quantifying differences between i infrastructure expansion and pipe network layout changes versus ii substantial reduction in transport network losses at a technical level the feasibility of an open source based implementation of an abm coupled to a milp optimisation model was demonstrated it can simulate the demand and resource flow based supply dynamics for integrated economic and environmental modelling purposes in a city region of several million people the implementation also shows the importance of a flexible design without adjusting the model structure but by changing only the input data file and then running the model again key adjustments can be evaluated in technology flow network and policy settings the multi objective criteria that are minimised can also be altered and can include both economic and environmental objectives the platform was also found to have emerging benefits from the inclusion of population income and labour aspects these aspects enabled the evaluation of the capacity a city region needs to implement proposed solutions at a decentralised government level this is achieved by quantifying labour needs and operational revenue flow requirements capacity at a financial labour and planning level is of particular concern for the sustainable development of low to lower middle income countries for example in ghana for which the use case was built the district governments are responsible for waste water treatment however these governments receive low tax revenues due to a large informal sector resulting in low staffing less than one local government staff per 1000 people versus 5 for high income countries and limited ability to attract national government or private sector finance future work includes the extension of the platform to support uncertainty in the modelling formulation for various parameters such as capital and operational expenditure or the magnitude of demand in resources and the design of a graphical user interface dedicated to generate yaml data files and expand visual output possibilities in a faster and user friendly way to enable uptake outside of the expert user community embedding different optimisation solvers to explore efficiency is also an interesting avenue acknowledgments a prodigious debt of gratitude goes to the department for international development uk for funding this activity under the future cities africa project grant number 203830 we also thank the ecological sequestration trust the ecological sequestration trust 2017 for arranging several webinars with local stakeholders from ghana and the platform demonstration debut june 16 at ghana the cities alliance international cities alliance 2017 and ghana team for their management support project debut workshop and insightful inputs also local experts from ghana foster mensah 3 3 centre for remote sensing and geographic information services university of ghana legon annie jiagge road accra ghana sampson madana arthur bernard and ohene ofori among many others were of immense help in data contributions and use case input appendix an example of a scenario in yaml data is given below as well as some automated graphical visual outputs from the platform are demonstrated below image 6a image 6b image 6c image 6d image 6e image 6f image 6g image 6h fig 6 an example of automated visual outputs from the platform geo localised districts in gama showing the initial pipe network for potable water fig 6 fig 7 the suggested by the platform expansions for 2030 in the sdg s 27 leaks scheme in red arcs to satisfy more efficiently the demands the expansions take into account the cost of pipe construction per meter of the respective distance between districts fig 7 fig 8 the optimal flows of potable water for 2030 in the sdg s 27 leaks scheme fig 8 fig 9 decentralised districts scenario in 2030 projection a large amount of aerated lagoons and decentralised activated sludge systems to facilitate the waste water treatment across gama fig 9 fig 10 2030 in the sdg s 27 leaks scheme inputs of raw water and electricity to meet 100 demands fig 10 
26430,it is crucial for sustainable planning to consider broad environmental and social dimensions and systemic implications of new infrastructure to build more resilient societies reduce poverty improve human well being mitigate climate change and address other global change processes this article presents resilience io 2 a platform to evaluate new infrastructure projects by assessing their design and effectiveness in meeting growing resource demands simulated using agent based modelling due to socio economic population changes we then use mixed integer linear programming to optimise a multi objective function to find cost optimal solutions inclusive of environmental metrics such as greenhouse gas emissions the solutions in space and time provide planning guidance for conventional and novel technology selection changes in network topology system costs and can incorporate any material waste energy labour or emissions flow as an application a use case is provided for the water sanitation and hygiene wash sector for a four million people city region in ghana keywords integrated environmental modelling iem decision support sub saharan africa wash agent based modelling mathematical programming 2010 msc 97m10 92d40 90c11 91b69 93a30 93b40 software data availability the resilience io wash model was built by charalampos p triantafyllidis xiaonan wang koen h van dam kamal kuriyan and nilay shah from imperial college london with data and technical support by rembrandt koppelaar zoltan kis and hannes kunz from the institute of integrated economic research the platform is designed for open source future release by using only open source software a beta version of the platform is available upon request from the authors including the data set to reproduce the presented use case the platform is of approximately 800mb in size on a windows based machine further details concerning the hardware software used can be found in table 3 1 introduction several modelling techniques have been proposed to gain insights in the delivery of sustainable goals in a scientific manner recently water quality performance assessment has been under the microscope massoudieh et al 2017 furthermore in alhamwi et al 2017 a gis based platform was introduced to facilitate storage and flexibilisation of technologies in urban areas in cominola et al 2015 the need for models that describe exogenous drivers affecting water and demand management was highlighted to inform strategic planning and policy formation while various platforms were also presented deoreo et al 1996 kowalski and marshallsay 2003 froehlich et al 2009 beal et al 2010 in hu et al 2015 multi threaded programming with hadoop based cloud computing was used to implement a multi agent system for environmental modelling furthermore in berglund 2015 agent based models abms are reviewed in the context of water management planning decisions the benefit of planning based on advanced system models lies in the ability to rapidly carry out project evaluation within a complex city systems perspective such models can remove the divide between high level master planning and isolated low level project planning by enabling a link of feedback between the two the exploration of more options and trade offs and enhanced visibility on positive and negative impacts in multiple dimensions thus lowering the risk and cost of planning key is not to rely on a pure techno economic planning approach e g reductionist infrastructure cost estimates since sustainable development social parameters and cost and benefits are inextricably bound together white and lee 2009 concerning the presented use case application models focusing on specific components of the water cycle are various and include agent based water demand models with attitudes norms and behavioral control towards water use koutiva and makropoulos 2016 linear optimisation lo models for water supply pipe networks sarbu and ostafe 2016 and sewer pipe networks safavi and geranmehr 2016 mixed integer non linear programming minlp approaches for pipe network optimisation moeini and afshar 2013 afshar et al 2015 groundwater and rainwater storage and network minlp chung et al 2009 mixed integer linear programming milp model based water treatment technology planning including energy use estimations alqattan et al 2015 and water reuse potential estimations using a milp architecture liu et al 2015 complete water system models are less common a class referred to as integrated urban water cycle models iuwcms when including all water cycle aspects and integrated urban water system models iuwsms that also integrate social environmental economic and other resource flows like energy bach et al 2014 based on a literature review from 1990 to 2015 a total of fourteen existing iuwsms were documented peña guzmán et al 2017 although some upon evaluation appear to be misclassified given their too narrow usability such as the commercial aquacycle tool that focuses on rain and storm water modelling sharma et al 2008 or the commercial mike urban software package for hydrology and flood modelling berggren et al 2012 social elements were covered in one out of fourteen models de haan et al 2013 and five allowed for energy and emissions the two most complete iuwsm models were found to be watermet2 behzadian et al 2014b and urban water optioneering tool uwot rozos and makropoulos 2013 watermet2 is an open access difference differential equation simulation model that integrates both natural and human water and waste water systems inclusive of four different scales indoor local catchment and city areas these include the ability to model water demand and supply balances pipeline fluxes energy requirements and greenhouse gas emissions behzadian et al 2014a uwot is an available on request spatially fixed fine grained minute to hour appliance household demand model with aggregation to neighborhood and city scale supply is spatially solved using genetic algorithm optimisation of a pre defined treatment plant and pipe network for water flow allocation the platform is built in simulink matlab linked to a technology and pipeline network library database rozos and makropoulos 2013 the main limitation of these tools is their rigidity as their goal is to analyse the performance of a user designed wash technology network system inclusive of any future interventions this as opposed to starting with performance criteria as constraints on the system including economic environmental and social requirements and using the tool to explore urban water system design solutions that fall within the desired performance to meet future urban water cycle needs the gap we attempt to bridge with this platform is therefore the following since technological innovation and planning is arguably crucial to achieve sustainable development anadon et al 2016 how can technical tools assist in i exploring planning solutions and providing the quantitative evidence on meeting the broader economic social and environmental requirements for implementing technological investments on the ground and ii bring learning and engagement of stakeholders in the full development cycle of such tools the first part is materialised by a systems modelling scheme we embrace the assemblage approach by combining polished and well established modelling formalisms including agent based modelling and mathematical programming abms are becoming more and more popular in environmental modelling sun et al 2016 and at the same time mathematical programming has seen enormous progress in the development of polished consistent and accurate solvers across the latest two decades while also being one of the most widely used tools to inform transparent decision making the intention of this approach is as analysed in voinov and shugart 2013 to allow models to exchange communication on run time as an integrated suite which outputs meaningful results and can potentially answer an array of questions that decision makers are interested in the second part is effectively achieved by the open access development cycle of the platform a major degree of involvement by potential users and stakeholders across a variety of disciplines took place to elaborate on the optimal design implementation and usefulness of the platform while not over complicating the models by conducting a series of communication exchanges with local experts in ghana from various institutions e g ghana water company university of ghana etc and delivery of an early version for testing we tried to shape the platform to best serve the user needs to assess policy implications so as to reach sustainability targets in the most realistic manner this article is organised as follows in section 2 we present the modelling techniques built in to the platform in section 3 we describe the link between the models and the input data and the associated mechanisms as well as the major implementation details section 4 demonstrates a series of questions that can be potentially answered by using resilience io section 5 presents the wash sector use case as a first application as well as the context upon which the analysis was built results follow in section 6 a comparison of the developed platform with the watermet2 and uwot models as well as limitations and advantages of our approach are given in the discussion section 7 we give our conclusions in section 8 finally the data set for the milp optimisation and examples of automated plots from the platform can be found in the appendix 2 methods as a novel implementation of the concept to combine technical planning with socio economic evaluation we built a set of models of infrastructure systems at a fine grain spatial and temporal scale within a socio economic context we start with human behavior and decisions by first defining population variability bentsi enchill et al 2010 ghana statistical service 2010 and simulating how it affects daily activities and location from which needs emerge for instance water demand as well as environmental impacts generation of waste water we therefore apply predictive inductive modelling in the sense that we used predefined sets of variables population characteristics and their projected change into the future that are coupled with demand requirements we then connect these demands to a supply flow optimisation model which links technology inputs and outputs in any dimension energy materials emissions wastes labour to find multi objective solutions to technology investment and operation including cost and sustainability criteria we follow the meticulous review in hamilton et al 2015 concerning the dimensions upon which software based tools which facilitate integrated environmental modelling iem should be designed and developed we also employ data sets as the intermediate tool between modular modelling methodologies presented herein so as to follow the discipline presented in laniak et al 2013 to this respect we treat the whole data set package as a scenario which not only defines the model output but enables to present different renditions based on user defined actions without altering the model structure and to establish a link to the next scenario run by outputting data sets which can then be used as inputs the description of our framework is following the sequentially implemented mechanisms 1 demographics calculation as the fingerprint of population and social economic conditions 2 agent based modelling abm as the simulation module to estimate demands 3 resource technology network modelling rtn as the decision support module to meet demands from the supply side a series of different software tools following the flow of design execution were used into the making of this platform yaml yaml 2017 human friendly data serialisation language for input output of parameter data java 8 java 2016 to code the demographics abm and rtn modules glpk glpk 2016 as the mixed integer linear programming milp open source optimisation solver and r r project 2016 for post processing and visualising outputs for abm and rtn all of the required packages and libraries used are bundled within the suite itself there is no need for extra installations from the user side to run the platform we will now analyse each of the modelling components separately 2 1 demographics calculation for the specified application sector the following characteristics articulate the agent variability district gender age group work force income access to drinking and non drinking water infrastructure rationing policy and toilet type the combination of these characteristics forms a unique combination which we call an agent combination archetype aca we also define a simpler set of company agents based on their type and number in each sector in relation to water usage we included four attributes for this module district sector water use on business basis m 3 day the changes in demographics are calculated on the basis of an initial master table with all 3 700 acas compiled for the baseline year using several databases the number of people and households belonging to each aca changes over time based on proportionally transposing demographic rates δ to each aca as such a unique master table for each future year is generated that can be used as an input for the abm demand component the influence of urban dynamics such as population migration within the modelled area and migration from to the outside world was not specifically modelled but included on an exogenous scenario basis future extension of the resilience io model to endogenously include this feature or linkages to existing models of urban dynamics galan et al 2009 is envisioned the rate parameter values are logged into a structured yaml file that is read by the demographics module the parameters and variables included in the demographics update of the master table are as follows including naming in light gray colour on the right side as for the rest of our model outline image 7 the first update to the socio demographic master table is carried out as follows to update for aging birth death and migration p t p t 1 δ a d t p t 1 δ d b t p t 1 δ d i t p t 1 δ i e t p t 1 δ e p t 1 p t b t d t m t next socio economic changes in employment are calculated to update the master table based on an assumed decline in unemployment the implementation uses a logistics curve as a simplified approximation of change in a growing economy such that the speed of change can easily be adjusted and a maximum unemployment rate is maintained as δ e m t 1 0 05 e r t 1 e r t max e m e m t 1 e m t δ e m t 1 δ u e t 1 0 05 1 e r t 1 1 e r t max e u e t 1 u e t 1 δ u e t 1 subsequently a set of socio economic calculations are made to update income level changes in the master table the calculation takes into account low medium and high income levels per population category as initiated from income estimates which were inferred from wage estimates and their distribution the calculation allows for either downward or upward migration from to income categories signalling income increase or decrease depending on the required simulation the calculation is carried out as follows l i t 1 l i t m i t l m m i t 1 m i t m i t l m m i t m h h i t 1 h i t m i t m h the number of households related to the population were proportionally estimated using a fixed ratio finally the adjustment for the companies in relation to employment change in a proportional manner is carried out as c c t 1 c c t 1 δ e m 2 2 agent based modelling for the agent based model bonabeau 2002 we used java based repast simphony repast simphony 2016 which is a free agent based simulation toolkit specifically designed for the systematic study of complex system behaviors a description of the abm in the overview design concepts detail odd protocol format grimm et al 2010 is given in table 2 first a synthetic population is generated which represents the actual population of the city visually shown in fig 1 some details about the agents and their attributes are shown in table 1 a population master table which contains over 3700 possible agent combination archetypes acas approximately 250 per district based on nine characteristic combinations as per table 1 is used by the abm to draw a random sample of agents to simulate using the number of people represented by an aca as the probability the result of this process is an agent population with a distribution of agent properties that closely resembles the actual population as represented acas cover 90 of real population variation the final simulation outcome based on individual agents is scaled up by their proportion to obtain results for the whole population of greater accra metropolitan area gama in this agent generation process each agent is allocated to a home district but in addition the model chooses a work location if applicable depending on the work force status this work location is typically near home for people on low income approximately 2 usd per day but can be in other parts of the city region for those with a higher income able to afford public or private vehicle transport the steps to form a demand profile for each district in the study include 1 estimate water use behavior and amount for each category of agent combination archetypes on discrete time intervals e g typically daily or hourly expressed as d i τ τ n n 1 where d i τ is the water demand of the i th archetype at discrete time interval τ and represents each time interval that composes a full day 2 select the proper regression functions using the data to obtain a time dependent formula that can describe at a fine grained 5 min interval the tendency and value range of water demand taking into account aca characteristics such as a polynomial or sum of sine regression models for example d i t j 1 n a j sin b j c j the choice of regression function depends on the water demand pattern that is simulated whereas in principle non linear functions are applicable a polynomial function was tested as having the best fit to the water demand data r 0 98 3 run the agent activity models that links location to water demands using the regression functions to obtain a five minute interval location specific water demand for each aca 4 write csv output files containing the temporal and spatial specific demand data scaled up from the sample simulated agents to the entire population 5 visualise the demand data see fig 5 and input the csv data set to the optimisation model rtn for further planning and operational purpose we initially assumed a fixed 80 of total water demand to be converted to waste water to be treated based on standard loss assumptions tchobanoglous et al 2014 this is to simply link the amount of waste water to total water usage however the user can define a different percentage or absolute values per district for the waste water generated inside the yaml data file the amount of waste water can also be obtained from abm based on agents activities if a more specific study is desired 2 3 resource technology network modelling to enable the open source and open access character of the platform the optimisation module should be able to basically reproduce the functionality of a modelling language such as the general algebraic modelling system gams with as possible minimum loss of generality this means that the java code should be capable of building the model based on the inputs from scratch and pass the formulated optimisation problem to the solver as any modelling language would do furthermore in order to cross validate consistency and check the integrity of the generated models from the rtn component we compared the equivalent model written in gams and performed element to element comparison of the compiled mps files respectively in matlab the resource technology network rtn is a mixed integer linear programming milp model bertsimas and tsitsiklis 1997 derived from a retrofit version of the original technologies and urban networks turn model kierstead and shah 2013 it calculates the flows and inter conversion of resources given specific technological infrastructure via a spatial and temporal approach the run results are suggested investments in technologies or transport network expansion following a cost minimisation approach so as to meet the projected water and sanitation demands generated from the abm component a distinct degree of flexibility is given throughout the whole model including the capability to introduce new technologies and any input output flow vector materials wastes energy labour and emissions allowing imports and exports restricting flows and defining upper bounds investment costs and leaks of transport links as well as changing maps of the initial transport network and setting where cell to cell flow expansion is admissible feasible also the optimisation uses a flexible weighted multi objective that can in the current version be tailored to incorporate economic financial cost environmental waste and emissions and social aspects labour in the use case implementation a joint financial cost and greenhouse gas emission minimisation objective was used a more detailed discussion about the way the user can craft input data to answer different questions is also given in section 4 the following is the description of the rtn model we used image 8 a set of constraints is defined to upper bound production rates the allocation of the units of technologies in relation to investments as well as to simulate how the resource balance is calculated as a result of production imports and bidirectional flows and leaks on the transport network so as to satisfy demands in potable water the weighted sum of metrics is the objective function to be minimised key performance indicators kpis serve as the metrics here as per the use case opex capex and emissions of ghg in c o 2 image 9 in terms of linear programming lp we will show how the model represents an milp problem consider the following primal linear programming problem in standard format m i n i m i s e c t x s u b j e c t t o a x b x 0 where x are the decision variables a r m n b r m c x r n t denotes transposition and r a n k a m 1 m n it is now easy to link the model components with the strict mathematical form of the milp problem coefficient matrix a is the set of constraints row by row as production limitations flow constraints mass balance etc c is the objective function weighted sum of metrics and b is the right hand side r h s which reflects to the specific resource demands in this case variable vector x is the combined sets of variables column by column in the problem p n q i n v r s l e a k s y 1 y 2 i m v m positivity of some variables is required as shown n i n v need to be integers and y 1 y 2 are binary variables thus converting the original lp to milp this set of variables is to be determined in such a way that all constraints are satisfied and at the same time the objective function is optimised this includes the production values p the number of units n for each different type of technology as well as the investments the production surplus r s in each cell and the flow amount q from to each cell for each resource being able to flow imports i m define resource inputs without the need of implemented technologies to produce them and the weighted sum of metrics is forming the objective function as a dot product of capex opex and c o 2 with their respective weights for the transport network topology and the associated flow constraints we use binary variables which stand for whether the network is already built 1 or not 0 between all the possible combinations of the cells the feasibility of constructing a pipe or technology in a cell is also set based on a binary variable approach to rule out impossible placements e g seawater desalination plants inland 3 data implementation details one of the most important structures in the platform is the demand for specific resources such as potable clean water which is generated by the abm module the demand is a function of resources cells and minor and major periods of time this enables flexibility in customising demand values differently within the same day over a year s length for instance also to simulate seasonal variation besides having the ability to read the abm demands using the generated csv file the user can also run the demands as manually defined exogenous inputs by defining them inside a structured yaml file this is to allow modular execution of the rtn component without the need to execute the abm module first a full list of components inside the yaml input data file are given in table 4 a yaml object is linked with a java class object in the sense that all of the yaml components are actually attributes of a single java class instance in this way maintenance and upgrade of the data input of this platform is straightforward clean and easy to implement a scheme showing how single read output yaml commands can access all of the scenario attributes at once for a single run is shown in fig 3 additionally no yml is a yaml file generated by the platform which contains the optimal infrastructure with investments so it can be used as a pre allocation status for a future run finally a series of sanity checks are performed upon reading the yaml data file to ensure consistency of the expected sizes and content of the components some fundamental technical attributes of the technological infrastructure are required in the yaml data file for instance capacity factors and nameplate capacities play a role in defining the maximum production rates of the technological units a series of tables are used to summarise conversion relationships between resources and technologies efficiency coefficients and flow import investments costs we also use a 2 d coordinate system for the centroids of the cells to calculate their relational distance in kilometers in addition the extra component in the yaml file c o o r d s serves as a way of changing the visual representation geolocalisation of the cells without affecting the calculations of resource flow cost which is handled by the x c y c attributes besides the coordinate requirement for visual representation this is to additionally scale the operational cost of flows or pipe expansions based on the distance of the corresponding flow path the link between the abm generated demands and the milp optimisation problem is clearly plugged in at the right hand side rhs of the milp problem and specifically in the mass balance constraint rows as the abm finishes calculating projected demands the results are stored in a csv file which can then be sequentially read to initialise demands for the rtn component the yaml file is read to provide parameter values for the milp model which are logged inside java via the glpk interface when the milp problem is built and passed to the optimisation solver to acquire the optimal solution if such exists although the model is feasible in nature investments can be used to further extend production output so as to meet demands some modifications can affect feasibility these include bounding the investments in specific cells in combination with the absence and limited expansion availability defined by the user of pipe network connectivity which if not restricted would allow for surplus production to flow so as to meet demands the platform has been designed and optimised using memory pre allocation sparse matrix handling with the ujmp java class package ujmp 2016 being essential to allow scaling of the model for very large dimensions using small sizes of ram fine tuning of the solver for the specific formulation etc for use with medium specifications laptops therefore it is lightweight in the required computational power and produces results within very reasonable cpu time for single scenario runs usually a few minutes or even less depending on the complexity of the problem the whole platform consists of the three modules demographics abm and rtn the required java run time environment jre and r programming language packages all bridged together upon execution it outputs automated plots inside categorised investments network flows infrastructure etc pdf files the graphical user interface gui shown at the initial execution of the platform is shown in fig 2 the ability to solve the formulated milp optimisation problem with commercial solvers such as cplex is also possible as the platform exports the mps problem format via the glpk interface before applying the glpk milp solver the overall flow of the implementation is shown in fig 4 the computational environment we used for the use case is shown in table 3 the total run time for all scenarios was around 23 min 1400 s most of the milps formulated under the rtn model are extremely sparse for instance for the 2030 sdg s scenario the milp is of 2 996 4 068 size rows columns or constraints variables and 12 700 non zeros the number of non zero entries in the coefficient matrix a of the milp problem which accounts for 12 700 2 996 4 068 0 1 density this enables wide applicability and accelerated converge with the latest mixed integer linear programming solvers at our disposal 4 functionality a series of specific questions can be answered via the optimisation module what kind of investments do we need to meet growing demands and sustainable development targets what capacity of which type and where to place the required technologies which changes to the resource flows transport network between cells e g city districts are needed what is the proposed topology of the pipe network how large is the capex associated with pipe expansions what is the operational cost per year and per capita how can the total output of c o 2 for the simulated period per year be reduced or managed what happens if a distinct connection or node e g transmission pipe facility becomes non functional for instance after an environmental calamity and what would a resilient network topology be that still allows for meeting demands how much of the flow resource is lost in the transport network e g pipes grids due to leaks losses and how does this affect results e g opex capex co2 which novel technologies are promising to use in terms of technical specifications costs and emissions what is the impact on system performance metrics of existing infrastructure plans if implemented what if we only substitute specific technologies and their numbers in specific districts or limit the expansion of some in specific and focus on different investments how is the network topology altered if we use transport links with different flow rates volumes in order to deliver insights into these questions the user can craft the input data so as to fashion the desirable simulation the analytical scenario is tailored by imposing limitations on resources technologies the flow network and policies specified by the user inside the yaml data text file the pre defined yaml structure as a series of tables and lists is used for this purpose such as to inject bounds to the expansion of the pipe network investments flows import of specific resource inputs needed e g raw water electricity etc define the weights of the metrics in the objective function so as to for instance give higher priority in a solution which is more emissions friendly experiment with different percentages of the abm demands pre allocate infrastructure and pipe network as well as upper bound each of those as desired specify a percentage of leaks on the existing pipe network as well as defining an available budget for the total cost among other settings 5 use case envisioning outcomes of ongoing wash projects and steps to meet wash targets the need to advance infrastructure planning capacity in sub sahara african countries is evident from their uneven progress as reported in the un sustainable development agenda 2030 united nations general assemply 2015 the capital city of ghana accra and fourteen neighboring administrative districts form the greater accra metropolitan area gama the geographic definition of gama as a city region was defined by local stakeholders using the locally defined metropolitan and municipal district assembly mmda structures in the country ghana statistical service 2014 from here on referred to as districts model calculations are carried out at the district level as a network to enable output matching to local planning targets set in district government medium term development plans dmtdp and are aggregated to the gama level in the results presented here gama is a rapidly growing metropolitan region where efforts to improve the water sanitation and hygiene wash situation have yielded mixed results household access to improved piped water grew on average by 81 to 83 from 2000 to 2010 and access to public and private improved toilet facilities increased from 58 to 81 ghana statistical service 2005 bentsi enchill et al 2010 however the percentage of waste water including waters which contain human excreta which was treated declined from around 10 to near zero between 2000 and 2010 whilst the population grew from three to four million people the city in recent years expanded potable water treatment at the kpone site by 190 000 m 3 day and opened a 60 000 m 3 day desalination plant resulting in an aggregate 54 treatment capacity increase in terms of waste water treatment the situation has deteriorated significantly as two existing large scale treatment plants the ama jamestown conventional treatment plant 16 000 m 3 day capacity and the tema community 3 lagoon treatment plant 20 000 m 3 day capacity broke down in 2004 and 2000 respectively the majority of waste water now ends up untreated in the environment directly or after collection and disposal the situation is not improving as the capacity of recently opened facilities is quite small which include a lagoon treatment plant at the university of ghana legon 6 400 m 3 day capacity and a human excreta de watering and sludge drying plant at the lavender hill beach site 800 m 3 day efforts are underway to rehabilitate the jamestown treatment plant but have met financing difficulties this use case serves to demonstrate how resilience io can provide knowledge support to implement macro planning targets for gama to improve the wash situation relative to the baseline year 2010 selected targets include those in the sustainable development goals sdgs and the ghana water sector strategic development plan wssdp ministry of water resources works and housing accra 2014 for 2012 to 2025 as developed by the ministry for water resources works and housing mwrwh the overarching urban objectives in the wssdp plan are to increase urban water and sanitation coverage to 100 in 2025 the use case calculates what combined projects infrastructure change and financing needs are required to meet these goals under different scenarios also taking into account the impact of currently ongoing projects on wash service access once completed the incorporated ongoing projects the world bank 2013 that we included in our simulation are accra sewerage improvement project asip gh gama sanitation and water project gh gama danida lavender hill sludge treatment slamson ghana korle lagoon cesspit treatment jamestown korle lagoon sewerage plant rehabilitation and the mudor faecal treatment plant also already completed projects were added if finished after the 2010 baseline and include teshie nungua desalination plant kpong china gezhouba and kpong tahal water treatment expansion we analysed the requirements from 2010 to 2030 to meet wash goals across different scenarios i wssdp decentralised districts where no pipe extensions are allowed with requirements to reach national 2025 wash targets focusing on district level infrastructure ii sdgs centralised with flexible pipe extensions with requirements to reach 2030 wash targets for the sustainable development goals based on city wide infrastructure and iii sdgs centralised and leakage reduction similar to the city wide systems scenario except that leakage rates from water and waste water pipe systems are set to 17 from the original 27 in the system from 2020 onward 6 results the use case input data was based on the 2010 ghana census among other private and public sources a series of technical characteristics were incorporated including pre allocation of existing infrastructure pipe network maps from to districts potable and waste water calibrated admissible transmission pipe and infrastructure expansions district to district transmission pipe leakage local tariff regulations on water use and waste water treatment and fifteen wash technology data sets including operational material and energy inputs and outputs operational labour inputs and financial investment and operational cost a summary of key indicators provided by the model as outputs is shown in table 5 population figures in gama grow between 2015 and 2030 from 4 4 to 6 5 million based on the demographics calculations we performed which included birth death and migration rates from the ghana census and were calibrated to un urban population projections to have a measure of comparison for our results we use the 2000 2015 estimates of the who for the per person costs to reach wash targets in sub saharan africa summarised in their report who 2004 the prediction was that the annual cost per person receiving interventions specifically intervention 5 is more closely linked to sdg scenario as presented here in table 14 would be around usd25 4 in a twenty years projection for 2010 2030 here this would account for nearly usd762 our calculations predict a cost of usd607 in the sdg 27 leaks scenario which is of the same order of magnitude 6 1 potable water potable water production from capacity extensions in the decentralised case rises from 0 54 to 0 85 million m 3 day between 2015 and 2030 in contrast in the sdgs infrastructure scenarios with leaks of 27 and 17 demand grows from 0 54 in 2015 to 0 87 and 0 76 million m 3 day in 2030 respectively for 2030 pipe leakage was evaluated at i wssdp decentralised 0 26 million ii sdgs centralised 0 27 million and iii sdgs centralised and leakage reduction 0 16 million m 3 day potable water demands without leaks were 0 58 million m 3 day in 2030 the lower sdg scenarios values arise from an optimised supply demand infrastructure location and pipe network resulting in lower pipe water flows and thus leakage by allowing pipe network extensions the total investment cost in new treatment capacity by 2030 to meet 100 potable water demands is 1 5 1 4 and 1 0 billion usd in the wssdp decentralised sdgs centralised and sdgs centralised and leakage reduction scenarios respectively investments were also calculated for potable and waste water pipe network expansions at 0 11 billion usd 2010 2030 for both sdgs centralised without and with leakage reduction respectively 6 2 waste water treatment the waste water evaluation yielded a required 0 47 million m 3 day in treatment capacity in 2030 in all three scenarios concerning leaks in 2030 2 085 m 3 day untreated waste water pipe leakage was established for the sdg centralised scenario leakage is so low because waste water treatment is found to be most cost effective using local technologies including aerated lagoons activated sludge and anaerobic digestion systems in each district thereby limiting the use of district to district waste water pipe networks capital expenditure for waste water treatment capacity from 2010 to 2030 for the three scenarios was 0 33 0 33 and 0 35 billion usd total operational cost for 2030 including potable water treatment can be found in table 5 6 3 emissions jobs the environmental impacts were captured by calculating total ghg emissions of the city region wash system since a small portion of waste water is currently treated ghg emissions from treatment infrastructure rises substantially in all scenarios in 2015 total system emissions were 3 7 thousand tonnes which increases to 107 thousand tonnes by 2030 in the three scenarios primarily due to waste water treatment growth from near 0 100 waste water treatment is fairly ghg intensive due to micro organisms turning sludge into either carbon dioxide or methane which ends up into the environment the calculations do not take into account what happens with untreated waste water that ends up in the environment as a baseline since a portion will naturally decompose aerobically or anaerobically in the environment by micro organisms the total system ghg emissions in k g c o 2 e q u i v a l e n t m 3 of treated water and waste water in 2030 are 81 80 and 87 thousands of tonnes correspondingly total jobs for all water and waste water treatment and distribution grow from 2 743 in 2015 to 3 253 4 380 and 3 862 jobs by 2030 in the wssdp decentralised sdgs centralised and sdgs centralised and leakage reduction scenarios respectively 7 discussion the functionality in the presented resilience io platform use case can be compared to iuwcm and iuwsm models since resilience io covers the water and waste water demand and supply cycle is able to incorporate other resource and pollutant vectors labour requirements and allows milp optimisation over both economic and environmental indicators in the use case using greenhouse gas emissions our optimisation approach allows exploration of wash system design by setting minimum performance criteria this is different from previous approaches like in watermet2 and uwot which calculate the performance of user defined designs and interventions particular differences of watermet2 to resilience io are i the approach is to test human designed plans as opposed to finding optimised supply resource technology system configurations ii a fixed set of treatment technologies with input output vectors as opposed to flexible introduction of new technologies in resilience io based on matrix insertion and iii a limit to daily time steps to settle supply and demand instead of hourly particular differences of uwot to resilience io are i the focus is on potable water supply and does not include waste water treatment ii the approach is to test the optimal operation of a user input based pipeline reservoir treatment system given a set of simulation demands as opposed to allowing for finding optimised entirely new network technology configurations iii uwot allows for appliance fixed household location demand as opposed to more flexible spatially mobile agents activity based demands the distinct advantages of our approach are that the platform is built for release as non commercial software we use behavioral modelling in a multi agent system to generate demands which enables simulations that can exploit fine grained variations in spatially defined variables of a population bonabeau 2002 bousquet and page 2004 multi user type variation as regarded effective in this context rixon et al 2007 we provide decision support for supply solutions via mathematical programming optimisation guignard spielberg and spielberg 2005 that can be tailored to specific sustainability needs for technology investment and operation due to flexible input output dimensions for each technology and a flexible multi criteria objective function our implementation is data driven and neither system sector nor region bounded and can be utilised in any city region for any demand supply flow context with the possibility of system specific functionality expansions the main limitations of the current state of the platform are that we do not yet cover all wash system features in particular water flow quality characterisation natural water reservoir stock dynamics and atmospheric water flows like storm water and flooding we do not provide automated functionality to rapidly evaluate the impacts of parameter uncertainties on wash system design scenario outcomes the platform is mainly usable by expert users due to the high level of technical detail involved combined with a text line data entry implementation 8 conclusions the resilience io platform results showed at a practical application level the possibilities of fine grained modelling of population characteristics technological facilities and associated resource and waste flows in space and time to provide insights for more resilient city region systems planning within a broader scope of economic social and environmental metrics in particular support was demonstrated for evaluation of future infrastructure needs for population and economic scenarios provisioning of total cost estimates to meet national and sdg targets visibility on environmental and social decision criteria such as jobs and ghg emission impacts and evaluation and prioritisation of planning options by quantifying differences between i infrastructure expansion and pipe network layout changes versus ii substantial reduction in transport network losses at a technical level the feasibility of an open source based implementation of an abm coupled to a milp optimisation model was demonstrated it can simulate the demand and resource flow based supply dynamics for integrated economic and environmental modelling purposes in a city region of several million people the implementation also shows the importance of a flexible design without adjusting the model structure but by changing only the input data file and then running the model again key adjustments can be evaluated in technology flow network and policy settings the multi objective criteria that are minimised can also be altered and can include both economic and environmental objectives the platform was also found to have emerging benefits from the inclusion of population income and labour aspects these aspects enabled the evaluation of the capacity a city region needs to implement proposed solutions at a decentralised government level this is achieved by quantifying labour needs and operational revenue flow requirements capacity at a financial labour and planning level is of particular concern for the sustainable development of low to lower middle income countries for example in ghana for which the use case was built the district governments are responsible for waste water treatment however these governments receive low tax revenues due to a large informal sector resulting in low staffing less than one local government staff per 1000 people versus 5 for high income countries and limited ability to attract national government or private sector finance future work includes the extension of the platform to support uncertainty in the modelling formulation for various parameters such as capital and operational expenditure or the magnitude of demand in resources and the design of a graphical user interface dedicated to generate yaml data files and expand visual output possibilities in a faster and user friendly way to enable uptake outside of the expert user community embedding different optimisation solvers to explore efficiency is also an interesting avenue acknowledgments a prodigious debt of gratitude goes to the department for international development uk for funding this activity under the future cities africa project grant number 203830 we also thank the ecological sequestration trust the ecological sequestration trust 2017 for arranging several webinars with local stakeholders from ghana and the platform demonstration debut june 16 at ghana the cities alliance international cities alliance 2017 and ghana team for their management support project debut workshop and insightful inputs also local experts from ghana foster mensah 3 3 centre for remote sensing and geographic information services university of ghana legon annie jiagge road accra ghana sampson madana arthur bernard and ohene ofori among many others were of immense help in data contributions and use case input appendix an example of a scenario in yaml data is given below as well as some automated graphical visual outputs from the platform are demonstrated below image 6a image 6b image 6c image 6d image 6e image 6f image 6g image 6h fig 6 an example of automated visual outputs from the platform geo localised districts in gama showing the initial pipe network for potable water fig 6 fig 7 the suggested by the platform expansions for 2030 in the sdg s 27 leaks scheme in red arcs to satisfy more efficiently the demands the expansions take into account the cost of pipe construction per meter of the respective distance between districts fig 7 fig 8 the optimal flows of potable water for 2030 in the sdg s 27 leaks scheme fig 8 fig 9 decentralised districts scenario in 2030 projection a large amount of aerated lagoons and decentralised activated sludge systems to facilitate the waste water treatment across gama fig 9 fig 10 2030 in the sdg s 27 leaks scheme inputs of raw water and electricity to meet 100 demands fig 10 
26431,importance of target oriented validation strategies for spatio temporal prediction models is illustrated using two case studies 1 modelling of air temperature t a i r in antarctica and 2 modelling of volumetric water content vw for the r j cook agronomy farm usa performance of a random k fold cross validation cv was compared to three target oriented strategies leave location out llo leave time out lto and leave location and time out llto cv results indicate that considerable differences between random k fold r 2 0 9 for t a i r and 0 92 for vw and target oriented cv llo r 2 0 24 for t a i r and 0 49 for vw exist highlighting the need for target oriented validation to avoid an overoptimistic view on models differences between random k fold and target oriented cv indicate spatial over fitting caused by misleading variables to decrease over fitting a forward feature selection in conjunction with target oriented cv is proposed it decreased over fitting and simultaneously improved target oriented performances llo cv r 2 0 47 for t a i r and 0 55 for vw keywords cross validation feature selection over fitting random forest spatio temporal target oriented validation 1 introduction machine learning algorithms are well established in environmental sciences lary et al 2016 kanevski et al 2009 and find application in a variety of fields as for example mapping of land cover ludwig et al 2016 gislason et al 2006 vegetation characteristics lehnert et al 2015 verrelst et al 2012 and soil properties gasch et al 2015 lieβ et al 2016 as well as in geomorphological messenzehl et al 2017 micheletti et al 2014 or climatological kühnlein et al 2014 hong et al 2004 meyer et al 2016a appelhans et al 2015 studies most of the applications focus on static spatial predictions and are not aiming at estimating a certain variable simultaneously in space and time however though machine learning algorithms are still rarely applied in spatio temporal models the number of applications is increasing gokaraju et al 2011 gasch et al 2015 appelhans et al 2015 meyer et al 2016b ho et al 2014 jing et al 2016 ke et al 2016 lary et al 2014 machine learning algorithms in space time applications learn from spatio temporal observations to predict a certain variable for unknown locations and for an unknown point in time within a defined model domain allowing a monitoring of the environmental variable the term prediction in this context should not to be confused with forecasting as most of the models are not aiming at predicting into the future but rather focus on predicting in past or present times as well as in space in contrast to model based geostatistics diggle and ribeiro 2007 as for example co kriging where one needs sufficiently distributed information on the variable at question for each interpolation time step spatio temporal prediction models link a set of independent variables to the response i e the variable in question and only use those independent variables for the subsequent spatio temporal prediction application a typical example of spatio temporal prediction models in environmental science might be the estimation of soil properties as done by gasch et al 2015 in this example soil properties volumetric water content soil temperature and bulk electrical conductivity are predicted in space and time on the basis of a machine learning model which is developed from a variety of spatial temporal and spatio temporal predictor variables as well as ground truth observations taken from data loggers studies by gasch et al 2015 and meyer et al 2016a have shown that the estimated performance of such models highly depends on the validation strategy in both cases high differences between the performance estimated by a random test subset of the total dataset and the performance estimated by a leave location out llo cross validation cv have been reported llo cv means that models are repeatedly trained by leaving the data from one location or a group of locations i e climate stations data loggers out and using the respective held back data for model validation the differences between a random subset validation lower error estimates and llo cv higher error estimates strongly suggest spatial over fitting as the models can very well predict on subsets of the time series of the locations used for training but fail in the prediction of unknown locations the prediction on unknown locations however is in most cases the major task of such models the llo cv error must therefore be considered as the decisive performance indicator of spatial as well as spatio temporal models similarly spatio temporal models have a risk of temporal over fitting which needs to be assessed by leave time out lto cv gudmundsson and seneviratne 2015 however it is these target oriented validation strategies that focus on the model performance in the context of unknown space or unknown time steps that are not yet fully prevailed in literature this is especially a problem as case studies ignoring the spatio temoral dependence in the data have to be considered too optimistic roberts et al 2017 even though llo and lto cv are used in some studies on spatial and spatio temporal models ho et al 2014 gudmundsson and seneviratne 2015 ruβ and brenning 2010 meyer et al 2017b brenning et al 2012 micheletti et al 2014 random k fold cv where the dataset is randomly partitioned into folds is still considered common practice ke et al 2016 messenzehl et al 2017 lieβ et al 2016 ludwig et al 2016 how to address spatial or spatio temporal over fitting in view to improved model selections over fitting in machine learning models when applied to spatial data most likely happens due to poor representation of spatio temporal sampling in predictor variable spaces hence carefully selecting and interpreting predictor variables is a logical remedy for improving performance of spatial models many spatio temporal prediction studies use auxiliary predictor variables which describe the properties of the location e g elevation slope soil type spatial coordinates these variables vary in space but not in time which means that each station has a unique combination of static variables we hypothesize hence that 1 these temporally static variables are prone to over fitting combinations of unique properties for each location are quasi comparable to a unique id of the locations which is then used as predictor using such variables the model is able to fit general characteristics of the individual time series 2 variables that lead to over fitting can be automatically identified and removed using a feature selection method that accounts for the target oriented performance 3 excluding misleading variables from the models does not only decrease over fitting but also leads to improved target oriented model performances feature selection is an intuitive solution to reduce the number of variables to the most important ones however the commonly used method for feature selection recursive feature elimination rfe see e g brungard et al 2015 meyer et al 2017a b ghosh and joshi 2014 stevens et al 2013 in the field of environmental mapping relies on variable importance scores which are calculated using solely the training subset kuhn and johnson 2013 if a variable leads to considerable over fitting it has a high importance in the models therefore this variable will be selected as important variable in the rfe process and is not removed regardless of a resulting high llo cv error alternative approaches for detecting the over fitting variables are hence required we consider two published case studies to demonstrate the effect of different validation strategies the risk of spatial or spatio temporal over fitting as well as the potential of feature selection algorithms to minimize the degree of over fitting to estimate the degree of over fitting we compare the results of a random k fold cv with the results of the target oriented validation strategies llo lto and leave location and time out llto cv we then compare the rfe method with a newly proposed forward feature selection ffs method that works in conjunction with target oriented performance to identify and remove variables that lead to over fitting as machine learning algorithm the well known random forest algorithm breiman 2001 was applied as it appeals to a large community of users we implement all steps of data analysis and modelling in the r environment for statistical programming r core team 2016 most of the analysis is based on the caret package kuhn 2016 that implements a wrapper to the random forest algorithm being used and provides functionality for data splitting and cv all newly produced r functions and modelling steps are fully documented in https github com environmentalinformatics marburg cast 2 case studies and description of the datasets 2 1 case study i modelling air temperature in antarctica the first case study follows the approach of meyer et al 2016a to spatio temporally predict t a i r in antarctica based on lst data from the moderate resolution imaging spectroradiometer modis and auxiliary predictor variables the dataset as it was used in the present study consists of 30666 hourly air temperature measurements from 32 weather stations distributed over antarctica for the year 2013 the t a i r values range from 78 40 c to 5 76 c with an average of 27 64 c and a standard deviation of 17 26 c beside of modis based lst as a spatio temporal predictor variable several auxiliary spatial predictor variables were used that basically describe the terrain in addition a number of predictor variables that remain spatially constant but vary in time were used as temporal predictor variables see table 1 for the full list of predictors used in this study and meyer et al 2016a for further information on the dataset 2 2 case study ii modelling volumetric water content of the cookfarm usa the second case study bases on the dataset applied in gasch et al 2015 to predict soil properties in 3d time and can be freely accessed from the gsif package in r the research site of this case study is the r j cook agronomy farm which is a 37 ha sized long term agroecosystem research site in the palouse region in the usa and operated by the washington state university the final dataset as prepared for this study consists of daily vw measurements from the years 2011 2013 taken by 5te sensors decagon devices inc pullman washington initially installed in five depth 0 3 0 6 0 9 1 2 and 1 5 m at 42 locations within the study site in this study we only focus on two dimensions plus time and limited the dataset to the depth of 0 3 m the dataset then contained 33397 training samples vw ranged from 0 093 m 3 m 3 to 0 613 m 3 m 3 with an average of 0 265 m 3 m 3 and a standard deviation of 0 076 m 3 m 3 the covariables available from the research dataset that were used in this study as potential predictors to predict vw are a number of spatially continuous variables describing the terrain further temporal variables as for example climate properties measured from the nearest meteorological station were used see table 1 for the full list of predictors used in this study and gasch et al 2015 for further information on the dataset 3 methods 3 1 random forest algorithm random forest bases on the concept of regression and classification trees i e a series of nested decision rules for the predictors that determine the response it repeatedly builds trees from random samples of the training data with each tree is a separate model of the ensemble the estimations of all trees are finally averaged to produce the final estimate breiman 2001 to overcome correlation between trees only a subset of predictors mtry is randomly selected at each split the best predictor from the random subset is used at the respective split to partition the data mtry is considered as a hyperparameter that needs to be tuned for a respective dataset in order to obtain an optimal trade off between under and over fitting of the data for a further description of random forest see breiman 2001 james et al 2013 and kuhn and johnson 2013 in this study the random forest implementation of the randomforest package liaw and wiener 2002 in r was applied and accessed via the caret package kuhn 2016 throughout the study each random forest model consisted of 500 trees after no increase of performance could be observed using a higher number of trees mtry was tuned for each value between two and the respective number of predictor variables 3 2 validation strategies to test the model performance on random subsets of the total datasets a commonly used random 10 fold cv was used therefore the data was split into 10 equally sized folds data splitting was done by stratified random sampling that ensures that the distribution of the response variable in each fold equals the distribution of the entire dataset models were then repeatedly trained by using the data of all except one fold and testing the model performance using the held back data fig 2 in order to quantify the performance of the models using target oriented validation strategies the performance in view to the following criteria was tested fig 1 1 predict on unknown locations tested by leave location out cross validation llo cv 2 predict on unknown points in time tested by leave time out cross validation lto cv 3 predict on unknown locations and unknown points in time tested by leave time and location out cross validation llto cv therefore the dataset was split into folds again but this time each fold left the data of complete locations llo or time steps lto or locations as well as time steps llto out fig 2 for both case studies the location of the data loggers defined a location and the dataset was split into 10 folds with respect to these locations for lto the day of the year was used as splitting criterion for t a i r antarctica for vw cookfarm data from more than one year was available allowing that individual months of each year could be left out for validation 12 months 3 years 36 unique time steps again the data was split into 10 folds by leaving complete time steps out for all target oriented validation strategies the procedure was comparable to the random k fold validation which gives a biased estimate of prediction performance models were repeatedly trained by using the data of all except one fold and testing the model performance for the held back data over fitting of the model in space and time was then quantified by comparing the random 10 fold cv results with the target oriented validation results 3 3 feature selection with the aim to remove predictors that are counterproductive in view to the target oriented performance we tested a rfe algorithm as well as a ffs algorithm that works in conjunction with target oriented validation fig 1 we used llo and llto cv as target oriented validation strategies as the ability of the model to predict on unknown locations was of upmost importance for both case studies rfe relies on variable importance scores that are calculated during the initial random forest model training the algorithm successively removes the least important variables to find the best performing set see kuhn and johnson 2013 for further details in this study we used the rfe implementation from the caret package kuhn 2016 as outlined above we assume that rfe is not a helpful approach to overcome spatio temporal over fitting as variables are not ranked according to target oriented performance as an alternative approach we developed and implemented a ffs algorithm in r algorithm 1 the algorithm first trains models i e random forest of all possible 2 variable combinations of the total set of predictor variables the best initial model in view to target oriented performance is kept the number of predictor variables is then iteratively increased the improvement of the model is tested for each additional predictor using target oriented cv the process stops when none of the remaining variables decreases the error of the currently best model the algorithm therefore fits a maximum of 2 n 1 2 2 models e g 81 models when 10 predictors are considered image 1 algorithm 1 pseudo code description similar to the ones from the caret package for the ffs algorithm resampling in this study bases either on llo or llto 4 results and discussions 4 1 target oriented validation for both case studies a random k fold cv showed a high performance with only low differences between observed and predicted values indicating a nearly perfect fit of the data model t a i r vw01 in table 2 however in view to unknown locations llo cv the performance decreased considerably models t a i r vw02 compared to models t a i r vw01 in table 2 this means that the model was generally less able to predict beyond the location of the training data compared to what might have been expected regarding the random k fold cv error the ability of the t a i r model to predict the outcome for an unknown day within the temporal model domain of 2013 remained high model t a i r 03 in table 2 thus in view to unknown locations and unknown days model t a i r 04 in table 2 the error was comparable to the llo cv error uncertainties in view to unknown locations were the major source of error the temporal error had more effect on the vw cookfarm example where complete months were left out for validation model vw03 and vw04 in table 2 since the differences between random k fold cv and target oriented cv are noticeably high the results highlight the need to perform cv in view to the model target in order to draw meaningful conclusions if the aim is to map the response variable one must consider llo cv as decisive error indicator as the random cv error can lead to considerable misinterpretations of the model performance especially when the model is to be applied on unknown years the potential of the model to predict beyond the years used for model training must also be considered in this case llto cv can assess the error in both space and time however the number of validation data decreases as the overlap between llo and lto is used this causes the results to be less robust compared to a separate view on llo and lto cv were more data are available for testing 4 2 detecting over fitting as the models performed well on random subsets of the entire datasets random k fold cv but had high errors when faced with unknown locations spatial over fitting must be suspected for both case studies the model could only lead to high performances when information about a respective location went into model training therefore the model was over fitting in space as only locations used for training could reliably be predicted by the model subsequently also llto cv showed high errors though temporal over fitting only slightly contributed to that error in case of the t a i r antarctica example in this case study over fitting in time was a minor issue at least on the considered time scale days in the case study of vw cookfarm the time scale used for data splitting was months of the individual years considering these larger time scales that were left out the model performance decreased compared to the random k fold cv performance r 2 0 79 compared to 0 92 see vw03 01 in table 2 thus temporal over fitting must be assumed in addition to spatial over fitting as only months that went into model training could reliably be predicted by the model 4 3 reducing over fitting and improving model performances to decrease the impact of over fitting rfe and the newly designed ffs were compared on the first sight rfe reduced over fitting in the t a i r antarctica example getting obvious in lower differences between random k fold cv and target oriented cv fig 3 a model t a i r 05 compared to t a i r 06 as well as t a i r 09 compared to t a i r 10 in table 2 this pattern however could not be supported by the vw cookfarm example where the differences between random k fold cv and target oriented cv remained equally high fig 3b model vw05 compared to vw06 as well as vw09 compared to vw10 in table 2 in fact this was the expected pattern as the variable importance ranking within the rfe is based on internal importance estimates fig 4 without consideration of the importance in view to target oriented errors the explanation for the effect shown in the t a i r example lies in the ranking of the variables fig 4a among the most important variables were apparently those that do not lead to an over fitting only the top three variables were selected by the rfe season time and lst see table 1 for explanation that could in this example lead to a reduced effect of over fitting including just one additional variable in this case aspect as this was the variable rated as next important see fig 4 was recognised as counterproductive by the rfe however the example of vw cookfarm demonstrates that this pattern is rather chance than a systematic ability of the rfe design to remove over fitting variables in the vw cookfarm example the variables that were ranked as important led to over fitting so that the rfe could not decrease this problem by removing least important variables over fitting in this example is generated because the variables were not ranked according to their target oriented importance within the models in fact the rfe algorithm kept all except three variables thus it yielded the best performance using nearly the full set of predictors which however could not remove over fitting the ffs algorithm in contrast could reliably reduce the differences between random k fold cv and llo as well as llto cv in both case studies when the respective less variable model was validated with random k fold cv the differences to the llo as well as llto cv error decreased fig 3 model vw t a i r 07 11 compared to vw t a i r 10 12 in table 2 this shows that removing misleading variables decreased the problem of spatial over fitting in the case study of t a i r antarctica it suggested the combination of season ice lst sensor month as necessary variables and rated all others as counterproductive for vw cookfarm the variables precip cum cdayt maxt wrcc mint wrcc crop were suggested to yield optimal results in view to llo as well as llto cv the variables that were rated as counterproductive and have been removed during ffs were spatially continuous but temporally constant variables the only exception was the variable ice for t a i r antarctica which however features a high overlap between logger locations thus it does not allow for a unique pointer on the individual logger locations especially in the case study of t a i r antarctica the temporally static variables formed a distinct pointer on the individual logger locations as each logger location featured unique combinations of the spatial variables i e unique combinations of slope aspect altitude therefore these variables are in combination comparable to an id for the loggers that was then used as predictor id like predictors enable the algorithms to access individual characteristics of the time series of the loggers which in turn leads to a misinterpretation of such variables these variables are associated with logger specific patterns that cover the true underlaying relations between these predictors and the response this suspicion is supported by a high internal importance of such variables within the models fig 4 especially in the vw cookfarm example e g ndre m bld phi but a removal of these variables during the ffs under these considerations the behaviour of the rfe to reduce the impact of over fitting in the t a i r antarctica example becomes understandable as the top ranked variables season time and lst are not prone to spatial over fitting the rfe could yield best results using only these three variables if an over fitting variable was amongst the top two variables over fitting could not have been resolved and the differences between random k fold and target oriented cv errors stayed high as in the vw cookfarm example removing counterproductive variables using ffs did not only lead to reduced over fitting but also to improved target oriented performances figs 3 and 5 table 2 this is especially obvious for the t a i r antarctica data where the llo cv r 2 increased from 0 24 to 0 47 model t a i r 07 compared to t a i r 02 in table 2 figs 3a and 5a the patterns for llto cv were the same model t a i r 11 compared to t a i r 04 in table 2 fig 5a also in the vw cookfarm example ffs led to an increased llo performance though the effect was less strong compared to the t a i r antarctica data fig 5b the llo cv r 2 increased from 0 49 to 0 55 model vw02 compared to vw07 in table 2 using only the selected variables for the llto cv error the ffs did not resulted in an improved model performance model vw04 compared to vw11 in table 2 fig 5b though over fitting could be significantly removed fig 3b obviously removing misinterpreted variables could not improve the performance which suggests that the potential of the variables to predict beyond the training locations and months is depleted however this model is now more robust as only a small subset of the initial variables are used and over fitting could be reduced though ffs is time consuming it is able to automatically detect and remove variables that are counterproductive in view to the target the computation time can be decreased by thorough pre selection of potential predictors in view to their effect in space and time to avoid id like pointers on individual locations or time steps considering the potential of ffs as shown in this study to remove counterproductive variables in view to a target oriented performance it is likely that the algorithm is able to improve a variety of published models beside of the two case studies meyer et al 2016a gasch et al 2015 as an example langella et al 2010 shi et al 2015 and janatian et al 2017 used latitude and longitude as predictors which are prone to create an id of the locations used for training the focus of this study was on spatio temporal models however most of the findings apply for purely spatial models as well this is supported by the studies of e g micheletti et al 2014 and roberts et al 2017 who left spatial units out for validation and yielded less optimistic results compared to a random k fold cv thus spatial over fitting is indicated also li et al 2011 included latitude and longitude as predictors in a purely spatial model and observed linear features in the resulting map if such models are validated with random k fold cv a statistically good fit is feigned but spatial over fitting occurs as a consequence of the misinterpretation of certain variables in such applications the proposed ffs in conjunction with target oriented validation in this case leave spatial unit out cv can improve the model results and will produce more robust results 5 conclusions this study addressed the widely ignored problem of dependencies caused by the nature of spatio temporal data it aimed at demonstrating the effect of target oriented validation and at finding a solution to detect and reduce spatial over fitting for this we used two previously published case studies we discovered high differences between random k fold and target oriented cv the random k fold cv r 2 of the t a i r antarctica study was 0 90 contrasting to a llo cv r 2 of 0 24 and the random k fold cv r 2 of the vw cookfarm study was 0 92 compared to a llo cv r 2 of 0 49 this shows that errors estimated with a standard random k fold cv can considerably deviate from target oriented error estimates which highlights the clear need for target oriented validation to avoid an overoptimistic view on results we further hypothesized that the observed patterns of spatio temporal over fitting are caused by temporally constant predictors e g elevation slope that act in conjunction with each other like an id this occurs when locations used for model training have unique spatial properties it appears that the models of both case studies were able to learn general characteristics of the time series of the individual locations the models were then very well able to predict subsets of the time series low random k fold cv error but then failed to predict beyond the training locations high ll t o cv error to automatically detect and remove variables that lead to over fitting we proposed using the ffs algorithm in conjunction with target oriented validation by removing the misleading predictors the ffs was able to automatically reduce spatio temporal over fitting which was reflected in similar errors for random k fold cv and target oriented cv after refitting the models using the ffs procedure the llo cv r 2 was 0 47 for t a i r antarctica and 0 55 for vw cookfarm hence the proposed method could improve the target oriented model performance in this study we applied the frequently used random forest algorithm though other algorithms have not explicitely been tested in this study the importance of target oriented validation as well as the risk of spatial over fitting is independent of the algorithm and applies to other flexible algorithms as e g neural networks support vector machines or cubist as indicated by meyer et al 2016a in the same way though predicting environmental variables in space and time remains challenging validation strategies suggested in this article allow assessing model errors objectively and allow identifying over fitting the shown importance of target oriented validation which is widely underestimated as well as the risk for considerable spatial overfitting indicate that the newly proposed modelling framework should become common research practice for spatio temporal prediction modelling despite the general opinion that random forests and other flexible algorithms are insensitive to over fitting unfavorable combinations of predictors and or distribution of the training data in space and time can lead to serious over fitting effects in this study variables that has caused that over fitting were removed from the models and the model performance has immediately improved however certain variables might be misleading but still contain valuable information how to minimize the over fitting effect of such variables but still use them in the spatial prediction remains to be solved with an increasing application of machine learning for spatio temporal predictions further studies and procedures for preventing over fitting in machine learning applications will hence be increasingly important acknowledgements this work was partly funded by the federal ministry of education and research bmbf within the idessa project grant no 01ll1301 which is part of the spaces program science partnership for the assessment of complex earth system processes it was further supported by the ross sea region terrestrial data analysis research program funded by the ministry of business and innovation new zealand grant no co9x1413 
26431,importance of target oriented validation strategies for spatio temporal prediction models is illustrated using two case studies 1 modelling of air temperature t a i r in antarctica and 2 modelling of volumetric water content vw for the r j cook agronomy farm usa performance of a random k fold cross validation cv was compared to three target oriented strategies leave location out llo leave time out lto and leave location and time out llto cv results indicate that considerable differences between random k fold r 2 0 9 for t a i r and 0 92 for vw and target oriented cv llo r 2 0 24 for t a i r and 0 49 for vw exist highlighting the need for target oriented validation to avoid an overoptimistic view on models differences between random k fold and target oriented cv indicate spatial over fitting caused by misleading variables to decrease over fitting a forward feature selection in conjunction with target oriented cv is proposed it decreased over fitting and simultaneously improved target oriented performances llo cv r 2 0 47 for t a i r and 0 55 for vw keywords cross validation feature selection over fitting random forest spatio temporal target oriented validation 1 introduction machine learning algorithms are well established in environmental sciences lary et al 2016 kanevski et al 2009 and find application in a variety of fields as for example mapping of land cover ludwig et al 2016 gislason et al 2006 vegetation characteristics lehnert et al 2015 verrelst et al 2012 and soil properties gasch et al 2015 lieβ et al 2016 as well as in geomorphological messenzehl et al 2017 micheletti et al 2014 or climatological kühnlein et al 2014 hong et al 2004 meyer et al 2016a appelhans et al 2015 studies most of the applications focus on static spatial predictions and are not aiming at estimating a certain variable simultaneously in space and time however though machine learning algorithms are still rarely applied in spatio temporal models the number of applications is increasing gokaraju et al 2011 gasch et al 2015 appelhans et al 2015 meyer et al 2016b ho et al 2014 jing et al 2016 ke et al 2016 lary et al 2014 machine learning algorithms in space time applications learn from spatio temporal observations to predict a certain variable for unknown locations and for an unknown point in time within a defined model domain allowing a monitoring of the environmental variable the term prediction in this context should not to be confused with forecasting as most of the models are not aiming at predicting into the future but rather focus on predicting in past or present times as well as in space in contrast to model based geostatistics diggle and ribeiro 2007 as for example co kriging where one needs sufficiently distributed information on the variable at question for each interpolation time step spatio temporal prediction models link a set of independent variables to the response i e the variable in question and only use those independent variables for the subsequent spatio temporal prediction application a typical example of spatio temporal prediction models in environmental science might be the estimation of soil properties as done by gasch et al 2015 in this example soil properties volumetric water content soil temperature and bulk electrical conductivity are predicted in space and time on the basis of a machine learning model which is developed from a variety of spatial temporal and spatio temporal predictor variables as well as ground truth observations taken from data loggers studies by gasch et al 2015 and meyer et al 2016a have shown that the estimated performance of such models highly depends on the validation strategy in both cases high differences between the performance estimated by a random test subset of the total dataset and the performance estimated by a leave location out llo cross validation cv have been reported llo cv means that models are repeatedly trained by leaving the data from one location or a group of locations i e climate stations data loggers out and using the respective held back data for model validation the differences between a random subset validation lower error estimates and llo cv higher error estimates strongly suggest spatial over fitting as the models can very well predict on subsets of the time series of the locations used for training but fail in the prediction of unknown locations the prediction on unknown locations however is in most cases the major task of such models the llo cv error must therefore be considered as the decisive performance indicator of spatial as well as spatio temporal models similarly spatio temporal models have a risk of temporal over fitting which needs to be assessed by leave time out lto cv gudmundsson and seneviratne 2015 however it is these target oriented validation strategies that focus on the model performance in the context of unknown space or unknown time steps that are not yet fully prevailed in literature this is especially a problem as case studies ignoring the spatio temoral dependence in the data have to be considered too optimistic roberts et al 2017 even though llo and lto cv are used in some studies on spatial and spatio temporal models ho et al 2014 gudmundsson and seneviratne 2015 ruβ and brenning 2010 meyer et al 2017b brenning et al 2012 micheletti et al 2014 random k fold cv where the dataset is randomly partitioned into folds is still considered common practice ke et al 2016 messenzehl et al 2017 lieβ et al 2016 ludwig et al 2016 how to address spatial or spatio temporal over fitting in view to improved model selections over fitting in machine learning models when applied to spatial data most likely happens due to poor representation of spatio temporal sampling in predictor variable spaces hence carefully selecting and interpreting predictor variables is a logical remedy for improving performance of spatial models many spatio temporal prediction studies use auxiliary predictor variables which describe the properties of the location e g elevation slope soil type spatial coordinates these variables vary in space but not in time which means that each station has a unique combination of static variables we hypothesize hence that 1 these temporally static variables are prone to over fitting combinations of unique properties for each location are quasi comparable to a unique id of the locations which is then used as predictor using such variables the model is able to fit general characteristics of the individual time series 2 variables that lead to over fitting can be automatically identified and removed using a feature selection method that accounts for the target oriented performance 3 excluding misleading variables from the models does not only decrease over fitting but also leads to improved target oriented model performances feature selection is an intuitive solution to reduce the number of variables to the most important ones however the commonly used method for feature selection recursive feature elimination rfe see e g brungard et al 2015 meyer et al 2017a b ghosh and joshi 2014 stevens et al 2013 in the field of environmental mapping relies on variable importance scores which are calculated using solely the training subset kuhn and johnson 2013 if a variable leads to considerable over fitting it has a high importance in the models therefore this variable will be selected as important variable in the rfe process and is not removed regardless of a resulting high llo cv error alternative approaches for detecting the over fitting variables are hence required we consider two published case studies to demonstrate the effect of different validation strategies the risk of spatial or spatio temporal over fitting as well as the potential of feature selection algorithms to minimize the degree of over fitting to estimate the degree of over fitting we compare the results of a random k fold cv with the results of the target oriented validation strategies llo lto and leave location and time out llto cv we then compare the rfe method with a newly proposed forward feature selection ffs method that works in conjunction with target oriented performance to identify and remove variables that lead to over fitting as machine learning algorithm the well known random forest algorithm breiman 2001 was applied as it appeals to a large community of users we implement all steps of data analysis and modelling in the r environment for statistical programming r core team 2016 most of the analysis is based on the caret package kuhn 2016 that implements a wrapper to the random forest algorithm being used and provides functionality for data splitting and cv all newly produced r functions and modelling steps are fully documented in https github com environmentalinformatics marburg cast 2 case studies and description of the datasets 2 1 case study i modelling air temperature in antarctica the first case study follows the approach of meyer et al 2016a to spatio temporally predict t a i r in antarctica based on lst data from the moderate resolution imaging spectroradiometer modis and auxiliary predictor variables the dataset as it was used in the present study consists of 30666 hourly air temperature measurements from 32 weather stations distributed over antarctica for the year 2013 the t a i r values range from 78 40 c to 5 76 c with an average of 27 64 c and a standard deviation of 17 26 c beside of modis based lst as a spatio temporal predictor variable several auxiliary spatial predictor variables were used that basically describe the terrain in addition a number of predictor variables that remain spatially constant but vary in time were used as temporal predictor variables see table 1 for the full list of predictors used in this study and meyer et al 2016a for further information on the dataset 2 2 case study ii modelling volumetric water content of the cookfarm usa the second case study bases on the dataset applied in gasch et al 2015 to predict soil properties in 3d time and can be freely accessed from the gsif package in r the research site of this case study is the r j cook agronomy farm which is a 37 ha sized long term agroecosystem research site in the palouse region in the usa and operated by the washington state university the final dataset as prepared for this study consists of daily vw measurements from the years 2011 2013 taken by 5te sensors decagon devices inc pullman washington initially installed in five depth 0 3 0 6 0 9 1 2 and 1 5 m at 42 locations within the study site in this study we only focus on two dimensions plus time and limited the dataset to the depth of 0 3 m the dataset then contained 33397 training samples vw ranged from 0 093 m 3 m 3 to 0 613 m 3 m 3 with an average of 0 265 m 3 m 3 and a standard deviation of 0 076 m 3 m 3 the covariables available from the research dataset that were used in this study as potential predictors to predict vw are a number of spatially continuous variables describing the terrain further temporal variables as for example climate properties measured from the nearest meteorological station were used see table 1 for the full list of predictors used in this study and gasch et al 2015 for further information on the dataset 3 methods 3 1 random forest algorithm random forest bases on the concept of regression and classification trees i e a series of nested decision rules for the predictors that determine the response it repeatedly builds trees from random samples of the training data with each tree is a separate model of the ensemble the estimations of all trees are finally averaged to produce the final estimate breiman 2001 to overcome correlation between trees only a subset of predictors mtry is randomly selected at each split the best predictor from the random subset is used at the respective split to partition the data mtry is considered as a hyperparameter that needs to be tuned for a respective dataset in order to obtain an optimal trade off between under and over fitting of the data for a further description of random forest see breiman 2001 james et al 2013 and kuhn and johnson 2013 in this study the random forest implementation of the randomforest package liaw and wiener 2002 in r was applied and accessed via the caret package kuhn 2016 throughout the study each random forest model consisted of 500 trees after no increase of performance could be observed using a higher number of trees mtry was tuned for each value between two and the respective number of predictor variables 3 2 validation strategies to test the model performance on random subsets of the total datasets a commonly used random 10 fold cv was used therefore the data was split into 10 equally sized folds data splitting was done by stratified random sampling that ensures that the distribution of the response variable in each fold equals the distribution of the entire dataset models were then repeatedly trained by using the data of all except one fold and testing the model performance using the held back data fig 2 in order to quantify the performance of the models using target oriented validation strategies the performance in view to the following criteria was tested fig 1 1 predict on unknown locations tested by leave location out cross validation llo cv 2 predict on unknown points in time tested by leave time out cross validation lto cv 3 predict on unknown locations and unknown points in time tested by leave time and location out cross validation llto cv therefore the dataset was split into folds again but this time each fold left the data of complete locations llo or time steps lto or locations as well as time steps llto out fig 2 for both case studies the location of the data loggers defined a location and the dataset was split into 10 folds with respect to these locations for lto the day of the year was used as splitting criterion for t a i r antarctica for vw cookfarm data from more than one year was available allowing that individual months of each year could be left out for validation 12 months 3 years 36 unique time steps again the data was split into 10 folds by leaving complete time steps out for all target oriented validation strategies the procedure was comparable to the random k fold validation which gives a biased estimate of prediction performance models were repeatedly trained by using the data of all except one fold and testing the model performance for the held back data over fitting of the model in space and time was then quantified by comparing the random 10 fold cv results with the target oriented validation results 3 3 feature selection with the aim to remove predictors that are counterproductive in view to the target oriented performance we tested a rfe algorithm as well as a ffs algorithm that works in conjunction with target oriented validation fig 1 we used llo and llto cv as target oriented validation strategies as the ability of the model to predict on unknown locations was of upmost importance for both case studies rfe relies on variable importance scores that are calculated during the initial random forest model training the algorithm successively removes the least important variables to find the best performing set see kuhn and johnson 2013 for further details in this study we used the rfe implementation from the caret package kuhn 2016 as outlined above we assume that rfe is not a helpful approach to overcome spatio temporal over fitting as variables are not ranked according to target oriented performance as an alternative approach we developed and implemented a ffs algorithm in r algorithm 1 the algorithm first trains models i e random forest of all possible 2 variable combinations of the total set of predictor variables the best initial model in view to target oriented performance is kept the number of predictor variables is then iteratively increased the improvement of the model is tested for each additional predictor using target oriented cv the process stops when none of the remaining variables decreases the error of the currently best model the algorithm therefore fits a maximum of 2 n 1 2 2 models e g 81 models when 10 predictors are considered image 1 algorithm 1 pseudo code description similar to the ones from the caret package for the ffs algorithm resampling in this study bases either on llo or llto 4 results and discussions 4 1 target oriented validation for both case studies a random k fold cv showed a high performance with only low differences between observed and predicted values indicating a nearly perfect fit of the data model t a i r vw01 in table 2 however in view to unknown locations llo cv the performance decreased considerably models t a i r vw02 compared to models t a i r vw01 in table 2 this means that the model was generally less able to predict beyond the location of the training data compared to what might have been expected regarding the random k fold cv error the ability of the t a i r model to predict the outcome for an unknown day within the temporal model domain of 2013 remained high model t a i r 03 in table 2 thus in view to unknown locations and unknown days model t a i r 04 in table 2 the error was comparable to the llo cv error uncertainties in view to unknown locations were the major source of error the temporal error had more effect on the vw cookfarm example where complete months were left out for validation model vw03 and vw04 in table 2 since the differences between random k fold cv and target oriented cv are noticeably high the results highlight the need to perform cv in view to the model target in order to draw meaningful conclusions if the aim is to map the response variable one must consider llo cv as decisive error indicator as the random cv error can lead to considerable misinterpretations of the model performance especially when the model is to be applied on unknown years the potential of the model to predict beyond the years used for model training must also be considered in this case llto cv can assess the error in both space and time however the number of validation data decreases as the overlap between llo and lto is used this causes the results to be less robust compared to a separate view on llo and lto cv were more data are available for testing 4 2 detecting over fitting as the models performed well on random subsets of the entire datasets random k fold cv but had high errors when faced with unknown locations spatial over fitting must be suspected for both case studies the model could only lead to high performances when information about a respective location went into model training therefore the model was over fitting in space as only locations used for training could reliably be predicted by the model subsequently also llto cv showed high errors though temporal over fitting only slightly contributed to that error in case of the t a i r antarctica example in this case study over fitting in time was a minor issue at least on the considered time scale days in the case study of vw cookfarm the time scale used for data splitting was months of the individual years considering these larger time scales that were left out the model performance decreased compared to the random k fold cv performance r 2 0 79 compared to 0 92 see vw03 01 in table 2 thus temporal over fitting must be assumed in addition to spatial over fitting as only months that went into model training could reliably be predicted by the model 4 3 reducing over fitting and improving model performances to decrease the impact of over fitting rfe and the newly designed ffs were compared on the first sight rfe reduced over fitting in the t a i r antarctica example getting obvious in lower differences between random k fold cv and target oriented cv fig 3 a model t a i r 05 compared to t a i r 06 as well as t a i r 09 compared to t a i r 10 in table 2 this pattern however could not be supported by the vw cookfarm example where the differences between random k fold cv and target oriented cv remained equally high fig 3b model vw05 compared to vw06 as well as vw09 compared to vw10 in table 2 in fact this was the expected pattern as the variable importance ranking within the rfe is based on internal importance estimates fig 4 without consideration of the importance in view to target oriented errors the explanation for the effect shown in the t a i r example lies in the ranking of the variables fig 4a among the most important variables were apparently those that do not lead to an over fitting only the top three variables were selected by the rfe season time and lst see table 1 for explanation that could in this example lead to a reduced effect of over fitting including just one additional variable in this case aspect as this was the variable rated as next important see fig 4 was recognised as counterproductive by the rfe however the example of vw cookfarm demonstrates that this pattern is rather chance than a systematic ability of the rfe design to remove over fitting variables in the vw cookfarm example the variables that were ranked as important led to over fitting so that the rfe could not decrease this problem by removing least important variables over fitting in this example is generated because the variables were not ranked according to their target oriented importance within the models in fact the rfe algorithm kept all except three variables thus it yielded the best performance using nearly the full set of predictors which however could not remove over fitting the ffs algorithm in contrast could reliably reduce the differences between random k fold cv and llo as well as llto cv in both case studies when the respective less variable model was validated with random k fold cv the differences to the llo as well as llto cv error decreased fig 3 model vw t a i r 07 11 compared to vw t a i r 10 12 in table 2 this shows that removing misleading variables decreased the problem of spatial over fitting in the case study of t a i r antarctica it suggested the combination of season ice lst sensor month as necessary variables and rated all others as counterproductive for vw cookfarm the variables precip cum cdayt maxt wrcc mint wrcc crop were suggested to yield optimal results in view to llo as well as llto cv the variables that were rated as counterproductive and have been removed during ffs were spatially continuous but temporally constant variables the only exception was the variable ice for t a i r antarctica which however features a high overlap between logger locations thus it does not allow for a unique pointer on the individual logger locations especially in the case study of t a i r antarctica the temporally static variables formed a distinct pointer on the individual logger locations as each logger location featured unique combinations of the spatial variables i e unique combinations of slope aspect altitude therefore these variables are in combination comparable to an id for the loggers that was then used as predictor id like predictors enable the algorithms to access individual characteristics of the time series of the loggers which in turn leads to a misinterpretation of such variables these variables are associated with logger specific patterns that cover the true underlaying relations between these predictors and the response this suspicion is supported by a high internal importance of such variables within the models fig 4 especially in the vw cookfarm example e g ndre m bld phi but a removal of these variables during the ffs under these considerations the behaviour of the rfe to reduce the impact of over fitting in the t a i r antarctica example becomes understandable as the top ranked variables season time and lst are not prone to spatial over fitting the rfe could yield best results using only these three variables if an over fitting variable was amongst the top two variables over fitting could not have been resolved and the differences between random k fold and target oriented cv errors stayed high as in the vw cookfarm example removing counterproductive variables using ffs did not only lead to reduced over fitting but also to improved target oriented performances figs 3 and 5 table 2 this is especially obvious for the t a i r antarctica data where the llo cv r 2 increased from 0 24 to 0 47 model t a i r 07 compared to t a i r 02 in table 2 figs 3a and 5a the patterns for llto cv were the same model t a i r 11 compared to t a i r 04 in table 2 fig 5a also in the vw cookfarm example ffs led to an increased llo performance though the effect was less strong compared to the t a i r antarctica data fig 5b the llo cv r 2 increased from 0 49 to 0 55 model vw02 compared to vw07 in table 2 using only the selected variables for the llto cv error the ffs did not resulted in an improved model performance model vw04 compared to vw11 in table 2 fig 5b though over fitting could be significantly removed fig 3b obviously removing misinterpreted variables could not improve the performance which suggests that the potential of the variables to predict beyond the training locations and months is depleted however this model is now more robust as only a small subset of the initial variables are used and over fitting could be reduced though ffs is time consuming it is able to automatically detect and remove variables that are counterproductive in view to the target the computation time can be decreased by thorough pre selection of potential predictors in view to their effect in space and time to avoid id like pointers on individual locations or time steps considering the potential of ffs as shown in this study to remove counterproductive variables in view to a target oriented performance it is likely that the algorithm is able to improve a variety of published models beside of the two case studies meyer et al 2016a gasch et al 2015 as an example langella et al 2010 shi et al 2015 and janatian et al 2017 used latitude and longitude as predictors which are prone to create an id of the locations used for training the focus of this study was on spatio temporal models however most of the findings apply for purely spatial models as well this is supported by the studies of e g micheletti et al 2014 and roberts et al 2017 who left spatial units out for validation and yielded less optimistic results compared to a random k fold cv thus spatial over fitting is indicated also li et al 2011 included latitude and longitude as predictors in a purely spatial model and observed linear features in the resulting map if such models are validated with random k fold cv a statistically good fit is feigned but spatial over fitting occurs as a consequence of the misinterpretation of certain variables in such applications the proposed ffs in conjunction with target oriented validation in this case leave spatial unit out cv can improve the model results and will produce more robust results 5 conclusions this study addressed the widely ignored problem of dependencies caused by the nature of spatio temporal data it aimed at demonstrating the effect of target oriented validation and at finding a solution to detect and reduce spatial over fitting for this we used two previously published case studies we discovered high differences between random k fold and target oriented cv the random k fold cv r 2 of the t a i r antarctica study was 0 90 contrasting to a llo cv r 2 of 0 24 and the random k fold cv r 2 of the vw cookfarm study was 0 92 compared to a llo cv r 2 of 0 49 this shows that errors estimated with a standard random k fold cv can considerably deviate from target oriented error estimates which highlights the clear need for target oriented validation to avoid an overoptimistic view on results we further hypothesized that the observed patterns of spatio temporal over fitting are caused by temporally constant predictors e g elevation slope that act in conjunction with each other like an id this occurs when locations used for model training have unique spatial properties it appears that the models of both case studies were able to learn general characteristics of the time series of the individual locations the models were then very well able to predict subsets of the time series low random k fold cv error but then failed to predict beyond the training locations high ll t o cv error to automatically detect and remove variables that lead to over fitting we proposed using the ffs algorithm in conjunction with target oriented validation by removing the misleading predictors the ffs was able to automatically reduce spatio temporal over fitting which was reflected in similar errors for random k fold cv and target oriented cv after refitting the models using the ffs procedure the llo cv r 2 was 0 47 for t a i r antarctica and 0 55 for vw cookfarm hence the proposed method could improve the target oriented model performance in this study we applied the frequently used random forest algorithm though other algorithms have not explicitely been tested in this study the importance of target oriented validation as well as the risk of spatial over fitting is independent of the algorithm and applies to other flexible algorithms as e g neural networks support vector machines or cubist as indicated by meyer et al 2016a in the same way though predicting environmental variables in space and time remains challenging validation strategies suggested in this article allow assessing model errors objectively and allow identifying over fitting the shown importance of target oriented validation which is widely underestimated as well as the risk for considerable spatial overfitting indicate that the newly proposed modelling framework should become common research practice for spatio temporal prediction modelling despite the general opinion that random forests and other flexible algorithms are insensitive to over fitting unfavorable combinations of predictors and or distribution of the training data in space and time can lead to serious over fitting effects in this study variables that has caused that over fitting were removed from the models and the model performance has immediately improved however certain variables might be misleading but still contain valuable information how to minimize the over fitting effect of such variables but still use them in the spatial prediction remains to be solved with an increasing application of machine learning for spatio temporal predictions further studies and procedures for preventing over fitting in machine learning applications will hence be increasingly important acknowledgements this work was partly funded by the federal ministry of education and research bmbf within the idessa project grant no 01ll1301 which is part of the spaces program science partnership for the assessment of complex earth system processes it was further supported by the ross sea region terrestrial data analysis research program funded by the ministry of business and innovation new zealand grant no co9x1413 
26432,a cell to cell routing rou module has been implemented in a large scale conceptual hydrological model developed for the entire landmass of india with a 5 km 5 km grid cell resolution the rou module is based on the principle of time variant spatially distributed direct hydrograph sddh travel time method to route streamflow to the catchment outlet the integrated model has been calibrated and validated at the kabini dam flow measurement site over 2001 2006 and 2007 2010 respectively nash sutcliffe efficiency nse is found to be 0 55 and 0 47 for the calibration and the validation period respectively the obtained nse value shows that the model is satisfactorily calibrated and rou module compliments the other modules the comparison between model simulation results with implemented and original routing schemes show that implemented method with modified parameterization outperforms the original one the overall results support the applicability of the implemented routing scheme keywords rou module sddh travel time modified parameterization 1 introduction in the estimation of watershed response to precipitation flow routing is essential du et al 2009 shelef and hilley 2013 markovic and koch 2015 niu et al 2017 it is a mathematical method to predict changes in the magnitude and the velocity of flood wave propagating down the streams or reservoirs linsley et al 1982 all hydrological models have two major components the first one dealing with the conversion of rainfall into the runoff and base flow and the second one dealing with the routing of runoff and base flow to the catchment outlet as streamflow todini 1988 2007 peel and bloschl 2011 from the implementation perspective olivera et al 2000 have classified flow routing algorithms into three types namely cell to cell flow routing vörösmarty et al 1989 liston et al 1994 miller et al 1994 lohmann et al 1996 coe 1997 2000 hagemann and dumenil 1998 melesse and graham 2004 du et al 2009 lόpez vicente et al 2014 terink et al 2015 jung et al 2015 mizukami et al 2016 piccolroaz et al 2016 adams et al 2017 source to sink flow routing naden et al 1999 olivera et al 2000 and element to element flow routing u s army corps of engineers usace 2001 goodrich et al 2002 among these the cell to cell routing method is the cardinal technique as it is simple to implement and provides simulated flow with a higher degree of accuracy liston et al 1994 marengo et al 1994 miller et al 1994 sausen et al 1994 coe 1997 hagemann and dumenil 1998 the cell to cell routing algorithms consider rainfall and flow properties distributed in space and time du et al 2009 they are developed by using either the kinematic wave vörösmarty et al 1989 liston et al 1994 or diffusion wave julien et al 1995 ogden 1998 downer et al 2002 approximations of saint venant equations the kinematic wave approximation considers only the frictional and gravity force terms whereas diffusive wave takes pressure term along with the frictional and gravity force terms of the saint venant equations singh 2002 the diffusive wave form of the equation is superior as it takes the backwater effects and attenuation into consideration ponce et al 1978 however ponce et al 1978 also stated that in regions where backwater is not a significant phenomenon kinematic wave models give similar results to the diffusive wave models because the numerical solution of the kinematic wave model is identical to the analytic solution of the diffusive wave model the kinematic waves are dominant for streamflow in the rivers without artificial structures and a kinematic wave model provides sufficiently accurate results for both overland and channel flow singh 2002 singh 2002 also stated that the hydrologic nature could indeed be approximated quite closely by kinematic wave theory therefore kinematic wave approximation technique is better option to use over the diffusive wave approximation technique for routing scheme in a hydrological model to simulate flow at the outlet of a catchment different kinematic wave models have been successfully applied for cell to cell flow routing in various studies for different grid cell resolutions over the past years vörösmarty et al 1989 used a water transport model wtm of resolution 0 5 0 5 to route runoff generated by drainage basin model dbm at amazon and tocantins river system each grid cell was assumed to be a linear reservoir they used a transfer coefficient retention time in each cell which was dependent on the geometries of the grid cells liston et al 1994 applied a routing scheme of 2 0 2 5 grid cell resolutions for mississippi river basin they used a transfer coefficient by calculating it using an empirical relationship between stream length overland slope and mean discharge later jayawardena and mahanama 2002 proposed a runoff routing method at a resolution of 5 5 9 km resolution similar to the model proposed by liston et al 1994 except for the inclusion of floodplain inundation as presented by vörösmarty et al 1989 they applied it to the mekong river basin in china and chao phraya river basin in thailand miller et al 1994 developed a linear reservoir river routing model of 2 2 5 resolutions for some world rivers coupled with an atmospheric ocean model the velocity of flow was calculated either empirically using a topography gradient or taken a constant value over time and space marengo et al 1994 applied routing method proposed by miller et al 1994 to the amazon and tocantins river basins oki et al 1996 used the runoff routing model of miller et al 1994 to model the amazon ob and amur river basins at a cell resolution of 5 6 5 6 fixing flow velocity in the river channel at 0 3 m s later costa and foley 1997 used the same model in the amazon river basin at a cell resolution of 0 5 0 5 ma et al 2000 also used the linear reservoir concept similar to the model proposed by miller et al 1994 and oki et al 1996 in their macroscale hydrological model to analyze the lena river basin at a cell resolution of 0 1 0 1 using a constant channel flow velocity of 0 4 m s coe 1997 developed a terrain based surface water area model swam using linear reservoir approach for simulating surface water area and river transport at the continental scale and applied to northern africa at a cell resolution of 5 5 9 km the transfer coefficient used to determine flux from the grid cell was taken to be proportional to the ratio of the distance between the grid cell centers and the effective velocity which in this case was chosen to be 0 003 ms 1 lohmann et al 1996 coupled a horizontal routing model with the land surface parameterization lsp schemes with the assumption that the horizontal routing process can be lumped as a linear time invariant system the routing model essentially describes the concentration time for runoff reaching the outlet of a grid cell and the transport of water in the channel system lohmann et al 1998 subsequently coupled the horizontal routing model with the two layer variable infiltration capacity hydrological model vic 2l ducharne et al 2003 developed a river transfer hydrological model rthm consisting of a river routing module with grid cell resolution of 25 km 25 km transfer of water from one cell to another was independent of the cell and the transfer coefficient was calculated as a function of the distance the slope between the two cells and a scaling factor melesse and graham 2004 introduced a routing model and estimated the flow based on travel time the entire basin was divided into two types of cells overland cells and channel cells depending upon the threshold value for the flow accumulation area the travel time was calculated by combining kinematic wave approximation with manning s equation for overland cells and steady state continuity equation in combination with manning s equation for the channel cells after that the travel time from each grid to outlet was computed as the sum of travel times of all grids across the flow path and direct runoff was estimated adding up the total flow from all the contributing grids at their respective travel time the major drawback of this model was that it did not account for travel time field variation occurring during the storm du et al 2009 addressed the issue of the invariant travel time of melesse and graham 2004 and developed a time variant spatially distributed direct hydrograph sddh travel time method by introducing travel time field variation due to the rainfall variation we have selected this approach to implement it in a large scale conceptual satellite based hydrological model shm as a separate flow routing rou module along with other four modules surface water sw snow s forest f and groundwater gw since in this routing approach calculation of overland and channel travel time uses a physically based method this technique needs adjustment while implementing it in a conceptual hydrological model the primary objective of this study is to test the applicability of a modified time variant sddh method as a flow routing rou module in the satellite based hydrological model shm the study proposes a conceptual cell to cell routing approach with modified parameterization for details see section 5 1 4 replacing two existing parameters of time variant sddh process with three new parameters and implements it in shm the integrated model is applied to a well monitored kabini dam sub basin for testing the rou module sensitivity analysis is conducted to study the effect of rou module parameters on the predicted hydrograph at basin outlet during validation the organization of the paper is as follows section 1 is the introduction followed by section 2 which gives the detailed description of the study area and data used section 3 presents a brief description of shm after that section 4 includes the information on availability and transferability of the integrated model section 5 describes methodology with a detailed procedure used for developing the routing scheme followed by results and discussion in section 6 and conclusions in section 7 2 study area and data kabini dam sub basin 2479 km2 of the kabini river basin 7847 km2 fig 1 is chosen as the study area it is located in the southern part of india the elevation in the study area ranges from 622 m to 2026 m above mean sea level according to the food and agriculture organization fao data soils in the study area may be broadly classified as sandy clay loam and loamy further the sub basin has the forest 87 percent agricultural 10 percent and non agricultural area as its primary land use land cover lulc the average annual temperature lies between 17 c and 37 c and annual mean rainfall is 3100 mm with june to september being the monsoon months for testing of routing scheme in kabini dam sub basin required data have been collected from various sources the advanced spaceborne thermal emission and reflection radiometer aster digital elevation model dem of 30 m 30 m resolution is used after resampling it into 5 km 5 km resolution the gridded 1 1 daily rainfall and temperature maximum and minimum data 2001 2010 collected from the india meteorological department imd pune have been bi linearly interpolated to 5 km 5 km gridded data and used as input in shm the soil map and lulc map of the study area have been taken from fao both the maps are of 1 km 1 km resolution and have been resampled to 5 km 5 km resolution 3 model description the collection and management of extensive spatial data through traditional methods of data acquisition is a challenging task for a vast country like india consequently a large scale hydrological modelling for the entire landmass of india has been an ambitious task to overcome this problem a large scale conceptual model shm has been developed under the pracriti 2 program of space application centre sac ahmedabad the model aims at preparing sustainable water management scenarios using remote sensing data from indian satellites to handle the fresh water crisis in india shm has 129258 grid cells of 5 km 5 km resolution covering the indian landmass the properties at the centre of a cell are assumed to represent the properties of the entire cell there are five modules sw f s gw and rou in shm as stated earlier shm grid cells representing forest and snow covered land are modeled using the f and s modules respectively whereas other grid cells are modeled using sw module in the sw module soil conservation service scs curve number cn method chow et al 1988 is used to estimate the surface runoff and hargreaves method hargreaves and samani 1985 is used to estimate the potential evapotranspiration pet a water balance method is used to determine the soil moisture the soil profile is considered as a single layered zone of 300 mm depth and moisture holding and moisture transmitting characteristics of the soil and underlying layer are considered while accounting for the soil moisture the wetting of the soil layer depends on the amount of infiltration the amount of water more than the maximum capacity of the zone percolates and contributes to the groundwater the soil moisture is depleted by evapotranspiration at a potential rate or actual rate depending on the moisture condition the potential rate is considered as long as sufficient moisture is available the f module functions based on water balancing and the dynamics of the subsurface to provide output in the form of runoff soil moisture evapotranspiration and contribution to groundwater using the technique and parameters stated in das et al 2014 subsurface is considered of having soil matrix and macropores there are two types of macropores main bypass and internal catchments the main bypass is responsible for direct contribution to groundwater soil matrix is assumed to have three layers which are crucial in the aspect of water balance and change in soil moisture after infiltration the saturation of three layers gets started from the top in batch and after complete saturation of the three layers the excess water goes to groundwater runoff generation after a precipitation event occurs according to the antecedent conditions in the subsurface the s module determines the snow density from snow albedo smith and halverson 1979 for estimating snowmelt depth by using two different algorithms viz temperature index algorithm and radiation temperature index algorithm since the study area does not have any snow covered land the s module is not considered in this study gw module uses the contribution from sw f and s modules and generates base flow following the water level variation process described in sekhar et al 2004 the resultant base flow along with the surface runoff generated from other modules is routed up to the outlet as streamflow details of rou module has been described in section 5 4 model availability and transferability shm has been developed through a multi institutional collaboration coordinated by indian institute of technology iit kharagpur the model is available in the agricultural and food engineering department iit kharagpur india the corresponding author of this paper can be contacted for further details shm has been coded in java programming language it is independent of operating system and runs on any pc with a minimum of 16 gb random access memory ram and mysql open source database software and java preinstalled the database of the model is storable and transferable in sql format however presently the java source code and mysql database are not available for modification the integrated model is being tested for use with data products derived from indian remote sensing satellites irs and results of daily simulation will soon be available online 5 methodology 5 1 routing scheme arc hydro tools have been used to deduce flow direction and stream network for the entire indian landmass subsequently we have implemented the time variant sddh approach with modified parameterization in shm the main point of this routing scheme lies in the travel time estimation at each time step fig 2 after that the flow direction of each cell is determined in the direction of steepest descent based on esri direction code of a particular cell the total number of cells that flow into each cell from the upstream area based on their flow direction is found to give the flow accumulation for each cell any cell with some upstream cells greater than or equal to the threshold value is considered as the channel cell whereas others are considered as an overland cell for flow network generation of entire indian landmass a threshold value of two i e 50 km2 is found appropriate after several iterations for delineating the channel network in the study area the flow velocity of each cell is determined separately depending upon whether it is an overland cell or a channel cell 5 1 1 overland cell velocity the kinematic wave approximation in combination with manning s equation singh and aravamuthan 1996 is used to estimate the flow velocity in the overland flow cells the continuity and momentum equations are written as 1 continuity equation h t q l i e 2 momentum equation s f s 0 where h is the depth of water on the surface m q is unit width discharge m2 s ie is vertical net incoming flux m s l is the length of the slope m t is time s sf is friction slope and so is the slope of the surface since vertical net incoming flux is zero in this routing scheme the surface flow rate is given by manning s equation 3 v o s f 1 2 h 2 3 n o where no is manning s roughness coefficient for overland cells m 1 3s and vo is overland flow velocity m s 5 1 2 channel cell velocity the continuity equation for the channel flow without any lateral flow is given by 4 a t q l 0 where a is flow section area of the channel m2 l is flow length m and q is cumulative discharge m3 s from a particular cell the flow is assumed to be in steady state then a t 0 and thus q l 0 which shows that q is constant hence eq 4 reduces to 5 q v c a v c bh 6 h q v c b where b is the channel width m default value is 5000 m in this routing scheme and vc is the channel cell velocity m s the channel flow velocity is given by manning s equation 7 v c s f 1 2 r 2 3 n c where r is the hydraulic radius m area of the given flow section divided by its wetted perimeter and nc is the manning roughness coefficient for channel cells m 1 3s manning s equation for the wide channels r h combined with kinematic wave approximation gives the channel velocity as 8 v c s o 3 10 q 2 5 b 2 5 n c 3 5 5 1 3 estimation of the arrival time of runoff up to outlet the travel time to or tc depending on the overland cell or channel cell respectively is computed by dividing the travel distance of a cell up to the next cell by velocity of flow generated in that cell the cumulative travel time σti of a grid cell is calculated by summing up of all the values of the travel time of the grid cells in the flow network of that grid up to the outlet the flow chart of the algorithm used to calculate the cumulative travel time is shown in fig 3 in this time variant sddh method the temporal variability of generated runoff and baseflow is taken into consideration by dividing the whole process into small time steps i here time step δt of one day is used the arrival time ta is determined as 9 ta σ ti i 1 δt a hydrograph is generated at each arrival time ta by summing the volumetric flow rates of all the grids having the same arrival time for all time intervals 5 1 4 parameterization of rou module the parameters of the time variant sddh travel time method from du et al 2009 are modified in this application the threshold value for the flow accumulation is fixed as two section 5 1 hence it is no longer a calibration parameter we classify manning s roughness coefficient into two classes manning s roughness coefficient for overland cell no and manning s roughness coefficient for channel cell nc further we consider the width of the channel b as one of the calibration parameters therefore two parameters of time variant sddh technique channel threshold a and parameter to account for the estimation error for manning s roughness coefficient and slope of a grid cell k stated in du et al 2009 are replaced by three new parameters no nc and b among these three parameters no is spatially distributed based on lulc and the other two are lumped accordingly the process equations have been modified as described in section 5 1 1 and 5 1 2 5 2 calibration and validation of shm to test the compatibility of routing scheme shm is calibrated and validated using the flow data at the upstream of the kabini dam flow measurement site the model is calibrated over 2001 2006 by considering 2001 as the warm up period and validated over 2007 2010 the test sub basin consists of 99 grid cells of shm fig 4 since automatic calibration procedure is yet to be included in the model a manual calibration procedure is adopted only the rou module parameters are considered for calibration and parameters of other modules are set as per their independent calibrations several trials have been performed to get the best values of the rou module parameters by comparing the simulated and observed streamflow at the sub basin outlet the performance of shm is evaluated using three quantitative statistical indicators namely nash sutcliffe efficiency nse the coefficient of determination r2 and percent bias pbias along with graphical technique as suggested by moriasi et al 2007 nse is defined as one minus the sum of the absolute squared difference between observed and simulated values normalized by the variance of observed values nash and sutcliffe 1970 the range varies from to 1 with 1 being the perfect fit it is chosen because of its extensive use in the field of hydrology which facilitates comparison between different studies it is given by 10 n s e 1 t 1 n q o q s i m 2 t 1 n q o q o 2 where qo is observed streamflow qsim is simulated streamflow q o is average observed streamflow and n is the number of terms in the time series the coefficient of determination r2 describes the proportion of the total variance in the observed data that can be explained by the model it ranges from 0 to 1 with higher values indicating better agreement and is given by 11 r 2 i 1 n q o q o q s i m q s i m i 1 n q o q o 0 5 i 1 n q s i m q s i m 0 5 2 where q s i m is the average simulated value of streamflow pbias measures the tendency of the simulated data to be larger or smaller than their observed counterparts its ideal value is 0 a positive value indicates model underestimation bias and a negative value indicates model overestimation bias it is expressed by 12 pbias i 1 n q i obs q i sim 100 i 1 n q i obs where q i obs is observed value of streamflow q i sim is simulated value of streamflow 5 3 sensitivity analysis of rou module parameters in this study sensitivity analysis has been conducted to identify model output variability with respect to changes in three parameters of the rou module i e no nc and b for validation period following two procedures the first procedure is applied to find out the model output variability against variations of a single rou module parameter around its calibrated value the second procedure is applied to find out the model output variability within the entire space of variability of the three parameters nse has been chosen as the objective function for the analysis because it is considered to be the best objective function for reflecting the overall fit of a simulated hydrograph with its observed counterpart servat and dezetter 1991 in the first procedure parameters are perturbed by 5 10 15 from calibrated value one at a time keeping the other parameter values constant in the second procedure sensitivity analysis is performed for 343 parameter combinations developed using seven sets of no nc and b values 5 10 15 perturbed values and calibrated values the model simulations are done with the perturbed parameter combinations for the validation period 2007 2010 and the simulated streamflow values are compared with the observed data and nse is estimated for each run then a 3d diagram is plotted using r an open source programming language considering the perturbation in percentage of the parameters no b and nc from their calibrated value in x y and z axis for analyzing the results 5 4 comparison with model simulation following du et al 2009 method keeping the rest of the model structure same we replace the implemented routing scheme with the routing method described in du et al 2009 and the new model set up is used for simulating streamflow at kabini dam flow measurement site for validation period 2007 2010 the parameter a is taken as two and k as three as for the threshold value two we get k value equal to three from fig 5 of du et al 2009 also the distributed manning s roughness coefficient value for each grid cell of kabini dam sub basin is derived and used in the model simulation run with the original method of du et al 2009 the normalized difference vegetation index ndvi based process described in hossain et al 2009 is used for the purpose subsequently the results of the modified method and the original method are compared using the statistical indices stated in section 5 2 5 5 comparison among model simulations with different grid cell resolutions changes in the grid cell resolution of the model may lead to varying values of slope flow direction and spatial distribution of flow networks and affect the model simulation results therefore we have compared model simulation results among grid cells of resolution 1 km 1 km 2 5 km 2 5 km 5 km 5 km and 10 km 10 km for this purpose the various input data like dem lulc soil etc were generated by resampling and interpolation as per the requirement using dem of different resolutions separate flow networks were generated and the new model setups were used for simulating the streamflow at the kabini dam flow measurement site for the validation period 2007 2010 6 results and discussion 6 1 model calibration the model setup of shm is calibrated at kabini dam flow measurement site from 2001 to 2006 on a daily basis with first one year as the warm up period simulated streamflow time series at kabini dam site has been plotted with observed streamflow on a daily scale as shown in fig 5 a the statistical indices nse 0 55 r2 0 56 and pbias 15 28 illustrate that the overall trend and temporal pattern of streamflow are captured by the model moriasi et al 2007 it is observed from 1 1 scatter plot fig 5 b that in calibration the linear relationship between observed and simulated streamflow is satisfactory for high and low flows whereas it shows discrepancy for medium flows more specifically the high flows generated during the monsoon period show good agreement with the observed streamflow the calibrated values of nc and b are found to be 0 03 and 4000 m respectively whereas values of no range from 0 015 rangeland grassland shrubland woodland to 0 03 forests 6 2 model validation shm shows satisfactory results for the validation period as it can capture the temporal pattern of observed streamflow on the daily time scale fig 6 a the nse r2 and pbias values for validation period are 0 47 0 52 and 3 16 respectively indicating a satisfactory relationship between observed and simulated streamflow values van liew et al 2007 viola et al 2009 durães et al 2011 fig 6 b presents the scatter plot of the linear relationship between simulated and observed streamflow for the validation period 6 3 sensitivity analysis of rou module parameters fig 7 a and b present the results of sensitivity analysis following two procedures as described in section 5 3 as evident from fig 7 a nse decreases for both positive and negative perturbations in any of the parameters which show that streamflow simulations are sensitive to all three rou module parameters similarly fig 7 b shows that streamflow simulations are sensitive to rou module parameters over their entire space of variability the dark redpoint in the middle of 3d diagram represents nse corresponding to the calibrated values of the parameters here also the nse values go down as the parameter values shift from their calibrated values the variability of nse over the parameter space however is not uniform due to complex interactions between parameter sets and the hydrological response beven 2006 haghnegahdar and razavi 2017 6 4 comparison with model simulation following du et al 2009 method table 1 presents the statistical performance evaluation of shm simulations with parameters corresponding to the modified and the original version of du et al 2009 method as evident the modified version results in improved nse and r2 values whereas pbias remains almost the same for both versions this implies that the modified parameterization of du et al 2009 method captures the overall trend and temporal pattern of streamflow better the improved results also support the applicability of the proposed method for grid cell resolution of 5 km 5 km however to establish the acceptability of 5 km 5 km grid cell resolution we compared the model simulation results for grid cells of 1 km 1 km 2 5 km 2 5 km 5 km 5 km and 10 km 10 km resolutions and the results are presented in the next sub section section 6 5 6 5 effect of grid cell resolution on model simulation results to analyze the effect of grid cell resolution on the model simulation results we compared the model simulation results for grid cells of 1 km 1 km 2 5 km 2 5 km 5 km 5 km and 10 km 10 km resolution table 2 presents the comparative statistics of model simulations with different grid cell resolutions along with model run timings as evident results improve with the finer grid cell resolution of 1 km 1 km and 2 5 km 2 5 km as compared to 5 km 5 km implemented in this study but the simulation times increase expectedly on the other hand with a coarser resolution of 10 km 10 km the model simulation accuracy is compromised the variations in the performance of different grid cell resolutions are due to the difference in travel times of generated runoff and base flow in each grid cell due to different flow network generated using dems of different resolutions wu et al 2007 vaze et al 2010 kuo et al 1999 also reported that the variations in model grid size affect the flow network generation and travel time of flow haga et al 2005 also concluded that lag times might vary depending on the dominant flow path it has also been reported that a coarser grid resolution may decrease spatial heterogeneity goodchild 1998 while a finer grid cell resolution may increase both model complexity and computational time arnold et al 2010 rathjens and oppelt 2012 ajami et al 2016 moreover we need to calibrate the model no matter how fine the resolution is and after calibration we may have approximately the same results for the model set up with different grid cell resolutions though the calibration parameter values may differ zhang and montgomery 1994 bruneau et al 1995 lee et al 2009 hence based on the model simulation results and model run timings use of 5 km 5 km grid cell resolution appears to be a logical choice finnerty et al 1997 arnold et al 2010 7 conclusions a modified time variant sddh technique is proposed and successfully implemented in a routing module which is integrated into a large scale conceptual hydrological model developed for the entire indian landmass the test results show that the modified cell to cell routing scheme performs better than the original physically based method in the kabini sub basin based on the trade off between the modelling accuracy and the simulation time a grid cell resolution of 5 km 5 km for the model is chosen as the optimum for the model as compared to the 1 km 1 km 2 5 km 2 5 km and 10 km 10 km grid cell resolutions the overall results show that the implemented routing module complements the other modules and supports the continuous model simulation on a daily timescale nevertheless the routing scheme needs to be evaluated in larger basins to be more confident about its applicability in large scale hydrological modelling acknowledgements this work is financially supported by space application centre sac ahmedabad grant no iit sric agfe dwi 2013 14 124 the technical support of programmer mr partha samanta in developing the model is acknowledged we also recognize the technical support of dr r p singh and dr p k gupta from sac ahmedabad and our other project partners from nerist itanagar iit guwahati and iisc bangalore 
26432,a cell to cell routing rou module has been implemented in a large scale conceptual hydrological model developed for the entire landmass of india with a 5 km 5 km grid cell resolution the rou module is based on the principle of time variant spatially distributed direct hydrograph sddh travel time method to route streamflow to the catchment outlet the integrated model has been calibrated and validated at the kabini dam flow measurement site over 2001 2006 and 2007 2010 respectively nash sutcliffe efficiency nse is found to be 0 55 and 0 47 for the calibration and the validation period respectively the obtained nse value shows that the model is satisfactorily calibrated and rou module compliments the other modules the comparison between model simulation results with implemented and original routing schemes show that implemented method with modified parameterization outperforms the original one the overall results support the applicability of the implemented routing scheme keywords rou module sddh travel time modified parameterization 1 introduction in the estimation of watershed response to precipitation flow routing is essential du et al 2009 shelef and hilley 2013 markovic and koch 2015 niu et al 2017 it is a mathematical method to predict changes in the magnitude and the velocity of flood wave propagating down the streams or reservoirs linsley et al 1982 all hydrological models have two major components the first one dealing with the conversion of rainfall into the runoff and base flow and the second one dealing with the routing of runoff and base flow to the catchment outlet as streamflow todini 1988 2007 peel and bloschl 2011 from the implementation perspective olivera et al 2000 have classified flow routing algorithms into three types namely cell to cell flow routing vörösmarty et al 1989 liston et al 1994 miller et al 1994 lohmann et al 1996 coe 1997 2000 hagemann and dumenil 1998 melesse and graham 2004 du et al 2009 lόpez vicente et al 2014 terink et al 2015 jung et al 2015 mizukami et al 2016 piccolroaz et al 2016 adams et al 2017 source to sink flow routing naden et al 1999 olivera et al 2000 and element to element flow routing u s army corps of engineers usace 2001 goodrich et al 2002 among these the cell to cell routing method is the cardinal technique as it is simple to implement and provides simulated flow with a higher degree of accuracy liston et al 1994 marengo et al 1994 miller et al 1994 sausen et al 1994 coe 1997 hagemann and dumenil 1998 the cell to cell routing algorithms consider rainfall and flow properties distributed in space and time du et al 2009 they are developed by using either the kinematic wave vörösmarty et al 1989 liston et al 1994 or diffusion wave julien et al 1995 ogden 1998 downer et al 2002 approximations of saint venant equations the kinematic wave approximation considers only the frictional and gravity force terms whereas diffusive wave takes pressure term along with the frictional and gravity force terms of the saint venant equations singh 2002 the diffusive wave form of the equation is superior as it takes the backwater effects and attenuation into consideration ponce et al 1978 however ponce et al 1978 also stated that in regions where backwater is not a significant phenomenon kinematic wave models give similar results to the diffusive wave models because the numerical solution of the kinematic wave model is identical to the analytic solution of the diffusive wave model the kinematic waves are dominant for streamflow in the rivers without artificial structures and a kinematic wave model provides sufficiently accurate results for both overland and channel flow singh 2002 singh 2002 also stated that the hydrologic nature could indeed be approximated quite closely by kinematic wave theory therefore kinematic wave approximation technique is better option to use over the diffusive wave approximation technique for routing scheme in a hydrological model to simulate flow at the outlet of a catchment different kinematic wave models have been successfully applied for cell to cell flow routing in various studies for different grid cell resolutions over the past years vörösmarty et al 1989 used a water transport model wtm of resolution 0 5 0 5 to route runoff generated by drainage basin model dbm at amazon and tocantins river system each grid cell was assumed to be a linear reservoir they used a transfer coefficient retention time in each cell which was dependent on the geometries of the grid cells liston et al 1994 applied a routing scheme of 2 0 2 5 grid cell resolutions for mississippi river basin they used a transfer coefficient by calculating it using an empirical relationship between stream length overland slope and mean discharge later jayawardena and mahanama 2002 proposed a runoff routing method at a resolution of 5 5 9 km resolution similar to the model proposed by liston et al 1994 except for the inclusion of floodplain inundation as presented by vörösmarty et al 1989 they applied it to the mekong river basin in china and chao phraya river basin in thailand miller et al 1994 developed a linear reservoir river routing model of 2 2 5 resolutions for some world rivers coupled with an atmospheric ocean model the velocity of flow was calculated either empirically using a topography gradient or taken a constant value over time and space marengo et al 1994 applied routing method proposed by miller et al 1994 to the amazon and tocantins river basins oki et al 1996 used the runoff routing model of miller et al 1994 to model the amazon ob and amur river basins at a cell resolution of 5 6 5 6 fixing flow velocity in the river channel at 0 3 m s later costa and foley 1997 used the same model in the amazon river basin at a cell resolution of 0 5 0 5 ma et al 2000 also used the linear reservoir concept similar to the model proposed by miller et al 1994 and oki et al 1996 in their macroscale hydrological model to analyze the lena river basin at a cell resolution of 0 1 0 1 using a constant channel flow velocity of 0 4 m s coe 1997 developed a terrain based surface water area model swam using linear reservoir approach for simulating surface water area and river transport at the continental scale and applied to northern africa at a cell resolution of 5 5 9 km the transfer coefficient used to determine flux from the grid cell was taken to be proportional to the ratio of the distance between the grid cell centers and the effective velocity which in this case was chosen to be 0 003 ms 1 lohmann et al 1996 coupled a horizontal routing model with the land surface parameterization lsp schemes with the assumption that the horizontal routing process can be lumped as a linear time invariant system the routing model essentially describes the concentration time for runoff reaching the outlet of a grid cell and the transport of water in the channel system lohmann et al 1998 subsequently coupled the horizontal routing model with the two layer variable infiltration capacity hydrological model vic 2l ducharne et al 2003 developed a river transfer hydrological model rthm consisting of a river routing module with grid cell resolution of 25 km 25 km transfer of water from one cell to another was independent of the cell and the transfer coefficient was calculated as a function of the distance the slope between the two cells and a scaling factor melesse and graham 2004 introduced a routing model and estimated the flow based on travel time the entire basin was divided into two types of cells overland cells and channel cells depending upon the threshold value for the flow accumulation area the travel time was calculated by combining kinematic wave approximation with manning s equation for overland cells and steady state continuity equation in combination with manning s equation for the channel cells after that the travel time from each grid to outlet was computed as the sum of travel times of all grids across the flow path and direct runoff was estimated adding up the total flow from all the contributing grids at their respective travel time the major drawback of this model was that it did not account for travel time field variation occurring during the storm du et al 2009 addressed the issue of the invariant travel time of melesse and graham 2004 and developed a time variant spatially distributed direct hydrograph sddh travel time method by introducing travel time field variation due to the rainfall variation we have selected this approach to implement it in a large scale conceptual satellite based hydrological model shm as a separate flow routing rou module along with other four modules surface water sw snow s forest f and groundwater gw since in this routing approach calculation of overland and channel travel time uses a physically based method this technique needs adjustment while implementing it in a conceptual hydrological model the primary objective of this study is to test the applicability of a modified time variant sddh method as a flow routing rou module in the satellite based hydrological model shm the study proposes a conceptual cell to cell routing approach with modified parameterization for details see section 5 1 4 replacing two existing parameters of time variant sddh process with three new parameters and implements it in shm the integrated model is applied to a well monitored kabini dam sub basin for testing the rou module sensitivity analysis is conducted to study the effect of rou module parameters on the predicted hydrograph at basin outlet during validation the organization of the paper is as follows section 1 is the introduction followed by section 2 which gives the detailed description of the study area and data used section 3 presents a brief description of shm after that section 4 includes the information on availability and transferability of the integrated model section 5 describes methodology with a detailed procedure used for developing the routing scheme followed by results and discussion in section 6 and conclusions in section 7 2 study area and data kabini dam sub basin 2479 km2 of the kabini river basin 7847 km2 fig 1 is chosen as the study area it is located in the southern part of india the elevation in the study area ranges from 622 m to 2026 m above mean sea level according to the food and agriculture organization fao data soils in the study area may be broadly classified as sandy clay loam and loamy further the sub basin has the forest 87 percent agricultural 10 percent and non agricultural area as its primary land use land cover lulc the average annual temperature lies between 17 c and 37 c and annual mean rainfall is 3100 mm with june to september being the monsoon months for testing of routing scheme in kabini dam sub basin required data have been collected from various sources the advanced spaceborne thermal emission and reflection radiometer aster digital elevation model dem of 30 m 30 m resolution is used after resampling it into 5 km 5 km resolution the gridded 1 1 daily rainfall and temperature maximum and minimum data 2001 2010 collected from the india meteorological department imd pune have been bi linearly interpolated to 5 km 5 km gridded data and used as input in shm the soil map and lulc map of the study area have been taken from fao both the maps are of 1 km 1 km resolution and have been resampled to 5 km 5 km resolution 3 model description the collection and management of extensive spatial data through traditional methods of data acquisition is a challenging task for a vast country like india consequently a large scale hydrological modelling for the entire landmass of india has been an ambitious task to overcome this problem a large scale conceptual model shm has been developed under the pracriti 2 program of space application centre sac ahmedabad the model aims at preparing sustainable water management scenarios using remote sensing data from indian satellites to handle the fresh water crisis in india shm has 129258 grid cells of 5 km 5 km resolution covering the indian landmass the properties at the centre of a cell are assumed to represent the properties of the entire cell there are five modules sw f s gw and rou in shm as stated earlier shm grid cells representing forest and snow covered land are modeled using the f and s modules respectively whereas other grid cells are modeled using sw module in the sw module soil conservation service scs curve number cn method chow et al 1988 is used to estimate the surface runoff and hargreaves method hargreaves and samani 1985 is used to estimate the potential evapotranspiration pet a water balance method is used to determine the soil moisture the soil profile is considered as a single layered zone of 300 mm depth and moisture holding and moisture transmitting characteristics of the soil and underlying layer are considered while accounting for the soil moisture the wetting of the soil layer depends on the amount of infiltration the amount of water more than the maximum capacity of the zone percolates and contributes to the groundwater the soil moisture is depleted by evapotranspiration at a potential rate or actual rate depending on the moisture condition the potential rate is considered as long as sufficient moisture is available the f module functions based on water balancing and the dynamics of the subsurface to provide output in the form of runoff soil moisture evapotranspiration and contribution to groundwater using the technique and parameters stated in das et al 2014 subsurface is considered of having soil matrix and macropores there are two types of macropores main bypass and internal catchments the main bypass is responsible for direct contribution to groundwater soil matrix is assumed to have three layers which are crucial in the aspect of water balance and change in soil moisture after infiltration the saturation of three layers gets started from the top in batch and after complete saturation of the three layers the excess water goes to groundwater runoff generation after a precipitation event occurs according to the antecedent conditions in the subsurface the s module determines the snow density from snow albedo smith and halverson 1979 for estimating snowmelt depth by using two different algorithms viz temperature index algorithm and radiation temperature index algorithm since the study area does not have any snow covered land the s module is not considered in this study gw module uses the contribution from sw f and s modules and generates base flow following the water level variation process described in sekhar et al 2004 the resultant base flow along with the surface runoff generated from other modules is routed up to the outlet as streamflow details of rou module has been described in section 5 4 model availability and transferability shm has been developed through a multi institutional collaboration coordinated by indian institute of technology iit kharagpur the model is available in the agricultural and food engineering department iit kharagpur india the corresponding author of this paper can be contacted for further details shm has been coded in java programming language it is independent of operating system and runs on any pc with a minimum of 16 gb random access memory ram and mysql open source database software and java preinstalled the database of the model is storable and transferable in sql format however presently the java source code and mysql database are not available for modification the integrated model is being tested for use with data products derived from indian remote sensing satellites irs and results of daily simulation will soon be available online 5 methodology 5 1 routing scheme arc hydro tools have been used to deduce flow direction and stream network for the entire indian landmass subsequently we have implemented the time variant sddh approach with modified parameterization in shm the main point of this routing scheme lies in the travel time estimation at each time step fig 2 after that the flow direction of each cell is determined in the direction of steepest descent based on esri direction code of a particular cell the total number of cells that flow into each cell from the upstream area based on their flow direction is found to give the flow accumulation for each cell any cell with some upstream cells greater than or equal to the threshold value is considered as the channel cell whereas others are considered as an overland cell for flow network generation of entire indian landmass a threshold value of two i e 50 km2 is found appropriate after several iterations for delineating the channel network in the study area the flow velocity of each cell is determined separately depending upon whether it is an overland cell or a channel cell 5 1 1 overland cell velocity the kinematic wave approximation in combination with manning s equation singh and aravamuthan 1996 is used to estimate the flow velocity in the overland flow cells the continuity and momentum equations are written as 1 continuity equation h t q l i e 2 momentum equation s f s 0 where h is the depth of water on the surface m q is unit width discharge m2 s ie is vertical net incoming flux m s l is the length of the slope m t is time s sf is friction slope and so is the slope of the surface since vertical net incoming flux is zero in this routing scheme the surface flow rate is given by manning s equation 3 v o s f 1 2 h 2 3 n o where no is manning s roughness coefficient for overland cells m 1 3s and vo is overland flow velocity m s 5 1 2 channel cell velocity the continuity equation for the channel flow without any lateral flow is given by 4 a t q l 0 where a is flow section area of the channel m2 l is flow length m and q is cumulative discharge m3 s from a particular cell the flow is assumed to be in steady state then a t 0 and thus q l 0 which shows that q is constant hence eq 4 reduces to 5 q v c a v c bh 6 h q v c b where b is the channel width m default value is 5000 m in this routing scheme and vc is the channel cell velocity m s the channel flow velocity is given by manning s equation 7 v c s f 1 2 r 2 3 n c where r is the hydraulic radius m area of the given flow section divided by its wetted perimeter and nc is the manning roughness coefficient for channel cells m 1 3s manning s equation for the wide channels r h combined with kinematic wave approximation gives the channel velocity as 8 v c s o 3 10 q 2 5 b 2 5 n c 3 5 5 1 3 estimation of the arrival time of runoff up to outlet the travel time to or tc depending on the overland cell or channel cell respectively is computed by dividing the travel distance of a cell up to the next cell by velocity of flow generated in that cell the cumulative travel time σti of a grid cell is calculated by summing up of all the values of the travel time of the grid cells in the flow network of that grid up to the outlet the flow chart of the algorithm used to calculate the cumulative travel time is shown in fig 3 in this time variant sddh method the temporal variability of generated runoff and baseflow is taken into consideration by dividing the whole process into small time steps i here time step δt of one day is used the arrival time ta is determined as 9 ta σ ti i 1 δt a hydrograph is generated at each arrival time ta by summing the volumetric flow rates of all the grids having the same arrival time for all time intervals 5 1 4 parameterization of rou module the parameters of the time variant sddh travel time method from du et al 2009 are modified in this application the threshold value for the flow accumulation is fixed as two section 5 1 hence it is no longer a calibration parameter we classify manning s roughness coefficient into two classes manning s roughness coefficient for overland cell no and manning s roughness coefficient for channel cell nc further we consider the width of the channel b as one of the calibration parameters therefore two parameters of time variant sddh technique channel threshold a and parameter to account for the estimation error for manning s roughness coefficient and slope of a grid cell k stated in du et al 2009 are replaced by three new parameters no nc and b among these three parameters no is spatially distributed based on lulc and the other two are lumped accordingly the process equations have been modified as described in section 5 1 1 and 5 1 2 5 2 calibration and validation of shm to test the compatibility of routing scheme shm is calibrated and validated using the flow data at the upstream of the kabini dam flow measurement site the model is calibrated over 2001 2006 by considering 2001 as the warm up period and validated over 2007 2010 the test sub basin consists of 99 grid cells of shm fig 4 since automatic calibration procedure is yet to be included in the model a manual calibration procedure is adopted only the rou module parameters are considered for calibration and parameters of other modules are set as per their independent calibrations several trials have been performed to get the best values of the rou module parameters by comparing the simulated and observed streamflow at the sub basin outlet the performance of shm is evaluated using three quantitative statistical indicators namely nash sutcliffe efficiency nse the coefficient of determination r2 and percent bias pbias along with graphical technique as suggested by moriasi et al 2007 nse is defined as one minus the sum of the absolute squared difference between observed and simulated values normalized by the variance of observed values nash and sutcliffe 1970 the range varies from to 1 with 1 being the perfect fit it is chosen because of its extensive use in the field of hydrology which facilitates comparison between different studies it is given by 10 n s e 1 t 1 n q o q s i m 2 t 1 n q o q o 2 where qo is observed streamflow qsim is simulated streamflow q o is average observed streamflow and n is the number of terms in the time series the coefficient of determination r2 describes the proportion of the total variance in the observed data that can be explained by the model it ranges from 0 to 1 with higher values indicating better agreement and is given by 11 r 2 i 1 n q o q o q s i m q s i m i 1 n q o q o 0 5 i 1 n q s i m q s i m 0 5 2 where q s i m is the average simulated value of streamflow pbias measures the tendency of the simulated data to be larger or smaller than their observed counterparts its ideal value is 0 a positive value indicates model underestimation bias and a negative value indicates model overestimation bias it is expressed by 12 pbias i 1 n q i obs q i sim 100 i 1 n q i obs where q i obs is observed value of streamflow q i sim is simulated value of streamflow 5 3 sensitivity analysis of rou module parameters in this study sensitivity analysis has been conducted to identify model output variability with respect to changes in three parameters of the rou module i e no nc and b for validation period following two procedures the first procedure is applied to find out the model output variability against variations of a single rou module parameter around its calibrated value the second procedure is applied to find out the model output variability within the entire space of variability of the three parameters nse has been chosen as the objective function for the analysis because it is considered to be the best objective function for reflecting the overall fit of a simulated hydrograph with its observed counterpart servat and dezetter 1991 in the first procedure parameters are perturbed by 5 10 15 from calibrated value one at a time keeping the other parameter values constant in the second procedure sensitivity analysis is performed for 343 parameter combinations developed using seven sets of no nc and b values 5 10 15 perturbed values and calibrated values the model simulations are done with the perturbed parameter combinations for the validation period 2007 2010 and the simulated streamflow values are compared with the observed data and nse is estimated for each run then a 3d diagram is plotted using r an open source programming language considering the perturbation in percentage of the parameters no b and nc from their calibrated value in x y and z axis for analyzing the results 5 4 comparison with model simulation following du et al 2009 method keeping the rest of the model structure same we replace the implemented routing scheme with the routing method described in du et al 2009 and the new model set up is used for simulating streamflow at kabini dam flow measurement site for validation period 2007 2010 the parameter a is taken as two and k as three as for the threshold value two we get k value equal to three from fig 5 of du et al 2009 also the distributed manning s roughness coefficient value for each grid cell of kabini dam sub basin is derived and used in the model simulation run with the original method of du et al 2009 the normalized difference vegetation index ndvi based process described in hossain et al 2009 is used for the purpose subsequently the results of the modified method and the original method are compared using the statistical indices stated in section 5 2 5 5 comparison among model simulations with different grid cell resolutions changes in the grid cell resolution of the model may lead to varying values of slope flow direction and spatial distribution of flow networks and affect the model simulation results therefore we have compared model simulation results among grid cells of resolution 1 km 1 km 2 5 km 2 5 km 5 km 5 km and 10 km 10 km for this purpose the various input data like dem lulc soil etc were generated by resampling and interpolation as per the requirement using dem of different resolutions separate flow networks were generated and the new model setups were used for simulating the streamflow at the kabini dam flow measurement site for the validation period 2007 2010 6 results and discussion 6 1 model calibration the model setup of shm is calibrated at kabini dam flow measurement site from 2001 to 2006 on a daily basis with first one year as the warm up period simulated streamflow time series at kabini dam site has been plotted with observed streamflow on a daily scale as shown in fig 5 a the statistical indices nse 0 55 r2 0 56 and pbias 15 28 illustrate that the overall trend and temporal pattern of streamflow are captured by the model moriasi et al 2007 it is observed from 1 1 scatter plot fig 5 b that in calibration the linear relationship between observed and simulated streamflow is satisfactory for high and low flows whereas it shows discrepancy for medium flows more specifically the high flows generated during the monsoon period show good agreement with the observed streamflow the calibrated values of nc and b are found to be 0 03 and 4000 m respectively whereas values of no range from 0 015 rangeland grassland shrubland woodland to 0 03 forests 6 2 model validation shm shows satisfactory results for the validation period as it can capture the temporal pattern of observed streamflow on the daily time scale fig 6 a the nse r2 and pbias values for validation period are 0 47 0 52 and 3 16 respectively indicating a satisfactory relationship between observed and simulated streamflow values van liew et al 2007 viola et al 2009 durães et al 2011 fig 6 b presents the scatter plot of the linear relationship between simulated and observed streamflow for the validation period 6 3 sensitivity analysis of rou module parameters fig 7 a and b present the results of sensitivity analysis following two procedures as described in section 5 3 as evident from fig 7 a nse decreases for both positive and negative perturbations in any of the parameters which show that streamflow simulations are sensitive to all three rou module parameters similarly fig 7 b shows that streamflow simulations are sensitive to rou module parameters over their entire space of variability the dark redpoint in the middle of 3d diagram represents nse corresponding to the calibrated values of the parameters here also the nse values go down as the parameter values shift from their calibrated values the variability of nse over the parameter space however is not uniform due to complex interactions between parameter sets and the hydrological response beven 2006 haghnegahdar and razavi 2017 6 4 comparison with model simulation following du et al 2009 method table 1 presents the statistical performance evaluation of shm simulations with parameters corresponding to the modified and the original version of du et al 2009 method as evident the modified version results in improved nse and r2 values whereas pbias remains almost the same for both versions this implies that the modified parameterization of du et al 2009 method captures the overall trend and temporal pattern of streamflow better the improved results also support the applicability of the proposed method for grid cell resolution of 5 km 5 km however to establish the acceptability of 5 km 5 km grid cell resolution we compared the model simulation results for grid cells of 1 km 1 km 2 5 km 2 5 km 5 km 5 km and 10 km 10 km resolutions and the results are presented in the next sub section section 6 5 6 5 effect of grid cell resolution on model simulation results to analyze the effect of grid cell resolution on the model simulation results we compared the model simulation results for grid cells of 1 km 1 km 2 5 km 2 5 km 5 km 5 km and 10 km 10 km resolution table 2 presents the comparative statistics of model simulations with different grid cell resolutions along with model run timings as evident results improve with the finer grid cell resolution of 1 km 1 km and 2 5 km 2 5 km as compared to 5 km 5 km implemented in this study but the simulation times increase expectedly on the other hand with a coarser resolution of 10 km 10 km the model simulation accuracy is compromised the variations in the performance of different grid cell resolutions are due to the difference in travel times of generated runoff and base flow in each grid cell due to different flow network generated using dems of different resolutions wu et al 2007 vaze et al 2010 kuo et al 1999 also reported that the variations in model grid size affect the flow network generation and travel time of flow haga et al 2005 also concluded that lag times might vary depending on the dominant flow path it has also been reported that a coarser grid resolution may decrease spatial heterogeneity goodchild 1998 while a finer grid cell resolution may increase both model complexity and computational time arnold et al 2010 rathjens and oppelt 2012 ajami et al 2016 moreover we need to calibrate the model no matter how fine the resolution is and after calibration we may have approximately the same results for the model set up with different grid cell resolutions though the calibration parameter values may differ zhang and montgomery 1994 bruneau et al 1995 lee et al 2009 hence based on the model simulation results and model run timings use of 5 km 5 km grid cell resolution appears to be a logical choice finnerty et al 1997 arnold et al 2010 7 conclusions a modified time variant sddh technique is proposed and successfully implemented in a routing module which is integrated into a large scale conceptual hydrological model developed for the entire indian landmass the test results show that the modified cell to cell routing scheme performs better than the original physically based method in the kabini sub basin based on the trade off between the modelling accuracy and the simulation time a grid cell resolution of 5 km 5 km for the model is chosen as the optimum for the model as compared to the 1 km 1 km 2 5 km 2 5 km and 10 km 10 km grid cell resolutions the overall results show that the implemented routing module complements the other modules and supports the continuous model simulation on a daily timescale nevertheless the routing scheme needs to be evaluated in larger basins to be more confident about its applicability in large scale hydrological modelling acknowledgements this work is financially supported by space application centre sac ahmedabad grant no iit sric agfe dwi 2013 14 124 the technical support of programmer mr partha samanta in developing the model is acknowledged we also recognize the technical support of dr r p singh and dr p k gupta from sac ahmedabad and our other project partners from nerist itanagar iit guwahati and iisc bangalore 
26433,wassi c is an ecohydrological model which couples water and carbon cycles with water use efficiency wue derived from global eddy flux observations however a significant limitation of the wassi c model is that it only runs serially high resolution simulations at a large scale are therefore computationally expensive and cause a run time memory burden using distributed mpi and shared openmp memory parallelism techniques we revised the original model as dwassi c we showed that using mpi was effective in reducing the computational run time and memory use two experiments were carried out to simulate water and carbon fluxes over the australian continent to test the sensitivity of the parallelized model to input data sets of different spatial resolutions as well as to wue parameters for different vegetation types these simulations were completed within minutes using dwassi c whereas they would not have been possible with the serial version the dwassi c model was able to simulate the seasonal dynamics of gross ecosystem productivity gep reasonably well when compared to observations at four eddy flux sites sensitivity analysis showed that simulated gep was more sensitive to wue during the summer compared to winter in australia and woody savannas and grasslands showed higher sensitivity than evergreen broadleaf forests and shrublands although our results are model specific the parallelization approach can be adopted in other similar ecosystem models for large scale applications keywords high performance computing ecohydrological modeling distributed memory parallelism shared memory parallelism water and carbon fluxes software availability name of software dwassi c developer wassi c was developed by ge sun dwassi c was developed by ning liu and mohsin shaikh contact address ge sun gesun fs fed us ning liu n liu murdoch edu au cost free software availability contact the developers github 1 introduction ecohydrological models describe the interactions between water vegetation and climate by coupling multiple hydrological and ecological processes they are very useful tools in assisting land managers and policy makers to simulate what if scenarios such as the effects of projected climate change on water resources and the terrestrial carbon cycle process based distributed ecohydrological models are commonly used by ecohydrologists as they not only achieve higher accuracy than empirical models chen et al 2015 but also couple the terrestrial water energy and biogeochemical cycles additionally process based models allow for investigations over much larger spatial and temporal resolutions as compared to traditional field studies fatichi et al 2016 remote sensing data are increasingly used as inputs to ecohydrological models to achieve more accurate simulations in comparison with models based solely on mathematical theory liu et al 1997 cao and woodward 1998 sun et al 2011b however without careful evaluation and quality control high resolution remote sensing data could induce systematic biases in ecohydrological simulations zhao et al 2005 eddy covariance flux towers which provide continuous measurements of ecosystem level fluxes of water and carbon baldocchi et al 2001 have been used for evaluating remote sensing data and ecohydrology models stoy et al 2006 sjostrom et al 2013 raczka et al 2013 zhou et al 2014 as the eddy covariance technique simultaneously measures water and carbon fluxes it reflects interactions between water and vegetation at the ecosystem scale and is therefore widely used to analyze changes in ecosystem carbon fluxes xiao et al 2012 and water use efficiency wue xiao et al 2013 the water supply stress index and carbon model wassi c developed by sun et al 2011b is an example of ecohydrological models which uses both remote sensing and eddy flux observations to simulate the coupling of water and carbon fluxes the accuracy of the wassi c model has been evaluated at a monthly time scale at 72 united states geological survey usgs gauging stations sun et al 2015 and in the upper zagunao watershed a sub catchment of the minjiang watershed in china liu et al 2013a b comparisons of the simulated streamflow against observations at the 72 usgs stations showed good overall model performance with correlation coefficients ranging from 0 71 to 1 0 sun et al 2015 with state of the art remote sensing technologies ecohydrological models such as wassi c driven with remote sensing data can simulate water and carbon processes at a very high resolution globally such high temporal and spatial resolution simulations are needed to investigate the interactions between the water and carbon cycles and for ecosystem management both at small and large scales wood et al 2011 state of the art remote sensing sensors have spatial resolutions ranging from centimeters on unmanned aerial vehicle platforms to meters on satellite platforms which provide a great opportunity for understanding vegetation dynamics application of such data would have immediate benefits for example hyper resolution modeling at 100 m or finer resolutions would allow for much better representation of the effects of spatial heterogeneity of topography soils and vegetation cover on hydrological dynamics wood et al 2011 this in turn will allow for a better representation of processes that are poorly represented in the current generation of models such as slope and aspect effects on surface incoming and reflected solar radiation and consequent effects on snowmelt soil moisture redistribution and evapotranspiration however processing hyper resolution modeling implies large data input into the resident memory of a central processing unit cpu followed by computation and consequent writing of the results to disk thus data movement is an obvious bottleneck in this process additionally advanced computational algorithms are needed to process satellite and other datasets via computationally demanding data assimilation procedures making use of multiple cores on a processor and distributing the computational domain to map onto multiple processors can be a prospective scalable solution to these issues the models of kollet et al 2010 and le et al 2015 perform each simulation unit independently and hence the model computational domain can easily be broken down into smaller individual segments as there is no dynamic interaction however for models including dynamic interaction with inter connected simulation units a dynamic parallelization method is required in order to parallelize a distributed model with inter connected simulation units li et al 2011 developed a dynamic parallelization method to balance computation load resulting in higher speedup and efficiency of parallel computing zhang et al 2016 further revised this dynamic parallelization method for hydrological model calibration using high performance computing hpc systems nonetheless at present massively parallel computational methods are not often implemented within ecohydrological models with the increasing availability of high resolution remote sensing products as well as high performance computers there is an opportunity to improve the accuracy of ecohydrological models and reduce their computing time the wasssi c model is a useful ecohydrological model but a major limitation is that the model does not operate in parallel continental scale simulations carried out at a pixel scale require large computational resources to generate output and post processing therefore the aim of this study was to develop a distributed version of the model dwassi c capable of using high spatial and temporal resolution remote sensing input data continental scale simulations were carried out over australia to test the sensitivity of water and carbon estimates to key model parameters specifically the sensitivity of gross ecosystem production gep to wue was investigated for each vegetation type using the new parallelized model 2 materials and methods 2 1 wassi c model the wassi c model is an ecohydrological model developed by sun et al 2011b and typically used to simulate monthly fluxes the main purpose of wassi c is to couple the water and carbon cycles with wue a ratio of gep to evapotranspiration et which is derived from global eddy flux observations wassi c consists of two empirical sub models a water supply stress index model and an empirical carbon model the input data include vegetation type soil parameters monthly mean meteorological forcing and remote sensed leaf area index lai and the main outputs are runoff et ecosystem respiration er and gep potential evapotranspiration pet is derived from lai precipitation p and reference evapotranspiration et0 sun et al 2011a in order to consider the effect of actual soil water storage on water fluxes et is calculated using the sacramento soil moisture accounting sac sma model anderson et al 2006 with pet the carbon sub model is an eddy flux derived wue empirical carbon model which calculates carbon fluxes from et and wue a schematic of the wassi c model is illustrated in fig 1 and a more detailed description of the wassi c model can be found on the model s website http www forestthreats org research tools wassi 2 1 1 monthly mean meteorological forcing to simulate carbon and water fluxes over the australian continent monthly gridded rainfall and temperature data were obtained from the ecosystem modeling and scaling infrastructure emast anuclimate v1 0 dataset at 0 01 spatial resolution from 1970 to 2013 hutchinson 2014 the daily rainfall data in situ used to generate the gridded product are from the australian bureau of meteorology bom network of weather stations anuclimate integrates a new approach to interpolate the station data to a regular grid using an improved background anomaly interpolation method and a new proximity to the coast modifier hutchinson 2014 2 1 2 monthly mean leaf area index lai the 0 01 gridded monthly lai for this study were generated by the land atmosphere interaction group at beijing normal university http globalchange bnu edu cn research lai yuan et al 2011 these data were originally obtained from moderate resolution imaging spectroradiometer modis lai products and then were improved by a two step integrated method in the first step missing and poor quality data were optimized with a modified temporal spatial filter and modis lai s quality control information this database was then fitted by timesat a software package for analysing time series of satellite sensor data using the savitzky golay smoothing model to further reduce the potential noise jonsson and eklundh 2004 the maximum rather than the mean of 8 day s lai in each month was used as the monthly lai value to omit the effect of potential noise and clouds zhao et al 2005 2 1 3 static soil and vegetation data gridded soil properties at 0 01 resolution were derived from the soil hydrological properties for the australia dataset mckenzie et al 2000 this dataset provides australia s soil hydrological properties for a and b horizons which are derived from soil mapping based on the atlas of australia soils the key soil hydrological properties used by wassi c include the soil depth soil plant available water holding capacity soil thickness saturated hydraulic conductivity field capacity wilting point and plant available water holding capacity using these soil properties 11 soil parameters for both a and b horizons were developed using anderson s method anderson et al 2006 these 11 parameters are required by the sac sma model to calculate the et in wassi c vegetation types were derived from the modis land cover product mcd12q1 which has a resolution of 500 m https lpdaac usgs gov dataset discovery modis modis products table mcd12q1 this dataset uses the international geosphere biosphere programme igbp classification scheme which consists of 17 general land cover types including 11 natural vegetation classes and 6 other land classes friedl et al 2010 taylor et al 2012 in this study the original 500 m modis land cover data were interpolated to 50 20 10 5 and 1 km using the majority resample algorithm dominant vegetation type http pro arcgis com en pro app tool reference data management resample htm for running simulations at different resolutions in order to test the performance of parallelization method to different resolutions described in more detail in section 2 3 2 2 framework of dwassi c the basic computing unit of the original wassi c model is a discrete watershed sun et al 2011b by using averaged climatic variables remote sensing and other land surface properties for each watershed the original wassi c model provides watershed scale water and carbon estimates however each watershed generally consists of various vegetation types and soil properties therefore a study at the watershed scale cannot provide enough spatial information on water and carbon processes to resolve this issue the parallelized and distributed version of wassi c referred to as dwassi c from here onwards was developed to simulate processes at the pixel scale the choice of pixel resolution is one of the key considerations when running the model this is illustrated in fig 2 showing the distribution of vegetation types across australia at 0 5 0 5 km resolution versus 50 50 km resolution at coarser resolutions areas close to the coastlines are more likely to be missclassified as compared to higher resolutions which illustrates the need for hyper resolution modeling however a key limitation is that the input data size for the wassi c model increases exponentially with the increase in resolution of gridded input data as shown in fig 3 for example the input dataset size increased from 50 mb to 850 gb when the pixel size was reduced from 50 50 km to 0 5 0 5 km while the number of grid points increased from 5644 to 53 971 840 the increasing input data size makes this a memory bound problem which can be handled by splitting the input data into smaller chunks to fit into the available main memory of a computing node 2 2 1 computing infrastructure simulations were executed on the pawsey supercomputing centre s computers called magnus and zeus magnus is a cray xc40 series supercomputer using intel xeon e5 2690 v3 haswell processors 2 6 ghz with a total of 1488 nodes with 24 cpus per node providing a total of 35 712 processor cores each compute node has access to 64 gb of memory zeus is an sgi cluster with compute nodes having intel xeon e5 e5 2670 v2 ivybridge processor 2 5 ghz with 20 cores and ram ranging from 128 to 512 gb per node magnus is designed for parallel applications whereas zeus is designed for high memory serial applications to identify the bottlenecks in the code serial simulations at different resolutions were carried out on zeus so as to make use of the larger amount of memory described in more detail in the next sub section based on these results parallel simulations were then carried out using magnus to take advantage of the higher number of compute nodes described in more detail in section 2 3 2 2 2 identifying the bottlenecks before parallelizing a model it is important to first identify the bottlenecks or hotspots where most of the compute time is spent in sequential execution of the code this is illustrated in table 1 showing the breakdown of time spent in different activities by the compute resource while running a dwassi c simulation at 5 km resolution on a single core on zeus table 1 shows that most of the time spent by dwassi c was executing cpu instructions most of the time in cpu was spent fetching data from main memory upon encountering cache misses improving data structures could improve this metric but the code is clearly memory bandwidth limited table 2 shows the variation in runtime memory usage with increasing resolution and illustrates an inherent limitation of the model with respect to how large the input datasets can be ingested when running the model on a single core the memory requirement for coarser to finer scales 50 0 5 km grows exponentially scaling up the memory by adding more ram on the same node may not be a viable option since the code is memory bandwidth limited additionally such large shared memory machines are expensive and thus rare scaling out using distributed memory architecture is a plausible solution as it provides two benefits a runtime memory can be extended to many nodes and b the bandwidth limitation can be hidden by optimizing the chunk of the grid running on each node table 3 breaks down the time spent in executing the cpu instructions in table 1 which is 95 for the 5 km resolution simulation almost all of the time spent by the cpu is in the subroutine waterbal which spends 45 of the time computing instructions in the local scope of the subroutine and the 38 in calling other subroutines most of the instructions are simple arithmetic operations but the amount of memory access calls by the cpu from table 1 suggests that the data locality could be improved data locality cache reuse or lack of it is a systemic problem in the code and it would therefore be advantageous to investigate the use of more cpus we decomposed the grid into subgrids subdomains and allow either threads or independent processes to processes these subgrids subdomains 2 2 3 model parallelization the basic computing unit of dwassi c is the pixel and there is no interaction between pixels this implies that each pixel can be computed processed independently for each pixel on the 2d modeling domain the input and output data can be seen as an n dimensional array the first two dimensions of this n dimensional array are the timestamps which include the year and the month as the simulation for each pixel is temporally successive domain decomposition is conducted to reduce computational time thus the domain containing all the pixels can be split into n subdomains two means of parallelism a shared memory and a distributed memory model were investigated to explore their respective advantages and limitations to exploit the multi core architecture of modern day chips a shared memory model was implemented using openmp which is an application programming interface for implementing multi platform shared memory programming models the model splits the domain into n subdomains and distributes the work to openmp threads the advantage of using this model is that it is able to utilize all the cores on a multi core chip all the threads can access the global memory address space in a thread safe manner the intermediate results are buffered and written by the master thread at the end it is important to highlight the inherent limitation of the shared memory model that the maximum number threads are limited by the physical core count of the processor and how many logical cpus each core can present the shared memory approach can have scalability issues when it comes to codes with frequent memory access as is the case here to address the inherent limitations of openmp i e the shared memory approach a distributed memory model was implemented as an embarrassingly parallel model using message passing interface mpi leveraging the independent computing for each pixel in this study in parallel computing an embarrassingly parallel workload is one where the problem domain can be decomposed into subdomains which can then run independently on compute units with minimal need of synchronization by communicating between them the gridded domain was segmented in n subdomains and distributed to p mpi processes all mpi processes get a subdomain of equal size if the total girds n are exactly divisible by the total number of mpi processes p if not then the last mpi process gets the remainder subdomain additionally unlike openmp each mpi process has its local memory address space and can only communicate to another mpi process via an mpi library call when these subdomains are distributed to an mpi process each mpi process may use openmp threads and employ the shared memory model as discussed above as a second level of parallelism thus the shared memory model using openmp can either be used as standalone parallel model if running on a single multicore node or as a hybrid model with mpi as illustrated in fig 4 showing the framework parallelizing dwassi 2 2 4 i o optimization reading and writing large datasets to the disk must keep up to avoid i o becoming a bottleneck in the case of shared memory parallelism where the dwassi c runs on a single shared memory node with multiple openmp threads file i o is handled by a single thread this is because disk i o is not a thread safe operation failing to scale i o performance with the improving compute performance would make the code i o bound this is discussed in more detail in section 3 1 in the case of the distributed memory model one mpi process does all the i o which means scattering and gathering input and output respectively using mpi this adds a communication cost and also creates a limitation that the data should fit into the memory of mpi process doing the file i o the other possibility could be that each mpi process does its own file i o this is a good strategy at small scales but high performance filesystems e g luster favor either shared parallel files or the case where a subset of mpi processes are responsible to do the file i o the later case is more scalable but requires some communication to scatter and gather data onto the designated mpi processes in our distributed memory implementation of dwassi c each mpi process reads and writes to a shared parallel file using the mpi io application programming interface additionally to reduce the archiving requirements the output was stored in binary format as opposed to ascii 2 3 model simulations in parallel to show case the usefulness of the parallelized dwassi cmodel sensitivity tests were carried out to test the sensitivity of gep to the wue parameter values used by the model this parameter was chosen as it couples water and carbon processes and is a key parameter for estimating gep sensitivity tests were carried out by running simulations with wue by 1 sd for each vegetation type as shown in table 4 in order to evaluate the model 4 eddy flux sites from the ozflux network beringer et al 2016 trudinger et al 2016 were used http data ozflux org au the vegetation types for these 4 sites au asm cleverly 2011 au tum van gorsel 2013 au ade beringer 2013a and au dap beringer 2013b are evergreen needleleaf forest enf evergreen broadleaf forest ebf woody savannas wsa and grassland gra respectively and the locations for these sites are shown in fig 2a 3 results 3 1 shared memory model using openmp fig 5 demonstrates the speed up achieved by introducing shared memory parallelism using openmp the profiling was run on single large memory node on pawsey s zeus cluster with one thread per core and up to 16 cores for simulations at 10 and 5 km resolution respectively openmp was implemented to parallelize the loops processing the input data and in both cases the openmp parallelization did not yield promising results with a reduction factor of approximately 1 2 when using 16 openmp threads versus a single thread this slight improvement in the overall compute time was largely due to the fraction of the compute time spent in executing the nested do loops parallelized by openmp in order to better understand the relatively poor performance of implementing openmp the total compute time was sub divided into the amount of time spent carrying out openmp operations referred to as the omp region the amount of time in carrying out file i o i e time spend in reading and writing to files and finally the amount of time spent in executing the subroutine output which formats the outputs and actually writes output files to disk this is illustrated in fig 6 which shows that the amount of time spent in omp region quickly plateaued after 8 threads for both the 10 km and 5 km resolution simulations the majority of the time was spent in executing the subroutine output which runs sequentially as file i o is thread unsafe and therefore sits outside the scope of openmp this subroutine takes approximately 65 of the overall compute time at 10 km resolution and approximately 82 at 5 km resolution with a single openmp thread and this increases rapidly with increasing numbers of threads thus introducing openmp made the code i o bound however file i o of fig 6 suggests that a mere 8 20 of total time in either case of input datasets is spent in actual file i o upon closer inspection it was found that most of the time was being spent in fortran s format calls within the output subroutine thus this operation became extremely expensive as the compute time in loops was reduced by introducing openmp 3 2 distributed memory model using mpi and hybrid mpi openmp implementations as has been described earlier the distributed memory model uses mpi to decompose and distribute the subdomains to worker mpi processes if only a single openmp thread is used then the implementation is mpi only and if more than 1 openmp threads are used then the implementation is a hybrid mpi openmp model as the worker mpi process can then spawn additional openmp threads and thus introduce shared memory parallelism fig 7 a shows the reduction in compute time by using an increasing number of mpi processes with 1 openmp thread i e a pure mpi implementation and 4 openmp threads i e a hybrid implementation for a 0 5 km resolution dwassi c simulation the compute time is reduced by approximately a factor of 2 by using more mpi processes when using a single openmp thread the effectiveness of using 4 rather than 1 openmp threads decreases as the number of mpi processes increases which suggests that using an increasing number of openmp threads may not be efficient for very high resolution simulations to better understand where compute time is spent fig 7 b shows a break down of the time spent by the output subroutine and what we refer to as user calls which includes all other calls including the main compute intensive subroutine waterbal table 3 the use of more mpi processes clearly results in a notable reduction in the amount of time spent in user calls but notably the compute time taken by the output subroutine also decreases albeit not as efficiently as user subroutines hence the introduction of mpi io to read and write files in parallel has removed i o bottleneck identified in the previous section since the use of 4 versus 1 openmp thread leads to a marked reduction in compute time when using 282 mpi processes fig 7 a we further investigated the impact of using more openmp threads as shown in fig 8 using 8 openmp threads only results in a slight improvement compared to 4 and using 16 actually results in a slight increase in compute time compared to 8 threads most of the improvement occurs when calling the user subroutines waterbal in particular whereas the output subroutine remains relatively unaffected as there is no openmp region in it the total compute time does not improve beyond 4 openmp threads as the openmp overhead denoted as omp ovhd in fig 8 d i e the time cost imposed by the operating system in managing threads becomes larger than the time spent in the openmp region of the code denoted as omp region in fig 8 d in summary for high resolution simulations using dwassi c the mpi approach should be adopted i e scaling to more mpi nodes while minimizing the number of openmp threads if a hybrid approach is used to further improve on the file writing performance the lustre filesystem file striping feature was explored file striping is a feature of lustre filesystem which is at the core of it being a high performance parallel filesystem a file can be written on more than one disk or object storage targets osts on a shared filesystem repeated access to data residing on a single disk can pose a bottleneck this is because data from other users may also be located on the same disk thus serializing the i o operation striping the file on multiple storage object reduces the footprint of the data on a single ost and the i o requests are fulfilled quickly multiple clients can read or write on different osts at the same time fig 9 shows the effect of using an increasing number of stripe counts on compute time for a 0 5 km resolution dwassi c simulation using 16 stripe counts significantly reduced the compute time but improvements beyond 64 stripe counts were minimal the results shown in figs 7 and 8 corresponded to the input datasets being read from 16 osts and the output files written on 112 osts in summary the key factor in improving the performance of dwassi c is to scale to more mpi processes in the following section we implement this approach to test the sensitivity of the model to one of its key parameters 3 3 sensitivity of gep to wue of dwassi c using the distributed version of dwassi c 5 km resolution simulations from 2000 to 2013 were conducted to investigate the sensitivity of the model to the wue parameter table 4 and the simulated gep was compared to fluxnet observations at 4 ozflux sites fig 2 this is illustrated in fig 10 showing monthly time series of the simulated and observed gep at the 4 ozflux sites the dwassi c model is able to capture the seasonal variations in gep as compared to the observed gep well simulated time series of gep were strongly correlated to observed gep p 0 05 for the sensitivity analysis the monthly mean simulated gep by the dwassi c model was compared to observations at each of the 4 ozflux sites as illustrated in fig 11 showing the monthly climatological comparisons of gep between the dwassi c model with wue 1 sd and observations at the 4 ozflux sites fig 12 shows the spatial sensitivity of gep to wue during winter and summer for those four vegetation types ebf showed the lowest sensitivity 2 5 g c m 2 d 1 to variations in the wue parameter while gep of wsa at the au ade site and gra at the au dap site demonstrated very high sensitivity to wue especially during the growing season about 10 g c m 2 d 1 figs 11 and 12 in addition fig 11 illustrates that there is a higher variation of gep in summer than in winter at all of the 4 sites spatially fig 12 demonstrates that gep in coastal areas was more sensitive to wue than inland areas moreover gep of grassland in north australia had the highest seasonal variation and was strongly sensitive to wue especially in summer which is the monsoon season the absolute difference in gep between control and control 1 sd for wsa was less than 1 g c m 2 d 1 in winter but more than 8 g c m 2 d 1 in summer the gep of osh in inland areas was not sensitive to wue irrespective of the season gep of ebf and cro along the south coast areas demonstrated moderate sensitivity to wue with absolute differences between the control and control 1 sd of approximately 2 5 g c m 2 d 1 the root mean square errors of the regressions between simulated and observed gep time series at the au asm au tum au ade and au dap were 0 8 1 5 1 9 and 2 1 g c m 2 d 1 respectively with the default wue for each vegetation type gep at au dap showed the best agreement between dwassi c and ozflux observations the dwassi c model showed a tendency to underestimate gep at the au tum site in summer but overestimated gep in summer at the au ade and au asm sites fig 11 3 4 water and carbon estimates over the australian continent from 2000 to 2013 water and carbon fluxes over the australian continent were simulated using the parallelized dwassi c model at a 5 5 km using the default parameters from 2000 to 2013 the mean annual et and gep were 0 54 mm d 1 and 1 0 g c m 2 d 1 respectively and the annual mean et generally showed a similar spatial pattern to gep as shown in fig 13 there were explicit gradients of water and carbon fluxes from inland arid zones to coastal temperate and tropical zones specifically et ranged from approximately 0 4 to 1 95 mm d 1 from the middle eastern and western inland arid zones to the eastern and southwestern temperate and northern tropical coastal zones while gep ranged from approximately 0 7 to 4 4 g c m 2 d 1 as for vegetation types ebf which was mainly distributed along the southwest and southeast temperate zones showed the highest carbon productivity with mean annual et and gep of 1 9 mm d 1 and 4 9 g c m 2 d 1 respectively although ebf makes up only approximately 1 of total vegetation cover this vegetation type still plays an important role in carbon sequestration as it is photosynthetically active throughout the year the dominant vegetation in australia is osh which accounts for 67 mainly in the inland arid zone and showed the lowest carbon productivity with et and gep around 0 7 mm year 1 and 1 6 g c m 2 d 1 respectively this was mainly the result of the shortage of water cro and wsa which were located in the transitional areas between forest and shrublands and account for approximately 10 of vegetation cover showed a similar capacity for carbon sequestration 3 1 g c m 2 d 1 but the et of wsa 1 7 mm d 1 was much higher than that of cro 1 2 mm year 1 sa in the northern tropical zone had modest carbon sequestration capacity with et and gep of approximately 2 1 mm d 1 and 4 6 g c m 2 d 1 respectively overall the dwassi c model can capture the seasonal cycles of both water and carbon fluxes reasonably well however biases between observations and simulated gep can still be large fig 10 and therefore the model needs further calibration and evaluation before answering more specific scientific questions such as the impacts of future climate change on the water and carbon cycles over the australian continent since the main aim of this paper is to show case the usefulness of parallelizing the model further model calibration and evaluation is outside the scope of this paper but will be the subject of future work 4 discussion and conclusions 4 1 parallel model the dwassi c model is memory bound and using shared memory parallelism openmp alone has significant limitations fig 5 the use of distributed memory parallelism mpi allows for the model domain to be decomposed into smaller chunks which are processed independently and this allows the code to run much more efficiently fig 7 single thread use using the hybrid approach with up to 16 openmp threads only did not increase performance markedly fig 8 and hence running the model as mpi only is also a viable option ideally the computation time should decrease linearly with the increase in mpi processors in the real world however adding more resources does not necessarily make the code run faster this is because the time taken by the parallelized computational part of the code reduces as the problem domain is decomposed and distributed more finely thus at one point the overhead of communication and or disk i o surpasses the expense of computation the scaling graphs in fig 7 demonstrate that there is an optimum number of mpi processes to maximize the usage of the computation resource of a computation node this optimum number or sweet spot is dependent on the size resolution of the problem domain and hence scaling tests should be carried out for different resolutions prior to running ensembles another advantage of the using the distributed memory model is the scalability in terms of memory a problem size posed by for example the 0 5 km resolution simulation is impractically large to simulate on any high end desktop or even a shared memory machine with a distributed model the more computation nodes available the larger the problem that can be mapped 4 2 effects of resolution of input data vegetation cover datasets with different resolutions were used to test the performance of parallelization method in addition to the spatial scale the temporal scale can also vary depending on the application of a model for the dwassi c model the basic temporal scale has been fixed as a monthly time period however this is still a controversial and complex issue for ecological studies because water and carbon processes at a specific scale are influenced by structure and function of other scales turnbull et al 2008 in addition these effects may even be non linear which changes the basic mathematical relationships for water and carbon processes during scaling yu et al 2008 thus a mechanistic interpretation of the behaviour of a system can only be derived by an assessment of the extent to which ecosystem structure and function are connected through time and space turnbull et al 2008 in this study we found that resolution of the input vegetation data sets had little effect on water and carbon estimates by the dwassi c model when the estimates were compared over the whole australian continent however for each pixel vegetation type is one of the most important factors influencing model estimates of water and carbon fluxes ecologically vegetation spatial organization and constitution affect both water availability and carbon sequestration currently the most commonly used remote sensing data used for terrestrial ecosystem detection are landsat and modis which have resolutions of 30 m and 500 m respectively even at the highest 30 m resolution each pixel may still represent combinations of several types of vegetation hydrology and soil characteristics all of which can contribute to higher variability of fluxes within one pixel ma et al 2015 found that a forest ecosystem carbon budget model for china at a 0 5 0 5 resolution overestimated the forest gross carbon dioxide uptake by approximately 8 7 because the vegetation fraction per grid cell was not taken into consideration similarly the dwassi c model only considers one vegetation type per grid cell and hence to accurately simulate water and carbon processes the vegetation fraction per grid cell should be taken into consideration and this is likely to add to the computational time vegetation is a key factor which determines hydrological partitioning in a watershed and the consequences for watershed scale hydrology however the role of vegetation in controlling the spatial and temporal dependence of water balance partitioning remains challenging to elucidate wood et al 1988 suggests that there is a specific spatial resolution for each model at which point the model is insensitive to higher resolutions wolock 1995 found that the simulation accuracy of a topographical based hydrological model topmodel increased with increasing resolution from 5 5 to 0 05 0 05 km in the sleepers river watershed vermont usa to resolve the difficulty of finding the most appropriate spatial resolution for hydrology models dehotin and braud 2008 developed a nested discretization method which allows a controlled and objective trade off between available data the resolution of the dominant water cycle components and the modeling objectives currently dwassi c has been only used for the watershed scale and its ideal simulation spatial resolution is still not clear therefore using the parallelized version of dwassi c the ideal spatial resolution of dwassi c and the effects of resolution of input data on the simulation can be more efficiently studied in the future 4 3 uncertainty of dwassi c and future work although the dwassi c model when operated using default wue parameters can clearly capture the seasonal cycle of water and carbon fluxes in comparison with observations a large area of australia such as the wsa and gra areas showed very high sensitivity of gep to wue especially during the growing season figs 10 and 11 these sensitivity results suggest that wue is a very important and highly sensitive parameter for carbon flux estimations even when using the default wue parameters differences between the simulated and observed fluxes can be large for example during the first half of 2007 for wsa fig 10 given that there are observations available from around 30 sites from the ozflux network http www ozflux org au and only two sites have been used for building the default model by sun et al 2011b there is clearly scope for further work in better constraining the model using the latest available observational fluxes from the ozflux network another source of uncertainty in eco hydrological models is the meteorological driving data slevin et al 2017 the gridded climate data used in this study is originally interpolated from point observations of weather stations across the australian continent therefore the number of observations and the accuracy of the interpolation determine the quality of the climate data jones et al 2009 a large portion of the inland arid australian continent is poorly covered by meteorological stations and hence there is greater uncertainty in these regions jones et al 2009 with the parallelized dwassi c model it is much easier and faster to conduct a model sensitivity analysis for the driving meteorological data currently all input and output data of dwassi c model are stored in binary format which has several disadvantages such as the need for more processing to further analyze and visualize the data and the need for additional documentation about the output binary format hierarchical data format hdf and network common data format netcdf provide a solution to these issues by enabling easier visualization and processing of model outputs and have the added advantage that the meta data are easily accessible from the output file which avoids the need for additional documentation additionally with netcdf hdf5 formats the input and output datasets can be read and written in parallel which would further reduce the compute time of the model the performance of netcdf hdf5 formats will be analyzed as part of future research acknowledgments this work was supported by resources provided by the pawsey supercomputing centre with funding from the australian government and the government of western australia a murdoch university doctoral scholarship ning liu and the chinese academy of forestry special research program for public welfare forestry 201304201 this work used eddy covariance data acquired and shared by the ozflux tern network dr jatin kala is supported by an australian research council discovery early career researcher grant de170100102 abbreviations the following abbreviations are used in this manuscript wassi c water supply stress index and carbon model dwassi c distributed wassi c openmp open multi processing mpi message passing interface i o input and output sac sma soil accounting content soil moisture availibity wue water use efficiency et evapotranspiration pet potential evapotranspiration gep gross ecosystem productivity et0 reference evapotranspiration er ecosystem respiration lai leaf area index modis moderate resolution imaging spectroradiometer mcd12q1 the land cover type climate modeling grid cmg product of modis igbp international geosphere biosphere programme ebf evergreen broadleaf forest enf evergreen needleleaf forest osh open shrubland wsa woody savannas sa savannas gra grassland cro cropland 
26433,wassi c is an ecohydrological model which couples water and carbon cycles with water use efficiency wue derived from global eddy flux observations however a significant limitation of the wassi c model is that it only runs serially high resolution simulations at a large scale are therefore computationally expensive and cause a run time memory burden using distributed mpi and shared openmp memory parallelism techniques we revised the original model as dwassi c we showed that using mpi was effective in reducing the computational run time and memory use two experiments were carried out to simulate water and carbon fluxes over the australian continent to test the sensitivity of the parallelized model to input data sets of different spatial resolutions as well as to wue parameters for different vegetation types these simulations were completed within minutes using dwassi c whereas they would not have been possible with the serial version the dwassi c model was able to simulate the seasonal dynamics of gross ecosystem productivity gep reasonably well when compared to observations at four eddy flux sites sensitivity analysis showed that simulated gep was more sensitive to wue during the summer compared to winter in australia and woody savannas and grasslands showed higher sensitivity than evergreen broadleaf forests and shrublands although our results are model specific the parallelization approach can be adopted in other similar ecosystem models for large scale applications keywords high performance computing ecohydrological modeling distributed memory parallelism shared memory parallelism water and carbon fluxes software availability name of software dwassi c developer wassi c was developed by ge sun dwassi c was developed by ning liu and mohsin shaikh contact address ge sun gesun fs fed us ning liu n liu murdoch edu au cost free software availability contact the developers github 1 introduction ecohydrological models describe the interactions between water vegetation and climate by coupling multiple hydrological and ecological processes they are very useful tools in assisting land managers and policy makers to simulate what if scenarios such as the effects of projected climate change on water resources and the terrestrial carbon cycle process based distributed ecohydrological models are commonly used by ecohydrologists as they not only achieve higher accuracy than empirical models chen et al 2015 but also couple the terrestrial water energy and biogeochemical cycles additionally process based models allow for investigations over much larger spatial and temporal resolutions as compared to traditional field studies fatichi et al 2016 remote sensing data are increasingly used as inputs to ecohydrological models to achieve more accurate simulations in comparison with models based solely on mathematical theory liu et al 1997 cao and woodward 1998 sun et al 2011b however without careful evaluation and quality control high resolution remote sensing data could induce systematic biases in ecohydrological simulations zhao et al 2005 eddy covariance flux towers which provide continuous measurements of ecosystem level fluxes of water and carbon baldocchi et al 2001 have been used for evaluating remote sensing data and ecohydrology models stoy et al 2006 sjostrom et al 2013 raczka et al 2013 zhou et al 2014 as the eddy covariance technique simultaneously measures water and carbon fluxes it reflects interactions between water and vegetation at the ecosystem scale and is therefore widely used to analyze changes in ecosystem carbon fluxes xiao et al 2012 and water use efficiency wue xiao et al 2013 the water supply stress index and carbon model wassi c developed by sun et al 2011b is an example of ecohydrological models which uses both remote sensing and eddy flux observations to simulate the coupling of water and carbon fluxes the accuracy of the wassi c model has been evaluated at a monthly time scale at 72 united states geological survey usgs gauging stations sun et al 2015 and in the upper zagunao watershed a sub catchment of the minjiang watershed in china liu et al 2013a b comparisons of the simulated streamflow against observations at the 72 usgs stations showed good overall model performance with correlation coefficients ranging from 0 71 to 1 0 sun et al 2015 with state of the art remote sensing technologies ecohydrological models such as wassi c driven with remote sensing data can simulate water and carbon processes at a very high resolution globally such high temporal and spatial resolution simulations are needed to investigate the interactions between the water and carbon cycles and for ecosystem management both at small and large scales wood et al 2011 state of the art remote sensing sensors have spatial resolutions ranging from centimeters on unmanned aerial vehicle platforms to meters on satellite platforms which provide a great opportunity for understanding vegetation dynamics application of such data would have immediate benefits for example hyper resolution modeling at 100 m or finer resolutions would allow for much better representation of the effects of spatial heterogeneity of topography soils and vegetation cover on hydrological dynamics wood et al 2011 this in turn will allow for a better representation of processes that are poorly represented in the current generation of models such as slope and aspect effects on surface incoming and reflected solar radiation and consequent effects on snowmelt soil moisture redistribution and evapotranspiration however processing hyper resolution modeling implies large data input into the resident memory of a central processing unit cpu followed by computation and consequent writing of the results to disk thus data movement is an obvious bottleneck in this process additionally advanced computational algorithms are needed to process satellite and other datasets via computationally demanding data assimilation procedures making use of multiple cores on a processor and distributing the computational domain to map onto multiple processors can be a prospective scalable solution to these issues the models of kollet et al 2010 and le et al 2015 perform each simulation unit independently and hence the model computational domain can easily be broken down into smaller individual segments as there is no dynamic interaction however for models including dynamic interaction with inter connected simulation units a dynamic parallelization method is required in order to parallelize a distributed model with inter connected simulation units li et al 2011 developed a dynamic parallelization method to balance computation load resulting in higher speedup and efficiency of parallel computing zhang et al 2016 further revised this dynamic parallelization method for hydrological model calibration using high performance computing hpc systems nonetheless at present massively parallel computational methods are not often implemented within ecohydrological models with the increasing availability of high resolution remote sensing products as well as high performance computers there is an opportunity to improve the accuracy of ecohydrological models and reduce their computing time the wasssi c model is a useful ecohydrological model but a major limitation is that the model does not operate in parallel continental scale simulations carried out at a pixel scale require large computational resources to generate output and post processing therefore the aim of this study was to develop a distributed version of the model dwassi c capable of using high spatial and temporal resolution remote sensing input data continental scale simulations were carried out over australia to test the sensitivity of water and carbon estimates to key model parameters specifically the sensitivity of gross ecosystem production gep to wue was investigated for each vegetation type using the new parallelized model 2 materials and methods 2 1 wassi c model the wassi c model is an ecohydrological model developed by sun et al 2011b and typically used to simulate monthly fluxes the main purpose of wassi c is to couple the water and carbon cycles with wue a ratio of gep to evapotranspiration et which is derived from global eddy flux observations wassi c consists of two empirical sub models a water supply stress index model and an empirical carbon model the input data include vegetation type soil parameters monthly mean meteorological forcing and remote sensed leaf area index lai and the main outputs are runoff et ecosystem respiration er and gep potential evapotranspiration pet is derived from lai precipitation p and reference evapotranspiration et0 sun et al 2011a in order to consider the effect of actual soil water storage on water fluxes et is calculated using the sacramento soil moisture accounting sac sma model anderson et al 2006 with pet the carbon sub model is an eddy flux derived wue empirical carbon model which calculates carbon fluxes from et and wue a schematic of the wassi c model is illustrated in fig 1 and a more detailed description of the wassi c model can be found on the model s website http www forestthreats org research tools wassi 2 1 1 monthly mean meteorological forcing to simulate carbon and water fluxes over the australian continent monthly gridded rainfall and temperature data were obtained from the ecosystem modeling and scaling infrastructure emast anuclimate v1 0 dataset at 0 01 spatial resolution from 1970 to 2013 hutchinson 2014 the daily rainfall data in situ used to generate the gridded product are from the australian bureau of meteorology bom network of weather stations anuclimate integrates a new approach to interpolate the station data to a regular grid using an improved background anomaly interpolation method and a new proximity to the coast modifier hutchinson 2014 2 1 2 monthly mean leaf area index lai the 0 01 gridded monthly lai for this study were generated by the land atmosphere interaction group at beijing normal university http globalchange bnu edu cn research lai yuan et al 2011 these data were originally obtained from moderate resolution imaging spectroradiometer modis lai products and then were improved by a two step integrated method in the first step missing and poor quality data were optimized with a modified temporal spatial filter and modis lai s quality control information this database was then fitted by timesat a software package for analysing time series of satellite sensor data using the savitzky golay smoothing model to further reduce the potential noise jonsson and eklundh 2004 the maximum rather than the mean of 8 day s lai in each month was used as the monthly lai value to omit the effect of potential noise and clouds zhao et al 2005 2 1 3 static soil and vegetation data gridded soil properties at 0 01 resolution were derived from the soil hydrological properties for the australia dataset mckenzie et al 2000 this dataset provides australia s soil hydrological properties for a and b horizons which are derived from soil mapping based on the atlas of australia soils the key soil hydrological properties used by wassi c include the soil depth soil plant available water holding capacity soil thickness saturated hydraulic conductivity field capacity wilting point and plant available water holding capacity using these soil properties 11 soil parameters for both a and b horizons were developed using anderson s method anderson et al 2006 these 11 parameters are required by the sac sma model to calculate the et in wassi c vegetation types were derived from the modis land cover product mcd12q1 which has a resolution of 500 m https lpdaac usgs gov dataset discovery modis modis products table mcd12q1 this dataset uses the international geosphere biosphere programme igbp classification scheme which consists of 17 general land cover types including 11 natural vegetation classes and 6 other land classes friedl et al 2010 taylor et al 2012 in this study the original 500 m modis land cover data were interpolated to 50 20 10 5 and 1 km using the majority resample algorithm dominant vegetation type http pro arcgis com en pro app tool reference data management resample htm for running simulations at different resolutions in order to test the performance of parallelization method to different resolutions described in more detail in section 2 3 2 2 framework of dwassi c the basic computing unit of the original wassi c model is a discrete watershed sun et al 2011b by using averaged climatic variables remote sensing and other land surface properties for each watershed the original wassi c model provides watershed scale water and carbon estimates however each watershed generally consists of various vegetation types and soil properties therefore a study at the watershed scale cannot provide enough spatial information on water and carbon processes to resolve this issue the parallelized and distributed version of wassi c referred to as dwassi c from here onwards was developed to simulate processes at the pixel scale the choice of pixel resolution is one of the key considerations when running the model this is illustrated in fig 2 showing the distribution of vegetation types across australia at 0 5 0 5 km resolution versus 50 50 km resolution at coarser resolutions areas close to the coastlines are more likely to be missclassified as compared to higher resolutions which illustrates the need for hyper resolution modeling however a key limitation is that the input data size for the wassi c model increases exponentially with the increase in resolution of gridded input data as shown in fig 3 for example the input dataset size increased from 50 mb to 850 gb when the pixel size was reduced from 50 50 km to 0 5 0 5 km while the number of grid points increased from 5644 to 53 971 840 the increasing input data size makes this a memory bound problem which can be handled by splitting the input data into smaller chunks to fit into the available main memory of a computing node 2 2 1 computing infrastructure simulations were executed on the pawsey supercomputing centre s computers called magnus and zeus magnus is a cray xc40 series supercomputer using intel xeon e5 2690 v3 haswell processors 2 6 ghz with a total of 1488 nodes with 24 cpus per node providing a total of 35 712 processor cores each compute node has access to 64 gb of memory zeus is an sgi cluster with compute nodes having intel xeon e5 e5 2670 v2 ivybridge processor 2 5 ghz with 20 cores and ram ranging from 128 to 512 gb per node magnus is designed for parallel applications whereas zeus is designed for high memory serial applications to identify the bottlenecks in the code serial simulations at different resolutions were carried out on zeus so as to make use of the larger amount of memory described in more detail in the next sub section based on these results parallel simulations were then carried out using magnus to take advantage of the higher number of compute nodes described in more detail in section 2 3 2 2 2 identifying the bottlenecks before parallelizing a model it is important to first identify the bottlenecks or hotspots where most of the compute time is spent in sequential execution of the code this is illustrated in table 1 showing the breakdown of time spent in different activities by the compute resource while running a dwassi c simulation at 5 km resolution on a single core on zeus table 1 shows that most of the time spent by dwassi c was executing cpu instructions most of the time in cpu was spent fetching data from main memory upon encountering cache misses improving data structures could improve this metric but the code is clearly memory bandwidth limited table 2 shows the variation in runtime memory usage with increasing resolution and illustrates an inherent limitation of the model with respect to how large the input datasets can be ingested when running the model on a single core the memory requirement for coarser to finer scales 50 0 5 km grows exponentially scaling up the memory by adding more ram on the same node may not be a viable option since the code is memory bandwidth limited additionally such large shared memory machines are expensive and thus rare scaling out using distributed memory architecture is a plausible solution as it provides two benefits a runtime memory can be extended to many nodes and b the bandwidth limitation can be hidden by optimizing the chunk of the grid running on each node table 3 breaks down the time spent in executing the cpu instructions in table 1 which is 95 for the 5 km resolution simulation almost all of the time spent by the cpu is in the subroutine waterbal which spends 45 of the time computing instructions in the local scope of the subroutine and the 38 in calling other subroutines most of the instructions are simple arithmetic operations but the amount of memory access calls by the cpu from table 1 suggests that the data locality could be improved data locality cache reuse or lack of it is a systemic problem in the code and it would therefore be advantageous to investigate the use of more cpus we decomposed the grid into subgrids subdomains and allow either threads or independent processes to processes these subgrids subdomains 2 2 3 model parallelization the basic computing unit of dwassi c is the pixel and there is no interaction between pixels this implies that each pixel can be computed processed independently for each pixel on the 2d modeling domain the input and output data can be seen as an n dimensional array the first two dimensions of this n dimensional array are the timestamps which include the year and the month as the simulation for each pixel is temporally successive domain decomposition is conducted to reduce computational time thus the domain containing all the pixels can be split into n subdomains two means of parallelism a shared memory and a distributed memory model were investigated to explore their respective advantages and limitations to exploit the multi core architecture of modern day chips a shared memory model was implemented using openmp which is an application programming interface for implementing multi platform shared memory programming models the model splits the domain into n subdomains and distributes the work to openmp threads the advantage of using this model is that it is able to utilize all the cores on a multi core chip all the threads can access the global memory address space in a thread safe manner the intermediate results are buffered and written by the master thread at the end it is important to highlight the inherent limitation of the shared memory model that the maximum number threads are limited by the physical core count of the processor and how many logical cpus each core can present the shared memory approach can have scalability issues when it comes to codes with frequent memory access as is the case here to address the inherent limitations of openmp i e the shared memory approach a distributed memory model was implemented as an embarrassingly parallel model using message passing interface mpi leveraging the independent computing for each pixel in this study in parallel computing an embarrassingly parallel workload is one where the problem domain can be decomposed into subdomains which can then run independently on compute units with minimal need of synchronization by communicating between them the gridded domain was segmented in n subdomains and distributed to p mpi processes all mpi processes get a subdomain of equal size if the total girds n are exactly divisible by the total number of mpi processes p if not then the last mpi process gets the remainder subdomain additionally unlike openmp each mpi process has its local memory address space and can only communicate to another mpi process via an mpi library call when these subdomains are distributed to an mpi process each mpi process may use openmp threads and employ the shared memory model as discussed above as a second level of parallelism thus the shared memory model using openmp can either be used as standalone parallel model if running on a single multicore node or as a hybrid model with mpi as illustrated in fig 4 showing the framework parallelizing dwassi 2 2 4 i o optimization reading and writing large datasets to the disk must keep up to avoid i o becoming a bottleneck in the case of shared memory parallelism where the dwassi c runs on a single shared memory node with multiple openmp threads file i o is handled by a single thread this is because disk i o is not a thread safe operation failing to scale i o performance with the improving compute performance would make the code i o bound this is discussed in more detail in section 3 1 in the case of the distributed memory model one mpi process does all the i o which means scattering and gathering input and output respectively using mpi this adds a communication cost and also creates a limitation that the data should fit into the memory of mpi process doing the file i o the other possibility could be that each mpi process does its own file i o this is a good strategy at small scales but high performance filesystems e g luster favor either shared parallel files or the case where a subset of mpi processes are responsible to do the file i o the later case is more scalable but requires some communication to scatter and gather data onto the designated mpi processes in our distributed memory implementation of dwassi c each mpi process reads and writes to a shared parallel file using the mpi io application programming interface additionally to reduce the archiving requirements the output was stored in binary format as opposed to ascii 2 3 model simulations in parallel to show case the usefulness of the parallelized dwassi cmodel sensitivity tests were carried out to test the sensitivity of gep to the wue parameter values used by the model this parameter was chosen as it couples water and carbon processes and is a key parameter for estimating gep sensitivity tests were carried out by running simulations with wue by 1 sd for each vegetation type as shown in table 4 in order to evaluate the model 4 eddy flux sites from the ozflux network beringer et al 2016 trudinger et al 2016 were used http data ozflux org au the vegetation types for these 4 sites au asm cleverly 2011 au tum van gorsel 2013 au ade beringer 2013a and au dap beringer 2013b are evergreen needleleaf forest enf evergreen broadleaf forest ebf woody savannas wsa and grassland gra respectively and the locations for these sites are shown in fig 2a 3 results 3 1 shared memory model using openmp fig 5 demonstrates the speed up achieved by introducing shared memory parallelism using openmp the profiling was run on single large memory node on pawsey s zeus cluster with one thread per core and up to 16 cores for simulations at 10 and 5 km resolution respectively openmp was implemented to parallelize the loops processing the input data and in both cases the openmp parallelization did not yield promising results with a reduction factor of approximately 1 2 when using 16 openmp threads versus a single thread this slight improvement in the overall compute time was largely due to the fraction of the compute time spent in executing the nested do loops parallelized by openmp in order to better understand the relatively poor performance of implementing openmp the total compute time was sub divided into the amount of time spent carrying out openmp operations referred to as the omp region the amount of time in carrying out file i o i e time spend in reading and writing to files and finally the amount of time spent in executing the subroutine output which formats the outputs and actually writes output files to disk this is illustrated in fig 6 which shows that the amount of time spent in omp region quickly plateaued after 8 threads for both the 10 km and 5 km resolution simulations the majority of the time was spent in executing the subroutine output which runs sequentially as file i o is thread unsafe and therefore sits outside the scope of openmp this subroutine takes approximately 65 of the overall compute time at 10 km resolution and approximately 82 at 5 km resolution with a single openmp thread and this increases rapidly with increasing numbers of threads thus introducing openmp made the code i o bound however file i o of fig 6 suggests that a mere 8 20 of total time in either case of input datasets is spent in actual file i o upon closer inspection it was found that most of the time was being spent in fortran s format calls within the output subroutine thus this operation became extremely expensive as the compute time in loops was reduced by introducing openmp 3 2 distributed memory model using mpi and hybrid mpi openmp implementations as has been described earlier the distributed memory model uses mpi to decompose and distribute the subdomains to worker mpi processes if only a single openmp thread is used then the implementation is mpi only and if more than 1 openmp threads are used then the implementation is a hybrid mpi openmp model as the worker mpi process can then spawn additional openmp threads and thus introduce shared memory parallelism fig 7 a shows the reduction in compute time by using an increasing number of mpi processes with 1 openmp thread i e a pure mpi implementation and 4 openmp threads i e a hybrid implementation for a 0 5 km resolution dwassi c simulation the compute time is reduced by approximately a factor of 2 by using more mpi processes when using a single openmp thread the effectiveness of using 4 rather than 1 openmp threads decreases as the number of mpi processes increases which suggests that using an increasing number of openmp threads may not be efficient for very high resolution simulations to better understand where compute time is spent fig 7 b shows a break down of the time spent by the output subroutine and what we refer to as user calls which includes all other calls including the main compute intensive subroutine waterbal table 3 the use of more mpi processes clearly results in a notable reduction in the amount of time spent in user calls but notably the compute time taken by the output subroutine also decreases albeit not as efficiently as user subroutines hence the introduction of mpi io to read and write files in parallel has removed i o bottleneck identified in the previous section since the use of 4 versus 1 openmp thread leads to a marked reduction in compute time when using 282 mpi processes fig 7 a we further investigated the impact of using more openmp threads as shown in fig 8 using 8 openmp threads only results in a slight improvement compared to 4 and using 16 actually results in a slight increase in compute time compared to 8 threads most of the improvement occurs when calling the user subroutines waterbal in particular whereas the output subroutine remains relatively unaffected as there is no openmp region in it the total compute time does not improve beyond 4 openmp threads as the openmp overhead denoted as omp ovhd in fig 8 d i e the time cost imposed by the operating system in managing threads becomes larger than the time spent in the openmp region of the code denoted as omp region in fig 8 d in summary for high resolution simulations using dwassi c the mpi approach should be adopted i e scaling to more mpi nodes while minimizing the number of openmp threads if a hybrid approach is used to further improve on the file writing performance the lustre filesystem file striping feature was explored file striping is a feature of lustre filesystem which is at the core of it being a high performance parallel filesystem a file can be written on more than one disk or object storage targets osts on a shared filesystem repeated access to data residing on a single disk can pose a bottleneck this is because data from other users may also be located on the same disk thus serializing the i o operation striping the file on multiple storage object reduces the footprint of the data on a single ost and the i o requests are fulfilled quickly multiple clients can read or write on different osts at the same time fig 9 shows the effect of using an increasing number of stripe counts on compute time for a 0 5 km resolution dwassi c simulation using 16 stripe counts significantly reduced the compute time but improvements beyond 64 stripe counts were minimal the results shown in figs 7 and 8 corresponded to the input datasets being read from 16 osts and the output files written on 112 osts in summary the key factor in improving the performance of dwassi c is to scale to more mpi processes in the following section we implement this approach to test the sensitivity of the model to one of its key parameters 3 3 sensitivity of gep to wue of dwassi c using the distributed version of dwassi c 5 km resolution simulations from 2000 to 2013 were conducted to investigate the sensitivity of the model to the wue parameter table 4 and the simulated gep was compared to fluxnet observations at 4 ozflux sites fig 2 this is illustrated in fig 10 showing monthly time series of the simulated and observed gep at the 4 ozflux sites the dwassi c model is able to capture the seasonal variations in gep as compared to the observed gep well simulated time series of gep were strongly correlated to observed gep p 0 05 for the sensitivity analysis the monthly mean simulated gep by the dwassi c model was compared to observations at each of the 4 ozflux sites as illustrated in fig 11 showing the monthly climatological comparisons of gep between the dwassi c model with wue 1 sd and observations at the 4 ozflux sites fig 12 shows the spatial sensitivity of gep to wue during winter and summer for those four vegetation types ebf showed the lowest sensitivity 2 5 g c m 2 d 1 to variations in the wue parameter while gep of wsa at the au ade site and gra at the au dap site demonstrated very high sensitivity to wue especially during the growing season about 10 g c m 2 d 1 figs 11 and 12 in addition fig 11 illustrates that there is a higher variation of gep in summer than in winter at all of the 4 sites spatially fig 12 demonstrates that gep in coastal areas was more sensitive to wue than inland areas moreover gep of grassland in north australia had the highest seasonal variation and was strongly sensitive to wue especially in summer which is the monsoon season the absolute difference in gep between control and control 1 sd for wsa was less than 1 g c m 2 d 1 in winter but more than 8 g c m 2 d 1 in summer the gep of osh in inland areas was not sensitive to wue irrespective of the season gep of ebf and cro along the south coast areas demonstrated moderate sensitivity to wue with absolute differences between the control and control 1 sd of approximately 2 5 g c m 2 d 1 the root mean square errors of the regressions between simulated and observed gep time series at the au asm au tum au ade and au dap were 0 8 1 5 1 9 and 2 1 g c m 2 d 1 respectively with the default wue for each vegetation type gep at au dap showed the best agreement between dwassi c and ozflux observations the dwassi c model showed a tendency to underestimate gep at the au tum site in summer but overestimated gep in summer at the au ade and au asm sites fig 11 3 4 water and carbon estimates over the australian continent from 2000 to 2013 water and carbon fluxes over the australian continent were simulated using the parallelized dwassi c model at a 5 5 km using the default parameters from 2000 to 2013 the mean annual et and gep were 0 54 mm d 1 and 1 0 g c m 2 d 1 respectively and the annual mean et generally showed a similar spatial pattern to gep as shown in fig 13 there were explicit gradients of water and carbon fluxes from inland arid zones to coastal temperate and tropical zones specifically et ranged from approximately 0 4 to 1 95 mm d 1 from the middle eastern and western inland arid zones to the eastern and southwestern temperate and northern tropical coastal zones while gep ranged from approximately 0 7 to 4 4 g c m 2 d 1 as for vegetation types ebf which was mainly distributed along the southwest and southeast temperate zones showed the highest carbon productivity with mean annual et and gep of 1 9 mm d 1 and 4 9 g c m 2 d 1 respectively although ebf makes up only approximately 1 of total vegetation cover this vegetation type still plays an important role in carbon sequestration as it is photosynthetically active throughout the year the dominant vegetation in australia is osh which accounts for 67 mainly in the inland arid zone and showed the lowest carbon productivity with et and gep around 0 7 mm year 1 and 1 6 g c m 2 d 1 respectively this was mainly the result of the shortage of water cro and wsa which were located in the transitional areas between forest and shrublands and account for approximately 10 of vegetation cover showed a similar capacity for carbon sequestration 3 1 g c m 2 d 1 but the et of wsa 1 7 mm d 1 was much higher than that of cro 1 2 mm year 1 sa in the northern tropical zone had modest carbon sequestration capacity with et and gep of approximately 2 1 mm d 1 and 4 6 g c m 2 d 1 respectively overall the dwassi c model can capture the seasonal cycles of both water and carbon fluxes reasonably well however biases between observations and simulated gep can still be large fig 10 and therefore the model needs further calibration and evaluation before answering more specific scientific questions such as the impacts of future climate change on the water and carbon cycles over the australian continent since the main aim of this paper is to show case the usefulness of parallelizing the model further model calibration and evaluation is outside the scope of this paper but will be the subject of future work 4 discussion and conclusions 4 1 parallel model the dwassi c model is memory bound and using shared memory parallelism openmp alone has significant limitations fig 5 the use of distributed memory parallelism mpi allows for the model domain to be decomposed into smaller chunks which are processed independently and this allows the code to run much more efficiently fig 7 single thread use using the hybrid approach with up to 16 openmp threads only did not increase performance markedly fig 8 and hence running the model as mpi only is also a viable option ideally the computation time should decrease linearly with the increase in mpi processors in the real world however adding more resources does not necessarily make the code run faster this is because the time taken by the parallelized computational part of the code reduces as the problem domain is decomposed and distributed more finely thus at one point the overhead of communication and or disk i o surpasses the expense of computation the scaling graphs in fig 7 demonstrate that there is an optimum number of mpi processes to maximize the usage of the computation resource of a computation node this optimum number or sweet spot is dependent on the size resolution of the problem domain and hence scaling tests should be carried out for different resolutions prior to running ensembles another advantage of the using the distributed memory model is the scalability in terms of memory a problem size posed by for example the 0 5 km resolution simulation is impractically large to simulate on any high end desktop or even a shared memory machine with a distributed model the more computation nodes available the larger the problem that can be mapped 4 2 effects of resolution of input data vegetation cover datasets with different resolutions were used to test the performance of parallelization method in addition to the spatial scale the temporal scale can also vary depending on the application of a model for the dwassi c model the basic temporal scale has been fixed as a monthly time period however this is still a controversial and complex issue for ecological studies because water and carbon processes at a specific scale are influenced by structure and function of other scales turnbull et al 2008 in addition these effects may even be non linear which changes the basic mathematical relationships for water and carbon processes during scaling yu et al 2008 thus a mechanistic interpretation of the behaviour of a system can only be derived by an assessment of the extent to which ecosystem structure and function are connected through time and space turnbull et al 2008 in this study we found that resolution of the input vegetation data sets had little effect on water and carbon estimates by the dwassi c model when the estimates were compared over the whole australian continent however for each pixel vegetation type is one of the most important factors influencing model estimates of water and carbon fluxes ecologically vegetation spatial organization and constitution affect both water availability and carbon sequestration currently the most commonly used remote sensing data used for terrestrial ecosystem detection are landsat and modis which have resolutions of 30 m and 500 m respectively even at the highest 30 m resolution each pixel may still represent combinations of several types of vegetation hydrology and soil characteristics all of which can contribute to higher variability of fluxes within one pixel ma et al 2015 found that a forest ecosystem carbon budget model for china at a 0 5 0 5 resolution overestimated the forest gross carbon dioxide uptake by approximately 8 7 because the vegetation fraction per grid cell was not taken into consideration similarly the dwassi c model only considers one vegetation type per grid cell and hence to accurately simulate water and carbon processes the vegetation fraction per grid cell should be taken into consideration and this is likely to add to the computational time vegetation is a key factor which determines hydrological partitioning in a watershed and the consequences for watershed scale hydrology however the role of vegetation in controlling the spatial and temporal dependence of water balance partitioning remains challenging to elucidate wood et al 1988 suggests that there is a specific spatial resolution for each model at which point the model is insensitive to higher resolutions wolock 1995 found that the simulation accuracy of a topographical based hydrological model topmodel increased with increasing resolution from 5 5 to 0 05 0 05 km in the sleepers river watershed vermont usa to resolve the difficulty of finding the most appropriate spatial resolution for hydrology models dehotin and braud 2008 developed a nested discretization method which allows a controlled and objective trade off between available data the resolution of the dominant water cycle components and the modeling objectives currently dwassi c has been only used for the watershed scale and its ideal simulation spatial resolution is still not clear therefore using the parallelized version of dwassi c the ideal spatial resolution of dwassi c and the effects of resolution of input data on the simulation can be more efficiently studied in the future 4 3 uncertainty of dwassi c and future work although the dwassi c model when operated using default wue parameters can clearly capture the seasonal cycle of water and carbon fluxes in comparison with observations a large area of australia such as the wsa and gra areas showed very high sensitivity of gep to wue especially during the growing season figs 10 and 11 these sensitivity results suggest that wue is a very important and highly sensitive parameter for carbon flux estimations even when using the default wue parameters differences between the simulated and observed fluxes can be large for example during the first half of 2007 for wsa fig 10 given that there are observations available from around 30 sites from the ozflux network http www ozflux org au and only two sites have been used for building the default model by sun et al 2011b there is clearly scope for further work in better constraining the model using the latest available observational fluxes from the ozflux network another source of uncertainty in eco hydrological models is the meteorological driving data slevin et al 2017 the gridded climate data used in this study is originally interpolated from point observations of weather stations across the australian continent therefore the number of observations and the accuracy of the interpolation determine the quality of the climate data jones et al 2009 a large portion of the inland arid australian continent is poorly covered by meteorological stations and hence there is greater uncertainty in these regions jones et al 2009 with the parallelized dwassi c model it is much easier and faster to conduct a model sensitivity analysis for the driving meteorological data currently all input and output data of dwassi c model are stored in binary format which has several disadvantages such as the need for more processing to further analyze and visualize the data and the need for additional documentation about the output binary format hierarchical data format hdf and network common data format netcdf provide a solution to these issues by enabling easier visualization and processing of model outputs and have the added advantage that the meta data are easily accessible from the output file which avoids the need for additional documentation additionally with netcdf hdf5 formats the input and output datasets can be read and written in parallel which would further reduce the compute time of the model the performance of netcdf hdf5 formats will be analyzed as part of future research acknowledgments this work was supported by resources provided by the pawsey supercomputing centre with funding from the australian government and the government of western australia a murdoch university doctoral scholarship ning liu and the chinese academy of forestry special research program for public welfare forestry 201304201 this work used eddy covariance data acquired and shared by the ozflux tern network dr jatin kala is supported by an australian research council discovery early career researcher grant de170100102 abbreviations the following abbreviations are used in this manuscript wassi c water supply stress index and carbon model dwassi c distributed wassi c openmp open multi processing mpi message passing interface i o input and output sac sma soil accounting content soil moisture availibity wue water use efficiency et evapotranspiration pet potential evapotranspiration gep gross ecosystem productivity et0 reference evapotranspiration er ecosystem respiration lai leaf area index modis moderate resolution imaging spectroradiometer mcd12q1 the land cover type climate modeling grid cmg product of modis igbp international geosphere biosphere programme ebf evergreen broadleaf forest enf evergreen needleleaf forest osh open shrubland wsa woody savannas sa savannas gra grassland cro cropland 
26434,the application of computer simulation models plays a significant role in the understanding of water dynamics in basins the recent and explosive growth of the processing capabilities of general purpose graphics processing units gpgpus has resulted in widespread interest in parallel computing from the modelling community in this paper we present a gpgpu implementation of finite differences solution of the equations of the 2d groundwater flow in unconfined aquifers for heterogeneous and anisotropic media we show that the gpgpu accelerated solution implemented using cuda 1 c c largely outperforms the corresponding serial solution in c c the results show that the gpgpu accelerated implementation is capable of providing up to a 56 fold speedup in the solution using an ordinary office computer equipped with an inexpensive gpu 2 card the code developed for this research is available for download and use at http modelagemambientaluffs blogspot com br keywords groundwater modelling unconfined aquifers heterogeneous and anisotropic media high performance computing cuda availability program name pargw developer tomas carlotto contact address thomas carl hotmail com year first available 2017 software required cuda toolkit 8 0 or later and cusp library program language cuda c package size 179 mb availability http modelagemambientaluffs blogspot com br cost free of charge 1 introduction spatially distributed models are widely recognized as important tools for the understanding of groundwater dynamics markstrom et al 2008 rouholahnejad et al 2012 zhang et al 2013 hwang et al 2014 le phong et al 2015 they are widely applicable in environmental analysis of flooded areas pollutant contamination of aquifers wells and reservoirs botros et al 2012 czarnecki et al 2010 allan freeze and witherspoon 1966 he et al 2011 hwang et al 2014 lamb and beven 1997 lange et al 2014 le phong et al 2015 lyne and hollick 1979 miller et al 2013 markstrom et al 2008 mendoza and martins 2006 voeckler et al 2014 the implementation of computer models has benefited from the rapid growth in computational capabilities observed over the last thirty years or so lange et al 2014 markstrom et al 2008 more recently the rapid emergence of parallel computing platforms based on gpgpu general purpose graphics processing units has provided an entirely new perspective regarding the processing capabilities of personal computers thus attracting the attention of the modelling community nickolls et al 2008 singh et al 2011 ji et al 2012 2014 le phong et al 2015 zhou et al 2013 in this context hydrological models are particularly suitable for massively parallel frameworks due to the state of the art of solution techniques for sparse linear systems via domain decomposition methods barrett et al 1994 formaggia et al 2006 the assessment of groundwater and surface water dynamics in real basins is a challenging issue which requires knowledge of several factors that influence the hydrological cycle such as the physical and chemical characteristics of the soil and of the environment as a whole a number of studies have been devoted to the development of hydrologic models whose aim is to describe groundwater flow while considering different levels of soil saturation and diverse geologic conditions freeze 1971 lange et al 2014 voeckler et al 2014 among the variety of groundwater models in the literature one can find models that describe groundwater flow in basins graaf et al 2014 czarnecki et al 2010 freeze 1971 marc et al 1998 most commonly groundwater models are designed to deal with the case of isotropic and homogeneous media harbaugh 2005 markstrom et al 2008 lange et al 2014 voeckler et al 2014 czarnecki et al 2010 although the assumptions on homogeneity and isotropy permit the simplification of the resulting equations and their solution they might not be adequate in describing more general situations in which the flow media is recognized to be substantially heterogeneous heath 1983 wainwright and mulligan 2004 harbaugh 2005 regarding the application of high performance computing to simulations of water and environment related phenomena several publications have shown the advantages of parallel computing frameworks le phong et al 2015 ji et al 2012 2014 zhou et al 2013 hwang et al 2014 nakajima 2013 rouholahnejad et al 2012 wu et al 2013 zhang et al 2013 the increased efficiency and reduced computational time of parallel frameworks allow for the simulation of physically based models over large areas with finer grid resolutions le phong et al 2015 nakajima 2013 ji et al 2012 2014 in addition the reduced computing time enables rapid modelling and analysis during emergency situations in the wake of environmental disasters rouholahnejad et al 2012 in this regard high performance computer models can play a fundamental role in reducing the response time following environmental disasters thus broadening the perspectives of achieving effective engineering solutions for emerging problems recent publications have shown that parallelized computer models are a feasible and affordable means of rapidly assessing engineering solutions as time is a major concern in the calibration of hydrologic models the parallelization of calibration algorithms in cpus has provided increased speedup and scalability rouholahnejad et al 2012 in this context gpu implementations are expected to provide even greater speedups rouholahnejad et al 2012 due to the increasing number of processors and the amount of memory available indeed in ref ji et al 2012 cudatm based solvers were applied to transient groundwater flow problems and achieved roughly a 4 fold speedup another noticeable fact of parallel implementations is the finer grained resolutions for computational mesh grids take for instance the parallel implementation of gcsflow gpu based conjunctive surface sub surface flow model which permitted the computation of surface and subsurface water flow for a domain with a topographic resolution of 1 2 m 1 2 m le phong et al 2015 another example of parallelization related to hydrologic models was the implementation of modflow mcdonald and harbaugh 2003 harbaugh 2005 in which gpu methods were shown to outperform multi cpu methods ji et al 2014 in this specific case the parallelized version of modflow was found to provide a 10 fold speedup in relation to the serial one ji et al 2014 this indicates that the application of gpgpu is a promising path to enhancing massive processing of scientific data in this paper we develop a gpgpu accelerated implementation of the groundwater flow in unconfined aquifers for anisotropic and heterogeneous media we generalize results previously published in the literature which consider the case of homogeneous and isotropic media and propose a massively parallel implementation that enables the solution of the groundwater flow problem for heterogeneous and anisotropic soils there are two main difficulties associated with the numerical solution of this model i the resulting equation is nonlinear as the hydraulic conductivity depends on the hydraulic head and ii due to the fact that the parameters are state dependent the matrix of the corresponding linear system has to be redefined at each time step which is time consuming these issues were solved by i considering a quasilinear approach in which the hydraulic conductivity variable parameter is one step behind the hydraulic conductivity and ii developing a dedicated kernel to redefine the matrix at each time step the model is discretized using the crank nicolson finite difference scheme crank and nicolson 1947 1996 and then solved using a cuda c c implementation based on cusp library bell and garland 2013 the main objective of this research is to provide an efficient and sufficiently general parallel implementation of the groundwater flow model as a means of testing the solution speedup we consider the groundwater flow problem for different grid resolutions running cuda c c parallel code and then its serial counterpart in c c the results show that the cuda c c parallel implementation achieves up to a 56 fold speedup in relation to the serial one the outline of the paper is as follows the materials and methods are presented in section 2 the gpgpu implementation performance results and application to a real basin are found in section 3 the results are discussed in section 4 and put into perspective with similar studies from the literature final remarks and future perspectives are dealt with in section 5 future perspectives 2 materials and methods in this section we present the model equations the discretization process and the solution method for the resulting linear system using cuda 2 1 the equations for groundwater flow in unconfined aquifers the groundwater flow can be modelled as the transient behavior of the hydraulic heads with this in mind ref boussinesq 1904 delleur 2006 fetter 2001 were studied in detail on the basis of the continuity equation and darcy s law considering the hydraulic conductivities k x x k y y k z z darcy s equation and the continuity equation for mass a 3d model for groundwater flow for the hydraulic head h can be written as 2 1 x ρ k x x h x y ρ k y y h y z ρ k z z h z v ρ q v s s h t v in which ρ is the fluid density v is the variation in volume q is associated with a sink source term and s s is the storage term given by s s ρ g α η β where g is gravity α is the compressibility of the geologic media η is the soil porosity and β is the water compressibility cleary 1989 in order to make the model tractable the following assumptions can be adopted the fluid is incompressible which means the fluid density is constant this is a reasonable assumption since the model is not dealing with coastal zones where there is a mix of fresh water and salt water within the soil layers nor groundwater contamination where certain contaminants are denser than the groundwater the soil is saturated this assumption is justified by the fact the model is represents the hydraulic head surface dynamics which takes place in saturated soil the flow is laminar within the range of validity of darcy s equation this assumption represents most of the groundwater flow although turbulent flow is observed in certain cases such as pumping in wells and very coarse grained soils it is not meant to be represented by the proposed model under these conditions eq 2 1 can be rewritten as 2 2 x k x x h x y k y y h y z k z z h z w x y z t s s h t where w x y z t is a source term dependent on time and position from eq 2 2 one can derive a 2d equation of groundwater flow by considering the integral over the vertical dimension z given as 2 3 h 0 b h d z 0 b d z 0 b h d z b which gives 2 4 x k x x b h x y k y y b h y k z z h z z b k z z h z z 0 w x y t s s b h t in confined aquifers b can be considered as a constant for each point of the grid however for unconfined aquifers the height of the saturated layer b varies as a function of the hydraulic head thus the resulting equation for groundwater flow in anisotropic and heterogeneous media in its 2d formulation can be written as 2 5 x t x x h x y t y y h y w x y t s h t where the parameters t x x k x x b t y y k y y b and s s s b in which t x x t y y are transmissivities in the directions x and y respectively s s is the specific storage and b is the height of the saturated layer while w x y t stands for the source sink term the source sink term w in the model describes the inflow or outflow of water in the control volume in our approach w is the outflow of groundwater to the surface which will start maintain the surface flow in this case w is a sink for the groundwater model and a source for the surface water model thus contributing to surface flow rate to model this process and for it to remain physically meaningful in the absence of the surface water model the following assumptions are made i groundwater outflow to the surface will become overland flow in a given cell if the hydraulic head in this cell is larger than the terrain elevation and ii the groundwater outflow equals to the volume of water computed by the model multiplied by the specific flow rate of the underground media s y this multiplication guarantees that the water outflow volume represents the volume that can be withdrawn from the underground media taking into account the capillary force using these considerations one can define the water outflow from the control volume as 2 6 i f h s i j h i j τ t h e n w i j τ 0 i f h s i j h i j τ t h e n w i j τ h s i j h i j τ where w i j τ 0 indicates the amount of water flowing out of the underground media and w i j τ 0 indicates that there is no outflow the height of the water column which will compose the baseflow is given as 2 7 h i j τ w i j τ s y the base flow is then given by 2 8 q c a l c c d r h i j τ δ x δ y δ t where c d r is the draining capacity of the boundary between the underground media and the surface δ x and δ y are the grid step sizes in the x y direction respectively and δ t is the time step as this formulation does not consider the surface flow dynamics it means that we are making the implicit assumption that the groundwater outflow will have enough time to reach the basin outlet 2 2 discretization of the model the major challenge in the modelling of unconfined aquifers is associated with the description of the moving physical boundary whereas the upper and lower boundaries do not change for confined aquifers which results in a fixed saturated layer height for each point of the mesh only the bottom boundary remains unchanged for unconfined aquifers the upper boundary is a function of the hydraulic head and as such varies over time and space besides the transmissivities are a function of the hydraulic head meaning that the problem becomes nonlinear and consequently affects the solution of the resulting equations in this case approximate solutions can be obtained by assuming that the height of the saturated layer in time τ 1 is a function of hydraulic head in time τ that is b τ 1 b h τ thereby the expression of the derivatives can be written as x k x x b h τ h τ 1 x under the assumption that the transmissivities do not change considerably from one time step to the next which is fine if the time steps are small i e there is very little variation in the hydraulic head from one time step to the other in this context the size of the time step gains importance since such variations have to be checked for consistency with the assumption from this formulation one obtains the linearized boussinesq equation chapman 1980 guo 1997 given by 2 9 x k x x b h h x y k y y b h h y w x y t s y h t where k x x k y y are hydraulic conductivities in the x y direction respectively h is the hydraulic head b h is the height of the saturated layer as a function of hydraulic head w is the source sink term and s y is the specific flow rate more commonly known as effective porosity fig 1 illustrates the relationship between topographic elevation h s the confining surface h b and the hydraulic head h on the basis of fig 1 one can define the height of the saturated layer in time τ as harbaugh 2005 essink 2000 2 10 b τ h τ h b the height of the saturated layer b is computed so that it satisfies eqs 2 11 2 13 which limit the saturated layer between the topographic elevation and the elevation of the confining surface harbaugh 2005 essink 2000 following this reasoning one obtains the following conditions 2 11 i f h s h τ h b t h e n b τ h τ h b 2 12 i f h s h τ t h e n b τ h s h b 2 13 i f h τ h b t h e n b τ 0 as previously mentioned in the case of groundwater flow in unconfined aquifers the physical boundary moves with time and space since it is a function of the hydraulic head in this case the height of the saturated layer must be calculated by means of eq 2 9 for each point in the mesh in each time step applying the approximation of derivatives by parts one obtains 1 x j t x x i j 1 2 τ h i j 1 τ 1 h i j τ 1 x j 1 2 t x x i j 1 2 τ h i j τ 1 h i j 1 τ 1 x j 1 2 1 y i t y y i 1 2 j τ h i 1 j τ 1 h i j τ 1 y i 1 2 t y y i 1 2 j τ h i j τ 1 h i 1 j τ 1 y i 1 2 2 14 w i j τ 1 s y i j τ t h i j τ 1 h i j τ where the transmissivities are defined as 2 15 t y y i 1 2 j τ k y y i 1 2 j b i 1 2 j τ 2 16 t y y i 1 2 j τ k y y i 1 2 j b i 1 2 j τ 2 17 t x x i j 1 2 τ k x x i j 1 2 b i j 1 2 τ 2 18 t x x i j 1 2 τ k x x i j 1 2 b i j 1 2 τ following the same procedure as before eq 2 14 can be rewritten in the simplified form 2 19 f i j τ h i j 1 τ 1 h i j τ 1 d i j τ h i j τ 1 h i j 1 τ 1 m i j τ h i 1 j τ 1 h i j τ 1 b i j τ h i j τ 1 h i 1 j τ 1 w i j t s y i j t h i j τ 1 h i j τ where w i j τ is given by 2 20 w i j τ β h i j τ c t o p i j and the further parameters are defined as follows 2 21 b i j τ 2 t y y i j τ t y y i 1 j τ t y y i j τ y i 1 t y y i 1 j τ y i y i 2 22 d i j τ 2 t x x i j τ t x x i j 1 τ t x x i j τ x j 1 t x x i j 1 τ x j x j 2 23 f i j τ 2 t x x i j τ t x x i j 1 τ t x x i j τ x j 1 t x x i j 1 τ x j x j 2 24 m i j τ 2 t y y i j τ t y y i 1 j τ t y y i j τ y i 1 t y y i 1 j τ y i y i by grouping like terms from eq 2 19 and adapting the resulting equation to the crank nicolson scheme crank and nicolson 1947 1996 the following implicit solution equation is obtained 2 25 1 λ i j f i j τ d i j τ m i j τ b i j τ h i j τ 1 λ i j f i j τ h i j 1 τ 1 d i j τ h i j 1 τ 1 m i j τ h i 1 j τ 1 b i j τ h i 1 j τ 1 w i j τ 1 λ i j f i j τ d i j τ m i j τ b i j τ h i j τ λ i j f i j τ h i j 1 τ d i j τ h i j 1 τ m i j τ h i 1 j τ b i j τ h i 1 j τ w i j τ where 2 26 λ i j t 2 s y i j it should be rememberedl that the matrix structures can be rewritten as 2 27 a τ h τ 1 δ τ h τ w τ and an explicit expression for the hydraulic heads in time τ 1 is given by eq 2 28 which reads 2 28 h τ 1 a τ 1 δ τ h τ w τ remember also that the conditions under which the problem is formulated guarantee that the linear system in eq 2 27 has a unique solution finally eq 2 28 represents the implicit form of the solution of eq 2 9 the above formulation can be applied in the study of groundwater flow in unconfined aquifers in heterogeneous and anisotropic media the solution requires that the matrix a τ is mounted at every time step since it is dependent on the system s unknowns 2 3 solution of the system cuda c c implementation the code for the formulation and solution of the equations was developed in visual studio community 2013 microsoft visual studio team 2013 with cuda toolkit 8 0 nvidia developer zone 2017 and the cusp library bell and garland 2013 the details of the implementation are given in the following subsections 2 3 1 generation and updating of time dependent system matrix and vectors recall from the discretization process that the terms a τ δ τ w τ from the linear system in eq 2 27 must be redefined at each time step due to the dependence of transmissivities on the hydraulic head h τ thus before the linear system in eq 2 27 is solved within a time step its terms must be updated in other words the matrix a τ and the vectors δ τ w τ must be mounted at each time step since this is massively repetitive task a cuda kernel was developed to deal with the calculation of each nonzero entry of the matrix and vectors the kernel is made available for download along with the solution code environmental modelling at uffs 2017 2 3 2 solving the linear system the linear system was solved iteratively at each time step by means of the biconjugate gradient stabilized method bi cgstab van dervorst and dekker 1988 the advantages that bi cgstab offers as an iterative solver are the reduced computational effort and memory requirements van dervorst and dekker 1988 we applied the implementation of bi cgstab available in the cusp library for cuda bell and garland 2013 2 3 3 description of the hardware equipment the solution was performed using an intel i 7 t m 7700 k cpu with 16 g b ram equipped with a nvidia geforce gtx 1060 gpu card 3 results the performance comparison between pipeline and parallel implementations of the groundwater flow model are presented in this section in addition as a demonstrative example an application of the model in the study of the behavior of the variation of soil saturation during an extended drought is also presented 3 1 performance of the gpgpu accelerated implementation the details of the performance of the gpgpu accelerated implementation are presented in fig 2 and table 1 the results show that the gpgpu accelerated solution yielded up to a 56 fold speedup in relation to the solution provided by a conventional serial code the considerable speedup can be attributed to the parallelization of the mounting of the matrices and vectors of the linear system at each time step it was observed that this process is quite time consuming in the serial version of the code rememberl that the hydraulic transmissivities are dependent on the hydraulic head so the system matrix and vectors a τ δ τ w τ must be redefined at each time step τ 3 2 application evaluation of the behavior of the basin during a drought the relationship between the variations on the level of the unconfined aquifer and the fluctuations in the water levels in water bodies is an important matter in hydrology it has important consequences upon the analysis of the potential of storage and transport in aquifers guo 1997 such analyses can support the planning of water supply systems and guide the installation of irrigation systems furthermore they allow water supply managers to estimate the optimal time to adopt measures and strategies to control or restrict the use of water the idea behind the simulation of a drought is to evaluate the level of the unconfined aquifer during the time when there are no inputs rain this is one very common application of groundwater flow models as applied to the management of catchment basins by means of this analysis the quantity of water and the time horizon in which the required output flow will occur can be estimated thus enhancing the technical basis for decision making in this section the gpgpu accelerated implementation of the groundwater flow model for unconfined aquifers is calibrated validated and applied to the study of the apuaê basin in brazil the objective is to show that such a study can be straightforwardly conducted by means of the cuda c c implementation presented in this paper the hardware requirement is an ordinary office computer equipped with an nvidia gpu card and the cuda toolkit 8 0 or later versions 56 as described in section 2 3 2 1 characterization of the basin the study basin is delineated assuming its outlet at the river level gauge passo colombelli 27 o 33 43 s 51 o 51 28 w which is maintained by the water resources agency ana this outlet location defines a contribution area of 3 660 k m 2 since the main river in the basin is called the apuaê river we have defined herein the basin name to be the apuaê basin the apuaê basin is part of the uruguay basin an important basin for the generation of hydroelectric power and which is located in the countries of brazil argentina and uruguay in brazil the uruguay basin is located in the states of santa catarina and rio grande do sul fig 3 depicts the location of the apuaê basin in the brazilian part of the uruguay basin as well its elevation elevations in the basin range from 387 m to 929 m above sea level slopes are distributed according to the following percentages of the basin area 67 72 0 o 22 o 26 34 22 o 44 o and 5 94 45 o 66 o the valleys are v shaped with hillslopes heights ranging from 100 m to 300 m decian 2012 the spatial distribution of soil depths in the basin were estimated by means of the well depths database 72 wells acquired from the groundwater system information cprm 2017 and the methodology presented in ref saulnier et al 1997 thus soil depths in the basin range from 8 to 230 m there are three classes of soil in the basin 1 cambisol soils middle northern to northeastern part which are characterized by an inexpressive b horizon differentiation in the soil profile and are mainly agricultural areas 2 latosol soils which cover the majority of the basin northwestern and middle southern parts this class of soil has no distinct horizons and its color varies from red to yellowish red these soils constitute the largest class of territorial expression and agricultural potential of the country being used for various crops reforestation and pasture ker 1998 and 3 nitosol class southern basin border present clayey to very clayey soils with slight increase of clay in the subsurface horizon muniz et al 2011 the apuaê river is 208 km in length with its riverhead and outlet at elevations of 812 m and 387 m respectively the average discharge recorded 1939 2014 at the basin outlet is 99 65 m 3 s 1 the average annual precipitation throughout the basin is around 1760 mm 1954 2014 the rainy season takes place during the months of september october and november and the dry season during the months of march april and may the runnoff coefficient is 0 48 i e about 52 of the total precipitation throughout the basin is lost to the atmosphere due to the evapotranspiration process this ratio is consistent with data from non urbanized basins the average maximum and minimum temperatures throughout the basin are 17 5 o c 23 6 o c and 13 2 o c respectively the apuaê basin is located within a region of intense agricultural activity based the cultivation of soybean and corn during the summer months and wheat and oat crops during the winter months only 36 of the total area is covered by forests see fig 3 this fact raises concern for water resources management as the water demand for irrigation and water supply increases 3 2 2 calibration and validation of the model the calibration of the model considered the case of heterogeneous and anisotropic media to this end the soil hydraulic conductivities for the basin were considered to be dependent upon land cover the land cover was classified into two categories forest and non forest on the basis of previous studies that concluded that hydraulic conductivities have higher values in soils covered by forest newman 1976 le maitre et al 2000 ridolfi et al 2006 the water outflow from each cell from each time step is summed up and represents the total subsurface flow at the basin outlet this approach disregards the overland flow travel time in drainage channels therefore implying it is shorter than the data time step day thereafter the calculated subsurface flow is compared with the master recession curve the mean value of hydraulic conductivity in the case of homogeneous and isotropic media was applied to establish the intervals of calibration for the heterogeneous and anisotropic case the upper and lower bounds for the values of hydraulic conductivity are presented in table 2 the results of the calibration process are shown in the subsequent figures fig 4 presents the master recession curve for the basin in red and was obtained by applying the matching strip method lamb and beven 1997 sujono et al 2002 to 29 sets of observed data describing recession events for the basin in blue fig 5 shows the evolution of relative calibration errors table 3 shows the values of the model parameters obtained from the calibration process the parameter values presented in table 3 correspond to the couple of values k f k n f that minimize the distance between the vector of actual discharge and simulated discharge in the drainage basin outlet the values in table 3 were found to be consistent with characteristic values of hydraulic conductivity from experimental studies that considered the same type of soil and land cover embrapa and ibge 2011 gonçalves and libardi 2013 3 2 3 simulation of an 80 day drought the application of the gpgpu accelerated groundwater flow model for unconfined aquifers is presented in this section we simulate an 80 day drought period to evaluate the behavior of the aquifer in the area covered by the basin and its impact upon the basin flow fig 6 presents the variation of soil saturation expressed by percentages of the basin area on the topographic surface as a function of time more intense variation in the soil saturation on the surface is verified during the first 30 days of simulation this behavior reflects the dependence of the hydraulic conductivity on the thickness of the saturated layer b whose value is higher at first and decreases as the aquifer level lowers the reduction in hydraulic transmissivities in the saturated layer causes a decrease in the value of base flow whose rate is determined by the recession coefficient r the exponential behavior of the discharge can be verified in fig 4 fig 6 provides a means for the physical interpretation of the process that generates the base flow it can be noted that the lower portions of the basin topography receive an influx of water from upstream as the soil saturation level is achieved the water flows to the drainage channels of the basin thus generating base flow due to the continuous flow of underground water to the drainage channels the thickness of the saturated layer decreases as a consequence of the dependence of transmissivities upon the thickness of the saturated layer the depth of the aquifer increases faster in regions where the saturated layer is thicker as the process evolves the water flows downstream until the depth of the aquifer reaches the depth of the confining surface at such point the groundwater flow to the river ceases this description of the behavior of the results based on the simulations agree with the physics of the problem this indicates that the model captures the essential features of the phenomenon under study 4 discussion the gpgpu accelerated implementation presented in this paper was found to be able to deliver the transient solution for a problem with 5 million unknowns in roughly 4 min as shown in the results section this result along with those cited from the literature is encouraging as one takes into consideration the increasing demands of scientific computing regarding the solution procedure the considerable speedup in the parallel code in relation to the serial one can be assigned to the large number of calculations needed to mount the linear system matrix and vectors at each time step the mounting of the system matrix and vectors is required as the groundwater model for unconfined aquifers means the hydraulic transmissivities are dependent on the thickness of the saturated layer which changes at each time step in the transient model in fact it was found that the mounting of the system is the most time consuming task in the overall solution process this is an important point to take into account in the design of the solution flowchart which should consider the parts of the code to be executed in parallel kernels regarding the solution of the groundwater flow model we note that the relationship between the variations in the depth of aquifers and the fluctuations in the flow rate of water in rivers is an important hydrological issue such relationships have significant importance in the analysis of water storage and transport potential in aquifers as pointed out in ref guo 1997 such analysis can provide information for the planning of water supply facilities and guidelines for the installation and use of irrigation systems in a broader sense they can provide fast and sufficiently accurate scenarios regarding the availability of water in a supply system furthermore they can guide projections and strategies regarding the most favourable time for water authorities to resort to rationing periods the simulation of groundwater flow for apuaê basin brazil by means of the gpgpu accelerated groundwater flow model permitted the assessment of important aspects of its dynamics while also serving as an illustration of the application of the computational model in this context a key point to note is that the transient solution of the groundwater model with over 5 million points was obtained by means of an inexpensive office computer and thus can be largely replicated 5 final remarks this study addressed the feasibility of the application of gpgpu to the problem of groundwater flow in unconfined aquifers for anisotropic and heterogeneous media the results agree with previously published papers in the sense that the parallel implementation largely outperforms the serial one thus enhancing the computational performance and reducing simulation time it was found that the cuda implementation of the groundwater flow equations was capable of outperforming the serial implementation and providing up to a 56 fold reduction in computation time with regard to future perspectives we would like to point out that it is likely that an increasing number of environmental models will have their parallelized version developed and implemented in order to check further advantages and drawbacks of the gpgpu application in the modelling of environmental phenomena the code for the computational model developed in this research can be obtained at http modelagemambientaluffs blogspot com br environmental modelling at uffs 2017 acknowledgements the authors would like to thank cnpq for its support under the grant no 460329 2014 6 
26434,the application of computer simulation models plays a significant role in the understanding of water dynamics in basins the recent and explosive growth of the processing capabilities of general purpose graphics processing units gpgpus has resulted in widespread interest in parallel computing from the modelling community in this paper we present a gpgpu implementation of finite differences solution of the equations of the 2d groundwater flow in unconfined aquifers for heterogeneous and anisotropic media we show that the gpgpu accelerated solution implemented using cuda 1 c c largely outperforms the corresponding serial solution in c c the results show that the gpgpu accelerated implementation is capable of providing up to a 56 fold speedup in the solution using an ordinary office computer equipped with an inexpensive gpu 2 card the code developed for this research is available for download and use at http modelagemambientaluffs blogspot com br keywords groundwater modelling unconfined aquifers heterogeneous and anisotropic media high performance computing cuda availability program name pargw developer tomas carlotto contact address thomas carl hotmail com year first available 2017 software required cuda toolkit 8 0 or later and cusp library program language cuda c package size 179 mb availability http modelagemambientaluffs blogspot com br cost free of charge 1 introduction spatially distributed models are widely recognized as important tools for the understanding of groundwater dynamics markstrom et al 2008 rouholahnejad et al 2012 zhang et al 2013 hwang et al 2014 le phong et al 2015 they are widely applicable in environmental analysis of flooded areas pollutant contamination of aquifers wells and reservoirs botros et al 2012 czarnecki et al 2010 allan freeze and witherspoon 1966 he et al 2011 hwang et al 2014 lamb and beven 1997 lange et al 2014 le phong et al 2015 lyne and hollick 1979 miller et al 2013 markstrom et al 2008 mendoza and martins 2006 voeckler et al 2014 the implementation of computer models has benefited from the rapid growth in computational capabilities observed over the last thirty years or so lange et al 2014 markstrom et al 2008 more recently the rapid emergence of parallel computing platforms based on gpgpu general purpose graphics processing units has provided an entirely new perspective regarding the processing capabilities of personal computers thus attracting the attention of the modelling community nickolls et al 2008 singh et al 2011 ji et al 2012 2014 le phong et al 2015 zhou et al 2013 in this context hydrological models are particularly suitable for massively parallel frameworks due to the state of the art of solution techniques for sparse linear systems via domain decomposition methods barrett et al 1994 formaggia et al 2006 the assessment of groundwater and surface water dynamics in real basins is a challenging issue which requires knowledge of several factors that influence the hydrological cycle such as the physical and chemical characteristics of the soil and of the environment as a whole a number of studies have been devoted to the development of hydrologic models whose aim is to describe groundwater flow while considering different levels of soil saturation and diverse geologic conditions freeze 1971 lange et al 2014 voeckler et al 2014 among the variety of groundwater models in the literature one can find models that describe groundwater flow in basins graaf et al 2014 czarnecki et al 2010 freeze 1971 marc et al 1998 most commonly groundwater models are designed to deal with the case of isotropic and homogeneous media harbaugh 2005 markstrom et al 2008 lange et al 2014 voeckler et al 2014 czarnecki et al 2010 although the assumptions on homogeneity and isotropy permit the simplification of the resulting equations and their solution they might not be adequate in describing more general situations in which the flow media is recognized to be substantially heterogeneous heath 1983 wainwright and mulligan 2004 harbaugh 2005 regarding the application of high performance computing to simulations of water and environment related phenomena several publications have shown the advantages of parallel computing frameworks le phong et al 2015 ji et al 2012 2014 zhou et al 2013 hwang et al 2014 nakajima 2013 rouholahnejad et al 2012 wu et al 2013 zhang et al 2013 the increased efficiency and reduced computational time of parallel frameworks allow for the simulation of physically based models over large areas with finer grid resolutions le phong et al 2015 nakajima 2013 ji et al 2012 2014 in addition the reduced computing time enables rapid modelling and analysis during emergency situations in the wake of environmental disasters rouholahnejad et al 2012 in this regard high performance computer models can play a fundamental role in reducing the response time following environmental disasters thus broadening the perspectives of achieving effective engineering solutions for emerging problems recent publications have shown that parallelized computer models are a feasible and affordable means of rapidly assessing engineering solutions as time is a major concern in the calibration of hydrologic models the parallelization of calibration algorithms in cpus has provided increased speedup and scalability rouholahnejad et al 2012 in this context gpu implementations are expected to provide even greater speedups rouholahnejad et al 2012 due to the increasing number of processors and the amount of memory available indeed in ref ji et al 2012 cudatm based solvers were applied to transient groundwater flow problems and achieved roughly a 4 fold speedup another noticeable fact of parallel implementations is the finer grained resolutions for computational mesh grids take for instance the parallel implementation of gcsflow gpu based conjunctive surface sub surface flow model which permitted the computation of surface and subsurface water flow for a domain with a topographic resolution of 1 2 m 1 2 m le phong et al 2015 another example of parallelization related to hydrologic models was the implementation of modflow mcdonald and harbaugh 2003 harbaugh 2005 in which gpu methods were shown to outperform multi cpu methods ji et al 2014 in this specific case the parallelized version of modflow was found to provide a 10 fold speedup in relation to the serial one ji et al 2014 this indicates that the application of gpgpu is a promising path to enhancing massive processing of scientific data in this paper we develop a gpgpu accelerated implementation of the groundwater flow in unconfined aquifers for anisotropic and heterogeneous media we generalize results previously published in the literature which consider the case of homogeneous and isotropic media and propose a massively parallel implementation that enables the solution of the groundwater flow problem for heterogeneous and anisotropic soils there are two main difficulties associated with the numerical solution of this model i the resulting equation is nonlinear as the hydraulic conductivity depends on the hydraulic head and ii due to the fact that the parameters are state dependent the matrix of the corresponding linear system has to be redefined at each time step which is time consuming these issues were solved by i considering a quasilinear approach in which the hydraulic conductivity variable parameter is one step behind the hydraulic conductivity and ii developing a dedicated kernel to redefine the matrix at each time step the model is discretized using the crank nicolson finite difference scheme crank and nicolson 1947 1996 and then solved using a cuda c c implementation based on cusp library bell and garland 2013 the main objective of this research is to provide an efficient and sufficiently general parallel implementation of the groundwater flow model as a means of testing the solution speedup we consider the groundwater flow problem for different grid resolutions running cuda c c parallel code and then its serial counterpart in c c the results show that the cuda c c parallel implementation achieves up to a 56 fold speedup in relation to the serial one the outline of the paper is as follows the materials and methods are presented in section 2 the gpgpu implementation performance results and application to a real basin are found in section 3 the results are discussed in section 4 and put into perspective with similar studies from the literature final remarks and future perspectives are dealt with in section 5 future perspectives 2 materials and methods in this section we present the model equations the discretization process and the solution method for the resulting linear system using cuda 2 1 the equations for groundwater flow in unconfined aquifers the groundwater flow can be modelled as the transient behavior of the hydraulic heads with this in mind ref boussinesq 1904 delleur 2006 fetter 2001 were studied in detail on the basis of the continuity equation and darcy s law considering the hydraulic conductivities k x x k y y k z z darcy s equation and the continuity equation for mass a 3d model for groundwater flow for the hydraulic head h can be written as 2 1 x ρ k x x h x y ρ k y y h y z ρ k z z h z v ρ q v s s h t v in which ρ is the fluid density v is the variation in volume q is associated with a sink source term and s s is the storage term given by s s ρ g α η β where g is gravity α is the compressibility of the geologic media η is the soil porosity and β is the water compressibility cleary 1989 in order to make the model tractable the following assumptions can be adopted the fluid is incompressible which means the fluid density is constant this is a reasonable assumption since the model is not dealing with coastal zones where there is a mix of fresh water and salt water within the soil layers nor groundwater contamination where certain contaminants are denser than the groundwater the soil is saturated this assumption is justified by the fact the model is represents the hydraulic head surface dynamics which takes place in saturated soil the flow is laminar within the range of validity of darcy s equation this assumption represents most of the groundwater flow although turbulent flow is observed in certain cases such as pumping in wells and very coarse grained soils it is not meant to be represented by the proposed model under these conditions eq 2 1 can be rewritten as 2 2 x k x x h x y k y y h y z k z z h z w x y z t s s h t where w x y z t is a source term dependent on time and position from eq 2 2 one can derive a 2d equation of groundwater flow by considering the integral over the vertical dimension z given as 2 3 h 0 b h d z 0 b d z 0 b h d z b which gives 2 4 x k x x b h x y k y y b h y k z z h z z b k z z h z z 0 w x y t s s b h t in confined aquifers b can be considered as a constant for each point of the grid however for unconfined aquifers the height of the saturated layer b varies as a function of the hydraulic head thus the resulting equation for groundwater flow in anisotropic and heterogeneous media in its 2d formulation can be written as 2 5 x t x x h x y t y y h y w x y t s h t where the parameters t x x k x x b t y y k y y b and s s s b in which t x x t y y are transmissivities in the directions x and y respectively s s is the specific storage and b is the height of the saturated layer while w x y t stands for the source sink term the source sink term w in the model describes the inflow or outflow of water in the control volume in our approach w is the outflow of groundwater to the surface which will start maintain the surface flow in this case w is a sink for the groundwater model and a source for the surface water model thus contributing to surface flow rate to model this process and for it to remain physically meaningful in the absence of the surface water model the following assumptions are made i groundwater outflow to the surface will become overland flow in a given cell if the hydraulic head in this cell is larger than the terrain elevation and ii the groundwater outflow equals to the volume of water computed by the model multiplied by the specific flow rate of the underground media s y this multiplication guarantees that the water outflow volume represents the volume that can be withdrawn from the underground media taking into account the capillary force using these considerations one can define the water outflow from the control volume as 2 6 i f h s i j h i j τ t h e n w i j τ 0 i f h s i j h i j τ t h e n w i j τ h s i j h i j τ where w i j τ 0 indicates the amount of water flowing out of the underground media and w i j τ 0 indicates that there is no outflow the height of the water column which will compose the baseflow is given as 2 7 h i j τ w i j τ s y the base flow is then given by 2 8 q c a l c c d r h i j τ δ x δ y δ t where c d r is the draining capacity of the boundary between the underground media and the surface δ x and δ y are the grid step sizes in the x y direction respectively and δ t is the time step as this formulation does not consider the surface flow dynamics it means that we are making the implicit assumption that the groundwater outflow will have enough time to reach the basin outlet 2 2 discretization of the model the major challenge in the modelling of unconfined aquifers is associated with the description of the moving physical boundary whereas the upper and lower boundaries do not change for confined aquifers which results in a fixed saturated layer height for each point of the mesh only the bottom boundary remains unchanged for unconfined aquifers the upper boundary is a function of the hydraulic head and as such varies over time and space besides the transmissivities are a function of the hydraulic head meaning that the problem becomes nonlinear and consequently affects the solution of the resulting equations in this case approximate solutions can be obtained by assuming that the height of the saturated layer in time τ 1 is a function of hydraulic head in time τ that is b τ 1 b h τ thereby the expression of the derivatives can be written as x k x x b h τ h τ 1 x under the assumption that the transmissivities do not change considerably from one time step to the next which is fine if the time steps are small i e there is very little variation in the hydraulic head from one time step to the other in this context the size of the time step gains importance since such variations have to be checked for consistency with the assumption from this formulation one obtains the linearized boussinesq equation chapman 1980 guo 1997 given by 2 9 x k x x b h h x y k y y b h h y w x y t s y h t where k x x k y y are hydraulic conductivities in the x y direction respectively h is the hydraulic head b h is the height of the saturated layer as a function of hydraulic head w is the source sink term and s y is the specific flow rate more commonly known as effective porosity fig 1 illustrates the relationship between topographic elevation h s the confining surface h b and the hydraulic head h on the basis of fig 1 one can define the height of the saturated layer in time τ as harbaugh 2005 essink 2000 2 10 b τ h τ h b the height of the saturated layer b is computed so that it satisfies eqs 2 11 2 13 which limit the saturated layer between the topographic elevation and the elevation of the confining surface harbaugh 2005 essink 2000 following this reasoning one obtains the following conditions 2 11 i f h s h τ h b t h e n b τ h τ h b 2 12 i f h s h τ t h e n b τ h s h b 2 13 i f h τ h b t h e n b τ 0 as previously mentioned in the case of groundwater flow in unconfined aquifers the physical boundary moves with time and space since it is a function of the hydraulic head in this case the height of the saturated layer must be calculated by means of eq 2 9 for each point in the mesh in each time step applying the approximation of derivatives by parts one obtains 1 x j t x x i j 1 2 τ h i j 1 τ 1 h i j τ 1 x j 1 2 t x x i j 1 2 τ h i j τ 1 h i j 1 τ 1 x j 1 2 1 y i t y y i 1 2 j τ h i 1 j τ 1 h i j τ 1 y i 1 2 t y y i 1 2 j τ h i j τ 1 h i 1 j τ 1 y i 1 2 2 14 w i j τ 1 s y i j τ t h i j τ 1 h i j τ where the transmissivities are defined as 2 15 t y y i 1 2 j τ k y y i 1 2 j b i 1 2 j τ 2 16 t y y i 1 2 j τ k y y i 1 2 j b i 1 2 j τ 2 17 t x x i j 1 2 τ k x x i j 1 2 b i j 1 2 τ 2 18 t x x i j 1 2 τ k x x i j 1 2 b i j 1 2 τ following the same procedure as before eq 2 14 can be rewritten in the simplified form 2 19 f i j τ h i j 1 τ 1 h i j τ 1 d i j τ h i j τ 1 h i j 1 τ 1 m i j τ h i 1 j τ 1 h i j τ 1 b i j τ h i j τ 1 h i 1 j τ 1 w i j t s y i j t h i j τ 1 h i j τ where w i j τ is given by 2 20 w i j τ β h i j τ c t o p i j and the further parameters are defined as follows 2 21 b i j τ 2 t y y i j τ t y y i 1 j τ t y y i j τ y i 1 t y y i 1 j τ y i y i 2 22 d i j τ 2 t x x i j τ t x x i j 1 τ t x x i j τ x j 1 t x x i j 1 τ x j x j 2 23 f i j τ 2 t x x i j τ t x x i j 1 τ t x x i j τ x j 1 t x x i j 1 τ x j x j 2 24 m i j τ 2 t y y i j τ t y y i 1 j τ t y y i j τ y i 1 t y y i 1 j τ y i y i by grouping like terms from eq 2 19 and adapting the resulting equation to the crank nicolson scheme crank and nicolson 1947 1996 the following implicit solution equation is obtained 2 25 1 λ i j f i j τ d i j τ m i j τ b i j τ h i j τ 1 λ i j f i j τ h i j 1 τ 1 d i j τ h i j 1 τ 1 m i j τ h i 1 j τ 1 b i j τ h i 1 j τ 1 w i j τ 1 λ i j f i j τ d i j τ m i j τ b i j τ h i j τ λ i j f i j τ h i j 1 τ d i j τ h i j 1 τ m i j τ h i 1 j τ b i j τ h i 1 j τ w i j τ where 2 26 λ i j t 2 s y i j it should be rememberedl that the matrix structures can be rewritten as 2 27 a τ h τ 1 δ τ h τ w τ and an explicit expression for the hydraulic heads in time τ 1 is given by eq 2 28 which reads 2 28 h τ 1 a τ 1 δ τ h τ w τ remember also that the conditions under which the problem is formulated guarantee that the linear system in eq 2 27 has a unique solution finally eq 2 28 represents the implicit form of the solution of eq 2 9 the above formulation can be applied in the study of groundwater flow in unconfined aquifers in heterogeneous and anisotropic media the solution requires that the matrix a τ is mounted at every time step since it is dependent on the system s unknowns 2 3 solution of the system cuda c c implementation the code for the formulation and solution of the equations was developed in visual studio community 2013 microsoft visual studio team 2013 with cuda toolkit 8 0 nvidia developer zone 2017 and the cusp library bell and garland 2013 the details of the implementation are given in the following subsections 2 3 1 generation and updating of time dependent system matrix and vectors recall from the discretization process that the terms a τ δ τ w τ from the linear system in eq 2 27 must be redefined at each time step due to the dependence of transmissivities on the hydraulic head h τ thus before the linear system in eq 2 27 is solved within a time step its terms must be updated in other words the matrix a τ and the vectors δ τ w τ must be mounted at each time step since this is massively repetitive task a cuda kernel was developed to deal with the calculation of each nonzero entry of the matrix and vectors the kernel is made available for download along with the solution code environmental modelling at uffs 2017 2 3 2 solving the linear system the linear system was solved iteratively at each time step by means of the biconjugate gradient stabilized method bi cgstab van dervorst and dekker 1988 the advantages that bi cgstab offers as an iterative solver are the reduced computational effort and memory requirements van dervorst and dekker 1988 we applied the implementation of bi cgstab available in the cusp library for cuda bell and garland 2013 2 3 3 description of the hardware equipment the solution was performed using an intel i 7 t m 7700 k cpu with 16 g b ram equipped with a nvidia geforce gtx 1060 gpu card 3 results the performance comparison between pipeline and parallel implementations of the groundwater flow model are presented in this section in addition as a demonstrative example an application of the model in the study of the behavior of the variation of soil saturation during an extended drought is also presented 3 1 performance of the gpgpu accelerated implementation the details of the performance of the gpgpu accelerated implementation are presented in fig 2 and table 1 the results show that the gpgpu accelerated solution yielded up to a 56 fold speedup in relation to the solution provided by a conventional serial code the considerable speedup can be attributed to the parallelization of the mounting of the matrices and vectors of the linear system at each time step it was observed that this process is quite time consuming in the serial version of the code rememberl that the hydraulic transmissivities are dependent on the hydraulic head so the system matrix and vectors a τ δ τ w τ must be redefined at each time step τ 3 2 application evaluation of the behavior of the basin during a drought the relationship between the variations on the level of the unconfined aquifer and the fluctuations in the water levels in water bodies is an important matter in hydrology it has important consequences upon the analysis of the potential of storage and transport in aquifers guo 1997 such analyses can support the planning of water supply systems and guide the installation of irrigation systems furthermore they allow water supply managers to estimate the optimal time to adopt measures and strategies to control or restrict the use of water the idea behind the simulation of a drought is to evaluate the level of the unconfined aquifer during the time when there are no inputs rain this is one very common application of groundwater flow models as applied to the management of catchment basins by means of this analysis the quantity of water and the time horizon in which the required output flow will occur can be estimated thus enhancing the technical basis for decision making in this section the gpgpu accelerated implementation of the groundwater flow model for unconfined aquifers is calibrated validated and applied to the study of the apuaê basin in brazil the objective is to show that such a study can be straightforwardly conducted by means of the cuda c c implementation presented in this paper the hardware requirement is an ordinary office computer equipped with an nvidia gpu card and the cuda toolkit 8 0 or later versions 56 as described in section 2 3 2 1 characterization of the basin the study basin is delineated assuming its outlet at the river level gauge passo colombelli 27 o 33 43 s 51 o 51 28 w which is maintained by the water resources agency ana this outlet location defines a contribution area of 3 660 k m 2 since the main river in the basin is called the apuaê river we have defined herein the basin name to be the apuaê basin the apuaê basin is part of the uruguay basin an important basin for the generation of hydroelectric power and which is located in the countries of brazil argentina and uruguay in brazil the uruguay basin is located in the states of santa catarina and rio grande do sul fig 3 depicts the location of the apuaê basin in the brazilian part of the uruguay basin as well its elevation elevations in the basin range from 387 m to 929 m above sea level slopes are distributed according to the following percentages of the basin area 67 72 0 o 22 o 26 34 22 o 44 o and 5 94 45 o 66 o the valleys are v shaped with hillslopes heights ranging from 100 m to 300 m decian 2012 the spatial distribution of soil depths in the basin were estimated by means of the well depths database 72 wells acquired from the groundwater system information cprm 2017 and the methodology presented in ref saulnier et al 1997 thus soil depths in the basin range from 8 to 230 m there are three classes of soil in the basin 1 cambisol soils middle northern to northeastern part which are characterized by an inexpressive b horizon differentiation in the soil profile and are mainly agricultural areas 2 latosol soils which cover the majority of the basin northwestern and middle southern parts this class of soil has no distinct horizons and its color varies from red to yellowish red these soils constitute the largest class of territorial expression and agricultural potential of the country being used for various crops reforestation and pasture ker 1998 and 3 nitosol class southern basin border present clayey to very clayey soils with slight increase of clay in the subsurface horizon muniz et al 2011 the apuaê river is 208 km in length with its riverhead and outlet at elevations of 812 m and 387 m respectively the average discharge recorded 1939 2014 at the basin outlet is 99 65 m 3 s 1 the average annual precipitation throughout the basin is around 1760 mm 1954 2014 the rainy season takes place during the months of september october and november and the dry season during the months of march april and may the runnoff coefficient is 0 48 i e about 52 of the total precipitation throughout the basin is lost to the atmosphere due to the evapotranspiration process this ratio is consistent with data from non urbanized basins the average maximum and minimum temperatures throughout the basin are 17 5 o c 23 6 o c and 13 2 o c respectively the apuaê basin is located within a region of intense agricultural activity based the cultivation of soybean and corn during the summer months and wheat and oat crops during the winter months only 36 of the total area is covered by forests see fig 3 this fact raises concern for water resources management as the water demand for irrigation and water supply increases 3 2 2 calibration and validation of the model the calibration of the model considered the case of heterogeneous and anisotropic media to this end the soil hydraulic conductivities for the basin were considered to be dependent upon land cover the land cover was classified into two categories forest and non forest on the basis of previous studies that concluded that hydraulic conductivities have higher values in soils covered by forest newman 1976 le maitre et al 2000 ridolfi et al 2006 the water outflow from each cell from each time step is summed up and represents the total subsurface flow at the basin outlet this approach disregards the overland flow travel time in drainage channels therefore implying it is shorter than the data time step day thereafter the calculated subsurface flow is compared with the master recession curve the mean value of hydraulic conductivity in the case of homogeneous and isotropic media was applied to establish the intervals of calibration for the heterogeneous and anisotropic case the upper and lower bounds for the values of hydraulic conductivity are presented in table 2 the results of the calibration process are shown in the subsequent figures fig 4 presents the master recession curve for the basin in red and was obtained by applying the matching strip method lamb and beven 1997 sujono et al 2002 to 29 sets of observed data describing recession events for the basin in blue fig 5 shows the evolution of relative calibration errors table 3 shows the values of the model parameters obtained from the calibration process the parameter values presented in table 3 correspond to the couple of values k f k n f that minimize the distance between the vector of actual discharge and simulated discharge in the drainage basin outlet the values in table 3 were found to be consistent with characteristic values of hydraulic conductivity from experimental studies that considered the same type of soil and land cover embrapa and ibge 2011 gonçalves and libardi 2013 3 2 3 simulation of an 80 day drought the application of the gpgpu accelerated groundwater flow model for unconfined aquifers is presented in this section we simulate an 80 day drought period to evaluate the behavior of the aquifer in the area covered by the basin and its impact upon the basin flow fig 6 presents the variation of soil saturation expressed by percentages of the basin area on the topographic surface as a function of time more intense variation in the soil saturation on the surface is verified during the first 30 days of simulation this behavior reflects the dependence of the hydraulic conductivity on the thickness of the saturated layer b whose value is higher at first and decreases as the aquifer level lowers the reduction in hydraulic transmissivities in the saturated layer causes a decrease in the value of base flow whose rate is determined by the recession coefficient r the exponential behavior of the discharge can be verified in fig 4 fig 6 provides a means for the physical interpretation of the process that generates the base flow it can be noted that the lower portions of the basin topography receive an influx of water from upstream as the soil saturation level is achieved the water flows to the drainage channels of the basin thus generating base flow due to the continuous flow of underground water to the drainage channels the thickness of the saturated layer decreases as a consequence of the dependence of transmissivities upon the thickness of the saturated layer the depth of the aquifer increases faster in regions where the saturated layer is thicker as the process evolves the water flows downstream until the depth of the aquifer reaches the depth of the confining surface at such point the groundwater flow to the river ceases this description of the behavior of the results based on the simulations agree with the physics of the problem this indicates that the model captures the essential features of the phenomenon under study 4 discussion the gpgpu accelerated implementation presented in this paper was found to be able to deliver the transient solution for a problem with 5 million unknowns in roughly 4 min as shown in the results section this result along with those cited from the literature is encouraging as one takes into consideration the increasing demands of scientific computing regarding the solution procedure the considerable speedup in the parallel code in relation to the serial one can be assigned to the large number of calculations needed to mount the linear system matrix and vectors at each time step the mounting of the system matrix and vectors is required as the groundwater model for unconfined aquifers means the hydraulic transmissivities are dependent on the thickness of the saturated layer which changes at each time step in the transient model in fact it was found that the mounting of the system is the most time consuming task in the overall solution process this is an important point to take into account in the design of the solution flowchart which should consider the parts of the code to be executed in parallel kernels regarding the solution of the groundwater flow model we note that the relationship between the variations in the depth of aquifers and the fluctuations in the flow rate of water in rivers is an important hydrological issue such relationships have significant importance in the analysis of water storage and transport potential in aquifers as pointed out in ref guo 1997 such analysis can provide information for the planning of water supply facilities and guidelines for the installation and use of irrigation systems in a broader sense they can provide fast and sufficiently accurate scenarios regarding the availability of water in a supply system furthermore they can guide projections and strategies regarding the most favourable time for water authorities to resort to rationing periods the simulation of groundwater flow for apuaê basin brazil by means of the gpgpu accelerated groundwater flow model permitted the assessment of important aspects of its dynamics while also serving as an illustration of the application of the computational model in this context a key point to note is that the transient solution of the groundwater model with over 5 million points was obtained by means of an inexpensive office computer and thus can be largely replicated 5 final remarks this study addressed the feasibility of the application of gpgpu to the problem of groundwater flow in unconfined aquifers for anisotropic and heterogeneous media the results agree with previously published papers in the sense that the parallel implementation largely outperforms the serial one thus enhancing the computational performance and reducing simulation time it was found that the cuda implementation of the groundwater flow equations was capable of outperforming the serial implementation and providing up to a 56 fold reduction in computation time with regard to future perspectives we would like to point out that it is likely that an increasing number of environmental models will have their parallelized version developed and implemented in order to check further advantages and drawbacks of the gpgpu application in the modelling of environmental phenomena the code for the computational model developed in this research can be obtained at http modelagemambientaluffs blogspot com br environmental modelling at uffs 2017 acknowledgements the authors would like to thank cnpq for its support under the grant no 460329 2014 6 
