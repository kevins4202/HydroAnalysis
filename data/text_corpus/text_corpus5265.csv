index,text
26325,this paper aims to fundamentally assess the resilience of salt marsh mudflat systems under sea level rise we applied an open source schematized 2d area model delft3d that couples intertidal flow wave action sediment transport geomorphological development with a population dynamics approach including temporal and spatial growth of vegetation and bio accumulation wave action maintains a high sediment concentration on the mudflat while the tidal motion transports the sediments within the vegetated marsh areas during flood the marsh mudflat system attained dynamic equilibrium within 120 years sediment deposition and bio accumulation within the marsh make the system initially resilient to sea level rise scenarios however after 50 60 years the marsh system starts to drown with vegetated levees being the last surviving features biomass accumulation and sediment supply are critical determinants for the marsh drowning rate and survival our model methodology can be applied to assess the resilience of vegetated coast lines and combined engineering solutions for long term sustainability graphical abstract image 1 keywords salt marshes mudflats bio geomorphology sea level rise slr waves mud morphodynamics 1 introduction traditionally the approaches to coastal defence have utilized structural measures to ensure a desired level of safety for the surrounding areas this concept has been instrumental for coastal management strategies around the world to ensure the protection of low lying regions against the imminent threat of inundation however not only has this measure resulted in dire impacts on local ecology and surrounding ecosystems airoldi et al 2005 but it appears not so sustainable in combating the scenarios with sea level rise slr instead innovative solutions could be implemented that comprise a combination of both structural and non structural measures one such non structural measure builds on the building with nature concept for the reduction of risk due to inundation and proposes the use of vegetated foreshores borsje et al 2011 temmerman et al 2013 vuik et al 2016 the growth of vegetation serves several ecosystem services that are pertinent toward flood management strategies the establishment of vegetation on the mudflat initially in patches results in the concentration of flows with a subsequent increase in bed shear stresses and the initiation of channels balke et al 2013 hu et al 2015 schwarz et al 2011 temmerman et al 2003 temmerman et al 2010 channels are known to ensure the drainage growth and expansion of the intertidal vegetation attema 2014 mudd and fagherazzi 2016 stark et al 2017 the expansion of the vegetation patches coupled with the ability of the vegetation to capture and trap sediments have led to the development of stable salt marsh mudflat systems bendoni et al 2016 fagherazzi et al 2012 kirwan et al 2016 maan et al 2015 schepers et al 2017 these stable systems reduce the magnitude of the tidal currents wave action and associated erosion patterns in bare mudflats bendoni et al 2016 tonelli et al 2010 moreover these vegetated systems are existing features which are instrumental for low lying countries that are experiencing the effects of slr higher spring tides larger waves increased overtopping frequencies and the loss of kilometres of foreshores among others trenhaile 2009 however according to bouma et al 2016 salt marsh systems are prone to erosion and accretion years attributed to a decrease in sediment supply and increases in the magnitude of the wave action and the frequency of extreme events allen 2000 cowell and thom 1995 mariotti and fagherazzi 2010 schwimmer 2001 stark et al 2017 vuik and jonkman 2016 pressures may also extend to the competition stresses induced within and among plant species and to human interventions for protection and reclamation purposes the erosion process may result in a substantial loss of the intertidal vegetation cover leaving low lying sections of the foreshore open to the full impact of the waves and tidal surges temmerman et al 2005 temmerman et al 2010 thampanya et al 2006 the effects of these triggers are made visible through the lateral dynamics of the marsh and the rate at which the marsh heightens and may have lasting impacts on the ecology of the system maan et al 2015 mudd et al 2010 nyman et al 2006 intertidal marsh mudflat areas are under tremendous pressure from slr by the year 2100 mean sea water level may realize increases ranging from 0 6 m to 1 1 m ipcc 2013 schile et al 2014 stocker 2014 studies have shown that the system s sediment balance both internally and externally may enable it to adjust the bed level at a rate that matches slr winterwerp et al 2013 however if the system is unable to timely adjust this will trigger a landward retreat of the vegetation as they are displaced to available areas of a higher elevation mudflats may drown under slr scenarios providing less sediments to adjacent salt marshes van der wegen and roelvink 2008 additionally when space is limited landward due to the construction of dykes or seawalls the vegetation is trapped between the prograding high water line and the obstacles this coastal squeeze results in the vegetation being unable to survive extensive durations of inundation bouma et al 2016 definite conclusions on the long term resilience i e magnitude and extent of salt marsh mudflat systems under slr scenarios are to date uncertain clough et al 2016 crosby et al 2016 schile et al 2014 the feedbacks between the plant growth and the geomorphological developments are known to trigger increases in the bed level due to the accumulation of organic and mineral components at a rate that potentially is able to match slr kirwan and megonigal 2013 turner et al 2004 however this increase also depends on factors such as land subsidence sediment supply vegetation productivity storm frequency and tectonic uplift spatial variability in the bed level channel network and vegetation cover play an important role as well these either reduce or increase the level of safety against submergence bouma et al 2016 craft et al 2009 crosby et al 2016 stralberg et al 2011 willemsen et al 2016 with an accelerated increase in the slr rate lower suspended sediment concentrations as rivers are frequently dammed and groynes reduce alongshore transport upstream and reductions in the plant productivity it remains uncertain if the marsh adaptation will be able to keep up with slr projections morris et al 2002 as such a better understanding of the processes which govern the development towards a potential equilibrium state and its adaptation under variations in the model parameters is beneficial bouma et al 2016 salt marsh ecosystems have been explored extensively with experimental and analytical studies and through numerical models of either a schematized or case specific nature a range of models have been utilized which capture in some combination the feedbacks between the biomass contribution plant growth tidal dynamics and morphology to determine the effect of slr on the bed elevation due to variations in inundation sediment supply and or plant productivity due to slr dijkstra 2008 temmerman et al 2007 van der wegen et al 2016 van loon steensma 2015 ye 2012 zhou et al 2016 the general conclusions of these studies highlight the importance of the rate of slr to marsh adaptation the salt marsh degenerates under high rates of slr but maintains a stable profile under low or mean slr rates kirwan and mudd 2012 most recently the patterns in carbon sequestration and accretion data were analysed globally against increases in slr to compare the actual resilience against modelled quantifications nyman et al 2006 measured accretion rates over the last century and more so in the last two decades were shown to be lower than the predicted or modelled values which are needed to sufficiently keep pace with slr craft et al 2009 crosby et al 2016 this was concluded for both the conservative rcp 2 6 and the extreme rcp 8 5 climate change scenarios with projected reductions in the marsh cover in excess of 90 for the latter craft et al 2009 crosby et al 2016 morris et al 2002 however given more recent estimates for slr horton et al 2014 sweet et al 2017 the rcp 8 5 slr scenarios may also be seen as conservative therefore this analysis intends to test the above mentioned behaviour using a schematized process based numerical modelling approach the numerical model was developed and validated quantitatively against existing theory data and laboratory studies the value of the current study compared to earlier studies is threefold firstly we considered an integrated marsh mudflat system including the sediment exchange between the mudflat and marsh secondly we explicitly considered wave action as the main driver for sediment re suspension with the associated tidal sediment dynamics finally we followed a 2d area approach accounting for the spatial dynamics of the marsh mudflat although the focus of this study is on a process understanding of the behaviour of the salt marsh mudflat system the schematized model setup was inspired by the conditions found in the dutch south western delta 2 methods 2 1 bio geomorphological model methods our schematized process based numerical modelling approach couples tidal hydrodynamics wave action sediment transport and morphodynamics delft3d with vegetation growth and bio accumulation via a matlab code both the delft3d software and the matlab tools utilized in this study are open source https oss deltares nl web delft3d 2 1 1 morphodynamic model approach the delft3d flow model solves the unsteady shallow water equations in two dimensions depth averaged since preliminary sensitivity runs showed that a 3d approach leads to similar results van der wegen and roelvink 2008 willemsen et al 2016 similar to van der wegen et al 2016 sediment erosion and deposition rates are calculated using the krone partheniades formulation for fine muddy sediments sediment transports are calculated based on the advection diffusion equation for the long term geomorphological development decades the bed level is updated every time step based on spatial gradients in sediment transport the application of a morphological factor 100 enhanced the bed level development compared to hydrodynamic timescales roelvink 2006 theoretically the use of the morphological time factor which accelerates the bed level variations can be easily applied to the growth diffusion and decay processes but is limited for the establishment however within our model the bio geomorphological time scale is significantly larger than the hydrodynamic time scales and therefore any changes wouldn t significantly impact the hydrodynamics attema 2014 roelvink 2011 the 10 m grid resolution and domain applied were determined by the visibility extent of the desired features within the salt marsh mudflat system such as the establishment and growth of the vegetation and the expected channel dimensions attema 2014 lokhorst 2016 oorschot et al 2016 cliff dynamics at the marsh edge are not within the scope of this study comparisons with a 5 m grid resolution showed minimal variation in the geo morphological developments along the platform and interface application of a cut cell approach to cover cliff dynamics in our methodology may be the subject of future research wave action was represented by the roller model extension of delft3d hydraulics 2002 as it is less computationally intensive than other more sophisticated wave models like swan the roller model predicts short wave groups propagating over a bed including wave dissipation by breaking and bed friction although the flow is impacted by vegetation friction the presence of vegetation does not affect the roller model bed friction in the current model formulation 2 1 2 dynamic vegetation model approach the vegetation growth module applies the population dynamics approach described by attema 2014 schwarz et al 2014 and temmerman et al 2005 however the coupling techniques and the representation of the vegetation across timescales were adapted for the trachytope extension of delft3d flow capturing both marsh and mudflat the net growth of the vegetation is represented by the following relation equation 1 net vegetation growth monden 2010 schwarz et al 2014 temmerman et al 2007 1 d p d p e s t d p g r o w t h d p d i f f x d p d i f f y d p i n u n d d p f l o w where dp represents the derivative of the total stem density of the salt marsh w r t time year stems m2 dp x represents the derivative of the stem density of the salt marsh w r t time stems m2 where x the establishment est growth diffusion inundation inund and shear stress flow kindly see appendix d for additional details the bio geomorphological model was developed to represent the influence of one vegetation type during the analysis for the spartina anglica species after every vegetation time step the marsh model records the change across the grid domain of the height vegetation density drag coefficient bed roughness and the relative coverage of the vegetation in each grid cell 2 1 3 offline coupling approach the trachytope extension of delft3d defines the location of the vegetation across the domain after every morphodynamic time step the output of delft3d bed level shear stresses water level and flow velocities is converted to input parameters for the growth model inundation and shear stresses subsequently after every vegetation time step the adjusted roughness and flow resistance parameters are then compiled in the trachytope input files deltares 2014 the roughness exerted by the vegetation on the flow is determined using the baptist et al 2007 relation where the higher the value the smoother the surface and the smaller the drag force afterward the delft3d flow model is then restarted with the new spatial layout of the vegetation and the bed level fig 1 the seasonal and daily variations in the water level which impact the spatial and temporal establishment of the vegetation also constrain the acceptable hydrodynamic and morphodynamic time steps on the other hand calculating bed level changes and vegetation growth every hydrodynamic time step 1 min would lead to excessive computation time therefore using a mf of 100 a single hydrodynamic tide represented three months of morphological development we coupled the vegetation growth model to delft3d coupled every three months each single tide which roughly covered the seasonal dynamics of vegetation growth this loop continued until an equilibrium bathymetry and vegetation density were attained 2 2 model setup 2 2 1 overview of typical salt marshes in the south western delta 2 2 1 1 domain schematization the locations that inspired the current study are located in the dutch south western delta more specifically the hellegat salt marsh the sint annaland salt marsh and the land of saeftinghe salt marsh fig 2 the marshes all of which were formed naturally and have open environments subject to both waves and tides were selected randomly to represent the variability of the marshes in the south western delta this variability extended to the size layout of marsh to mudflat channel patterns and velocities though these three sites were highlighted results were also compared to marshes along the western scheldt where studies with similar approaches were available temmerman et al 2005 temmerman et al 2013 van loon steensma 2015 winterwerp et al 2013 the three case study locations were chosen to determine both the boundary and domain conditions they provided descriptions for the dimensions of the domain however the bathymetry and boundary conditions were chosen to be representative of the hellegat salt marsh with regards to the quantitative validation of the model results the data available at the time of this study were detailed towards to the hellegat salt marsh the dimensions for the length and width of model were chosen so as to ensure a balance between the computational time and an apt representation of the interactions between the salt marsh and mudflat the model domain would first represent a slice of the typical system with the smallest possible length of 500 m which would not impede the formation of channels with the exception of the land of saeftinghe salt marshes the typical width was below 1500 m therefore the marsh platform for allowable growth was extended 1500 m with a mudflat at the seaward edge extending a further 1000 m fig 3 2 2 1 2 physical setting the hellegat salt marsh is situated along a meandering channel and is subject to both the tidal dynamics semi diurnal tidal regime and a mean tidal range of 3 9 m and wave action vuik et al 2016 this channel has suffered extensive erosion and is currently reinforced with dykes to protect the hinterland areas during spring tides the mean tidal range increases and varies from 4 49 m to 5 93 m over the last century the mean water level has risen by approximately 3 mm at the mouth of the western scheldt and 15 mm in the inner section of the estuary temmerman et al 2005 there is a clear reduction in the magnitude of the waves which propagate from the north sea and dissipate their energy over the smaller depths during extreme storm events the wave heights recorded in the western scheldt estuary have maximum values ranging from 2 m to 3 m li et al 2014 sistermans and nieuwenhuis 2004 however average wave heights near the marsh are about 0 7 m and slowly decrease as the waves propagate over the vegetated marsh which has a maximum depth of 1 9 m above the marsh surface vuik et al 2016 the sediment compositions of the estuary consist of fine sediments with sizes ranging from 24 μm to 56 μm in the channels mudflats and deeper parts of the shoals while along the vegetated salt marsh sections the size is approximately 88 μm however despite the classification as a cohesive sediment area it should be noted that there is a bit of fine sand in the system which results in maximum sizes of 119 μm in the densely vegetated areas rahman 2015 the coarser sediments were not considered due to the intended schematization of a developing marsh the hellegat salt marsh has geomorphological development characteristic of an old high marsh with a relatively flat platform 1 40 slope dissected by a developed network of channels or tidal creeks temmerman et al 2004 the average marsh elevation is 1 1 m nap dutch ordinance level which is close to local mean sea level bouma et al 2016 spartina anglica dominates the marsh vegetation with an average number of stems m2 of 1200 and an average stem thickness of 3 mm vuik et al 2016 beyond the edge of the salt marsh platform there exists a bare tidal mudflat which eventually deepens as the centre of the channel approaches it is important to note that at the time of the research information regarding the soil sediment composition and the effects of wind waves were not available 2 2 2 detailed schematization model parameters the model setup describes a 2dh section of a salt marsh mudflat system 2500 m cross shore by 500 m longshore with grid cells of a 10 m grid resolution the grid cell size is apt as channel widths within the western scheldt are often of a magnitude of 10 m the bed increases from 5 m at the seaward boundary only open boundary to 1 m at the edge of the initial mudflat platform over a cross shore distance of 1500 m the platform then further extends another 1000 m to the landward boundary along the platform random perturbations of 1 cm were added to the bed level to stimulate pattern development a uniform chézy roughness value of 65 m 1 2 s was used to represent a smooth bed of cohesive sediment without vegetation the model was forced with a harmonic water level boundary at the seaward edge having a single 12 h period tide of 2 m the length of the tidal cycle extended 21 h with a hydrodynamic spin up time of 540 min and a 12 h period including geomorphological development a constant wave climate was imposed consisting of waves with 0 5 m significant wave height vuik et al 2016 vuik et al 2016 a 2 s peak period and entering the domain perpendicularly a uniform suspended sediment concentration ssc of 0 025 kg m3 was defined at the seaward boundary with a thatcher harleman time lag of 120 min this time lag describes the period of gradual ssc change at the seaward boundary at the turning of the tide and prevents sudden variations in the suspended sediment concentrations and associated numerical instabilities the model assumes zero gradients alongshore areas with a shear stresses in excess of a critical shear stress of 0 5n m2 will be eroded deposition occurs at a rate of the product of the fall velocity w 0 5 mm s and the ssc for further detail see appendix a for a compilation of the model parameters within the context of this research equilibrium will depend on both the vegetation density and the geomorphological developments for the geomorphology aspect equilibrium may refer to a profile which has a dynamic nature over longer timescales but remains static or stable vertically or horizontally over shorter timescales the lateral retreat or expansion of the leading edge of the marsh is characteristic of the shift horizontally while the vertical movement refers to the increase or decrease of the bed elevation salt marshes attain an equilibrium density after 20 30 years during favourable site specific conditions attema 2014 schwarz et al 2014 with the geomorphological development attaining equilibrium on a decadal scale therefore the model extended for 120 years of vegetation growth see table 1 for parameters to capture a profile in equilibrium the slr was imposed with both a linear and exponential increase to achieve this the mean sea level msl was raised at the end of each year by a value prescribed by a linear or exponential function for the ipcc rcp 8 5 climate scenario the ipcc was chosen over the national oceanic and atmospheric administration noaa due to its global applicability and coverage after an equilibrium mudflat profile was generated over the 120 years the slr scenarios were imposed over 100 years the msl increases by 1 137 m under the high approximation and 0 6 m and 0 8 m for the low and mean approximations respectively the contribution of the biomass above and below ground is examined via sensitivity runs where the range varies from a conservative contribution of 1 mm year to 3 mm year craft et al 2009 crosby et al 2016 morris et al 2002 temmerman et al 2013 van maanen et al 2015 scenarios also included variations in the wave height ssc sediment properties and yearly accretion balances within the marsh 6 mm year and 9 mm year crosby et al 2016 3 bio geomorphological development and model validation 3 1 development to equilibrium during the initial years the geomorphological development of the salt marsh mudflat system was found to lead the salt marsh growth as plant establishment is limited by inundation and erosion stresses fig 3 initially the vegetation establishes in patches on the platform resulting in the formation of channels due to flow concentrations around these patches as the channels incise the platform further deposition of sediment along the banks of the channels form characteristic levees see appendix b video 2 for the animation of the salt marsh mudflat base model supplemental materials when the vegetation reached its maximum stem density after 35 40 years the channel patterns are stabilized by the vegetation and they form the main drainage area for the intertidal vegetated zone geomorphological development is then limited to the heightening of the marsh with a slow progradation of the marsh edge appendix b the channel flow is asymmetric with a short high velocity flood period and a long low velocity ebb period the magnitude of the velocities within the vegetation were notably lower than the channels but maintained the characteristic surges during flood and ebb supplementary video related to this article can be found at https doi org 10 1016 j envsoft 2018 08 004 the following are the supplementary data related to this article video 1 5 video 1 5 video 2 6 video 2 6 even after 120 years the marsh edge continues to prograde due to the supply of sediments from the seaward boundary shear stresses near the marsh edge and in the incising channels continue to exceed the critical shear stress for erosion 0 5 n m2 fig 4 and fig 5 the main reason for this being the dissipation of the wave energy for a similar yet 1d study van der wegen et al 2017 showed that geomorphological equilibrium is eventually reached by an evolving mudflat despite the fact that the critical erosion shear stress is continuously exceeded during the tide although wave action leads to high shear stresses and related high sediment concentrations the tide residual sediment transport becomes negligible so that there is no net geo morphodynamic development net sediment deposition occurs mainly behind the marsh edge and around the channels in levee formation deposits fig 6 and appendix c after the 120 years the marsh profile shows minimal variation equilibrium is not reached in the strict sense since the marsh edge continues to prograde at a slow rate nevertheless the system was considered to be stable to allow further sensitivity and slr scenario analysis see video animation in appendix b 3 2 impact of vegetation waves on the morphology without vegetation the channel braiding patterns are replaced by straight channels which incise the platform further while allowing for the distribution of the sediment across the mudflat figs 5 and 6 the channels are able to incise the platform further because of the low roughness in the absence of the vegetation the level of the platform despite being narrower than the vegetated case becomes higher as sediments are easily transported landward when the waves are excluded the vegetated platform is wider as the reduced re suspension supports the progradation however the transport of the suspended sediment into the marsh toward the landward sections is significantly lowered waves lead to higher matured marshlands the presence of the vegetation increased the flow velocities within the channel while resulting in a significant decline of the velocities on the shoals larger height differences between the muddy marsh and the mudflat enhanced the erosion rate steepening the transition especially in the presence of waves figs 6 and 7 a this observation is similar to that of bouma et al 2016 and bouma et al 2005 fig 7 d shows that the magnitude of the wave forcing will contribute towards the development of steep transitions at the marsh edge 3 3 sensitivity analysis process understanding here we have summarized the results for high impact parameters which affect vegetation density and bed level figs 7 and 8 the findings provide possibilities for realistic adaptations in restoration and protection measures nb for this system marsh platforms below 0 7 m above the mean water level did not support the establishment of the vegetation 3 3 1 wave height from the analysis of the base model waves have been identified as the main driver for the lateral retreat appendix b figs 5 and 6 of the marsh platform waves suspend sediments from the mudflat and these sediments are transported landward during flood sediments deposited landward are subject to limited shear stresses due to the trapping capacity of the marsh so that ebb currents transport less sediment seaward during ebb this resulted in the heightening of the platform level as sediments filled the accommodation space therefore the variation in the wave height provides an understanding of the system behaviour under varying wave climates overall fig 7 d shows that larger waves produce steeper marsh edge transitions with a retreating edge conversely lower waves allow for larger sections of high marsh lands on which mature and stable plant species reside compared to purely tidal environments waves are seen to create higher platform levels when the wave height is lowered the geomorphological developments resemble that of a purely tidal environment in alternative wave climates this can be used as a tool in promoting growth within restoration strategies fig 8 d also reveals that an increase of the wave height lowers the vegetation coverage where the doubling of the wave height results in an approximate reduction of 1 106 stems year 3 3 2 roughness tidal amplitude and platform bed level a higher roughness lower chézy value leads to a concentration of flows generating narrower and deeper channels which prograde into the marsh area further additionally a higher roughness enhances the erosion of the mudflat and platform due to enhanced sediment suspension resulting in a retreating marsh edge and heightened platform fig 7 i within the morphological model the constant chézy value affects flow and sediment transport bare areas only and wave attenuation the roughness generated by the vegetation affects only the flow and sediment transport in marsh areas as the tidal amplitude is lowered with a constant platform height the degree to which the platform is flooded reduces therefore the mudflat is eroded because the waves dissipate all of their energy on the mudflat at the marsh edge continual wave dissipation results in the deepening of the mudflat level and the formation of steep transitions when the tidal amplitude equals or exceeds the magnitude of the marsh level the wave dissipation is limited within the area of the mean water high level and gentler transitions are formed at the marsh edge regarding the evolution of the stem growth lower tidal amplitudes and by extension lower inundation stresses allow for higher stem densities within the domain fig 8 the platform level variations show a greater affinity for the distribution of sediment along the transition area and mudflat whereas the variations in the amplitude allow for sediment to be deposited along both the marsh platform and also at the transition area see appendix c figure c2 3 3 3 sediment characteristics with the combination of both waves and tides the typical range for the shear stress lies between 0 3 and 0 5 n m2 this range is in line with validated model studies including wave action and tidal forces borsje et al 2011 cheon and suh 2016 macvean and lacy 2014 townend et al 2011 van der wegen et al 2016 results fig 7 c reveal that lower critical shear stress values lead to an eroding mudflat and a landward transport of sediments building up the marsh platform therefore restoration efforts which in practice simulate sufficient stirring of the sediment offshore will allow for increased deposition within the marsh lower fall velocities lead to a similar behaviour fig 7 h however though the marsh is elevated the width is reduced due to the lateral retreat of the platform the stem density is reduced with lower critical shear stress and fall velocity values fig 8 c the runs with a lower critical shear stress produce straight lined channel patterns with increased mortality rates however generally the modelled channel formation patterns and channel and marsh velocities are overestimated compared to the values found in literature kirwan and megonigal 2013 maan et al 2015 temmerman et al 2007 temmerman et al 2010 this may be attributed to the use of the roller model to represent the wave energy where due to the lack of diffraction considered along the one directional plane of movement the shear stresses are often in excess of 0 5n m2 especially during the flood tide the marsh platform seems not heavily affected by the boundary sediment concentrations fig 7 e but the mudflat is higher for a larger ssc at the boundary during flood the ssc levels are lower than the ssc imposed at the boundary 25 mg l in ebb flow higher ssc are observed in the channels along the mudflat with minimal values in the channels within the salt marsh as such sediments are deposited readily along the mudflat where fall velocities are optimum higher ssc values result in a filling of the accommodation space allowing for the progradation of the marsh fig 9 3 3 4 plant growth characteristics and biomass most morphological variation was seen in the critical inundation height runs fig 7 g followed by the plant height the less resilient the plant species is to inundation the lower the stem density the lower vegetation coverage then allows for greater deposition along the marsh as higher sediment loads are transported to the marsh fig 7 as the height of the vegetation increases the lower the flow velocities and the magnitude of channel incisions in the marsh platform this adds to the protection offered by the vegetation to the marsh mudflat edge during ebb flow as the reduced velocities will result in smaller magnitudes of erosion due to flow with lower plant heights there is increased erosion of the marsh edge as the transition steepens once the maximum stem density coverage is reached this results in a more pronounced formation of channels within the marsh platform channels in the marsh area become narrower limiting the transport of sediment into the marsh relative increases in the marsh platform are notably lower with the decrease in elevation at the marsh edge there is an overall regression of the marsh edge with continued dynamics at the boundary after 120 years model results show not only the heightening of the marsh platform but the gradual progradation of the marsh edge are observed as the accommodation space is filled for larger bio accumulation rates fig 7 f the validation of the plant growth and decay characteristics were carried out quantitatively based on comparisons with attema 2014 and monden 2010 3 4 slr scenarios in all three slr scenarios the landward low lying areas drown leaving zones of elevated vegetated land masses disconnected from the shore figs 10 and 11 see appendix b video 1 for the animation of the salt marsh mudflat base model supplemental materials the landward sections of the marsh are relatively low due to the levee formation closer to the marsh edge the profiles reveal that under the slr scenarios the bed level increases but at a rate that is insufficient to match the rate of slr as shown by the stem density plots in figs 10 and 11 as such there is a loss of the intertidal wetland over time initially quite gradual but then increases after 50 years of slr for the model without biomass contributions fig 10 b this translates to a landward shift of the salt marshes to survive where space is available the slr results in increased water depths which allow waves to propagate further into the salt marsh the larger tidal prism increases the flow velocities thereby enhancing seaward erosion and landward deposition sediment is being transported from the mudflat unto the marsh which heightens but drowns due to insufficient volumes under the exponential increase in the sea level there is a threshold below which the salt marshes can survive and this extends some 40 50 years for all scenarios fig 10 b however beyond this period the high water level begins to drown areas that once formed the upper intertidal areas the vegetation mortality now switches to one dominated by inundation stresses overall the salt marsh system without interventions will not survive the long term impacts of slr a comparison was carried out between the exponential and linear increases in the mean water level the exponential scenario did reflect an extended period for which the salt marshes were able to increase the bed level at a rate that exceeded the slr but the linear scenario only extended approximately 10 years of slr however after 100 years of slr the vegetation cover for both systems was the same it should be noted that this model was developed for sediment rich systems and not sediment poor systems such as in the east of the usa 4 discussion our schematized 2d depth averaged model satisfactorily describes the interaction between the waves tides sediment transport morphodynamics in delft3d flow and the bio accumulation and plant growth dynamics of the spartina anglica in matlab the research aimed to increase the process understanding of the salt marsh system and also allow for applications toward resilience measures under slr inspired by the dutch south western delta the bio geomorphological model reproduced a realistic salt marsh mudflat system in near equilibrium after a century while reaching a maximum stem density after 40 years the interaction between the mudflat and marsh area provides key details on the triggers for the geomorphological developments within the marsh mudflat system wave action was found to be the primary trigger for the sediment supply towards the salt marsh with the formation of steep marsh edge transitions as the waves enter the domain most of the energy is lost through wave breaking and dissipation along the mudflat sediment is continuously stirred and transported to the marsh area sediment deposits in the marsh in levee type patterns close to the channel edges and platform edge once the vegetation establishes and grows on the platform the roughness increases favouring more deposition on the platform more landward areas face less deposition as the elevation of the marsh platform increases the wave energy becomes concentrated around the mean water level this results in a dynamic oscillatory flow pattern which erodes the sediment and transports it along the edge of the marsh this observation is similar to that of bouma et al 2016 and bouma et al 2005 who suggested that larger height differences between the muddy marsh and the mudflat enhance the erosion rate especially in the presence of waves this sediment is later transported landward following the channels as the wave height increases there is greater transport of sediment towards the platform with steeper transitions between the marsh and the mudflat steeper transitions are as a result of the larger magnitude of the oscillatory wave forces this may be critical for restoration strategies which attempt to promote growth in alternative wave climates additionally when compared with a tidally dominant system coastal areas exposed to waves have higher marsh platforms a system s resilience to slr will depend on its ability to increase the bed level at a rate that exceeds the slr this rate is dependent on the inundation depth at high tide the external sediment supply and the organic deposition potential we found that the salt marsh mudflat system drowns under all imposed slr scenarios to varying degrees with variations in the sediment supply and biomass however the bio accumulation rate is the most critical parameter affecting the resilience under slr high bio accumulation rates even lead to marsh survival including the heightening and gradual progradation of the marsh fig 12 results show that the bio accumulation rate has a greater impact on bed level increases under slr once the vegetation density is maintained biomass rates 1 3 mm yr contribute substantially towards the overall bed level increases when compared to the sediment supply fig 13 however in hellegat salt marsh the biomass production is not sufficient to ensure survival under slr production rates in excess of 3 mm yr are unrealistic as the marsh has an existing belowground biomass capacity of 1 4 4 7 kg m2 rahman 2015 as such this must be supplemented by increases in the external sediment supply to achieve higher yearly accretion rates systems with an accretion balance exceeding 5 mm yr were found resilient while the marsh platform expands for values up to 9 mm yr moreover high accretion balances that consider the sediment budget external rivers offshore artificial supply and internal biomass storage and production are more effective in reducing vulnerability under slr our modelling approach allows for a closer analysis of the spatial dynamics of drowning thorne et al 2014 especially the levee type patterns evolving close to the channel and marsh edges that have a significant impact on the response of the system under slr these levee areas are the last to drown under slr scenarios overall the inclusion of the wave dynamics proved critical for the geo morphological developments of the marsh mudflat system and improved quantifications for resilience to slr further works should however explore the dynamics of the sediment budget for specific locations 5 conclusion our bio geomorphological modelling approach was able to reproduce a mudflat marsh system in equilibrium after 120 years the process based methodology allowed for a thorough sensitivity analysis of the model parameter space including hydrodynamic forcing sediment characteristics and vegetation dynamics with the belowground biomass accumulation our findings were quantitatively validated primarily against the findings of relevant modelling studies the model produced realistic channel formations with characteristic flood and ebb hydrodynamics wave action was shown to be a key process as it suspends sediments on the mudflat and transports them to the marsh platform during flood this resulted in the heightening of the marsh platform and the formation of steep marsh mudflat transitions higher wave heights produce narrower and more elevated marshlands our area model showed the evolution of levee type deposition patterns along the channels the critical shear stress settling velocity critical plant inundation height and tidal variations are key model parameters in the bio geomorphological development imposing 100 years of slr scenarios on the equilibrium profile allowed us to analyse key factors impacting the resilience of the marsh mudflat system to slr despite the net import of sediments and biomass productivity of the system all slr scenarios eventually led to the partial or complete drowning of the marsh mudflat system the channel networks expanded landward and incised the marsh platform the vegetated levee type patterns were the last features to survive exponential increases in slr showed extended periods in which the salt marshes were able to increase the bed level at a rate that exceeded the slr but the linear scenarios did not imposing higher accretion rates may allow the salt marsh survive slr scenarios another key to marsh survival under slr will stem from increasing the overall sediment supply allowing for higher yearly accumulation rates model approaches with ecological components enhance the process understanding and reinforce innovative solutions in the restoration and protection of these valued intertidal vegetation species future research may utilize process based approaches to evaluate engineering solutions for protection and restoration strategies and study the dynamics of other vegetation types like mangroves with applications to case specific areas software and or data availability we applied a process based numerical modelling approach which coupled offline delft 3d flow and matlab both the delft3d software and matlab tools used in this study are open source and freely available online https oss deltares nl web delft3d the delft3d suite was developed by deltares with the main office located at rotterdamseweg 185 2629 hd delft the netherlands contact can be made through the contact form https oss deltares nl web delft3d contact or via the sales department with the following email and contact number sales deltaressystems nl 31 0 88 335 8188 the delft3d flow flow morphology mor and waves wave modules were first made available in 2011 and is written using fortran and c c language rules lesser 2009 the running of the model requires the use of matlab versions 2013 or higher this software can be attained through purchase student version or trial online https nl mathworks com products matlab html contact can be made to mathworks the developer through its representative in the netherlands dr holtroplaan 5b phone 31 40 2156700 or via the corporate headquarters the sources of all datasets and parameter values used for the model developed have been provided in the methods section and in appendix a of the paper additionally the typical input files for the setup of the base model along with the plant growth script have been provided in the supplemental materials with regards to the hardware required a standard pc with minimum 8 gb ram acknowledgements i would first like to express my gratitude to the staff of coastal and port development department of ihe delft institute for water education and deltares who have been instrumental in shaping the framework for this research and generously gave of their time and efforts p w j m willemsen was supported by the research programme be safe nwo 850 13 010 financed primarily by the netherlands organisation for scientific research nwo 850 13 012 this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors appendix a supplementary data the following are the supplementary data related to this article model setup files for the base salt marsh mudflat model zip model setup files for the base salt marsh mudflat model zip appendices a b c and d appendices a b c and d appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 004 
26325,this paper aims to fundamentally assess the resilience of salt marsh mudflat systems under sea level rise we applied an open source schematized 2d area model delft3d that couples intertidal flow wave action sediment transport geomorphological development with a population dynamics approach including temporal and spatial growth of vegetation and bio accumulation wave action maintains a high sediment concentration on the mudflat while the tidal motion transports the sediments within the vegetated marsh areas during flood the marsh mudflat system attained dynamic equilibrium within 120 years sediment deposition and bio accumulation within the marsh make the system initially resilient to sea level rise scenarios however after 50 60 years the marsh system starts to drown with vegetated levees being the last surviving features biomass accumulation and sediment supply are critical determinants for the marsh drowning rate and survival our model methodology can be applied to assess the resilience of vegetated coast lines and combined engineering solutions for long term sustainability graphical abstract image 1 keywords salt marshes mudflats bio geomorphology sea level rise slr waves mud morphodynamics 1 introduction traditionally the approaches to coastal defence have utilized structural measures to ensure a desired level of safety for the surrounding areas this concept has been instrumental for coastal management strategies around the world to ensure the protection of low lying regions against the imminent threat of inundation however not only has this measure resulted in dire impacts on local ecology and surrounding ecosystems airoldi et al 2005 but it appears not so sustainable in combating the scenarios with sea level rise slr instead innovative solutions could be implemented that comprise a combination of both structural and non structural measures one such non structural measure builds on the building with nature concept for the reduction of risk due to inundation and proposes the use of vegetated foreshores borsje et al 2011 temmerman et al 2013 vuik et al 2016 the growth of vegetation serves several ecosystem services that are pertinent toward flood management strategies the establishment of vegetation on the mudflat initially in patches results in the concentration of flows with a subsequent increase in bed shear stresses and the initiation of channels balke et al 2013 hu et al 2015 schwarz et al 2011 temmerman et al 2003 temmerman et al 2010 channels are known to ensure the drainage growth and expansion of the intertidal vegetation attema 2014 mudd and fagherazzi 2016 stark et al 2017 the expansion of the vegetation patches coupled with the ability of the vegetation to capture and trap sediments have led to the development of stable salt marsh mudflat systems bendoni et al 2016 fagherazzi et al 2012 kirwan et al 2016 maan et al 2015 schepers et al 2017 these stable systems reduce the magnitude of the tidal currents wave action and associated erosion patterns in bare mudflats bendoni et al 2016 tonelli et al 2010 moreover these vegetated systems are existing features which are instrumental for low lying countries that are experiencing the effects of slr higher spring tides larger waves increased overtopping frequencies and the loss of kilometres of foreshores among others trenhaile 2009 however according to bouma et al 2016 salt marsh systems are prone to erosion and accretion years attributed to a decrease in sediment supply and increases in the magnitude of the wave action and the frequency of extreme events allen 2000 cowell and thom 1995 mariotti and fagherazzi 2010 schwimmer 2001 stark et al 2017 vuik and jonkman 2016 pressures may also extend to the competition stresses induced within and among plant species and to human interventions for protection and reclamation purposes the erosion process may result in a substantial loss of the intertidal vegetation cover leaving low lying sections of the foreshore open to the full impact of the waves and tidal surges temmerman et al 2005 temmerman et al 2010 thampanya et al 2006 the effects of these triggers are made visible through the lateral dynamics of the marsh and the rate at which the marsh heightens and may have lasting impacts on the ecology of the system maan et al 2015 mudd et al 2010 nyman et al 2006 intertidal marsh mudflat areas are under tremendous pressure from slr by the year 2100 mean sea water level may realize increases ranging from 0 6 m to 1 1 m ipcc 2013 schile et al 2014 stocker 2014 studies have shown that the system s sediment balance both internally and externally may enable it to adjust the bed level at a rate that matches slr winterwerp et al 2013 however if the system is unable to timely adjust this will trigger a landward retreat of the vegetation as they are displaced to available areas of a higher elevation mudflats may drown under slr scenarios providing less sediments to adjacent salt marshes van der wegen and roelvink 2008 additionally when space is limited landward due to the construction of dykes or seawalls the vegetation is trapped between the prograding high water line and the obstacles this coastal squeeze results in the vegetation being unable to survive extensive durations of inundation bouma et al 2016 definite conclusions on the long term resilience i e magnitude and extent of salt marsh mudflat systems under slr scenarios are to date uncertain clough et al 2016 crosby et al 2016 schile et al 2014 the feedbacks between the plant growth and the geomorphological developments are known to trigger increases in the bed level due to the accumulation of organic and mineral components at a rate that potentially is able to match slr kirwan and megonigal 2013 turner et al 2004 however this increase also depends on factors such as land subsidence sediment supply vegetation productivity storm frequency and tectonic uplift spatial variability in the bed level channel network and vegetation cover play an important role as well these either reduce or increase the level of safety against submergence bouma et al 2016 craft et al 2009 crosby et al 2016 stralberg et al 2011 willemsen et al 2016 with an accelerated increase in the slr rate lower suspended sediment concentrations as rivers are frequently dammed and groynes reduce alongshore transport upstream and reductions in the plant productivity it remains uncertain if the marsh adaptation will be able to keep up with slr projections morris et al 2002 as such a better understanding of the processes which govern the development towards a potential equilibrium state and its adaptation under variations in the model parameters is beneficial bouma et al 2016 salt marsh ecosystems have been explored extensively with experimental and analytical studies and through numerical models of either a schematized or case specific nature a range of models have been utilized which capture in some combination the feedbacks between the biomass contribution plant growth tidal dynamics and morphology to determine the effect of slr on the bed elevation due to variations in inundation sediment supply and or plant productivity due to slr dijkstra 2008 temmerman et al 2007 van der wegen et al 2016 van loon steensma 2015 ye 2012 zhou et al 2016 the general conclusions of these studies highlight the importance of the rate of slr to marsh adaptation the salt marsh degenerates under high rates of slr but maintains a stable profile under low or mean slr rates kirwan and mudd 2012 most recently the patterns in carbon sequestration and accretion data were analysed globally against increases in slr to compare the actual resilience against modelled quantifications nyman et al 2006 measured accretion rates over the last century and more so in the last two decades were shown to be lower than the predicted or modelled values which are needed to sufficiently keep pace with slr craft et al 2009 crosby et al 2016 this was concluded for both the conservative rcp 2 6 and the extreme rcp 8 5 climate change scenarios with projected reductions in the marsh cover in excess of 90 for the latter craft et al 2009 crosby et al 2016 morris et al 2002 however given more recent estimates for slr horton et al 2014 sweet et al 2017 the rcp 8 5 slr scenarios may also be seen as conservative therefore this analysis intends to test the above mentioned behaviour using a schematized process based numerical modelling approach the numerical model was developed and validated quantitatively against existing theory data and laboratory studies the value of the current study compared to earlier studies is threefold firstly we considered an integrated marsh mudflat system including the sediment exchange between the mudflat and marsh secondly we explicitly considered wave action as the main driver for sediment re suspension with the associated tidal sediment dynamics finally we followed a 2d area approach accounting for the spatial dynamics of the marsh mudflat although the focus of this study is on a process understanding of the behaviour of the salt marsh mudflat system the schematized model setup was inspired by the conditions found in the dutch south western delta 2 methods 2 1 bio geomorphological model methods our schematized process based numerical modelling approach couples tidal hydrodynamics wave action sediment transport and morphodynamics delft3d with vegetation growth and bio accumulation via a matlab code both the delft3d software and the matlab tools utilized in this study are open source https oss deltares nl web delft3d 2 1 1 morphodynamic model approach the delft3d flow model solves the unsteady shallow water equations in two dimensions depth averaged since preliminary sensitivity runs showed that a 3d approach leads to similar results van der wegen and roelvink 2008 willemsen et al 2016 similar to van der wegen et al 2016 sediment erosion and deposition rates are calculated using the krone partheniades formulation for fine muddy sediments sediment transports are calculated based on the advection diffusion equation for the long term geomorphological development decades the bed level is updated every time step based on spatial gradients in sediment transport the application of a morphological factor 100 enhanced the bed level development compared to hydrodynamic timescales roelvink 2006 theoretically the use of the morphological time factor which accelerates the bed level variations can be easily applied to the growth diffusion and decay processes but is limited for the establishment however within our model the bio geomorphological time scale is significantly larger than the hydrodynamic time scales and therefore any changes wouldn t significantly impact the hydrodynamics attema 2014 roelvink 2011 the 10 m grid resolution and domain applied were determined by the visibility extent of the desired features within the salt marsh mudflat system such as the establishment and growth of the vegetation and the expected channel dimensions attema 2014 lokhorst 2016 oorschot et al 2016 cliff dynamics at the marsh edge are not within the scope of this study comparisons with a 5 m grid resolution showed minimal variation in the geo morphological developments along the platform and interface application of a cut cell approach to cover cliff dynamics in our methodology may be the subject of future research wave action was represented by the roller model extension of delft3d hydraulics 2002 as it is less computationally intensive than other more sophisticated wave models like swan the roller model predicts short wave groups propagating over a bed including wave dissipation by breaking and bed friction although the flow is impacted by vegetation friction the presence of vegetation does not affect the roller model bed friction in the current model formulation 2 1 2 dynamic vegetation model approach the vegetation growth module applies the population dynamics approach described by attema 2014 schwarz et al 2014 and temmerman et al 2005 however the coupling techniques and the representation of the vegetation across timescales were adapted for the trachytope extension of delft3d flow capturing both marsh and mudflat the net growth of the vegetation is represented by the following relation equation 1 net vegetation growth monden 2010 schwarz et al 2014 temmerman et al 2007 1 d p d p e s t d p g r o w t h d p d i f f x d p d i f f y d p i n u n d d p f l o w where dp represents the derivative of the total stem density of the salt marsh w r t time year stems m2 dp x represents the derivative of the stem density of the salt marsh w r t time stems m2 where x the establishment est growth diffusion inundation inund and shear stress flow kindly see appendix d for additional details the bio geomorphological model was developed to represent the influence of one vegetation type during the analysis for the spartina anglica species after every vegetation time step the marsh model records the change across the grid domain of the height vegetation density drag coefficient bed roughness and the relative coverage of the vegetation in each grid cell 2 1 3 offline coupling approach the trachytope extension of delft3d defines the location of the vegetation across the domain after every morphodynamic time step the output of delft3d bed level shear stresses water level and flow velocities is converted to input parameters for the growth model inundation and shear stresses subsequently after every vegetation time step the adjusted roughness and flow resistance parameters are then compiled in the trachytope input files deltares 2014 the roughness exerted by the vegetation on the flow is determined using the baptist et al 2007 relation where the higher the value the smoother the surface and the smaller the drag force afterward the delft3d flow model is then restarted with the new spatial layout of the vegetation and the bed level fig 1 the seasonal and daily variations in the water level which impact the spatial and temporal establishment of the vegetation also constrain the acceptable hydrodynamic and morphodynamic time steps on the other hand calculating bed level changes and vegetation growth every hydrodynamic time step 1 min would lead to excessive computation time therefore using a mf of 100 a single hydrodynamic tide represented three months of morphological development we coupled the vegetation growth model to delft3d coupled every three months each single tide which roughly covered the seasonal dynamics of vegetation growth this loop continued until an equilibrium bathymetry and vegetation density were attained 2 2 model setup 2 2 1 overview of typical salt marshes in the south western delta 2 2 1 1 domain schematization the locations that inspired the current study are located in the dutch south western delta more specifically the hellegat salt marsh the sint annaland salt marsh and the land of saeftinghe salt marsh fig 2 the marshes all of which were formed naturally and have open environments subject to both waves and tides were selected randomly to represent the variability of the marshes in the south western delta this variability extended to the size layout of marsh to mudflat channel patterns and velocities though these three sites were highlighted results were also compared to marshes along the western scheldt where studies with similar approaches were available temmerman et al 2005 temmerman et al 2013 van loon steensma 2015 winterwerp et al 2013 the three case study locations were chosen to determine both the boundary and domain conditions they provided descriptions for the dimensions of the domain however the bathymetry and boundary conditions were chosen to be representative of the hellegat salt marsh with regards to the quantitative validation of the model results the data available at the time of this study were detailed towards to the hellegat salt marsh the dimensions for the length and width of model were chosen so as to ensure a balance between the computational time and an apt representation of the interactions between the salt marsh and mudflat the model domain would first represent a slice of the typical system with the smallest possible length of 500 m which would not impede the formation of channels with the exception of the land of saeftinghe salt marshes the typical width was below 1500 m therefore the marsh platform for allowable growth was extended 1500 m with a mudflat at the seaward edge extending a further 1000 m fig 3 2 2 1 2 physical setting the hellegat salt marsh is situated along a meandering channel and is subject to both the tidal dynamics semi diurnal tidal regime and a mean tidal range of 3 9 m and wave action vuik et al 2016 this channel has suffered extensive erosion and is currently reinforced with dykes to protect the hinterland areas during spring tides the mean tidal range increases and varies from 4 49 m to 5 93 m over the last century the mean water level has risen by approximately 3 mm at the mouth of the western scheldt and 15 mm in the inner section of the estuary temmerman et al 2005 there is a clear reduction in the magnitude of the waves which propagate from the north sea and dissipate their energy over the smaller depths during extreme storm events the wave heights recorded in the western scheldt estuary have maximum values ranging from 2 m to 3 m li et al 2014 sistermans and nieuwenhuis 2004 however average wave heights near the marsh are about 0 7 m and slowly decrease as the waves propagate over the vegetated marsh which has a maximum depth of 1 9 m above the marsh surface vuik et al 2016 the sediment compositions of the estuary consist of fine sediments with sizes ranging from 24 μm to 56 μm in the channels mudflats and deeper parts of the shoals while along the vegetated salt marsh sections the size is approximately 88 μm however despite the classification as a cohesive sediment area it should be noted that there is a bit of fine sand in the system which results in maximum sizes of 119 μm in the densely vegetated areas rahman 2015 the coarser sediments were not considered due to the intended schematization of a developing marsh the hellegat salt marsh has geomorphological development characteristic of an old high marsh with a relatively flat platform 1 40 slope dissected by a developed network of channels or tidal creeks temmerman et al 2004 the average marsh elevation is 1 1 m nap dutch ordinance level which is close to local mean sea level bouma et al 2016 spartina anglica dominates the marsh vegetation with an average number of stems m2 of 1200 and an average stem thickness of 3 mm vuik et al 2016 beyond the edge of the salt marsh platform there exists a bare tidal mudflat which eventually deepens as the centre of the channel approaches it is important to note that at the time of the research information regarding the soil sediment composition and the effects of wind waves were not available 2 2 2 detailed schematization model parameters the model setup describes a 2dh section of a salt marsh mudflat system 2500 m cross shore by 500 m longshore with grid cells of a 10 m grid resolution the grid cell size is apt as channel widths within the western scheldt are often of a magnitude of 10 m the bed increases from 5 m at the seaward boundary only open boundary to 1 m at the edge of the initial mudflat platform over a cross shore distance of 1500 m the platform then further extends another 1000 m to the landward boundary along the platform random perturbations of 1 cm were added to the bed level to stimulate pattern development a uniform chézy roughness value of 65 m 1 2 s was used to represent a smooth bed of cohesive sediment without vegetation the model was forced with a harmonic water level boundary at the seaward edge having a single 12 h period tide of 2 m the length of the tidal cycle extended 21 h with a hydrodynamic spin up time of 540 min and a 12 h period including geomorphological development a constant wave climate was imposed consisting of waves with 0 5 m significant wave height vuik et al 2016 vuik et al 2016 a 2 s peak period and entering the domain perpendicularly a uniform suspended sediment concentration ssc of 0 025 kg m3 was defined at the seaward boundary with a thatcher harleman time lag of 120 min this time lag describes the period of gradual ssc change at the seaward boundary at the turning of the tide and prevents sudden variations in the suspended sediment concentrations and associated numerical instabilities the model assumes zero gradients alongshore areas with a shear stresses in excess of a critical shear stress of 0 5n m2 will be eroded deposition occurs at a rate of the product of the fall velocity w 0 5 mm s and the ssc for further detail see appendix a for a compilation of the model parameters within the context of this research equilibrium will depend on both the vegetation density and the geomorphological developments for the geomorphology aspect equilibrium may refer to a profile which has a dynamic nature over longer timescales but remains static or stable vertically or horizontally over shorter timescales the lateral retreat or expansion of the leading edge of the marsh is characteristic of the shift horizontally while the vertical movement refers to the increase or decrease of the bed elevation salt marshes attain an equilibrium density after 20 30 years during favourable site specific conditions attema 2014 schwarz et al 2014 with the geomorphological development attaining equilibrium on a decadal scale therefore the model extended for 120 years of vegetation growth see table 1 for parameters to capture a profile in equilibrium the slr was imposed with both a linear and exponential increase to achieve this the mean sea level msl was raised at the end of each year by a value prescribed by a linear or exponential function for the ipcc rcp 8 5 climate scenario the ipcc was chosen over the national oceanic and atmospheric administration noaa due to its global applicability and coverage after an equilibrium mudflat profile was generated over the 120 years the slr scenarios were imposed over 100 years the msl increases by 1 137 m under the high approximation and 0 6 m and 0 8 m for the low and mean approximations respectively the contribution of the biomass above and below ground is examined via sensitivity runs where the range varies from a conservative contribution of 1 mm year to 3 mm year craft et al 2009 crosby et al 2016 morris et al 2002 temmerman et al 2013 van maanen et al 2015 scenarios also included variations in the wave height ssc sediment properties and yearly accretion balances within the marsh 6 mm year and 9 mm year crosby et al 2016 3 bio geomorphological development and model validation 3 1 development to equilibrium during the initial years the geomorphological development of the salt marsh mudflat system was found to lead the salt marsh growth as plant establishment is limited by inundation and erosion stresses fig 3 initially the vegetation establishes in patches on the platform resulting in the formation of channels due to flow concentrations around these patches as the channels incise the platform further deposition of sediment along the banks of the channels form characteristic levees see appendix b video 2 for the animation of the salt marsh mudflat base model supplemental materials when the vegetation reached its maximum stem density after 35 40 years the channel patterns are stabilized by the vegetation and they form the main drainage area for the intertidal vegetated zone geomorphological development is then limited to the heightening of the marsh with a slow progradation of the marsh edge appendix b the channel flow is asymmetric with a short high velocity flood period and a long low velocity ebb period the magnitude of the velocities within the vegetation were notably lower than the channels but maintained the characteristic surges during flood and ebb supplementary video related to this article can be found at https doi org 10 1016 j envsoft 2018 08 004 the following are the supplementary data related to this article video 1 5 video 1 5 video 2 6 video 2 6 even after 120 years the marsh edge continues to prograde due to the supply of sediments from the seaward boundary shear stresses near the marsh edge and in the incising channels continue to exceed the critical shear stress for erosion 0 5 n m2 fig 4 and fig 5 the main reason for this being the dissipation of the wave energy for a similar yet 1d study van der wegen et al 2017 showed that geomorphological equilibrium is eventually reached by an evolving mudflat despite the fact that the critical erosion shear stress is continuously exceeded during the tide although wave action leads to high shear stresses and related high sediment concentrations the tide residual sediment transport becomes negligible so that there is no net geo morphodynamic development net sediment deposition occurs mainly behind the marsh edge and around the channels in levee formation deposits fig 6 and appendix c after the 120 years the marsh profile shows minimal variation equilibrium is not reached in the strict sense since the marsh edge continues to prograde at a slow rate nevertheless the system was considered to be stable to allow further sensitivity and slr scenario analysis see video animation in appendix b 3 2 impact of vegetation waves on the morphology without vegetation the channel braiding patterns are replaced by straight channels which incise the platform further while allowing for the distribution of the sediment across the mudflat figs 5 and 6 the channels are able to incise the platform further because of the low roughness in the absence of the vegetation the level of the platform despite being narrower than the vegetated case becomes higher as sediments are easily transported landward when the waves are excluded the vegetated platform is wider as the reduced re suspension supports the progradation however the transport of the suspended sediment into the marsh toward the landward sections is significantly lowered waves lead to higher matured marshlands the presence of the vegetation increased the flow velocities within the channel while resulting in a significant decline of the velocities on the shoals larger height differences between the muddy marsh and the mudflat enhanced the erosion rate steepening the transition especially in the presence of waves figs 6 and 7 a this observation is similar to that of bouma et al 2016 and bouma et al 2005 fig 7 d shows that the magnitude of the wave forcing will contribute towards the development of steep transitions at the marsh edge 3 3 sensitivity analysis process understanding here we have summarized the results for high impact parameters which affect vegetation density and bed level figs 7 and 8 the findings provide possibilities for realistic adaptations in restoration and protection measures nb for this system marsh platforms below 0 7 m above the mean water level did not support the establishment of the vegetation 3 3 1 wave height from the analysis of the base model waves have been identified as the main driver for the lateral retreat appendix b figs 5 and 6 of the marsh platform waves suspend sediments from the mudflat and these sediments are transported landward during flood sediments deposited landward are subject to limited shear stresses due to the trapping capacity of the marsh so that ebb currents transport less sediment seaward during ebb this resulted in the heightening of the platform level as sediments filled the accommodation space therefore the variation in the wave height provides an understanding of the system behaviour under varying wave climates overall fig 7 d shows that larger waves produce steeper marsh edge transitions with a retreating edge conversely lower waves allow for larger sections of high marsh lands on which mature and stable plant species reside compared to purely tidal environments waves are seen to create higher platform levels when the wave height is lowered the geomorphological developments resemble that of a purely tidal environment in alternative wave climates this can be used as a tool in promoting growth within restoration strategies fig 8 d also reveals that an increase of the wave height lowers the vegetation coverage where the doubling of the wave height results in an approximate reduction of 1 106 stems year 3 3 2 roughness tidal amplitude and platform bed level a higher roughness lower chézy value leads to a concentration of flows generating narrower and deeper channels which prograde into the marsh area further additionally a higher roughness enhances the erosion of the mudflat and platform due to enhanced sediment suspension resulting in a retreating marsh edge and heightened platform fig 7 i within the morphological model the constant chézy value affects flow and sediment transport bare areas only and wave attenuation the roughness generated by the vegetation affects only the flow and sediment transport in marsh areas as the tidal amplitude is lowered with a constant platform height the degree to which the platform is flooded reduces therefore the mudflat is eroded because the waves dissipate all of their energy on the mudflat at the marsh edge continual wave dissipation results in the deepening of the mudflat level and the formation of steep transitions when the tidal amplitude equals or exceeds the magnitude of the marsh level the wave dissipation is limited within the area of the mean water high level and gentler transitions are formed at the marsh edge regarding the evolution of the stem growth lower tidal amplitudes and by extension lower inundation stresses allow for higher stem densities within the domain fig 8 the platform level variations show a greater affinity for the distribution of sediment along the transition area and mudflat whereas the variations in the amplitude allow for sediment to be deposited along both the marsh platform and also at the transition area see appendix c figure c2 3 3 3 sediment characteristics with the combination of both waves and tides the typical range for the shear stress lies between 0 3 and 0 5 n m2 this range is in line with validated model studies including wave action and tidal forces borsje et al 2011 cheon and suh 2016 macvean and lacy 2014 townend et al 2011 van der wegen et al 2016 results fig 7 c reveal that lower critical shear stress values lead to an eroding mudflat and a landward transport of sediments building up the marsh platform therefore restoration efforts which in practice simulate sufficient stirring of the sediment offshore will allow for increased deposition within the marsh lower fall velocities lead to a similar behaviour fig 7 h however though the marsh is elevated the width is reduced due to the lateral retreat of the platform the stem density is reduced with lower critical shear stress and fall velocity values fig 8 c the runs with a lower critical shear stress produce straight lined channel patterns with increased mortality rates however generally the modelled channel formation patterns and channel and marsh velocities are overestimated compared to the values found in literature kirwan and megonigal 2013 maan et al 2015 temmerman et al 2007 temmerman et al 2010 this may be attributed to the use of the roller model to represent the wave energy where due to the lack of diffraction considered along the one directional plane of movement the shear stresses are often in excess of 0 5n m2 especially during the flood tide the marsh platform seems not heavily affected by the boundary sediment concentrations fig 7 e but the mudflat is higher for a larger ssc at the boundary during flood the ssc levels are lower than the ssc imposed at the boundary 25 mg l in ebb flow higher ssc are observed in the channels along the mudflat with minimal values in the channels within the salt marsh as such sediments are deposited readily along the mudflat where fall velocities are optimum higher ssc values result in a filling of the accommodation space allowing for the progradation of the marsh fig 9 3 3 4 plant growth characteristics and biomass most morphological variation was seen in the critical inundation height runs fig 7 g followed by the plant height the less resilient the plant species is to inundation the lower the stem density the lower vegetation coverage then allows for greater deposition along the marsh as higher sediment loads are transported to the marsh fig 7 as the height of the vegetation increases the lower the flow velocities and the magnitude of channel incisions in the marsh platform this adds to the protection offered by the vegetation to the marsh mudflat edge during ebb flow as the reduced velocities will result in smaller magnitudes of erosion due to flow with lower plant heights there is increased erosion of the marsh edge as the transition steepens once the maximum stem density coverage is reached this results in a more pronounced formation of channels within the marsh platform channels in the marsh area become narrower limiting the transport of sediment into the marsh relative increases in the marsh platform are notably lower with the decrease in elevation at the marsh edge there is an overall regression of the marsh edge with continued dynamics at the boundary after 120 years model results show not only the heightening of the marsh platform but the gradual progradation of the marsh edge are observed as the accommodation space is filled for larger bio accumulation rates fig 7 f the validation of the plant growth and decay characteristics were carried out quantitatively based on comparisons with attema 2014 and monden 2010 3 4 slr scenarios in all three slr scenarios the landward low lying areas drown leaving zones of elevated vegetated land masses disconnected from the shore figs 10 and 11 see appendix b video 1 for the animation of the salt marsh mudflat base model supplemental materials the landward sections of the marsh are relatively low due to the levee formation closer to the marsh edge the profiles reveal that under the slr scenarios the bed level increases but at a rate that is insufficient to match the rate of slr as shown by the stem density plots in figs 10 and 11 as such there is a loss of the intertidal wetland over time initially quite gradual but then increases after 50 years of slr for the model without biomass contributions fig 10 b this translates to a landward shift of the salt marshes to survive where space is available the slr results in increased water depths which allow waves to propagate further into the salt marsh the larger tidal prism increases the flow velocities thereby enhancing seaward erosion and landward deposition sediment is being transported from the mudflat unto the marsh which heightens but drowns due to insufficient volumes under the exponential increase in the sea level there is a threshold below which the salt marshes can survive and this extends some 40 50 years for all scenarios fig 10 b however beyond this period the high water level begins to drown areas that once formed the upper intertidal areas the vegetation mortality now switches to one dominated by inundation stresses overall the salt marsh system without interventions will not survive the long term impacts of slr a comparison was carried out between the exponential and linear increases in the mean water level the exponential scenario did reflect an extended period for which the salt marshes were able to increase the bed level at a rate that exceeded the slr but the linear scenario only extended approximately 10 years of slr however after 100 years of slr the vegetation cover for both systems was the same it should be noted that this model was developed for sediment rich systems and not sediment poor systems such as in the east of the usa 4 discussion our schematized 2d depth averaged model satisfactorily describes the interaction between the waves tides sediment transport morphodynamics in delft3d flow and the bio accumulation and plant growth dynamics of the spartina anglica in matlab the research aimed to increase the process understanding of the salt marsh system and also allow for applications toward resilience measures under slr inspired by the dutch south western delta the bio geomorphological model reproduced a realistic salt marsh mudflat system in near equilibrium after a century while reaching a maximum stem density after 40 years the interaction between the mudflat and marsh area provides key details on the triggers for the geomorphological developments within the marsh mudflat system wave action was found to be the primary trigger for the sediment supply towards the salt marsh with the formation of steep marsh edge transitions as the waves enter the domain most of the energy is lost through wave breaking and dissipation along the mudflat sediment is continuously stirred and transported to the marsh area sediment deposits in the marsh in levee type patterns close to the channel edges and platform edge once the vegetation establishes and grows on the platform the roughness increases favouring more deposition on the platform more landward areas face less deposition as the elevation of the marsh platform increases the wave energy becomes concentrated around the mean water level this results in a dynamic oscillatory flow pattern which erodes the sediment and transports it along the edge of the marsh this observation is similar to that of bouma et al 2016 and bouma et al 2005 who suggested that larger height differences between the muddy marsh and the mudflat enhance the erosion rate especially in the presence of waves this sediment is later transported landward following the channels as the wave height increases there is greater transport of sediment towards the platform with steeper transitions between the marsh and the mudflat steeper transitions are as a result of the larger magnitude of the oscillatory wave forces this may be critical for restoration strategies which attempt to promote growth in alternative wave climates additionally when compared with a tidally dominant system coastal areas exposed to waves have higher marsh platforms a system s resilience to slr will depend on its ability to increase the bed level at a rate that exceeds the slr this rate is dependent on the inundation depth at high tide the external sediment supply and the organic deposition potential we found that the salt marsh mudflat system drowns under all imposed slr scenarios to varying degrees with variations in the sediment supply and biomass however the bio accumulation rate is the most critical parameter affecting the resilience under slr high bio accumulation rates even lead to marsh survival including the heightening and gradual progradation of the marsh fig 12 results show that the bio accumulation rate has a greater impact on bed level increases under slr once the vegetation density is maintained biomass rates 1 3 mm yr contribute substantially towards the overall bed level increases when compared to the sediment supply fig 13 however in hellegat salt marsh the biomass production is not sufficient to ensure survival under slr production rates in excess of 3 mm yr are unrealistic as the marsh has an existing belowground biomass capacity of 1 4 4 7 kg m2 rahman 2015 as such this must be supplemented by increases in the external sediment supply to achieve higher yearly accretion rates systems with an accretion balance exceeding 5 mm yr were found resilient while the marsh platform expands for values up to 9 mm yr moreover high accretion balances that consider the sediment budget external rivers offshore artificial supply and internal biomass storage and production are more effective in reducing vulnerability under slr our modelling approach allows for a closer analysis of the spatial dynamics of drowning thorne et al 2014 especially the levee type patterns evolving close to the channel and marsh edges that have a significant impact on the response of the system under slr these levee areas are the last to drown under slr scenarios overall the inclusion of the wave dynamics proved critical for the geo morphological developments of the marsh mudflat system and improved quantifications for resilience to slr further works should however explore the dynamics of the sediment budget for specific locations 5 conclusion our bio geomorphological modelling approach was able to reproduce a mudflat marsh system in equilibrium after 120 years the process based methodology allowed for a thorough sensitivity analysis of the model parameter space including hydrodynamic forcing sediment characteristics and vegetation dynamics with the belowground biomass accumulation our findings were quantitatively validated primarily against the findings of relevant modelling studies the model produced realistic channel formations with characteristic flood and ebb hydrodynamics wave action was shown to be a key process as it suspends sediments on the mudflat and transports them to the marsh platform during flood this resulted in the heightening of the marsh platform and the formation of steep marsh mudflat transitions higher wave heights produce narrower and more elevated marshlands our area model showed the evolution of levee type deposition patterns along the channels the critical shear stress settling velocity critical plant inundation height and tidal variations are key model parameters in the bio geomorphological development imposing 100 years of slr scenarios on the equilibrium profile allowed us to analyse key factors impacting the resilience of the marsh mudflat system to slr despite the net import of sediments and biomass productivity of the system all slr scenarios eventually led to the partial or complete drowning of the marsh mudflat system the channel networks expanded landward and incised the marsh platform the vegetated levee type patterns were the last features to survive exponential increases in slr showed extended periods in which the salt marshes were able to increase the bed level at a rate that exceeded the slr but the linear scenarios did not imposing higher accretion rates may allow the salt marsh survive slr scenarios another key to marsh survival under slr will stem from increasing the overall sediment supply allowing for higher yearly accumulation rates model approaches with ecological components enhance the process understanding and reinforce innovative solutions in the restoration and protection of these valued intertidal vegetation species future research may utilize process based approaches to evaluate engineering solutions for protection and restoration strategies and study the dynamics of other vegetation types like mangroves with applications to case specific areas software and or data availability we applied a process based numerical modelling approach which coupled offline delft 3d flow and matlab both the delft3d software and matlab tools used in this study are open source and freely available online https oss deltares nl web delft3d the delft3d suite was developed by deltares with the main office located at rotterdamseweg 185 2629 hd delft the netherlands contact can be made through the contact form https oss deltares nl web delft3d contact or via the sales department with the following email and contact number sales deltaressystems nl 31 0 88 335 8188 the delft3d flow flow morphology mor and waves wave modules were first made available in 2011 and is written using fortran and c c language rules lesser 2009 the running of the model requires the use of matlab versions 2013 or higher this software can be attained through purchase student version or trial online https nl mathworks com products matlab html contact can be made to mathworks the developer through its representative in the netherlands dr holtroplaan 5b phone 31 40 2156700 or via the corporate headquarters the sources of all datasets and parameter values used for the model developed have been provided in the methods section and in appendix a of the paper additionally the typical input files for the setup of the base model along with the plant growth script have been provided in the supplemental materials with regards to the hardware required a standard pc with minimum 8 gb ram acknowledgements i would first like to express my gratitude to the staff of coastal and port development department of ihe delft institute for water education and deltares who have been instrumental in shaping the framework for this research and generously gave of their time and efforts p w j m willemsen was supported by the research programme be safe nwo 850 13 010 financed primarily by the netherlands organisation for scientific research nwo 850 13 012 this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors appendix a supplementary data the following are the supplementary data related to this article model setup files for the base salt marsh mudflat model zip model setup files for the base salt marsh mudflat model zip appendices a b c and d appendices a b c and d appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 004 
26326,epidemiologic studies rely on accurately characterizing spatiotemporal variation in air pollutant concentrations this work presents two model fusion approaches that use publicly available chemical transport simulations dispersion model simulations and observations to estimate air pollutant concentrations at a neighborhood level spatial resolution while incorporating comprehensive chemistry and emissions sources the first method is additive and the alternative method is multiplicative these approaches are applied to atlanta ga at a 250 m grid resolution to obtain daily 24 hr averaged pm2 5 and 1 hr max co and nox concentrations during the years 2003 2008 for use in health studies the modeled concentrations provide comprehensive estimates with steep spatial gradients near roadways secondary formation and loss and effects of regional sources that can influence daily variation in ambient pollutant concentrations results show high temporal and spatial correlation and low biases across monitors providing accurate pollutant concentration estimates for epidemiologic analyses keywords air pollution model fusion fine spatial resolution cmaq rline dispersion models 1 introduction air pollution has been linked to adverse health effects including cardiorespiratory morbidity and mortality brunekreef and holgate 2002 delfino et al 2005 pope et al 2009 and adverse birth outcomes chang et al 2012 darrow et al 2011 a limitation of many population based health studies is the inability to estimate steep spatial gradients in intraurban pollutant exposures driven by local emission sources like vehicle traffic on roadways simulation studies show that an inability to accurately capture spatial variability in heterogeneous pollutants can bias risk ratio estimates in epidemiologic studies goldman et al 2011 and errors in exposure misclassification due to spatial variability assumptions are especially important for epidemiologic studies of long term pollutant exposures wilson et al 2005 reliance on air quality observational data for exposure analyses can introduce exposure measurement error because the lack of a dense monitoring network can lead to unobserved spatial variation in pollutant concentrations gryparis et al 2009 sarnat et al 2010 wilson et al 2005 air quality modeling addresses this problem by creating spatially and temporally resolved concentration fields constructed from simulating emissions chemistry physics and meteorology impacting pollutants however specific model results are often limited by either spatial resolution or an inability to capture complex chemistry and a vast array of emissions sources two types of emissions based models are commonly used chemical transport models and dispersion models e g gaussian plume and variants eulerian grid based chemical transport models simulate the transport and chemical transformation of pollutants emitted from thousands of emissions sources over a large spatial domain however because all emissions are evenly distributed within one computational grid often 36 km2 but seldom below 1 km2 these models do not simulate local effects of individual sources which can result in steep spatial gradients that occur on scales 1 km vehicles on roadways are an example of a local source that can drive steep spatial gradients in pollutants weijers et al 2004 59 5 million people lived within 500 m of heavily trafficked roads in 2010 and pm2 5 mass and component concentrations can double close to road sources beevers et al 2013 rowangould 2013 zhu et al 2002 these high concentrations on roadways are not discernable using low resolution grids leading to the need for additional exposure assessment methods to accurately characterize pollutant gradients in urban areas dispersion models can capture these gradients by using plume puff or particle representations but often do not take into account non linear chemical reactions that contribute to the formation of major pollutants like pm2 5 they are also not used to derive daily estimates over large spatial domains 1000s of km s due to model parameter limitations additionally all types of models are subject to biases from model parameters and inputs therefore neither chemical transport models nor dispersion models alone can estimate temporally and spatially resolved air pollutant concentrations with comprehensive emissions precursor information and chemistry reducing exposure misclassification by improving spatial and temporal resolution of air pollutant concentration estimates reducing model biases including emissions from all sources and simulating chemistry in best estimate simulations of concentrations is critical to minimizing error in epidemiologic studies different methods have been utilized to reduce error and improve spatial resolution of air pollutant concentration estimates at unmonitored locations land use regression lur variable models have proven effective tools for fine scale modeling by incorporating landscape characteristics such as elevation and distance to roadway with data from monitors and or dispersion models marshall et al 2008 michanowicz et al 2016 however lur models are specific to one study area with particular land use characteristics other modeling approaches utilize various techniques to combine observations and or different model outputs to represent intraurban air pollution one method uses linear combinations of wavelet basis functions to blend data from monitors crooks and isakov 2013 the photochemical model cmaq community multiscale air quality model byun and schere 2006 and the plume dispersion model aermod ams epa regulatory model cimorelli et al 2005 while another method nests the local dispersion model adms urban mchugh et al 1997 in the regional photochemical model camx comprehensive air quality model with extensions environ international corporation 2014 stocker et al 2014 hybrid methods add fine scale dispersion model outputs to broader scale estimates of pollutant concentrations often referred to as background concentrations previous hybrid studies have used observations from central monitoring stations after subtracting out concentrations due to local emissions for estimates of urban background stein et al 2007 other work utilized chemical transport models like cmaq run without local emissions to determine urban background dionisio et al 2013 stein et al 2007 an advanced method for calculating urban background was developed using space time ordinary kriging to combine monitoring data cmaq and cmaq with zeroed out emissions arunachalam et al 2014 these background estimates are added to model outputs from dispersion models like aermod and rline research line model snyder et al 2013 and or lagrangian models like hysplit hybrid single particle lagrangian integrated trajectory model that characterize primary concentrations from stationary and roadway sources chang et al 2017 dionisio et al 2013 stein et al 2007 care must be taken with these hybrid methods to prevent double counting of emissions in the dispersion models and background calculations here a method is developed for fusing multi model and observational data to estimate total pollutant concentrations from local and regional sources at a fine spatial resolution without needing to estimate the urban background a priori with a separate chemical transport model run without direct emissions of the pollutants of interest removing the need to simulate background concentrations using multiple photochemical air quality model runs but instead calculating urban background empirically can save computational time and prevent the methods from missing the chemistry and secondary formation or loss associated with local sources that chemical transport models cannot simulate if those emissions are zeroed out we present the development of two novel computationally efficient model fusion approaches to estimate air pollutant concentrations at a fine spatial resolution with comprehensive emissions and chemistry and their applications to the atlanta ga region each model fusion method developed in this work uses mathematical combinations of outputs from a chemical transport model that provides chemistry and local and regional sources and a dispersion model that provides fine spatial resolution simulations of inert pollutants from a local source along with limited observations as available one method is an additive approach and the other is a multiplicative approach both methods are applied to atlanta ga using simulations from the chemical transport model cmaq and rline rline is a steady state dispersion model that simulates near surface releases of primary and chemically inert pollutants from line sources like vehicles on roadways snyder et al 2013 and with 1 6 billion vehicles worldwide the association between mobile source emissions and disease development is of particular concern hei panel on the health effects of traffic related air pollution 2010 emissions from road transportation are estimated to be the largest source contribution to premature deaths due to pm2 5 pollution in the united states caiazzo et al 2013 and previous studies have shown that resolving steep spatial gradients in pollutant concentration near roadways is beneficial to epidemiologic studies chang et al 2017 which is why rline was chosen for the applications of these model fusion approaches results of the model fusion applications include a time series of daily concentration estimates of 1 hr co and nox and 24 average pm2 5 at a 250 m grid resolution across atlanta ga during the years 2003 2008 although the applications of these methods focus on vehicles as local sources the methods can be extended to other local non roadway facilities overall the inclusion of comprehensive regional and local sources and chemistry with fine scale spatial gradients can provide comprehensive estimates of pollutant concentrations reducing biases in exposure estimates used for epidemiologic analyses results are currently being used in spatiotemporal epidemiologic analyses of birth outcomes associated with air pollution and city planning and environmental justice studies investigating the relationships between air pollution health socio economic factors and infrastructure characteristics davis et al 2017 2 methods this work describes the development of two novel methods that fuse data from multiple models and observations to create comprehensive estimates of air pollutant concentrations at a fine spatial resolution the two model fusion approaches combine results from a chemical transport model ctm and dispersion model disp to obtain pollutant pm2 5 nox and co concentration estimates the model fusion approaches are applicable to pollutants whose small scale spatial variation is captured by the dispersion model although these model fusion methods could theoretically simulate many types of pollutants using inputs from different types of models the specific applications of these methods presented in this paper fuse annual average outputs from the local scale dispersion model rline 250 m resolution and daily simulations from the regional scale cmaq model 12 km resolution byun and schere 2006 to estimate daily 24 hr averaged pm2 5 and 1 hr maximum nox and co concentrations at a 250 m field resolution over atlanta ga from 2003 to 2008 cmaq results are used as the ctm inputs because cmaq is a state of the science eulerian grid chemical transport model rline is chosen as the dispersion model for these model fusion applications because roadway pollution is of interest to the epidemiologic analyses utilizing the results of this work and rline is a newly developed and well evaluated line source model developed to handle link based emissions for roadways due to the presence of biases in both models cmaq and rline results were adjusted using observations and data fusion methods a priori before model estimates were fused together friberg et al 2016 zhai et al 2016 such biases are present in the air quality model results due to model formulation e g rline is subject to high biases under low dispersion conditions zhai et al 2016 while cmaq biases exist due to errors in numerical dispersion byun and schere 2006 and model inputs like emissions data and meteorological fields both the data fused cmaq and observation calibrated rline results for the pollutants of interest during the years 2003 2008 are publicly available and have been evaluated in previous work friberg et al 2016 zhai et al 2016 although use of pre calibrated ctm and disp results is viewed as preferable due to biases in each individual model it is not a prerequisite for the applications of these model fusion methods furthermore while mobile sources are responsible for a majority of the urban area emissions of nox and co they have a smaller contribution to pm2 5 concentrations in the area which along with high biases on and near roadways in rline results for nox and co led to the development of slightly different model fusion approaches one method uses an additive approach and is applied to provide daily concentration fields of particulate matter pm2 5 which includes substantial secondary source contributions that are relatively spatially homogeneous the second method uses a multiplicative approach and is applied to estimate spatiotemporal fields of the gaseous pollutants carbon dioxide co and nitrogen oxides no no2 nox which have steep concentration gradients near roadways both approaches were tested with all three pollutants as a sensitivity analysis table s2 2 1 model fusion methods 2 1 1 additive method for fine particulate matter pm2 5 the first step in the additive model fusion method is to remove primary roadway pm2 5 from the ctm estimates to avoid double counting of roadway emissions included in both the ctm and disp outputs to achieve this the fine scale disp values are averaged into the coarser grid resolution matching the ctm computational grid for clarity the averaged disp values at the coarser resolution will be referred to as d i s p coarse d i s p coarse pm2 5 values which represent primary roadway pm2 5 concentrations are subtracted from the ctm estimates leaving an estimate of urban background as defined as all secondarily formed pm2 5 and primary pm2 5 from all sources excluding vehicles at the ctm grid resolution next results of this subtraction i e the urban background at the coarse ctm resolution are smoothed to the fine scale grid resolution matching the disp spatial resolution using a triangulation based linear interpolation these smoothed results represent urban background at a fine spatial resolution finally because dispersion processes are linear the initial disp estimates are added to the interpolated urban background at each matching grid location adding primary roadway pm2 5 back into the estimates and producing the final pollutant concentration approximations at the same resolution of the dispersion model eq 1 1 p m 2 5 c t m d i s p c o a r s e i n t e r p o l a t e d d i s p here pm2 5 is the pm2 5 concentration estimates at the desired fine spatial resolution resulting from the model fusion method which is the same resolution as the dispersion model ctm is the chemical transport model pm2 5 estimates d i s p coarse is the disp estimates averaged to match the grid resolution of the ctm and disp is the dispersion model estimates at the desired fine spatial resolution to summarize concentrations of primary roadway pm2 5 are placed in their respective locations inside ctm grids after removing average roadway primary pm2 5 from the ctm estimates to avoid double counting the result is daily concentration estimates of total fine particulate matter incorporating chemistry and complete local and regional emissions at a fine spatial resolution that matches the disp inputs 2 1 2 multiplicative method for gaseous pollutants co and nox in the application of the additive method to co and nox it was found that biases in the input models led to negative concentration estimates in urban background and final concentrations the averages of the dispersion model estimates at the coarser grid resolution i e the ctm grid resolution d i s p c o a r s e were higher than chemical transport model estimates at the same grid cell on certain days leading to negative concentrations when d i s p coarse values were subtracted from ctm estimates see si for details in general when specific locations within a 12 km grid cell have concentrations much higher than background leading to high d i s p coarse estimates or background is small compared to the variation captured in the dispersion model the additive method can lead to negative predictions due to errors in the model inputs to avoid this phenomenon instead of subtracting ctm estimates are divided by d i s p coarse values in the multiplicative method this division results in linear adjustment factors for each grid cell at the coarser resolution these adjustment factors physically represent the inverse fraction of total pollutant that is estimated to be primary from roadways the spatial field of these adjustment factors is smoothed to the fine grid scale of the disp estimates using triangulation based linear interpolation finally each adjustment factor at the fine spatial resolution is multiplied by the disp value at the matching grid cell to obtain final concentration estimates eq 2 2 g a s c t m d i s p c o a r s e i n t e r p o l a t e d d i s p to summarize this method rescales the disp estimates using the linear relationship between ctm estimates and disp values to obtain air pollutant estimates at the spatial resolution of the disp simulations if the linear adjustment factor is higher than one the total pollutant concentration estimated by the ctm is higher than the primary roadway concentration estimated by the d i s p coarse and the estimates from disp will be scaled higher to match the total concentration if the linear adjustment factor is below one the d i s p coarse estimate of primary roadway pollutant concentration is higher than the ctm estimate of total pollutant concentration implying that the disp model is biased high and the linear adjustment factor will scale the disp estimate down although applications of the multiplicative method were limited to the gaseous pollutants nox and co this method should be applied to any pollutant with biases in model input estimates such as overly rapid vertical dispersion in cmaq leading to biased low surface concentrations and excessive accumulation of pollutants due to wind direction aligning with roadway links in rline leading to biased high concentrations that lead to higher d i s p coarse than the ctm estimates 2 2 methods for inputs to model fusion processes for application to atlanta ga 2 2 1 chemical transport model input cmaq is a state of the science eulerian grid regional chemical transport model incorporating emissions from a myriad of sources and complex chemistry including gas particle partitioning byun and schere 2006 the use of cmaq in these model fusion methods accounts for photochemical evolution of the pollutants loss and formation as well as the impacts of regionally scaled sources in atlanta specifically much of the pm2 5 is formed through secondary processes such as the production of ammonium sulfate zhai et al 2017 secondary organic aerosol soa from both anthropogenic and biogenic sources kleindienst et al 2010 and co from the photochemical degradation of biogenic emissions choi et al 2010 hudman et al 2008 for the model fusion applications daily cmaq fields for 24 hr pm2 5 maximum daily 1 hr concentration of co and maximum daily 1 hr concentration nox during the years 2003 2008 were obtained from previous work by friberg et al 2016 that developed a data fusion method to optimize cmaq outputs in the friberg et al 2016 data fusion method observational data was blended with the environmental protection agency s epa phase cmaq runs at 12 km resolution cdc national environmental public health tracking network cmaq v4 7 carbon bond mechanism cb05 24 vertical layers emissions obtained from the 2002 national emissions inventory run with the sparse operator kernel emissions smoke meteorology obtained from the weather research and forecasting model wrf v3 3 1 briefly the algorithm in friberg et al 2016 utilized a weighted average of one method that corrected daily cmaq estimates for annual and seasonal biases cmaq1 and a second method the used temporal variance from observations and spatial structure from the annual mean cmaq field to obtain optimized air pollutant concentration fields cmaq2 the weight of the averages is based on a temporal variance that weights the second method cmaq2 heavier near monitors and the first method cmaq1 heavier away from monitors further details on this method and evaluation of this observation fused cmaq data set can be found in friberg et al 2016 the resulting simulations which we will refer to as obs cmaq exhibit much less bias in the three pollutants of interest daily 24 hr pm2 5 and 1 hr maximum co and nox than the raw epa phase cmaq simulations providing more accurate inputs to the model fusion approaches than raw cmaq for sensitivity analyses the model fusion methods were also run with cmaq outputs that were not calibrated with daily observations instead cmaq was only adjusted for annual means and seasonal biases using methods and regression coefficients described in detail in friberg et al 2016 in other words the model fusion methods were run with cmaq2 results for a sensitivity analysis and an estimate of the performance of the model fusion methods away from monitors where cmaq2 is weighted more heavily in the obs cmaq estimates 2 2 2 dispersion model input rline is a steady state line source dispersion model developed by the u s epa to simulate primary and chemically inert pollutants snyder et al 2013 rline was chosen as the dispersion model for the application of these model fusion methods because it is designed to simulate primary concentrations from line sources such as mobile traffic for the applications of the model fusion approaches publicly available annual average rline fields for the years 2003 2008 developed in previous work were used zhai et al 2016 briefly zhai et al 2016 ran rline using hourly meteorology generated for years 2003 2008 using aermet cimorelli et al 2005 u s environmental protection agency 2004 and aerminute u s environmental protection agency 2015 and link based emissions from the atlanta regional commission s arc 20 county activity based travel demand model d onofrio 2016 zhai et al 2016 utilized a modified version of the stability array star method to group meteorological variables including wind direction wind speed and monin obukhov length into 78 categories to simulate annual mean impacts of on road mobile sources on pollutant concentrations chang et al 2015 u s environmental protection agency 1997 zhai et al 2016 arc emissions were only available for 2010 and thus were scaled to prior years to account for changes in vehicular traffic and emissions roadway link pattern and vehicle type distribution did not change significantly during the study time period and annual average moves emissions for the 20 county atlanta area were available for each year of the study period so moves emissions were used to calculate year specific annual average link based emissions via scaling of 2010 arc emissions using these year adjusted annual average link based emissions and data on frequency of occurrence for each of the 78 meteorological categories for each year annual average rline concentration estimates at 250 m resolution were developed for 24 hr pm2 5 and 1 hr maximum co and nox for the years 2003 2008 over atlanta ga the rline results were found to be biased high so zhai et al 2016 developed a method to calibrate the results using linear regressions with observations in this case using five monitors for co and seven monitors for nox in the atlanta ga area pm2 5 from rline was calibrated using a linear regression of log transformed rline with chemical mass balance gas constraint cmb gc mobile source impact estimates derived using data from three monitor sites and a priori source profiles zhai et al 2016 this calibration substantially reduced the normalized mean bias of the model results so these calibrated simulations which we will refer to as obs rline were used in the model fusion approaches further description of rline calibration and evaluation of these methods and results can be found in zhai et al 2016 3 results the additive and multiplicative model fusion methods were applied to obtain daily estimates of 24 hr average pm2 5 and 1 hr maximum nox and co for atlanta ga during the years 2003 2008 the general mathematical equations for the model fusion methods eq 1 and eq 2 can be rewritten to apply to the specific inputs used in this paper and are expressed in equations 3 and 4 3 p m 2 5 x d o b s c m a q l d o b s r l i n e c o a r s e l y i n t e r p o l a t e d o b s r l i n e x y 4 g a s x d o b s c m a q l d o b s r l i n e c o a r s e l y i n t e r p o l a t e d o b s r l i n e x y where pollutant concentration estimates are provided at 250 m grid cells located at x for each day d daily obs cmaq and annual average r l i n e c o a r s e estimates are provided at 12 km grid cell locations l and annual average obs rline estimates are provided for each year y equations 3 and 4 are specific to the model fusion applications presented in this work using previously developed obs cmaq and obs rline data but the methods could be applied to other models at various grid and time scales the additive and multiplicative model fusion methods are represented graphically for an example year 2005 fig 1 s1 s2 annual average results for the same year are depicted for 1 hr maximum nox and co using the multiplicative method fig 2 plots of annual averages of each pollutant for all years of the study period can be found in the supplemental figures s3 5 major highways are clearly visible on spatial distribution maps of the resulting model fusion estimates for each pollutant figs 1 and 2 s3 5 indicating an ability of the model fusion methods to yield small scale spatial gradients in pollutant concentration consistently high concentrations in the central part of atlanta are due to vehicle emissions from the downtown connector where three interstates i 20 i 85 and i 75 merge into one eight lane highway the busiest section experienced over 272 000 vehicles per day in 2005 cimorelli et al 2005 other roadway traffic in the region a railyard and other major sources west of the downtown connector 3 1 monitoring data for evaluation performances of obs cmaq obs rline and model fusion results for each pollutant were evaluated using available urban and rural monitoring stations in the atlanta region eight monitors for pm2 5 five monitors for co and five monitors for nox these same monitoring sites were used in the data assimilation methods to reduce bias in the model inputs obs cmaq and obs rline but a sensitivity analyses was performed on the model fusion methods using cmaq inputs that had not been adjusted with daily observations observations were taken from monitors that are part of the central speciation network csn and southeast aerosol research and characterization network search network and monitors operated by the georgia environmental protection department ga epd table s1 edgerton et al 2005 hansen et al 2003 solomon et al 2014 there were multiple pm2 5 monitoring instruments at yorkville and south dekalb during the study period so observations from each monitor were averaged into one value for that site before evaluating model performance data were available daily or every 3 days depending on the site and available during the entire study time period years 2003 2008 except for one pm2 5 monitor and one co monitor each missing one year of data table s1 3 2 evaluation of model fusion methods results of the model fusion methods applications to pm2 5 nox and co in atlanta ga were compared to monitoring data to assess and evaluate model fusion performance using the publicly available obs cmaq and obs rline simulations tables 1 and 2 these model fusion data are being used in current epidemiologic and environmental justice studies model fusion concentration estimates were compared to available site day measurements paired in time and space during the entire study period years 2003 2008 evaluation statistics were calculated per available site day for each monitor averaged into annual values for each monitor and then averaged over all monitoring sites for each year during the study period the median minimum and maximum values over the time period are reported table 1 further evaluation of the model fusion algorithms was conducted by rerunning the model fusion methods for each pollutant over the entire study period years 2003 2008 with cmaq results that were adjusted for annual and seasonal biases using regression coefficients developed in friberg et al 2016 but do not incorporate the daily observations that were used in the evaluation these results allow the evaluation of the model fusion approaches against daily observations without using those same observations in the model fusion inputs obs cmaq providing a 100 withholding test for the model fusion results this evaluation can be used to assess model fusion performance away from monitors that rely on cmaq information more than observations due to the nature of the data fusion method used to develop obs cmaq friberg et al 2016 even without fusing cmaq inputs with daily observations the model fusion methods perform well with median normalized mean biases of 7 8 1 1 and 8 3 for 24 hr average pm2 5 1 hr maximum co and 1 hr maximum nox respectively across available observation site days during 2003 2008 table 1 temporal correlations were lower for the 100 withholding test than for the model fusion estimates with obs cmaq 100 withholding model fusion r 0 77 0 54 and 0 60 for pm2 5 co and nox respectively model fusion with obs cmaq r 0 99 0 93 0 98 for pm2 5 co and nox respectively table 1 these results show that for temporal correlation the inputs to the model fusion processes greatly impact evaluation statistics the model fusion algorithms combine two model simulations and do not adjust them with observations to reduce biases so adjustments with daily observations a priori increases temporal correlations however the biases and errors are relatively low for the 100 withholding test indicating that the model fusion methods are generally unbiased in capturing concentrations away from monitors where the obs cmaq inputs are not affected significantly by daily observations due to the data fusion method utilized and described in friberg et al 2016 3 3 comparison of model fusion results to obs cmaq performances of the applications of the model fusion methods to pm2 5 co and nox in atlanta were compared to the performance of obs cmaq to investigate the value of finer spatial resolution estimates tables 1 and 2 the performance metrics between the model fusion results and obs cmaq are comparable which is to be expected as the performance of the model fusion methods relies heavily on model inputs including obs cmaq there is general improvement in the medians and ranges of normalized mean bias and normalized mean error for model fused 1 hr maximum co and nox estimates compared to obs cmaq estimates these minimized biases and errors show the value in the model fusion methods capabilities to incorporate obs rline information and obtain finer spatial resolution performance metrics for 24 hr averaged pm2 5 do not vary significantly between obs cmaq and model fused results because mobile sources are not the dominant source of pm2 5 in the region most of the pm2 5 in the southeast is secondary including ammonium sulfate and secondary biogenic pollutants zhai et al 2017 obs rline only provides adjustments to the primary mobile source impacts while obs cmaq and monitor data include primary and secondary pm2 5 from all sources so obs rline contributes less information to total pm2 5 than to total nox and co which are dominated by vehicles emissions driving the larger performance increase between model fused and obs cmaq nox and co compared to model fused and obs cmaq pm2 5 nevertheless the model fused pm2 5 performs well compared to obs cmaq simulations and provides results at a 16 times finer spatial resolution in other words the model fusion methods perform well and estimate steep spatial gradients in concentrations that cannot be simulated by obs cmaq due to limitations of the coarse grid resolution spatial correlation analyses illustrate the contribution of both obs cmaq and obs rline to the model fusion results spatial correlation pearson s correlation coefficient values were calculated across monitoring locations using annual averages during the years 2003 2008 table 2 the spatial correlations for 1 hr maximum co and nox multiplicative model fusion results are substantially higher than spatial correlations for obs cmaq results due to the finer spatial resolution of the model fused estimates which incorporate obs rline results and more accurately capture the steep gradients in the pollutant concentrations furthermore spatial correlations for model fused estimates are larger than spatial correlation values for obs rline for each pollutant because the model fusion methods use obs cmaq to capture chemistry and sources that obs rline does not account for providing more comprehensive and accurate estimates note that the obs rline median spatial correlation for pm2 5 is low because this model only captures primary pm2 5 from roadways not total pm2 5 from multiple primary sources and secondary processes that are captured by monitors thus the simulations from both observation fused models used as inputs obs cmaq and obs rline contribute to overall performance of the model fusion methods finally the ranges of spatial correlations for 24 hr averaged pm2 5 are large and median values relatively small compared to nox and co because it is more difficult for models to capture small spatial variations secondary pollutant formation makes pm2 5 more spatially uniform than nox and co resulting in observations varying less between monitors additionally annual average model fusion estimates using obs cmaq and obs rline as inputs were compared to annual average obs cmaq simulations at each 250 m grid cell across the entire spatial domain rather than just monitoring locations to investigate the effects of obtaining finer spatial resolution estimates across the study area many 250 m model fusion results agree well with 12 km obs cmaq simulations as shown by the majority of model fusion values falling near a 1 1 line with obs cmaq values fig 3 but the 250 m model fusion estimates capture more extreme concentrations mostly at roadways that are averaged out in 12 km obs cmaq simulations but captured by obs rline vertical lines on the density scatter plots fig 3 show the ability of the model fusion methods to capture gradients within one 12 km grid cell by illustrating the distribution of model fusion estimates for one obs cmaq estimate many of the highest estimates are found directly on or near major roadways which are in accordance with observations beevers et al 2013 cong et al 2016 sarnat 2017 the multiplicative pollutant method was tested with 24 hr average pm2 5 to investigate the utility of the multiplicative pollutant model fusion method with other pollutants that have larger background contributions than nox and co overall performance for model fused 24 hr average pm2 5 estimates using the multiplicative method was poorer than the additive method table s2 this lower performance results from the many sources of particulate matter contributing to urban background so primary roadway pm2 5 is being scaled by a heterogeneous mixture of particulate matter including both primary emissions and secondary formation the multiplicative method can magnify errors when compared to the additive method further the additive method was applied to 1 hr maximum nox and co in spite of the occasions when some of the final concentrations go negative for comparison to the multiplicative approach overall the statistics between the additive and multiplicative methods are comparable for nox and co with a slightly more negative mean bias and higher mean error for the additive method however the incorrect negative concentration estimates using the additive approach persisted throughout the time period with 72 and 14 of the days for 1 hr maximum nox and co respectively producing at least one gird cell in the domain with a value below zero therefore when determining which model fusion method to use for a specific pollutant if d i s p coarse values are higher than ctm values at certain locations due to biased high dispersion model outputs and or biased low ctm values the multiplicative model fusion method should be used to avoid unphysical results in these cases the background concentrations are often very low i e a majority of the concentration of that pollutant is from primary vehicle emissions when using rline as the dispersion model input otherwise the additive model fusion method should be used to limit biases and errors however with relatively short run times both can be tested and evaluated against observations to determine the best method on an individual basis 4 discussion capturing steep spatial gradients in pollutant concentrations near roadways is critical for exposure assessment in epidemiologic studies monitoring stations are too sparse to accurately capture spatial gradients in pollutant concentration and can miss very high concentrations near roadways some air quality models can estimate fine spatial resolutions dispersion models but do not simulate atmospheric chemistry processes or incorporate regional emissions sources other models like chemical transport models can simulate atmospheric processing and comprehensive emission sources but typically at a coarse resolution 1 km the model fusion methods presented in this paper utilize the strengths of both types of models i e the regional chemical transport model cmaq and the line dispersion model rline and compares well to other air quality modeling techniques yu et al 2018 the time limiting factor of these model fusion approaches is running the chemical transport model and dispersion model for inputs but there is publicly available chemical transport and dispersion model data these methods limit computational time by avoiding the need to re run a chemical transport model with zeroed out emissions to obtain background concentration estimates arunachalam et al 2014 chang et al 2017 stein et al 2007 these model fusion methods present a relatively quick approach for obtaining finely resolved spatial concentration estimates that avoids double counting of emissions allowing for the development and evaluation of a long term series of daily data for multiple pollutants additionally by calculating urban background within the model fusion methods rather than a priori by zeroing out local emissions in cmaq the chemistry and secondary loss formation that arise from the local emissions are not sacrificed results shown in this paper are specific to daily 12 km obs cmaq and annual averaged 250 m obs rline inputs but neither spatial nor temporal resolutions of both the inputs and outputs are fixed in these methods spatial resolution can change by adjusting the interpolation step additionally these model fusion methods can be applied to other pollutants and can be used with simulations from different models as inputs if those data are more readily available or the focus of the study is capturing spatial gradients near local sources of emissions other than roadways there are three main limitations of the model fusion methods presented in this paper first the evaluation statistics specifically temporal correlation of these methods are dependent on the performance of the inputs specifically in the applications presented in this paper much of the daily variation in the results is due to the daily variation in the observations used to calibrate cmaq although raw cmaq rline or other model inputs can be used in the model fusion processes a thorough evaluation of model inputs and adjusting these fields to reduce biases using methods such as the ones presented in friberg et al 2016 and zhai et al 2016 are recommended before resulting model fields are fused in the model fusion methods additionally fusing input cmaq and rline with observations incorporates monitor information into estimating urban background and capturing the spatial gradients near roadways which the model fusion methods alone do not achieve simulating steep spatial gradients in the dispersion model is critical because the fusion methods rely on the dispersion model inputs to capture all fine scale spatial variation in the pollutant of interest second the interpolation step assumes that the urban background as defined as pollution derived from all sources except the primary mobile fraction can be linearly interpolated in space if the urban background consists mostly of spatially uniform pollutants that vary smoothly in space this assumption holds true in georgia roughly 60 of the pm2 5 mass is secondary which is spatially smooth zhai et al 2017 however if the background pollution is dominated by spatially heterogeneous pollutants with strong local sources surrounded by steep spatial gradients that are not captured in the dispersion model inputs this assumption may not hold true in this work nox and co are spatially heterogeneous in the study area but the major local source driving this heterogeneity is vehicles and is captured by the obs rline inputs and is thus not included in urban background finally for the multiplicative fusion method the dispcoarse values must all be non zero to avoid division by zero if dispcoarse is zero implying that there is no primary pollutant in that coarse grid cell estimated by the dispersion model i e if there is no primary pollution associated with vehicles on roadways using obs rline the ctm value should be assumed as the total concentration of the area without undergoing the model fusion process though this would not be expected to occur often and did not occur in these applications one limitation with evaluating these model fusion approaches is the lack of monitoring stations surrounding roadways there is very limited near roadway data to calibrate obs rline with to reduce bias in spatial gradient estimates which the model fusion approaches rely on for all fine scale spatial variation the lack of monitoring stations with daily data also limits the ability to evaluate the spatial gradients simulated by the model fusion methods on a daily basis using daily obs cmaq and annual averaged obs rline as inputs nevertheless these model fusion approaches compare well to available data and estimates near roadways are similar to measurements on roadways from other studies including the more recent near road monitoring study in atlanta sarnat 2017 overall the model fusion results capture high pollutant concentrations at roadways that obs cmaq alone cannot while also retaining complex chemistry and contributions from sources that obs rline does not simulate the fine spatial resolution of the resulting pollutant concentrations is useful for reducing exposure measurement error in health studies of spatially heterogeneous pollutants impacted by chemistry such as studies investigating pollutant exposure while living in close proximity to roadways more broadly these model fusion results have many applications for epidemiologic analyses city planning and environmental justice studies that require pollutant concentration data at a fine spatial resolution acknowledgements this publication was developed under assistance agreement no epa834799 awarded by the u s environmental protection agency to emory university and georgia institute of technology it has not been formally reviewed by epa the views expressed in this document are solely those of the authors and do not necessarily reflect those of the agency epa does not endorse any products or commercial services mentioned in this publication this paper was also made possible by the arcs foundation appendix a supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 008 conflicts of interest the authors declare no conflict of interest 
26326,epidemiologic studies rely on accurately characterizing spatiotemporal variation in air pollutant concentrations this work presents two model fusion approaches that use publicly available chemical transport simulations dispersion model simulations and observations to estimate air pollutant concentrations at a neighborhood level spatial resolution while incorporating comprehensive chemistry and emissions sources the first method is additive and the alternative method is multiplicative these approaches are applied to atlanta ga at a 250 m grid resolution to obtain daily 24 hr averaged pm2 5 and 1 hr max co and nox concentrations during the years 2003 2008 for use in health studies the modeled concentrations provide comprehensive estimates with steep spatial gradients near roadways secondary formation and loss and effects of regional sources that can influence daily variation in ambient pollutant concentrations results show high temporal and spatial correlation and low biases across monitors providing accurate pollutant concentration estimates for epidemiologic analyses keywords air pollution model fusion fine spatial resolution cmaq rline dispersion models 1 introduction air pollution has been linked to adverse health effects including cardiorespiratory morbidity and mortality brunekreef and holgate 2002 delfino et al 2005 pope et al 2009 and adverse birth outcomes chang et al 2012 darrow et al 2011 a limitation of many population based health studies is the inability to estimate steep spatial gradients in intraurban pollutant exposures driven by local emission sources like vehicle traffic on roadways simulation studies show that an inability to accurately capture spatial variability in heterogeneous pollutants can bias risk ratio estimates in epidemiologic studies goldman et al 2011 and errors in exposure misclassification due to spatial variability assumptions are especially important for epidemiologic studies of long term pollutant exposures wilson et al 2005 reliance on air quality observational data for exposure analyses can introduce exposure measurement error because the lack of a dense monitoring network can lead to unobserved spatial variation in pollutant concentrations gryparis et al 2009 sarnat et al 2010 wilson et al 2005 air quality modeling addresses this problem by creating spatially and temporally resolved concentration fields constructed from simulating emissions chemistry physics and meteorology impacting pollutants however specific model results are often limited by either spatial resolution or an inability to capture complex chemistry and a vast array of emissions sources two types of emissions based models are commonly used chemical transport models and dispersion models e g gaussian plume and variants eulerian grid based chemical transport models simulate the transport and chemical transformation of pollutants emitted from thousands of emissions sources over a large spatial domain however because all emissions are evenly distributed within one computational grid often 36 km2 but seldom below 1 km2 these models do not simulate local effects of individual sources which can result in steep spatial gradients that occur on scales 1 km vehicles on roadways are an example of a local source that can drive steep spatial gradients in pollutants weijers et al 2004 59 5 million people lived within 500 m of heavily trafficked roads in 2010 and pm2 5 mass and component concentrations can double close to road sources beevers et al 2013 rowangould 2013 zhu et al 2002 these high concentrations on roadways are not discernable using low resolution grids leading to the need for additional exposure assessment methods to accurately characterize pollutant gradients in urban areas dispersion models can capture these gradients by using plume puff or particle representations but often do not take into account non linear chemical reactions that contribute to the formation of major pollutants like pm2 5 they are also not used to derive daily estimates over large spatial domains 1000s of km s due to model parameter limitations additionally all types of models are subject to biases from model parameters and inputs therefore neither chemical transport models nor dispersion models alone can estimate temporally and spatially resolved air pollutant concentrations with comprehensive emissions precursor information and chemistry reducing exposure misclassification by improving spatial and temporal resolution of air pollutant concentration estimates reducing model biases including emissions from all sources and simulating chemistry in best estimate simulations of concentrations is critical to minimizing error in epidemiologic studies different methods have been utilized to reduce error and improve spatial resolution of air pollutant concentration estimates at unmonitored locations land use regression lur variable models have proven effective tools for fine scale modeling by incorporating landscape characteristics such as elevation and distance to roadway with data from monitors and or dispersion models marshall et al 2008 michanowicz et al 2016 however lur models are specific to one study area with particular land use characteristics other modeling approaches utilize various techniques to combine observations and or different model outputs to represent intraurban air pollution one method uses linear combinations of wavelet basis functions to blend data from monitors crooks and isakov 2013 the photochemical model cmaq community multiscale air quality model byun and schere 2006 and the plume dispersion model aermod ams epa regulatory model cimorelli et al 2005 while another method nests the local dispersion model adms urban mchugh et al 1997 in the regional photochemical model camx comprehensive air quality model with extensions environ international corporation 2014 stocker et al 2014 hybrid methods add fine scale dispersion model outputs to broader scale estimates of pollutant concentrations often referred to as background concentrations previous hybrid studies have used observations from central monitoring stations after subtracting out concentrations due to local emissions for estimates of urban background stein et al 2007 other work utilized chemical transport models like cmaq run without local emissions to determine urban background dionisio et al 2013 stein et al 2007 an advanced method for calculating urban background was developed using space time ordinary kriging to combine monitoring data cmaq and cmaq with zeroed out emissions arunachalam et al 2014 these background estimates are added to model outputs from dispersion models like aermod and rline research line model snyder et al 2013 and or lagrangian models like hysplit hybrid single particle lagrangian integrated trajectory model that characterize primary concentrations from stationary and roadway sources chang et al 2017 dionisio et al 2013 stein et al 2007 care must be taken with these hybrid methods to prevent double counting of emissions in the dispersion models and background calculations here a method is developed for fusing multi model and observational data to estimate total pollutant concentrations from local and regional sources at a fine spatial resolution without needing to estimate the urban background a priori with a separate chemical transport model run without direct emissions of the pollutants of interest removing the need to simulate background concentrations using multiple photochemical air quality model runs but instead calculating urban background empirically can save computational time and prevent the methods from missing the chemistry and secondary formation or loss associated with local sources that chemical transport models cannot simulate if those emissions are zeroed out we present the development of two novel computationally efficient model fusion approaches to estimate air pollutant concentrations at a fine spatial resolution with comprehensive emissions and chemistry and their applications to the atlanta ga region each model fusion method developed in this work uses mathematical combinations of outputs from a chemical transport model that provides chemistry and local and regional sources and a dispersion model that provides fine spatial resolution simulations of inert pollutants from a local source along with limited observations as available one method is an additive approach and the other is a multiplicative approach both methods are applied to atlanta ga using simulations from the chemical transport model cmaq and rline rline is a steady state dispersion model that simulates near surface releases of primary and chemically inert pollutants from line sources like vehicles on roadways snyder et al 2013 and with 1 6 billion vehicles worldwide the association between mobile source emissions and disease development is of particular concern hei panel on the health effects of traffic related air pollution 2010 emissions from road transportation are estimated to be the largest source contribution to premature deaths due to pm2 5 pollution in the united states caiazzo et al 2013 and previous studies have shown that resolving steep spatial gradients in pollutant concentration near roadways is beneficial to epidemiologic studies chang et al 2017 which is why rline was chosen for the applications of these model fusion approaches results of the model fusion applications include a time series of daily concentration estimates of 1 hr co and nox and 24 average pm2 5 at a 250 m grid resolution across atlanta ga during the years 2003 2008 although the applications of these methods focus on vehicles as local sources the methods can be extended to other local non roadway facilities overall the inclusion of comprehensive regional and local sources and chemistry with fine scale spatial gradients can provide comprehensive estimates of pollutant concentrations reducing biases in exposure estimates used for epidemiologic analyses results are currently being used in spatiotemporal epidemiologic analyses of birth outcomes associated with air pollution and city planning and environmental justice studies investigating the relationships between air pollution health socio economic factors and infrastructure characteristics davis et al 2017 2 methods this work describes the development of two novel methods that fuse data from multiple models and observations to create comprehensive estimates of air pollutant concentrations at a fine spatial resolution the two model fusion approaches combine results from a chemical transport model ctm and dispersion model disp to obtain pollutant pm2 5 nox and co concentration estimates the model fusion approaches are applicable to pollutants whose small scale spatial variation is captured by the dispersion model although these model fusion methods could theoretically simulate many types of pollutants using inputs from different types of models the specific applications of these methods presented in this paper fuse annual average outputs from the local scale dispersion model rline 250 m resolution and daily simulations from the regional scale cmaq model 12 km resolution byun and schere 2006 to estimate daily 24 hr averaged pm2 5 and 1 hr maximum nox and co concentrations at a 250 m field resolution over atlanta ga from 2003 to 2008 cmaq results are used as the ctm inputs because cmaq is a state of the science eulerian grid chemical transport model rline is chosen as the dispersion model for these model fusion applications because roadway pollution is of interest to the epidemiologic analyses utilizing the results of this work and rline is a newly developed and well evaluated line source model developed to handle link based emissions for roadways due to the presence of biases in both models cmaq and rline results were adjusted using observations and data fusion methods a priori before model estimates were fused together friberg et al 2016 zhai et al 2016 such biases are present in the air quality model results due to model formulation e g rline is subject to high biases under low dispersion conditions zhai et al 2016 while cmaq biases exist due to errors in numerical dispersion byun and schere 2006 and model inputs like emissions data and meteorological fields both the data fused cmaq and observation calibrated rline results for the pollutants of interest during the years 2003 2008 are publicly available and have been evaluated in previous work friberg et al 2016 zhai et al 2016 although use of pre calibrated ctm and disp results is viewed as preferable due to biases in each individual model it is not a prerequisite for the applications of these model fusion methods furthermore while mobile sources are responsible for a majority of the urban area emissions of nox and co they have a smaller contribution to pm2 5 concentrations in the area which along with high biases on and near roadways in rline results for nox and co led to the development of slightly different model fusion approaches one method uses an additive approach and is applied to provide daily concentration fields of particulate matter pm2 5 which includes substantial secondary source contributions that are relatively spatially homogeneous the second method uses a multiplicative approach and is applied to estimate spatiotemporal fields of the gaseous pollutants carbon dioxide co and nitrogen oxides no no2 nox which have steep concentration gradients near roadways both approaches were tested with all three pollutants as a sensitivity analysis table s2 2 1 model fusion methods 2 1 1 additive method for fine particulate matter pm2 5 the first step in the additive model fusion method is to remove primary roadway pm2 5 from the ctm estimates to avoid double counting of roadway emissions included in both the ctm and disp outputs to achieve this the fine scale disp values are averaged into the coarser grid resolution matching the ctm computational grid for clarity the averaged disp values at the coarser resolution will be referred to as d i s p coarse d i s p coarse pm2 5 values which represent primary roadway pm2 5 concentrations are subtracted from the ctm estimates leaving an estimate of urban background as defined as all secondarily formed pm2 5 and primary pm2 5 from all sources excluding vehicles at the ctm grid resolution next results of this subtraction i e the urban background at the coarse ctm resolution are smoothed to the fine scale grid resolution matching the disp spatial resolution using a triangulation based linear interpolation these smoothed results represent urban background at a fine spatial resolution finally because dispersion processes are linear the initial disp estimates are added to the interpolated urban background at each matching grid location adding primary roadway pm2 5 back into the estimates and producing the final pollutant concentration approximations at the same resolution of the dispersion model eq 1 1 p m 2 5 c t m d i s p c o a r s e i n t e r p o l a t e d d i s p here pm2 5 is the pm2 5 concentration estimates at the desired fine spatial resolution resulting from the model fusion method which is the same resolution as the dispersion model ctm is the chemical transport model pm2 5 estimates d i s p coarse is the disp estimates averaged to match the grid resolution of the ctm and disp is the dispersion model estimates at the desired fine spatial resolution to summarize concentrations of primary roadway pm2 5 are placed in their respective locations inside ctm grids after removing average roadway primary pm2 5 from the ctm estimates to avoid double counting the result is daily concentration estimates of total fine particulate matter incorporating chemistry and complete local and regional emissions at a fine spatial resolution that matches the disp inputs 2 1 2 multiplicative method for gaseous pollutants co and nox in the application of the additive method to co and nox it was found that biases in the input models led to negative concentration estimates in urban background and final concentrations the averages of the dispersion model estimates at the coarser grid resolution i e the ctm grid resolution d i s p c o a r s e were higher than chemical transport model estimates at the same grid cell on certain days leading to negative concentrations when d i s p coarse values were subtracted from ctm estimates see si for details in general when specific locations within a 12 km grid cell have concentrations much higher than background leading to high d i s p coarse estimates or background is small compared to the variation captured in the dispersion model the additive method can lead to negative predictions due to errors in the model inputs to avoid this phenomenon instead of subtracting ctm estimates are divided by d i s p coarse values in the multiplicative method this division results in linear adjustment factors for each grid cell at the coarser resolution these adjustment factors physically represent the inverse fraction of total pollutant that is estimated to be primary from roadways the spatial field of these adjustment factors is smoothed to the fine grid scale of the disp estimates using triangulation based linear interpolation finally each adjustment factor at the fine spatial resolution is multiplied by the disp value at the matching grid cell to obtain final concentration estimates eq 2 2 g a s c t m d i s p c o a r s e i n t e r p o l a t e d d i s p to summarize this method rescales the disp estimates using the linear relationship between ctm estimates and disp values to obtain air pollutant estimates at the spatial resolution of the disp simulations if the linear adjustment factor is higher than one the total pollutant concentration estimated by the ctm is higher than the primary roadway concentration estimated by the d i s p coarse and the estimates from disp will be scaled higher to match the total concentration if the linear adjustment factor is below one the d i s p coarse estimate of primary roadway pollutant concentration is higher than the ctm estimate of total pollutant concentration implying that the disp model is biased high and the linear adjustment factor will scale the disp estimate down although applications of the multiplicative method were limited to the gaseous pollutants nox and co this method should be applied to any pollutant with biases in model input estimates such as overly rapid vertical dispersion in cmaq leading to biased low surface concentrations and excessive accumulation of pollutants due to wind direction aligning with roadway links in rline leading to biased high concentrations that lead to higher d i s p coarse than the ctm estimates 2 2 methods for inputs to model fusion processes for application to atlanta ga 2 2 1 chemical transport model input cmaq is a state of the science eulerian grid regional chemical transport model incorporating emissions from a myriad of sources and complex chemistry including gas particle partitioning byun and schere 2006 the use of cmaq in these model fusion methods accounts for photochemical evolution of the pollutants loss and formation as well as the impacts of regionally scaled sources in atlanta specifically much of the pm2 5 is formed through secondary processes such as the production of ammonium sulfate zhai et al 2017 secondary organic aerosol soa from both anthropogenic and biogenic sources kleindienst et al 2010 and co from the photochemical degradation of biogenic emissions choi et al 2010 hudman et al 2008 for the model fusion applications daily cmaq fields for 24 hr pm2 5 maximum daily 1 hr concentration of co and maximum daily 1 hr concentration nox during the years 2003 2008 were obtained from previous work by friberg et al 2016 that developed a data fusion method to optimize cmaq outputs in the friberg et al 2016 data fusion method observational data was blended with the environmental protection agency s epa phase cmaq runs at 12 km resolution cdc national environmental public health tracking network cmaq v4 7 carbon bond mechanism cb05 24 vertical layers emissions obtained from the 2002 national emissions inventory run with the sparse operator kernel emissions smoke meteorology obtained from the weather research and forecasting model wrf v3 3 1 briefly the algorithm in friberg et al 2016 utilized a weighted average of one method that corrected daily cmaq estimates for annual and seasonal biases cmaq1 and a second method the used temporal variance from observations and spatial structure from the annual mean cmaq field to obtain optimized air pollutant concentration fields cmaq2 the weight of the averages is based on a temporal variance that weights the second method cmaq2 heavier near monitors and the first method cmaq1 heavier away from monitors further details on this method and evaluation of this observation fused cmaq data set can be found in friberg et al 2016 the resulting simulations which we will refer to as obs cmaq exhibit much less bias in the three pollutants of interest daily 24 hr pm2 5 and 1 hr maximum co and nox than the raw epa phase cmaq simulations providing more accurate inputs to the model fusion approaches than raw cmaq for sensitivity analyses the model fusion methods were also run with cmaq outputs that were not calibrated with daily observations instead cmaq was only adjusted for annual means and seasonal biases using methods and regression coefficients described in detail in friberg et al 2016 in other words the model fusion methods were run with cmaq2 results for a sensitivity analysis and an estimate of the performance of the model fusion methods away from monitors where cmaq2 is weighted more heavily in the obs cmaq estimates 2 2 2 dispersion model input rline is a steady state line source dispersion model developed by the u s epa to simulate primary and chemically inert pollutants snyder et al 2013 rline was chosen as the dispersion model for the application of these model fusion methods because it is designed to simulate primary concentrations from line sources such as mobile traffic for the applications of the model fusion approaches publicly available annual average rline fields for the years 2003 2008 developed in previous work were used zhai et al 2016 briefly zhai et al 2016 ran rline using hourly meteorology generated for years 2003 2008 using aermet cimorelli et al 2005 u s environmental protection agency 2004 and aerminute u s environmental protection agency 2015 and link based emissions from the atlanta regional commission s arc 20 county activity based travel demand model d onofrio 2016 zhai et al 2016 utilized a modified version of the stability array star method to group meteorological variables including wind direction wind speed and monin obukhov length into 78 categories to simulate annual mean impacts of on road mobile sources on pollutant concentrations chang et al 2015 u s environmental protection agency 1997 zhai et al 2016 arc emissions were only available for 2010 and thus were scaled to prior years to account for changes in vehicular traffic and emissions roadway link pattern and vehicle type distribution did not change significantly during the study time period and annual average moves emissions for the 20 county atlanta area were available for each year of the study period so moves emissions were used to calculate year specific annual average link based emissions via scaling of 2010 arc emissions using these year adjusted annual average link based emissions and data on frequency of occurrence for each of the 78 meteorological categories for each year annual average rline concentration estimates at 250 m resolution were developed for 24 hr pm2 5 and 1 hr maximum co and nox for the years 2003 2008 over atlanta ga the rline results were found to be biased high so zhai et al 2016 developed a method to calibrate the results using linear regressions with observations in this case using five monitors for co and seven monitors for nox in the atlanta ga area pm2 5 from rline was calibrated using a linear regression of log transformed rline with chemical mass balance gas constraint cmb gc mobile source impact estimates derived using data from three monitor sites and a priori source profiles zhai et al 2016 this calibration substantially reduced the normalized mean bias of the model results so these calibrated simulations which we will refer to as obs rline were used in the model fusion approaches further description of rline calibration and evaluation of these methods and results can be found in zhai et al 2016 3 results the additive and multiplicative model fusion methods were applied to obtain daily estimates of 24 hr average pm2 5 and 1 hr maximum nox and co for atlanta ga during the years 2003 2008 the general mathematical equations for the model fusion methods eq 1 and eq 2 can be rewritten to apply to the specific inputs used in this paper and are expressed in equations 3 and 4 3 p m 2 5 x d o b s c m a q l d o b s r l i n e c o a r s e l y i n t e r p o l a t e d o b s r l i n e x y 4 g a s x d o b s c m a q l d o b s r l i n e c o a r s e l y i n t e r p o l a t e d o b s r l i n e x y where pollutant concentration estimates are provided at 250 m grid cells located at x for each day d daily obs cmaq and annual average r l i n e c o a r s e estimates are provided at 12 km grid cell locations l and annual average obs rline estimates are provided for each year y equations 3 and 4 are specific to the model fusion applications presented in this work using previously developed obs cmaq and obs rline data but the methods could be applied to other models at various grid and time scales the additive and multiplicative model fusion methods are represented graphically for an example year 2005 fig 1 s1 s2 annual average results for the same year are depicted for 1 hr maximum nox and co using the multiplicative method fig 2 plots of annual averages of each pollutant for all years of the study period can be found in the supplemental figures s3 5 major highways are clearly visible on spatial distribution maps of the resulting model fusion estimates for each pollutant figs 1 and 2 s3 5 indicating an ability of the model fusion methods to yield small scale spatial gradients in pollutant concentration consistently high concentrations in the central part of atlanta are due to vehicle emissions from the downtown connector where three interstates i 20 i 85 and i 75 merge into one eight lane highway the busiest section experienced over 272 000 vehicles per day in 2005 cimorelli et al 2005 other roadway traffic in the region a railyard and other major sources west of the downtown connector 3 1 monitoring data for evaluation performances of obs cmaq obs rline and model fusion results for each pollutant were evaluated using available urban and rural monitoring stations in the atlanta region eight monitors for pm2 5 five monitors for co and five monitors for nox these same monitoring sites were used in the data assimilation methods to reduce bias in the model inputs obs cmaq and obs rline but a sensitivity analyses was performed on the model fusion methods using cmaq inputs that had not been adjusted with daily observations observations were taken from monitors that are part of the central speciation network csn and southeast aerosol research and characterization network search network and monitors operated by the georgia environmental protection department ga epd table s1 edgerton et al 2005 hansen et al 2003 solomon et al 2014 there were multiple pm2 5 monitoring instruments at yorkville and south dekalb during the study period so observations from each monitor were averaged into one value for that site before evaluating model performance data were available daily or every 3 days depending on the site and available during the entire study time period years 2003 2008 except for one pm2 5 monitor and one co monitor each missing one year of data table s1 3 2 evaluation of model fusion methods results of the model fusion methods applications to pm2 5 nox and co in atlanta ga were compared to monitoring data to assess and evaluate model fusion performance using the publicly available obs cmaq and obs rline simulations tables 1 and 2 these model fusion data are being used in current epidemiologic and environmental justice studies model fusion concentration estimates were compared to available site day measurements paired in time and space during the entire study period years 2003 2008 evaluation statistics were calculated per available site day for each monitor averaged into annual values for each monitor and then averaged over all monitoring sites for each year during the study period the median minimum and maximum values over the time period are reported table 1 further evaluation of the model fusion algorithms was conducted by rerunning the model fusion methods for each pollutant over the entire study period years 2003 2008 with cmaq results that were adjusted for annual and seasonal biases using regression coefficients developed in friberg et al 2016 but do not incorporate the daily observations that were used in the evaluation these results allow the evaluation of the model fusion approaches against daily observations without using those same observations in the model fusion inputs obs cmaq providing a 100 withholding test for the model fusion results this evaluation can be used to assess model fusion performance away from monitors that rely on cmaq information more than observations due to the nature of the data fusion method used to develop obs cmaq friberg et al 2016 even without fusing cmaq inputs with daily observations the model fusion methods perform well with median normalized mean biases of 7 8 1 1 and 8 3 for 24 hr average pm2 5 1 hr maximum co and 1 hr maximum nox respectively across available observation site days during 2003 2008 table 1 temporal correlations were lower for the 100 withholding test than for the model fusion estimates with obs cmaq 100 withholding model fusion r 0 77 0 54 and 0 60 for pm2 5 co and nox respectively model fusion with obs cmaq r 0 99 0 93 0 98 for pm2 5 co and nox respectively table 1 these results show that for temporal correlation the inputs to the model fusion processes greatly impact evaluation statistics the model fusion algorithms combine two model simulations and do not adjust them with observations to reduce biases so adjustments with daily observations a priori increases temporal correlations however the biases and errors are relatively low for the 100 withholding test indicating that the model fusion methods are generally unbiased in capturing concentrations away from monitors where the obs cmaq inputs are not affected significantly by daily observations due to the data fusion method utilized and described in friberg et al 2016 3 3 comparison of model fusion results to obs cmaq performances of the applications of the model fusion methods to pm2 5 co and nox in atlanta were compared to the performance of obs cmaq to investigate the value of finer spatial resolution estimates tables 1 and 2 the performance metrics between the model fusion results and obs cmaq are comparable which is to be expected as the performance of the model fusion methods relies heavily on model inputs including obs cmaq there is general improvement in the medians and ranges of normalized mean bias and normalized mean error for model fused 1 hr maximum co and nox estimates compared to obs cmaq estimates these minimized biases and errors show the value in the model fusion methods capabilities to incorporate obs rline information and obtain finer spatial resolution performance metrics for 24 hr averaged pm2 5 do not vary significantly between obs cmaq and model fused results because mobile sources are not the dominant source of pm2 5 in the region most of the pm2 5 in the southeast is secondary including ammonium sulfate and secondary biogenic pollutants zhai et al 2017 obs rline only provides adjustments to the primary mobile source impacts while obs cmaq and monitor data include primary and secondary pm2 5 from all sources so obs rline contributes less information to total pm2 5 than to total nox and co which are dominated by vehicles emissions driving the larger performance increase between model fused and obs cmaq nox and co compared to model fused and obs cmaq pm2 5 nevertheless the model fused pm2 5 performs well compared to obs cmaq simulations and provides results at a 16 times finer spatial resolution in other words the model fusion methods perform well and estimate steep spatial gradients in concentrations that cannot be simulated by obs cmaq due to limitations of the coarse grid resolution spatial correlation analyses illustrate the contribution of both obs cmaq and obs rline to the model fusion results spatial correlation pearson s correlation coefficient values were calculated across monitoring locations using annual averages during the years 2003 2008 table 2 the spatial correlations for 1 hr maximum co and nox multiplicative model fusion results are substantially higher than spatial correlations for obs cmaq results due to the finer spatial resolution of the model fused estimates which incorporate obs rline results and more accurately capture the steep gradients in the pollutant concentrations furthermore spatial correlations for model fused estimates are larger than spatial correlation values for obs rline for each pollutant because the model fusion methods use obs cmaq to capture chemistry and sources that obs rline does not account for providing more comprehensive and accurate estimates note that the obs rline median spatial correlation for pm2 5 is low because this model only captures primary pm2 5 from roadways not total pm2 5 from multiple primary sources and secondary processes that are captured by monitors thus the simulations from both observation fused models used as inputs obs cmaq and obs rline contribute to overall performance of the model fusion methods finally the ranges of spatial correlations for 24 hr averaged pm2 5 are large and median values relatively small compared to nox and co because it is more difficult for models to capture small spatial variations secondary pollutant formation makes pm2 5 more spatially uniform than nox and co resulting in observations varying less between monitors additionally annual average model fusion estimates using obs cmaq and obs rline as inputs were compared to annual average obs cmaq simulations at each 250 m grid cell across the entire spatial domain rather than just monitoring locations to investigate the effects of obtaining finer spatial resolution estimates across the study area many 250 m model fusion results agree well with 12 km obs cmaq simulations as shown by the majority of model fusion values falling near a 1 1 line with obs cmaq values fig 3 but the 250 m model fusion estimates capture more extreme concentrations mostly at roadways that are averaged out in 12 km obs cmaq simulations but captured by obs rline vertical lines on the density scatter plots fig 3 show the ability of the model fusion methods to capture gradients within one 12 km grid cell by illustrating the distribution of model fusion estimates for one obs cmaq estimate many of the highest estimates are found directly on or near major roadways which are in accordance with observations beevers et al 2013 cong et al 2016 sarnat 2017 the multiplicative pollutant method was tested with 24 hr average pm2 5 to investigate the utility of the multiplicative pollutant model fusion method with other pollutants that have larger background contributions than nox and co overall performance for model fused 24 hr average pm2 5 estimates using the multiplicative method was poorer than the additive method table s2 this lower performance results from the many sources of particulate matter contributing to urban background so primary roadway pm2 5 is being scaled by a heterogeneous mixture of particulate matter including both primary emissions and secondary formation the multiplicative method can magnify errors when compared to the additive method further the additive method was applied to 1 hr maximum nox and co in spite of the occasions when some of the final concentrations go negative for comparison to the multiplicative approach overall the statistics between the additive and multiplicative methods are comparable for nox and co with a slightly more negative mean bias and higher mean error for the additive method however the incorrect negative concentration estimates using the additive approach persisted throughout the time period with 72 and 14 of the days for 1 hr maximum nox and co respectively producing at least one gird cell in the domain with a value below zero therefore when determining which model fusion method to use for a specific pollutant if d i s p coarse values are higher than ctm values at certain locations due to biased high dispersion model outputs and or biased low ctm values the multiplicative model fusion method should be used to avoid unphysical results in these cases the background concentrations are often very low i e a majority of the concentration of that pollutant is from primary vehicle emissions when using rline as the dispersion model input otherwise the additive model fusion method should be used to limit biases and errors however with relatively short run times both can be tested and evaluated against observations to determine the best method on an individual basis 4 discussion capturing steep spatial gradients in pollutant concentrations near roadways is critical for exposure assessment in epidemiologic studies monitoring stations are too sparse to accurately capture spatial gradients in pollutant concentration and can miss very high concentrations near roadways some air quality models can estimate fine spatial resolutions dispersion models but do not simulate atmospheric chemistry processes or incorporate regional emissions sources other models like chemical transport models can simulate atmospheric processing and comprehensive emission sources but typically at a coarse resolution 1 km the model fusion methods presented in this paper utilize the strengths of both types of models i e the regional chemical transport model cmaq and the line dispersion model rline and compares well to other air quality modeling techniques yu et al 2018 the time limiting factor of these model fusion approaches is running the chemical transport model and dispersion model for inputs but there is publicly available chemical transport and dispersion model data these methods limit computational time by avoiding the need to re run a chemical transport model with zeroed out emissions to obtain background concentration estimates arunachalam et al 2014 chang et al 2017 stein et al 2007 these model fusion methods present a relatively quick approach for obtaining finely resolved spatial concentration estimates that avoids double counting of emissions allowing for the development and evaluation of a long term series of daily data for multiple pollutants additionally by calculating urban background within the model fusion methods rather than a priori by zeroing out local emissions in cmaq the chemistry and secondary loss formation that arise from the local emissions are not sacrificed results shown in this paper are specific to daily 12 km obs cmaq and annual averaged 250 m obs rline inputs but neither spatial nor temporal resolutions of both the inputs and outputs are fixed in these methods spatial resolution can change by adjusting the interpolation step additionally these model fusion methods can be applied to other pollutants and can be used with simulations from different models as inputs if those data are more readily available or the focus of the study is capturing spatial gradients near local sources of emissions other than roadways there are three main limitations of the model fusion methods presented in this paper first the evaluation statistics specifically temporal correlation of these methods are dependent on the performance of the inputs specifically in the applications presented in this paper much of the daily variation in the results is due to the daily variation in the observations used to calibrate cmaq although raw cmaq rline or other model inputs can be used in the model fusion processes a thorough evaluation of model inputs and adjusting these fields to reduce biases using methods such as the ones presented in friberg et al 2016 and zhai et al 2016 are recommended before resulting model fields are fused in the model fusion methods additionally fusing input cmaq and rline with observations incorporates monitor information into estimating urban background and capturing the spatial gradients near roadways which the model fusion methods alone do not achieve simulating steep spatial gradients in the dispersion model is critical because the fusion methods rely on the dispersion model inputs to capture all fine scale spatial variation in the pollutant of interest second the interpolation step assumes that the urban background as defined as pollution derived from all sources except the primary mobile fraction can be linearly interpolated in space if the urban background consists mostly of spatially uniform pollutants that vary smoothly in space this assumption holds true in georgia roughly 60 of the pm2 5 mass is secondary which is spatially smooth zhai et al 2017 however if the background pollution is dominated by spatially heterogeneous pollutants with strong local sources surrounded by steep spatial gradients that are not captured in the dispersion model inputs this assumption may not hold true in this work nox and co are spatially heterogeneous in the study area but the major local source driving this heterogeneity is vehicles and is captured by the obs rline inputs and is thus not included in urban background finally for the multiplicative fusion method the dispcoarse values must all be non zero to avoid division by zero if dispcoarse is zero implying that there is no primary pollutant in that coarse grid cell estimated by the dispersion model i e if there is no primary pollution associated with vehicles on roadways using obs rline the ctm value should be assumed as the total concentration of the area without undergoing the model fusion process though this would not be expected to occur often and did not occur in these applications one limitation with evaluating these model fusion approaches is the lack of monitoring stations surrounding roadways there is very limited near roadway data to calibrate obs rline with to reduce bias in spatial gradient estimates which the model fusion approaches rely on for all fine scale spatial variation the lack of monitoring stations with daily data also limits the ability to evaluate the spatial gradients simulated by the model fusion methods on a daily basis using daily obs cmaq and annual averaged obs rline as inputs nevertheless these model fusion approaches compare well to available data and estimates near roadways are similar to measurements on roadways from other studies including the more recent near road monitoring study in atlanta sarnat 2017 overall the model fusion results capture high pollutant concentrations at roadways that obs cmaq alone cannot while also retaining complex chemistry and contributions from sources that obs rline does not simulate the fine spatial resolution of the resulting pollutant concentrations is useful for reducing exposure measurement error in health studies of spatially heterogeneous pollutants impacted by chemistry such as studies investigating pollutant exposure while living in close proximity to roadways more broadly these model fusion results have many applications for epidemiologic analyses city planning and environmental justice studies that require pollutant concentration data at a fine spatial resolution acknowledgements this publication was developed under assistance agreement no epa834799 awarded by the u s environmental protection agency to emory university and georgia institute of technology it has not been formally reviewed by epa the views expressed in this document are solely those of the authors and do not necessarily reflect those of the agency epa does not endorse any products or commercial services mentioned in this publication this paper was also made possible by the arcs foundation appendix a supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 008 conflicts of interest the authors declare no conflict of interest 
26327,an open source scalable and model independent non intrusive implementation of an iterative ensemble smoother has been developed to alleviate the computational burden associated with history matching and uncertainty quantification of real world scale environmental models that have very high dimensional parameter spaces the tool named pestpp ies implements the ensemble smoother form of the popular gauss levenberg marquardt algorithm uses the pest model interface protocols and includes a built in parallel run manager multiple lambda testing and model run failure tolerance as a demonstration of its capabilities pestpp ies is applied to a synthetic groundwater model with thousands of parameters and to a real world groundwater flow and transport model with tens of thousands of parameters pestpp ies is shown to efficiently and effectively condition parameters in both cases and can provide means to estimate posterior forecast uncertainty when the forecasts depend on large numbers of parameters keywords modeling uncertainty iterative ensemble smoother gauss levenberg marquardt code and data availability statically linked pestpp ies binaries for pc and mac osx operating systems are included in the code repository https github com dwelter pestpp along with a microsoft visual studio solution and makefiles for both mac osx and linux operating systems the repository includes several fully worked pestpp ies examples ranging from the 1 parameter analytical verification test from chen and oliver 2013 to a 1 1 million parameter synthetic model used to test the scalability of the implementation the freyberg example freyberg 1988 presented below is also included in the benchmark suit the datasets used for the hauraki example application are available upon request from waikato regional council to increase transparency and reproducibility of the examples presented herein the model input datasets for both of these example models were constructed with flopy bakker et al 2016 and the pest model interfaces were constructed with pyemu white et al 2016 the driver python scripts used to construct the model interfaces for freyberg example are included in the code repository 1 introduction for an environmental model to serve effectively in a decision support role it must include estimates of the reliability of the important simulated outcomes e g the forecasts of interest this reliability analysis typically relies on some form of uncertainty quantification where model input uncertainty is discretized into parameters and the model is used to propagate the uncertainty in these parameters to uncertainty in the forecasts given the central role of parameterization in uncertainty analysis it is critically important to use enough parameters to appropriately and robustly represent model input uncertainty e g moore and doherty 2005 dausman et al 2010 knowling and werner 2016 white et al 2017 unfortunately most algorithms for real world environmental model parameter estimation pe and uncertainty quantification uq are computationally constrained by number of adjustable parameter the curse of dimensionality because of this constraint assumptions must be employed to reduce the number of parameters a form of model simplification doherty and christensen 2011 cooley and christensen 2006 this form of simplification is not without consequence it can lead to model error phenomena such as parameter compensation and undetectable forecast bias e g doherty and christensen 2011 white et al 2014 recently iterative ensemble smoothers ies have emerged as a class of algorithms for pe and uq that relax or eliminate the computational bound induced by the number of parameters chen and oliver 2012 these algorithms can be formulated as model independent non intrusive making them an attractive option for application to existing or legacy environmental modeling analyses while many successful applications of ies are presented in the literature e g bocquet and sakov 2013 emerick and reynolds 2013 bocquet and sakov 2014 bailey and baã1 2010 crestani et al 2013 among others to my knowledge there is not a generally applicable model independent scalable implementation of ies that can be readily employed by practitioners to solve a wide range of real world environmental modeling problems to fill this need herein i present pestpp ies a model independent form of ies based on the glm formulation of chen and oliver 2013 the remainder of this manuscript presents a brief overview of glm and ies theory describes the implementation of the pestpp ies and presents application of pestpp ies to two example problems the supplementary data includes detailed input instructions and algorithmic workflows for pestpp ies as well as additional information from the second example problem 2 background and theory the ensemble smoother es was first proposed by van leeuwen and evensen 1996 as a batch update alternative to the sequential ensemble kalman filter enkf evensen 1994 where all past states and parameters are estimated in single update step while the es enjoyed some success it did not perform as well as the ensemble kalman filter van leeuwen and evensen 1996 to improve the performance of es chen and oliver 2013 wrapped an es approximation to the tangent linear operator jacobian matrix within the iterative framework of the widely used gauss levenberg marquardt glm algorithm oliver et al 2008 doherty 2015 the iterative nature of the ies greatly improved the algorithms ability to minimize the sum of squared residuals ℒ 2 norm objective function for nonlinear problems while the ensemble approximation to the jacobian matrix of the glm algorithm greatly reduced the computational constraint induced by using large numbers of parameters briefly the standard regularized maximum a posteriori form gauss levenberg marquardt algorithm glm hanke 1997 is 1 δ θ j t σ ε 1 j 1 λ σ θ 1 1 σ θ 1 θ θ 0 j t d sim d obs where j is the tangent linear operator known as the jacobian matrix λ is the marquardt dampening parameter σ ε is the measurement noise covariance matrix σ θ is the prior parameter covariance matrix d sim d obs is the residual vector θ is the current parameter vector θ 0 is the initial parameter vector and δ θ is the parameter upgrade change vector equation 1 differs from the maximum likelihood form of the glm by inclusion of σ θ which enforces regularization on δ θ many modern model independent implementations e g pest doherty 2015 pestpp welter et al 2015 ucode poeter et al 2014 of the glm algorithm rely on filling the jacobian matrix of equation 1 with finite difference approximations to the partial first derivatives 2 j sim i par j sim i par j δsim i δpar j where sim i is the simulated equivalent to observation obs i using the finite difference approximation is advantageous because it allows implementations of glm to be parallelized and model independent non intrusive the model independence trait is important because it allows any input to any forward model to be treated as a parameter and any output from any forward model including processed and derived outputs to be treated as observations it also facilitates flexibility in forming the composite objective function involving several types of observations an important consideration when dealing with imperfect real world models e g doherty and welter 2010 white et al 2014 however using finite difference approximated derivatives has a downside the computational burden of filling the jacobian matrix is directly related to the number of parameters as the number of parameters increases so too does the number of model runs required to fill the jacobian matrix during each iteration of glm to overcome this computational constraint chen and oliver 2013 reformulated the successful glm algorithm to use a jacobian matrix derived empirically from an ensemble of random parameter values the resulting glm formulation is following chen and oliver 2013 3 δ θ j e m p t σ ε 1 j e m p 1 λ σ θ 1 1 σ θ 1 θ θ 0 j e m p t d sim d obs where d obs and d sim are the observation and simulated equivalent ensembles n e n o b s respectively θ and θ 0 are the current and initial parameter ensembles n e n p a r and δ θ is the parameter upgrade matrix n e n p a r using this formulation j e m p the empirical jacobian is calculated as 4 j e m p σ ε 1 2 δ sim δ par 1 σ θ 1 2 where 5 δ sim σ ε 1 2 d sim d sim n e 1 6 δ par σ θ 1 2 θ θ n e 1 where d sim and θ are the mean values of the simulated equivalents to observations and parameters across their respective ensembles and n e is the number of realizations denotes a broadcast subtraction operation in this formulation to fill an approximate jacobian matrix the model needs to only be run once for each member of the ensemble i e realization rather than once for each parameter effectively eliminating the computational burden induced by using a large number of parameters this formulation maintains the ability to be model independent and includes the flexibility of standard deterministic implementations of the glm e g doherty 2015 welter et al 2015 poeter et al 2014 additionally since an ensemble of parameter realizations typically drawn from the prior parameter covariance matrix σ θ is propagated through the algorithm until an acceptable fit with observations is found this algorithm also yields an estimate of the posterior parameter distribution that can be used to quantify uncertainty in forecasts of interest interested readers are referred to chen and oliver 2013 and the references cited therein for more detailed and rigorous explanation of this algorithm 3 implementation and workflow 3 1 general a form of equation 3 has been implemented in pestpp software suite welter et al 2015 and is named pestpp ies the implementation is based on the lm enrml algorithm of chen and oliver 2013 the code is written mostly in c 11 and makes heavy use of the existing pestpp code base and the eigen numerical linear algebra template library guennebaud et al 2010 the pestpp ies ies implementation achieves model independence through the use of the popular pest model interface protocols doherty 2015 some key features of the implementation include serial or parallel ensemble evaluation the latter of which completed via a built in tcp ip run manager see welter et al 2015 for parallel run manager usage details multiple and option parallel λ testing experience in environmental modeling suggests evaluating different values of λ can greatly increase the efficiency of finding an optimal solution doherty 2015 subset ensemble evaluation although evaluating multiple lambdas is an important consideration in the ies framework of chen and oliver 2013 each lambda evaluation requires propagating the entire ensemble forward i e running the model once for each realization pestpp ies allows users to evaluate only a subset of the whole ensemble for each lambda value thereby saving many model evaluations while still identifying which value of lambda is likely optimal in reducing the objective function once the optimal lambda value is found for a given iteration the remaining realizations in the corresponding ensemble are evaluated in parallel before the next iteration is started fault tolerance failed run and bad run handling if a particular realization of parameter values causes the forward run to fail not run to completion or yields an objective function that is beyond a user specified threshold of acceptable then that realization is simply dropped from the analysis support for drawing multivariate gaussian realizations support for inequality constraints observations assigned an observation group name beginning with less l less than or greater g greater than will be treated as inequality constraints if the corresponding simulated value does not violate the inequality its residual e g location in d sim d obs is assigned zero it does not contribute to the objective function or the upgrade calculations see white et al 2018 for more information about implementing these types of constraints 3 2 input pestpp ies can construct all of the necessary elements needed to apply equation 3 on the fly from the information in tbhe pest control file allowing the numerous existing and current modeling analyses using pest and pestpp to apply this tool without any modification however detailed algorithmic control is exposed through optional keyword arguments added to the end of the pest control file for example existing parameter and or observation ensembles can be loaded from csv files as can a restart observation ensemble this functionality allows practitioners to restart a pestpp ies analysis from a previous analysis use parameter realizations generated from geostatistical simulations or to pre filter a parameter ensemble by eliminating realizations that do not suite some external criteria a complete listing of the optional inputs for pestpp ies along with algorithmic flowcharts are included in the supplementary data 3 3 output pestpp ies writes a run record file rec and a performance log file pfm during each application furthermore the current parameter ensemble and corresponding simulated ensemble are written to comma separated value csv files at the end of each iteration pestpp ies also writes several iteration summary csv files corresponding to the measurement regularization composite and actual objective function values for active realizations through the use of the ies verbose level 3 optional argument see supplementary data users can also have all temporary matrices used in various calculations dumped to ascii files 3 4 other considerations in applying pestpp ies the prior parameter distribution n μ θ σ θ plays a central role in equation 3 and is critical to a successful application of pestpp ies it therefore deserves special consideration the mean vector of the prior parameter distribution μ θ is taken from the initial parameter estimates in the pest control file the prior parameter covariance matrix σ θ can be formed in several ways if correlation is not recognized in prior parameter distribution then σ θ can be constructed on the fly and without user intervention from the parameter bounds in the pest control file this is the default case if no additional options are specified by default this approach assumes the parameter bounds represent a 4 standard deviation envelop 2 σ around the initial parameter values which is approximately 95 confidence interval although users can specify a different number of standard deviations that the parameter bounds represent see supplementary data if correlation is recognized in the prior parameter distribution then users may specify an optional parameter covariance matrix file this file follows the pest file formats and may be ascii or binary format in many cases the correlation in the prior parameter distribution may be implied by a geostatistical structure e g variogram in which case practitioners may use the support utilities in pyemu white et al 2016 to help construct this matrix the algorithm encoded in pestpp ies is significantly different from standard deterministic implementation of glm as such there are some important considerations that can greatly improve the efficiency of applying pestpp ies to complex numerically delicate real world environmental models for example in standard glm practitioners must take care to use very small solver closure tolerances in the forward model to ensure the fidelity of the finite difference partial first derivatives in the j matrix however pestpp ies functions with a much coarser j high fidelity model outputs are therefore not as important for successful history matching meaning larger closure tolerances may be used which in some settings may decrease the computational burden of each model forward run in sampling from the prior parameter distribution parameter realizations may be drawn that induce instability in the forward model this instability may manifest as either a failure of the forward model run corrupted null model outputs or as an extreme increase model runtime pestpp ies offers several ways to cope with these problems firstly pestpp ies tolerates failures of the forward run seamlessly realizations that cause the model forward run to fail detected by the run manager are removed from the active ensembles secondly through the ies bad phi argument see supplementary data users can identify forward runs with corrupt and or null output which are also removed from the active ensembles to cope with the issue of an extreme increased forward run time the parallel run manager built into pestpp ies accepts an additional argument overdue giveup fac which results in forward runs that have been running for greater than o v e r d u e g i v e u p f a c m e a n r u n t i m e being marked as model run failures 4 example applications two example applications are presented to demonstrate the applicability of pestpp ies to efficiently history match environmental models with high dimensional input spaces first a synthetic groundwater model based on the model of freyberg 1988 is considered the freyberg application is used to validate the performance of pestpp ies then pestpp ies is applied to a real world groundwater flow and transport model of the hauraki plains new zealand to demonstrate the capability of pestpp ies to history match and quantify uncertainty in a high dimensional real world setting 4 1 freyberg the first example application is a synthetic modflow 2005 harbaugh 2005 model due to freyberg 1988 this example application was used to evaluate the performance and outcomes of pestpp ies the groundwater flow model has 1 layer 40 rows and 20 columns of 250 m finite difference model cells the model has three stress periods 1 history matching 1 day steady state 2 forecast 1825 day transient 3 forecast 1 day steady state using the same forcing as stress period ii conceptually water enters the model domain via rainfall recharge and exists the domain via abstraction wells surface water discharge modflow riv type boundary and basin outflow specified head boundary fig 1 stress periods ii and iii receive less recharge and have increased groundwater abstraction for this example application three outputs from the model were treated as forecasts groundwater level at an unmeasured location at the end of the stress period ii surface water groundwater exchange rate for all riv boundaries during stress period ii particle travel time during the during the forecast stress periods ii and iii the particle travel time forecast was implemented using a single particle released near the divide fig 1 and tracked forward using modpath6 pollock 2012 until reaching a boundary stress period iii was added to ensure that the particle reaches a boundary these forecasts represent a wide range of model outputs and also represent outputs that are influenced by different parameters and parameter components e g different degrees of null space dependence moore and doherty 2006 which provides an important check on the algorithm encoded in pestpp ies forecast quantities of interest were added to the pest model interface as zero weighted observations to facilitate monitoring of these important quantities throughout testing and application of pestpp ies e g white 2017 by including the forecast in the pest model interface the final observation ensemble d s i m includes estimates of the forecast posterior distribution the true model inputs hydraulic conductivity fig 1 historic and forecast recharge historic and forecast groundwater abstraction were run forward with modflow 2005 and modpath6 to generate the true value for the 13 observation locations fig 1 as well as for the forecasts of interest normally distributed stochastic noise with a mean of 0 0 and standard deviation of 2 0 m was added to the 13 true groundwater levels to form the history matching dataset 4 1 1 parameterization a total of 2412 parameters were specified to represent model input uncertainty the parameterization includes many model inputs that are commonly recognized as uncertain in real world groundwater models table 1 the prior parameter covariance matrix σ θ was specified as a block diagonal matrix spatially distributed parameters were assigned distance based covariates using an exponential variogram with a range of 2500 m and sill of 1 0 the resulting covariance matrix for each spatially distributed parameter type was scaled by the standard deviations shown in table 1 4 1 2 application three different pestpp ies analyses were undertaken using 30 50 and 100 realizations respectively 10 iterations of equation 3 were completed for each of the analyses other optional arguments used for this analysis include ies lambda mults 0 1 1 0 10 0 evaluate three values for λ each iteration ies subset size 10 run the first ten realizations for each of the three θ λ ensembles to identify which is optimal ies use prior scaling false parcov filename prior cov use a σ θ stored in ascii format in the file prior cov as a verification of the uncertainty estimates yielded by pestpp ies a monte carlo rejection sampling analysis e g tarantola 2005 was completed by evaluating 100 000 parameter realizations generated from the initial parameter estimates and prior parameter covariance matrix samples from the prior distribution once the 100 000 model runs were complete they were post processed realizations yielding an objective function less than or equal to 13 0 the theoretical error based weighting minimum were used to collectively form the posterior distribution for the three forecasts the posterior forecast distributions yielded by the rejection sampling were compared to the posterior distributions yielded by pestpp ies note the monte carlo analysis was also completed using pestpp ies by specifying ies num reals 100000 and setting the pest control variable noptmax to 1 4 1 3 results each of the 30 50 and 100 realization cases matched the 13 groundwater level observations to near or below the theoretical minimum objective function value of 13 0 in a very limited number of model runs fig 2 a single iteration of standard deterministic glm requires 2412 model runs for comparative purposes pestpp welter et al 2015 was applied to this problem using regularized tikhonov tikhonov and arsenin 1977 inversion and on the fly subspace reparameterization e g the svd assist methodology of tonkin and doherty 2005 pestpp achieved an objective function minimum of 13 0 after required 2450 model runs more than 10 times the average number of pestpp ies model runs more importantly pestpp yields a single set of history matched parameters whereas pestpp ies yields an ensemble of history matched parameters a single hydraulic conductivity realization from the 10th iteration of each of the three cases is shown on fig 3 along with the minimum error variance hydraulic conductivity fields from the 10th iteration the minimum error variance parameters are included in the pestpp ies analysis by adding the initial parameter values in the control file as an additional realization a default behavior these plots reveal that the conditioned stochastic realizations maintain their general stochastic character while the minimum error variance field shows considerably less variability compared to the stochastic fields similar to the solution yielded by application of equation 1 e g deterministic glm this is an encouraging outcome since in certain modeling settings a single deterministic parameter set may be of use the minimum error variance parameters are the best candidates in this situation see fig 4 the monte carlo rejection sampling identified 275 realizations that yield an objective function value less than or equal to 13 0 an acceptance rate of approximately 0 275 these 275 realizations were collectively treated as the correct posterior distribution for each of the three forecasts comparing the posterior distributions yielded by pestpp ies to the correct posterior distributions indicates that pestpp ies is appropriately estimating both the central tendency and the distribution of the forecast posterior distributions for the three forecasts used in this test problem 4 2 hauraki plains pestpp ies was applied to a real world groundwater flow and transport model constructed for the hauraki plains on the north island of new zealand fig 5 briefly the model is a finite difference modflow nwt niswonger et al 2011 model with 7 layers 124 rows and 70 columns with a uniform grid spacing of 1000 m model layer thicknesses were assigned to coincide with production and or monitoring zones layer thicknesses are shown in the supplementary data the model is steady state and uses long term average forcing conditions and long term average groundwater levels and surface water fluxes for conditioning conceptually water enters the aquifer system through recharge and discharges to the local surface water system which flows north to the firth of thames the initial estimates of horizontal hydraulic conductivity were derived from a geologic model of the hauraki plains white et al 2015 the geologic model was spatially intersected with the model layers forming hydraulic conductivity zones the layer based zonation is show in the supplementary data the fate and transport of nitrate is an important issue in the hauraki plains as such mt3d usgs bedekar et al 2016 was used to simulate the movement of nitrate within the aquifer and surface water system surface water transport was implemented using the sft process available in mt3d usgs for a period of 50 years of steady state hydrologic conditions and nitrate loading nitrate loading was simulated as a mass loading boundary condition in model layer 1 based on the long term loading estimates for the hauraki plains derived from overseer modeling ledgard et al 1999 de nitrification was simulated using a first order decay rate initial estimates for the spatial distribution of decay rates were derived from wilson et al 2018 the initial first order decay rate estimates are shown in the supplementary data other relevant model features include long term average groundwater abstraction derived from available records groundwater abstraction is simulated with the modflow wel package long term average recharge estimates were used from the new zealand topnet model mcbride et al 2016 and simulated with the modflow rch package surface water network simulated using the modflow sfr package niswonger and prudic 2005 note long term average runoff and direct inflows to reaches derived from the topnet model simulations for the purposes of this example analysis the forecasts of interest are long term average groundwater levels at all active model cells in layer 1 3106 forecast values long term average surface water groundwater exchange for all sfr segments with in the model domain fig 5 183 forecast values long term average groundwater nitrate concentration at all active model cells in layer 1 3106 forecast values similar to the freyberg example application the forecast quantities of interest are listed as zero weight observations in the pest control file resulting in the simulated forecast values being recorded after every model run in this way the final simulated ensemble d s i m contains samples from the posterior forecast distributions 4 2 1 parameterization model input uncertainty was discretized into 83 905 parameters all spatially distributed input uncertainty was conceptualized as occurring both at the scale of the geologic model large scale zones and also at the model cell scale grid scale to accomodate these two scales of recognized model input uncertainty the parameterization of spatially distributed hydraulic properties used multiplier arrays of both geologic model scale zones and arrays of individual model cell multipliers in this way the zone based parameters estimate the broad scale mean property value within a given zone while the grid scale parameters account for intra zone heterogeneity this combined multiplier array parameterization was applied to horizontal and vertical hydraulic conductivity porosity and first order reaction coefficients in all model layers and to recharge vertical hydraulic conductivity was specified as the ratio of horizontal to vertical hydraulic conductivity additional parameters were specified for hydraulic conductance runoff and inflow for each of the 183 sfr segments runoff and inflow multiplier parameters were only specified for segments with non zero runoff or inflow respectively the nitrate loading rate each active cell in model layer 1 was assigned a multiplier parameter as well given the expected relation between the groundwater abstraction rates and the history matching observations and forecasts of interest and the presence of uncertainty in the abstraction rate estimates a multiplier parameter was added to each of 617 abstraction wells the prior parameter covariance matrix σ θ used in the application of pestpp ies to the hauraki plains model was conceptualized as a block diagonal matrix where off diagonal elements i e covariates were specified for the grid scale parameters using an exponential variogram with a range of 10 000 m and a sill of 1 0 the resulting covariance matrices associated with grid scale parameters were multiplied by the variances implied by the associated prior parameter confidence limits shown in the supplementary data the remaining parameters were treated as independent in the prior parameter covariance matrix both the prior parameter covariance matrix and the initial parameter ensemble were generated using pyemu white et al 2016 4 2 2 observations a total of 495 long term average groundwater levels and 33 surface water fluxes were used for history matching locations are shown in the supplementary data the long term average groundwater level observations were assigned a measurement standard deviation based on the number of measurements available for averaging at a given location number of measurements 10 1 0 m 10 number of measurements 5 4 0 m 5 number of measurements 10 m the long term average surface water flux observations were assigned measurement standard deviation of 10 of the long term average flux reflecting the expected heteroscedastic character of the measurement errors given the lack of observations of water table elevation simulated groundwater level in model layer 1 see supplementary data for groundwater level observation locations it is important to provide some constraints to enforce the physical condition that the simulated water table be at near or below land surface this preferred condition was implemented using less than inequality constraints the simulated groundwater levels in every active model cell in model layer 1 assigned an observed value equal to land surface the modflow discretization top input these observations were assigned to an observation group less k0 hds which identifies them as less than inequality constraints each inequality constraint was assigned a weight of 0 1 4 2 3 application an ensemble of 100 realizations was used for the hauraki plains analysis since the observations used for history matching do not contain information pertaining to porosity first order reaction rates or nitrate loading rates these parameters were not subjected to estimation rather the values for these parameters were not adjusted from the stochastic values in initial parameter ensemble θ 0 this can be thought of as a form of localization regularization chen and oliver 2017 to prevent unwarranted adjustment of these parameters future work will focus on the inclusion of transport specific observations and conditioning these parameters however given the dependence of simulated groundwater and surface water nitrate concentrations on these parameters it is important to express their uncertainty in the analysis the following optional arguments were used in the application of pestpp ies to the hauraki plains model ies lambda mults 0 1 1 0 10 ies subset size 10 ies parameter ensemble hauraki par csv ies bad phi 50000 4 2 4 results pestpp ies was run for 6 iterations for a total of 827 model runs fig 6 the match to both the long term average groundwater levels and surface water fluxes were improved significantly after 3 iterations figs 6 and 7 in general the posterior parameter realizations retain the stochastic character of the prior realizations only they have been adjusted via equation 3 to better reproduce the observation data for example the prior and posterior horizontal hydraulic conductivity arrays for the first realization in the ensemble are shown on fig 8 additional realizations are shown in the supplementary data the spatial distribution of the percent forecast uncertainty reduction resulting from history matching shows spatial variability as well as differences across the three forecast types fig 9 percent forecast uncertainty reduction was calculated using the initial and final simulated equivalent to observation ensembles as 7 δ s i g m a s 100 0 1 0 σ s σ s where σ s and σ s are the prior and posterior standard deviations respectively for forecast s this calculation was repeated for each forecast i e each active model cell for groundwater level and nitrate concentration forecast and each sfr reach for surface water groundwater exchange forecasts as expected history matching to several hundred groundwater levels has reduced the uncertainty in simulated groundwater levels at many unmeasured locations furthermore the uncertainty in surface water groundwater exchanged was also substantially reduced for several sfr segments however many sfr segments display no reduction in uncertainty from history matching the spatial distribution of nitrate shows less reduction in uncertainty as a result of history matching groundwater levels and surface water fluxes the spatial pattern of uncertainty reduction is also more complex than the groundwater level forecasts likely attributable to this forecast s dependence on local scale heterogeneity in general the forecast uncertainty results are not unexpected and underscore the importance of uncertainty quantification to place model results in a reliability context 5 conclusion and future directions pestpp ies is an open source scalable and model independent implementation of the glm iterative ensemble smoother algorithm of chen and oliver 2013 the implementation includes a built in parallel run manager and facilities for dealing with issues that arise in real world environmental modeling e g multiple lambda testing failed bad run handling minimum error variance parameter inclusion herein the ability of pestpp ies to efficiently quantify posterior forecast uncertainty for both a synthetic and real world groundwater model has been demonstrated future work will focus on tighter integration with and better support for geostatistical simulations as well as research towards supporting geostatistical realizations with higher order moments such those arising from multi point geostatistics research will also focus on enhanced support for inequality constraints and formal objective function optimization into the pestpp ies framework as well as integration of machine learning techniques to improve the empirical jacobian matrix estimate also important will be the ability to localize the cross covariances between parameters and observations using expert knowledge and or physical constraints e g chen and oliver 2017 this is an active research direction the availability of a model independent iterative ensemble smoother provides a platform for practitioners to apply this important and emerging technology to a wide range of environmental modeling analyses and allows practitioners to account for model input uncertainty at more realistic spatial and temporal scales this direct and robust accounting is expected to lead to better forecast uncertainty estimates that will improve the use of environmental models in decision making acknowledgments this work was funded in part by waikato regional council i would like to acknowledge several colleagues who i have discussed iterative ensemble smoothers with including john doherty mike fienen randy hunt and matt knowling i would also like to acknowledge brioch hemmings and zara rawlinson for pulling together the initial hauraki plains model input datasets and john hadfeld and beavan jenkins at waikato regional council for providing several datasets used in construction and history matching of the hauraki plains model appendix a supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 si 201806001481051017 pdf si 201806001481051017 pdf appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 009 
26327,an open source scalable and model independent non intrusive implementation of an iterative ensemble smoother has been developed to alleviate the computational burden associated with history matching and uncertainty quantification of real world scale environmental models that have very high dimensional parameter spaces the tool named pestpp ies implements the ensemble smoother form of the popular gauss levenberg marquardt algorithm uses the pest model interface protocols and includes a built in parallel run manager multiple lambda testing and model run failure tolerance as a demonstration of its capabilities pestpp ies is applied to a synthetic groundwater model with thousands of parameters and to a real world groundwater flow and transport model with tens of thousands of parameters pestpp ies is shown to efficiently and effectively condition parameters in both cases and can provide means to estimate posterior forecast uncertainty when the forecasts depend on large numbers of parameters keywords modeling uncertainty iterative ensemble smoother gauss levenberg marquardt code and data availability statically linked pestpp ies binaries for pc and mac osx operating systems are included in the code repository https github com dwelter pestpp along with a microsoft visual studio solution and makefiles for both mac osx and linux operating systems the repository includes several fully worked pestpp ies examples ranging from the 1 parameter analytical verification test from chen and oliver 2013 to a 1 1 million parameter synthetic model used to test the scalability of the implementation the freyberg example freyberg 1988 presented below is also included in the benchmark suit the datasets used for the hauraki example application are available upon request from waikato regional council to increase transparency and reproducibility of the examples presented herein the model input datasets for both of these example models were constructed with flopy bakker et al 2016 and the pest model interfaces were constructed with pyemu white et al 2016 the driver python scripts used to construct the model interfaces for freyberg example are included in the code repository 1 introduction for an environmental model to serve effectively in a decision support role it must include estimates of the reliability of the important simulated outcomes e g the forecasts of interest this reliability analysis typically relies on some form of uncertainty quantification where model input uncertainty is discretized into parameters and the model is used to propagate the uncertainty in these parameters to uncertainty in the forecasts given the central role of parameterization in uncertainty analysis it is critically important to use enough parameters to appropriately and robustly represent model input uncertainty e g moore and doherty 2005 dausman et al 2010 knowling and werner 2016 white et al 2017 unfortunately most algorithms for real world environmental model parameter estimation pe and uncertainty quantification uq are computationally constrained by number of adjustable parameter the curse of dimensionality because of this constraint assumptions must be employed to reduce the number of parameters a form of model simplification doherty and christensen 2011 cooley and christensen 2006 this form of simplification is not without consequence it can lead to model error phenomena such as parameter compensation and undetectable forecast bias e g doherty and christensen 2011 white et al 2014 recently iterative ensemble smoothers ies have emerged as a class of algorithms for pe and uq that relax or eliminate the computational bound induced by the number of parameters chen and oliver 2012 these algorithms can be formulated as model independent non intrusive making them an attractive option for application to existing or legacy environmental modeling analyses while many successful applications of ies are presented in the literature e g bocquet and sakov 2013 emerick and reynolds 2013 bocquet and sakov 2014 bailey and baã1 2010 crestani et al 2013 among others to my knowledge there is not a generally applicable model independent scalable implementation of ies that can be readily employed by practitioners to solve a wide range of real world environmental modeling problems to fill this need herein i present pestpp ies a model independent form of ies based on the glm formulation of chen and oliver 2013 the remainder of this manuscript presents a brief overview of glm and ies theory describes the implementation of the pestpp ies and presents application of pestpp ies to two example problems the supplementary data includes detailed input instructions and algorithmic workflows for pestpp ies as well as additional information from the second example problem 2 background and theory the ensemble smoother es was first proposed by van leeuwen and evensen 1996 as a batch update alternative to the sequential ensemble kalman filter enkf evensen 1994 where all past states and parameters are estimated in single update step while the es enjoyed some success it did not perform as well as the ensemble kalman filter van leeuwen and evensen 1996 to improve the performance of es chen and oliver 2013 wrapped an es approximation to the tangent linear operator jacobian matrix within the iterative framework of the widely used gauss levenberg marquardt glm algorithm oliver et al 2008 doherty 2015 the iterative nature of the ies greatly improved the algorithms ability to minimize the sum of squared residuals ℒ 2 norm objective function for nonlinear problems while the ensemble approximation to the jacobian matrix of the glm algorithm greatly reduced the computational constraint induced by using large numbers of parameters briefly the standard regularized maximum a posteriori form gauss levenberg marquardt algorithm glm hanke 1997 is 1 δ θ j t σ ε 1 j 1 λ σ θ 1 1 σ θ 1 θ θ 0 j t d sim d obs where j is the tangent linear operator known as the jacobian matrix λ is the marquardt dampening parameter σ ε is the measurement noise covariance matrix σ θ is the prior parameter covariance matrix d sim d obs is the residual vector θ is the current parameter vector θ 0 is the initial parameter vector and δ θ is the parameter upgrade change vector equation 1 differs from the maximum likelihood form of the glm by inclusion of σ θ which enforces regularization on δ θ many modern model independent implementations e g pest doherty 2015 pestpp welter et al 2015 ucode poeter et al 2014 of the glm algorithm rely on filling the jacobian matrix of equation 1 with finite difference approximations to the partial first derivatives 2 j sim i par j sim i par j δsim i δpar j where sim i is the simulated equivalent to observation obs i using the finite difference approximation is advantageous because it allows implementations of glm to be parallelized and model independent non intrusive the model independence trait is important because it allows any input to any forward model to be treated as a parameter and any output from any forward model including processed and derived outputs to be treated as observations it also facilitates flexibility in forming the composite objective function involving several types of observations an important consideration when dealing with imperfect real world models e g doherty and welter 2010 white et al 2014 however using finite difference approximated derivatives has a downside the computational burden of filling the jacobian matrix is directly related to the number of parameters as the number of parameters increases so too does the number of model runs required to fill the jacobian matrix during each iteration of glm to overcome this computational constraint chen and oliver 2013 reformulated the successful glm algorithm to use a jacobian matrix derived empirically from an ensemble of random parameter values the resulting glm formulation is following chen and oliver 2013 3 δ θ j e m p t σ ε 1 j e m p 1 λ σ θ 1 1 σ θ 1 θ θ 0 j e m p t d sim d obs where d obs and d sim are the observation and simulated equivalent ensembles n e n o b s respectively θ and θ 0 are the current and initial parameter ensembles n e n p a r and δ θ is the parameter upgrade matrix n e n p a r using this formulation j e m p the empirical jacobian is calculated as 4 j e m p σ ε 1 2 δ sim δ par 1 σ θ 1 2 where 5 δ sim σ ε 1 2 d sim d sim n e 1 6 δ par σ θ 1 2 θ θ n e 1 where d sim and θ are the mean values of the simulated equivalents to observations and parameters across their respective ensembles and n e is the number of realizations denotes a broadcast subtraction operation in this formulation to fill an approximate jacobian matrix the model needs to only be run once for each member of the ensemble i e realization rather than once for each parameter effectively eliminating the computational burden induced by using a large number of parameters this formulation maintains the ability to be model independent and includes the flexibility of standard deterministic implementations of the glm e g doherty 2015 welter et al 2015 poeter et al 2014 additionally since an ensemble of parameter realizations typically drawn from the prior parameter covariance matrix σ θ is propagated through the algorithm until an acceptable fit with observations is found this algorithm also yields an estimate of the posterior parameter distribution that can be used to quantify uncertainty in forecasts of interest interested readers are referred to chen and oliver 2013 and the references cited therein for more detailed and rigorous explanation of this algorithm 3 implementation and workflow 3 1 general a form of equation 3 has been implemented in pestpp software suite welter et al 2015 and is named pestpp ies the implementation is based on the lm enrml algorithm of chen and oliver 2013 the code is written mostly in c 11 and makes heavy use of the existing pestpp code base and the eigen numerical linear algebra template library guennebaud et al 2010 the pestpp ies ies implementation achieves model independence through the use of the popular pest model interface protocols doherty 2015 some key features of the implementation include serial or parallel ensemble evaluation the latter of which completed via a built in tcp ip run manager see welter et al 2015 for parallel run manager usage details multiple and option parallel λ testing experience in environmental modeling suggests evaluating different values of λ can greatly increase the efficiency of finding an optimal solution doherty 2015 subset ensemble evaluation although evaluating multiple lambdas is an important consideration in the ies framework of chen and oliver 2013 each lambda evaluation requires propagating the entire ensemble forward i e running the model once for each realization pestpp ies allows users to evaluate only a subset of the whole ensemble for each lambda value thereby saving many model evaluations while still identifying which value of lambda is likely optimal in reducing the objective function once the optimal lambda value is found for a given iteration the remaining realizations in the corresponding ensemble are evaluated in parallel before the next iteration is started fault tolerance failed run and bad run handling if a particular realization of parameter values causes the forward run to fail not run to completion or yields an objective function that is beyond a user specified threshold of acceptable then that realization is simply dropped from the analysis support for drawing multivariate gaussian realizations support for inequality constraints observations assigned an observation group name beginning with less l less than or greater g greater than will be treated as inequality constraints if the corresponding simulated value does not violate the inequality its residual e g location in d sim d obs is assigned zero it does not contribute to the objective function or the upgrade calculations see white et al 2018 for more information about implementing these types of constraints 3 2 input pestpp ies can construct all of the necessary elements needed to apply equation 3 on the fly from the information in tbhe pest control file allowing the numerous existing and current modeling analyses using pest and pestpp to apply this tool without any modification however detailed algorithmic control is exposed through optional keyword arguments added to the end of the pest control file for example existing parameter and or observation ensembles can be loaded from csv files as can a restart observation ensemble this functionality allows practitioners to restart a pestpp ies analysis from a previous analysis use parameter realizations generated from geostatistical simulations or to pre filter a parameter ensemble by eliminating realizations that do not suite some external criteria a complete listing of the optional inputs for pestpp ies along with algorithmic flowcharts are included in the supplementary data 3 3 output pestpp ies writes a run record file rec and a performance log file pfm during each application furthermore the current parameter ensemble and corresponding simulated ensemble are written to comma separated value csv files at the end of each iteration pestpp ies also writes several iteration summary csv files corresponding to the measurement regularization composite and actual objective function values for active realizations through the use of the ies verbose level 3 optional argument see supplementary data users can also have all temporary matrices used in various calculations dumped to ascii files 3 4 other considerations in applying pestpp ies the prior parameter distribution n μ θ σ θ plays a central role in equation 3 and is critical to a successful application of pestpp ies it therefore deserves special consideration the mean vector of the prior parameter distribution μ θ is taken from the initial parameter estimates in the pest control file the prior parameter covariance matrix σ θ can be formed in several ways if correlation is not recognized in prior parameter distribution then σ θ can be constructed on the fly and without user intervention from the parameter bounds in the pest control file this is the default case if no additional options are specified by default this approach assumes the parameter bounds represent a 4 standard deviation envelop 2 σ around the initial parameter values which is approximately 95 confidence interval although users can specify a different number of standard deviations that the parameter bounds represent see supplementary data if correlation is recognized in the prior parameter distribution then users may specify an optional parameter covariance matrix file this file follows the pest file formats and may be ascii or binary format in many cases the correlation in the prior parameter distribution may be implied by a geostatistical structure e g variogram in which case practitioners may use the support utilities in pyemu white et al 2016 to help construct this matrix the algorithm encoded in pestpp ies is significantly different from standard deterministic implementation of glm as such there are some important considerations that can greatly improve the efficiency of applying pestpp ies to complex numerically delicate real world environmental models for example in standard glm practitioners must take care to use very small solver closure tolerances in the forward model to ensure the fidelity of the finite difference partial first derivatives in the j matrix however pestpp ies functions with a much coarser j high fidelity model outputs are therefore not as important for successful history matching meaning larger closure tolerances may be used which in some settings may decrease the computational burden of each model forward run in sampling from the prior parameter distribution parameter realizations may be drawn that induce instability in the forward model this instability may manifest as either a failure of the forward model run corrupted null model outputs or as an extreme increase model runtime pestpp ies offers several ways to cope with these problems firstly pestpp ies tolerates failures of the forward run seamlessly realizations that cause the model forward run to fail detected by the run manager are removed from the active ensembles secondly through the ies bad phi argument see supplementary data users can identify forward runs with corrupt and or null output which are also removed from the active ensembles to cope with the issue of an extreme increased forward run time the parallel run manager built into pestpp ies accepts an additional argument overdue giveup fac which results in forward runs that have been running for greater than o v e r d u e g i v e u p f a c m e a n r u n t i m e being marked as model run failures 4 example applications two example applications are presented to demonstrate the applicability of pestpp ies to efficiently history match environmental models with high dimensional input spaces first a synthetic groundwater model based on the model of freyberg 1988 is considered the freyberg application is used to validate the performance of pestpp ies then pestpp ies is applied to a real world groundwater flow and transport model of the hauraki plains new zealand to demonstrate the capability of pestpp ies to history match and quantify uncertainty in a high dimensional real world setting 4 1 freyberg the first example application is a synthetic modflow 2005 harbaugh 2005 model due to freyberg 1988 this example application was used to evaluate the performance and outcomes of pestpp ies the groundwater flow model has 1 layer 40 rows and 20 columns of 250 m finite difference model cells the model has three stress periods 1 history matching 1 day steady state 2 forecast 1825 day transient 3 forecast 1 day steady state using the same forcing as stress period ii conceptually water enters the model domain via rainfall recharge and exists the domain via abstraction wells surface water discharge modflow riv type boundary and basin outflow specified head boundary fig 1 stress periods ii and iii receive less recharge and have increased groundwater abstraction for this example application three outputs from the model were treated as forecasts groundwater level at an unmeasured location at the end of the stress period ii surface water groundwater exchange rate for all riv boundaries during stress period ii particle travel time during the during the forecast stress periods ii and iii the particle travel time forecast was implemented using a single particle released near the divide fig 1 and tracked forward using modpath6 pollock 2012 until reaching a boundary stress period iii was added to ensure that the particle reaches a boundary these forecasts represent a wide range of model outputs and also represent outputs that are influenced by different parameters and parameter components e g different degrees of null space dependence moore and doherty 2006 which provides an important check on the algorithm encoded in pestpp ies forecast quantities of interest were added to the pest model interface as zero weighted observations to facilitate monitoring of these important quantities throughout testing and application of pestpp ies e g white 2017 by including the forecast in the pest model interface the final observation ensemble d s i m includes estimates of the forecast posterior distribution the true model inputs hydraulic conductivity fig 1 historic and forecast recharge historic and forecast groundwater abstraction were run forward with modflow 2005 and modpath6 to generate the true value for the 13 observation locations fig 1 as well as for the forecasts of interest normally distributed stochastic noise with a mean of 0 0 and standard deviation of 2 0 m was added to the 13 true groundwater levels to form the history matching dataset 4 1 1 parameterization a total of 2412 parameters were specified to represent model input uncertainty the parameterization includes many model inputs that are commonly recognized as uncertain in real world groundwater models table 1 the prior parameter covariance matrix σ θ was specified as a block diagonal matrix spatially distributed parameters were assigned distance based covariates using an exponential variogram with a range of 2500 m and sill of 1 0 the resulting covariance matrix for each spatially distributed parameter type was scaled by the standard deviations shown in table 1 4 1 2 application three different pestpp ies analyses were undertaken using 30 50 and 100 realizations respectively 10 iterations of equation 3 were completed for each of the analyses other optional arguments used for this analysis include ies lambda mults 0 1 1 0 10 0 evaluate three values for λ each iteration ies subset size 10 run the first ten realizations for each of the three θ λ ensembles to identify which is optimal ies use prior scaling false parcov filename prior cov use a σ θ stored in ascii format in the file prior cov as a verification of the uncertainty estimates yielded by pestpp ies a monte carlo rejection sampling analysis e g tarantola 2005 was completed by evaluating 100 000 parameter realizations generated from the initial parameter estimates and prior parameter covariance matrix samples from the prior distribution once the 100 000 model runs were complete they were post processed realizations yielding an objective function less than or equal to 13 0 the theoretical error based weighting minimum were used to collectively form the posterior distribution for the three forecasts the posterior forecast distributions yielded by the rejection sampling were compared to the posterior distributions yielded by pestpp ies note the monte carlo analysis was also completed using pestpp ies by specifying ies num reals 100000 and setting the pest control variable noptmax to 1 4 1 3 results each of the 30 50 and 100 realization cases matched the 13 groundwater level observations to near or below the theoretical minimum objective function value of 13 0 in a very limited number of model runs fig 2 a single iteration of standard deterministic glm requires 2412 model runs for comparative purposes pestpp welter et al 2015 was applied to this problem using regularized tikhonov tikhonov and arsenin 1977 inversion and on the fly subspace reparameterization e g the svd assist methodology of tonkin and doherty 2005 pestpp achieved an objective function minimum of 13 0 after required 2450 model runs more than 10 times the average number of pestpp ies model runs more importantly pestpp yields a single set of history matched parameters whereas pestpp ies yields an ensemble of history matched parameters a single hydraulic conductivity realization from the 10th iteration of each of the three cases is shown on fig 3 along with the minimum error variance hydraulic conductivity fields from the 10th iteration the minimum error variance parameters are included in the pestpp ies analysis by adding the initial parameter values in the control file as an additional realization a default behavior these plots reveal that the conditioned stochastic realizations maintain their general stochastic character while the minimum error variance field shows considerably less variability compared to the stochastic fields similar to the solution yielded by application of equation 1 e g deterministic glm this is an encouraging outcome since in certain modeling settings a single deterministic parameter set may be of use the minimum error variance parameters are the best candidates in this situation see fig 4 the monte carlo rejection sampling identified 275 realizations that yield an objective function value less than or equal to 13 0 an acceptance rate of approximately 0 275 these 275 realizations were collectively treated as the correct posterior distribution for each of the three forecasts comparing the posterior distributions yielded by pestpp ies to the correct posterior distributions indicates that pestpp ies is appropriately estimating both the central tendency and the distribution of the forecast posterior distributions for the three forecasts used in this test problem 4 2 hauraki plains pestpp ies was applied to a real world groundwater flow and transport model constructed for the hauraki plains on the north island of new zealand fig 5 briefly the model is a finite difference modflow nwt niswonger et al 2011 model with 7 layers 124 rows and 70 columns with a uniform grid spacing of 1000 m model layer thicknesses were assigned to coincide with production and or monitoring zones layer thicknesses are shown in the supplementary data the model is steady state and uses long term average forcing conditions and long term average groundwater levels and surface water fluxes for conditioning conceptually water enters the aquifer system through recharge and discharges to the local surface water system which flows north to the firth of thames the initial estimates of horizontal hydraulic conductivity were derived from a geologic model of the hauraki plains white et al 2015 the geologic model was spatially intersected with the model layers forming hydraulic conductivity zones the layer based zonation is show in the supplementary data the fate and transport of nitrate is an important issue in the hauraki plains as such mt3d usgs bedekar et al 2016 was used to simulate the movement of nitrate within the aquifer and surface water system surface water transport was implemented using the sft process available in mt3d usgs for a period of 50 years of steady state hydrologic conditions and nitrate loading nitrate loading was simulated as a mass loading boundary condition in model layer 1 based on the long term loading estimates for the hauraki plains derived from overseer modeling ledgard et al 1999 de nitrification was simulated using a first order decay rate initial estimates for the spatial distribution of decay rates were derived from wilson et al 2018 the initial first order decay rate estimates are shown in the supplementary data other relevant model features include long term average groundwater abstraction derived from available records groundwater abstraction is simulated with the modflow wel package long term average recharge estimates were used from the new zealand topnet model mcbride et al 2016 and simulated with the modflow rch package surface water network simulated using the modflow sfr package niswonger and prudic 2005 note long term average runoff and direct inflows to reaches derived from the topnet model simulations for the purposes of this example analysis the forecasts of interest are long term average groundwater levels at all active model cells in layer 1 3106 forecast values long term average surface water groundwater exchange for all sfr segments with in the model domain fig 5 183 forecast values long term average groundwater nitrate concentration at all active model cells in layer 1 3106 forecast values similar to the freyberg example application the forecast quantities of interest are listed as zero weight observations in the pest control file resulting in the simulated forecast values being recorded after every model run in this way the final simulated ensemble d s i m contains samples from the posterior forecast distributions 4 2 1 parameterization model input uncertainty was discretized into 83 905 parameters all spatially distributed input uncertainty was conceptualized as occurring both at the scale of the geologic model large scale zones and also at the model cell scale grid scale to accomodate these two scales of recognized model input uncertainty the parameterization of spatially distributed hydraulic properties used multiplier arrays of both geologic model scale zones and arrays of individual model cell multipliers in this way the zone based parameters estimate the broad scale mean property value within a given zone while the grid scale parameters account for intra zone heterogeneity this combined multiplier array parameterization was applied to horizontal and vertical hydraulic conductivity porosity and first order reaction coefficients in all model layers and to recharge vertical hydraulic conductivity was specified as the ratio of horizontal to vertical hydraulic conductivity additional parameters were specified for hydraulic conductance runoff and inflow for each of the 183 sfr segments runoff and inflow multiplier parameters were only specified for segments with non zero runoff or inflow respectively the nitrate loading rate each active cell in model layer 1 was assigned a multiplier parameter as well given the expected relation between the groundwater abstraction rates and the history matching observations and forecasts of interest and the presence of uncertainty in the abstraction rate estimates a multiplier parameter was added to each of 617 abstraction wells the prior parameter covariance matrix σ θ used in the application of pestpp ies to the hauraki plains model was conceptualized as a block diagonal matrix where off diagonal elements i e covariates were specified for the grid scale parameters using an exponential variogram with a range of 10 000 m and a sill of 1 0 the resulting covariance matrices associated with grid scale parameters were multiplied by the variances implied by the associated prior parameter confidence limits shown in the supplementary data the remaining parameters were treated as independent in the prior parameter covariance matrix both the prior parameter covariance matrix and the initial parameter ensemble were generated using pyemu white et al 2016 4 2 2 observations a total of 495 long term average groundwater levels and 33 surface water fluxes were used for history matching locations are shown in the supplementary data the long term average groundwater level observations were assigned a measurement standard deviation based on the number of measurements available for averaging at a given location number of measurements 10 1 0 m 10 number of measurements 5 4 0 m 5 number of measurements 10 m the long term average surface water flux observations were assigned measurement standard deviation of 10 of the long term average flux reflecting the expected heteroscedastic character of the measurement errors given the lack of observations of water table elevation simulated groundwater level in model layer 1 see supplementary data for groundwater level observation locations it is important to provide some constraints to enforce the physical condition that the simulated water table be at near or below land surface this preferred condition was implemented using less than inequality constraints the simulated groundwater levels in every active model cell in model layer 1 assigned an observed value equal to land surface the modflow discretization top input these observations were assigned to an observation group less k0 hds which identifies them as less than inequality constraints each inequality constraint was assigned a weight of 0 1 4 2 3 application an ensemble of 100 realizations was used for the hauraki plains analysis since the observations used for history matching do not contain information pertaining to porosity first order reaction rates or nitrate loading rates these parameters were not subjected to estimation rather the values for these parameters were not adjusted from the stochastic values in initial parameter ensemble θ 0 this can be thought of as a form of localization regularization chen and oliver 2017 to prevent unwarranted adjustment of these parameters future work will focus on the inclusion of transport specific observations and conditioning these parameters however given the dependence of simulated groundwater and surface water nitrate concentrations on these parameters it is important to express their uncertainty in the analysis the following optional arguments were used in the application of pestpp ies to the hauraki plains model ies lambda mults 0 1 1 0 10 ies subset size 10 ies parameter ensemble hauraki par csv ies bad phi 50000 4 2 4 results pestpp ies was run for 6 iterations for a total of 827 model runs fig 6 the match to both the long term average groundwater levels and surface water fluxes were improved significantly after 3 iterations figs 6 and 7 in general the posterior parameter realizations retain the stochastic character of the prior realizations only they have been adjusted via equation 3 to better reproduce the observation data for example the prior and posterior horizontal hydraulic conductivity arrays for the first realization in the ensemble are shown on fig 8 additional realizations are shown in the supplementary data the spatial distribution of the percent forecast uncertainty reduction resulting from history matching shows spatial variability as well as differences across the three forecast types fig 9 percent forecast uncertainty reduction was calculated using the initial and final simulated equivalent to observation ensembles as 7 δ s i g m a s 100 0 1 0 σ s σ s where σ s and σ s are the prior and posterior standard deviations respectively for forecast s this calculation was repeated for each forecast i e each active model cell for groundwater level and nitrate concentration forecast and each sfr reach for surface water groundwater exchange forecasts as expected history matching to several hundred groundwater levels has reduced the uncertainty in simulated groundwater levels at many unmeasured locations furthermore the uncertainty in surface water groundwater exchanged was also substantially reduced for several sfr segments however many sfr segments display no reduction in uncertainty from history matching the spatial distribution of nitrate shows less reduction in uncertainty as a result of history matching groundwater levels and surface water fluxes the spatial pattern of uncertainty reduction is also more complex than the groundwater level forecasts likely attributable to this forecast s dependence on local scale heterogeneity in general the forecast uncertainty results are not unexpected and underscore the importance of uncertainty quantification to place model results in a reliability context 5 conclusion and future directions pestpp ies is an open source scalable and model independent implementation of the glm iterative ensemble smoother algorithm of chen and oliver 2013 the implementation includes a built in parallel run manager and facilities for dealing with issues that arise in real world environmental modeling e g multiple lambda testing failed bad run handling minimum error variance parameter inclusion herein the ability of pestpp ies to efficiently quantify posterior forecast uncertainty for both a synthetic and real world groundwater model has been demonstrated future work will focus on tighter integration with and better support for geostatistical simulations as well as research towards supporting geostatistical realizations with higher order moments such those arising from multi point geostatistics research will also focus on enhanced support for inequality constraints and formal objective function optimization into the pestpp ies framework as well as integration of machine learning techniques to improve the empirical jacobian matrix estimate also important will be the ability to localize the cross covariances between parameters and observations using expert knowledge and or physical constraints e g chen and oliver 2017 this is an active research direction the availability of a model independent iterative ensemble smoother provides a platform for practitioners to apply this important and emerging technology to a wide range of environmental modeling analyses and allows practitioners to account for model input uncertainty at more realistic spatial and temporal scales this direct and robust accounting is expected to lead to better forecast uncertainty estimates that will improve the use of environmental models in decision making acknowledgments this work was funded in part by waikato regional council i would like to acknowledge several colleagues who i have discussed iterative ensemble smoothers with including john doherty mike fienen randy hunt and matt knowling i would also like to acknowledge brioch hemmings and zara rawlinson for pulling together the initial hauraki plains model input datasets and john hadfeld and beavan jenkins at waikato regional council for providing several datasets used in construction and history matching of the hauraki plains model appendix a supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 si 201806001481051017 pdf si 201806001481051017 pdf appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 009 
26328,this paper provides empirical and experimental assessments of thematic knowledge discourses based on two case studies in the us virgin islands and florida we utilize a latent semantic indexing analysis over natural language corpus to classify and categorize knowledge categories we computed tf idf scores and associated co occurrence jaccard similarity scores to construct semantic knowledge networks using network analysis we computed structural metrics over four composite groups neighbor based centrality equivalence and position the analysis show that structural network characteristics of environmental knowledge can exponentially predict associations between knowledge categories we show that connectivity play a critical role on acquisition representation and diffusion patterns of knowledge within local communities we provide evidence of a global prevalence of a shared knowledge core we show that core social ecological attributes of knowledge follow scale free power law distributions and stable equilibrium network structures we identify two distinct models of bidirectional translation a bottom up and a top down keywords semantic networks knowledge networks social ecological systems integrated local ecological knowledge latent semantic analysis social network analysis 1 introduction in the last decade multiple studies have underlined the importance of the relationship between knowledge and environmental conservation in many of these studies traditional ecological knowledge tek and or local ecological knowledge lek is are shown to have strong connections with environmental conservation and social ecological stewardship basurto et al 2013 becker and ghimire 2003 berkes and turner 2006 others tackle the relationship between scientific knowledge and sustainability kristjanson et al 2009 villa athanasiadis and rizzoli 2009 for example provide a review of semantic knowledge models addressing ecological and environmental modeling applications and discuss the broader adoption and feasibility of new approaches rivera minsker work and roth 2014 use a text mining framework to classify and develop sustainability criteria and indicators at a regional scale in many of these studies the concept of knowledge is addressed as a rather abstract and in a somewhat descriptive manner kiptot 2007 tàbara and chabay 2013 in other studies the content of knowledge was evaluated against decisions and behaviors related to environmental conservation and action in these local systems dutta morshed aryal d este and das 2014 grant and berkes 2007 and in some studies knowledge was measured as a list on survey responses cinner et al 2010 often the concepts of collective knowledge and social learning are used interchangeably and without a clear set of definitions and or boundaries related to their perspective functions and effective roles in the social ecological systems on the other hand many studies focus on formal or formalized ontologies and ontological frameworks martínez garcía et al 2018 polhill et al 2016 collective knowledge systems interact and operate across the full extent of social ecological systems they incorporate individual and cognitive characteristics of knowledge social perceptions of reality berger and luckmann 1967 ecological reflections of reality as well as institutional and governance dimensions knowledge systems exist and function in the heart of informal institutions and social norms but also directly relate our everyday social realities to formal institutional rules and arrangements social ecological stewardship critically depends and builds upon existing collective knowledge structures 1 1 effective institutional governance in managing social ecological commons our ability to examine and analyze the efficiency efficacy and effectiveness of our governance systems and the institutional arrangements in place and at work towards managing our commons critically depends first on the context nature and characteristics of the governance systems themselves it also depends on the weak network or web of connections between institutional processes that form management functions and key systemic components of the linked social ecological system the presence of weak links or ties between core institutional processes and both social and ecosystem functions are often the catalytic drivers of institutional change efficient governance systems with respect to social ecological system management can function both to the benefit of the ecosystem services and functions of the natural system and to the strengthening of the social system and the communities of practice within the management scales of reach from the ecological standpoint ecosystem health and ecological resilience are among the most important functions and processes that one needs to pay attention to albeit a number of secondary ecosystem and landscape processes bare significance to the analysis from the social standpoint the triplet of cognitive dispositional collective or social and institutional interactions represent important components of such an analysis fig 1 in social ecological systems the governance of the commons more often than not emerges as a function of local community social and ecological stewardship without it is difficult if not impossible to achieve governance structures that have the necessary legitimacy power and control to negotiate efficient and adaptive institutions of change cowie and borrett 2005 de vos et al 2013 it is exactly because such adaptive institutional arrangements reflect broader community and societal goals or aspirations the presence of a synergistic relationship between local stewardship and governance of the commons is critical ghorbani et al 2017 on the other hand stewardship at the community level alone cannot successfully achieve social ecological sustainability and resilience in systems where multi governance of commons is required this is because of systemic and institutional externalities entering and likely affecting the social ecological local system in question the degree to which a successful level of management is achieved in the dual stewardship governance of the commons system perhaps has something to do with the ability and capacity of transference across linked systems and domains of knowledge the capacity of the dualistic system of knowledge transference is inevitably linked to the capability set and network of interconnected associations across and within the key players or actors of both systems such systemic interconnectedness critically depends on the functioning and roles of knowledge producers knowledge translators bi directional and knowledge diffusers in the system the three roles are complimentary mutually reinforcing and non exclusive of each other 1 1 1 knowledge producers they emerge and function at both scales of the two subsystems local global yet the nature of the knowledge produced at each scale differs substantially albeit dependent of each other local knowledge producers operate at the community scale in the fringe of the natural social prevalence of the phenomena problems they acquire and produce knowledge by merging and combining empirical or observational data information with local and traditional ecological knowledge as well as with scientific knowledge the produced knowledge more likely than not is raw unprocessed and customized to fit the level of local community understanding of reality and the community mental knowledge representation global knowledge producers at the level of the governance of the commons often produce knowledge that adheres to strategic and policy objectives and social ecological imperative realities in which the commons exist and operate they accumulate and synthesize new knowledge by combining and contextualizing or re contextualizing existing knowledge emerging from the local community mental model interactions the level of efficiency in such contextualization and synthesizing of new knowledge depends to the degree in which the latter finds its way or reaches their operant level or scale of perception 1 1 2 knowledge bidirectional translators they operate often at the margins and neutral zone between the local and higher scales of inference their role is to bi directionally translate knowledge from the one subsystem to another and vice versa sato 2014 they often seek and assist stewardship efforts at the community level all the while promoting cooperation and coordination among other players and roles at higher levels e g decision makers managers policy makers governance and institutional players scientists organizations etc their role is critical as enablers or bridges connecting small world types of networks that they otherwise would not be connected without their brokerage by assuming this role they open knowledge transference pathways capable of producing a wide array of interesting and fascinating systemic phenomena such phenomena include the emergence of power law type of relationships in knowledge networks generative social emergence across multiple social and ecological scales and perhaps importantly keeping open bi directional pathways of communication and socially relevant situational understanding of reality the latter represents a necessary condition in negotiating appropriate and efficient institutional arrangements required in managing and governing the commons 1 1 3 knowledge diffusers they represent a group of social actors capable of transforming knowledge structures into tangible real and beneficial social ecological outcomes they are the ones that complete the cycle from information to knowledge to a particular type of actions such actions represent knowledge relevant or knowledge intensive decision and policy making the context content and extent of their knowledge diffusion establishes the conditions and situations necessary for the emergence of adaptive proactive and reactive action while action itself does not necessarily implies or requires knowledge transference the type of action that knowledge diffusers enable is such that ensures that the options and choices available have a direct and undisputable mapping into real and clear outcomes with societal significance this clear mapping between choices and distinct outcomes necessitates knowledge as critical and fundamental element of each strategic mix of choices and outcomes 1 2 collective social processes and social ecological knowledge organization a significant part of the scientific literature deals with semantic processes as an integral part of semantic cognition semantic ability e g activation association inter connectiveness or retrieval is found to be seriously impaired in the presence of various social illnesses that affect cognitive ability for example semantic ability impairment is found to be associated with mental patients suffering from schizophrenia chiu et al 2003 condray et al 2003 paulsen et al 1996 and other psychotic conditions fritzsche 2003 alzheimer s disease kazui et al 2003 rogers and friedman 2008 dementia graham et al 2000 robertson and köhler 2007 amnesia and epilepsy giovagnoli et al 2009 various brain or auditory damages breese and hillis 2004 or other social psychological conditions such as selective retrieval of unwanted memories levy and anderson 2008 these and other relevant findings indicate how central is our ability to perform cognitive semantic tasks in order to function in a human social environment at the collective social level of interactions the issues go beyond social or cognitive pathologies and indicate strengths or deficiencies of the social system as a whole if our ability to organize and semantically categorize our knowledge is a proxy of our overall social intelligence enfield 2009 then as societies we benefit and perhaps advance our levels of collective social achievements by taking advantage and exploring this social intelligence in different or higher orders of knowledge organization our ability as organized communities and societies to achieve higher levels of knowledge organization is therefore a characteristic integral perhaps of our social learning capability in an era where technology and complexity of social interactions advance and develop in parallel our abilities as social groups to process reorganize utilize and functionalize information in semantically rich ways are paramount new knowledge is generated and required not simply in the form of unprocessed information availability but as deliberate and functional response to the needs of our social complexity of interactions the semantic web greaves and mika 2008 for example does not itself generates new fundamental knowledge but rather alters its availability and accessibility to wider social audiences reorganizes it in new and highly functional ways or semantic categorizations and enhances our ability to perform more and more complex cognitive and socially critical tasks at both the individual and the collective social levels in a sense semantic social knowledge representation encodes and categorizes a world of semantically important objects that belong to the past present and future alike the past through the historical collective evolution of norms aspirations and beliefs the present through values and attitudes the future through expectations and visions that individuals and local societies hold and pursue collective semantic social knowledge representation also encodes both physical and psychological space alike physical space through socially embedded geographies historical and cultural spaces and landscapes geographical narratives and named places social space through social distances distinctive social roles and actor settings psychological scales and magnitudes collective dispositions and mapping of shared realities experiences and norms this blending of space and time into a unique social continuum is what semantic knowledge representation classifies and encapsulates within its ontologies collective social processes and collective decision making depends on the level of knowledge and its representation in the system of interactions among and across objects of collective inference in other words how the system behaves is a matter of complexity of both its parts but also on its interactions systemic interactions i e part part relationships and part whole whole part interactions are contextualized in terms of informational or entropic context that is primarily knowledge based or at least knowledge contingent to the extent that collective networks of interactions e g semantic encode and encapsulate such interactivity across social actors in the society and community as evidence suggests in support of this proposition then collective knowledge representation has the propensity of capturing important social dynamics such social dynamics include social emergence centrality and collective social influence over behavior decisions and actions if we agree that many social dimensions of change require an inductive approach and cannot be reconstructed at least empirically using deductive approaches or social mental models then we can see that the level of support in often uncertain and incomplete inferences regarding social change can only be achieved by introducing heuristics stemming from evidence at the collective social level of interactions this is especially true when non monotonic reasoning is both present and necessary to interpret the evidence at hand williams 2009 argues that there cannot be natural necessity in the social world but only social necessity in probabilistic terms of the word natural necessity requires the presence of consistency and regularity williams 2009 which is often and commonly encountered in the physical world e g ecosystem function and processes but not in the social world of interactions non monotonic phenomena in human judgment e g induction abduction belief revision knowledge based reasoning clearly showcase how variation in levels of certainty uncertainty of evidence or knowledge or information leads to nonlinear and often profound shifts in inferences or conclusions reached brewka et al 2004 pinkas 1995 rott 2001 voorbraak 2004 furthermore collective reasoning propositions are probabilistic or probabilogic and variational by default chater et al 2006 haenni 2005 in other words they vary across individuals within populations and across populations within space and time 1 3 the dual character of collective social representation there are two key characteristics and functions of collective social representation relevant to this discussion the first involve collective dynamics as inferential mechanisms that explain and describe social patterns at the collective level such mechanisms of inference are necessarily heuristic by nature and serve the purpose of allowing interpretation of social actions in the context of whole communities and societies the second considers collective dynamics as means of reasoning about societal wide processes decisions behaviors and actions such processes do not necessarily fall under the realm of inferential or heuristic assessments but rather serve as indicators for explaining the functionality of collective social structures as such i e facilitating the emergence of individual processes by elevating their semiotic or ontological importance and operation at higher levels of collective social hierarchies in the first category we can ask for example how individuals form and facilitate collective processes such as semantic emergence social roles and social actors centrality prestige brokerage etc in the second category we need to ask how existing collective structures regardless of their constituent formation process allow for individuals to reason about changes including social change oliver et al 2012 1 4 semantic inference and informational propositions semantic similarity represents a correlated function of space and time continuum within a certain state of social structure semantically similar representations of social reality imply a certain degree of shared understanding of such reality or otherwise stated a shared level of knowledge representation within a shared or collective mental mode two important parameters and their combined functioning are implied in such propositional formulation first let us examine the case where a number of individuals sharing a certain level of semantic similarity in their knowledge representation as this emerges through stated or revealed empirical inference such semantic similarities are assessed through a socially explicit or implicit contextual environment or social space the latter represents an assumptional proposition and relates to the nature of the empirical evidence at hand or the empirical nature of the evidence sampled through our experimental social science methods such individuals within such a socially relevant context are more likely than not to occupy a level of shared functioning within their respective social space or environment from the individual standpoint this is to say that there is an expectation of a certain level of overlap within the social universe of their interactions such as sharing group memberships having certain similarities on their social and demographic background or sharing one or more social economic or cultural characteristics this social topology albeit not directly assessed in terms of shared spaces is reflected through their shared semantic knowledge representation implicitly and axiomatically from the collective standpoint one can see the social system of individual interactions as a form of topological density distribution whose boundaries are multidimensional and whose dimensions in turn represent social characteristics i e the fabric of social structure under study regardless the approach taken from the individual to the social or from the social to the individual the level of semantic information encoded in the empirical structure of evidence represents at least in part a judgmental heuristic on the collective social space occupied by the semantic distribution and their associations this assumptional proposition of course is rendered more valid as another testable assumption of semantic specificity becomes stronger or supported by the empirical data consequently the more general the semantic knowledge representation is the less warranted may be the assumption of its correspondence with shared overlapping social space and therefore the less likely it may be that semantic similarity relates to collective social interactions in a latter part of this paper we will explore further the theoretical and empirical consequences of such propositional inference secondly let us consider the theoretical statistical inference coming from information theory as early as the late 1950 s it is shown by jaynes that within a statistical subjective interpretation of informational entropy the probability of a certain state can acquire a temporal interpretation i e formulated in the basis of a fraction of the time that the system spends in state n jaynes 1957 p 627 within a socially relevant analytical framework such a subjective probability can be viewed as the level of exposure conditional to the characteristics of the social structure under study the matter of temporal conditional exposure inferentially relates to the emergence of semantic similarities within a collective social knowledge framework as in the previous case this level of temporal exposure can be viewed both from an individual and from a collective social perspective from the viewpoint of the individual the lengthier the exposure i e the fraction of time spend at state n in relation to the total time under study the more likely it is to either diffuse knowledge or acquire knowledge or both at all cases such knowledge to be assumed to be a probabilistic distribution function influenced by exposure levels from the viewpoint of the social perspective the density of social interactions may relate to the density of time exposed to social ideas or social conditional structure in both viewpoints exposure can be subsumed to be associated with shared semantic structure of social interactions on the other hand the level of association between exposure and semantic cohesion can be negatively influenced by the intensity of the social interactions themselves such intensity like the spatial considerations deepens as specificity increases and weakens with increasing levels of semantic generality at the extremes of the spatio temporal arguments the informational entropy of the semantic inference is rather trivial for as we all exposed to a certain shared level and representation of the world around us i e a level of generality eliciting global or universal social value or ideological worldview systems there is a trivial level of generality that renders scientific inference as uninteresting similarly in the heart of the structure of our social interactions all individuals within a society face the eventuality of having a minimum level of shared understanding of reality their social environment and a level of shared exposure at abstract social states e g national or international ideological general dispositions regarding fundamental aspects of humanity etc nevertheless none of the above arguments would mean much if it was not for an important range of statistical discoveries in the areas of computational linguistics and the statistical nature of human language from its semantic knowledge representation point of view the integration of linguistic sociological mathematical and statistical sciences cannot be better served as in the case of the literature observing the theoretical and empirical nature of the semantic structure of human language generalized semantic language distributions are shown to follow a mathematical distribution widely known as zipf s law piantadosi et al 2011 1 5 structure of the study and research questions this study attempts to address three important research questions first what is the structure and characteristics of environmental knowledge across individual stakeholders communities of practice and institutional arrangements secondly how well semantic network representations of environmental knowledge represent collective social processes and interactions thirdly how and to what degree the structure and self organization of semantic networks influence the nature and characteristics of collective knowledge itself to answer these key research questions we employed a multi stage multi dimensional analysis framework shown in fig 2 and explained in detail in the methodology section the adopted analysis methodology follows the state of the art literature in terms of semantic network analysis dib et al 2018 fronzetti colladon 2018 l li et al 2017 reagan et al 2017 takase et al 2016 especially in relation to the body of literature related to environmental knowledge redington and chater 1997 wesche and armitage 2010 and complex social ecological systems alexandridis and maru 2012 tàbara and chabay 2013 modeling social ecological knowledge through semantic network inference has provided valuable insights into the complexity of interactions that constitutes our complex coupled human natural systems of reality it has similarly enhanced our awareness and understanding of the social institutional and systemic forces that influence and being influenced by environmental change while driving social and economic response mechanisms and strategies including those related to adaptation to climate change 2 materials and methods the methods used in this study include methods for a data collection methods including generating a natural text semantic corpus dataset with participant and group attributes from qualitative focus group and workshop exercises b semantic categorization and classification methods including performing latent semantic indexing and linguistic categorization along with clustering and classification of semantically categorized entities and c semantic network analysis methods including graph theoretic network structural analysis of the data 2 1 data collection methods the data used in the analysis based on five scenario based focus groups in the island of st thomas us virgin islands and one large multi stakeholder international workshop in sarasota florida the us virgin islands focus groups were conducted during the period of september 2012 through january 2013 while the florida workshop was conducted in october 2015 key characteristics for each of the two case studies are shown in the following table 1 the key demographic characteristics of participants in the us virgin islands are also described further in webb 2013 and to an extent mirror basic demographic characteristics of the us virgin islands at large the participants in the florida dataset was based on an international workshop on integrated local environmental knowledge and included participants from the scientific community in the mote marine laboratory local and state agencies local organizations and ngos along with community volunteers a total of 13 focus group workshop exercises are included in the analysis five in vi and 8 in fl incorporating narrative perspectives of a total of 57 participants 31 in vi and 26 in fl in both case studies the focus groups and workshop group exercises followed specific thematic protocols in order to ensure consistency of discourses there were four thematic sections in each of the focus group exercises in the us virgin islands and two thematic sessions in each of the workshop exercises in florida specifically the thematic entities for the us virgin islands were i discussing drivers of environmental change drivers ii defining and discussing environmental sustainability sustainability iii developing scenarios for the future scenarios and iv discussing social ecological resilience resilience similarly in the florida case study the thematic entities discussed by the workshop participants were discussing environmental restoration efforts restoration and discussing environmental stewardship approaches stewardship the discourse narratives in both case studies were audio recorded for each group exercise and were verbatim transcribed following transcription the raw text files underwent a pre processing sequence 1 identify case studies each case study was assigned a unique study id sid 2 separate discourse narrative cases in each transcription a discourse narrative case is a unique set of one or more paragraphs for which a single participant talks during the discourse exchange each of these discourse narrative cases represents a unique row on the database and is identified through a unique variable named discourse id did 3 identify discussion theme for each transcription we identified and separate each thematic group of the discourse the theme of the discourse these were provided with a unique theme id tid across case studies and across focus group or session replications of the experiments 4 identify participants for each discourse id we identify the person participant who provided it and was added in each row using a participant id pid variable 5 identify focus group session for each of the case studies we accumulated transcription data for each of the focus groups session replications of the experiments each group session was given a unique group id gid that identifies the session group replication 6 identify participant attributes each participant is associated with a number of attribute data including gender age occupation group in which they participated these were added as column variables for each row associating attributes with participant data 2 2 analysis methods 2 2 1 data pre processing the transcribed text data were slightly modified in order to a remove all transcribed instances by the facilitator in the study during this step only the discourse statements made by the participants themselves remained in the study b correct grammar or spelling errors and convert spoken linguistic forms to formal ones e g can t to cannot i ll to i will etc and c converted to a tabular format containing one discourse statement per row along with key participant group and theme characteristics as additional columns the modified data generated a text corpus database that was used in the semantic analysis of the study the final textual corpus database constructed includes 2618 discourse narrative cases 1001 38 2 for the virgin islands case study and 1617 61 8 for the florida case study on average it contains 45 9 narrative cases per participant or 3 5 of total narrative cases per participant while the absolute number of cases per participant differs between virgin islands 32 3 narrative cases participant and florida 62 2 cases participant their overall fractions as percent of total cases is similar 3 2 for vi and 3 8 for fl the final dataset also contains on average 45 9 narrative cases per focus group session or 3 5 of total cases per focus group in terms of its linguistic content the textual corpus characteristics and summary statistics are shown in table 2 the main units of analysis for the main analytical methods used in this study are cases paragraphs sentences and words thus the size of the textual corpus dataset has adequate statistical power for the latent semantic indexing and parameter estimation methods used for more information on the final dataset see s3 file 2 2 2 semantic extraction extracting semantically relevant linguistic concepts from the textual corpus involves a number of process steps first the textual corpus is checked against an exclusionary list identifying and removing temporarily from the text corpus non linguistic words i e only linguistically and semantically relevant concepts are kept in the text the exclusionary list is based on formal english language dictionary the remaining corpus include verbs connectors modalities adjectives pronouns etc secondly the concepts remaining in the linguistically relevant text corpus are tokenized tokenization includes concept substitution through an english lemmatization algorithm i e further reducing any infected spelling of words to its lexical root reducing words to their canonical forms including plural singular forms past tense present tense etc lemma form each of the lemma form represents a linguistic token that is used for the categorization process in the next session the semantic processing for the textual corpus data in this study reduced the original 188 912 words to 106 144 tokenized words thus reducing the size of the corpus by 43 8 the percent reduction was almost identical across the two case studies 43 7 for vi and 43 9 for fl the process corpus includes 3825 word forms 2427 for vi and 2639 for fl the ratio between type forms and token words in 0 036 0 046 for vi and 0 05 for fl further summary semantic processing statistics are shown in table 3 2 2 3 concept categorization and association semantic concept categorization from textual corpus data depends in its original stages on identifying co occurrence patterns among linguistically relevant words within cases paragraphs or sentences of the corpus therefore worlds with higher co occurrence frequencies not only appear together in the textual corpus but also do so in high frequencies b li wang and zhang 2012 given the fact that our textual corpus dataset includes multiple participant and focus groups higher co occurrence frequencies also reflect a higher level of collective mental models of knowledge representation alexandridis and maru 2012 in other words they reflect a more social rather than cognitive or individual perception of reality on the other hand semantic similarity in patterns of co occurrence allows us to generate measures of paired association among concepts words or categories groups of words with relatively high frequencies of co occurrence within a textual corpus such associations in the forms of semantic networks are shown to be hierarchical by nature crestani 1997 ravasz and barabasi 2003 thus the scalability of such networks allows us to retrieve semantic networks from data ranging from almost full graphs to abstract graphs the research question that emerges in such cases is what is the similarity threshold that maximizes the informational content i e minimizes the informational entropy of the knowledge contained within them this question relates closely to the isomorphism property of semantic networks alexandridis and maru 2012 chia and ong 2006 and the scale free distribution of semantic association in graph theoretic terms therefore given a scale free semantic network the previous research question becomes a matter of finding the minimum cut off threshold level of semantic similarity for which the scale free properties of the network and thus its semantic isomorphism remains unchanged at such level adding more nodes to a network who s link distribution follows a power law will not change its network structural characteristics and the network becomes simply saturated in order to test the latest proposition we adopted a study design that a fixes the number of nodes in the semantically extracted networks to the top 100 concepts based on their analytic tf idf index and b varies the threshold of associative semantic link similarity of co occurrence patterns by using its percentile distribution in terms of the fixed number of nodes this step is necessary in order to perform comparative network analysis i e the networks vary only on their structure and not by their size in addition the tf idf coefficient representing the product of the term frequency how frequently a term appears across paragraphs sentences etc and the inverse document frequency discounting terms whose expected within case frequency is larger tf idf is a commonly used information retrieval metric used in latent semantic indexing ai et al 2010 rivera et al 2014 xagi et al 2010 symbolically 1 t f i d f t f t d i d f t d f t d max t d f t d log d 1 d d t d where t is a given term word or category d is a given document case in the corpus f t d is the frequency of term t in case d t is any other term appearing in case d d is the number of cases in the corpus the denominator of the inverse document frequency is adjusted adding 1 to avoid division with zero and computes the number of cases where the term t appears in terms of the number of links in the study following the concept categorization and association process in the next session we first computed the percentiles of the semantic similarity appearing in all pairs of the top 100 nodes selected by their largest tf idf and then generated three network versions using the 5 keeping 95 of the pairs the 50 keeping 50 of the pairs those above their mean semantic similarity coefficient and the 95 keeping the top 5 of pairs based on their similarity coefficient in the analysis section we analyzed all three networks for each of the three cases a network containing both case studies in a unified corpus a network containing only the virgin islands text corpus cases and a network containing only the florida text corpus cases the primary concept categorization was created by the data using a simple categorization algorithm after semantic extraction process described in the previous session the algorithm involves the following steps 1 adds word categories with minimum frequency of occurrence in the corpus f min 10 only words that appear in at least 10 cases are kept 2 calculate tf idf values for the kept categories and sort by decreasing tf idf values 3 keep the top 100 concepts with the highest tf idf values and drop the remaining the categorization algorithm was run for three corpus configurations once for all case study data combined vi and fl and once for each of the case studies separately each of the configurations generated a list of 100 categories in addition to the primary categorization algorithm we also implemented alternative categorization algorithms using known semantic categorization dictionaries to compute categorization statistics for these alternative classifications the algorithm computes frequencies of occurrence in the textual corpus for each of the dictionary defined categories and their relative tf idf values the dictionaries used for the alternative categorization are presented in the next paragraphs the wordstat sentiment dictionary wordstat 2016 combines positive and negative semantic words from the harvard iv 4 dictionary stone 1997 the martindale s regressive imagery dictionary martindale 1981 1986 and the linguistic and word count dictionary tausczik and pennebaker 2009 the martindale s regressive imagery dictionary martindale 1981 1986 contains 3000 words classified across two fundamental constructs primary or primordial processes and secondary or conceptual thinking processes according to the relevant literature martindale 1999 smith et al 1995 textual corpus with higher fraction of secondary processes are representative of more structured and abstract intellectual capabilities grounded in social realities the laver and garry policy position dictionary laver and garry 2000 contains 415 policy related words and word patterns over 19 level 2 grouped categories while originally developed to analyze political positions from policy actors it does have analytical value for this study albeit not without limitations it can provide an indicative and comparative measure to the extent to which policy related issues form a part of the discourse data narratives it thus enable us to contextually evaluate the study s narratives with respect to the policy dimensions of knowledge construction 2 2 4 semantic network construction the semantically extracted and categorized concepts from the two previous stages were coded back to the original textual corpus by sentence paragraph and case in order to generate associative semantic knowledge network one additional methodological stage of the analysis was performed we computed a co occurrence analysis based on similarity of concept patterns of co occurrence per paragraph in the textual corpus we used the jaccard s coefficient of occurrence to calculate the similarity matrix of pattern association jaccard s coefficient of similarity is a well known statistical measure of association leydesdorff 2008 real and vargas 1996 symbolically for rows x i 1 n and columns y j 1 n in the n n concept co occurrence matrix the jaccard s index of similarity for any two cell elements of the matrix c ij x i y j is 2 j c i j x i y j x i y j x i y j where the nominator denotes paragraphs where both concepts occur together and the denominator denotes the sum of paragraphs in which concepts co occur paragraphs in which the first concept occurs but not the second and paragraphs where the second concept occurs but not the first the jaccard s similarity index range is 0 j 1 for each of the case study semantic similarity matrices a number of network attributes were also included from the original attributional dataset by accounting for attributes demographic group theme including case frequencies in which each concept was identified e g number of cases with female versus number of cases with male participant for each of the network nodes the use of similarity index is also used in network volatility and event link analysis hu et al 2017 to address the predictive ability and evolution of network structures since we are interested in directional semantic knowledge analysis we compared the difference in tf idf of the concepts in each complementary pairing of relationships i e comparing the tf idf difference between the x i y i and y i x i our methodological convention is to consider knowledge semantic influence moving from nodes with smaller influence to ones with larger influence i e more central nodes attract less central ones this methodological assumption is also supported by the preferential attachment property present in scale free network distributions alexandridis and maru 2012 newman 2001 thus the directional network conversion algorithm was 3 s i j s i j i f t f i d f i t f i d f j 0 i f t f i d f i t f i d f j where i j are row and column nodes respectively from to the zero condition applies in cases with identical tf idf values denoting the diagonal network cases i e where i j same node 2 2 5 semantic network structural analysis we computed a number of graph theoretic network metrics in the semantic knowledge networks of the study in general these belong to four group types a neighbor metrics measuring structural link characteristics of the semantic networks based on their associative connectivity patters drieger 2013 knoke and yang 2008 scott 2000 b centrality metrics measuring structural cohesion and centrality of network nodes overall jackson 2008 xu et al 2010 zhang 2010 c equivalence metrics measuring the efficiency and effect of connectivity patterns in the network reichardt and white 2007 sailer 1978 and d positional metrics clarifying the structural role that various node play in the semantic network alexandridis and maru 2012 bader et al 2008 haybron 2000 in addition we considered and analyzed statistical approaches and models of network structure such as the dyadic interaction block model p1 brendel and krawczyk 2010 white et al 1976 the non negative matrix factorization nnmf model hasan and zaki 2011 the local outlier matrix factorization lomf model singular value decomposition svd sivakumar et al 2011 watkins 2004 and hierarchical clustering models ravasz and barabasi 2003 based on both continuous and categorical network association metrics fig 3 below provides a graphical representation of the graph theoretic network analysis metrics computed and used in this study left subgraph along with additional statistical network models right subgraph the metric variables denoted in red circles was first computed and analyzed from the semantic knowledge networks in the study the composite coefficients green stars were calculated and used in evaluating the effect of semantic network structure on the formation and functioning of knowledge systems section 3 2 3 the aggregate variables blue diamonds are only shown in the graphs as a level of typological grouping categorization of factors rather than computed variables we further evaluated the presence of scale free distributions in our semantic knowledge networks we estimated the hypothesis that the connectivity jaccard similarity and the document corpus importance tf idf in our data follow a power law cumulative distribution symbolically we estimated the α parameter of the power law model where 4 p x c x α using the cumulative probability p x x min the evaluation uses a maximum likelihood method to estimate the alpha parameter exponent of the model we also used the kolmogorov smirnov goodness of fit statistic to test the null hypothesis that the cumulative probability distribution follows an exponential distribution with exponent α this methodology for evaluating scale free distributions in empirical data is described in more details the relevant literature barabási and albert 1999 clauset et al 2009 3 results 3 1 semantic categorization there is an overall 43 7 overlap across semantic categories in the data between the two case studies virgin islands and florida as can be seen in table 4 the mean tf idf and narrative case co occurrence frequency is 70 8 and 57 4 higher in overlapped categories the summary of the semantic grammar of the classified knowledge narratives is shown in fig 4 for all case studies in terms of the verbs used almost 42 reflect stative statements i e statements expressing a specific state or situation another 31 are factive verbs i e used in statements asserting facts or presupposition of truth statements the third important category reflexive verbs 26 are mainly used in statements describing personal or collective actions or processes involving oneself or social groups in terms of connectors addition 43 5 of classified narratives reflects the additive nature of environmental knowledge both as a process and as an equilibrium state in terms of semantic modalities the collective narrative discourse in both case studies reflects a mixture of modal states including manner space time intensity negation assertion and doubt the use of adjectives in the collective knowledge narratives in both case studies reflect primarily an objective semantic state 63 reaffirming the central role of local environmental knowledge as a broader mental representation of the social construction of reality with a degree of subjectivity 26 this fact is further reflected at the analysis of the pronouns used in the participant s classified narratives a total of 46 8 of the narratives reflect individual pronouns i you he she covering both objective and subjective adjective statements while another 33 9 of the narratives reflect purely collective statements we they somebody when participants discuss their knowledge with relation to social realities 3 1 1 alternative categorization schemes in order to understand in better ways the context and content of the narrative environmental knowledge captured in our data we applied a number of alternative classification schemes in the corpus dataset specifically we classified the narratives using a the wordstat sentiment dictionary loughran and mcdonald 2011 wordstat 2016 b the martindale s regressive imagery dictionary martindale 1975 1981 and c the laver and gary policy dictionary laver and garry 2000 the graphs in fig 5 below summarize the alternative classification grouped categories for the combined textual corpus data and for each of the separate case study s corpus subsets the top row of graphs showcases the application of the wordstat sentiments dictionary classification the bottom row of the figure summarizes the results of applying the regressive imagery dictionary classification in all cases we used the highest level hierarchical categorization level 1 in both classification schemes the darker color bars represent the percent of sentences in the corpus classified within each category while the lighter color bars show the percent of paragraphs cases classified for the same category in terms of the positive negative distinctions captured in the wordstat classification in all replications of the study the positive discourse narrative aspects represent at least 70 of the classified cases the respective negative coverage of the corpus ranges between 50 and 60 in terms of the emotional primary secondary continuum captured in the rid classification consistently the of cases classified as secondary ranges from 62 to 75 with primary categories ranging from 46 to 60 and the emotional ones ranging from 17 to 34 similarly fig 6 summarizes the laver and garry policy positions dictionary categories level 1 for the combined corpus data and for each of the case studies separately the classification results show consistently economic aspects of policy related discourse occupying a significant percentage of the textual corpus coverage between 24 and 30 followed by cultural policy dimensions coverage ranging between 12 5 and 24 5 3 1 2 categorization by attribute characteristics we analyzed the original semantic categories with respect to the participant group and theme attribute characteristics in terms of the key participant demographic characteristics table 5 shows the case frequency of the top up to five semantic categories in both case studies by gender and age group in terms of the gender attribute the top five categories statistically significant favoring male over female participants p 0 001 are more or less representative of objective outcome realities and situational characteristics of social ecological realities they focus on local products agriculture or market in general in the virgin island case and on restoration water and bay related issues along with time related issues in the florida case on the other hand the top categories that favor female over male participants have generally less statistical power than the ones for the male participants they focus more on institutional arrangements processes and generally more subjective perceptions of reality in fact in the virgin islands case female participants talk more about community and enforcement issues general environmental issues island wide in the florida case study female participants are more differentiated by discussing issues related to feelings meetings and other similar subjective aspects of reality in terms of their age group characteristics the top categories favoring higher frequencies for young adults than any other participants are ones referring to job opportunities start general impacts school and action in the virgin islands case study given that most of the young adult participants in the vi case are school teachers and young volunteers it is clear that these discourse topics reflect a more participatory experiential and bottom up reality driven narrative on the other hand in the florida case study the same categories e g school student education differentiate more the other age group categories the multi dimensional scaling analysis by attributes of the semantic text corpus data are shown in the following figs 7 and 8 in all cases the first three dimensions have eigenvalues above 1 0 and they explain on average 84 4 of the variability in the data 76 5 for vi 80 5 for fl and 96 1 for the combined case studies the classified semantic categories are plotted using their estimated multi dimensional scaling coordinates in each of the paired dimensions with the third dimension shown as a contour then the mean x y and z coordinates for each of the focus group in each of the case studies are calculated and plotted in the same graph nodes or categories closer to the axe s center i e axes with value of zero are closer to the mean values of each dimension fig 7 plots the semantically classified knowledge categories against the study s focus groups representing relatively cohesive social professional and institutional participant groups similarly fig 8 plots the semantically classified knowledge categories against the broader occupational groups of the participants we generalized specific occupations across both case studies to differentiate between participant roles as scientists educators business professional government agency participants or community ngo groups 3 2 semantic knowledge network analysis 3 2 1 summary of key semantic network metrics fig 9 provides a visual network representation of the neighbor in degree distribution among semantic nodes in the knowledge network for the case study narratives nodes with larger diameter have higher neighbor in degree coefficient which indicates the degree of semantic knowledge in flows from the periphery to the center weighted by the semantic similarity of the links three types of nodes based on their coefficients are identified ordinary nodes receiver type nodes and transmitter type nodes the receiver nodes are generally receiving strong semantic knowledge influence flows while the transmitter nodes are strong sources of semantic influence flows the structural hole network analysis for the classified narrative data provided some very interesting results specifically the degree of redundancy in the network connectivity link connectivity is 27 2 for the network constructed with all studies narratives 22 1 in the virgin islands network and 24 1 in the florida network the opposite results hold true for the network s efficiency which represents 1 redundancy summed over all network nodes the efficiency coefficient for all narratives is 67 9 while the efficiency for the virgin islands network is 70 6 and 69 2 for the florida network another important neighbor network analysis concept is that of homophily neighbor s homophily analysis shows us the degree to which the nodes of a given network are similar in a certain property with it s neighbor s i e the nodes with which the focal node is connected with homophily reflects the property of birds of a feather flock together we analyzed our semantic knowledge network homophily characteristics with respect to their tf idf classification indexes we compared each node s tf idf value ego with the mean tf idf value of it s neighbors alters and computed the mean differences across all nodes in each knowledge network we finally compared these differences within the size of the network using the 5 expansive 50 medium and 95 smaller percentiles of network sizes our results indicate a decreasing pattern of differences in tf idf values between ego and altars specifically the difference for the expansive network is 30 0 from 89 84 to 62 93 for the medium size network drops to 27 4 from 89 84 to 65 25 and drops further to 18 9 from 93 97 to 76 25 for the smaller network we computed statistics for the shortest path between any two nodes in our semantic networks the shortest path algorithms identify the geodesic the path with the shortest possible distance from all possible alternative path connectivity between two nodes in a network we computed summary statistics of the shortest path distance measured in standardized euclidean distance from 0 to 1 with 1 network diameter we used the 95 percentile networks which reflect the top 5 of the connectivity relations in the network the mean geodesic distance for the combined case study network is 0 115 0 109 for the virgin islands case study and 0 131 for the florida case study the percent of reachable nodes outwards from the center to the periphery is 30 7 of the nodes vi 28 7 fl 27 0 on the contrary the percent of reachable nodes inwards from the periphery to the center is 12 8 of the nodes vi 13 2 fl 15 3 our results also show that the percentages of out reachable nodes and in reachable nodes tend to become more equalized as the network size increases because of the scale free nature of link relationships the results of implementing a path finder network algorithm pfnet based on euclidean distance dissimilarity are shown in fig 10 the pfnet dissimilarity process implements a minimum spanning tree network algorithm nepomniaschaya 2006 schvaneveldt and norwood 1990 i e finds the optimal tree structure in a network that minimizes the sum of tf idf similarity coefficients in the semantic network the resulting pruned networks represent the minimal path structure of the semantic knowledge that keeps the network connected in the two case studies the pfnet dissimilarity algorithm yields a semantic knowledge tree structure that includes 102 out of 3740 links in vi and 102 out of 3875 links in fl 2 73 and 2 63 of the full semantic networks respectively using an average 7 6 of the total tf idf similarity weights the status katz centrality coefficients for the semantic networks of the two studies show that in status centrality from the periphery to the center of the network is decreasing by almost 50 in comparison with the out status centrality in both case studies these results are clearly quantifying the relative importance and influential role that central knowledge structures play in the network four additional network structural analysis metrics yielded statistically significant differences between the two case studies the neighbor level ego density larger in vi the information centrality larger in vi the local reachability density factor slightly larger in vi and the k distance factor larger in fl since the distributional assumption of normality is violated in all four variables the kolmogorov smirnov test for normality was statistically significant we used non parametric testing for the difference both the kruskal wallis rank test for equality of populations kw and the wilcoxon mann whitney rank test mw had statistically significant chi square kw and z mw values for the ego density χ 2 98 12 z 9 91 p 0 0001 for the information centrality χ 2 24 94 z 4 99 p 0 0001 for the local reachability density χ 2 27 07 z 5 20 p 0 0001 for the k density χ 2 22 80 z 4 78 p 0 0001 the ego density coefficient shows that associated neighbor groups of semantic knowledge concepts tend to be denser in the vi and more loosely connected in the fl case study the information centrality coefficient shows that transmission of knowledge as information flows is stronger in the vi in comparison with the fl case study the local outlier factor matrix model coefficients local reachability density and k distance show that knowledge is slightly more reachable within k nearest neighbors in the vi case study while the average distance of any knowledge node to its k nearest neighbors is higher in the fl case study local reachability lrd k and k distance d k are closely related since 5 l r d k x y max d k y d x y for any two nodes x and y in the network campos et al 2016 jackson 2008 the box plots comparing the two case studies for the four identified coefficients are shown in fig 11 3 2 2 evaluating power law distributions in semantic networks we assessed the presence of scale free distributions both at the semantic knowledge network level as well as the node attribute distribution level as described in the methodology section we used the kolmogorov smirnov statistic to evaluate the presence of power law relations clauset et al 2009 table 6 shows the results of the statistical evaluation the α coefficient represents the estimated exponent of the power law relationship the x min coefficient denotes the probability estimation of the model i e p x x min the k s coefficients provide the values of the kolgomorov smirnov statistic text the p values of the statistical evaluation with the exception of the 50 and 95 percentiles reduced networks in the combined case study networks are not statistically significant not allow us to reject the null hypothesis that the network jaccard similarity in degree coefficient follows a power law scale free distribution in other words the semantic knowledge influence increases exponentially as more we move from peripheral towards collectively shared knowledge concepts visually the presence of scale free distributions in our case study knowledge networks is also shown in the graphs of the following fig 12 the y axis of the graphs shows the cumulative probability distribution function cdf of the in degree jaccard similarity in each of the case studies the x axis shows the raking of the 100 semantic network nodes categories rank 1 is assigned to the node with the highest in degree and rank 100 to the node with the lowest one the axes are plotted on a log scale the graphs show how few categories marked as dots have the highest in degree similarity while the majority of the nodes have a moderate or low in degree similarity 3 2 3 evaluating the effect of semantic network structure on the formation of knowledge systems the computed raw semantic network graph theoretic metrics are divided into four major types neighbor metrics measuring neighbor link based characteristics centrality metrics measuring node and network based characteristics equivalence metrics measuring structural network equivalence characteristics and position metrics measuring role based network characteristics these graph theoretic types of analytical groups represent some unobserved latent variables related to each of their described characteristics we constructed latent composite scale factor models of these types using a reliability analysis specifically we evaluated the best subset of the metrics that maximize their cronbach s alpha reliability coefficient cronbach s alpha estimation allow us to assess item graph metrics reliability towards a latent scale we perform each alpha estimation for each of the case studies as well to the combined corpus narrative data consequently four latent scale coefficients were computed for our network metrics data neighbor scale centrality scale equivalence scale and position scale coefficients the alpha values ranged from 0 943 to 0 958 for the neighbor scale 21 items metrics 0 960 to 0 963 for the centrality scale 26 items metrics 0 751 to 0 922 for the equivalence scale 10 items metrics and nearing 1 0 for the position scale 2 items metrics the average inter item correlation ranged between 0 31 and 0 54 across case studies and scales the full reliability estimate statistics for the four composite network scales can be found in appendix 2 we fitted a nonlinear exponential growth model between the tf idf values measured from the latent semantic analysis of the narratives and the four composite semantic knowledge network scale indexes for each one of the main network systemic characteristics neighbor characteristics network centrality network equivalence and network position the symbolic form of the estimation is 6 y c e β 0 β 1 x 1 β i x i or 7 t f i d f e β 0 β 1 x n e i g h b o r β 2 x c e n t r a l i t y β 3 x e q u i v a l e n c e β 4 x p o s i t i o n ε the results of the exponential growth nonlinear estimation are shown in the following table 7 the top part of the table provides the model estimates and the bottom part the parameter estimation of the coefficients all case studies had excellent adjusted fit r 2 was excellent 0 9946 for the combined case studies semantic knowledge network 0 9879 for the virgin islands semantic knowledge network and 0 9956 for the florida semantic knowledge network the mean exponent i e the predicted values for the linear exponent component 8 β 0 β 1 x n e i g h b o r β 2 x c e n t r a l i t y β 1 x e q u i v a l e n c e β 4 x p o s i t i o n is around 4 4 1 for vi and 4 2 for fl with standard deviation of 0 4 the exponent s values range from 3 6 vi to 5 8 fl as can be seen from the parameter estimates in table 7 the effects of the neighbor coefficient increase tf idf by 26 5 in the virgin islands versus only 16 3 in the florida case study the results are opposite for the centrality coefficient its effect is smaller in the vi 29 7 than in the fl case study 35 5 the equivalence coefficient is marginally negative in the vi decreasing tf idf by 3 6 and marginally positive in fl increasing tf idf by 7 1 finally the position coefficient is marginally negative decreasing tf idf in both case studies by 6 and 6 5 in vi and fl respectively the predicted values of the exponential growth model estimation are shown in the following fig 13 in all case studies the structure of the semantic network characteristics has exponentially increasing combined effects on the semantic importance of the narratives 3 3 network clustering classification the following fig 14 below presents a visualization of the major cluster group of contents classified through a community graph plot algorithm cit the algorithm enabled the identification of seven broader cluster groups in the virgin islands case study and eight cluster groups in the florida case study the algorithm maximizes cross group semantic distances and minimizes within group semantic distances based on estimated cluster membership profile heuristically as can be seen from the graphs the two case studies have a strong degree of overlap in four key areas a knowledge related to economic considerations socio economics and business in both case studies b educational knowledge education community outreach in vi vs science education in fl c knowledge related to agriculture and fishing livelihoods agriculture and culture in vi vs fishing and fisheries in fl and d local ecological knowledge related to sustainability and stewardship central components in both graphs on the other hand in the virgin islands the community participants focus more on knowledge narratives related to policy and action as well as environmental planning and management themes that are not emerging in the florida case study florida participants focus on ecosystem and ecological aspects of marine and environmental management including water quality along with emphasizing the relationship between science and society both of these generative themes are not emerging in the virgin islands case study in addition to the network level clustering of semantic knowledge we also applied an alternative clustering classification based on natural language processing specifically we performed a factor analysis on the 100 extracted concepts using their co occurrence frequency matrix by paragraph in the corpus document in each of the case studies we selected the ten factors with the highest eigenvalues the factor scores were rotated using a varimax method the eigenvalues range from 8 5 to 1 72 in the virgin islands case study and from 8 1 to 1 69 in the florida case study the category with the most extensive coverage of the textual corpus percentage of cases in the virgin islands case study was social ecological impacts 33 1 while the one for the florida case study was science and community 17 4 following the factor analysis a co occurrence jaccard similarity analysis was performed over the factor groupings classes across the two case studies the following fig 15 shows a heatmap graph of the standardized z score jaccard similarity coefficients we can see that the stronger similarity occurs between the science and community in the florida case study and the environmental policy management drivers of environmental change pair of factors in the virgin islands case study strong similarities also occur in general economic impacts in both case studies and between environmental education fl and livelihoods and opportunities vi overall the total overlap between the two factor categories in terms of the semantic textual corpus is 60 1 compared to the 43 7 overlap observed in the original categories see section 3 1 there is an additional 16 4 overlap that is due to the generalization of semantic knowledge categories 4 conclusions and discussion in terms of the overarching goals and research questions of the study the results provide a clear and concise description of the structure and characteristics of the environmental knowledge across participants i e stakeholders communities of practice similarly through the network classification and groupings of semantic networks our model results raise a series of important dimensions related to the institutional and social arrangements that are related to core environmental knowledge the study results clearly provide a data driven semantic network representation of both environmental knowledge structures at the socio linguistic and cognitive level and the collective social level of discourse interactions finally the social network analysis results in terms of centrality density neighborhood effects structural hole equivalence redundancy efficiency homophily shortest path analysis and power law analysis provide evidence of the strong relationship and association between the structure of self organization in semantic inference and the characteristics of collective knowledge as a social ecological system of interactions the categorization overlap analysis indicates that categories that appear in both case studies generally have significantly higher co occurrence and tf idf values than the categories that uniquely appear in each case study thus the central core of the semantic knowledge network is shared between the two studies reflecting the extent to which environmental local knowledge represents a broader global veridicality of social ecological knowledge conversely the remaining uniquely identified semantic categories in each case study approximately 56 of the concepts reflect discourse narratives that are either specific with respect to the locality or the semantic discourse theme or the group participant characteristics the application of the three alternative categorization schemes to the textual narrative corpus data allows us to examine some general contextual aspects of the collective knowledge discourse captured in our data specifically it appears that knowledge involves both positive and negative aspects of social ecological change with the former group being more prominent than the second overall we can see that in general a positive row balance characterizes the participants environmental knowledge reflecting a more optimistic outlook we can also infer from the rid classification results that local environmental knowledge reflects by large a significant proportion of secondary processes such processes involve analytical logical and abstract thinking rather than primary and emotional responses while the latter do have a place in the composition of our collective knowledge structure their role is not as prevalent as the secondary aspects of knowledge processing these results also allow us to clearly understand why knowledge structures represents a more advanced and valuable system than simply processing information the qualitative difference between information and knowledge rests by large on the ability of our communities and social groups to engage in secondary processing of raw cognitive or situational stimuli e g emotional primary in comparing the attribute characteristics of study participants with respect of their semantic categories in the categorization by attributes session we identified how there exists a gender based gap in knowledge compositional characteristics female participants differ in terms of their focus on subjective and procedural aspects of knowledge as constructs of their social realities while male participants differ mostly by their focus on objective and situational outcomes this distinction is generally present in both case studies thus it is unlikely that it has much local cultural significant and more related to the cognitive and mental attributes of male and female psychology in general in that sense may represent an intrinsic characteristic of knowledge rather than one that is exogenously driven when we compare the age based attributes of the participants with respect to their respective semantic categorization frequencies the observed gap is of a different nature it has to do mostly in terms of the qualitative origins of knowledge on the one hand we have knowledge processes that are formed and driven by experience and situational perceptions of reality these experiential and situational forces represent generally bottom up participatory mechanisms on the other hand we have knowledge processes that are informed and driven by deontic processes i e what ought to or what could be rather than what is the latter knowledge processes in contrast represent top down often policy or institutional rule driven mechanisms this is the case when comparing the virgin islands young adult differentiating categories focusing mainly on career opportunities job school etc with the florida case study where similar categories favor the opposite age group more likely than not these results provide support for the proposition that the florida participants have a view of reality from outside experiential or situational settings the high degree of redundancy we found as part of the structural hole network analysis approx 27 allows us to make inferences on the structural stability and robustness of the local environmental knowledge as represented by the narrative knowledge semantic networks the higher the redundancy the more paths exists connecting any two random nodes in the network thus making difficult to disconnect parts of the knowledge network conversely the higher the redundancy of a network the lower it s efficiency the structure of the semantic networks analyzed for this study appears to maintain an approximate relationship of 2 5 1 between efficiency and redundancy i e a balance between marginal network connectivity and full network connectivity this is an additional finding re affirming our network results related to the presence of power law distribution in the network structure combined together they indicate an equilibrium state of the semantic knowledge network and therefore increases our inferential ability about the structure of the social ecological knowledge represented in our data the results obtained from the analysis of homophily for our semantic knowledge networks allows us to support the proposition that local social ecological knowledge structures become more cohesive in value and pattern as we move from the periphery towards the center of the knowledge network in other words the more central knowledge categories are the more they are similar to their connected neighbors and vice versa this proposition also supports the idea of an increasing homogeneity in semantic knowledge character as we move from the periphery to the center knowledge becomes more and more homogenous as it assimilates and spreads within the local community i e becomes more central the opposite is true with regards to knowledge heterogeneity heterogeneous knowledge structures are found more towards the network s periphery this is important especially when new knowledge and ideas enter the knowledge structures they first represent heterogeneous entities as they move up the network isomorphic hierarchy become more central they trade heterogeneity with homogeneity by acquiring values more closer to their connected neighbors the geodesic distance and reachability results of the shortest path network analysis of the data shows us that at least closer to the core of the semantic knowledge the percentage of nodes that are outwards reachable is significantly higher than the percentage of nodes that are inwards reachable in other words starting from the center node we can reach more nodes as opposed from starting from the periphery in terms of semantic knowledge structure these results provide support of the proposition that knowledge diffusion patterns how the central knowledge is diffused in the local community are stronger than knowledge acquisition patterns how new knowledge is acquired and assimilated in the community new knowledge patterns can reach and connect with smaller amount of existing knowledge than existing knowledge affecting new ideas this may be perhaps another quantitative way to show that new and innovative knowledge needs to reach a certain threshold in order to assimilate fully in the case of our analysis connecting with more than 12 of existing knowledge structures the multidimensional scaling for the focus groups reveal that certain focus groups have coordinates significantly deviating from the mean for example in the virgin islands case study groups representing more localized community groups such as the non government nrm group or the farmer s coop appear to be placed in the periphery of the multidimensional graph in the florida case study the second session of both the third and fourth focus groups also appear to have peripheral values at least on the second dimension these results allow us to infer that the discourse narratives captured by the semantic analysis have substantially diverging knowledge content and thus contribute more to the heterogeneity of knowledge in terms of the multidimensional scaling analysis of the occupational role of the participant s groups one important result warrants discussion by comparing the same group position across the two case studies we can see that while in the virgin islands the role of scientists and science organizations is semantically closer to those of community business groups in the florida case study scientists and science organization roles are more closely aligned with government agency views and knowledge discourse these diverging comparative results are perhaps indicative of two distinct models of the role of science or scientists as bidirectional translators of knowledge the first is characteristic of a bottom up approach where scientists and scientific organizations work closely and more tightly with the local community and locally embedded organizations the second provides evidence of a top down approach where scientists serve in support of government policy interventions or regulatory agency approaches to environmental and natural resource management while the efficiency of these two generative approaches is not evaluated as part of this study it is an important distinction that has both theoretical and empirical significance in the relevant scientific literature the results obtained by fitting a nonlinear exponential growth regression of the four network composite scale metric categories neighbor centrality equivalence and position to the semantic importance of node categories tf idf allowed us to assess both the combined and the relative effects of each of the four coefficients in predicting the structure of the semantic knowledge networks overall more than 95 of the tf idf in the semantic corpus can be predicted by the four network coefficients in other words the graph theoretic structure of the semantic networks represents can predict the importance of knowledge categories within a given thematic narrative of these four coefficients neighbor characteristics i e the structural connectivity patterns of knowledge and centrality i e the cohesiveness and structure of knowledge categories are the most important predictors in the virgin islands case study the most important predictor is connectivity neighbor but in the florida case study the most important predictor is cohesion centrality the latest results are also indicative of the two perceptual models of knowledge the bottom up in the vi and the top down in fl while both of the case studies allow for a mix of both centrality and neighbor influences to strongly influence knowledge structures the particulars of each mix allow us to make certain inferences the higher importance of how participants connect and associate their knowledge structure within their knowledge networks provides comparatively stronger evidence of a bottom up knowledge system it is more likely than not that new environmental knowledge generation in the virgin islands flows by new knowledge contributes to the overall connectivity of a network this type of relationship characteristic in the structure of network is particularly present in the case weak ties in small world networks omidi and masoudi nejad 2010 watts 1999 similarly the higher importance on how centrally cohesive are knowledge concepts in a network provides evidence of a top down knowledge system for the florida case study this type of network characteristics are more prevalent in centralized and tightly knit network structures borgatti 2005 these two distinct modes of knowledge structural dynamics can be reflective of the focal role of knowledge as a social construct of reality the first one favors knowledge acquisition patterns more than knowledge diffusion patterns e g virgin islands essentially implying that knowledge generation emerges closer to the fringe of the knowledge networks and flows toward the center the second reverses these trends favors more knowledge diffusion flows than knowledge acquisition implying knowledge generation emergence near the core of the knowledge structure flowing outwards the results of both the community graph plot clustering algorithm classification and the semantic factor analysis classification on the original textual corpus provide us with a broader bird eye understanding of the grouped thematic associations present in the two case study knowledge discourses while both of the methodologies demonstrate a strong power to identify and discriminate groups of knowledge concepts they also help us understand how thematically driven knowledge groups associate with each other there is a significant percentage of heuristic overlap in grouping classifications between the two case studies especially given the fact that the discourse narrative topics are thematically independent from each other at the time of data collection this in turn provides a strong indicator and argument in support of the theoretical proposition that we share a core of our environmental knowledge across communities and cultures we argue that more likely than not this part represents a more global part of environmental knowledge one that is part of our general socio cognitive understanding of environmental change it reflects our general cross societal and cross cultural knowledge related to global environmental change and it is likely to be more influenced by exogenous or broader factors that determine our social understanding of reality within a globalized world no matter what our individual not shared components of our local environmental knowledge are there will always be a fertile core of knowledge where one can plant the seed of true global environmental stewardship the one that connects communities of purpose sense of place and communities of practice with respect to social ecological systems acknowledgments partial funding for this research was provided from the us national science foundation office of integrated activities through the virgin islands experimental program for stimulating competitive research vi epscor award no 1355437 mare nostrum caribbean stewardship through strategic research and workforce development the authors wish to acknowledge the contribution of a number of colleagues including dr michael crosby mote marine laboratory florida usa for hosting and organizing the florida workshop dr hiroshi miki rihn institute for earlier and latter comments on the analysis trajectories and draft we also want to gratefully acknowledge the contributions of the study participants representing professionals agency and government officials community members volunteers and non government organizations for their participation in the workshop and focus group exercises appendix a supplementary data s1 appendix summary statistics of semantic network matrix a comprehensive list of tables providing detailed statistics of the computed semantic network metrics in the study s2 appendix network composite scores reliability tables comprehensive list of tables containing detailed composite score reliability coefficients for the four composite scales for each of the case study implementations s3 file semantic narrative corpus dataset the tabular dataset containing the corpus narratives for each study along with participant and field study attributes the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 026 
26328,this paper provides empirical and experimental assessments of thematic knowledge discourses based on two case studies in the us virgin islands and florida we utilize a latent semantic indexing analysis over natural language corpus to classify and categorize knowledge categories we computed tf idf scores and associated co occurrence jaccard similarity scores to construct semantic knowledge networks using network analysis we computed structural metrics over four composite groups neighbor based centrality equivalence and position the analysis show that structural network characteristics of environmental knowledge can exponentially predict associations between knowledge categories we show that connectivity play a critical role on acquisition representation and diffusion patterns of knowledge within local communities we provide evidence of a global prevalence of a shared knowledge core we show that core social ecological attributes of knowledge follow scale free power law distributions and stable equilibrium network structures we identify two distinct models of bidirectional translation a bottom up and a top down keywords semantic networks knowledge networks social ecological systems integrated local ecological knowledge latent semantic analysis social network analysis 1 introduction in the last decade multiple studies have underlined the importance of the relationship between knowledge and environmental conservation in many of these studies traditional ecological knowledge tek and or local ecological knowledge lek is are shown to have strong connections with environmental conservation and social ecological stewardship basurto et al 2013 becker and ghimire 2003 berkes and turner 2006 others tackle the relationship between scientific knowledge and sustainability kristjanson et al 2009 villa athanasiadis and rizzoli 2009 for example provide a review of semantic knowledge models addressing ecological and environmental modeling applications and discuss the broader adoption and feasibility of new approaches rivera minsker work and roth 2014 use a text mining framework to classify and develop sustainability criteria and indicators at a regional scale in many of these studies the concept of knowledge is addressed as a rather abstract and in a somewhat descriptive manner kiptot 2007 tàbara and chabay 2013 in other studies the content of knowledge was evaluated against decisions and behaviors related to environmental conservation and action in these local systems dutta morshed aryal d este and das 2014 grant and berkes 2007 and in some studies knowledge was measured as a list on survey responses cinner et al 2010 often the concepts of collective knowledge and social learning are used interchangeably and without a clear set of definitions and or boundaries related to their perspective functions and effective roles in the social ecological systems on the other hand many studies focus on formal or formalized ontologies and ontological frameworks martínez garcía et al 2018 polhill et al 2016 collective knowledge systems interact and operate across the full extent of social ecological systems they incorporate individual and cognitive characteristics of knowledge social perceptions of reality berger and luckmann 1967 ecological reflections of reality as well as institutional and governance dimensions knowledge systems exist and function in the heart of informal institutions and social norms but also directly relate our everyday social realities to formal institutional rules and arrangements social ecological stewardship critically depends and builds upon existing collective knowledge structures 1 1 effective institutional governance in managing social ecological commons our ability to examine and analyze the efficiency efficacy and effectiveness of our governance systems and the institutional arrangements in place and at work towards managing our commons critically depends first on the context nature and characteristics of the governance systems themselves it also depends on the weak network or web of connections between institutional processes that form management functions and key systemic components of the linked social ecological system the presence of weak links or ties between core institutional processes and both social and ecosystem functions are often the catalytic drivers of institutional change efficient governance systems with respect to social ecological system management can function both to the benefit of the ecosystem services and functions of the natural system and to the strengthening of the social system and the communities of practice within the management scales of reach from the ecological standpoint ecosystem health and ecological resilience are among the most important functions and processes that one needs to pay attention to albeit a number of secondary ecosystem and landscape processes bare significance to the analysis from the social standpoint the triplet of cognitive dispositional collective or social and institutional interactions represent important components of such an analysis fig 1 in social ecological systems the governance of the commons more often than not emerges as a function of local community social and ecological stewardship without it is difficult if not impossible to achieve governance structures that have the necessary legitimacy power and control to negotiate efficient and adaptive institutions of change cowie and borrett 2005 de vos et al 2013 it is exactly because such adaptive institutional arrangements reflect broader community and societal goals or aspirations the presence of a synergistic relationship between local stewardship and governance of the commons is critical ghorbani et al 2017 on the other hand stewardship at the community level alone cannot successfully achieve social ecological sustainability and resilience in systems where multi governance of commons is required this is because of systemic and institutional externalities entering and likely affecting the social ecological local system in question the degree to which a successful level of management is achieved in the dual stewardship governance of the commons system perhaps has something to do with the ability and capacity of transference across linked systems and domains of knowledge the capacity of the dualistic system of knowledge transference is inevitably linked to the capability set and network of interconnected associations across and within the key players or actors of both systems such systemic interconnectedness critically depends on the functioning and roles of knowledge producers knowledge translators bi directional and knowledge diffusers in the system the three roles are complimentary mutually reinforcing and non exclusive of each other 1 1 1 knowledge producers they emerge and function at both scales of the two subsystems local global yet the nature of the knowledge produced at each scale differs substantially albeit dependent of each other local knowledge producers operate at the community scale in the fringe of the natural social prevalence of the phenomena problems they acquire and produce knowledge by merging and combining empirical or observational data information with local and traditional ecological knowledge as well as with scientific knowledge the produced knowledge more likely than not is raw unprocessed and customized to fit the level of local community understanding of reality and the community mental knowledge representation global knowledge producers at the level of the governance of the commons often produce knowledge that adheres to strategic and policy objectives and social ecological imperative realities in which the commons exist and operate they accumulate and synthesize new knowledge by combining and contextualizing or re contextualizing existing knowledge emerging from the local community mental model interactions the level of efficiency in such contextualization and synthesizing of new knowledge depends to the degree in which the latter finds its way or reaches their operant level or scale of perception 1 1 2 knowledge bidirectional translators they operate often at the margins and neutral zone between the local and higher scales of inference their role is to bi directionally translate knowledge from the one subsystem to another and vice versa sato 2014 they often seek and assist stewardship efforts at the community level all the while promoting cooperation and coordination among other players and roles at higher levels e g decision makers managers policy makers governance and institutional players scientists organizations etc their role is critical as enablers or bridges connecting small world types of networks that they otherwise would not be connected without their brokerage by assuming this role they open knowledge transference pathways capable of producing a wide array of interesting and fascinating systemic phenomena such phenomena include the emergence of power law type of relationships in knowledge networks generative social emergence across multiple social and ecological scales and perhaps importantly keeping open bi directional pathways of communication and socially relevant situational understanding of reality the latter represents a necessary condition in negotiating appropriate and efficient institutional arrangements required in managing and governing the commons 1 1 3 knowledge diffusers they represent a group of social actors capable of transforming knowledge structures into tangible real and beneficial social ecological outcomes they are the ones that complete the cycle from information to knowledge to a particular type of actions such actions represent knowledge relevant or knowledge intensive decision and policy making the context content and extent of their knowledge diffusion establishes the conditions and situations necessary for the emergence of adaptive proactive and reactive action while action itself does not necessarily implies or requires knowledge transference the type of action that knowledge diffusers enable is such that ensures that the options and choices available have a direct and undisputable mapping into real and clear outcomes with societal significance this clear mapping between choices and distinct outcomes necessitates knowledge as critical and fundamental element of each strategic mix of choices and outcomes 1 2 collective social processes and social ecological knowledge organization a significant part of the scientific literature deals with semantic processes as an integral part of semantic cognition semantic ability e g activation association inter connectiveness or retrieval is found to be seriously impaired in the presence of various social illnesses that affect cognitive ability for example semantic ability impairment is found to be associated with mental patients suffering from schizophrenia chiu et al 2003 condray et al 2003 paulsen et al 1996 and other psychotic conditions fritzsche 2003 alzheimer s disease kazui et al 2003 rogers and friedman 2008 dementia graham et al 2000 robertson and köhler 2007 amnesia and epilepsy giovagnoli et al 2009 various brain or auditory damages breese and hillis 2004 or other social psychological conditions such as selective retrieval of unwanted memories levy and anderson 2008 these and other relevant findings indicate how central is our ability to perform cognitive semantic tasks in order to function in a human social environment at the collective social level of interactions the issues go beyond social or cognitive pathologies and indicate strengths or deficiencies of the social system as a whole if our ability to organize and semantically categorize our knowledge is a proxy of our overall social intelligence enfield 2009 then as societies we benefit and perhaps advance our levels of collective social achievements by taking advantage and exploring this social intelligence in different or higher orders of knowledge organization our ability as organized communities and societies to achieve higher levels of knowledge organization is therefore a characteristic integral perhaps of our social learning capability in an era where technology and complexity of social interactions advance and develop in parallel our abilities as social groups to process reorganize utilize and functionalize information in semantically rich ways are paramount new knowledge is generated and required not simply in the form of unprocessed information availability but as deliberate and functional response to the needs of our social complexity of interactions the semantic web greaves and mika 2008 for example does not itself generates new fundamental knowledge but rather alters its availability and accessibility to wider social audiences reorganizes it in new and highly functional ways or semantic categorizations and enhances our ability to perform more and more complex cognitive and socially critical tasks at both the individual and the collective social levels in a sense semantic social knowledge representation encodes and categorizes a world of semantically important objects that belong to the past present and future alike the past through the historical collective evolution of norms aspirations and beliefs the present through values and attitudes the future through expectations and visions that individuals and local societies hold and pursue collective semantic social knowledge representation also encodes both physical and psychological space alike physical space through socially embedded geographies historical and cultural spaces and landscapes geographical narratives and named places social space through social distances distinctive social roles and actor settings psychological scales and magnitudes collective dispositions and mapping of shared realities experiences and norms this blending of space and time into a unique social continuum is what semantic knowledge representation classifies and encapsulates within its ontologies collective social processes and collective decision making depends on the level of knowledge and its representation in the system of interactions among and across objects of collective inference in other words how the system behaves is a matter of complexity of both its parts but also on its interactions systemic interactions i e part part relationships and part whole whole part interactions are contextualized in terms of informational or entropic context that is primarily knowledge based or at least knowledge contingent to the extent that collective networks of interactions e g semantic encode and encapsulate such interactivity across social actors in the society and community as evidence suggests in support of this proposition then collective knowledge representation has the propensity of capturing important social dynamics such social dynamics include social emergence centrality and collective social influence over behavior decisions and actions if we agree that many social dimensions of change require an inductive approach and cannot be reconstructed at least empirically using deductive approaches or social mental models then we can see that the level of support in often uncertain and incomplete inferences regarding social change can only be achieved by introducing heuristics stemming from evidence at the collective social level of interactions this is especially true when non monotonic reasoning is both present and necessary to interpret the evidence at hand williams 2009 argues that there cannot be natural necessity in the social world but only social necessity in probabilistic terms of the word natural necessity requires the presence of consistency and regularity williams 2009 which is often and commonly encountered in the physical world e g ecosystem function and processes but not in the social world of interactions non monotonic phenomena in human judgment e g induction abduction belief revision knowledge based reasoning clearly showcase how variation in levels of certainty uncertainty of evidence or knowledge or information leads to nonlinear and often profound shifts in inferences or conclusions reached brewka et al 2004 pinkas 1995 rott 2001 voorbraak 2004 furthermore collective reasoning propositions are probabilistic or probabilogic and variational by default chater et al 2006 haenni 2005 in other words they vary across individuals within populations and across populations within space and time 1 3 the dual character of collective social representation there are two key characteristics and functions of collective social representation relevant to this discussion the first involve collective dynamics as inferential mechanisms that explain and describe social patterns at the collective level such mechanisms of inference are necessarily heuristic by nature and serve the purpose of allowing interpretation of social actions in the context of whole communities and societies the second considers collective dynamics as means of reasoning about societal wide processes decisions behaviors and actions such processes do not necessarily fall under the realm of inferential or heuristic assessments but rather serve as indicators for explaining the functionality of collective social structures as such i e facilitating the emergence of individual processes by elevating their semiotic or ontological importance and operation at higher levels of collective social hierarchies in the first category we can ask for example how individuals form and facilitate collective processes such as semantic emergence social roles and social actors centrality prestige brokerage etc in the second category we need to ask how existing collective structures regardless of their constituent formation process allow for individuals to reason about changes including social change oliver et al 2012 1 4 semantic inference and informational propositions semantic similarity represents a correlated function of space and time continuum within a certain state of social structure semantically similar representations of social reality imply a certain degree of shared understanding of such reality or otherwise stated a shared level of knowledge representation within a shared or collective mental mode two important parameters and their combined functioning are implied in such propositional formulation first let us examine the case where a number of individuals sharing a certain level of semantic similarity in their knowledge representation as this emerges through stated or revealed empirical inference such semantic similarities are assessed through a socially explicit or implicit contextual environment or social space the latter represents an assumptional proposition and relates to the nature of the empirical evidence at hand or the empirical nature of the evidence sampled through our experimental social science methods such individuals within such a socially relevant context are more likely than not to occupy a level of shared functioning within their respective social space or environment from the individual standpoint this is to say that there is an expectation of a certain level of overlap within the social universe of their interactions such as sharing group memberships having certain similarities on their social and demographic background or sharing one or more social economic or cultural characteristics this social topology albeit not directly assessed in terms of shared spaces is reflected through their shared semantic knowledge representation implicitly and axiomatically from the collective standpoint one can see the social system of individual interactions as a form of topological density distribution whose boundaries are multidimensional and whose dimensions in turn represent social characteristics i e the fabric of social structure under study regardless the approach taken from the individual to the social or from the social to the individual the level of semantic information encoded in the empirical structure of evidence represents at least in part a judgmental heuristic on the collective social space occupied by the semantic distribution and their associations this assumptional proposition of course is rendered more valid as another testable assumption of semantic specificity becomes stronger or supported by the empirical data consequently the more general the semantic knowledge representation is the less warranted may be the assumption of its correspondence with shared overlapping social space and therefore the less likely it may be that semantic similarity relates to collective social interactions in a latter part of this paper we will explore further the theoretical and empirical consequences of such propositional inference secondly let us consider the theoretical statistical inference coming from information theory as early as the late 1950 s it is shown by jaynes that within a statistical subjective interpretation of informational entropy the probability of a certain state can acquire a temporal interpretation i e formulated in the basis of a fraction of the time that the system spends in state n jaynes 1957 p 627 within a socially relevant analytical framework such a subjective probability can be viewed as the level of exposure conditional to the characteristics of the social structure under study the matter of temporal conditional exposure inferentially relates to the emergence of semantic similarities within a collective social knowledge framework as in the previous case this level of temporal exposure can be viewed both from an individual and from a collective social perspective from the viewpoint of the individual the lengthier the exposure i e the fraction of time spend at state n in relation to the total time under study the more likely it is to either diffuse knowledge or acquire knowledge or both at all cases such knowledge to be assumed to be a probabilistic distribution function influenced by exposure levels from the viewpoint of the social perspective the density of social interactions may relate to the density of time exposed to social ideas or social conditional structure in both viewpoints exposure can be subsumed to be associated with shared semantic structure of social interactions on the other hand the level of association between exposure and semantic cohesion can be negatively influenced by the intensity of the social interactions themselves such intensity like the spatial considerations deepens as specificity increases and weakens with increasing levels of semantic generality at the extremes of the spatio temporal arguments the informational entropy of the semantic inference is rather trivial for as we all exposed to a certain shared level and representation of the world around us i e a level of generality eliciting global or universal social value or ideological worldview systems there is a trivial level of generality that renders scientific inference as uninteresting similarly in the heart of the structure of our social interactions all individuals within a society face the eventuality of having a minimum level of shared understanding of reality their social environment and a level of shared exposure at abstract social states e g national or international ideological general dispositions regarding fundamental aspects of humanity etc nevertheless none of the above arguments would mean much if it was not for an important range of statistical discoveries in the areas of computational linguistics and the statistical nature of human language from its semantic knowledge representation point of view the integration of linguistic sociological mathematical and statistical sciences cannot be better served as in the case of the literature observing the theoretical and empirical nature of the semantic structure of human language generalized semantic language distributions are shown to follow a mathematical distribution widely known as zipf s law piantadosi et al 2011 1 5 structure of the study and research questions this study attempts to address three important research questions first what is the structure and characteristics of environmental knowledge across individual stakeholders communities of practice and institutional arrangements secondly how well semantic network representations of environmental knowledge represent collective social processes and interactions thirdly how and to what degree the structure and self organization of semantic networks influence the nature and characteristics of collective knowledge itself to answer these key research questions we employed a multi stage multi dimensional analysis framework shown in fig 2 and explained in detail in the methodology section the adopted analysis methodology follows the state of the art literature in terms of semantic network analysis dib et al 2018 fronzetti colladon 2018 l li et al 2017 reagan et al 2017 takase et al 2016 especially in relation to the body of literature related to environmental knowledge redington and chater 1997 wesche and armitage 2010 and complex social ecological systems alexandridis and maru 2012 tàbara and chabay 2013 modeling social ecological knowledge through semantic network inference has provided valuable insights into the complexity of interactions that constitutes our complex coupled human natural systems of reality it has similarly enhanced our awareness and understanding of the social institutional and systemic forces that influence and being influenced by environmental change while driving social and economic response mechanisms and strategies including those related to adaptation to climate change 2 materials and methods the methods used in this study include methods for a data collection methods including generating a natural text semantic corpus dataset with participant and group attributes from qualitative focus group and workshop exercises b semantic categorization and classification methods including performing latent semantic indexing and linguistic categorization along with clustering and classification of semantically categorized entities and c semantic network analysis methods including graph theoretic network structural analysis of the data 2 1 data collection methods the data used in the analysis based on five scenario based focus groups in the island of st thomas us virgin islands and one large multi stakeholder international workshop in sarasota florida the us virgin islands focus groups were conducted during the period of september 2012 through january 2013 while the florida workshop was conducted in october 2015 key characteristics for each of the two case studies are shown in the following table 1 the key demographic characteristics of participants in the us virgin islands are also described further in webb 2013 and to an extent mirror basic demographic characteristics of the us virgin islands at large the participants in the florida dataset was based on an international workshop on integrated local environmental knowledge and included participants from the scientific community in the mote marine laboratory local and state agencies local organizations and ngos along with community volunteers a total of 13 focus group workshop exercises are included in the analysis five in vi and 8 in fl incorporating narrative perspectives of a total of 57 participants 31 in vi and 26 in fl in both case studies the focus groups and workshop group exercises followed specific thematic protocols in order to ensure consistency of discourses there were four thematic sections in each of the focus group exercises in the us virgin islands and two thematic sessions in each of the workshop exercises in florida specifically the thematic entities for the us virgin islands were i discussing drivers of environmental change drivers ii defining and discussing environmental sustainability sustainability iii developing scenarios for the future scenarios and iv discussing social ecological resilience resilience similarly in the florida case study the thematic entities discussed by the workshop participants were discussing environmental restoration efforts restoration and discussing environmental stewardship approaches stewardship the discourse narratives in both case studies were audio recorded for each group exercise and were verbatim transcribed following transcription the raw text files underwent a pre processing sequence 1 identify case studies each case study was assigned a unique study id sid 2 separate discourse narrative cases in each transcription a discourse narrative case is a unique set of one or more paragraphs for which a single participant talks during the discourse exchange each of these discourse narrative cases represents a unique row on the database and is identified through a unique variable named discourse id did 3 identify discussion theme for each transcription we identified and separate each thematic group of the discourse the theme of the discourse these were provided with a unique theme id tid across case studies and across focus group or session replications of the experiments 4 identify participants for each discourse id we identify the person participant who provided it and was added in each row using a participant id pid variable 5 identify focus group session for each of the case studies we accumulated transcription data for each of the focus groups session replications of the experiments each group session was given a unique group id gid that identifies the session group replication 6 identify participant attributes each participant is associated with a number of attribute data including gender age occupation group in which they participated these were added as column variables for each row associating attributes with participant data 2 2 analysis methods 2 2 1 data pre processing the transcribed text data were slightly modified in order to a remove all transcribed instances by the facilitator in the study during this step only the discourse statements made by the participants themselves remained in the study b correct grammar or spelling errors and convert spoken linguistic forms to formal ones e g can t to cannot i ll to i will etc and c converted to a tabular format containing one discourse statement per row along with key participant group and theme characteristics as additional columns the modified data generated a text corpus database that was used in the semantic analysis of the study the final textual corpus database constructed includes 2618 discourse narrative cases 1001 38 2 for the virgin islands case study and 1617 61 8 for the florida case study on average it contains 45 9 narrative cases per participant or 3 5 of total narrative cases per participant while the absolute number of cases per participant differs between virgin islands 32 3 narrative cases participant and florida 62 2 cases participant their overall fractions as percent of total cases is similar 3 2 for vi and 3 8 for fl the final dataset also contains on average 45 9 narrative cases per focus group session or 3 5 of total cases per focus group in terms of its linguistic content the textual corpus characteristics and summary statistics are shown in table 2 the main units of analysis for the main analytical methods used in this study are cases paragraphs sentences and words thus the size of the textual corpus dataset has adequate statistical power for the latent semantic indexing and parameter estimation methods used for more information on the final dataset see s3 file 2 2 2 semantic extraction extracting semantically relevant linguistic concepts from the textual corpus involves a number of process steps first the textual corpus is checked against an exclusionary list identifying and removing temporarily from the text corpus non linguistic words i e only linguistically and semantically relevant concepts are kept in the text the exclusionary list is based on formal english language dictionary the remaining corpus include verbs connectors modalities adjectives pronouns etc secondly the concepts remaining in the linguistically relevant text corpus are tokenized tokenization includes concept substitution through an english lemmatization algorithm i e further reducing any infected spelling of words to its lexical root reducing words to their canonical forms including plural singular forms past tense present tense etc lemma form each of the lemma form represents a linguistic token that is used for the categorization process in the next session the semantic processing for the textual corpus data in this study reduced the original 188 912 words to 106 144 tokenized words thus reducing the size of the corpus by 43 8 the percent reduction was almost identical across the two case studies 43 7 for vi and 43 9 for fl the process corpus includes 3825 word forms 2427 for vi and 2639 for fl the ratio between type forms and token words in 0 036 0 046 for vi and 0 05 for fl further summary semantic processing statistics are shown in table 3 2 2 3 concept categorization and association semantic concept categorization from textual corpus data depends in its original stages on identifying co occurrence patterns among linguistically relevant words within cases paragraphs or sentences of the corpus therefore worlds with higher co occurrence frequencies not only appear together in the textual corpus but also do so in high frequencies b li wang and zhang 2012 given the fact that our textual corpus dataset includes multiple participant and focus groups higher co occurrence frequencies also reflect a higher level of collective mental models of knowledge representation alexandridis and maru 2012 in other words they reflect a more social rather than cognitive or individual perception of reality on the other hand semantic similarity in patterns of co occurrence allows us to generate measures of paired association among concepts words or categories groups of words with relatively high frequencies of co occurrence within a textual corpus such associations in the forms of semantic networks are shown to be hierarchical by nature crestani 1997 ravasz and barabasi 2003 thus the scalability of such networks allows us to retrieve semantic networks from data ranging from almost full graphs to abstract graphs the research question that emerges in such cases is what is the similarity threshold that maximizes the informational content i e minimizes the informational entropy of the knowledge contained within them this question relates closely to the isomorphism property of semantic networks alexandridis and maru 2012 chia and ong 2006 and the scale free distribution of semantic association in graph theoretic terms therefore given a scale free semantic network the previous research question becomes a matter of finding the minimum cut off threshold level of semantic similarity for which the scale free properties of the network and thus its semantic isomorphism remains unchanged at such level adding more nodes to a network who s link distribution follows a power law will not change its network structural characteristics and the network becomes simply saturated in order to test the latest proposition we adopted a study design that a fixes the number of nodes in the semantically extracted networks to the top 100 concepts based on their analytic tf idf index and b varies the threshold of associative semantic link similarity of co occurrence patterns by using its percentile distribution in terms of the fixed number of nodes this step is necessary in order to perform comparative network analysis i e the networks vary only on their structure and not by their size in addition the tf idf coefficient representing the product of the term frequency how frequently a term appears across paragraphs sentences etc and the inverse document frequency discounting terms whose expected within case frequency is larger tf idf is a commonly used information retrieval metric used in latent semantic indexing ai et al 2010 rivera et al 2014 xagi et al 2010 symbolically 1 t f i d f t f t d i d f t d f t d max t d f t d log d 1 d d t d where t is a given term word or category d is a given document case in the corpus f t d is the frequency of term t in case d t is any other term appearing in case d d is the number of cases in the corpus the denominator of the inverse document frequency is adjusted adding 1 to avoid division with zero and computes the number of cases where the term t appears in terms of the number of links in the study following the concept categorization and association process in the next session we first computed the percentiles of the semantic similarity appearing in all pairs of the top 100 nodes selected by their largest tf idf and then generated three network versions using the 5 keeping 95 of the pairs the 50 keeping 50 of the pairs those above their mean semantic similarity coefficient and the 95 keeping the top 5 of pairs based on their similarity coefficient in the analysis section we analyzed all three networks for each of the three cases a network containing both case studies in a unified corpus a network containing only the virgin islands text corpus cases and a network containing only the florida text corpus cases the primary concept categorization was created by the data using a simple categorization algorithm after semantic extraction process described in the previous session the algorithm involves the following steps 1 adds word categories with minimum frequency of occurrence in the corpus f min 10 only words that appear in at least 10 cases are kept 2 calculate tf idf values for the kept categories and sort by decreasing tf idf values 3 keep the top 100 concepts with the highest tf idf values and drop the remaining the categorization algorithm was run for three corpus configurations once for all case study data combined vi and fl and once for each of the case studies separately each of the configurations generated a list of 100 categories in addition to the primary categorization algorithm we also implemented alternative categorization algorithms using known semantic categorization dictionaries to compute categorization statistics for these alternative classifications the algorithm computes frequencies of occurrence in the textual corpus for each of the dictionary defined categories and their relative tf idf values the dictionaries used for the alternative categorization are presented in the next paragraphs the wordstat sentiment dictionary wordstat 2016 combines positive and negative semantic words from the harvard iv 4 dictionary stone 1997 the martindale s regressive imagery dictionary martindale 1981 1986 and the linguistic and word count dictionary tausczik and pennebaker 2009 the martindale s regressive imagery dictionary martindale 1981 1986 contains 3000 words classified across two fundamental constructs primary or primordial processes and secondary or conceptual thinking processes according to the relevant literature martindale 1999 smith et al 1995 textual corpus with higher fraction of secondary processes are representative of more structured and abstract intellectual capabilities grounded in social realities the laver and garry policy position dictionary laver and garry 2000 contains 415 policy related words and word patterns over 19 level 2 grouped categories while originally developed to analyze political positions from policy actors it does have analytical value for this study albeit not without limitations it can provide an indicative and comparative measure to the extent to which policy related issues form a part of the discourse data narratives it thus enable us to contextually evaluate the study s narratives with respect to the policy dimensions of knowledge construction 2 2 4 semantic network construction the semantically extracted and categorized concepts from the two previous stages were coded back to the original textual corpus by sentence paragraph and case in order to generate associative semantic knowledge network one additional methodological stage of the analysis was performed we computed a co occurrence analysis based on similarity of concept patterns of co occurrence per paragraph in the textual corpus we used the jaccard s coefficient of occurrence to calculate the similarity matrix of pattern association jaccard s coefficient of similarity is a well known statistical measure of association leydesdorff 2008 real and vargas 1996 symbolically for rows x i 1 n and columns y j 1 n in the n n concept co occurrence matrix the jaccard s index of similarity for any two cell elements of the matrix c ij x i y j is 2 j c i j x i y j x i y j x i y j where the nominator denotes paragraphs where both concepts occur together and the denominator denotes the sum of paragraphs in which concepts co occur paragraphs in which the first concept occurs but not the second and paragraphs where the second concept occurs but not the first the jaccard s similarity index range is 0 j 1 for each of the case study semantic similarity matrices a number of network attributes were also included from the original attributional dataset by accounting for attributes demographic group theme including case frequencies in which each concept was identified e g number of cases with female versus number of cases with male participant for each of the network nodes the use of similarity index is also used in network volatility and event link analysis hu et al 2017 to address the predictive ability and evolution of network structures since we are interested in directional semantic knowledge analysis we compared the difference in tf idf of the concepts in each complementary pairing of relationships i e comparing the tf idf difference between the x i y i and y i x i our methodological convention is to consider knowledge semantic influence moving from nodes with smaller influence to ones with larger influence i e more central nodes attract less central ones this methodological assumption is also supported by the preferential attachment property present in scale free network distributions alexandridis and maru 2012 newman 2001 thus the directional network conversion algorithm was 3 s i j s i j i f t f i d f i t f i d f j 0 i f t f i d f i t f i d f j where i j are row and column nodes respectively from to the zero condition applies in cases with identical tf idf values denoting the diagonal network cases i e where i j same node 2 2 5 semantic network structural analysis we computed a number of graph theoretic network metrics in the semantic knowledge networks of the study in general these belong to four group types a neighbor metrics measuring structural link characteristics of the semantic networks based on their associative connectivity patters drieger 2013 knoke and yang 2008 scott 2000 b centrality metrics measuring structural cohesion and centrality of network nodes overall jackson 2008 xu et al 2010 zhang 2010 c equivalence metrics measuring the efficiency and effect of connectivity patterns in the network reichardt and white 2007 sailer 1978 and d positional metrics clarifying the structural role that various node play in the semantic network alexandridis and maru 2012 bader et al 2008 haybron 2000 in addition we considered and analyzed statistical approaches and models of network structure such as the dyadic interaction block model p1 brendel and krawczyk 2010 white et al 1976 the non negative matrix factorization nnmf model hasan and zaki 2011 the local outlier matrix factorization lomf model singular value decomposition svd sivakumar et al 2011 watkins 2004 and hierarchical clustering models ravasz and barabasi 2003 based on both continuous and categorical network association metrics fig 3 below provides a graphical representation of the graph theoretic network analysis metrics computed and used in this study left subgraph along with additional statistical network models right subgraph the metric variables denoted in red circles was first computed and analyzed from the semantic knowledge networks in the study the composite coefficients green stars were calculated and used in evaluating the effect of semantic network structure on the formation and functioning of knowledge systems section 3 2 3 the aggregate variables blue diamonds are only shown in the graphs as a level of typological grouping categorization of factors rather than computed variables we further evaluated the presence of scale free distributions in our semantic knowledge networks we estimated the hypothesis that the connectivity jaccard similarity and the document corpus importance tf idf in our data follow a power law cumulative distribution symbolically we estimated the α parameter of the power law model where 4 p x c x α using the cumulative probability p x x min the evaluation uses a maximum likelihood method to estimate the alpha parameter exponent of the model we also used the kolmogorov smirnov goodness of fit statistic to test the null hypothesis that the cumulative probability distribution follows an exponential distribution with exponent α this methodology for evaluating scale free distributions in empirical data is described in more details the relevant literature barabási and albert 1999 clauset et al 2009 3 results 3 1 semantic categorization there is an overall 43 7 overlap across semantic categories in the data between the two case studies virgin islands and florida as can be seen in table 4 the mean tf idf and narrative case co occurrence frequency is 70 8 and 57 4 higher in overlapped categories the summary of the semantic grammar of the classified knowledge narratives is shown in fig 4 for all case studies in terms of the verbs used almost 42 reflect stative statements i e statements expressing a specific state or situation another 31 are factive verbs i e used in statements asserting facts or presupposition of truth statements the third important category reflexive verbs 26 are mainly used in statements describing personal or collective actions or processes involving oneself or social groups in terms of connectors addition 43 5 of classified narratives reflects the additive nature of environmental knowledge both as a process and as an equilibrium state in terms of semantic modalities the collective narrative discourse in both case studies reflects a mixture of modal states including manner space time intensity negation assertion and doubt the use of adjectives in the collective knowledge narratives in both case studies reflect primarily an objective semantic state 63 reaffirming the central role of local environmental knowledge as a broader mental representation of the social construction of reality with a degree of subjectivity 26 this fact is further reflected at the analysis of the pronouns used in the participant s classified narratives a total of 46 8 of the narratives reflect individual pronouns i you he she covering both objective and subjective adjective statements while another 33 9 of the narratives reflect purely collective statements we they somebody when participants discuss their knowledge with relation to social realities 3 1 1 alternative categorization schemes in order to understand in better ways the context and content of the narrative environmental knowledge captured in our data we applied a number of alternative classification schemes in the corpus dataset specifically we classified the narratives using a the wordstat sentiment dictionary loughran and mcdonald 2011 wordstat 2016 b the martindale s regressive imagery dictionary martindale 1975 1981 and c the laver and gary policy dictionary laver and garry 2000 the graphs in fig 5 below summarize the alternative classification grouped categories for the combined textual corpus data and for each of the separate case study s corpus subsets the top row of graphs showcases the application of the wordstat sentiments dictionary classification the bottom row of the figure summarizes the results of applying the regressive imagery dictionary classification in all cases we used the highest level hierarchical categorization level 1 in both classification schemes the darker color bars represent the percent of sentences in the corpus classified within each category while the lighter color bars show the percent of paragraphs cases classified for the same category in terms of the positive negative distinctions captured in the wordstat classification in all replications of the study the positive discourse narrative aspects represent at least 70 of the classified cases the respective negative coverage of the corpus ranges between 50 and 60 in terms of the emotional primary secondary continuum captured in the rid classification consistently the of cases classified as secondary ranges from 62 to 75 with primary categories ranging from 46 to 60 and the emotional ones ranging from 17 to 34 similarly fig 6 summarizes the laver and garry policy positions dictionary categories level 1 for the combined corpus data and for each of the case studies separately the classification results show consistently economic aspects of policy related discourse occupying a significant percentage of the textual corpus coverage between 24 and 30 followed by cultural policy dimensions coverage ranging between 12 5 and 24 5 3 1 2 categorization by attribute characteristics we analyzed the original semantic categories with respect to the participant group and theme attribute characteristics in terms of the key participant demographic characteristics table 5 shows the case frequency of the top up to five semantic categories in both case studies by gender and age group in terms of the gender attribute the top five categories statistically significant favoring male over female participants p 0 001 are more or less representative of objective outcome realities and situational characteristics of social ecological realities they focus on local products agriculture or market in general in the virgin island case and on restoration water and bay related issues along with time related issues in the florida case on the other hand the top categories that favor female over male participants have generally less statistical power than the ones for the male participants they focus more on institutional arrangements processes and generally more subjective perceptions of reality in fact in the virgin islands case female participants talk more about community and enforcement issues general environmental issues island wide in the florida case study female participants are more differentiated by discussing issues related to feelings meetings and other similar subjective aspects of reality in terms of their age group characteristics the top categories favoring higher frequencies for young adults than any other participants are ones referring to job opportunities start general impacts school and action in the virgin islands case study given that most of the young adult participants in the vi case are school teachers and young volunteers it is clear that these discourse topics reflect a more participatory experiential and bottom up reality driven narrative on the other hand in the florida case study the same categories e g school student education differentiate more the other age group categories the multi dimensional scaling analysis by attributes of the semantic text corpus data are shown in the following figs 7 and 8 in all cases the first three dimensions have eigenvalues above 1 0 and they explain on average 84 4 of the variability in the data 76 5 for vi 80 5 for fl and 96 1 for the combined case studies the classified semantic categories are plotted using their estimated multi dimensional scaling coordinates in each of the paired dimensions with the third dimension shown as a contour then the mean x y and z coordinates for each of the focus group in each of the case studies are calculated and plotted in the same graph nodes or categories closer to the axe s center i e axes with value of zero are closer to the mean values of each dimension fig 7 plots the semantically classified knowledge categories against the study s focus groups representing relatively cohesive social professional and institutional participant groups similarly fig 8 plots the semantically classified knowledge categories against the broader occupational groups of the participants we generalized specific occupations across both case studies to differentiate between participant roles as scientists educators business professional government agency participants or community ngo groups 3 2 semantic knowledge network analysis 3 2 1 summary of key semantic network metrics fig 9 provides a visual network representation of the neighbor in degree distribution among semantic nodes in the knowledge network for the case study narratives nodes with larger diameter have higher neighbor in degree coefficient which indicates the degree of semantic knowledge in flows from the periphery to the center weighted by the semantic similarity of the links three types of nodes based on their coefficients are identified ordinary nodes receiver type nodes and transmitter type nodes the receiver nodes are generally receiving strong semantic knowledge influence flows while the transmitter nodes are strong sources of semantic influence flows the structural hole network analysis for the classified narrative data provided some very interesting results specifically the degree of redundancy in the network connectivity link connectivity is 27 2 for the network constructed with all studies narratives 22 1 in the virgin islands network and 24 1 in the florida network the opposite results hold true for the network s efficiency which represents 1 redundancy summed over all network nodes the efficiency coefficient for all narratives is 67 9 while the efficiency for the virgin islands network is 70 6 and 69 2 for the florida network another important neighbor network analysis concept is that of homophily neighbor s homophily analysis shows us the degree to which the nodes of a given network are similar in a certain property with it s neighbor s i e the nodes with which the focal node is connected with homophily reflects the property of birds of a feather flock together we analyzed our semantic knowledge network homophily characteristics with respect to their tf idf classification indexes we compared each node s tf idf value ego with the mean tf idf value of it s neighbors alters and computed the mean differences across all nodes in each knowledge network we finally compared these differences within the size of the network using the 5 expansive 50 medium and 95 smaller percentiles of network sizes our results indicate a decreasing pattern of differences in tf idf values between ego and altars specifically the difference for the expansive network is 30 0 from 89 84 to 62 93 for the medium size network drops to 27 4 from 89 84 to 65 25 and drops further to 18 9 from 93 97 to 76 25 for the smaller network we computed statistics for the shortest path between any two nodes in our semantic networks the shortest path algorithms identify the geodesic the path with the shortest possible distance from all possible alternative path connectivity between two nodes in a network we computed summary statistics of the shortest path distance measured in standardized euclidean distance from 0 to 1 with 1 network diameter we used the 95 percentile networks which reflect the top 5 of the connectivity relations in the network the mean geodesic distance for the combined case study network is 0 115 0 109 for the virgin islands case study and 0 131 for the florida case study the percent of reachable nodes outwards from the center to the periphery is 30 7 of the nodes vi 28 7 fl 27 0 on the contrary the percent of reachable nodes inwards from the periphery to the center is 12 8 of the nodes vi 13 2 fl 15 3 our results also show that the percentages of out reachable nodes and in reachable nodes tend to become more equalized as the network size increases because of the scale free nature of link relationships the results of implementing a path finder network algorithm pfnet based on euclidean distance dissimilarity are shown in fig 10 the pfnet dissimilarity process implements a minimum spanning tree network algorithm nepomniaschaya 2006 schvaneveldt and norwood 1990 i e finds the optimal tree structure in a network that minimizes the sum of tf idf similarity coefficients in the semantic network the resulting pruned networks represent the minimal path structure of the semantic knowledge that keeps the network connected in the two case studies the pfnet dissimilarity algorithm yields a semantic knowledge tree structure that includes 102 out of 3740 links in vi and 102 out of 3875 links in fl 2 73 and 2 63 of the full semantic networks respectively using an average 7 6 of the total tf idf similarity weights the status katz centrality coefficients for the semantic networks of the two studies show that in status centrality from the periphery to the center of the network is decreasing by almost 50 in comparison with the out status centrality in both case studies these results are clearly quantifying the relative importance and influential role that central knowledge structures play in the network four additional network structural analysis metrics yielded statistically significant differences between the two case studies the neighbor level ego density larger in vi the information centrality larger in vi the local reachability density factor slightly larger in vi and the k distance factor larger in fl since the distributional assumption of normality is violated in all four variables the kolmogorov smirnov test for normality was statistically significant we used non parametric testing for the difference both the kruskal wallis rank test for equality of populations kw and the wilcoxon mann whitney rank test mw had statistically significant chi square kw and z mw values for the ego density χ 2 98 12 z 9 91 p 0 0001 for the information centrality χ 2 24 94 z 4 99 p 0 0001 for the local reachability density χ 2 27 07 z 5 20 p 0 0001 for the k density χ 2 22 80 z 4 78 p 0 0001 the ego density coefficient shows that associated neighbor groups of semantic knowledge concepts tend to be denser in the vi and more loosely connected in the fl case study the information centrality coefficient shows that transmission of knowledge as information flows is stronger in the vi in comparison with the fl case study the local outlier factor matrix model coefficients local reachability density and k distance show that knowledge is slightly more reachable within k nearest neighbors in the vi case study while the average distance of any knowledge node to its k nearest neighbors is higher in the fl case study local reachability lrd k and k distance d k are closely related since 5 l r d k x y max d k y d x y for any two nodes x and y in the network campos et al 2016 jackson 2008 the box plots comparing the two case studies for the four identified coefficients are shown in fig 11 3 2 2 evaluating power law distributions in semantic networks we assessed the presence of scale free distributions both at the semantic knowledge network level as well as the node attribute distribution level as described in the methodology section we used the kolmogorov smirnov statistic to evaluate the presence of power law relations clauset et al 2009 table 6 shows the results of the statistical evaluation the α coefficient represents the estimated exponent of the power law relationship the x min coefficient denotes the probability estimation of the model i e p x x min the k s coefficients provide the values of the kolgomorov smirnov statistic text the p values of the statistical evaluation with the exception of the 50 and 95 percentiles reduced networks in the combined case study networks are not statistically significant not allow us to reject the null hypothesis that the network jaccard similarity in degree coefficient follows a power law scale free distribution in other words the semantic knowledge influence increases exponentially as more we move from peripheral towards collectively shared knowledge concepts visually the presence of scale free distributions in our case study knowledge networks is also shown in the graphs of the following fig 12 the y axis of the graphs shows the cumulative probability distribution function cdf of the in degree jaccard similarity in each of the case studies the x axis shows the raking of the 100 semantic network nodes categories rank 1 is assigned to the node with the highest in degree and rank 100 to the node with the lowest one the axes are plotted on a log scale the graphs show how few categories marked as dots have the highest in degree similarity while the majority of the nodes have a moderate or low in degree similarity 3 2 3 evaluating the effect of semantic network structure on the formation of knowledge systems the computed raw semantic network graph theoretic metrics are divided into four major types neighbor metrics measuring neighbor link based characteristics centrality metrics measuring node and network based characteristics equivalence metrics measuring structural network equivalence characteristics and position metrics measuring role based network characteristics these graph theoretic types of analytical groups represent some unobserved latent variables related to each of their described characteristics we constructed latent composite scale factor models of these types using a reliability analysis specifically we evaluated the best subset of the metrics that maximize their cronbach s alpha reliability coefficient cronbach s alpha estimation allow us to assess item graph metrics reliability towards a latent scale we perform each alpha estimation for each of the case studies as well to the combined corpus narrative data consequently four latent scale coefficients were computed for our network metrics data neighbor scale centrality scale equivalence scale and position scale coefficients the alpha values ranged from 0 943 to 0 958 for the neighbor scale 21 items metrics 0 960 to 0 963 for the centrality scale 26 items metrics 0 751 to 0 922 for the equivalence scale 10 items metrics and nearing 1 0 for the position scale 2 items metrics the average inter item correlation ranged between 0 31 and 0 54 across case studies and scales the full reliability estimate statistics for the four composite network scales can be found in appendix 2 we fitted a nonlinear exponential growth model between the tf idf values measured from the latent semantic analysis of the narratives and the four composite semantic knowledge network scale indexes for each one of the main network systemic characteristics neighbor characteristics network centrality network equivalence and network position the symbolic form of the estimation is 6 y c e β 0 β 1 x 1 β i x i or 7 t f i d f e β 0 β 1 x n e i g h b o r β 2 x c e n t r a l i t y β 3 x e q u i v a l e n c e β 4 x p o s i t i o n ε the results of the exponential growth nonlinear estimation are shown in the following table 7 the top part of the table provides the model estimates and the bottom part the parameter estimation of the coefficients all case studies had excellent adjusted fit r 2 was excellent 0 9946 for the combined case studies semantic knowledge network 0 9879 for the virgin islands semantic knowledge network and 0 9956 for the florida semantic knowledge network the mean exponent i e the predicted values for the linear exponent component 8 β 0 β 1 x n e i g h b o r β 2 x c e n t r a l i t y β 1 x e q u i v a l e n c e β 4 x p o s i t i o n is around 4 4 1 for vi and 4 2 for fl with standard deviation of 0 4 the exponent s values range from 3 6 vi to 5 8 fl as can be seen from the parameter estimates in table 7 the effects of the neighbor coefficient increase tf idf by 26 5 in the virgin islands versus only 16 3 in the florida case study the results are opposite for the centrality coefficient its effect is smaller in the vi 29 7 than in the fl case study 35 5 the equivalence coefficient is marginally negative in the vi decreasing tf idf by 3 6 and marginally positive in fl increasing tf idf by 7 1 finally the position coefficient is marginally negative decreasing tf idf in both case studies by 6 and 6 5 in vi and fl respectively the predicted values of the exponential growth model estimation are shown in the following fig 13 in all case studies the structure of the semantic network characteristics has exponentially increasing combined effects on the semantic importance of the narratives 3 3 network clustering classification the following fig 14 below presents a visualization of the major cluster group of contents classified through a community graph plot algorithm cit the algorithm enabled the identification of seven broader cluster groups in the virgin islands case study and eight cluster groups in the florida case study the algorithm maximizes cross group semantic distances and minimizes within group semantic distances based on estimated cluster membership profile heuristically as can be seen from the graphs the two case studies have a strong degree of overlap in four key areas a knowledge related to economic considerations socio economics and business in both case studies b educational knowledge education community outreach in vi vs science education in fl c knowledge related to agriculture and fishing livelihoods agriculture and culture in vi vs fishing and fisheries in fl and d local ecological knowledge related to sustainability and stewardship central components in both graphs on the other hand in the virgin islands the community participants focus more on knowledge narratives related to policy and action as well as environmental planning and management themes that are not emerging in the florida case study florida participants focus on ecosystem and ecological aspects of marine and environmental management including water quality along with emphasizing the relationship between science and society both of these generative themes are not emerging in the virgin islands case study in addition to the network level clustering of semantic knowledge we also applied an alternative clustering classification based on natural language processing specifically we performed a factor analysis on the 100 extracted concepts using their co occurrence frequency matrix by paragraph in the corpus document in each of the case studies we selected the ten factors with the highest eigenvalues the factor scores were rotated using a varimax method the eigenvalues range from 8 5 to 1 72 in the virgin islands case study and from 8 1 to 1 69 in the florida case study the category with the most extensive coverage of the textual corpus percentage of cases in the virgin islands case study was social ecological impacts 33 1 while the one for the florida case study was science and community 17 4 following the factor analysis a co occurrence jaccard similarity analysis was performed over the factor groupings classes across the two case studies the following fig 15 shows a heatmap graph of the standardized z score jaccard similarity coefficients we can see that the stronger similarity occurs between the science and community in the florida case study and the environmental policy management drivers of environmental change pair of factors in the virgin islands case study strong similarities also occur in general economic impacts in both case studies and between environmental education fl and livelihoods and opportunities vi overall the total overlap between the two factor categories in terms of the semantic textual corpus is 60 1 compared to the 43 7 overlap observed in the original categories see section 3 1 there is an additional 16 4 overlap that is due to the generalization of semantic knowledge categories 4 conclusions and discussion in terms of the overarching goals and research questions of the study the results provide a clear and concise description of the structure and characteristics of the environmental knowledge across participants i e stakeholders communities of practice similarly through the network classification and groupings of semantic networks our model results raise a series of important dimensions related to the institutional and social arrangements that are related to core environmental knowledge the study results clearly provide a data driven semantic network representation of both environmental knowledge structures at the socio linguistic and cognitive level and the collective social level of discourse interactions finally the social network analysis results in terms of centrality density neighborhood effects structural hole equivalence redundancy efficiency homophily shortest path analysis and power law analysis provide evidence of the strong relationship and association between the structure of self organization in semantic inference and the characteristics of collective knowledge as a social ecological system of interactions the categorization overlap analysis indicates that categories that appear in both case studies generally have significantly higher co occurrence and tf idf values than the categories that uniquely appear in each case study thus the central core of the semantic knowledge network is shared between the two studies reflecting the extent to which environmental local knowledge represents a broader global veridicality of social ecological knowledge conversely the remaining uniquely identified semantic categories in each case study approximately 56 of the concepts reflect discourse narratives that are either specific with respect to the locality or the semantic discourse theme or the group participant characteristics the application of the three alternative categorization schemes to the textual narrative corpus data allows us to examine some general contextual aspects of the collective knowledge discourse captured in our data specifically it appears that knowledge involves both positive and negative aspects of social ecological change with the former group being more prominent than the second overall we can see that in general a positive row balance characterizes the participants environmental knowledge reflecting a more optimistic outlook we can also infer from the rid classification results that local environmental knowledge reflects by large a significant proportion of secondary processes such processes involve analytical logical and abstract thinking rather than primary and emotional responses while the latter do have a place in the composition of our collective knowledge structure their role is not as prevalent as the secondary aspects of knowledge processing these results also allow us to clearly understand why knowledge structures represents a more advanced and valuable system than simply processing information the qualitative difference between information and knowledge rests by large on the ability of our communities and social groups to engage in secondary processing of raw cognitive or situational stimuli e g emotional primary in comparing the attribute characteristics of study participants with respect of their semantic categories in the categorization by attributes session we identified how there exists a gender based gap in knowledge compositional characteristics female participants differ in terms of their focus on subjective and procedural aspects of knowledge as constructs of their social realities while male participants differ mostly by their focus on objective and situational outcomes this distinction is generally present in both case studies thus it is unlikely that it has much local cultural significant and more related to the cognitive and mental attributes of male and female psychology in general in that sense may represent an intrinsic characteristic of knowledge rather than one that is exogenously driven when we compare the age based attributes of the participants with respect to their respective semantic categorization frequencies the observed gap is of a different nature it has to do mostly in terms of the qualitative origins of knowledge on the one hand we have knowledge processes that are formed and driven by experience and situational perceptions of reality these experiential and situational forces represent generally bottom up participatory mechanisms on the other hand we have knowledge processes that are informed and driven by deontic processes i e what ought to or what could be rather than what is the latter knowledge processes in contrast represent top down often policy or institutional rule driven mechanisms this is the case when comparing the virgin islands young adult differentiating categories focusing mainly on career opportunities job school etc with the florida case study where similar categories favor the opposite age group more likely than not these results provide support for the proposition that the florida participants have a view of reality from outside experiential or situational settings the high degree of redundancy we found as part of the structural hole network analysis approx 27 allows us to make inferences on the structural stability and robustness of the local environmental knowledge as represented by the narrative knowledge semantic networks the higher the redundancy the more paths exists connecting any two random nodes in the network thus making difficult to disconnect parts of the knowledge network conversely the higher the redundancy of a network the lower it s efficiency the structure of the semantic networks analyzed for this study appears to maintain an approximate relationship of 2 5 1 between efficiency and redundancy i e a balance between marginal network connectivity and full network connectivity this is an additional finding re affirming our network results related to the presence of power law distribution in the network structure combined together they indicate an equilibrium state of the semantic knowledge network and therefore increases our inferential ability about the structure of the social ecological knowledge represented in our data the results obtained from the analysis of homophily for our semantic knowledge networks allows us to support the proposition that local social ecological knowledge structures become more cohesive in value and pattern as we move from the periphery towards the center of the knowledge network in other words the more central knowledge categories are the more they are similar to their connected neighbors and vice versa this proposition also supports the idea of an increasing homogeneity in semantic knowledge character as we move from the periphery to the center knowledge becomes more and more homogenous as it assimilates and spreads within the local community i e becomes more central the opposite is true with regards to knowledge heterogeneity heterogeneous knowledge structures are found more towards the network s periphery this is important especially when new knowledge and ideas enter the knowledge structures they first represent heterogeneous entities as they move up the network isomorphic hierarchy become more central they trade heterogeneity with homogeneity by acquiring values more closer to their connected neighbors the geodesic distance and reachability results of the shortest path network analysis of the data shows us that at least closer to the core of the semantic knowledge the percentage of nodes that are outwards reachable is significantly higher than the percentage of nodes that are inwards reachable in other words starting from the center node we can reach more nodes as opposed from starting from the periphery in terms of semantic knowledge structure these results provide support of the proposition that knowledge diffusion patterns how the central knowledge is diffused in the local community are stronger than knowledge acquisition patterns how new knowledge is acquired and assimilated in the community new knowledge patterns can reach and connect with smaller amount of existing knowledge than existing knowledge affecting new ideas this may be perhaps another quantitative way to show that new and innovative knowledge needs to reach a certain threshold in order to assimilate fully in the case of our analysis connecting with more than 12 of existing knowledge structures the multidimensional scaling for the focus groups reveal that certain focus groups have coordinates significantly deviating from the mean for example in the virgin islands case study groups representing more localized community groups such as the non government nrm group or the farmer s coop appear to be placed in the periphery of the multidimensional graph in the florida case study the second session of both the third and fourth focus groups also appear to have peripheral values at least on the second dimension these results allow us to infer that the discourse narratives captured by the semantic analysis have substantially diverging knowledge content and thus contribute more to the heterogeneity of knowledge in terms of the multidimensional scaling analysis of the occupational role of the participant s groups one important result warrants discussion by comparing the same group position across the two case studies we can see that while in the virgin islands the role of scientists and science organizations is semantically closer to those of community business groups in the florida case study scientists and science organization roles are more closely aligned with government agency views and knowledge discourse these diverging comparative results are perhaps indicative of two distinct models of the role of science or scientists as bidirectional translators of knowledge the first is characteristic of a bottom up approach where scientists and scientific organizations work closely and more tightly with the local community and locally embedded organizations the second provides evidence of a top down approach where scientists serve in support of government policy interventions or regulatory agency approaches to environmental and natural resource management while the efficiency of these two generative approaches is not evaluated as part of this study it is an important distinction that has both theoretical and empirical significance in the relevant scientific literature the results obtained by fitting a nonlinear exponential growth regression of the four network composite scale metric categories neighbor centrality equivalence and position to the semantic importance of node categories tf idf allowed us to assess both the combined and the relative effects of each of the four coefficients in predicting the structure of the semantic knowledge networks overall more than 95 of the tf idf in the semantic corpus can be predicted by the four network coefficients in other words the graph theoretic structure of the semantic networks represents can predict the importance of knowledge categories within a given thematic narrative of these four coefficients neighbor characteristics i e the structural connectivity patterns of knowledge and centrality i e the cohesiveness and structure of knowledge categories are the most important predictors in the virgin islands case study the most important predictor is connectivity neighbor but in the florida case study the most important predictor is cohesion centrality the latest results are also indicative of the two perceptual models of knowledge the bottom up in the vi and the top down in fl while both of the case studies allow for a mix of both centrality and neighbor influences to strongly influence knowledge structures the particulars of each mix allow us to make certain inferences the higher importance of how participants connect and associate their knowledge structure within their knowledge networks provides comparatively stronger evidence of a bottom up knowledge system it is more likely than not that new environmental knowledge generation in the virgin islands flows by new knowledge contributes to the overall connectivity of a network this type of relationship characteristic in the structure of network is particularly present in the case weak ties in small world networks omidi and masoudi nejad 2010 watts 1999 similarly the higher importance on how centrally cohesive are knowledge concepts in a network provides evidence of a top down knowledge system for the florida case study this type of network characteristics are more prevalent in centralized and tightly knit network structures borgatti 2005 these two distinct modes of knowledge structural dynamics can be reflective of the focal role of knowledge as a social construct of reality the first one favors knowledge acquisition patterns more than knowledge diffusion patterns e g virgin islands essentially implying that knowledge generation emerges closer to the fringe of the knowledge networks and flows toward the center the second reverses these trends favors more knowledge diffusion flows than knowledge acquisition implying knowledge generation emergence near the core of the knowledge structure flowing outwards the results of both the community graph plot clustering algorithm classification and the semantic factor analysis classification on the original textual corpus provide us with a broader bird eye understanding of the grouped thematic associations present in the two case study knowledge discourses while both of the methodologies demonstrate a strong power to identify and discriminate groups of knowledge concepts they also help us understand how thematically driven knowledge groups associate with each other there is a significant percentage of heuristic overlap in grouping classifications between the two case studies especially given the fact that the discourse narrative topics are thematically independent from each other at the time of data collection this in turn provides a strong indicator and argument in support of the theoretical proposition that we share a core of our environmental knowledge across communities and cultures we argue that more likely than not this part represents a more global part of environmental knowledge one that is part of our general socio cognitive understanding of environmental change it reflects our general cross societal and cross cultural knowledge related to global environmental change and it is likely to be more influenced by exogenous or broader factors that determine our social understanding of reality within a globalized world no matter what our individual not shared components of our local environmental knowledge are there will always be a fertile core of knowledge where one can plant the seed of true global environmental stewardship the one that connects communities of purpose sense of place and communities of practice with respect to social ecological systems acknowledgments partial funding for this research was provided from the us national science foundation office of integrated activities through the virgin islands experimental program for stimulating competitive research vi epscor award no 1355437 mare nostrum caribbean stewardship through strategic research and workforce development the authors wish to acknowledge the contribution of a number of colleagues including dr michael crosby mote marine laboratory florida usa for hosting and organizing the florida workshop dr hiroshi miki rihn institute for earlier and latter comments on the analysis trajectories and draft we also want to gratefully acknowledge the contributions of the study participants representing professionals agency and government officials community members volunteers and non government organizations for their participation in the workshop and focus group exercises appendix a supplementary data s1 appendix summary statistics of semantic network matrix a comprehensive list of tables providing detailed statistics of the computed semantic network metrics in the study s2 appendix network composite scores reliability tables comprehensive list of tables containing detailed composite score reliability coefficients for the four composite scales for each of the case study implementations s3 file semantic narrative corpus dataset the tabular dataset containing the corpus narratives for each study along with participant and field study attributes the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 026 
26329,a network of independently trained gaussian processes stackedgp is introduced to obtain predictions of geospatial quantities of interest model outputs with quantified uncertainties the uncertain nature of model outputs is due to model inadequacy parametric uncertainty and measurement noise stackedgp framework supports component based modeling in environmental science enhances predictions of quantities of interest through a cascade of intermediate predictions usually addressed by cokriging and propagates uncertainties through emulated dynamical systems driven by uncertain forcing variables by using analytical first and second order moments of a gaussian process with uncertain inputs using squared exponential and polynomial kernels approximated expectations of model outputs that require an arbitrary composition of functions can be obtained the performance of the proposed nonparametric stacked model in model composition and cascading predictions is measured in a wildfire and mineral resource problem using real data and its application to time series prediction is demonstrated in a 2d puff advection problem keywords component based modeling uncertainty propagation nonparametric hierarchical model cokriging data driven emulators software availability an open source python software package stackedgp is available as of january 2018 under gnu general public license v2 and hosted by uqlab on bitbucket https bitbucket org uqlab stackedgp to create general stackedgp models perform optimizations and calculate predictions this software does not require specific hardware but it is dependent on the following python packages numpy scipy gpy since 2012 and sklearn preprocessing the main developer is the first author of the paper kareem abdelfatah who can be reached at krabea email sc edu 1 introduction complex environmental models are modular and hierarchical he et al 2002 grützner 1995 letcher and jakeman 2009 jørgensen 2010 as no one model can describe the entire behavior of a complex system complex models requires coupling of submodels built using various sources of data for example component based modeling is used in forest landscape modeling he et al 2002 where fire and wind models are coupled with vegetation models to estimate the total burned area and in crop modeling where pest population models are coupled with biophysics models to estimate crop growth whish et al 2015 the central challenge with component based modeling is that there is a compound effect of uncertainties coming from errors due to structural submodel inadequacies and noise in experimental data that need to be quantified and propagated to the model outputs this model composition can be arbitrary and highly nested to capture the phenomenon of interest and can be used to make predictions for potentially unobserved quantities of interest this paper develops a general probabilistic modeling framework to address the above two challenges component based modeling under structural uncertainty and propagation of uncertainties to quantities of interest one of the current challenges in component based modeling arises from the fitted parametric nature of submodels with no information most of the time on the magnitude of the uncertainty of the parameters while parametric uncertainty can be quantified in these cases and propagated to the quantities of interest using sampling methods the uncertainty in model predictions may still be underestimated by ignoring model form uncertainty in this paper we proposed to use a data driven and nonparametric approach to build submodels and develop an expectation based approach to propagate the uncertainty the proposed model is based on a network of independently trained gaussian processes accompanied by an approximate scheme to obtain expectations of quantities of interest that require model composition gaussian processes gp williams and rasmussen 1996 rasmussen 1997 rasmussen and williams 2005 are nonparametric statistical models that compactly describe distributions over functions with continuous domains this makes them ideal to quantify uncertainties for environmental subprocesses by modeling measurement noise and structure inadequacies that arise with usual parametric approaches since all environmental subprocesses components are modeled using gps the resulted probabilistic model is a stacked gaussian process stackedgp in this hierarchical setting gps modeling forcing variables govern the input space of gps modeling environmental state variables to provide the intuition behind the motivation for stackedgp consider the following simple environmental example of predicting fungal toxin production in corn y quantity of interest at various spatial locations x interpolation methods do not work directly as the quantity of interest is virtually unobservable over the spatial domain li et al 2015 as a result this effort requires the derivation of a model composition where the toxin production is modeled as a function of temperature z which at its turn is easily observable and can be estimated at any location via spatial interpolation models finding a parametrization for both models to obtain the fungal toxin production at different locations is a non trivial task furthermore the uncertainties in these models as well as in the measurements need to be estimated and propagated to the quantity of interest fig 1 shows a simple scenario that illustrate the motivation of this environmental example it shows a proposed stackedgp comprising two gaussian processes for both temperature interpolation and modeling toxin production as a function of temperature to predict with quantified uncertainties the toxin production at various spatial locations the two gps are built using two datasets d a t a z is obtained from field measurements and consists of temperature values z recorded at different locations x and d a t a y is obtained from wet lab experiments and consists of fungal toxin production y at different temperatures z the final probabilistic prediction of toxin production y as a function of location x is obtained using stackedgp by integrating out the uncertainty in the temperature z the interactive python code to generate this experiment can be found on our online stackedgp repository 1 1 https bitbucket org uqlab stackedgp src master synthetic datasets and is sketched in section 1 the novelty of the paper is in the derivation of a novel approximate algorithm to propagate uncertainties through an arbitrary stackedgp to the quantities of interest using both squared exponential and polynomial kernels rasmussen and williams 2005 the second contribution is the application of stackedgp to several representative examples in environmental science wildfire geology of mineral resources and atmospheric transport by emphasizing improvements in modeling areas such as component based modeling cokriging and emulation of dynamical systems finally a python package 2 2 https bitbucket org uqlab stackedgp is provided to build arbitrary stackedgp models and study uncertainty propagation using the proposed algorithm all the environmental examples in this paper are included in our online code repository the proposed modeling framework is relevant in a number of environmental science problems such as wildfire and forest landscape modeling millington et al 2009 he et al 2002 where component based modeling is used to model various spatio temporal subprocesses vegetation soil moisture and solar radiation to predict quantities of interest such as total burned area in this paper we study the applicability of the proposed method in predicting the total burned area by encoding the modular structure of the canadian forest fire weather index fwi system taylor and alexander 2006 into the stackedgp in addition to supporting the component based environmental modeling stackedgp can be used to enhance predictions of quantities of interest using intermediate predictions of secondary variables which is usually addressed using cokriging wackernagel 1996 enhanced predictions of quantities of interest can be obtained by stacking gps for predicting intermediate secondary responses that govern the input space of gps used to predict primary responses several examples can illustrate the idea such as estimating ozone concentrations singh et al 2011 using the results of chemical transport model simulation as secondary variables and predicting cadmium concentration using concentration of other metals as secondary variables in swiss jura goovaerts 1997 wilson et al 2012 stackedgp is not designed to capture the correlations of response variables however stackedgp models can be constructed by stacking gps for predicting intermediate secondary responses that govern the input space of gps used to predict primary responses this hierarchical framework outperforms other methods as described in the numerical results section where jura dataset wilson et al 2012 is used to assess the prediction accuracy of model with intermediate predictions in environmental sciences uncertainty propagation through dynamical systems is also relevant when high fidelity models are emulated castelletti et al 2012 bayarri et al 2007 conti and o hagan 2010 bhattacharya 2007 for example propagating uncertainties through atmospheric dispersion models nielsen et al 1999 sykes et al 2006 can be tackled through emulation in this case emulators can be used to speed up the uncertainty propagation process and obtain estimates of quantities of interest with quantified uncertainties konda et al 2010 cheng and sandu 2009 conti et al 2009 this is pertinent in operational context when model predictions guide decision making processes and uncertainty propagation and data assimilation terejanu et al 2007 2008 need to be performed in real time stackedgp is especially applicable in the context of gp emulators driven by forcing variables predicted by other gp or stackedgp models a simple 2d puff advection example is provided to showcase stackedgp s applicability in uncertainty propagation using emulated dynamical systems this work unifies the approach of girard et al 2002 and li et al 2015 in li et al 2015 the authors introduced stackedgp to predict carcinogenic toxin concentrations using environmental conditions and monte carlo sampling was used to propagate the uncertainty through the stacked model and estimate the mean and variance of the quantity of interest since sampling requires a high computational cost here the uncertainty propagation through the network is achieved approximately by leveraging the exact moments for the predictive mean and variance derived by girard et al 2002 for a single gp with uncertain inputs and squared exponential kernel stackedgp is conceptually different from deep gps damianou and lawrence 2013 where no data is available for the latent nodes and where the latent variable model requires to jointly infer the hyperparameters corresponding to the mappings between the layers a model carrying the same name was introduced by neumann et al 2009 where a stacked gaussian process was proposed to model pedestrian and public transit flows in urban areas the model proposed by neumann et al 2009 is capable of capturing shared common causes using a joint bayesian inference for multiple tasks in our work the inference is performed independently for each gp node and the uncertainty is approximately propagated through the network stackedgp provides flexibility in kernel selection for intermediate nodes rbf polynomial as well as kernels obtained via their sum and has no restriction in selecting a suitable kernel for input nodes since the gp nodes are independently trained using multiple datasets the running time of the stackedgp grows linearly with the number of nodes and can be sped up through embarrassing parallel training of gps the paper is organized as follows section 1 provides information about the stackedgp python package this is followed by a brief introduction to gp in section 4 section 5 re derives the expectations of a gp with uncertain inputs for squared exponential kernel and provides a novel derivation for the polynomial kernel section 6 generalizes the stackedgp to an arbitrary number of layers and nodes and discusses the advantages and limitations of the proposed model three numerical results are presented in section 7 and conclusions are given in section 8 2 brief tutorial for stackedgp software package the stackedgp software package hosted on bitbucket https bitbucket org uqlab stackedgp contains all the examples used in this paper to showcase the applicability of the stackedgp the python implementation using stackedgp of the hypothetical 1d example in the introduction see fig 1 is provided below in code 1 image 1 code 1 example stackedgp api 3 gaussian process background unlike parametric models non parametric models provide infinite dimensional parameters for modeling the distribution of the data gaussian processes are popular non parametric models rasmussen and williams 2005 williams and rasmussen 1996 williams 1998 reggente et al 2014 that have found various applications in the environmental modeling community they are used as data driven models capable to predict various quantities of interest with quantified uncertainties such as ultra fine particles reggente et al 2014 mean temperatures over north atlantic ocean higdon 1998 wind speed hu and wang 2015 and monthly streamflow sun et al 2014 just to name a few when the training data for gps comes from simulators rather than field measurements then gps become computational efficient surrogate models or emulators of high fidelity models kennedy et al 2002 o hagan 2006 conti and o hagan 2010 with various applications in environmental modeling such as fire emissions katurji et al 2015 ocean and climate circulation tokmakian et al 2012 urban drainage machac et al 2016 and computational fluid dynamics moonen and allegrini 2015 given d x z a set of n data points each consisting of d inputs x ℜ n d and one output z ℜ n the output of the ith data point z i is modeled as follows 1 z i g x i ε i z 2 ε i z n 0 σ ε z 2 3 g gp 0 k z here g represents a latent function with zero mean gaussian process prior and kernel or covariance function k z the kernel measures the similarity between two inputs x i and x j for example the squared exponential or radial basis function rbf kernel is defined as follows 4 k z x i x j ϕ exp θ x i x j 2 the hyperparemeters σ ε z 2 and e g ϕ and θ corresponding to the rbf kernel are estimated using the maximum likelihood approach where the log likelihood is given by 5 ln p 1 2 z t k z σ ε z 2 i z x ϕ θ σ ε z 2 1 z 1 2 ln k z σ ε z 2 i n 2 ln 2 π and the covariance matrix k z is an n n gram matrix with elements k i j k z x i x i once the hyperparameters are estimated the predictive distribution of z at a new testing input x is given by the following normal distribution 6 z n μ z σ z 2 7 μ z k z t c z 1 z 8 σ z 2 k z x x σ z 2 k z t c z 1 k z 9 c z k z σ ε z 2 i in the following section we provide the background for a simple stackedgp as an extension to gp with uncertain inputs as initially developed by girard et al 2002 4 simple stackedgp two chained gaussian processes consider the following simple stackedgp in fig 2 given by two chained gps with their own training dataset the input to the first gp is given by the vector x the output of the first gp z governs the input to the second gp and y is the final output of the stackedgp in fig 2 the goal of this section is to introduce the mechanism of obtaining analytical expectations of two layer stackedgps for both rbf and polynomial kernels note that the predictive distribution of even a simple stackedgp as the one in fig 2 is non gaussian however its mean and variance can be obtained analytically in the next section we will generalize the approach to obtain the approximate expectations of stackedgps with arbitrary number of layers and nodes per layer we start with providing analytical expressions for mean and variance for a general kernel and follow with specific expressions for rbf kernel as initially derived by girard et al 2002 and then with a novel derivation for polynomial kernel the predicted mean of the stackedgp with input x is obtained using the law of total expectation by integrating out the intermediate variable z 10 e y y x e z e y y x z here e y y x z k y t c y 1 y is the expectation of a standard gp with input z and output y and it can be expanded as follows 11 e y y x z y t i 1 n c y 1 i k y z z i where c y is the covariance matrix of the second gp and k y z z i is the kernel between the predicted variable z and the i t h training data point z i and n is the number of training points for the target node the final predicted analytical mean of y can be written as 12 e y y x y t i 1 n c y 1 i e z k y z z i δ 1 e z k y z z i is the key integration to obtain the analytical predicted mean the expectation in eq 12 is with respect to a normal distribution with mean μ z and variance σ z 2 as obtained from the prediction of the first gp the expectation can be obtained analytically for rbf and polynomial kernels as shown in the following two subsections the variance of the stackedgp can be obtained similarly using the law of total variance 13 var y y x e z var y y x z var z e y y x z e z k y z z σ y 2 k y t c y 1 k y var z k y t c y 1 y σ ε y 2 e z k y z z δ 2 e z k y t c y 1 k y var z k y t c y 1 y here σ ε y 2 is the noise variance of the target gp and e z k y t c y 1 k y can be obtained using the following expansion 14 e z k y t c y 1 k y i 1 n j 1 n c y 1 i j e z k y z z i k y z z j δ 3 the last term in eq 13 is given by 15 var z k y t c y 1 y y t c y 1 σ k c y 1 y where σ k var z k y ℜ n n can be expressed as 16 σ k e z k y k y t e z k y e z k y note that σ k is computed using the two integrations of δ 1 and δ 3 in the following two subsections we will provide the analytical first and second moments of stackedgp for rbf and polynomial kernels 4 1 rbf kernel simple case using the rbf kernel k y z z i ϕ exp θ z z i 2 to evaluate δ 1 in eq 12 we obtain δ 1 ϕ 1 2 θ σ z 2 1 2 θ exp z i μ z 2 2 σ z 2 1 2 θ 17 e y y x ϕ y t 1 2 θ σ z 2 1 2 θ i 1 n c y 1 i exp z i μ z 2 2 σ z 2 1 2 θ here θ is the corresponding length scale in the target node ϕ is the kernel s variance and y t is the output training points that have been used during training of the target gp node for rbf kernel δ 2 ϕ and δ 3 in eq 13 can be calculated using the following expression δ 3 ϕ 2 1 4 θ 1 4 θ σ z 2 exp θ z i z j 2 2 z i z j 2 μ z 2 2 1 4 θ σ z 2 here z i is the i t h input training data point for the target node finally the predicted variance is given by 18 var y y x σ ε y 2 ϕ y t c y 1 σ k c y 1 y ϕ 2 i 1 n j 1 n c y 1 i j 1 4 θ 1 4 θ σ z 2 exp θ z i z j 2 2 z i z j 2 μ z 2 2 1 4 θ σ z 2 these analytical expressions corresponding to the rbf kernel coincide with those derived by girard et al 2002 and candela et al 2003 we have provided them here for completeness and to emphasize the role of uncertainty in the network as described in the following sections in the next subsection we provide novel analytical expressions for the predicted mean and variance of stackedgp when using polynomial kernels 4 2 polynomial kernel simple case following the same simple stackedgp configuration and a d order polynomial kernel at the target node k y z z i z z i d the predicted mean of eq 12 can be calculated as e y y x y t i 1 n c y 1 i a d z i d where δ 1 a d z i d and a d follows the non central moments of the normal distribution namely 19 a d u 0 d 2 d 2 u 2 u 1 σ z 2 u μ z d 2 u the expression for the predicted variance in eq 13 is obtained by substituting δ 2 a 2 d and δ 3 a 2 d z i d z j d where a 2 d is calculated using eq 19 finally the predicted variance in the case of polynomial kernel is given by var p o l y y y x σ ε y 2 a 2 d y t c y 1 σ k c y 1 y i 1 n j 1 n a 2 d z i d z j d c y 1 i j 5 stacked gaussian process generalization the goal of this section is to extend the previous stackedgp to an arbitrary number of layers and nodes per layer first we start by presenting the analytical mean and variance of a two layer stackedgp with arbitrary number of nodes in the first layer second we provide a discussion on accommodating an arbitrary number of output nodes in the second layer finally we present an algorithm to compute the approximate mean and variance of a generalized stackedgp and discuss the advantages and limitations of the model 5 1 generalized number of nodes in the first layer of a two layer stackedgp consider an arbitrary number of nodes in the first layer as an extension of the simple two layer stackedgp in the previous section while keeping the single output see fig 3 the analytical expectations presented here will require the independence assumption for the input uncertainties in the target node namely the outputs of the first layer z z 1 z 2 z m 1 t are considered independent see eq 20 in addition the multidimensional kernel is assumed to be obtained as a product of 1d kernels this can easily be extended to sum of kernels and sum of products of 1d kernels 20 k y z z j 1 m 1 k y z j z j thus the expectation of the kernel is factorized as follows 21 e z k y z z j 1 m 1 e z j k y z j z j eq 12 for the mean is generalized as follows 22 e y y x v t c y 1 y where the elements of the vector v ℜ n 1 correspond to the training data points z i for i 1 n and act as kernels under the uncertain inputs 23 v i e z k y z z i j 1 m 1 e z j k y z j z j i similarly given eq 14 the predicted variance of the target node can be generalized as follows 24 v a r y y x σ ε y 2 δ 2 g y t c 1 σ k c 1 y ζ n n c 1 h where the symbol is used for element wise product or hadamard product and the elements of h ℜ n n reflect integrations under the uncertain inputs of the product of two kernel functions as given in eq 20 and evaluated at different training data points 25 h i j e z k y z z i k y z z j 5 1 1 rbf kernel generalized number of nodes in the first layer the analytical mean in the case of the rbf kernel for the output node is obtained using the following elements of the v vector in eq 22 26 v i w q i 27 w j 1 m 1 1 2 θ j 1 2 θ j σ z j 2 28 q i ϕ exp j 1 m 1 z j i μ z j 2 2 1 2 θ j σ z j 2 here i 1 n where n is the number of training data points for the output node m 1 is the number of inputs to the output gp node and z j i is the j t h element of the i t h training data point note that the predicted mean of the stackedgp has the same form as the standard gp but with two main differences first the kernel evaluations v i measure the similarity between the i t h training data and the predicted mean μ z from the previous layer instead of the direct input second the similarity is discounted based on the input uncertainty σ z j 2 note that if we set σ z j 2 to zero we obtain a common product of rbf kernels corresponding to each node in the first layer however the larger the input uncertainty for a particular node the lower the similarity on that particular dimension to obtain the analytical variance for the rbf kernel in eq 24 we use the following relations δ 2 g ϕ and h u p where the scalar u and the elements of p ℜ n n are given by 29 u j 1 m 1 1 4 θ j 1 4 θ j σ z j 2 30 p a b ϕ 2 exp j 1 m 1 θ j z j a z j b 2 2 z j a z j b 2 μ z j 2 2 1 4 θ j σ z j 2 using eq 16 we can get the following expression for σ k 31 σ k u p w 2 t where the elements of the matrix t ℜ n n are defined as 32 t a b ϕ 2 exp j 1 m 1 z j a μ z j 2 z j b μ z j 2 2 1 2 θ j σ z j 2 we emphasize the impact of input uncertainty on the predictive mean and variance which is key in obtaining better predictions namely the input uncertainty weighs the contributions of the particular input to the gp node s prediction note that if the uncertainty from the first layer σ z j 2 0 then we obtain the same standard variance of a gaussian process namely the scalers u and w become one and p a b t a b k y z a μ z j k y z b μ z j t which yields σ k 0 and thus ζ 0 in eq 24 as a result in the case of certain inputs the predicted variance of the stackedgp is similar to the standard gp namely σ ε y 2 ϕ k y t c 1 k y here k y is the kernel evaluated at the training point and the predicted mean of the first layer in other words if we have certain inputs we get standard gp prediction otherwise the uncertainty in the first layer is propagated to the second layer increasing the predictive uncertainty of the stackedgp output in the next section we expand these derivations to polynomial kernels 5 1 2 polynomial kernel generalized number of nodes in the first layer the analytical mean in the case of polynomial kernel of order d for the output node is obtained using the following multinomial expansion for the i t h element of the v vector in eq 22 33 v i p 1 p 2 p m 1 d d p 1 p 2 p m 1 1 t m 1 a p t z t i p t here p i indicates the power of the t t h input with 1 t m 1 in additions d p 1 p 2 p m 1 d p 1 p 2 p m 1 and the coefficient a p t follows the non central moment of the normal distribution shown in eq 19 note that in the absence of input uncertainty namely setting σ z j 2 0 we actually set all but the first term in eq 19 to zero which results in the same formula for the mean of a standard gp with a polynomial kernel of order d to obtain the analytical variance for the polynomial kernel in eq 24 we use the following relations 34 δ 2 g p 1 p 2 p m 1 d d p 1 p 2 p m 1 1 t m 1 a 2 p t 35 a 2 p t u 0 2 p t 2 2 p t 2 u 2 u 1 σ z t 2 u μ z t 2 p t 2 u using eq 16 we can get the expression for σ k 36 σ k h v v t where the elements of the matrix h ℜ n n are obtained using the following multinomial expansion 37 h i j p 1 p m 1 d q 1 q m 1 d d p 1 p m 1 d q 1 q m 1 1 t m 1 a p t q t z t i p t z t j q t 38 a p t q t u 0 p t q t 2 p t q t 2 u 2 u 1 σ z t 2 u μ z t p t q t 2 u similarly as in the rbf case if there is no uncertainty coming from the first layer namely σ z j 2 0 then h v v t which yields σ k 0 and thus ζ 0 in eq 24 since h a b k y z a μ z j k y z b μ z j t this leads to a predicted variance of the stackedgp similar to the standard gp with polynomial kernel σ ε y 2 δ 2 g k y t c 1 k y here k y is the polynomial kernel evaluated at the predicted mean of the first layer and the training points note that the first two moments can be easily obtained also for kernels that involve sums of rbf and polynomial kernels in the following section we discuss how we can expand the two layer network to arbitrary number of outputs and finally the assumptions needed to obtain approximate expectations in a stackedgp with arbitrary number of layers and nodes per layer 5 2 stackedgp with arbitrary number of layers and nodes per layer the only assumption in the previous sections is that the outputs of layers that propagate as inputs to the next layer are independent this applies also to the extension of the previous stackedgp to an arbitrary number of outputs in the last layer this assumption is for convenience as the derivations are significantly more involving however the methodology can accommodate correlated inputs for example co kriging methods cressie 1992 and dependent gps boyle and frean 2005 provide an alternative formulation for obtaining coupled outputs any of these models might be used to generate correlated outputs for any layer however these correlations need to be incorporated into the stackedgp expectations in our numerical results we have opted to pre process the training data using independent component analysis ica to obtain independent projections that are finally used to train the gps note that this procedure does not include the deterministic input observations we plan to extend the derivations to account for correlations in our next study the objective of this section is to build a stackedgp to model an m l dimensional function y x as shown in fig 4 the model has l stacked layers with each layer having m i gp nodes l refers to the index of the layer and the value of m l can be different from layer to layer we assume that we are given the following set of training datasets d t r a i n d 1 d 2 d q where q i 1 l m i represents the total number of nodes in the model in this stacked model each node is independently trained using its own available dataset d q where q 1 q thus each node acts as a standalone standard gp where the hyper parameter optimization inference is conducted using node specific datasets while for two layer stackedgp the mean and the variance can be obtained analytical for both rbf and polynomial kernel in the case of three or more layers the expectations are intractable for the rbf kernel and in the case of polynomial kernels they involve keeping track of large number of terms we have opted to approximately propagate the uncertainty from layer to layer and approximate the expectations of the stackedgp note that even if we are able to obtain analytical expectations for a chain of two gps the underlying distribution is still non gaussian as a result in addition to the independence assumption for the outputs of each layer we add another assumption which involves approximating the distribution of the output of each layer with a gaussian distribution given the analytical mean and variance we use the maximum entropy principle to obtain the gaussian approximation shore and johnson 1980 trebicki and sobczyk 1996 the effect of this approximation is an increase in the uncertainty that is propagated through the network resulting in conservative predictions in large networks or multi step predictions this uncertainty inflation due to maximum entropy approach might have a significant impact however this impact is minimized in applications such as data assimilation where frequent measurements can reduce the predicted uncertainty furthermore a sensitivity analysis can be used to determine the nodes and the inputs that contribute the most to the final uncertainty of the quantity of interest this way one can allocate resources such as targeted data collection or kernel tuning to improve the gp model of the node with the highest uncertainty contribution finally eqs 22 and 24 provide the main mechanism to obtain the approximate mean and variance of a layer given the predictions of the previous layer this process is applied sequentially until the mean and variance of the final quantities of interest are obtained algorithm 1 demonstrates how a general stackedgp is built and the steps required to obtain the desired expectations algorithm 1 stackedgp model building and uncertainty propagation image 2 one limitation of the model is related to the matrix inversion required by the standard gp model which takes o n 3 operations where n is the number of training data points for a particular node several approaches have been proposed to deal with the curse of dimensionality kernel mixing higdon 1998 sparse gp with pseudo inputs snelson and ghahramani 2006 incremental local gaussian regression meier et al 2014 and inversion free approaches anitescu et al 2017 when the output of various layers is high dimensional then dimensionality reduction techniques can be added to pre process the training data higdon et al 2008 also various operations in algorithm 4 are easily parallelizable namely the optimization for hyperparameter estimation of each node can be carried out in parallel as well as within layer propagation of information from the previous layer obviously this computational efficiency over multi output methods comes at a cost of properly accommodating for the correlation of the outputs 6 numerical results in this section we provide three different examples to demonstrate the applicability of stackedgp the first application corresponds to the jura geological dataset where the stackedgp is used to enhance the prediction of a primary response using intermediate predictions of secondary responses in the second example we use stackedgp to combine two real datasets to predict the burned area as part of a forest fire application finally we demonstrate the use of stackedgp in the context of emulated dynamical systems for 2d puff advection driven by uncertain inputs for multi step predictions 6 1 cascading predictions jura dataset in this subsection we use jura dataset collected by the swiss federal institute of technology at lauasanne atteia et al 1994 webster et al 1994 the dataset contains concentration samples of several heavy metals at 359 different locations similar to previous experiments goovaerts 1997 alvarez and lawrence 2011a wilson et al 2012 we are interested in predicting cadmium concentrations the primary response at 100 locations given 259 training measurement points the training data contains location information and concentrations of various metals cd zn ni cr co pb and cu at the sampled sites the primary response is the concentration of cd and the other metals are considered secondary responses note that standard gaussian processes model each response variable independently and thus knowledge of secondary responses cannot help in predicting the primary one teh et al 2005 in this case a standard gaussian process standardgp will use a training dataset with only locations as inputs and cd measurements as target alvarez and lawrence 2011a wilson et al 2012 multi output regression models such as co kriging cressie 1992 can use the correlation between secondary and primary response to improve the prediction of cd the stackedgp while it does not model the correlation between primary and secondary responses it can be used to enhance the prediction of the primary response using intermediate predictions of the secondary responses in the heterotopic case goovaerts 1997 the primary target is undersampled relative to the secondary variables this provides access to secondary information such as ni and zn at 100 locations being estimated as a result a standard gaussian process can be built to have ni and zn directly as inputs here we will denote it as standardgp zn ni this is also the case for comparing our results with other six multi task regression models as reported by wilson et al 2012 and tabulated in table 1 wilson et al 2012 developed a gaussian process regression network gprn to model the correlations between multiple outputs such as primary and secondary responses the outputs are given by weighted linear combinations of latent functions where gp priors are defined over the weights unlike similar studies for semiparametric latent factor model slfm teh et al 2005 boyle and frean 2005 where the weights are considered fixed as these models have no analytical solutions to learn its hyper parameters the authors use different approximation methods such as variational bayes vb fox and roberts 2012 the slfm has been motivated from intrinsic coregionalization model icm goovaerts 1997 in geostatistics however unlike icm the slfm includes gaussian process hyper parameters such as length scales during the learning process in additions convolution gp model for multiple outputs cmogp is another regression model where each output at each x x is a mixture of latent gaussian processes mixed across the whole input domain x stackedgp is not designed to capture the correlations of response variables however stackedgp models can be constructed by stacking gps for predicting intermediate secondary responses that govern the input space of gps used to predict primary responses this hierarchical framework outperforms other methods as shown in table 1 the first proposed stackedgp uses the first layer to model zn and ni based on locations and the second layer to model cd based on the locations and the estimated output of the first layer see fig 5 in the heterotopic case the stackedgp can use directly the available measurements of ni and zn instead of predictions by setting the uncertainty associated with these measurements to zero in this case the stackedgp acts as the standardgp zn ni three other structures are proposed by using intermediate predictions of co cr and co and cr together 3 3 interactive python for all stackedgp structures for jura dataset can be found on https bitbucket org uqlab stackedgp src master cadmium prediction demo in this case we have a three layer stackedgp to model cd see fig 6 the first layer is the same as in the previous setup the second layer models intermediate responses co cr and co and cr the third layer is used to model cd based on the second layer predictions in additions to the input output of the first layer namely location and zn and ni fig 6 also shows the predicted spatial fields for different metals the predicted mean concentration of each metal is depicted as a heat map where x axis and y axis represent latitude and longitude respectively table 1 shows the results of these stacked structures stackedgp co stackedgp cr and stackedgp co cr while measurements of ni and zn are available in the testing scenarios there are no measurements for co and cr during testing thus cd predictions of these three stackedgps rely on predictions of co and cr using locations and ni and zn measurements at these locations the mean absolute error mae between the true and estimated cd is calculated at the 100 target locations the experimental setup follows alvarez and lawrence 2011a and wilson et al 2012 for which the simulation is restarted 10 times using different initializations of the parameters namely the length scale for the rbf kernel in case of the stackedgp the average and standard deviation of mae over these 10 runs is reported in table 1 overall stackedgp gives better results as compared with the other models also when zn and ni measurements are available as assumed by the other multi output regression models wilson et al 2012 alvarez and lawrence 2011a then a standardgp ni zn can provide a lower mae than the other six multi output regression models however stackedgp can provide a better performance over the standard zn ni by making use of intermediate predictions of secondary responses for all these experiments we found that the log transformation and normalization can lead to better results for multi responses in the middle layer we used independent component analysis ica to obtain independent projections of secondary responses this is required as the current derivation assumes that inputs to a gp node are independent the complexity of most of multi task models e g cmogp slfm is o n 3 p 3 where n is size of the training dataset and p is the number of output responses alvarez and lawrence 2011b wilson et al 2012 as gprn depends on approximation methods such as variational bayes it needs several iterations to reach suitable hyper parameters a larger the number of iterations increases the time complexity of the model therefore it may achieve lower complexity such as o p n 3 at the cost of obtaining a lower accuracy by decreasing the number of iterations stackedgp scales linearly with the number of nodes in the structure because of the independent training of the nodes which can be done in parallel in the worst case stackedgp is o p n 3 nonetheless sparse approximation techniques can be used to further reduce this complexity in the case of large training datasets snelson 2007 damianou et al 2011 furthermore standardgp co kriging and icm have o n 3 complexity but they achieve a lower accuracy as compared with the other mulit task models 6 2 model composition forest fire dataset the prediction of the burned area from forest fires has been discussed in different studies such as cortez and morais 2007 and castelli et al 2015 the burned area of forest fires has been predicted using meteorological conditions e g temperature wind and or several canadian forest fire weather indices taylor and alexander 2006 for rating fire danger namely fine fuel moisture code ffmc duff moisture code dmc drought code dc initial spread index isi and buildup index bui as shown in fig 7 in this application we are interested in developing a stackedgp 4 4 interactive python for stackedgp structure for forest fire dataset can be found at https bitbucket org uqlab stackedgp src master forestfire by first modeling the fire indices using meteorological variables t from one dataset presented in van wagner et al 1985 and then model the burned area based on fire indices using another dataset presented in cortez and morais 2007 the proposed stackedgp is depicted in fig 8 the gp nodes corresponding to the four fire indices ffmc dmc dc and isi are trained from data published in van wagner et al 1985 according to the hierarchical structure shown in fig 7 while the second dataset cortez and morais 2007 contains meteorological conditions along with the fire indices and burned area we assume that the meteorological conditions are missing in the training phase from this dataset and use only the fire indices and burned area data to train the gp node in the last layer of the stackedgp a 10 fold cross validation is applied to the dataset published by cortez and morais 2007 to train the burned area node and test the whole stackedgp model because of the skewed distribution of the burned area values and to ensure positive value for our predictions instead of directly modeling the burned area using stackedgp we have modeled the log of the burned area as a result the final mean and variance of the burned area b t as a function of the meteorological conditions t is given by eqs 39 and 40 respectively in additions we have found that scaling the target variable to have zero mean and unit variance to be a beneficial preprocessing step 39 e b e σ ln b 2 1 e 2 μ ln b σ ln b 2 40 v a r b e μ ln b 0 5 σ ln b 2 here μ ln b and σ ln b are the output of the probabilistic analytical stackedgp eqs 22 and 24 in the case of the rbf kernel see section 6 1 1 the result of modeling the burned area using the stackedgp is shown in table 2 the stackedgp model is compared with the results of 5 other regression models reported by cortez and morais 2007 because these regression models have been tested using different input spaces table 2 tabulates the best results achieved by each model as described in cortez and morais 2007 even though the stackedgp predicts the burned area based on estimated indices from the first dataset and not the actual values as presented in the second dataset it is still able to give comparable results with the other models that make use of meteorological conditions and or fire indices available in the second dataset this experiment emphasizes that the stackedgp is able to combine knowledge from multiple datasets with noticeable performance 6 3 uncertainty propagation atmospheric transport gaussian processes with uncertain inputs have been previously used in multi step time series predictions girard et al 2003 candela et al 2003 modeling multi step ahead predictions can be achieved by feeding back the predicted mean and variance at each time and propagating the uncertainty to the next time step this idea has been used in different time series applications such as electricity forecasting lourenço and santos 2010 and water demand forecasting wang et al 2014 here we expand this concept by further driving the dynamical system using another gp for propagating uncertainty in an atmospheric transport problem we consider a simple advection of a 2d gaussian shaped puff nielsen et al 1999 terejanu et al 2007 the states of the puff evolve using the following equations 41 x k 1 x k u x x k δ t 42 y k 1 y k u y y k δ t 43 d k 1 d k u x 2 x k u y 2 y k δ t here x k y k is the position of the center of the puff and the downwind distance from the source d k is used to compute the puff radius σ k p d k q in models such as rimpuff nielsen et al 1999 based on karlsruhe jülich diffusion coefficients reddy et al 2006 p q the goal here is to build a gp emulator for the above dynamical system knowing that the release location is fixed at x 0 0 km y 0 0 km and the wind velocity is uncertain with normally distributed wind components u x u y 44 u x u y n 4 m s 1 m s the gp emulator h is constructed using 15 training trajectories that start at the same release location but correspond to different wind fields that randomly sampled from the distribution in eq 44 the total simulation time is 30min with a time step δ t 90 sec as a result k has range of 20 steps during the simulation time 45 x k 1 y k 1 d k 1 h x k y k u x x k u y x k another gp model is constructed to determine the wind field based on 16 wind sensors positioned 4 km apart in both directions the wind sensor readings are just independent and identically distributed samples from eq 44 46 u x x u y y g x y note that in this particular case the wind velocity at different locations is correlated both emulators use rbf kernels and they are stacked to build a recurrent stackedgp as shown in fig 9 to assess the effect of the two assumptions in constructing the stackedgp 5 5 interactive python for stackedgp structure for atmospheric transport experiment can be found on https bitbucket org uqlab stackedgp src master uncertainty propagation atmospheric transport independent inputs for each layer and gaussian distribution approximation for the output of each layer we compared the approximate mean and variance of the puff states from stackedgp using the proposed algorithm with those resulted from a monte carlo propagation of uncertainty through the stackedgp using 1000 samples fig 10 shows the approximate predicted gaussian distribution of the states along with the histogram of the monte carlo samples propagated through the stackedgp table 3 lists the predicted mean and standard deviation of the puff states at different time steps note that even though the state equations for the location of the puff are linear because they are emulated using a gp which at its turn is driven by a gp model for the wind field the distribution of the stackedgp output may depart from the gaussian distribution the assumption of approximating the output with a gaussian distribution may result in biasing the mean location the statistical significant difference between the stackedgp approximate mean propagation and its monte carlo estimate confirms the impact of this approximation as shown in table 3 furthermore the assumption of ignoring the correlation structure between the outputs of stackedgp may result in an artificial inflation of the uncertainty in our simple example this is clearly manifested in larger standard deviations for the downwind using approximate propagation as compared with the monte carlo estimate this impact on uncertainty propagation might be exacerbated when more nonlinear models are used which limits the horizon of uncertainty propagation obviously the gain in computational speed combined with field measurements in the context of data assimilation may position these stacked model as real contenders for real time applications we plan to investigate in the future the application of stackedgp to data assimilation 7 conclusions a stacked model of independently trained gaussian processes called stackedgp is proposed as a modeling framework in the context of model composition this is especially of interest in environmental modeling where e g model composition is used to generate large scale predictions by combining geographical interpolation models with phenomenological models developed in the lab an approximate approach is developed to obtain estimates of the quantities of interest with quantified uncertainties this leverages the analytical moments of a gaussian process with uncertain inputs when squared exponential and polynomial kernels are used the stackedgp can be extended to any number of nodes and layers and has no restriction in selecting a suitable kernel for the input nodes the numerical results show the utility of using stackedgp to learn from multiple datasets and propagate the uncertainty to quantities of interest while it is not specifically designed to model correlations between secondary and primary responses stackedgp can be used to enhance the prediction of primary responses by creating an intermediate layer of predictions of secondary responses this comes with a lower computational complexity as compared with multi output methods and can make use of off the shelves gaussian processes while in the current paper we assume that outputs of intermediate layers are independent and resolve this using independent component analysis preprocessing we plan to extend our derivation to account for these correlations in the next study this will allow multi output models to act as nodes in the proposed stackedgp along with the independence assumption the other drawback of the proposed uncertainty propagation algorithm is the gaussian assumption of the predictive distribution while this is motivated using maximum entropy principle in a multi step prediction setting it overestimates the predicted uncertainty acknowledgments this material is based upon work supported by the national science foundation under grand no 1504728 and 1632824 dr terejanu has been supported by the national institute of food and agriculture nifa usda under grand no 2017 67017 26167 appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 022 
26329,a network of independently trained gaussian processes stackedgp is introduced to obtain predictions of geospatial quantities of interest model outputs with quantified uncertainties the uncertain nature of model outputs is due to model inadequacy parametric uncertainty and measurement noise stackedgp framework supports component based modeling in environmental science enhances predictions of quantities of interest through a cascade of intermediate predictions usually addressed by cokriging and propagates uncertainties through emulated dynamical systems driven by uncertain forcing variables by using analytical first and second order moments of a gaussian process with uncertain inputs using squared exponential and polynomial kernels approximated expectations of model outputs that require an arbitrary composition of functions can be obtained the performance of the proposed nonparametric stacked model in model composition and cascading predictions is measured in a wildfire and mineral resource problem using real data and its application to time series prediction is demonstrated in a 2d puff advection problem keywords component based modeling uncertainty propagation nonparametric hierarchical model cokriging data driven emulators software availability an open source python software package stackedgp is available as of january 2018 under gnu general public license v2 and hosted by uqlab on bitbucket https bitbucket org uqlab stackedgp to create general stackedgp models perform optimizations and calculate predictions this software does not require specific hardware but it is dependent on the following python packages numpy scipy gpy since 2012 and sklearn preprocessing the main developer is the first author of the paper kareem abdelfatah who can be reached at krabea email sc edu 1 introduction complex environmental models are modular and hierarchical he et al 2002 grützner 1995 letcher and jakeman 2009 jørgensen 2010 as no one model can describe the entire behavior of a complex system complex models requires coupling of submodels built using various sources of data for example component based modeling is used in forest landscape modeling he et al 2002 where fire and wind models are coupled with vegetation models to estimate the total burned area and in crop modeling where pest population models are coupled with biophysics models to estimate crop growth whish et al 2015 the central challenge with component based modeling is that there is a compound effect of uncertainties coming from errors due to structural submodel inadequacies and noise in experimental data that need to be quantified and propagated to the model outputs this model composition can be arbitrary and highly nested to capture the phenomenon of interest and can be used to make predictions for potentially unobserved quantities of interest this paper develops a general probabilistic modeling framework to address the above two challenges component based modeling under structural uncertainty and propagation of uncertainties to quantities of interest one of the current challenges in component based modeling arises from the fitted parametric nature of submodels with no information most of the time on the magnitude of the uncertainty of the parameters while parametric uncertainty can be quantified in these cases and propagated to the quantities of interest using sampling methods the uncertainty in model predictions may still be underestimated by ignoring model form uncertainty in this paper we proposed to use a data driven and nonparametric approach to build submodels and develop an expectation based approach to propagate the uncertainty the proposed model is based on a network of independently trained gaussian processes accompanied by an approximate scheme to obtain expectations of quantities of interest that require model composition gaussian processes gp williams and rasmussen 1996 rasmussen 1997 rasmussen and williams 2005 are nonparametric statistical models that compactly describe distributions over functions with continuous domains this makes them ideal to quantify uncertainties for environmental subprocesses by modeling measurement noise and structure inadequacies that arise with usual parametric approaches since all environmental subprocesses components are modeled using gps the resulted probabilistic model is a stacked gaussian process stackedgp in this hierarchical setting gps modeling forcing variables govern the input space of gps modeling environmental state variables to provide the intuition behind the motivation for stackedgp consider the following simple environmental example of predicting fungal toxin production in corn y quantity of interest at various spatial locations x interpolation methods do not work directly as the quantity of interest is virtually unobservable over the spatial domain li et al 2015 as a result this effort requires the derivation of a model composition where the toxin production is modeled as a function of temperature z which at its turn is easily observable and can be estimated at any location via spatial interpolation models finding a parametrization for both models to obtain the fungal toxin production at different locations is a non trivial task furthermore the uncertainties in these models as well as in the measurements need to be estimated and propagated to the quantity of interest fig 1 shows a simple scenario that illustrate the motivation of this environmental example it shows a proposed stackedgp comprising two gaussian processes for both temperature interpolation and modeling toxin production as a function of temperature to predict with quantified uncertainties the toxin production at various spatial locations the two gps are built using two datasets d a t a z is obtained from field measurements and consists of temperature values z recorded at different locations x and d a t a y is obtained from wet lab experiments and consists of fungal toxin production y at different temperatures z the final probabilistic prediction of toxin production y as a function of location x is obtained using stackedgp by integrating out the uncertainty in the temperature z the interactive python code to generate this experiment can be found on our online stackedgp repository 1 1 https bitbucket org uqlab stackedgp src master synthetic datasets and is sketched in section 1 the novelty of the paper is in the derivation of a novel approximate algorithm to propagate uncertainties through an arbitrary stackedgp to the quantities of interest using both squared exponential and polynomial kernels rasmussen and williams 2005 the second contribution is the application of stackedgp to several representative examples in environmental science wildfire geology of mineral resources and atmospheric transport by emphasizing improvements in modeling areas such as component based modeling cokriging and emulation of dynamical systems finally a python package 2 2 https bitbucket org uqlab stackedgp is provided to build arbitrary stackedgp models and study uncertainty propagation using the proposed algorithm all the environmental examples in this paper are included in our online code repository the proposed modeling framework is relevant in a number of environmental science problems such as wildfire and forest landscape modeling millington et al 2009 he et al 2002 where component based modeling is used to model various spatio temporal subprocesses vegetation soil moisture and solar radiation to predict quantities of interest such as total burned area in this paper we study the applicability of the proposed method in predicting the total burned area by encoding the modular structure of the canadian forest fire weather index fwi system taylor and alexander 2006 into the stackedgp in addition to supporting the component based environmental modeling stackedgp can be used to enhance predictions of quantities of interest using intermediate predictions of secondary variables which is usually addressed using cokriging wackernagel 1996 enhanced predictions of quantities of interest can be obtained by stacking gps for predicting intermediate secondary responses that govern the input space of gps used to predict primary responses several examples can illustrate the idea such as estimating ozone concentrations singh et al 2011 using the results of chemical transport model simulation as secondary variables and predicting cadmium concentration using concentration of other metals as secondary variables in swiss jura goovaerts 1997 wilson et al 2012 stackedgp is not designed to capture the correlations of response variables however stackedgp models can be constructed by stacking gps for predicting intermediate secondary responses that govern the input space of gps used to predict primary responses this hierarchical framework outperforms other methods as described in the numerical results section where jura dataset wilson et al 2012 is used to assess the prediction accuracy of model with intermediate predictions in environmental sciences uncertainty propagation through dynamical systems is also relevant when high fidelity models are emulated castelletti et al 2012 bayarri et al 2007 conti and o hagan 2010 bhattacharya 2007 for example propagating uncertainties through atmospheric dispersion models nielsen et al 1999 sykes et al 2006 can be tackled through emulation in this case emulators can be used to speed up the uncertainty propagation process and obtain estimates of quantities of interest with quantified uncertainties konda et al 2010 cheng and sandu 2009 conti et al 2009 this is pertinent in operational context when model predictions guide decision making processes and uncertainty propagation and data assimilation terejanu et al 2007 2008 need to be performed in real time stackedgp is especially applicable in the context of gp emulators driven by forcing variables predicted by other gp or stackedgp models a simple 2d puff advection example is provided to showcase stackedgp s applicability in uncertainty propagation using emulated dynamical systems this work unifies the approach of girard et al 2002 and li et al 2015 in li et al 2015 the authors introduced stackedgp to predict carcinogenic toxin concentrations using environmental conditions and monte carlo sampling was used to propagate the uncertainty through the stacked model and estimate the mean and variance of the quantity of interest since sampling requires a high computational cost here the uncertainty propagation through the network is achieved approximately by leveraging the exact moments for the predictive mean and variance derived by girard et al 2002 for a single gp with uncertain inputs and squared exponential kernel stackedgp is conceptually different from deep gps damianou and lawrence 2013 where no data is available for the latent nodes and where the latent variable model requires to jointly infer the hyperparameters corresponding to the mappings between the layers a model carrying the same name was introduced by neumann et al 2009 where a stacked gaussian process was proposed to model pedestrian and public transit flows in urban areas the model proposed by neumann et al 2009 is capable of capturing shared common causes using a joint bayesian inference for multiple tasks in our work the inference is performed independently for each gp node and the uncertainty is approximately propagated through the network stackedgp provides flexibility in kernel selection for intermediate nodes rbf polynomial as well as kernels obtained via their sum and has no restriction in selecting a suitable kernel for input nodes since the gp nodes are independently trained using multiple datasets the running time of the stackedgp grows linearly with the number of nodes and can be sped up through embarrassing parallel training of gps the paper is organized as follows section 1 provides information about the stackedgp python package this is followed by a brief introduction to gp in section 4 section 5 re derives the expectations of a gp with uncertain inputs for squared exponential kernel and provides a novel derivation for the polynomial kernel section 6 generalizes the stackedgp to an arbitrary number of layers and nodes and discusses the advantages and limitations of the proposed model three numerical results are presented in section 7 and conclusions are given in section 8 2 brief tutorial for stackedgp software package the stackedgp software package hosted on bitbucket https bitbucket org uqlab stackedgp contains all the examples used in this paper to showcase the applicability of the stackedgp the python implementation using stackedgp of the hypothetical 1d example in the introduction see fig 1 is provided below in code 1 image 1 code 1 example stackedgp api 3 gaussian process background unlike parametric models non parametric models provide infinite dimensional parameters for modeling the distribution of the data gaussian processes are popular non parametric models rasmussen and williams 2005 williams and rasmussen 1996 williams 1998 reggente et al 2014 that have found various applications in the environmental modeling community they are used as data driven models capable to predict various quantities of interest with quantified uncertainties such as ultra fine particles reggente et al 2014 mean temperatures over north atlantic ocean higdon 1998 wind speed hu and wang 2015 and monthly streamflow sun et al 2014 just to name a few when the training data for gps comes from simulators rather than field measurements then gps become computational efficient surrogate models or emulators of high fidelity models kennedy et al 2002 o hagan 2006 conti and o hagan 2010 with various applications in environmental modeling such as fire emissions katurji et al 2015 ocean and climate circulation tokmakian et al 2012 urban drainage machac et al 2016 and computational fluid dynamics moonen and allegrini 2015 given d x z a set of n data points each consisting of d inputs x ℜ n d and one output z ℜ n the output of the ith data point z i is modeled as follows 1 z i g x i ε i z 2 ε i z n 0 σ ε z 2 3 g gp 0 k z here g represents a latent function with zero mean gaussian process prior and kernel or covariance function k z the kernel measures the similarity between two inputs x i and x j for example the squared exponential or radial basis function rbf kernel is defined as follows 4 k z x i x j ϕ exp θ x i x j 2 the hyperparemeters σ ε z 2 and e g ϕ and θ corresponding to the rbf kernel are estimated using the maximum likelihood approach where the log likelihood is given by 5 ln p 1 2 z t k z σ ε z 2 i z x ϕ θ σ ε z 2 1 z 1 2 ln k z σ ε z 2 i n 2 ln 2 π and the covariance matrix k z is an n n gram matrix with elements k i j k z x i x i once the hyperparameters are estimated the predictive distribution of z at a new testing input x is given by the following normal distribution 6 z n μ z σ z 2 7 μ z k z t c z 1 z 8 σ z 2 k z x x σ z 2 k z t c z 1 k z 9 c z k z σ ε z 2 i in the following section we provide the background for a simple stackedgp as an extension to gp with uncertain inputs as initially developed by girard et al 2002 4 simple stackedgp two chained gaussian processes consider the following simple stackedgp in fig 2 given by two chained gps with their own training dataset the input to the first gp is given by the vector x the output of the first gp z governs the input to the second gp and y is the final output of the stackedgp in fig 2 the goal of this section is to introduce the mechanism of obtaining analytical expectations of two layer stackedgps for both rbf and polynomial kernels note that the predictive distribution of even a simple stackedgp as the one in fig 2 is non gaussian however its mean and variance can be obtained analytically in the next section we will generalize the approach to obtain the approximate expectations of stackedgps with arbitrary number of layers and nodes per layer we start with providing analytical expressions for mean and variance for a general kernel and follow with specific expressions for rbf kernel as initially derived by girard et al 2002 and then with a novel derivation for polynomial kernel the predicted mean of the stackedgp with input x is obtained using the law of total expectation by integrating out the intermediate variable z 10 e y y x e z e y y x z here e y y x z k y t c y 1 y is the expectation of a standard gp with input z and output y and it can be expanded as follows 11 e y y x z y t i 1 n c y 1 i k y z z i where c y is the covariance matrix of the second gp and k y z z i is the kernel between the predicted variable z and the i t h training data point z i and n is the number of training points for the target node the final predicted analytical mean of y can be written as 12 e y y x y t i 1 n c y 1 i e z k y z z i δ 1 e z k y z z i is the key integration to obtain the analytical predicted mean the expectation in eq 12 is with respect to a normal distribution with mean μ z and variance σ z 2 as obtained from the prediction of the first gp the expectation can be obtained analytically for rbf and polynomial kernels as shown in the following two subsections the variance of the stackedgp can be obtained similarly using the law of total variance 13 var y y x e z var y y x z var z e y y x z e z k y z z σ y 2 k y t c y 1 k y var z k y t c y 1 y σ ε y 2 e z k y z z δ 2 e z k y t c y 1 k y var z k y t c y 1 y here σ ε y 2 is the noise variance of the target gp and e z k y t c y 1 k y can be obtained using the following expansion 14 e z k y t c y 1 k y i 1 n j 1 n c y 1 i j e z k y z z i k y z z j δ 3 the last term in eq 13 is given by 15 var z k y t c y 1 y y t c y 1 σ k c y 1 y where σ k var z k y ℜ n n can be expressed as 16 σ k e z k y k y t e z k y e z k y note that σ k is computed using the two integrations of δ 1 and δ 3 in the following two subsections we will provide the analytical first and second moments of stackedgp for rbf and polynomial kernels 4 1 rbf kernel simple case using the rbf kernel k y z z i ϕ exp θ z z i 2 to evaluate δ 1 in eq 12 we obtain δ 1 ϕ 1 2 θ σ z 2 1 2 θ exp z i μ z 2 2 σ z 2 1 2 θ 17 e y y x ϕ y t 1 2 θ σ z 2 1 2 θ i 1 n c y 1 i exp z i μ z 2 2 σ z 2 1 2 θ here θ is the corresponding length scale in the target node ϕ is the kernel s variance and y t is the output training points that have been used during training of the target gp node for rbf kernel δ 2 ϕ and δ 3 in eq 13 can be calculated using the following expression δ 3 ϕ 2 1 4 θ 1 4 θ σ z 2 exp θ z i z j 2 2 z i z j 2 μ z 2 2 1 4 θ σ z 2 here z i is the i t h input training data point for the target node finally the predicted variance is given by 18 var y y x σ ε y 2 ϕ y t c y 1 σ k c y 1 y ϕ 2 i 1 n j 1 n c y 1 i j 1 4 θ 1 4 θ σ z 2 exp θ z i z j 2 2 z i z j 2 μ z 2 2 1 4 θ σ z 2 these analytical expressions corresponding to the rbf kernel coincide with those derived by girard et al 2002 and candela et al 2003 we have provided them here for completeness and to emphasize the role of uncertainty in the network as described in the following sections in the next subsection we provide novel analytical expressions for the predicted mean and variance of stackedgp when using polynomial kernels 4 2 polynomial kernel simple case following the same simple stackedgp configuration and a d order polynomial kernel at the target node k y z z i z z i d the predicted mean of eq 12 can be calculated as e y y x y t i 1 n c y 1 i a d z i d where δ 1 a d z i d and a d follows the non central moments of the normal distribution namely 19 a d u 0 d 2 d 2 u 2 u 1 σ z 2 u μ z d 2 u the expression for the predicted variance in eq 13 is obtained by substituting δ 2 a 2 d and δ 3 a 2 d z i d z j d where a 2 d is calculated using eq 19 finally the predicted variance in the case of polynomial kernel is given by var p o l y y y x σ ε y 2 a 2 d y t c y 1 σ k c y 1 y i 1 n j 1 n a 2 d z i d z j d c y 1 i j 5 stacked gaussian process generalization the goal of this section is to extend the previous stackedgp to an arbitrary number of layers and nodes per layer first we start by presenting the analytical mean and variance of a two layer stackedgp with arbitrary number of nodes in the first layer second we provide a discussion on accommodating an arbitrary number of output nodes in the second layer finally we present an algorithm to compute the approximate mean and variance of a generalized stackedgp and discuss the advantages and limitations of the model 5 1 generalized number of nodes in the first layer of a two layer stackedgp consider an arbitrary number of nodes in the first layer as an extension of the simple two layer stackedgp in the previous section while keeping the single output see fig 3 the analytical expectations presented here will require the independence assumption for the input uncertainties in the target node namely the outputs of the first layer z z 1 z 2 z m 1 t are considered independent see eq 20 in addition the multidimensional kernel is assumed to be obtained as a product of 1d kernels this can easily be extended to sum of kernels and sum of products of 1d kernels 20 k y z z j 1 m 1 k y z j z j thus the expectation of the kernel is factorized as follows 21 e z k y z z j 1 m 1 e z j k y z j z j eq 12 for the mean is generalized as follows 22 e y y x v t c y 1 y where the elements of the vector v ℜ n 1 correspond to the training data points z i for i 1 n and act as kernels under the uncertain inputs 23 v i e z k y z z i j 1 m 1 e z j k y z j z j i similarly given eq 14 the predicted variance of the target node can be generalized as follows 24 v a r y y x σ ε y 2 δ 2 g y t c 1 σ k c 1 y ζ n n c 1 h where the symbol is used for element wise product or hadamard product and the elements of h ℜ n n reflect integrations under the uncertain inputs of the product of two kernel functions as given in eq 20 and evaluated at different training data points 25 h i j e z k y z z i k y z z j 5 1 1 rbf kernel generalized number of nodes in the first layer the analytical mean in the case of the rbf kernel for the output node is obtained using the following elements of the v vector in eq 22 26 v i w q i 27 w j 1 m 1 1 2 θ j 1 2 θ j σ z j 2 28 q i ϕ exp j 1 m 1 z j i μ z j 2 2 1 2 θ j σ z j 2 here i 1 n where n is the number of training data points for the output node m 1 is the number of inputs to the output gp node and z j i is the j t h element of the i t h training data point note that the predicted mean of the stackedgp has the same form as the standard gp but with two main differences first the kernel evaluations v i measure the similarity between the i t h training data and the predicted mean μ z from the previous layer instead of the direct input second the similarity is discounted based on the input uncertainty σ z j 2 note that if we set σ z j 2 to zero we obtain a common product of rbf kernels corresponding to each node in the first layer however the larger the input uncertainty for a particular node the lower the similarity on that particular dimension to obtain the analytical variance for the rbf kernel in eq 24 we use the following relations δ 2 g ϕ and h u p where the scalar u and the elements of p ℜ n n are given by 29 u j 1 m 1 1 4 θ j 1 4 θ j σ z j 2 30 p a b ϕ 2 exp j 1 m 1 θ j z j a z j b 2 2 z j a z j b 2 μ z j 2 2 1 4 θ j σ z j 2 using eq 16 we can get the following expression for σ k 31 σ k u p w 2 t where the elements of the matrix t ℜ n n are defined as 32 t a b ϕ 2 exp j 1 m 1 z j a μ z j 2 z j b μ z j 2 2 1 2 θ j σ z j 2 we emphasize the impact of input uncertainty on the predictive mean and variance which is key in obtaining better predictions namely the input uncertainty weighs the contributions of the particular input to the gp node s prediction note that if the uncertainty from the first layer σ z j 2 0 then we obtain the same standard variance of a gaussian process namely the scalers u and w become one and p a b t a b k y z a μ z j k y z b μ z j t which yields σ k 0 and thus ζ 0 in eq 24 as a result in the case of certain inputs the predicted variance of the stackedgp is similar to the standard gp namely σ ε y 2 ϕ k y t c 1 k y here k y is the kernel evaluated at the training point and the predicted mean of the first layer in other words if we have certain inputs we get standard gp prediction otherwise the uncertainty in the first layer is propagated to the second layer increasing the predictive uncertainty of the stackedgp output in the next section we expand these derivations to polynomial kernels 5 1 2 polynomial kernel generalized number of nodes in the first layer the analytical mean in the case of polynomial kernel of order d for the output node is obtained using the following multinomial expansion for the i t h element of the v vector in eq 22 33 v i p 1 p 2 p m 1 d d p 1 p 2 p m 1 1 t m 1 a p t z t i p t here p i indicates the power of the t t h input with 1 t m 1 in additions d p 1 p 2 p m 1 d p 1 p 2 p m 1 and the coefficient a p t follows the non central moment of the normal distribution shown in eq 19 note that in the absence of input uncertainty namely setting σ z j 2 0 we actually set all but the first term in eq 19 to zero which results in the same formula for the mean of a standard gp with a polynomial kernel of order d to obtain the analytical variance for the polynomial kernel in eq 24 we use the following relations 34 δ 2 g p 1 p 2 p m 1 d d p 1 p 2 p m 1 1 t m 1 a 2 p t 35 a 2 p t u 0 2 p t 2 2 p t 2 u 2 u 1 σ z t 2 u μ z t 2 p t 2 u using eq 16 we can get the expression for σ k 36 σ k h v v t where the elements of the matrix h ℜ n n are obtained using the following multinomial expansion 37 h i j p 1 p m 1 d q 1 q m 1 d d p 1 p m 1 d q 1 q m 1 1 t m 1 a p t q t z t i p t z t j q t 38 a p t q t u 0 p t q t 2 p t q t 2 u 2 u 1 σ z t 2 u μ z t p t q t 2 u similarly as in the rbf case if there is no uncertainty coming from the first layer namely σ z j 2 0 then h v v t which yields σ k 0 and thus ζ 0 in eq 24 since h a b k y z a μ z j k y z b μ z j t this leads to a predicted variance of the stackedgp similar to the standard gp with polynomial kernel σ ε y 2 δ 2 g k y t c 1 k y here k y is the polynomial kernel evaluated at the predicted mean of the first layer and the training points note that the first two moments can be easily obtained also for kernels that involve sums of rbf and polynomial kernels in the following section we discuss how we can expand the two layer network to arbitrary number of outputs and finally the assumptions needed to obtain approximate expectations in a stackedgp with arbitrary number of layers and nodes per layer 5 2 stackedgp with arbitrary number of layers and nodes per layer the only assumption in the previous sections is that the outputs of layers that propagate as inputs to the next layer are independent this applies also to the extension of the previous stackedgp to an arbitrary number of outputs in the last layer this assumption is for convenience as the derivations are significantly more involving however the methodology can accommodate correlated inputs for example co kriging methods cressie 1992 and dependent gps boyle and frean 2005 provide an alternative formulation for obtaining coupled outputs any of these models might be used to generate correlated outputs for any layer however these correlations need to be incorporated into the stackedgp expectations in our numerical results we have opted to pre process the training data using independent component analysis ica to obtain independent projections that are finally used to train the gps note that this procedure does not include the deterministic input observations we plan to extend the derivations to account for correlations in our next study the objective of this section is to build a stackedgp to model an m l dimensional function y x as shown in fig 4 the model has l stacked layers with each layer having m i gp nodes l refers to the index of the layer and the value of m l can be different from layer to layer we assume that we are given the following set of training datasets d t r a i n d 1 d 2 d q where q i 1 l m i represents the total number of nodes in the model in this stacked model each node is independently trained using its own available dataset d q where q 1 q thus each node acts as a standalone standard gp where the hyper parameter optimization inference is conducted using node specific datasets while for two layer stackedgp the mean and the variance can be obtained analytical for both rbf and polynomial kernel in the case of three or more layers the expectations are intractable for the rbf kernel and in the case of polynomial kernels they involve keeping track of large number of terms we have opted to approximately propagate the uncertainty from layer to layer and approximate the expectations of the stackedgp note that even if we are able to obtain analytical expectations for a chain of two gps the underlying distribution is still non gaussian as a result in addition to the independence assumption for the outputs of each layer we add another assumption which involves approximating the distribution of the output of each layer with a gaussian distribution given the analytical mean and variance we use the maximum entropy principle to obtain the gaussian approximation shore and johnson 1980 trebicki and sobczyk 1996 the effect of this approximation is an increase in the uncertainty that is propagated through the network resulting in conservative predictions in large networks or multi step predictions this uncertainty inflation due to maximum entropy approach might have a significant impact however this impact is minimized in applications such as data assimilation where frequent measurements can reduce the predicted uncertainty furthermore a sensitivity analysis can be used to determine the nodes and the inputs that contribute the most to the final uncertainty of the quantity of interest this way one can allocate resources such as targeted data collection or kernel tuning to improve the gp model of the node with the highest uncertainty contribution finally eqs 22 and 24 provide the main mechanism to obtain the approximate mean and variance of a layer given the predictions of the previous layer this process is applied sequentially until the mean and variance of the final quantities of interest are obtained algorithm 1 demonstrates how a general stackedgp is built and the steps required to obtain the desired expectations algorithm 1 stackedgp model building and uncertainty propagation image 2 one limitation of the model is related to the matrix inversion required by the standard gp model which takes o n 3 operations where n is the number of training data points for a particular node several approaches have been proposed to deal with the curse of dimensionality kernel mixing higdon 1998 sparse gp with pseudo inputs snelson and ghahramani 2006 incremental local gaussian regression meier et al 2014 and inversion free approaches anitescu et al 2017 when the output of various layers is high dimensional then dimensionality reduction techniques can be added to pre process the training data higdon et al 2008 also various operations in algorithm 4 are easily parallelizable namely the optimization for hyperparameter estimation of each node can be carried out in parallel as well as within layer propagation of information from the previous layer obviously this computational efficiency over multi output methods comes at a cost of properly accommodating for the correlation of the outputs 6 numerical results in this section we provide three different examples to demonstrate the applicability of stackedgp the first application corresponds to the jura geological dataset where the stackedgp is used to enhance the prediction of a primary response using intermediate predictions of secondary responses in the second example we use stackedgp to combine two real datasets to predict the burned area as part of a forest fire application finally we demonstrate the use of stackedgp in the context of emulated dynamical systems for 2d puff advection driven by uncertain inputs for multi step predictions 6 1 cascading predictions jura dataset in this subsection we use jura dataset collected by the swiss federal institute of technology at lauasanne atteia et al 1994 webster et al 1994 the dataset contains concentration samples of several heavy metals at 359 different locations similar to previous experiments goovaerts 1997 alvarez and lawrence 2011a wilson et al 2012 we are interested in predicting cadmium concentrations the primary response at 100 locations given 259 training measurement points the training data contains location information and concentrations of various metals cd zn ni cr co pb and cu at the sampled sites the primary response is the concentration of cd and the other metals are considered secondary responses note that standard gaussian processes model each response variable independently and thus knowledge of secondary responses cannot help in predicting the primary one teh et al 2005 in this case a standard gaussian process standardgp will use a training dataset with only locations as inputs and cd measurements as target alvarez and lawrence 2011a wilson et al 2012 multi output regression models such as co kriging cressie 1992 can use the correlation between secondary and primary response to improve the prediction of cd the stackedgp while it does not model the correlation between primary and secondary responses it can be used to enhance the prediction of the primary response using intermediate predictions of the secondary responses in the heterotopic case goovaerts 1997 the primary target is undersampled relative to the secondary variables this provides access to secondary information such as ni and zn at 100 locations being estimated as a result a standard gaussian process can be built to have ni and zn directly as inputs here we will denote it as standardgp zn ni this is also the case for comparing our results with other six multi task regression models as reported by wilson et al 2012 and tabulated in table 1 wilson et al 2012 developed a gaussian process regression network gprn to model the correlations between multiple outputs such as primary and secondary responses the outputs are given by weighted linear combinations of latent functions where gp priors are defined over the weights unlike similar studies for semiparametric latent factor model slfm teh et al 2005 boyle and frean 2005 where the weights are considered fixed as these models have no analytical solutions to learn its hyper parameters the authors use different approximation methods such as variational bayes vb fox and roberts 2012 the slfm has been motivated from intrinsic coregionalization model icm goovaerts 1997 in geostatistics however unlike icm the slfm includes gaussian process hyper parameters such as length scales during the learning process in additions convolution gp model for multiple outputs cmogp is another regression model where each output at each x x is a mixture of latent gaussian processes mixed across the whole input domain x stackedgp is not designed to capture the correlations of response variables however stackedgp models can be constructed by stacking gps for predicting intermediate secondary responses that govern the input space of gps used to predict primary responses this hierarchical framework outperforms other methods as shown in table 1 the first proposed stackedgp uses the first layer to model zn and ni based on locations and the second layer to model cd based on the locations and the estimated output of the first layer see fig 5 in the heterotopic case the stackedgp can use directly the available measurements of ni and zn instead of predictions by setting the uncertainty associated with these measurements to zero in this case the stackedgp acts as the standardgp zn ni three other structures are proposed by using intermediate predictions of co cr and co and cr together 3 3 interactive python for all stackedgp structures for jura dataset can be found on https bitbucket org uqlab stackedgp src master cadmium prediction demo in this case we have a three layer stackedgp to model cd see fig 6 the first layer is the same as in the previous setup the second layer models intermediate responses co cr and co and cr the third layer is used to model cd based on the second layer predictions in additions to the input output of the first layer namely location and zn and ni fig 6 also shows the predicted spatial fields for different metals the predicted mean concentration of each metal is depicted as a heat map where x axis and y axis represent latitude and longitude respectively table 1 shows the results of these stacked structures stackedgp co stackedgp cr and stackedgp co cr while measurements of ni and zn are available in the testing scenarios there are no measurements for co and cr during testing thus cd predictions of these three stackedgps rely on predictions of co and cr using locations and ni and zn measurements at these locations the mean absolute error mae between the true and estimated cd is calculated at the 100 target locations the experimental setup follows alvarez and lawrence 2011a and wilson et al 2012 for which the simulation is restarted 10 times using different initializations of the parameters namely the length scale for the rbf kernel in case of the stackedgp the average and standard deviation of mae over these 10 runs is reported in table 1 overall stackedgp gives better results as compared with the other models also when zn and ni measurements are available as assumed by the other multi output regression models wilson et al 2012 alvarez and lawrence 2011a then a standardgp ni zn can provide a lower mae than the other six multi output regression models however stackedgp can provide a better performance over the standard zn ni by making use of intermediate predictions of secondary responses for all these experiments we found that the log transformation and normalization can lead to better results for multi responses in the middle layer we used independent component analysis ica to obtain independent projections of secondary responses this is required as the current derivation assumes that inputs to a gp node are independent the complexity of most of multi task models e g cmogp slfm is o n 3 p 3 where n is size of the training dataset and p is the number of output responses alvarez and lawrence 2011b wilson et al 2012 as gprn depends on approximation methods such as variational bayes it needs several iterations to reach suitable hyper parameters a larger the number of iterations increases the time complexity of the model therefore it may achieve lower complexity such as o p n 3 at the cost of obtaining a lower accuracy by decreasing the number of iterations stackedgp scales linearly with the number of nodes in the structure because of the independent training of the nodes which can be done in parallel in the worst case stackedgp is o p n 3 nonetheless sparse approximation techniques can be used to further reduce this complexity in the case of large training datasets snelson 2007 damianou et al 2011 furthermore standardgp co kriging and icm have o n 3 complexity but they achieve a lower accuracy as compared with the other mulit task models 6 2 model composition forest fire dataset the prediction of the burned area from forest fires has been discussed in different studies such as cortez and morais 2007 and castelli et al 2015 the burned area of forest fires has been predicted using meteorological conditions e g temperature wind and or several canadian forest fire weather indices taylor and alexander 2006 for rating fire danger namely fine fuel moisture code ffmc duff moisture code dmc drought code dc initial spread index isi and buildup index bui as shown in fig 7 in this application we are interested in developing a stackedgp 4 4 interactive python for stackedgp structure for forest fire dataset can be found at https bitbucket org uqlab stackedgp src master forestfire by first modeling the fire indices using meteorological variables t from one dataset presented in van wagner et al 1985 and then model the burned area based on fire indices using another dataset presented in cortez and morais 2007 the proposed stackedgp is depicted in fig 8 the gp nodes corresponding to the four fire indices ffmc dmc dc and isi are trained from data published in van wagner et al 1985 according to the hierarchical structure shown in fig 7 while the second dataset cortez and morais 2007 contains meteorological conditions along with the fire indices and burned area we assume that the meteorological conditions are missing in the training phase from this dataset and use only the fire indices and burned area data to train the gp node in the last layer of the stackedgp a 10 fold cross validation is applied to the dataset published by cortez and morais 2007 to train the burned area node and test the whole stackedgp model because of the skewed distribution of the burned area values and to ensure positive value for our predictions instead of directly modeling the burned area using stackedgp we have modeled the log of the burned area as a result the final mean and variance of the burned area b t as a function of the meteorological conditions t is given by eqs 39 and 40 respectively in additions we have found that scaling the target variable to have zero mean and unit variance to be a beneficial preprocessing step 39 e b e σ ln b 2 1 e 2 μ ln b σ ln b 2 40 v a r b e μ ln b 0 5 σ ln b 2 here μ ln b and σ ln b are the output of the probabilistic analytical stackedgp eqs 22 and 24 in the case of the rbf kernel see section 6 1 1 the result of modeling the burned area using the stackedgp is shown in table 2 the stackedgp model is compared with the results of 5 other regression models reported by cortez and morais 2007 because these regression models have been tested using different input spaces table 2 tabulates the best results achieved by each model as described in cortez and morais 2007 even though the stackedgp predicts the burned area based on estimated indices from the first dataset and not the actual values as presented in the second dataset it is still able to give comparable results with the other models that make use of meteorological conditions and or fire indices available in the second dataset this experiment emphasizes that the stackedgp is able to combine knowledge from multiple datasets with noticeable performance 6 3 uncertainty propagation atmospheric transport gaussian processes with uncertain inputs have been previously used in multi step time series predictions girard et al 2003 candela et al 2003 modeling multi step ahead predictions can be achieved by feeding back the predicted mean and variance at each time and propagating the uncertainty to the next time step this idea has been used in different time series applications such as electricity forecasting lourenço and santos 2010 and water demand forecasting wang et al 2014 here we expand this concept by further driving the dynamical system using another gp for propagating uncertainty in an atmospheric transport problem we consider a simple advection of a 2d gaussian shaped puff nielsen et al 1999 terejanu et al 2007 the states of the puff evolve using the following equations 41 x k 1 x k u x x k δ t 42 y k 1 y k u y y k δ t 43 d k 1 d k u x 2 x k u y 2 y k δ t here x k y k is the position of the center of the puff and the downwind distance from the source d k is used to compute the puff radius σ k p d k q in models such as rimpuff nielsen et al 1999 based on karlsruhe jülich diffusion coefficients reddy et al 2006 p q the goal here is to build a gp emulator for the above dynamical system knowing that the release location is fixed at x 0 0 km y 0 0 km and the wind velocity is uncertain with normally distributed wind components u x u y 44 u x u y n 4 m s 1 m s the gp emulator h is constructed using 15 training trajectories that start at the same release location but correspond to different wind fields that randomly sampled from the distribution in eq 44 the total simulation time is 30min with a time step δ t 90 sec as a result k has range of 20 steps during the simulation time 45 x k 1 y k 1 d k 1 h x k y k u x x k u y x k another gp model is constructed to determine the wind field based on 16 wind sensors positioned 4 km apart in both directions the wind sensor readings are just independent and identically distributed samples from eq 44 46 u x x u y y g x y note that in this particular case the wind velocity at different locations is correlated both emulators use rbf kernels and they are stacked to build a recurrent stackedgp as shown in fig 9 to assess the effect of the two assumptions in constructing the stackedgp 5 5 interactive python for stackedgp structure for atmospheric transport experiment can be found on https bitbucket org uqlab stackedgp src master uncertainty propagation atmospheric transport independent inputs for each layer and gaussian distribution approximation for the output of each layer we compared the approximate mean and variance of the puff states from stackedgp using the proposed algorithm with those resulted from a monte carlo propagation of uncertainty through the stackedgp using 1000 samples fig 10 shows the approximate predicted gaussian distribution of the states along with the histogram of the monte carlo samples propagated through the stackedgp table 3 lists the predicted mean and standard deviation of the puff states at different time steps note that even though the state equations for the location of the puff are linear because they are emulated using a gp which at its turn is driven by a gp model for the wind field the distribution of the stackedgp output may depart from the gaussian distribution the assumption of approximating the output with a gaussian distribution may result in biasing the mean location the statistical significant difference between the stackedgp approximate mean propagation and its monte carlo estimate confirms the impact of this approximation as shown in table 3 furthermore the assumption of ignoring the correlation structure between the outputs of stackedgp may result in an artificial inflation of the uncertainty in our simple example this is clearly manifested in larger standard deviations for the downwind using approximate propagation as compared with the monte carlo estimate this impact on uncertainty propagation might be exacerbated when more nonlinear models are used which limits the horizon of uncertainty propagation obviously the gain in computational speed combined with field measurements in the context of data assimilation may position these stacked model as real contenders for real time applications we plan to investigate in the future the application of stackedgp to data assimilation 7 conclusions a stacked model of independently trained gaussian processes called stackedgp is proposed as a modeling framework in the context of model composition this is especially of interest in environmental modeling where e g model composition is used to generate large scale predictions by combining geographical interpolation models with phenomenological models developed in the lab an approximate approach is developed to obtain estimates of the quantities of interest with quantified uncertainties this leverages the analytical moments of a gaussian process with uncertain inputs when squared exponential and polynomial kernels are used the stackedgp can be extended to any number of nodes and layers and has no restriction in selecting a suitable kernel for the input nodes the numerical results show the utility of using stackedgp to learn from multiple datasets and propagate the uncertainty to quantities of interest while it is not specifically designed to model correlations between secondary and primary responses stackedgp can be used to enhance the prediction of primary responses by creating an intermediate layer of predictions of secondary responses this comes with a lower computational complexity as compared with multi output methods and can make use of off the shelves gaussian processes while in the current paper we assume that outputs of intermediate layers are independent and resolve this using independent component analysis preprocessing we plan to extend our derivation to account for these correlations in the next study this will allow multi output models to act as nodes in the proposed stackedgp along with the independence assumption the other drawback of the proposed uncertainty propagation algorithm is the gaussian assumption of the predictive distribution while this is motivated using maximum entropy principle in a multi step prediction setting it overestimates the predicted uncertainty acknowledgments this material is based upon work supported by the national science foundation under grand no 1504728 and 1632824 dr terejanu has been supported by the national institute of food and agriculture nifa usda under grand no 2017 67017 26167 appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 022 
