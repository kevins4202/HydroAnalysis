index,text
25965,environmental decision support tools often fail to be effectively applied in society because tools are use inspired rather than user ready stakeholder engagement by researchers is frequently the missing critical component here university researchers collaborated with the bangladesh water development board bwdb in developing a cloud based satellite remote sensing tool for monitoring suspended sediment concentrations ssc in major bangladesh rivers bangladesh faces complex river erosion and sediment management challenges the tool helps to overcome bwdb s constraints of limited resources and ground based monitoring capacity the tool maps estimated ssc over satellite images and provides long term estimated ssc time series at user designated points the key elements of success in the engagement process were the partnership and trust between the researchers and stakeholder stakeholder leadership and ownership diversity of stakeholder roles engaged researcher s understanding of the stakeholder problem and context and open source easy to use tool weaknesses in the engagement process can be improved on in future work keywords stakeholder engagement capacity building satellite remote sensing suspended sediment 1 introduction environmental researchers frequently develop use inspired tools to improve monitoring modeling and understanding of earth systems however these research tools are not always user ready or directly integrated into societal application and decision making hossain 2015 hossain et al 2014c irwin et al 2018 mcintosh et al 2011 prados et al 2019 this situation is analogous to two standalone mountains one symbolizing research and the other symbolizing society in between the mountains is a desolate valley of death national research council 2000 as tools from research often perish before making it to society for uptake the crux is that research tools cannot cross the valley and go from use inspired to user ready on their own researchers must take their tools descend from their mountain and journey across the valley with stakeholders although stakeholder engagement is critical for environmental decision support tools to become operational in society it is rarely made a priority for researchers stakeholder engagement is typically seen as being time consuming expensive complicated risky and unworthy of high impact scientific journal publication glicken 2000 environmental researchers that do aim to conduct stakeholder engagement may struggle with developing a sound approach and tangible goals stakeholder engagement entails the integration of technical and social processes korfmacher 2001 which environmental researchers are rarely trained to do while stakeholder engagement is not new e g baroudi et al 1986 king and rodriguez 1981 there is no consensus in environmental literature about best practices sandink et al 2016 examples in literature have varying project contexts and constraints and therefore standards for success are highly variable despite these challenges there are valuable recommendations and criteria for stakeholder engagement in literature for example best practice recommendations for successful use of environmental decision support systems were given by mcintosh et al 2011 practices were grouped into five categories designing for ease of use designing for usefulness establishing trust and credibility promoting the decision support system for acceptance and starting small and simple eleven criteria for success in stakeholder engagement were developed by sandink et al 2016 based on those commonly found in literature these included clear problem has been defined with the assistance of stakeholders know the stakeholder group stakeholders involved early and in as many development phases as possible gain trust establish neutrality and iterative processes applied seven recommendations for crossing the valley of death were provided by hossain et al 2014 these included aim for full ownership of a system by stakeholders seek two way feedback and listen more than talk train stakeholder agency from the ground up through hands on tasks in a thorough and patient way insights on user requirements and stakeholder interaction have also been well explored in the context of participatory modeling e g voinov and bousquet 2010 voinov and brown 2008 along with the recommendations in literature increasing environmental data availability and technological capacity provide immense opportunities to expand stakeholder engagement in developing environmental decision support tools two specific examples of advancements are satellite remote sensing and cloud computing earth observing satellites are rapidly increasing the volume and types of environmental data over 500 earth observing satellites have been launched over the past 50 years and over 150 more are planned to launch over the next 12 years ceos 2020 guo 2017 yao et al 2020 earth observing satellites provide a global vantage and repeatable objective measurements united nations office for outer space affairs 2018 most satellite data are freely and publicly available but there are barriers to effectively using these data such as their massive storage size high variability and high complexity however expansion of cloud storage and computing capabilities are lowering these barriers for example google earth engine gee is a fully cloud based high performance computing platform that stores multi petabytes of pre processed satellite imagery gorelick et al 2017 gee has web based javascript api and integrated development environment in which developers can rapidly implement algorithms and visualize outputs on an interactive map developers can also create apps with graphical user interfaces that allow non programmers to interact with maps and algorithm outputs gee and its datasets are freely available for non commercial use the advances of satellite remote sensing and cloud computing have immense potential to support environmental management in economically developing regions these regions typically have limitations in their in situ environmental data quality and quantity local computing bandwidth power supply and internet reliability satellite remote sensing supplements in situ data and cloud computing lessens dependence on local computer resources however for effective technology transfer and local capacity building in developing regions there must also be proper education training and co development of decision support tools while there are researchers and institutions forging these paths e g biswas and hossain 2018 hossain et al 2014c prados et al 2019 significant needs remain considering that economically developing regions are generally most vulnerable to the increasing pressures of environmental change it is urgent that technologies are effectively implemented to support their adaptation resilience and capacity independent group of scientists appointed by the secretary general 2019 globally the new normal continues to evolve with interconnected environmental health economic and social challenges at the same time environmental data and technological capabilities are rapidly expanding and providing immense opportunities for innovative and scalable decision support tools for the tools to directly serve society the scientific research community must intentionally evolve rethink priorities and enter into new territories of stakeholder engagement in doing so it is imperative that institutions and funding agencies support these efforts and equipped the next generation of environmental researchers for this paradigm shift brown et al 2015 irwin et al 2018 lettenmaier 2008 national research council 2014 examples of environmental researchers conducting convergence and user ready research in partnership with stakeholders can help to guide these efforts this paper describes the stakeholder engagement process in developing a cloud based satellite remote sensing tool for monitoring suspended sediment concentrations ssc in major rivers of bangladesh this tool called bross bangladesh remote sensing of suspended sediments was co developed by researchers from the university of washington in usa and staff from a stakeholder agency the bangladesh water development board bwdb the tool development and stakeholder engagement process were conducted over one year the guiding research question of this paper is what were the critical elements to a successful stakeholder engagement process in co developing bross the objectives of this paper are to 1 demonstrate a generalizable path by which researchers can conduct productive stakeholder engagement in a relatively short time frame one year 2 highlight the key strengths and weaknesses of the stakeholder engagement process and 3 provide socially relevant research directions for satellite remote sensing of river sediment and related software while various publications highlight strengths and lessons learned in stakeholder engagement processes this results in an extensive list for researchers which may distract their focus from the most important elements this study is unique in that it distills critical steps strengths and lessons learned and can help researchers and stakeholders in prioritizing the elements of their engagement approach also while stakeholder engagement has largely been explored in the context of participatory hydrologic modeling this study focuses on a far less studied topic of satellite based monitoring of sediment thus socially relevant research directions for the scientific community are unique and have the potential to lead to more widespread use of a technology that is underutilized in societal decision making in the text that follows section 2 provides the background on the study area stakeholder and problem section 3 contains the bross tool purpose and description section 4 describes the methods used in the stakeholder engagement process section 5 provides the results discussion and lessons learned on the key strengths and weaknesses of the process section 6 concludes the paper 2 background on study area stakeholder agency and problem 2 1 bangladesh hydrology geomorphology and sediment dynamics the country of bangladesh fig 1 lies in the active delta of three of the world s largest rivers the ganges brahmaputra and meghna in bangladesh the brahmaputra is known as the jamuna river downstream of the jamuna and ganges rivers confluence is the padma river the meghna river is divided between the upper meghna river upstream of its confluence with the padma river and the lower meghna river downstream of its confluence with the padma river bangladesh also has hundreds of smaller rivers throughout the landscape bangladesh s major rivers originate in the himalayan and east indian mountains flowing through various countries before reaching bangladesh and discharging into the bay of bengal average annual streamflow discharge from the three major rivers is 1 35 1012 trillion m3 year small et al 2009 average annual sediment discharge estimates range from 500 to 2400 mt per year and are dominated 98 by sediment loads from the padma and jamuna rivers rahman et al 2018 the majority 80 of streamflow occurs during the summer june to september due to the combined impacts of seasonal monsoons and snowmelt from upstream mountains small et al 2009 subramanian and ramanathan 1996 bangladesh s major rivers are highly prone to erosion deposition channel shifting and char riverine islands and bars development this is largely due to the extremely high rate and fluctuation of streamflow and sediment transport particularly in the summer monsoon season sarker et al 2011 the riverbanks and channel beds primarily consisting of fine sand and silty sediments with occasional clay have little resistance against erosive forces billah 2018 mclean et al 2012 sarker et al 2011 2003 the rivers are mostly flat 2 7 5 cm km sarker et al 2003 making erosion accretion and shifting patterns widespread and difficult to predict the jamuna river is an intensely braided river that erodes thousands of hectares of mainland floodplain each year in recent years its braiding intensity and width have increased sarker et al 2014 the ganges river is predominantly a meandering planform with some braided reaches the padma and lower meghna rivers vary spatially and temporally between single thread meandering and multi channel braiding patterns sarker et al 2003 shajahan and reja 2012 length average bank erosion rates are higher in the jamuna padma and lower meghna rivers compared to the ganges and upper meghna rivers sarker et al 2003 however compared to the jamuna and meghna rivers ssc is highest in the ganges storm surges and cyclones also cause severe erosion deposition and channel shifting primarily amongst the lower meghna river 2 2 societal impacts of river sediment dynamics bangladesh is a developing emerging country international monetary fund 2019 its population is 160 million people and its population density of 1240 people per square km of land is one of the highest in the world food and agriculture organization and world bank 2018 millions of people in bangladesh live alongside rivers or on chars throughout the river system and many livelihoods are directly tied to natural resources e g agriculture fisheries sarker et al 2003 shajahan and reja 2012 riverbank erosion frequently degrades and destroys housing farmland property and public infrastructure each year erosion leaves thousands of people landless and homeless and millions negatively impacted in other ways islam and rashid 2011 accretion along the banks and channel beds also inhibits ship navigation hence river erosion accretion is directly and heavily integrated into the society and economy of bangladesh 2 3 stakeholder agency mission and needs the stakeholder agency bwdb is a federal government entity focused on water resources engineering and management environmental sustainability and improvement of livelihoods throughout the country the agency has major long standing efforts to monitor manage and mitigate the impacts of riverbank erosion and accretion however the vastness and complexity of the river system as well as limited resources available to the agency are major constraints the stakeholder agency s existing in situ suspended sediment monitoring capacity is not adequate for the needed understanding and prediction of river system sediment dynamics the agency has 25 in situ sediment monitoring points rahman et al 2018 but only four are located along the major rivers fig 1 the agency has conducted in situ suspended sediment monitoring throughout the river system since the 1960s however sediment data have not been collected continuously and data from some time periods and locations is less reliable rahman et al 2018 a large reason for these issues is that in situ sediment monitoring is time consuming complicated and expensive and the agency has not had the resources to bolster their sediment monitoring approach in situ sediment monitoring can also be difficult and dangerous during the monsoon season when erosion accretion is prominent and sediment loads are highest riverbank protection has been implemented along the major rivers since the early 1970 s sarker et al 2011 however there are substantial lengths of non or poorly protected vulnerable riverbanks where people live and farm the stakeholder agency continuously installs permanent riverbank protection which is mostly revetments layered with concrete blocks and geotextile bags filled with sand however along with limited resources there is also the constraint of time between monsoon seasons to implement permanent protection measures during the monsoon season the agency also rapidly responds to emergency erosion events by implementing temporary protection which is typically geotextile bags filled with sand since erosion points are difficult to predict it is challenging for the agency to prioritize and prepare for both permanent and temporary protection needs in addition the agency conducts river dredging to manage erosion and accretion however the submerged locations of sediment deposition are difficult to determine the costs for these various measures are extremely high the stakeholder agency is interested in improving their prediction of erosion and accretion hotspots so that they may more effectively allocate limited resources to engineering and mitigation measures and ultimately protect human livelihoods areas of erosion accretion frequently migrate throughout the vast complex rivers and are difficult to strategically monitor with the agency s sparse in situ sediment monitoring capacity the ability to monitor sediment dynamics throughout the entire river system particularly using satellite remote sensing can help to inform the agency where potential erosion accretion points may be and where further on the ground investigation is warranted 3 bross tool purpose and description 3 1 bross tool purpose bross estimates ssc throughout major bangladesh rivers using satellite remote sensing surface reflectance data the agency primarily plans to use bross to improve understanding of where migrating points of significant erosion accretion are occurring in addition bross is intended to help improve mandated forecasting of sediment loads during monsoon seasons and to help improve the understanding of the horizontal and vertical profiles of river sediment the purpose and plans for bross were directly defined by the stakeholder agency bwdb in the early stages of the tool development 3 2 bross tool description the bross domain is within bangladesh along the main stems of the jamuna ganges padma and lower meghna rivers fig 1 ssc predictions are of two forms time series of ssc averaged over a region of interest roi within the bross domain and ssc mapped throughout the domain for a single satellite image bross uses visible red green blue and near infrared nir band data from the landsat sentinel 2 and modis satellite series table 1 the different spatial resolutions temporal resolutions and durations of these satellites have trade offs in decision making landsat and sentinel 2 data have relatively high spatial resolution and can distinguish ssc patterns along river cross sections and banks however the temporal resolutions are relatively low and can miss critical hydrologic events modis data have high temporal resolution however the spatial resolution is relatively poor and only appropriate for reach scale ssc averages of wide 500m rivers while sentinel 2 data have relatively high spatial and temporal resolution the short historical records inhibit strong calibration for predicting ssc models for predicting ssc from satellite data were calibrated using in situ suspended sediment data collected by the stakeholder agency bwdb from 1968 to 2019 at the four monitoring points along the jamuna ganges and padma rivers fig 1 bross integrates two modeling approaches to predict ssc from satellite data regression and artificial neural networks ann regression is a simple widespread approach that is relatively simple to implement e g beveridge et al 2020 markert et al 2018 for bross an exponential model is used to relate ssc to a ratio of satellite band data e g ssc a exp b red green c where values for a b and c are determined by regression ann uses pattern recognition to capture both linear and non linear relationships peterson et al 2018 sudheer et al 2006 ann is relatively complex and challenging to implement therefore is less commonly used for remote sensing of river sediment however in this study and others e g peterson et al 2018 ann overall has superior performance to regression table 2 bross uses ann model inputs of single bands band combinations month of the sample collection and satellite identification together regression and ann give a plausible range of ssc which is valuable for decision making under uncertainty bross has two operational platforms which are both easily accessible for users via a web browser one platform is gee https earthengine google com section 1 gorelick et al 2017 where bross is implemented using the gee javascript and user interface apis the second platform operates via a local desktop server and google cloud and is hereinafter called the local server platform the local server platform back end uses scripts written in the linux operating system using python scripting language the front end development uses all open source tools and software following ahmad and hossain 2019a and biswas and hossain 2018 briefly this entails local web server environment setup using xampp cross platform x apache a mariadb m php p and perl p https www apachefriends org index html map visualization using the javascript library called leaflet https leafletjs com chart production using the javascript library called highcharts http www highcharts com ssc outputs processed to the geotiff format for rendering on the webpage and exporting using geospatial data abstraction library https www gdal org dynamic and interactive user interface development using the styling and event handling libraries of materialize https materializecss com and jquery https jquery com both platforms produce long term time series of estimated ssc for user designated regions based on satellite data fig 2 a and b fig 3a and b the time series can be downloaded both platforms also produce colored maps of the estimated ssc distribution in the bross domain fig 1 for individual satellite images fig 2a and 3a users can click on individual pixels of mapped ssc to obtain the value when generating ssc time series and maps both platforms mask out pixels of the land surface clouds and cloud shadows so that only water surface pixels are included while these are the commonalities of the platforms there are notable differences in the key features table 3 interfaces fig 2a and 3a and workflows 2b and 3b of the two platforms for example the local server platform includes both the ann and regression models whereas gee only includes the regression model also gee allows users to sketch the region of interest for the ssc time series whereas the local server only generates time series at predefined locations the stakeholder can add predefined locations as desired the bross gee tool is at https bross users earthengine app view gee the bross local server tool is at http depts washington edu saswe bross further technical details about bross are in the technical manual and further details about the user interfaces and functionality are in the two user manuals all manuals and codes for both bross tools are found on github at https github com cbev bross 4 stakeholder agency engagement methods the approach to co developing bross followed strategies commonly applied and recommended in conducting convergence research and stakeholder engagement national research council 2014 sandink et al 2016 voinov and bousquet 2010 the generalized steps and timeline are adaptable to other researcher stakeholder partnerships locations and environmental challenges fig 4 for example the steps can also be applied for researchers and stakeholders in the same country region and social cultural context throughout sections 4 1 to 4 6 figures depict generalized sub steps while the text describes the bross co development process as a case study of how these steps may be carried out by early career researchers hereinafter the following terminology is used to describe the primary roles in the co development process developer is the primary university researcher engaging in the co development process researchers or research group is the university research group that the developer is part of stakeholder agency or agency for short is bwdb leading agency staff is three staff from bwdb that have executive decision making powers in the agency and play a leading role in the co development process agency sediment expert is one of the leading agency staff who is an expert on numerical modeling of sediment transport and riverbank erosion 4 1 step 1 establish partnership and become familiar with stakeholder agency the research group focuses on conducting user ready research promoting access to water resources information and improving livelihoods in challenging environments with sustainable satellite remote sensing and hydrologic modeling applications as part of this mission the research group has been collaborating with the stakeholder agency and other bangladesh water management institutions on satellite remote sensing applications since 2011 the research group has provided education and training on satellite remote sensing fig 5 part a co developed multiple tools to support agency operations and provided ongoing technical support hossain et al 2014c 2014b 2014a through this long standing partnership the research group has institutional understanding of the stakeholder agency s context challenges capacity and priorities fig 5 part b from a technical standpoint this includes understanding of bangladesh s hydrologic system where and what forms of hydrologic data the agency collects and how the agency operates between office and field operations from a social standpoint the research group is also familiar with the culture of the agency and broader geographical region the agency s management structure and the agency s resource constraints having this broad technical and social understanding is important before narrowing to a specific project topic the partnership was initially established by the research group leader contacting stakeholder agencies expressing interest in collaborating and focusing on building relationships with multiple members of the agencies through these relationships there was mutual learning about how the researchers and agencies could work together and where initial education and training was needed researchers that do not have the privilege of a preexisting relationship with a stakeholder agency may need to initiate a partnership building process they can do this by similarly networking with stakeholders and first focusing on robust relationship building voinov and bousquet 2010 the time and effort to establish partnerships is situational and depends on multiple diverse factors such as the familiarity of the agency with research approaches the establishment of the partnership can be integrated with the remaining steps 4 2 step 2 meet with stakeholder agency to match agency priorities to researcher expertise the co development of bross was initiated by the agency during an in person meeting between the researchers and stakeholder agency to discuss general collaboration opportunities for decision support tools the steps for matching agency priorities to researcher expertise were carried out in this meeting the researchers organized this meeting and hosted the three leading agency staff at the researchers university for two days the researchers presented their focus areas and related decision support tools as examples of what could be implemented for the agency fig 6 part a the developer discussed their work on satellite remote sensing for ssc estimation in another region beveridge et al 2020 the developer also demonstrated a web based tool for satellite monitoring of ssc that was implemented by another group of researchers and stakeholders markert et al 2018 the agency expressed interest in co developing a similar satellite based application for ssc to support their engineering and management priorities for river sediment erosion and accretion the primary interest of the leading agency staff was for the ssc distribution of major rivers to be qualitatively mapped on individual satellite images in order to improve understanding of where migrating points of significant erosion accretion are occurring the agency invited the developer to conduct a follow up trip to bangladesh to facilitate the agency engagement process upon the agency s invitation the developer committed to the co developing bross and conducting the trip next the researchers and leading agency staff established the short mid and long term priorities and action items for the bross co development and agency uptake fig 6 part b these were apply local in situ suspended sediment data to calibrate and validate a ssc regression model from satellite data short to mid term priority develop a cloud computing based automated system for visualizing remotely sensed ssc of bangladesh major rivers using gee mid term priority assign a dedicated agency staff to master the remote sensing technique and carry out research to monitor riverbank erosion and accretion hot spots short to mid term priority in addition the developer met with the agency sediment expert who shared about the agency s in situ sediment monitoring approach and data availability fig 6 part c based on this information they planned the regression model calibration and bross prototype development the agency sediment expert agreed to obtain the relevant in situ sediment and streamflow data from the agency and provide them to the developer the developer and agency sediment expert also discussed the prototype design workflow etc based on the earlier meetings the workflow for bross on gee fig 2b was devised without consideration of ann with the understanding that it may change based on feedback from the agency during the follow up engagement trip during this meeting the agency sediment expert also expressed interest in using bross in conjunction with models to better understand the horizontal and vertical profile of ssc it is possible that there would not be as readily of a match between the stakeholder agency priorities and plausible tools presented by the researcher in that case it is still valuable for the researchers and agency to discuss the agency s general management priorities and expected outcomes for the short term and long term while there may not be an immediate need that the researcher can respond to it may be discovered that there is potential for tool co development in the future e g after a certain milestone is met similarly it will help for the agency to understand plausible tools that the researchers can develop as an unexpected need for a similar tool may arise in the future it is important that co developed tools are not forced or implemented prematurely this will likely detract from the effectiveness and long term sustainability of the tool and generally be a poor use of time and resources with a partnership in place step 1 there is more flexibility to be patient with opportunities and let them develop in appropriate time 4 3 step 3 develop prototype tool and prepare for follow up stakeholder agency engagement meeting event prior to the follow up agency engagement trip the developer prioritized calibrating models for predicting ssc from satellite reflectance data producing a prototype system for the agency s trial use and preparing for the trip the model calibration and prototype system production were primarily conducted by the developer fig 7 parts a b however the agency sediment expert was involved in reviewing intermittent results discussing issues and determining next steps throughout the process fig 7 part c the leading agency staff provided the developer with the in situ data and the developer downloaded relevant satellite data from gee initially models to predict ssc from satellite data were calibrated and tested using regression strong regression model results were difficult to obtain due to the high complexity of the river system and limitations of the satellite data and in situ data rahman et al 2018 see technical manual because of the regression model limitations the researchers proposed using ann models to predict ssc from satellite data to initially test the feasibility of ann model in the bross domain the researchers used the matlab neural network toolbox the mathworks inc natick massachusetts and training approach from ahmad and hossain 2019b after the ann results proved to be stronger than the regression results ann models were developed using the keras library in python the matlab toolbox is simpler to use however the ann models had to be programmed in python to be implemented in bross the bross prototype was developed in gee based on the design discussed by the researchers and leading agency staff step 2 the prototype had limited user interface capabilities compared to the proposed final version so that excessive time was not spent on functionalities that may not be used however the prototype design allowed users to envision the proposed bross workflow and outputs the prototype only used landsat 8 satellite data and the regression model to predict ssc the prototype design allowed users to select an in situ monitoring location review the regression predicted ssc time series for the full landsat 8 record and click on points of the time series to view the predicted ssc map for the specified date the researchers and leading agency staff scheduled the developer s follow up engagement trip to bangladesh for two weeks of january 2020 fig 7 part d they collaboratively established the trip objectives developed the itinerary and determined coverage of costs there was mutual understanding that the trip objectives were focused on agency engagement and would entail both office meetings and field site visits the leading agency staff directed the trip preparations by inviting meeting attendees and planning the field site travel logistics the agency planned to cover the expenses for room and board during the field site visits and the researchers planned to cover the airfare and remaining room and board costs during this preparation period the developer also prepared for the trip by studying the regional language culture and social customs part of this was by leveraging the research group s institutional knowledge about the region and the agency culture and operations 4 4 step 4 conduct follow up agency engagement meeting event to collect feedback on tool and validate societal impact the follow up stakeholder agency engagement trip primarily composed of meetings with agency staff and collaborators and visits to field sites where sediment challenges are prominent fig 8 the leading agency staff organized all activities and involved a diversity of participants table 4 the formal presentation at the agency headquarters in dhaka fig 1 was the first major activity of the trip fig 8 part a during the formal meeting the developer first gave a presentation that encompassed the following objectives of the agency engagement process objectives of the bross tool brief technical background on satellite remote sensing of ssc including what the primary limitations are preliminary regression and ann model calibration results demonstration of the bross prototype operation and description of additional features proposed for the final bross design then the meeting participants asked questions and provided feedback section 5 1 following the agency s typical standard protocol meeting participants were provided with a printed memo at the beginning of the meeting which summarized the presentation the memo also contained questions for the participants that could be responded to during the meeting and or after the meeting in an optional online survey the questions were about the value of bross in addressing agency needs points of confusion about bross and how bross could be improved the memo and survey questions helped to focus the meeting objectives and discussion it also allowed meeting participants multiple opportunities to provide questions and feedback following the formal presentation the developer led an informal hands on session on operating bross fig 8 part b there was a step by step demonstration of how to operate bross and the potential additional functionalities that could be integrated the participants were asked to suggest changes in the bross workflow and design and share whether the potential additional functionalities would be confusing or complicated for operators while the formal meeting and hands on training session were planned before the trip multiple unplanned on demand meetings arose after the formal meeting with attendees interested in further discussion fig 8 part c two additional meetings were held with executive engineers of the flood forecasting department the engineers posed an additional bross objective of supporting mandated forecasting of flood suspended sediment loads one meeting was held with a local university research group which was conducting synergistic ground based and remote monitoring of river erosion rahman et al 2019 one meeting was held with the staff from a local nonprofit research institution which was conducting satellite based monitoring of river geomorphology as a result of these meetings there were potential additional uses of bross additional feedback on the bross design and additional technical information shared with the developer to support the bross performance the developer also spent five days visiting two large project field sites where there are major engineering and management challenges that motivate the agency s interest in bross fig 8 part d one site was sirajganj two days and the other was the coastal polder area three days fig 1 sirajganj is along the jamuna river and is one of the most vulnerable riverbank erosion areas in bangladesh the coastal polder area is along the lower meghna river and the coast of the bay of bengal the area is subject to severe riverbank erosion storm surge and cyclones both areas are considered high risk because of the density of human communities living near water bodies that frequently suffer loss and damage of land and property at the field sites that were visited the agency conducts ongoing engineering measures such as riverbank protection and dyke levee installation the field site visits were guided by agency staff with extensive site knowledge and experience the agency staff demonstrated the on the ground approaches constraints challenges and needs related to sediment erosion and accretion the agency staff validated that the broad spatial and temporal perspective of bross would help to direct monitor and evaluate their field efforts the developer and agency staff also visited the local communities surrounding the field sites the community members and agency staff validated the potential societal impact of bross by sharing about how the communities are impacted by sediment related disasters how current engineering measures are improving livelihoods and what issues remain 4 5 step 5 update and deliver tool iterating as needed after completing the follow up stakeholder agency engagement trip the developer updated bross based on agency feedback the calibration of regression and ann models were enhanced by incorporating the technical information and agency feedback gathered during the follow up engagement trip fig 9 part a the bross tool back end and front end updates fig 9 part b and writing of the user and technical manuals fig 9 part c were done with careful consideration of the agency needs and concerns as well as the technical expertise of the agency s bross operators in april 2020 the bross tool manuals and codes were released to the leading agency staff fig 9 part d in the email delivery the developer provided a summary of the tool updates and deliverables along with links to the bross platforms and manuals which were publicly available online the developer encouraged bross to be tested and disseminated in the agency as appropriate and for agency staff to contact the developer with questions concerns and needs one week after the release the agency sediment expert organized a call with the developer and research group leader to share feedback ask questions and discuss next steps section 5 1 the expert s questions were largely technical and follow up action items were strategically delegated between the researchers and agency the developer and expert carried out follow up action items as appropriate additional time beyond one week may be needed for stakeholders to fully assess issues and multiple iterations of meetings and subsequent tool improvements may be needed over extended periods of time 4 6 step 6 provide ongoing complimentary technical support moving forward the stakeholder agency is the primary owner of bross and responsible for leading its ongoing use and improvement since the deployment of bross the agency has been working to integrate bross into their sediment monitoring engineering and management approaches recommended future improvements for bross were included in the technical manual and were largely based on agency concerns and feedback collected throughout the engagement process users that have questions or find issues with bross can fill out an online form which is linked to in the tools and user manuals the researchers maintain the responses to the form and will transition this responsibility to the agency when appropriate the researchers are available for ongoing technical support and intend to fix issues and answer questions as they arise major updates to bross may be conducted by the researchers in the future at the request of the agency the agency also has potential to engage with other institutions that may use and or expand on bross 5 results discussion and lessons learned 5 1 stakeholder agency feedback the stakeholder agency feedback on bross can be used by the broader research community to help guide future research directions in satellite remote sensing river sediment dynamics and stakeholder engagement overall the agency staff had constructive feedback and clearly grasped bross s purpose functionality underlying theory and limitations the agency also strongly took initiative in having full ownership of bross s future application and development this demonstrated effective and sustainable capacity building the majority of stakeholder feedback was provided during the formal meeting on the follow up agency engagement trip section 4 4 of the thirty one meeting participants eight provided verbal feedback at the meeting and one provided written responses however none responded to the optional online survey the verbal feedback at the meeting was generally from upper level management agency staff and experts from external institutions the questions concerns and comments about bross were broadly on the science and technology application to engineering and management and ongoing research and development table 5 the concerns about the science and technology were largely inherent issues of satellite remote sensing in general biswas and hossain 2018 and in particular for sediment beveridge et al 2020 concerns about application to engineering and management were largely unknown in the present state of sediment research and management fu et al 2019 however the concerns could be addressed with ongoing research and application from the broader researcher community and from the agency for example bross outputs can be integrated with erosion prediction models developed for bangladesh rivers e g biswas et al 2016 through discussions with the developer the agency staff came to understand which questions and issues they posed were largely active research topics and beyond the present scope of bross the agency was interested in pursuing the unknowns in partnership with the researchers and local institutions the agency staff agreed with the proposed design and features e g including ann and multiple satellite options however if they had disagreed this formal meeting would have been the opportunity to revise the final bross workflow and design during the hands on training session with eight junior and mid level agency staff that would be directly involved in bross operations section 4 4 the proposed final bross design was accepted the staff found the graphical user interface to be easy to use the staff felt comfortable with the user customization options for the models satellites monitoring points and outputs however it was not ideal to have customization beyond this e g different options for regression equations most of the staff were not confident with updating the bross programming codes so staff familiar with computer programming would need to be dedicated to bross updates the staff concerns were similar to those discussed in the formal meeting table 5 but primarily focused on cloud cover impacts and satellite penetration depth they recognized that they needed to be aware of these limitations in using bross after bross was released for testing feedback from the agency sediment expert section 4 5 revealed opportunities to further improve the tool s uptake and sustainability table 6 notably the expert explained that quantification of the predicted annual sediment load volume would help decision makers to better understand the reliability of bross this task was delegated to the agency to facilitate technology transfer and stakeholder ownership similarly other follow up action items were appropriately delegated between the researchers and agency the agency sediment expert planned to involve junior staff that had computer programming expertise in the agency s follow up tasks and ongoing bross development and application 5 2 bross skill assessment the sustainability and stakeholder agency uptake of bross critically depend on the tool s skill overall the stakeholder agency found the performance of bross to meet expectations and show promise for supporting operations the skill of bross was largely enhanced by iterating the data analysis and model calibration with the agency sediment expert as well as by obtaining supporting technical information during the follow up engagement trip the familiarity of the agency staff and external institutions with the river system in situ data and pre existing agency satellite remote sensing applications helped to integrate local understanding and overcome technical challenges for example the agency staff shared which in situ data monitoring points had more reliable data and could be focused on in the model calibration the agency staff and experts also explained how and why sediment reflectance properties varied spatially and temporally throughout the river system various insights from the agency staff and external institutions could not have been obtained by the developer in isolation from the agency and external organizations the performance of bross varies for each satellite and model table 2 the annual sediment load predictions for the river system derived from bross table 7 are the same order of magnitude as most recently published estimate based on observations 470 mt year year 2015 it was also found that the load is decreasing 10 mt year rahman et al 2018 annual suspended load estimates were computed using monthly ssc predictions from bross and monthly streamflow discharge observations collected by the agency the agency understands bross is not currently reliable for precise estimates of ssc and sediment fluxes however bross does provide realistic outputs which meet the agency s interest in qualitative ssc distribution maps the results are also adequately skillful to supplement existing sediment monitoring and forecasting approaches 5 3 lessons learned strengths and weaknesses of stakeholder engagement process the constructive stakeholder agency feedback section 5 1 and the skill of bross section 5 2 are two standards by which the stakeholder engagement process and user ready outcome of bross are considered successful however there are also five overarching strengths of the bross stakeholder engagement process that are highlighted as the key elements of success these strengths relate to stakeholder engagement and convergence research approaches practices and principles commonly recommended in literature hossain et al 2014c mcintosh et al 2011 national research council 2014 sandink et al 2016 voinov and bousquet 2010 voinov and brown 2008 the five key strengths of this study are interconnected and can be meaningfully understood hierarchically fig 10 it was critical that more time and resources were dedicated to underlying strengths in order to support the overlying strengths and essentially the user ready outcome of bross along with these strengths were key weaknesses of the study these strengths and weaknesses translate to lessons learned as presented below like the engagement approach section 4 the strengths and lessons learned are generalizable to other convergence research and stakeholder engagement efforts in various contexts 5 3 1 lesson 1 build a foundation of committed partnership and mutual trust the foundational strength of the stakeholder engagement process was that bross was initiated from a long standing partnership and mutual trust between the stakeholder agency and research group while the developer was not involved in collaborations with the agency prior to co developing bross the developer was able to leverage this foundation for a timely and effective engagement process the research group s ultimate aim in the partnership has been to support the agency in building long term capacity and autonomy in overcoming water management challenges with accessible science and technology the research group has demonstrated that they are not using the agency for their own benefit of achieving publications neither is the research group developing such complex technologies that the agency is indefinitely depending on the researcher group s guidance rather the research group has invested time and energy into an authentic partnership by training and properly transferring technology to the agency this has facilitated multiple ongoing co development projects between the researchers and agency the agency is empowered to confidently initiate and take ownership of satellite remote sensing tools including bross over time 5 3 2 lesson 2 support agency leadership and ownership throughout the co development process the second key strength was that the agency led and owned the bross initiation development and plans for moving forward this ensures authentic capacity building and long term sustainability of bross the agency initiated the bross collaboration based on operational challenges and priorities that they perceived on their own rather than having been imposed by researchers this had two main implications one was that bross was directly answering the agency s perceived needs and there were immediate benefits that the agency realized the second implication was that the agency was invested in bross throughout the co development process and will deliberately take ownership of bross moving forward as the involvement of the developer lessens after the initiation agency leadership and ownership was supported in multiple ways for example the bross co development process involved multiple iterations of results and outcomes with agency staff which ultimately enabled the agency to guide the process and final product also the follow up engagement trip occurred early in the co development process hence early on a broad range of agency staff could understand the underlying theory outputs strengths weaknesses and limitations of bross and meaningfully shape the tool design and functions moving forward the agency is in the position to independently manage and improve bross with local resources the researchers will continue to provide complimentary bross support as a technical backstop so that the agency is more confident with long term ownership hossain et al 2014c the primary weakness relating to agency leadership and ownership was that other than the agency sediment expert the agency staff did not have the opportunity to review the prototype bross tool prior to the follow up engagement trip to improve this the prototype tool a short memo and an online survey could have been provided to relevant agency staff and other meeting attendees prior to the engagement trip this would have given the agency more time to test the prototype tool reflect provide preliminary feedback and prepare for the in person meeting discussions the developer could have also reviewed preliminary feedback prior to the engagement trip and addressed it during the formal presentation although the follow up engagement trip was fruitful these steps could have likely enhanced the outcomes 5 3 3 lesson 3 integrate and solicit feedback from diverse stakeholder roles in appropriate contexts a diversity of stakeholder agency staff and external institution personnel were involved throughout the engagement process and notably during the follow up engagement trip table 4 the different roles were engaged in the appropriate ways this strength is directly attributable to the leading agency staff guiding the trip plans as they more than the researchers understood who should be involved and how to properly involve them for example junior staff were important to involve since they would be the primary tool operators junior staff were more comfortable sharing feedback during the informal hands on meeting rather than the large formal meeting hence involving them in both meetings was important in addition field staff demonstrated the agency s sediment engineering and management approaches and challenges and validated how bross could support field operations it was valuable to engage with field staff onsite where they could directly show these elements field staff also facilitated conversations with the local communities that they have developed relationships with as a result of their ongoing presence in the communities while planning the involvement with diverse stakeholder roles was critical it was also valuable that the developer s schedule intentionally had open time for on demand meetings with any interested staff and collaborators along with benefitting the skill of bross these spontaneous meetings strengthened the diversity of perspectives that the developer received a weakness of the integrating diverse feedback in the engagement process was that some of the agency staff did not share verbal or written feedback on bross since they were uncomfortable with their ability to communicate in english thus it would have been beneficial to provide opportunities for the agency to share feedback in bengali the national language of bangladesh for example surveys could have been more extensively integrated throughout the co development process e g immediately following the trip immediately after the release and staff could have been encouraged to respond in their native language if preferred along these lines it would have also been valuable to provide deliverables e g user manuals in both english and bengali 5 3 4 lesson 4 strive to understand the stakeholder agency problem and broader context with humility and respect as a direct result of the underlying strengths of the engagement process long term partnership stakeholder agency leadership and diversity of stakeholder roles involved the developer gained a rich understanding of the technical problem the agency s capacity and operations and the broader e g political economic social cultural context this strength of having a holistic understanding enabled the developer to effectively support sustainable capacity building of the agency however the developer also aimed to maintain humility respect and flexibility throughout the co development process with this disposition the developer was aware of their own role as an outsider amidst the broader context their own limits and gaps in understanding and the limits of their scientific and technological contributions in solving the agency s complex challenges the developer s overall efforts to inform their own perspective strengthened the relationship with agency staff external institutions and general public and fostered support and knowledge sharing the perspective also enabled the developer and agency to share realistic expectations for the co development process and tool outcomes to cultivate understanding of the agency problem and broader context the developer took extensive measures that would often be considered unnecessary in co developing a decision support system for example the developer took a bengali language and scripting course in the months prior to the follow up engagement trip although english is widely spoken in bangladesh the developer aimed to have basic bengali communication skills to show respect and foster a stronger connection with the bangladeshi people the developer also studied social and cultural norms and prepared to follow them to the extent feasible such as by dressing in a culturally appropriate clothing style while the developer was working across a dramatically different culture this concept also applies to researchers working with stakeholders in their same region or country researchers and stakeholders may speak the same official language and be immersed in the same socio political context however there are likely differences in their institutional vernacular and culture that it is important for researchers to understand and adapt to 5 3 5 lesson 5 aim for a tool function and design that is compatible with the stakeholder s operations dedicating time to the underlying strengths long term partnership stakeholder agency leadership diversity of stakeholder roles involved and understanding stakeholder context naturally yielded tool functionality and design that was compatible with the stakeholder agency s operations this strength of a compatible tool design and functionality entails multiple facets such as ease of use which was supported by the bross graphical user interface design and simple accessibility via web browser supplementation of existing practices e g in situ suspended sediment monitoring not needing to replace or significantly change existing protocols mcintosh et al 2011 sandink et al 2016 open source and adaptable design of bross allowing it to be maintained by the agency long term without major outsourcing or overhead costs similarity to other satellite remote sensing based applications that the agency uses 6 conclusion this paper described the stakeholder engagement process for the co development of an environmental decision support tool bross and highlighted key strengths weaknesses and lessons learned overall the agency and researchers determined the co development process and end product to be successful the primary researcher developer involved in the stakeholder engagement process was from a university engineering department the stakeholder agency was bwdb a bangladesh federal government agency focused on water resources engineering and management the agency faces complex river sediment management challenges of widespread erosion and accretion the agency has been constrained in addressing them due to limited resources and sediment monitoring capacity bross was primarily co developed to improve understanding of where migrating points of significant erosion accretion are occurring using ssc predictions bross integrates the technologies of satellite remote sensing and cloud computing which help to overcome the agency s main operational challenges bross estimates ssc throughout major bangladesh rivers using satellite remote sensing surface reflectance data bross provides broad spatial and temporal coverage of ssc patterns via maps and time series for user specified areas bross has two platforms which are both easy to use interactive and freely accessible via web browser bross has acceptable skill for the agency s operations predicting annual sediment loads within the observed ranges these scientific and technological elements and outcomes of bross are critical to it being directly integrated onto the agency s operations and decision making i e user ready however the success of bross being user ready is also attributable to the key strengths in stakeholder engagement process the foundation of the bross stakeholder engagement process was a strong partnership between the stakeholder agency and university research group that the developer was part of strength 1 out of this partnership the agency initiated and led the co development process strength 2 the developer calibrated models and prepared the tool in close iterative communication with the agency a major milestone of the engagement process was the follow up stakeholder engagement trip to bangladesh that the developer conducted to solicit feedback on the bross prototype and validate its societal impact during this trip the developer engaged with a diversity of stakeholder roles strength 3 and sought to understand the stakeholder agency problem and broader context with humility and respect strength 4 following the trip the developer updated the tool according to the stakeholder feedback and other insights gained on the trip the tool was released to the stakeholder agency along with user manuals and code to support long term agency ownership the developer iterated the final release with the agency and will continue to provide complimentary technical support moving forward the stakeholder agency is the primary bross owner the agency position to independently manage and improve bross with local resources due to the tool s design and functionality being compatible with agency operations strength 5 the long term success of bross will be determined by its ability to support the agency s erosion accretion engineering and management decisions forecasting of suspended sediment loads and understanding of river sediment profiles these long term outcomes are likely to be achieved due to the skill of bross the strong measures taken for sustainability and adaptability of bross and the agency s increasing capacity unfortunately it is rare for stakeholder engagement to be integrated in the training of researchers that are developing environmental monitoring and modeling tools this paper is intended to demonstrate a generalizable approach and lessons learned in crossing the valley of death as a road map for researchers however research institutions funding agencies and stakeholder agencies can also use this study to support user ready research for example all parties can invest time and resources in researcher stakeholder partnerships and promote opportunities for scientists engineers to develop stakeholder engagement skills furthermore all parties can support the shift of environmental research agendas to be authentically driven by societal needs brown et al 2015 this journey across the valley of death may seem risky to some but it is a greater risk to the future society and environment not to make it software data availability name bross google earth engine gee developer name email claire beveridge cbev uw edu developer address dept of civil and environmental engineering university of washington usa date first available april 2020 software required web browser availability cost free program language javascript program code https code earthengine google com b5e957d40b1f1f41d7df05c754df68f2 https github com cbev bross name bross local server developer name email claire beveridge cbev uw edu shahryar khalique ahmad skahmad uw edu developer address dept of civil and environmental engineering university of washington usa date first available april 2020 software required web browser google cloud keras https keras io xampp https www apachefriends org index html geospatial data abstraction library https www gdal org leaflet https leafletjs com highcharts http www highcharts com materialize https materializecss com and jquery https jquery com availability cost free program language python back end javascript html front end program code https github com cbev bross tree master local server scripts declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was primarily funded by the national science foundation nsf graduate research fellowship program under grant no dge 1762114 to the first author this work also benefitted from nasa water applied science program grant nnx15ac63g and nsf infews program ear 1740042 to the second author and travel support from the university of washington department of civil environmental engineering to the first author the first author thanks bwdb for generously supporting this work by providing data and thoughtful feedback on bross as well as hosting the first author for two weeks in bangladesh the authors thank those who attended meetings and supported field visits on the developer s trip to bangladesh special thanks to dr maminul haque sarker the center for environmental and geographic information services cegis river delta and coastal morphology group professor md munsur rahman and bangladesh university of engineering and technology buet institute of water and flood management iwfm for their valuable insights on bangladesh rivers and feedback on bross the authors also thank hisham eldardiry and dr matthew bonnema for support on bross development indira bose for support with stakeholder engagement and shrabanti paul md shariful alam shofekul islam udoy raihan and md delwer hossain for their support during the follow up engagement trip the authors thank dr alexander horner devine dr david butman dr alison duvall and two anonymous reviewers for their feedback on the manuscript which greatly improved it 
25965,environmental decision support tools often fail to be effectively applied in society because tools are use inspired rather than user ready stakeholder engagement by researchers is frequently the missing critical component here university researchers collaborated with the bangladesh water development board bwdb in developing a cloud based satellite remote sensing tool for monitoring suspended sediment concentrations ssc in major bangladesh rivers bangladesh faces complex river erosion and sediment management challenges the tool helps to overcome bwdb s constraints of limited resources and ground based monitoring capacity the tool maps estimated ssc over satellite images and provides long term estimated ssc time series at user designated points the key elements of success in the engagement process were the partnership and trust between the researchers and stakeholder stakeholder leadership and ownership diversity of stakeholder roles engaged researcher s understanding of the stakeholder problem and context and open source easy to use tool weaknesses in the engagement process can be improved on in future work keywords stakeholder engagement capacity building satellite remote sensing suspended sediment 1 introduction environmental researchers frequently develop use inspired tools to improve monitoring modeling and understanding of earth systems however these research tools are not always user ready or directly integrated into societal application and decision making hossain 2015 hossain et al 2014c irwin et al 2018 mcintosh et al 2011 prados et al 2019 this situation is analogous to two standalone mountains one symbolizing research and the other symbolizing society in between the mountains is a desolate valley of death national research council 2000 as tools from research often perish before making it to society for uptake the crux is that research tools cannot cross the valley and go from use inspired to user ready on their own researchers must take their tools descend from their mountain and journey across the valley with stakeholders although stakeholder engagement is critical for environmental decision support tools to become operational in society it is rarely made a priority for researchers stakeholder engagement is typically seen as being time consuming expensive complicated risky and unworthy of high impact scientific journal publication glicken 2000 environmental researchers that do aim to conduct stakeholder engagement may struggle with developing a sound approach and tangible goals stakeholder engagement entails the integration of technical and social processes korfmacher 2001 which environmental researchers are rarely trained to do while stakeholder engagement is not new e g baroudi et al 1986 king and rodriguez 1981 there is no consensus in environmental literature about best practices sandink et al 2016 examples in literature have varying project contexts and constraints and therefore standards for success are highly variable despite these challenges there are valuable recommendations and criteria for stakeholder engagement in literature for example best practice recommendations for successful use of environmental decision support systems were given by mcintosh et al 2011 practices were grouped into five categories designing for ease of use designing for usefulness establishing trust and credibility promoting the decision support system for acceptance and starting small and simple eleven criteria for success in stakeholder engagement were developed by sandink et al 2016 based on those commonly found in literature these included clear problem has been defined with the assistance of stakeholders know the stakeholder group stakeholders involved early and in as many development phases as possible gain trust establish neutrality and iterative processes applied seven recommendations for crossing the valley of death were provided by hossain et al 2014 these included aim for full ownership of a system by stakeholders seek two way feedback and listen more than talk train stakeholder agency from the ground up through hands on tasks in a thorough and patient way insights on user requirements and stakeholder interaction have also been well explored in the context of participatory modeling e g voinov and bousquet 2010 voinov and brown 2008 along with the recommendations in literature increasing environmental data availability and technological capacity provide immense opportunities to expand stakeholder engagement in developing environmental decision support tools two specific examples of advancements are satellite remote sensing and cloud computing earth observing satellites are rapidly increasing the volume and types of environmental data over 500 earth observing satellites have been launched over the past 50 years and over 150 more are planned to launch over the next 12 years ceos 2020 guo 2017 yao et al 2020 earth observing satellites provide a global vantage and repeatable objective measurements united nations office for outer space affairs 2018 most satellite data are freely and publicly available but there are barriers to effectively using these data such as their massive storage size high variability and high complexity however expansion of cloud storage and computing capabilities are lowering these barriers for example google earth engine gee is a fully cloud based high performance computing platform that stores multi petabytes of pre processed satellite imagery gorelick et al 2017 gee has web based javascript api and integrated development environment in which developers can rapidly implement algorithms and visualize outputs on an interactive map developers can also create apps with graphical user interfaces that allow non programmers to interact with maps and algorithm outputs gee and its datasets are freely available for non commercial use the advances of satellite remote sensing and cloud computing have immense potential to support environmental management in economically developing regions these regions typically have limitations in their in situ environmental data quality and quantity local computing bandwidth power supply and internet reliability satellite remote sensing supplements in situ data and cloud computing lessens dependence on local computer resources however for effective technology transfer and local capacity building in developing regions there must also be proper education training and co development of decision support tools while there are researchers and institutions forging these paths e g biswas and hossain 2018 hossain et al 2014c prados et al 2019 significant needs remain considering that economically developing regions are generally most vulnerable to the increasing pressures of environmental change it is urgent that technologies are effectively implemented to support their adaptation resilience and capacity independent group of scientists appointed by the secretary general 2019 globally the new normal continues to evolve with interconnected environmental health economic and social challenges at the same time environmental data and technological capabilities are rapidly expanding and providing immense opportunities for innovative and scalable decision support tools for the tools to directly serve society the scientific research community must intentionally evolve rethink priorities and enter into new territories of stakeholder engagement in doing so it is imperative that institutions and funding agencies support these efforts and equipped the next generation of environmental researchers for this paradigm shift brown et al 2015 irwin et al 2018 lettenmaier 2008 national research council 2014 examples of environmental researchers conducting convergence and user ready research in partnership with stakeholders can help to guide these efforts this paper describes the stakeholder engagement process in developing a cloud based satellite remote sensing tool for monitoring suspended sediment concentrations ssc in major rivers of bangladesh this tool called bross bangladesh remote sensing of suspended sediments was co developed by researchers from the university of washington in usa and staff from a stakeholder agency the bangladesh water development board bwdb the tool development and stakeholder engagement process were conducted over one year the guiding research question of this paper is what were the critical elements to a successful stakeholder engagement process in co developing bross the objectives of this paper are to 1 demonstrate a generalizable path by which researchers can conduct productive stakeholder engagement in a relatively short time frame one year 2 highlight the key strengths and weaknesses of the stakeholder engagement process and 3 provide socially relevant research directions for satellite remote sensing of river sediment and related software while various publications highlight strengths and lessons learned in stakeholder engagement processes this results in an extensive list for researchers which may distract their focus from the most important elements this study is unique in that it distills critical steps strengths and lessons learned and can help researchers and stakeholders in prioritizing the elements of their engagement approach also while stakeholder engagement has largely been explored in the context of participatory hydrologic modeling this study focuses on a far less studied topic of satellite based monitoring of sediment thus socially relevant research directions for the scientific community are unique and have the potential to lead to more widespread use of a technology that is underutilized in societal decision making in the text that follows section 2 provides the background on the study area stakeholder and problem section 3 contains the bross tool purpose and description section 4 describes the methods used in the stakeholder engagement process section 5 provides the results discussion and lessons learned on the key strengths and weaknesses of the process section 6 concludes the paper 2 background on study area stakeholder agency and problem 2 1 bangladesh hydrology geomorphology and sediment dynamics the country of bangladesh fig 1 lies in the active delta of three of the world s largest rivers the ganges brahmaputra and meghna in bangladesh the brahmaputra is known as the jamuna river downstream of the jamuna and ganges rivers confluence is the padma river the meghna river is divided between the upper meghna river upstream of its confluence with the padma river and the lower meghna river downstream of its confluence with the padma river bangladesh also has hundreds of smaller rivers throughout the landscape bangladesh s major rivers originate in the himalayan and east indian mountains flowing through various countries before reaching bangladesh and discharging into the bay of bengal average annual streamflow discharge from the three major rivers is 1 35 1012 trillion m3 year small et al 2009 average annual sediment discharge estimates range from 500 to 2400 mt per year and are dominated 98 by sediment loads from the padma and jamuna rivers rahman et al 2018 the majority 80 of streamflow occurs during the summer june to september due to the combined impacts of seasonal monsoons and snowmelt from upstream mountains small et al 2009 subramanian and ramanathan 1996 bangladesh s major rivers are highly prone to erosion deposition channel shifting and char riverine islands and bars development this is largely due to the extremely high rate and fluctuation of streamflow and sediment transport particularly in the summer monsoon season sarker et al 2011 the riverbanks and channel beds primarily consisting of fine sand and silty sediments with occasional clay have little resistance against erosive forces billah 2018 mclean et al 2012 sarker et al 2011 2003 the rivers are mostly flat 2 7 5 cm km sarker et al 2003 making erosion accretion and shifting patterns widespread and difficult to predict the jamuna river is an intensely braided river that erodes thousands of hectares of mainland floodplain each year in recent years its braiding intensity and width have increased sarker et al 2014 the ganges river is predominantly a meandering planform with some braided reaches the padma and lower meghna rivers vary spatially and temporally between single thread meandering and multi channel braiding patterns sarker et al 2003 shajahan and reja 2012 length average bank erosion rates are higher in the jamuna padma and lower meghna rivers compared to the ganges and upper meghna rivers sarker et al 2003 however compared to the jamuna and meghna rivers ssc is highest in the ganges storm surges and cyclones also cause severe erosion deposition and channel shifting primarily amongst the lower meghna river 2 2 societal impacts of river sediment dynamics bangladesh is a developing emerging country international monetary fund 2019 its population is 160 million people and its population density of 1240 people per square km of land is one of the highest in the world food and agriculture organization and world bank 2018 millions of people in bangladesh live alongside rivers or on chars throughout the river system and many livelihoods are directly tied to natural resources e g agriculture fisheries sarker et al 2003 shajahan and reja 2012 riverbank erosion frequently degrades and destroys housing farmland property and public infrastructure each year erosion leaves thousands of people landless and homeless and millions negatively impacted in other ways islam and rashid 2011 accretion along the banks and channel beds also inhibits ship navigation hence river erosion accretion is directly and heavily integrated into the society and economy of bangladesh 2 3 stakeholder agency mission and needs the stakeholder agency bwdb is a federal government entity focused on water resources engineering and management environmental sustainability and improvement of livelihoods throughout the country the agency has major long standing efforts to monitor manage and mitigate the impacts of riverbank erosion and accretion however the vastness and complexity of the river system as well as limited resources available to the agency are major constraints the stakeholder agency s existing in situ suspended sediment monitoring capacity is not adequate for the needed understanding and prediction of river system sediment dynamics the agency has 25 in situ sediment monitoring points rahman et al 2018 but only four are located along the major rivers fig 1 the agency has conducted in situ suspended sediment monitoring throughout the river system since the 1960s however sediment data have not been collected continuously and data from some time periods and locations is less reliable rahman et al 2018 a large reason for these issues is that in situ sediment monitoring is time consuming complicated and expensive and the agency has not had the resources to bolster their sediment monitoring approach in situ sediment monitoring can also be difficult and dangerous during the monsoon season when erosion accretion is prominent and sediment loads are highest riverbank protection has been implemented along the major rivers since the early 1970 s sarker et al 2011 however there are substantial lengths of non or poorly protected vulnerable riverbanks where people live and farm the stakeholder agency continuously installs permanent riverbank protection which is mostly revetments layered with concrete blocks and geotextile bags filled with sand however along with limited resources there is also the constraint of time between monsoon seasons to implement permanent protection measures during the monsoon season the agency also rapidly responds to emergency erosion events by implementing temporary protection which is typically geotextile bags filled with sand since erosion points are difficult to predict it is challenging for the agency to prioritize and prepare for both permanent and temporary protection needs in addition the agency conducts river dredging to manage erosion and accretion however the submerged locations of sediment deposition are difficult to determine the costs for these various measures are extremely high the stakeholder agency is interested in improving their prediction of erosion and accretion hotspots so that they may more effectively allocate limited resources to engineering and mitigation measures and ultimately protect human livelihoods areas of erosion accretion frequently migrate throughout the vast complex rivers and are difficult to strategically monitor with the agency s sparse in situ sediment monitoring capacity the ability to monitor sediment dynamics throughout the entire river system particularly using satellite remote sensing can help to inform the agency where potential erosion accretion points may be and where further on the ground investigation is warranted 3 bross tool purpose and description 3 1 bross tool purpose bross estimates ssc throughout major bangladesh rivers using satellite remote sensing surface reflectance data the agency primarily plans to use bross to improve understanding of where migrating points of significant erosion accretion are occurring in addition bross is intended to help improve mandated forecasting of sediment loads during monsoon seasons and to help improve the understanding of the horizontal and vertical profiles of river sediment the purpose and plans for bross were directly defined by the stakeholder agency bwdb in the early stages of the tool development 3 2 bross tool description the bross domain is within bangladesh along the main stems of the jamuna ganges padma and lower meghna rivers fig 1 ssc predictions are of two forms time series of ssc averaged over a region of interest roi within the bross domain and ssc mapped throughout the domain for a single satellite image bross uses visible red green blue and near infrared nir band data from the landsat sentinel 2 and modis satellite series table 1 the different spatial resolutions temporal resolutions and durations of these satellites have trade offs in decision making landsat and sentinel 2 data have relatively high spatial resolution and can distinguish ssc patterns along river cross sections and banks however the temporal resolutions are relatively low and can miss critical hydrologic events modis data have high temporal resolution however the spatial resolution is relatively poor and only appropriate for reach scale ssc averages of wide 500m rivers while sentinel 2 data have relatively high spatial and temporal resolution the short historical records inhibit strong calibration for predicting ssc models for predicting ssc from satellite data were calibrated using in situ suspended sediment data collected by the stakeholder agency bwdb from 1968 to 2019 at the four monitoring points along the jamuna ganges and padma rivers fig 1 bross integrates two modeling approaches to predict ssc from satellite data regression and artificial neural networks ann regression is a simple widespread approach that is relatively simple to implement e g beveridge et al 2020 markert et al 2018 for bross an exponential model is used to relate ssc to a ratio of satellite band data e g ssc a exp b red green c where values for a b and c are determined by regression ann uses pattern recognition to capture both linear and non linear relationships peterson et al 2018 sudheer et al 2006 ann is relatively complex and challenging to implement therefore is less commonly used for remote sensing of river sediment however in this study and others e g peterson et al 2018 ann overall has superior performance to regression table 2 bross uses ann model inputs of single bands band combinations month of the sample collection and satellite identification together regression and ann give a plausible range of ssc which is valuable for decision making under uncertainty bross has two operational platforms which are both easily accessible for users via a web browser one platform is gee https earthengine google com section 1 gorelick et al 2017 where bross is implemented using the gee javascript and user interface apis the second platform operates via a local desktop server and google cloud and is hereinafter called the local server platform the local server platform back end uses scripts written in the linux operating system using python scripting language the front end development uses all open source tools and software following ahmad and hossain 2019a and biswas and hossain 2018 briefly this entails local web server environment setup using xampp cross platform x apache a mariadb m php p and perl p https www apachefriends org index html map visualization using the javascript library called leaflet https leafletjs com chart production using the javascript library called highcharts http www highcharts com ssc outputs processed to the geotiff format for rendering on the webpage and exporting using geospatial data abstraction library https www gdal org dynamic and interactive user interface development using the styling and event handling libraries of materialize https materializecss com and jquery https jquery com both platforms produce long term time series of estimated ssc for user designated regions based on satellite data fig 2 a and b fig 3a and b the time series can be downloaded both platforms also produce colored maps of the estimated ssc distribution in the bross domain fig 1 for individual satellite images fig 2a and 3a users can click on individual pixels of mapped ssc to obtain the value when generating ssc time series and maps both platforms mask out pixels of the land surface clouds and cloud shadows so that only water surface pixels are included while these are the commonalities of the platforms there are notable differences in the key features table 3 interfaces fig 2a and 3a and workflows 2b and 3b of the two platforms for example the local server platform includes both the ann and regression models whereas gee only includes the regression model also gee allows users to sketch the region of interest for the ssc time series whereas the local server only generates time series at predefined locations the stakeholder can add predefined locations as desired the bross gee tool is at https bross users earthengine app view gee the bross local server tool is at http depts washington edu saswe bross further technical details about bross are in the technical manual and further details about the user interfaces and functionality are in the two user manuals all manuals and codes for both bross tools are found on github at https github com cbev bross 4 stakeholder agency engagement methods the approach to co developing bross followed strategies commonly applied and recommended in conducting convergence research and stakeholder engagement national research council 2014 sandink et al 2016 voinov and bousquet 2010 the generalized steps and timeline are adaptable to other researcher stakeholder partnerships locations and environmental challenges fig 4 for example the steps can also be applied for researchers and stakeholders in the same country region and social cultural context throughout sections 4 1 to 4 6 figures depict generalized sub steps while the text describes the bross co development process as a case study of how these steps may be carried out by early career researchers hereinafter the following terminology is used to describe the primary roles in the co development process developer is the primary university researcher engaging in the co development process researchers or research group is the university research group that the developer is part of stakeholder agency or agency for short is bwdb leading agency staff is three staff from bwdb that have executive decision making powers in the agency and play a leading role in the co development process agency sediment expert is one of the leading agency staff who is an expert on numerical modeling of sediment transport and riverbank erosion 4 1 step 1 establish partnership and become familiar with stakeholder agency the research group focuses on conducting user ready research promoting access to water resources information and improving livelihoods in challenging environments with sustainable satellite remote sensing and hydrologic modeling applications as part of this mission the research group has been collaborating with the stakeholder agency and other bangladesh water management institutions on satellite remote sensing applications since 2011 the research group has provided education and training on satellite remote sensing fig 5 part a co developed multiple tools to support agency operations and provided ongoing technical support hossain et al 2014c 2014b 2014a through this long standing partnership the research group has institutional understanding of the stakeholder agency s context challenges capacity and priorities fig 5 part b from a technical standpoint this includes understanding of bangladesh s hydrologic system where and what forms of hydrologic data the agency collects and how the agency operates between office and field operations from a social standpoint the research group is also familiar with the culture of the agency and broader geographical region the agency s management structure and the agency s resource constraints having this broad technical and social understanding is important before narrowing to a specific project topic the partnership was initially established by the research group leader contacting stakeholder agencies expressing interest in collaborating and focusing on building relationships with multiple members of the agencies through these relationships there was mutual learning about how the researchers and agencies could work together and where initial education and training was needed researchers that do not have the privilege of a preexisting relationship with a stakeholder agency may need to initiate a partnership building process they can do this by similarly networking with stakeholders and first focusing on robust relationship building voinov and bousquet 2010 the time and effort to establish partnerships is situational and depends on multiple diverse factors such as the familiarity of the agency with research approaches the establishment of the partnership can be integrated with the remaining steps 4 2 step 2 meet with stakeholder agency to match agency priorities to researcher expertise the co development of bross was initiated by the agency during an in person meeting between the researchers and stakeholder agency to discuss general collaboration opportunities for decision support tools the steps for matching agency priorities to researcher expertise were carried out in this meeting the researchers organized this meeting and hosted the three leading agency staff at the researchers university for two days the researchers presented their focus areas and related decision support tools as examples of what could be implemented for the agency fig 6 part a the developer discussed their work on satellite remote sensing for ssc estimation in another region beveridge et al 2020 the developer also demonstrated a web based tool for satellite monitoring of ssc that was implemented by another group of researchers and stakeholders markert et al 2018 the agency expressed interest in co developing a similar satellite based application for ssc to support their engineering and management priorities for river sediment erosion and accretion the primary interest of the leading agency staff was for the ssc distribution of major rivers to be qualitatively mapped on individual satellite images in order to improve understanding of where migrating points of significant erosion accretion are occurring the agency invited the developer to conduct a follow up trip to bangladesh to facilitate the agency engagement process upon the agency s invitation the developer committed to the co developing bross and conducting the trip next the researchers and leading agency staff established the short mid and long term priorities and action items for the bross co development and agency uptake fig 6 part b these were apply local in situ suspended sediment data to calibrate and validate a ssc regression model from satellite data short to mid term priority develop a cloud computing based automated system for visualizing remotely sensed ssc of bangladesh major rivers using gee mid term priority assign a dedicated agency staff to master the remote sensing technique and carry out research to monitor riverbank erosion and accretion hot spots short to mid term priority in addition the developer met with the agency sediment expert who shared about the agency s in situ sediment monitoring approach and data availability fig 6 part c based on this information they planned the regression model calibration and bross prototype development the agency sediment expert agreed to obtain the relevant in situ sediment and streamflow data from the agency and provide them to the developer the developer and agency sediment expert also discussed the prototype design workflow etc based on the earlier meetings the workflow for bross on gee fig 2b was devised without consideration of ann with the understanding that it may change based on feedback from the agency during the follow up engagement trip during this meeting the agency sediment expert also expressed interest in using bross in conjunction with models to better understand the horizontal and vertical profile of ssc it is possible that there would not be as readily of a match between the stakeholder agency priorities and plausible tools presented by the researcher in that case it is still valuable for the researchers and agency to discuss the agency s general management priorities and expected outcomes for the short term and long term while there may not be an immediate need that the researcher can respond to it may be discovered that there is potential for tool co development in the future e g after a certain milestone is met similarly it will help for the agency to understand plausible tools that the researchers can develop as an unexpected need for a similar tool may arise in the future it is important that co developed tools are not forced or implemented prematurely this will likely detract from the effectiveness and long term sustainability of the tool and generally be a poor use of time and resources with a partnership in place step 1 there is more flexibility to be patient with opportunities and let them develop in appropriate time 4 3 step 3 develop prototype tool and prepare for follow up stakeholder agency engagement meeting event prior to the follow up agency engagement trip the developer prioritized calibrating models for predicting ssc from satellite reflectance data producing a prototype system for the agency s trial use and preparing for the trip the model calibration and prototype system production were primarily conducted by the developer fig 7 parts a b however the agency sediment expert was involved in reviewing intermittent results discussing issues and determining next steps throughout the process fig 7 part c the leading agency staff provided the developer with the in situ data and the developer downloaded relevant satellite data from gee initially models to predict ssc from satellite data were calibrated and tested using regression strong regression model results were difficult to obtain due to the high complexity of the river system and limitations of the satellite data and in situ data rahman et al 2018 see technical manual because of the regression model limitations the researchers proposed using ann models to predict ssc from satellite data to initially test the feasibility of ann model in the bross domain the researchers used the matlab neural network toolbox the mathworks inc natick massachusetts and training approach from ahmad and hossain 2019b after the ann results proved to be stronger than the regression results ann models were developed using the keras library in python the matlab toolbox is simpler to use however the ann models had to be programmed in python to be implemented in bross the bross prototype was developed in gee based on the design discussed by the researchers and leading agency staff step 2 the prototype had limited user interface capabilities compared to the proposed final version so that excessive time was not spent on functionalities that may not be used however the prototype design allowed users to envision the proposed bross workflow and outputs the prototype only used landsat 8 satellite data and the regression model to predict ssc the prototype design allowed users to select an in situ monitoring location review the regression predicted ssc time series for the full landsat 8 record and click on points of the time series to view the predicted ssc map for the specified date the researchers and leading agency staff scheduled the developer s follow up engagement trip to bangladesh for two weeks of january 2020 fig 7 part d they collaboratively established the trip objectives developed the itinerary and determined coverage of costs there was mutual understanding that the trip objectives were focused on agency engagement and would entail both office meetings and field site visits the leading agency staff directed the trip preparations by inviting meeting attendees and planning the field site travel logistics the agency planned to cover the expenses for room and board during the field site visits and the researchers planned to cover the airfare and remaining room and board costs during this preparation period the developer also prepared for the trip by studying the regional language culture and social customs part of this was by leveraging the research group s institutional knowledge about the region and the agency culture and operations 4 4 step 4 conduct follow up agency engagement meeting event to collect feedback on tool and validate societal impact the follow up stakeholder agency engagement trip primarily composed of meetings with agency staff and collaborators and visits to field sites where sediment challenges are prominent fig 8 the leading agency staff organized all activities and involved a diversity of participants table 4 the formal presentation at the agency headquarters in dhaka fig 1 was the first major activity of the trip fig 8 part a during the formal meeting the developer first gave a presentation that encompassed the following objectives of the agency engagement process objectives of the bross tool brief technical background on satellite remote sensing of ssc including what the primary limitations are preliminary regression and ann model calibration results demonstration of the bross prototype operation and description of additional features proposed for the final bross design then the meeting participants asked questions and provided feedback section 5 1 following the agency s typical standard protocol meeting participants were provided with a printed memo at the beginning of the meeting which summarized the presentation the memo also contained questions for the participants that could be responded to during the meeting and or after the meeting in an optional online survey the questions were about the value of bross in addressing agency needs points of confusion about bross and how bross could be improved the memo and survey questions helped to focus the meeting objectives and discussion it also allowed meeting participants multiple opportunities to provide questions and feedback following the formal presentation the developer led an informal hands on session on operating bross fig 8 part b there was a step by step demonstration of how to operate bross and the potential additional functionalities that could be integrated the participants were asked to suggest changes in the bross workflow and design and share whether the potential additional functionalities would be confusing or complicated for operators while the formal meeting and hands on training session were planned before the trip multiple unplanned on demand meetings arose after the formal meeting with attendees interested in further discussion fig 8 part c two additional meetings were held with executive engineers of the flood forecasting department the engineers posed an additional bross objective of supporting mandated forecasting of flood suspended sediment loads one meeting was held with a local university research group which was conducting synergistic ground based and remote monitoring of river erosion rahman et al 2019 one meeting was held with the staff from a local nonprofit research institution which was conducting satellite based monitoring of river geomorphology as a result of these meetings there were potential additional uses of bross additional feedback on the bross design and additional technical information shared with the developer to support the bross performance the developer also spent five days visiting two large project field sites where there are major engineering and management challenges that motivate the agency s interest in bross fig 8 part d one site was sirajganj two days and the other was the coastal polder area three days fig 1 sirajganj is along the jamuna river and is one of the most vulnerable riverbank erosion areas in bangladesh the coastal polder area is along the lower meghna river and the coast of the bay of bengal the area is subject to severe riverbank erosion storm surge and cyclones both areas are considered high risk because of the density of human communities living near water bodies that frequently suffer loss and damage of land and property at the field sites that were visited the agency conducts ongoing engineering measures such as riverbank protection and dyke levee installation the field site visits were guided by agency staff with extensive site knowledge and experience the agency staff demonstrated the on the ground approaches constraints challenges and needs related to sediment erosion and accretion the agency staff validated that the broad spatial and temporal perspective of bross would help to direct monitor and evaluate their field efforts the developer and agency staff also visited the local communities surrounding the field sites the community members and agency staff validated the potential societal impact of bross by sharing about how the communities are impacted by sediment related disasters how current engineering measures are improving livelihoods and what issues remain 4 5 step 5 update and deliver tool iterating as needed after completing the follow up stakeholder agency engagement trip the developer updated bross based on agency feedback the calibration of regression and ann models were enhanced by incorporating the technical information and agency feedback gathered during the follow up engagement trip fig 9 part a the bross tool back end and front end updates fig 9 part b and writing of the user and technical manuals fig 9 part c were done with careful consideration of the agency needs and concerns as well as the technical expertise of the agency s bross operators in april 2020 the bross tool manuals and codes were released to the leading agency staff fig 9 part d in the email delivery the developer provided a summary of the tool updates and deliverables along with links to the bross platforms and manuals which were publicly available online the developer encouraged bross to be tested and disseminated in the agency as appropriate and for agency staff to contact the developer with questions concerns and needs one week after the release the agency sediment expert organized a call with the developer and research group leader to share feedback ask questions and discuss next steps section 5 1 the expert s questions were largely technical and follow up action items were strategically delegated between the researchers and agency the developer and expert carried out follow up action items as appropriate additional time beyond one week may be needed for stakeholders to fully assess issues and multiple iterations of meetings and subsequent tool improvements may be needed over extended periods of time 4 6 step 6 provide ongoing complimentary technical support moving forward the stakeholder agency is the primary owner of bross and responsible for leading its ongoing use and improvement since the deployment of bross the agency has been working to integrate bross into their sediment monitoring engineering and management approaches recommended future improvements for bross were included in the technical manual and were largely based on agency concerns and feedback collected throughout the engagement process users that have questions or find issues with bross can fill out an online form which is linked to in the tools and user manuals the researchers maintain the responses to the form and will transition this responsibility to the agency when appropriate the researchers are available for ongoing technical support and intend to fix issues and answer questions as they arise major updates to bross may be conducted by the researchers in the future at the request of the agency the agency also has potential to engage with other institutions that may use and or expand on bross 5 results discussion and lessons learned 5 1 stakeholder agency feedback the stakeholder agency feedback on bross can be used by the broader research community to help guide future research directions in satellite remote sensing river sediment dynamics and stakeholder engagement overall the agency staff had constructive feedback and clearly grasped bross s purpose functionality underlying theory and limitations the agency also strongly took initiative in having full ownership of bross s future application and development this demonstrated effective and sustainable capacity building the majority of stakeholder feedback was provided during the formal meeting on the follow up agency engagement trip section 4 4 of the thirty one meeting participants eight provided verbal feedback at the meeting and one provided written responses however none responded to the optional online survey the verbal feedback at the meeting was generally from upper level management agency staff and experts from external institutions the questions concerns and comments about bross were broadly on the science and technology application to engineering and management and ongoing research and development table 5 the concerns about the science and technology were largely inherent issues of satellite remote sensing in general biswas and hossain 2018 and in particular for sediment beveridge et al 2020 concerns about application to engineering and management were largely unknown in the present state of sediment research and management fu et al 2019 however the concerns could be addressed with ongoing research and application from the broader researcher community and from the agency for example bross outputs can be integrated with erosion prediction models developed for bangladesh rivers e g biswas et al 2016 through discussions with the developer the agency staff came to understand which questions and issues they posed were largely active research topics and beyond the present scope of bross the agency was interested in pursuing the unknowns in partnership with the researchers and local institutions the agency staff agreed with the proposed design and features e g including ann and multiple satellite options however if they had disagreed this formal meeting would have been the opportunity to revise the final bross workflow and design during the hands on training session with eight junior and mid level agency staff that would be directly involved in bross operations section 4 4 the proposed final bross design was accepted the staff found the graphical user interface to be easy to use the staff felt comfortable with the user customization options for the models satellites monitoring points and outputs however it was not ideal to have customization beyond this e g different options for regression equations most of the staff were not confident with updating the bross programming codes so staff familiar with computer programming would need to be dedicated to bross updates the staff concerns were similar to those discussed in the formal meeting table 5 but primarily focused on cloud cover impacts and satellite penetration depth they recognized that they needed to be aware of these limitations in using bross after bross was released for testing feedback from the agency sediment expert section 4 5 revealed opportunities to further improve the tool s uptake and sustainability table 6 notably the expert explained that quantification of the predicted annual sediment load volume would help decision makers to better understand the reliability of bross this task was delegated to the agency to facilitate technology transfer and stakeholder ownership similarly other follow up action items were appropriately delegated between the researchers and agency the agency sediment expert planned to involve junior staff that had computer programming expertise in the agency s follow up tasks and ongoing bross development and application 5 2 bross skill assessment the sustainability and stakeholder agency uptake of bross critically depend on the tool s skill overall the stakeholder agency found the performance of bross to meet expectations and show promise for supporting operations the skill of bross was largely enhanced by iterating the data analysis and model calibration with the agency sediment expert as well as by obtaining supporting technical information during the follow up engagement trip the familiarity of the agency staff and external institutions with the river system in situ data and pre existing agency satellite remote sensing applications helped to integrate local understanding and overcome technical challenges for example the agency staff shared which in situ data monitoring points had more reliable data and could be focused on in the model calibration the agency staff and experts also explained how and why sediment reflectance properties varied spatially and temporally throughout the river system various insights from the agency staff and external institutions could not have been obtained by the developer in isolation from the agency and external organizations the performance of bross varies for each satellite and model table 2 the annual sediment load predictions for the river system derived from bross table 7 are the same order of magnitude as most recently published estimate based on observations 470 mt year year 2015 it was also found that the load is decreasing 10 mt year rahman et al 2018 annual suspended load estimates were computed using monthly ssc predictions from bross and monthly streamflow discharge observations collected by the agency the agency understands bross is not currently reliable for precise estimates of ssc and sediment fluxes however bross does provide realistic outputs which meet the agency s interest in qualitative ssc distribution maps the results are also adequately skillful to supplement existing sediment monitoring and forecasting approaches 5 3 lessons learned strengths and weaknesses of stakeholder engagement process the constructive stakeholder agency feedback section 5 1 and the skill of bross section 5 2 are two standards by which the stakeholder engagement process and user ready outcome of bross are considered successful however there are also five overarching strengths of the bross stakeholder engagement process that are highlighted as the key elements of success these strengths relate to stakeholder engagement and convergence research approaches practices and principles commonly recommended in literature hossain et al 2014c mcintosh et al 2011 national research council 2014 sandink et al 2016 voinov and bousquet 2010 voinov and brown 2008 the five key strengths of this study are interconnected and can be meaningfully understood hierarchically fig 10 it was critical that more time and resources were dedicated to underlying strengths in order to support the overlying strengths and essentially the user ready outcome of bross along with these strengths were key weaknesses of the study these strengths and weaknesses translate to lessons learned as presented below like the engagement approach section 4 the strengths and lessons learned are generalizable to other convergence research and stakeholder engagement efforts in various contexts 5 3 1 lesson 1 build a foundation of committed partnership and mutual trust the foundational strength of the stakeholder engagement process was that bross was initiated from a long standing partnership and mutual trust between the stakeholder agency and research group while the developer was not involved in collaborations with the agency prior to co developing bross the developer was able to leverage this foundation for a timely and effective engagement process the research group s ultimate aim in the partnership has been to support the agency in building long term capacity and autonomy in overcoming water management challenges with accessible science and technology the research group has demonstrated that they are not using the agency for their own benefit of achieving publications neither is the research group developing such complex technologies that the agency is indefinitely depending on the researcher group s guidance rather the research group has invested time and energy into an authentic partnership by training and properly transferring technology to the agency this has facilitated multiple ongoing co development projects between the researchers and agency the agency is empowered to confidently initiate and take ownership of satellite remote sensing tools including bross over time 5 3 2 lesson 2 support agency leadership and ownership throughout the co development process the second key strength was that the agency led and owned the bross initiation development and plans for moving forward this ensures authentic capacity building and long term sustainability of bross the agency initiated the bross collaboration based on operational challenges and priorities that they perceived on their own rather than having been imposed by researchers this had two main implications one was that bross was directly answering the agency s perceived needs and there were immediate benefits that the agency realized the second implication was that the agency was invested in bross throughout the co development process and will deliberately take ownership of bross moving forward as the involvement of the developer lessens after the initiation agency leadership and ownership was supported in multiple ways for example the bross co development process involved multiple iterations of results and outcomes with agency staff which ultimately enabled the agency to guide the process and final product also the follow up engagement trip occurred early in the co development process hence early on a broad range of agency staff could understand the underlying theory outputs strengths weaknesses and limitations of bross and meaningfully shape the tool design and functions moving forward the agency is in the position to independently manage and improve bross with local resources the researchers will continue to provide complimentary bross support as a technical backstop so that the agency is more confident with long term ownership hossain et al 2014c the primary weakness relating to agency leadership and ownership was that other than the agency sediment expert the agency staff did not have the opportunity to review the prototype bross tool prior to the follow up engagement trip to improve this the prototype tool a short memo and an online survey could have been provided to relevant agency staff and other meeting attendees prior to the engagement trip this would have given the agency more time to test the prototype tool reflect provide preliminary feedback and prepare for the in person meeting discussions the developer could have also reviewed preliminary feedback prior to the engagement trip and addressed it during the formal presentation although the follow up engagement trip was fruitful these steps could have likely enhanced the outcomes 5 3 3 lesson 3 integrate and solicit feedback from diverse stakeholder roles in appropriate contexts a diversity of stakeholder agency staff and external institution personnel were involved throughout the engagement process and notably during the follow up engagement trip table 4 the different roles were engaged in the appropriate ways this strength is directly attributable to the leading agency staff guiding the trip plans as they more than the researchers understood who should be involved and how to properly involve them for example junior staff were important to involve since they would be the primary tool operators junior staff were more comfortable sharing feedback during the informal hands on meeting rather than the large formal meeting hence involving them in both meetings was important in addition field staff demonstrated the agency s sediment engineering and management approaches and challenges and validated how bross could support field operations it was valuable to engage with field staff onsite where they could directly show these elements field staff also facilitated conversations with the local communities that they have developed relationships with as a result of their ongoing presence in the communities while planning the involvement with diverse stakeholder roles was critical it was also valuable that the developer s schedule intentionally had open time for on demand meetings with any interested staff and collaborators along with benefitting the skill of bross these spontaneous meetings strengthened the diversity of perspectives that the developer received a weakness of the integrating diverse feedback in the engagement process was that some of the agency staff did not share verbal or written feedback on bross since they were uncomfortable with their ability to communicate in english thus it would have been beneficial to provide opportunities for the agency to share feedback in bengali the national language of bangladesh for example surveys could have been more extensively integrated throughout the co development process e g immediately following the trip immediately after the release and staff could have been encouraged to respond in their native language if preferred along these lines it would have also been valuable to provide deliverables e g user manuals in both english and bengali 5 3 4 lesson 4 strive to understand the stakeholder agency problem and broader context with humility and respect as a direct result of the underlying strengths of the engagement process long term partnership stakeholder agency leadership and diversity of stakeholder roles involved the developer gained a rich understanding of the technical problem the agency s capacity and operations and the broader e g political economic social cultural context this strength of having a holistic understanding enabled the developer to effectively support sustainable capacity building of the agency however the developer also aimed to maintain humility respect and flexibility throughout the co development process with this disposition the developer was aware of their own role as an outsider amidst the broader context their own limits and gaps in understanding and the limits of their scientific and technological contributions in solving the agency s complex challenges the developer s overall efforts to inform their own perspective strengthened the relationship with agency staff external institutions and general public and fostered support and knowledge sharing the perspective also enabled the developer and agency to share realistic expectations for the co development process and tool outcomes to cultivate understanding of the agency problem and broader context the developer took extensive measures that would often be considered unnecessary in co developing a decision support system for example the developer took a bengali language and scripting course in the months prior to the follow up engagement trip although english is widely spoken in bangladesh the developer aimed to have basic bengali communication skills to show respect and foster a stronger connection with the bangladeshi people the developer also studied social and cultural norms and prepared to follow them to the extent feasible such as by dressing in a culturally appropriate clothing style while the developer was working across a dramatically different culture this concept also applies to researchers working with stakeholders in their same region or country researchers and stakeholders may speak the same official language and be immersed in the same socio political context however there are likely differences in their institutional vernacular and culture that it is important for researchers to understand and adapt to 5 3 5 lesson 5 aim for a tool function and design that is compatible with the stakeholder s operations dedicating time to the underlying strengths long term partnership stakeholder agency leadership diversity of stakeholder roles involved and understanding stakeholder context naturally yielded tool functionality and design that was compatible with the stakeholder agency s operations this strength of a compatible tool design and functionality entails multiple facets such as ease of use which was supported by the bross graphical user interface design and simple accessibility via web browser supplementation of existing practices e g in situ suspended sediment monitoring not needing to replace or significantly change existing protocols mcintosh et al 2011 sandink et al 2016 open source and adaptable design of bross allowing it to be maintained by the agency long term without major outsourcing or overhead costs similarity to other satellite remote sensing based applications that the agency uses 6 conclusion this paper described the stakeholder engagement process for the co development of an environmental decision support tool bross and highlighted key strengths weaknesses and lessons learned overall the agency and researchers determined the co development process and end product to be successful the primary researcher developer involved in the stakeholder engagement process was from a university engineering department the stakeholder agency was bwdb a bangladesh federal government agency focused on water resources engineering and management the agency faces complex river sediment management challenges of widespread erosion and accretion the agency has been constrained in addressing them due to limited resources and sediment monitoring capacity bross was primarily co developed to improve understanding of where migrating points of significant erosion accretion are occurring using ssc predictions bross integrates the technologies of satellite remote sensing and cloud computing which help to overcome the agency s main operational challenges bross estimates ssc throughout major bangladesh rivers using satellite remote sensing surface reflectance data bross provides broad spatial and temporal coverage of ssc patterns via maps and time series for user specified areas bross has two platforms which are both easy to use interactive and freely accessible via web browser bross has acceptable skill for the agency s operations predicting annual sediment loads within the observed ranges these scientific and technological elements and outcomes of bross are critical to it being directly integrated onto the agency s operations and decision making i e user ready however the success of bross being user ready is also attributable to the key strengths in stakeholder engagement process the foundation of the bross stakeholder engagement process was a strong partnership between the stakeholder agency and university research group that the developer was part of strength 1 out of this partnership the agency initiated and led the co development process strength 2 the developer calibrated models and prepared the tool in close iterative communication with the agency a major milestone of the engagement process was the follow up stakeholder engagement trip to bangladesh that the developer conducted to solicit feedback on the bross prototype and validate its societal impact during this trip the developer engaged with a diversity of stakeholder roles strength 3 and sought to understand the stakeholder agency problem and broader context with humility and respect strength 4 following the trip the developer updated the tool according to the stakeholder feedback and other insights gained on the trip the tool was released to the stakeholder agency along with user manuals and code to support long term agency ownership the developer iterated the final release with the agency and will continue to provide complimentary technical support moving forward the stakeholder agency is the primary bross owner the agency position to independently manage and improve bross with local resources due to the tool s design and functionality being compatible with agency operations strength 5 the long term success of bross will be determined by its ability to support the agency s erosion accretion engineering and management decisions forecasting of suspended sediment loads and understanding of river sediment profiles these long term outcomes are likely to be achieved due to the skill of bross the strong measures taken for sustainability and adaptability of bross and the agency s increasing capacity unfortunately it is rare for stakeholder engagement to be integrated in the training of researchers that are developing environmental monitoring and modeling tools this paper is intended to demonstrate a generalizable approach and lessons learned in crossing the valley of death as a road map for researchers however research institutions funding agencies and stakeholder agencies can also use this study to support user ready research for example all parties can invest time and resources in researcher stakeholder partnerships and promote opportunities for scientists engineers to develop stakeholder engagement skills furthermore all parties can support the shift of environmental research agendas to be authentically driven by societal needs brown et al 2015 this journey across the valley of death may seem risky to some but it is a greater risk to the future society and environment not to make it software data availability name bross google earth engine gee developer name email claire beveridge cbev uw edu developer address dept of civil and environmental engineering university of washington usa date first available april 2020 software required web browser availability cost free program language javascript program code https code earthengine google com b5e957d40b1f1f41d7df05c754df68f2 https github com cbev bross name bross local server developer name email claire beveridge cbev uw edu shahryar khalique ahmad skahmad uw edu developer address dept of civil and environmental engineering university of washington usa date first available april 2020 software required web browser google cloud keras https keras io xampp https www apachefriends org index html geospatial data abstraction library https www gdal org leaflet https leafletjs com highcharts http www highcharts com materialize https materializecss com and jquery https jquery com availability cost free program language python back end javascript html front end program code https github com cbev bross tree master local server scripts declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was primarily funded by the national science foundation nsf graduate research fellowship program under grant no dge 1762114 to the first author this work also benefitted from nasa water applied science program grant nnx15ac63g and nsf infews program ear 1740042 to the second author and travel support from the university of washington department of civil environmental engineering to the first author the first author thanks bwdb for generously supporting this work by providing data and thoughtful feedback on bross as well as hosting the first author for two weeks in bangladesh the authors thank those who attended meetings and supported field visits on the developer s trip to bangladesh special thanks to dr maminul haque sarker the center for environmental and geographic information services cegis river delta and coastal morphology group professor md munsur rahman and bangladesh university of engineering and technology buet institute of water and flood management iwfm for their valuable insights on bangladesh rivers and feedback on bross the authors also thank hisham eldardiry and dr matthew bonnema for support on bross development indira bose for support with stakeholder engagement and shrabanti paul md shariful alam shofekul islam udoy raihan and md delwer hossain for their support during the follow up engagement trip the authors thank dr alexander horner devine dr david butman dr alison duvall and two anonymous reviewers for their feedback on the manuscript which greatly improved it 
25966,in ecological and other transactional energy matter flow networks accurate quantification of flows between compartments can be difficult and costly for models at steady state or undergoing linear change energy matter conservation together with the steady state condition can be exploited to estimate unknown flows from known ones in compartmental network models some flows are more important than others in terms of their connections to other flows participation in cycles geodesic distance to the environment in the graph theoretical sense and other topological features in respect to estimating unknown flows such importance differences also come into play pursuing this we formulate a link importance index linx that quantifies each flow s importance in a model this index identifies and quantifies the redundancy imposed by network topology and mathematical conservation rules we anticipate that it will find use in minimizing the cost and effort of data collection while also increasing model accuracy keywords energy matter budget network model systems modeling input output analysis link importance optimization software availability software name linx developer caner kazanci kelly j black system requirements linux mac os windows program language matlab c availability matlab file exchange https www mathworks com matlabcentral fileexchange 72143 linx github https github com kellyblack linx license gpl 3 0 1 introduction in recent decades of ecological and other applied complex systems modeling numerous methods have been developed to quantify flow networks representing qualitative food webs a typical approach to determining flows begins with a literature search for observational or experimental studies providing data for computation or estimation of model flows such as jørgensen et al 1991 that provide an annotated compendium of process rates with references may be quite useful if the literature does not provide adequately reliable data the next step often entails gathering empirical data at the model field site in the food web field methods such as gut content analysis stable isotope analysis pacella et al 2013 phillips and gregg 2003 post 2002 and fatty acid composition iverson et al 2004 have been used to quantify dietary composition for use in network flow quantification given the difficulty and high cost of gathering empirical data yodzis and innes 1992 methods based on inference and computation have been described one such approach uses allometric principles derived from empirical observation of organism physiology and biochemical rate processes to calculate hypothetical flows in steady state networks barnes et al 2014 this method has deep roots in ecology and finds its greatest recent development in what has become known as metabolic theory in ecology brown et al 2004 sibly et al 2012 some network quantification procedures are based on community assembly rules in which algorithms are used to build network structure and determine flows fath et al 2007 these methods include a modified niche model halnes et al 2007 and a structured food web approach with network flows drawn from a probability distribution morris et al 2005 ulanowicz and scharler 2008 describe two network quantification methods joint apportionment and reverse mold filling that require minimal mathematical inference and avoid the complexities of optimization routines ecopath with ecosim christensen and walters 2004 uses some of these methods to help construct and quantify a network of species interactions finally as flows or ranges thereof are determined the model must be checked for consistency since conservation laws provide constraints which the measured flows may not satisfy specifically the difference between the total input into and output from each compartment should be within expected range of values for smaller systems this might be done by hand using expert knowledge of the system but mathematical optimization schemes such as linear inverse modeling lim introduced into ecology by vézina and platt 1988 have been developed considerably and applied in many ecological contexts breed et al 2004 saint béat et al 2018 marquis et al 2007 vézina and pace 1994 van oevelen et al 2010 savenkoff et al 2001 despite the existence of extensive literature focusing on computational methods that help quantify flows in network models there is as yet no methodology to help guide and optimize the process of gathering empirical data approaches like lim and balancing methods are helpful after empirical data collection but are not designed to interact with the collection process itself the same ecological principles and computational ideas that make such computational methods possible can be exploited to help optimize empirical data collection to lower the cost and effort as well as to increase accuracy the methodology developed herein originated out of the observation that for steady state network flow models certain quantified flows may provide more or less information about other unquantified flows the difference is determined solely by the location of a flow within the network structure given a list of compartments nodes vertices and connections links flows the method works by assigning importance values to each connection the higher the importance value the more the information gained by the quantification of that flow our method could be used in the context of any of the network flow quantification approaches described above where incomplete flow quantification leads to the question which set of remaining unquantified flows should become focal for further empirical or analytical quantification furthermore it is noteworthy that certain flow magnitudes may provide information that results in a more stable calculation of the remaining unquantified flows as well see section 6 that is to say a small error in one measurement may propagate in different ways compared to the measurement of another flow in its place one of the goals of our method is to determine which flows result in a lower potential variance of the calculations of the remaining flows in the presence of measurement errors applying our method in this way could improve the efficiency of model making and aid in deploying research efforts in the construction of ecosystem flow networks mathematical formulation of the link importance index linx of this paper involves linear algebra however the general idea motivating it can be described verbally as is done in section 2 using a three compartment food chain model a slightly more complicated model is needed to show the relevance and usefulness of a linx and understand how it works we use a simple model with three compartments and five flows to do this in section 4 after covering necessary assumptions and notations in section 3 a general linx formulation is derived in section 5 and refined and finalized in section 6 section 7 includes a demonstration using the oyster reef ecosystem model dame and patten 1981 with emphasis on practical considerations such as data availability computational feasibility is discussed in section 8 followed by the conclusion section 9 2 flow importance index the idea in a network model at steady state not every flow needs to be quantified empirically because the steady state assumption introduces constraints in fig 1 for example determining any one of the four flows is sufficient to quantify the remaining three flows because the steady state condition forces all the flows to be equal their importance values are also equal because the amount of information each provides about the others is identical because each flow determines all the others a chain model is too simple to fully motivate a generalized importance index concept the example in section 4 will demonstrate using a more complicated network how some flow values can be more useful than others in determining unknown values in that section we will introduce the link importance index linx that will quantify the usefulness of each of the flows flows with higher linx values will give more restrictions on the whole system than those with lower indices we preferred link importance index to flow importance index because the mathematical formulation of linx is independent of the flow values it is based purely on network topology the structure and organization of connections or links among compartments linx does not only help identify the most helpful flows to build a complete model but also provides information about model accuracy in theory determining all four flow values in the model shown in fig 1 would be totally unnecessary and redundant as the values must all be equal in broader practice however depending on the amount of error involved in determining an individual flow averaging the four measured flow values in this example should increase accuracy because it effectively quadruples the sample size in general a higher linx value will imply that accurate measurement of that flow will contribute significantly more to the overall model accuracy than a flow with lower linx value in situations where restrictions on cost and effort limit the utilization of highly accurate measurement methods for the entire flow data the guidance provided by the flow importance index can be quite useful the source of the extra information provided by the measurement of certain flows is due to the steady state assumption that the total amount of input received by a compartment equals its total output per unit time in fact linx is still applicable to certain models not at steady state appropriate mathematical representation of a flow network is necessary to formulate the effect of the steady state assumption and how it can be relaxed in linx usage as next discussed 3 notation and the steady state assumption it is common to represent quantified flows in steady state multi compartment models by matrices the representation of the flow orientation differs in literature for instance patten 1978 employs a columns j to rows i flow orientation f j i to represent the flow from compartment i to j ulanowicz 1986 represents the same flow using a rows i to columns j flow orientation t i j since the environment is not represented as a compartment in standard network analyses e g patten and ulanowicz environmental inputs into or outputs from compartments are not included in the flow matrix but are represented separately by vectors the environmental input into and output from compartment i are denoted by z i and y i respectively by patten 1978 ulanowicz 1986 denotes environmental inputs as d i and distinguishes two categories of environmental outputs usable export e i and unusable respiration r i the mathematical representation of the steady state assumption is significantly simpler when all flows including the environmental inputs and outputs are enumerated and represented in vector form instead for that reason vézina and platt 1988 forego the use of matrices entirely and instead use a vector r where r i represents the i th flow we adopt this convention for present purposes however we use a different letter f i in keeping with the notation of patten the information imposed by the steady state assumption on a food chain f i f j for all i j is quite simple which is not necessarily the case for a general ecosystem model in order to accurately identify the information imposed by the steady state assumption on a general ecosystem model we need a formal way to represent how flows and compartments are connected to each other for a given ecosystem model in graph theory the incidence matrix is used for this purpose since open ecosystem models are not really graphs 1 1 for open ecosystem models each environmental input and output is connected to a single compartment as the environment is not included within the boundaries of the model for a graph however each flow edge needs to be attached to exactly two compartments vertices by definition hypergraphs do not have this restriction also called set systems hell and nesetřil 1970 they are significantly more general mathematical constructs than graphs and therefore not the most suitable abstraction for the ecosystem models we focus on we use a generalized version of the incidence matrix called the stoichiometric matrix which is the same as the incidence matrix for closed systems with no environmental inputs or outputs the stoichiometric matrix s eq 1 along with flow vector f provides the simplest mathematical representation of the conservation laws eq 3 for a model with n compartments and k flows including the environmental inputs and outputs the stoichiometric matrix resendis antonio 2013 has n rows and k columns and is defined as follows 1 s i j 1 if flow f j is to compartment i 1 if flow f j is from compartment i 0 neither for example the stoichiometric matrix of the three compartment food chain in fig 1 is 2 s 1 1 0 0 0 1 1 0 0 0 1 1 f f 1 f 2 f 3 f 4 where flows and compartments are ordered as f 1 f 2 f 3 f 4 and a b c respectively the vector f contains flow magnitudes ordered in the same way as the columns of the stoichiometric matrix for example the second column of s contains 1 on its first row and 1 on its second row representing the flow from the first compartment a to the second b in other words when a unit flow occurs from a to b there will be a unit decrease in a a unit increase in b and no change in c thus the second column of s 1 1 0 corresponds to the second flow f 2 the fact that a model is at steady state can be represented in terms of the matrix s and the vector f as 3 s f 0 for example for the three compartment food chain in fig 1 this simple equation yields the correct steady state conditions as follows s f 0 f 1 f 2 0 f 2 f 3 0 f 3 f 4 0 f 1 f 2 f 3 f 4 in general for an n compartment model the steady state assumption provides n linear equations involving flow values that can be exploited to determine unknown flows without needing to quantify all of them while the steady state assumption is essential for the method to work it is not absolutely needed the same approach still works if the change in compartment storage values can be assumed to occur at a constant rate for example if a certain compartment gains or loses approximately a certain amount of storage per unit time over a time period then all the methods presented in this paper remain applicable during that time period in general if the rate of change of the storage value of compartment i is given as c i then s and f satisfy the following equation instead of eq 3 s f c 1 c n this does not apply to systems where storage values fluctuate significantly which can be characterized by the high values of the magnitudes of the second derivative of the storage values over the observed time period we will continue to use the phrase steady state assumption throughout this manuscript for simplicity and clarity nevertheless the entire content of this manuscript remains valid for models where changes in storage values can be assumed to occur at constant rates 4 flow importance index a simple example the three compartment food chain model fig 1 is too simple an example to demonstrate how a link importance index is to be formulated in general material covered in section 3 enables us to demonstrate how the flow importance measure is computed for the simple three compartment model shown in fig 2 unlike the previous example no single flow value is enough to determine all five flows in this model we need to find the minimum number of flows that need to be quantified in order to compute all flows in this system for this consider the steady state equations for each compartment a f 1 f 2 f 3 4 b f 2 f 4 f 5 c f 3 f 4 this linear system has five variables degrees of freedom and three equations restrictions leaving two degrees of freedom this means if we fix two of the flows the remaining three should be uniquely determined that is quantifying only two flows will usually suffice to determine all five flows of this model to do this explicitly we rewrite the steady state equations 4 using the stoichiometric matrix as shown in eq 3 5 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 f 1 f 2 f 3 f 4 f 5 0 0 0 to demonstrate we can randomly choose two of the five flows and compute the remaining three based on the two we chose as an example we choose f 4 and f 5 and compute f 1 f 2 and f 3 in terms of f 4 and f 5 to do this we need a system of equations that expresses f 1 f 2 and f 3 in terms of f 4 and f 5 this follows from eq 5 1 1 1 0 1 0 0 0 1 f 1 f 2 f 3 0 0 1 1 1 0 f 4 f 5 here the 3 3 matrix on the left is made from the first three columns of the stoichiometric matrix 4 that correspond to the unknown flows f 1 f 2 and f 3 and the 3 2 matrix on the right is formed from the remaining columns of s that correspond to f 4 and f 5 then assuming the 3 3 matrix on the left is invertible we get the equations we need to compute f 1 f 2 and f 3 demonstrating that quantifying f 4 and f 5 is sufficient to determine all flow rates 6 f 1 f 2 f 3 1 1 1 0 1 0 0 0 1 1 0 0 1 1 1 0 f 4 f 5 f 5 f 4 f 5 f 4 this exercise informs us about the necessary and sufficient condition that the matrix formed by the columns of s that correspond to the unknown flows should be invertible if this matrix were not invertible it would be impossible to derive the equations we need implying that it would be impossible to uniquely compute f 1 f 2 and f 3 using f 4 and f 5 to demonstrate such a case if we try to find f 2 f 3 and f 4 in terms of f 1 and f 5 we get 7 1 1 0 1 0 1 0 1 1 f 2 f 3 f 4 1 0 0 1 0 0 f 1 f 5 in this the 3 3 matrix on the left is not invertible meaning that knowing f 1 and f 5 is not sufficient to determine f 2 f 3 and f 4 the intuitive reason for this is that measuring both f 1 and f 5 is redundant as the steady state assumption implies that f 1 f 5 similarly f 3 f 4 any other choice of two flows however can be used to determine all five flows in the model the goal of linx is to rank the amount of information on other flow values gained by quantifying a specific flow the extra information is afforded by the steady state assumption and the amount of this information depends on the location of the compartment within the network and the entire network topology as an example for the simple food chain model of fig 1 measuring any one of the four flows does provide the values of the three remaining flows as well effectively quadrupling the information provided by a single measurement this unusually high gain is due to the steady state condition combined with the fact that each compartment has exactly one input and one output forcing all flows to be equal to each other linx indicates that for this model quantifying any one of the flows provides the same amount of information assessing the contribution of a single flow for the three compartment model in fig 2 is more complicated as at minimum two flows are needed to compute all five flows only if we quantify a flow in question and another flow will it then be possible to compute all other flows given that the matrix formed by the columns of the stoichiometric matrix corresponding to the remaining three flows is invertible 6 if that matrix is not invertible 7 then those two flows alone will not be enough to determine all flows in the system based on this observation we define linx as the proportion of the cases that makes it possible to compute all five flows when a flow in question is paired with another flow this statement is generalized 8 formulated 10 and improved 13 in the following sections as an example we compute the flow importance index of f 3 there are four different sets of two flows that contain f 3 and one other flow f 1 f 3 f 2 f 3 f 3 f 4 f 3 f 5 considering the invertibility of the resulting matrices formed by each of these sets eqs 6 and 7 we see that all five flows in the model can be computed by measuring the flows in any of these sets except for f 3 f 4 in other words out of the four flow sets that have the potential to compute all five flows only three of them can be used to do so therefore we define the flow importance index of f 3 as 3 4 we can compute the importance indices of all flows by first making a list of all sets containing two different flows the two cross outs contain redundant flows one can use the two flows in any of the eight remaining sets to compute all five flows we define the link importance index linx m f i as the fraction of sets with two elements including f i that can be used to quantify all flows previously we observed that out of the four sets containing f 3 only three can be used to quantify all flows the same statement is valid for f 1 f 4 and f 5 however f 2 is different in that all four groups containing f 2 can be used to quantify all flows the flow importance indices for fig 2 model are m f 1 m f 3 m f 4 m f 5 3 4 0 75 m f 2 4 4 1 the value of m f 2 is higher than the others because once we know the value of f 2 quantifying any one of the remaining four flows is sufficient to derive all flows this is not true for any of the other flows implying that quantifying f 2 before others or more accurately than others is advantageous which is the goal of linx this section has illustrated how linx is computed for a simple model in the next section we derive a preliminary general mathematical formulation for linx and in section 6 we improve this formulation to give the full definition of linx 5 linx preliminary general formulation to construct a preliminary general formulation for linx we need first to find out the minimum number of flows needed to determine all flows in multi compartment models in general for an n compartment model which always contains k n flows including environmental inputs and outputs at least k n flows must be quantified to determine all flows this is because the steady state condition forms a linear system with k variables and n equations leaving k n degrees of freedom see appendix a for proof however as in the above example not every set of k n flows will work this observation leads to our preliminary formulation that is given by 8 m f i number of flow sets of size k n including f i sufficient to determine all flows number of flow sets of size k n including f i we formulate the denominator first the number of sets containing k n distinct flows is the number of subsets of the set of all flows f f 1 f k containing k n elements this in turn is the number of combinations of k flows taken k n at a time k k n k k n n since f i has to be one of the elements we need to count the combinations of the remaining k 1 flows taken k n 1 at a time which is k 1 k n 1 n k 1 n next we formulate the numerator which is the number of the sets counted in the denominator that can be used to determine all the flows for instance to compute m f 3 for the model depicted in fig 2 of the previous section first identify all sets containing two flows one of which is f 3 f 1 f 3 f 2 f 3 f 3 f 4 f 3 f 5 the number of such sets is 5 1 3 4 which is the denominator of the linx definition in eq 8 the numerator of the preliminary linx definition only counts the sets that allow the computation of all flows to find out if this is the case for a given set we need to form the linear equations that express the undetermined flows in terms of the quantified flows as we did in eqs 6 and 7 for example let us derive the system of linear equations for the first set a f 1 f 3 the steady state condition s f 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 f 1 f 2 f 3 f 4 f 5 0 0 0 this linear equation should express the unknown flows f 2 f 4 f 5 in terms of the known flows f 1 f 3 and test if a unique solution exists in order to do so we need to divide the matrix s and vector f into two parts accordingly 9 1 1 0 0 0 1 f 1 f 3 1 0 0 1 1 1 0 1 0 f 2 f 4 f 5 0 0 0 in general let f f 1 f 2 f k represent the set of all flows for a subset a f we define f a as a vector containing only the flows in a similarly we define s a as a matrix formed by the columns of s that correspond to the flows in a then a general version of eq 9 is s a f a s f a f f a 0 where the set f a contains all flows that are not in a i e the complement of a for our specific example f a f 2 f 4 f 5 then the general form of the linear equation that represents the unknown flows f a in terms of the known flows a is f f a s f a 1 s a f a for our specific example this equation is f 2 f 4 f 5 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 f 1 f 3 which can be simplified as f 2 f 4 f 5 1 0 0 1 1 1 0 1 0 1 f 1 f 3 0 f 3 the flows f 1 f 3 can be used to determine all five flows if this equation is solvable which only possible if the shown 3 3 matrix is invertible in general the flows in set a can be used to uniquely determine all flows f if and only if the matrix s f a is invertible in other words det s f a 0 this observation is all we need to construct a general formula for m f i which is the proportion of all flow sets of size k n containing f i which can be used to compute all flows 10 m f i 1 k 1 n a f f i a a k n sgn det s f a in the expression under the sum the symbol a represents the number of elements cardinality of a thus the sum is over the subsets a of f that have k n elements and contain the i th flow f i the number of such sets is k 1 n which is the denominator for the numerator we should count only the sets a with det s f a 0 we do that by taking the absolute value and sign function of the determinant so that it equals one if this condition is satisfied and zero otherwise 11 sgn det s f a 1 if s f a is invertible 0 otherwise by this preliminary definition linx always takes values between zero and one if the linx value of a flow is one then it would certainly be a good idea to quantify that flow first whereas flows with low linx values should be the last ones to be quantified for most well connected ecosystem models linx will usually be less than one half unlike the values we observed for the simple models we used for demonstration 6 linx improved formulation computation of unknown flows based on quantified flows requires the solution of a linear system of equations u x v where v is a vector based on quantified flows u is a matrix based on the model s network structure and x represents the unknown flows to be determined for the three compartment model shown in fig 2 one of these linear equations is 12 1 0 0 1 1 1 0 1 0 u s f a f 2 f 4 f 5 x f 3 f 1 0 f 3 v one issue with the solution of such equations is the propagation of error from the quantified flows f 1 f 3 to the computed solution f 2 f 4 f 5 in ideal conditions one might naively expect that 10 error in f 1 f 3 would imply a 10 error on f 2 f 4 f 5 in reality the error in f 2 f 4 f 5 can be as high as but cannot exceed 37 3 this error amplification factor of 3 73 of a linear equation system is determined by the condition number of its matrix u which is computed as the ratio of its maximum singular value to its minimum singular value beezer 2015 an n n linear system will be under determined and will not have a unique solution if the corresponding matrix u is not invertible the condition number of a non invertible matrix is meaning that any small error can be infinitely magnified matrices that are very close to being non invertible have high condition numbers meaning that the solutions they provide have high potential to magnify the errors in quantified empirical data this fact is relevant to linx as it is based on the ability of a given flow when combined with others to determine all the flows in a model determining the entire flow data in a model requires the solution of a linear system like eq 12 the preliminary definition of linx 10 is entirely based on whether or not this linear system is solvable 11 but not how accurate the resulting numerical approximation is for the whole system in other words it does not factor in how the existing error in quantified flows will propagate to the computed flows if for a given set of flows the condition number is high the computed flows will not be nearly as reliable decreasing the value of the additional information provided by quantifying those focal flows accurate assessment of this additional information is precisely the goal of linx therefore the additional information provided by the condition number should be factored into its formulation this is relatively straightforward to accomplish original m f i 1 k 1 n a f f i a a k n sgn det s f a 13 revised m f i 1 m a f f i a a k n 1 κ s f a m max j a f f j a a k n 1 κ s f a the improved formulation is different in two ways first the expression inside the sum is replaced with the multiplicative inverse of the condition number κ of the relevant matrix the expression sgn det s f a takes the value 1 or 0 depending on the matrix s f a being invertible or not respectively if s f a is not invertible then 1 κ s f a 1 0 as well matching the preliminary definition unlike the preliminary definition however 1 κ s f a often takes values significantly lower than one if s f a is invertible for example table 1 shows these values for the three compartment model in fig 2 for the three sets with non zero determinants the inverse of the condition numbers are 0 25 0 28 and 0 38 this poses a problem as just replacing the expression inside the sum the improved importance index would take the value 0 28 0 38 0 0 25 4 0 23 whereas the preliminary formulation gives 1 1 0 1 4 0 75 to compensate for the characteristic low values of the improved formulation we do not divide the sum by the number of sets k 1 n but by the maximum such sum this way the preliminary and improved importance indices achieve comparable values as shown on table 2 2 2 actually for large systems the preliminary linx will also generally produce importance indices significantly less than one thus in general to compare the preliminary index with the improved index it would be best to scale both of them so that the largest importance value is 1 while the proportion of the values are roughly preserved the additional information provided by the condition numbers placed the flows f 3 and f 4 ahead of f 1 and f 5 whereas all four flows had the same importance value according to the older definition fig 3 shows a more realistic comparison of the improved versus preliminary linx formulations using a classical ecological energy flow model odum 1957 kemp and boynton 2004 this model has n 5 compartments and k 14 flows so there exist 14 1 5 1287 flow groups containing any specific flow in fig 3 the thickness of a flow line indicates its importance the clear difference between the two diagrams demonstrates the significant effect of condition numbers on the importance of flows which was not observed in the simple three compartment model table 1 even though the silver springs model is small in size its slightly more complicated network topology compared to the three compartment model causes a difference significant enough to change the order of importance of flows when the improved formulation is used for example according to the preliminary definition the environmental output of the carnivores compartment carnivores environment is more important than the flow from carnivores to detritus carnivores detritus on the other hand it is exactly the opposite when the improved formulation is used even though the improved formulation is slightly more resource intensive to compute from here on we will employ it by default as it provides a more accurate measure of link importance the following two sections discuss several issues that may arise in using linx for model building refining or tuning we address some of these with a view to effective application of the methodology 7 practical considerations data availability quantifying a flow not only informs about its value but values of other flows as well because of the steady state assumption linx simply computes the amount of this additional information to determine the importance of each link this information cannot be accurately computed for flows about which prior information exists from previous observations experiments or literature such empirical information will render linx information partially useless also known flow values will provide information on other flows as well which may significantly alter linx values it is not just known flow values that affect linx values if a model contains flows that are difficult to measure accurately 3 3 an ecological example of a difficult to measure flow would be a feeding flow for a rare predator that is known to exist in the model but is so rare that obtaining a consumption rate for it is nearly impossible even considering the range of empirical methods available such as direct observation or gut content analysis or indirect methods such as metabolic estimates or stable isotope analysis then any additional information that can help quantify those flows will be commensurately valuable such difficult to measure flows can have a significant effect on the linx values as well this section explains how linx is computed in these two situations using the three compartment model of fig 2 and illustrates the ideas using an intertidal oyster reef ecosystem model fig 4 we consider three scenarios that can arise about flow f 1 in determining linx values in fig 2 model 1 f 1 is already known via pre existing data or because it is the focus of the research question and so have a high importance by default 2 it is not feasible to determine f 1 empirically and no data for it exists and 3 f 1 is known and it is not feasible to determine f 2 accurately combination of the first two cases for simplicity in this example only we will use the preliminary linx formulation it is possible to compute the linx values using the preliminary formulation simply by counting whereas the condition numbers of several matrices need to be computed in the case of the improved formulation in practice the improved formulation is preferred due to its accuracy therefore it will be used for the intertidal oyster reef model analysis presented at the end of this section fig 4 recall that we defined m f i for the three compartment model in section 4 as the fraction of sets with two flows k n 5 3 2 including f i that can be used to quantify all flows listing all flow sets with two elements and striking out the two unusable ones section 4 we computed all five linx values to be m f 2 1 and m f i 0 75 i 1 3 4 5 for the three scenarios listed above the importance index can be constructed using this same basic idea table 3 lists the relevant sets for each of the three different scenarios as well as the importance indices computed using them for the first scenario f 1 is already known so it should participate in all flow sets of size two which reduces the number of such sets from ten to only four it is exactly the opposite in the case of the second scenario where f 1 cannot participate in any of the sets since it is assumed that it is not feasible to quantify f 1 using direct methods such as field experiments or literature review scenario 3 is a combination of the previous cases so f 1 should be in all sets and f 2 cannot be in any of the sets the computation of importance indices of all flows is done according to the same definition above using the preliminary formulation and results are shown on table 3 comparing the linx values for each of the three scenarios we observe some similarities as well as significant differences for example the linx value f 3 originally m f 3 0 75 changes to 1 0 67 and 1 for each of the three scenarios respectively the linx value of f 5 on the other hand originally m f 5 0 75 changes to 1 0 and 1 for each of the three scenarios respectively drastic changes in m f 5 under different scenarios show that prior information and data availability can affect importance of flows significantly and hence should be taken into account this observation is not specific to this small and simple model and the preliminary formulation of linx fig 4 shows four different prior information and data availability scenarios for an intertidal oyster reef ecosystem using the improved formulation of linx 8 practical considerations computational feasibility in this section we describe how to compute the linx values using matlab and then discuss issues of performance and feasibility a matlab code that automatically computes the linx values given a model s stoichiometric matrix is available at github kazanci and black 2020 and matlab central file exchange kazanci 2020 this code is compatible with the freely available gnu octave software eaton et al 2014 as well as matlab for instance one can compute the linx values for the three compartment model of fig 2 using only its stoichiometric matrix eq 5 as follows the results match the ones provided in table 2 use of the stoichiometric matrix for ecosystem modeling is not as common as the adjacency matrix a second matlab code named a2s m is available with linx m that automatically converts a given adjacency matrix that includes environmental inputs and outputs to the stoichiometric matrix therefore the same results shown above can be obtained by executing the command linx a2s a where a is the adjacency matrix of the model it is also possible to use the same code in case of prior information or if some flows are difficult to obtain for instance the linx values corresponding to the three scenarios provided on table 3 can be computed as follows by simply listing the flows that are known or are infeasible to compute these results are slightly different than provided on table 3 because in the latter case the preliminary formulation of linx was used for its simplicity the results presented here use the improved formulation figs 3 and 4 are automatically generated using graphviz graphviz uses a text file with dot extension to create images in multiple formats a matlab file named linxdiagram m that automatically generates this text file which then can be used to create a network diagram is provided at github kazanci and black 2020 and matlab central file exchange kazanci 2020 one limitation of this code is its slowness for large models this is because all possible combinations of flows must be examined which is a computationally expensive process it is possible to make use of the row reduced echelon form rref of the stoichiometric matrix to limit the number of combinations to be tested this more complicated algorithm scales better for larger models and further details are provided in appendix b a faster c code based on this algorithm is provided along with the matlab code at github kazanci and black 2020 in the case of extremely large models even the faster c code might not be feasibly utilized it is not possible to provide a specific threshold however as network topology and prior flow information section 7 have a significant effect on speed in general models with high connectances dunne et al 2002 are expected to run slower and models with prior information will definitely run faster for example the number of flow combinations that needs to be tested for the oyster reef ecosystem model shown in fig 4 is 27132 under the three different scenarios shown on the same figure the number of combinations decrease to 5005 105 and 1365 for cases b c and d respectively in other words knowledge that four of the nineteen flows are unlikely to be determined empirically makes the algorithm about 270 times faster in theory in practice the oyster reef model is small enough that all cases run almost instantly on a regular personal computer this happens to be case for models of similar size the difference can be drastic for large or complex models however model size and complexity are decidedly limiting factors in the applicability of the present linx methodology to realistically complex systems studies such as those undertaken in large continuing studies like the us national science foundation s long term ecological research lter and national ecological observatory network neon programs even at such ambitious research scales our linx methodology could provide guidance for targeting empirical research to quantify flows in incomplete model flow networks as they are created quantified and revised over the life of projects in most cases large complex ecosystem models are the result of long term team based research projects and empirical research would provide prior information on some of the model flows since prior information often provides for feasible application of the linx algorithm many of these models could use this methodology large models typically contain model sectors which are groups of compartments that share many characteristics such as microautotrophs microheterotrophs or macroheterotrophs since teams of researchers are often responsible for a certain model sector such sectors might have some flows quantified and could be analyzed with the linx algorithm as sub system matrices as well as the full model being analyzed at various stages in the model building process 9 conclusion computational methods serve as an essential part of compartmental network flow modeling their utilization however usually occurs after parameters have been determined through empirical data collection missing data methods and literature search the computational method of this paper is applied early in the parametrization phase of modeling after some flow data but not all have been acquired exploiting conservation laws and network topology linx enables modelers to make informed decisions that guide the data collection helping to build a more accurate model while minimizing resources necessary to determine flow values the usefulness of linx is not just limited to before data collection as fig 4 demonstrates quantifying four flows produces system wide large changes in the linx values of the subsequent partially quantified flow network in other words the quantification of a few focal flows can dramatically change the linx values of other flows the process described in section 7 can be utilized repeatedly during the model building process and can identify which remaining flows are going to be redundant eliminating further costly and potentially unnecessary empirical measurements furthermore modelers may utilize redundant flows to increase the accuracy of other flows that may contain relatively greater errors in other words they can use linx to identify which specific flows will help increase the accuracy of the others finally the significant effect of prior knowledge about specific flows on the linx values of other flows reflects on the organization of large complex self assembling systems like ecosystems from the way our present algorithm works it is possible to infer that the importance structure of flows in ecosystems is very fluid and changing indeed in response to other network flows extant or not extant at any given time or place credit authorship contribution statement caner kazanci conceived the ideas developed and refined the formulation software writing original draft malcolm r adams conceived the ideas developed and refined the formulation writing original draft aladeen al basheer conceived the ideas developed and refined the formulation writing original draft kelly j black conceived the ideas developed and refined the formulation software writing original draft nicholas lindell contribution to the proof of the theorem writing original draft bernard c patten writing original draft stuart j whipple conceived the ideas developed and refined the formulation writing original draft acknowledgment all authors participated in the preparation of the publication contributed critically to the drafts and gave final approval for publication appendix a minimum number of flows required to determine all flows theorem 1 the rank of the n k stoichiometric matrix s of any open and connected ecosystem model equals n the number of its compartments in other words its rows are linearly independent and so are n of its columns proof first we note that n k i e there are at least as many flows as there are compartments since the system is connected and open also rank s min n k n by definition assume for contradiction that rank s n then there is a nontrivial linear combination of the rows r m such that a 1 m 1 n c m r m 0 but c m 0 for some m note that each of the k flows is attached to either one or two compartments by construction then each column of s has either one or two nonzero entries since the system is open there is at least one column with only one nonzero entry let the i th column be one of these it corresponds to an environmental flow into or out of some compartment s corresponding to the s th row r s of the matrix this flow is not connected to any other compartment we use induction on the length of the shortest path from compartment b to s to arrive at a contradiction as follows the i th entry of every row but r s is zero necessitating c s 0 inductively assume any compartment a with shortest path length l from s has c a 0 let compartment b have distance l 1 to compartment s certainly b must be connected via flow j to some a which satisfies the inductive hypothesis only r a and r b have nonzero entries in column j so that c a 0 c b 0 also the system is connected whence every coefficient is zero by the induction above contradicting c m 0 for some m thus rank s n remark this theorem shows that there exist collections of n flows such that the corresponding columns of s are linearly independent and thus the n n matrix formed by these columns is invertible however as evident from examples in the paper it is often not the case that every collection of n columns yields an invertible matrix appendix b a faster algorithm the minimum number of flows required to determine all flows is established within the main body of the primary paper one immediate task is to determine an algorithm to determine which combinations of the flows determine the feasible sets in other words which set of n flows can be used to calculate all of the remaining flows we will call these sets acceptable here we define two methods for searching the acceptable sets of flows the first is a brute force method and the second makes use of the row reduced echelon form rref of the stoichiometric matrix to limit the number of combinations that are tested codes for both approaches are publicly available kazanci and black 2020 the first method is to simply examine all possible n element sets of flows an advantage of this method is it makes use of standard libraries readily available to calculate the combinations to test another advantage is that it is complete and robust every combination is tested and none will be missed the shortcoming is that the algorithm does not scale well to larger systems and a system with a larger number of nodes may require a long time to search through all possible combinations the approach to examine all possible combinations is a computationally expensive process so another approach is explored that is designed to reduce the operation count the primary disadvantage of the alternate method is that its implementation is more complicated the basic idea is that the rref of the stoichiometric matrix can be used to more narrowly define which columns of the stoichiometric matrix are linearly dependent to better describe the approach we first describe the problem given in section 3 of the original paper as a reminder the linear system for the example can be expressed in the form b 1 s 1 1 0 0 0 1 1 0 0 0 1 1 f 1 f 2 f 3 f 4 0 we note that eq b 1 represents a system in the form b 2 s f 0 in general the matrix s has n rows and k columns the vector f has k rows and the zero vector on the right hand side has n rows an alternate representation for the system is to express s in terms of its columns and the vector f as b 3 s 1 s 2 s k f 1 f 2 f k 0 here the column vector s i represents the entries in the stoichiometric matrix corresponding to whether or not there is an inflow or outflow for node i for a given flow the system can be expanded to express it in an equivalent form b 4 f 1 s 1 f 2 s 2 f k s k 0 the goal is to test all sets of n linearly independent vectors from s i the linear system given in eq b 2 can be put in row reduced echelon form this will transform the system where the s i can be more easily compared as a way to rule out which columns might be expressed in terms of another column we first examine the example system given in eq b 1 the system with the rref of the matrix is given by 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 f 0 the task is to determine all sets that contain three column vectors that represent a linearly independent set the first thing to notice is that the first row in the rref matrix implies that s 1 and s 5 must be equal unless there is a nonzero entry in s 5 after the first row then there is no need to check a set of vectors that have both s 1 and s 5 the search for the first vector in the candidate sets can be conducted by using the first row and searching all sets where the first vector is s 1 or the first vector is s 5 given these two initial vectors for a candidate set the second row can then be searched because there are zeros in the second row for columns 1 and 3 a set of candidates for the second vector in the acceptable sets include the vectors s 2 s 4 or s 5 finally based on the third row the only non zero entries are in columns three and four so candidates for the acceptable sets include those sets where the third vector is either vectors s 3 or s 4 the general algorithm makes direct use of the row reduced echelon form of eq b 2 the recursive algorithm proceeds by first testing sets where the first vector is one of columns that has a non zero coefficient in the first row of the rref form of the stoichiometric matrix each column in the second row that has a non zero coefficient is then included for the second element of the possible sets the algorithm then proceeds through each of the following rows until all possible sets of the columns have been tested compared to the exhaustive method described above this algorithm reduces the number of columns to test the expense though is the use of a recursive algorithm that adds additional overhead another downside of the algorithm is that it requires additional book keeping and checking to insure that previously used combinations are not reused the c code requires that the stoichiometric matrix be defined in a regular text file the first lines in the text file consist of the stoichiometric matrix each line is a row in the matrix and the numbers are separated by spaces two optional lines can be appended below the stoichiometric matrix one line can be used to define which flows are known in advance known the other line can be used to specify which flows cannot be measured unknown an example of a file is given below where it is assumed that the flows for nodes 1 3 and 6 are known while the flows for nodes 2 5 and 11 cannot be determined when the c program is run the name of the program optimalflow is used to start the program the name of the file to read is provided on the command line after the program name for example if the file shown above is named oyster txt then the command to run the program is opimtimalflow oyster txt the program will read the file and determine approximations to the impact for each node an example of the output for the text file above is given below 
25966,in ecological and other transactional energy matter flow networks accurate quantification of flows between compartments can be difficult and costly for models at steady state or undergoing linear change energy matter conservation together with the steady state condition can be exploited to estimate unknown flows from known ones in compartmental network models some flows are more important than others in terms of their connections to other flows participation in cycles geodesic distance to the environment in the graph theoretical sense and other topological features in respect to estimating unknown flows such importance differences also come into play pursuing this we formulate a link importance index linx that quantifies each flow s importance in a model this index identifies and quantifies the redundancy imposed by network topology and mathematical conservation rules we anticipate that it will find use in minimizing the cost and effort of data collection while also increasing model accuracy keywords energy matter budget network model systems modeling input output analysis link importance optimization software availability software name linx developer caner kazanci kelly j black system requirements linux mac os windows program language matlab c availability matlab file exchange https www mathworks com matlabcentral fileexchange 72143 linx github https github com kellyblack linx license gpl 3 0 1 introduction in recent decades of ecological and other applied complex systems modeling numerous methods have been developed to quantify flow networks representing qualitative food webs a typical approach to determining flows begins with a literature search for observational or experimental studies providing data for computation or estimation of model flows such as jørgensen et al 1991 that provide an annotated compendium of process rates with references may be quite useful if the literature does not provide adequately reliable data the next step often entails gathering empirical data at the model field site in the food web field methods such as gut content analysis stable isotope analysis pacella et al 2013 phillips and gregg 2003 post 2002 and fatty acid composition iverson et al 2004 have been used to quantify dietary composition for use in network flow quantification given the difficulty and high cost of gathering empirical data yodzis and innes 1992 methods based on inference and computation have been described one such approach uses allometric principles derived from empirical observation of organism physiology and biochemical rate processes to calculate hypothetical flows in steady state networks barnes et al 2014 this method has deep roots in ecology and finds its greatest recent development in what has become known as metabolic theory in ecology brown et al 2004 sibly et al 2012 some network quantification procedures are based on community assembly rules in which algorithms are used to build network structure and determine flows fath et al 2007 these methods include a modified niche model halnes et al 2007 and a structured food web approach with network flows drawn from a probability distribution morris et al 2005 ulanowicz and scharler 2008 describe two network quantification methods joint apportionment and reverse mold filling that require minimal mathematical inference and avoid the complexities of optimization routines ecopath with ecosim christensen and walters 2004 uses some of these methods to help construct and quantify a network of species interactions finally as flows or ranges thereof are determined the model must be checked for consistency since conservation laws provide constraints which the measured flows may not satisfy specifically the difference between the total input into and output from each compartment should be within expected range of values for smaller systems this might be done by hand using expert knowledge of the system but mathematical optimization schemes such as linear inverse modeling lim introduced into ecology by vézina and platt 1988 have been developed considerably and applied in many ecological contexts breed et al 2004 saint béat et al 2018 marquis et al 2007 vézina and pace 1994 van oevelen et al 2010 savenkoff et al 2001 despite the existence of extensive literature focusing on computational methods that help quantify flows in network models there is as yet no methodology to help guide and optimize the process of gathering empirical data approaches like lim and balancing methods are helpful after empirical data collection but are not designed to interact with the collection process itself the same ecological principles and computational ideas that make such computational methods possible can be exploited to help optimize empirical data collection to lower the cost and effort as well as to increase accuracy the methodology developed herein originated out of the observation that for steady state network flow models certain quantified flows may provide more or less information about other unquantified flows the difference is determined solely by the location of a flow within the network structure given a list of compartments nodes vertices and connections links flows the method works by assigning importance values to each connection the higher the importance value the more the information gained by the quantification of that flow our method could be used in the context of any of the network flow quantification approaches described above where incomplete flow quantification leads to the question which set of remaining unquantified flows should become focal for further empirical or analytical quantification furthermore it is noteworthy that certain flow magnitudes may provide information that results in a more stable calculation of the remaining unquantified flows as well see section 6 that is to say a small error in one measurement may propagate in different ways compared to the measurement of another flow in its place one of the goals of our method is to determine which flows result in a lower potential variance of the calculations of the remaining flows in the presence of measurement errors applying our method in this way could improve the efficiency of model making and aid in deploying research efforts in the construction of ecosystem flow networks mathematical formulation of the link importance index linx of this paper involves linear algebra however the general idea motivating it can be described verbally as is done in section 2 using a three compartment food chain model a slightly more complicated model is needed to show the relevance and usefulness of a linx and understand how it works we use a simple model with three compartments and five flows to do this in section 4 after covering necessary assumptions and notations in section 3 a general linx formulation is derived in section 5 and refined and finalized in section 6 section 7 includes a demonstration using the oyster reef ecosystem model dame and patten 1981 with emphasis on practical considerations such as data availability computational feasibility is discussed in section 8 followed by the conclusion section 9 2 flow importance index the idea in a network model at steady state not every flow needs to be quantified empirically because the steady state assumption introduces constraints in fig 1 for example determining any one of the four flows is sufficient to quantify the remaining three flows because the steady state condition forces all the flows to be equal their importance values are also equal because the amount of information each provides about the others is identical because each flow determines all the others a chain model is too simple to fully motivate a generalized importance index concept the example in section 4 will demonstrate using a more complicated network how some flow values can be more useful than others in determining unknown values in that section we will introduce the link importance index linx that will quantify the usefulness of each of the flows flows with higher linx values will give more restrictions on the whole system than those with lower indices we preferred link importance index to flow importance index because the mathematical formulation of linx is independent of the flow values it is based purely on network topology the structure and organization of connections or links among compartments linx does not only help identify the most helpful flows to build a complete model but also provides information about model accuracy in theory determining all four flow values in the model shown in fig 1 would be totally unnecessary and redundant as the values must all be equal in broader practice however depending on the amount of error involved in determining an individual flow averaging the four measured flow values in this example should increase accuracy because it effectively quadruples the sample size in general a higher linx value will imply that accurate measurement of that flow will contribute significantly more to the overall model accuracy than a flow with lower linx value in situations where restrictions on cost and effort limit the utilization of highly accurate measurement methods for the entire flow data the guidance provided by the flow importance index can be quite useful the source of the extra information provided by the measurement of certain flows is due to the steady state assumption that the total amount of input received by a compartment equals its total output per unit time in fact linx is still applicable to certain models not at steady state appropriate mathematical representation of a flow network is necessary to formulate the effect of the steady state assumption and how it can be relaxed in linx usage as next discussed 3 notation and the steady state assumption it is common to represent quantified flows in steady state multi compartment models by matrices the representation of the flow orientation differs in literature for instance patten 1978 employs a columns j to rows i flow orientation f j i to represent the flow from compartment i to j ulanowicz 1986 represents the same flow using a rows i to columns j flow orientation t i j since the environment is not represented as a compartment in standard network analyses e g patten and ulanowicz environmental inputs into or outputs from compartments are not included in the flow matrix but are represented separately by vectors the environmental input into and output from compartment i are denoted by z i and y i respectively by patten 1978 ulanowicz 1986 denotes environmental inputs as d i and distinguishes two categories of environmental outputs usable export e i and unusable respiration r i the mathematical representation of the steady state assumption is significantly simpler when all flows including the environmental inputs and outputs are enumerated and represented in vector form instead for that reason vézina and platt 1988 forego the use of matrices entirely and instead use a vector r where r i represents the i th flow we adopt this convention for present purposes however we use a different letter f i in keeping with the notation of patten the information imposed by the steady state assumption on a food chain f i f j for all i j is quite simple which is not necessarily the case for a general ecosystem model in order to accurately identify the information imposed by the steady state assumption on a general ecosystem model we need a formal way to represent how flows and compartments are connected to each other for a given ecosystem model in graph theory the incidence matrix is used for this purpose since open ecosystem models are not really graphs 1 1 for open ecosystem models each environmental input and output is connected to a single compartment as the environment is not included within the boundaries of the model for a graph however each flow edge needs to be attached to exactly two compartments vertices by definition hypergraphs do not have this restriction also called set systems hell and nesetřil 1970 they are significantly more general mathematical constructs than graphs and therefore not the most suitable abstraction for the ecosystem models we focus on we use a generalized version of the incidence matrix called the stoichiometric matrix which is the same as the incidence matrix for closed systems with no environmental inputs or outputs the stoichiometric matrix s eq 1 along with flow vector f provides the simplest mathematical representation of the conservation laws eq 3 for a model with n compartments and k flows including the environmental inputs and outputs the stoichiometric matrix resendis antonio 2013 has n rows and k columns and is defined as follows 1 s i j 1 if flow f j is to compartment i 1 if flow f j is from compartment i 0 neither for example the stoichiometric matrix of the three compartment food chain in fig 1 is 2 s 1 1 0 0 0 1 1 0 0 0 1 1 f f 1 f 2 f 3 f 4 where flows and compartments are ordered as f 1 f 2 f 3 f 4 and a b c respectively the vector f contains flow magnitudes ordered in the same way as the columns of the stoichiometric matrix for example the second column of s contains 1 on its first row and 1 on its second row representing the flow from the first compartment a to the second b in other words when a unit flow occurs from a to b there will be a unit decrease in a a unit increase in b and no change in c thus the second column of s 1 1 0 corresponds to the second flow f 2 the fact that a model is at steady state can be represented in terms of the matrix s and the vector f as 3 s f 0 for example for the three compartment food chain in fig 1 this simple equation yields the correct steady state conditions as follows s f 0 f 1 f 2 0 f 2 f 3 0 f 3 f 4 0 f 1 f 2 f 3 f 4 in general for an n compartment model the steady state assumption provides n linear equations involving flow values that can be exploited to determine unknown flows without needing to quantify all of them while the steady state assumption is essential for the method to work it is not absolutely needed the same approach still works if the change in compartment storage values can be assumed to occur at a constant rate for example if a certain compartment gains or loses approximately a certain amount of storage per unit time over a time period then all the methods presented in this paper remain applicable during that time period in general if the rate of change of the storage value of compartment i is given as c i then s and f satisfy the following equation instead of eq 3 s f c 1 c n this does not apply to systems where storage values fluctuate significantly which can be characterized by the high values of the magnitudes of the second derivative of the storage values over the observed time period we will continue to use the phrase steady state assumption throughout this manuscript for simplicity and clarity nevertheless the entire content of this manuscript remains valid for models where changes in storage values can be assumed to occur at constant rates 4 flow importance index a simple example the three compartment food chain model fig 1 is too simple an example to demonstrate how a link importance index is to be formulated in general material covered in section 3 enables us to demonstrate how the flow importance measure is computed for the simple three compartment model shown in fig 2 unlike the previous example no single flow value is enough to determine all five flows in this model we need to find the minimum number of flows that need to be quantified in order to compute all flows in this system for this consider the steady state equations for each compartment a f 1 f 2 f 3 4 b f 2 f 4 f 5 c f 3 f 4 this linear system has five variables degrees of freedom and three equations restrictions leaving two degrees of freedom this means if we fix two of the flows the remaining three should be uniquely determined that is quantifying only two flows will usually suffice to determine all five flows of this model to do this explicitly we rewrite the steady state equations 4 using the stoichiometric matrix as shown in eq 3 5 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 f 1 f 2 f 3 f 4 f 5 0 0 0 to demonstrate we can randomly choose two of the five flows and compute the remaining three based on the two we chose as an example we choose f 4 and f 5 and compute f 1 f 2 and f 3 in terms of f 4 and f 5 to do this we need a system of equations that expresses f 1 f 2 and f 3 in terms of f 4 and f 5 this follows from eq 5 1 1 1 0 1 0 0 0 1 f 1 f 2 f 3 0 0 1 1 1 0 f 4 f 5 here the 3 3 matrix on the left is made from the first three columns of the stoichiometric matrix 4 that correspond to the unknown flows f 1 f 2 and f 3 and the 3 2 matrix on the right is formed from the remaining columns of s that correspond to f 4 and f 5 then assuming the 3 3 matrix on the left is invertible we get the equations we need to compute f 1 f 2 and f 3 demonstrating that quantifying f 4 and f 5 is sufficient to determine all flow rates 6 f 1 f 2 f 3 1 1 1 0 1 0 0 0 1 1 0 0 1 1 1 0 f 4 f 5 f 5 f 4 f 5 f 4 this exercise informs us about the necessary and sufficient condition that the matrix formed by the columns of s that correspond to the unknown flows should be invertible if this matrix were not invertible it would be impossible to derive the equations we need implying that it would be impossible to uniquely compute f 1 f 2 and f 3 using f 4 and f 5 to demonstrate such a case if we try to find f 2 f 3 and f 4 in terms of f 1 and f 5 we get 7 1 1 0 1 0 1 0 1 1 f 2 f 3 f 4 1 0 0 1 0 0 f 1 f 5 in this the 3 3 matrix on the left is not invertible meaning that knowing f 1 and f 5 is not sufficient to determine f 2 f 3 and f 4 the intuitive reason for this is that measuring both f 1 and f 5 is redundant as the steady state assumption implies that f 1 f 5 similarly f 3 f 4 any other choice of two flows however can be used to determine all five flows in the model the goal of linx is to rank the amount of information on other flow values gained by quantifying a specific flow the extra information is afforded by the steady state assumption and the amount of this information depends on the location of the compartment within the network and the entire network topology as an example for the simple food chain model of fig 1 measuring any one of the four flows does provide the values of the three remaining flows as well effectively quadrupling the information provided by a single measurement this unusually high gain is due to the steady state condition combined with the fact that each compartment has exactly one input and one output forcing all flows to be equal to each other linx indicates that for this model quantifying any one of the flows provides the same amount of information assessing the contribution of a single flow for the three compartment model in fig 2 is more complicated as at minimum two flows are needed to compute all five flows only if we quantify a flow in question and another flow will it then be possible to compute all other flows given that the matrix formed by the columns of the stoichiometric matrix corresponding to the remaining three flows is invertible 6 if that matrix is not invertible 7 then those two flows alone will not be enough to determine all flows in the system based on this observation we define linx as the proportion of the cases that makes it possible to compute all five flows when a flow in question is paired with another flow this statement is generalized 8 formulated 10 and improved 13 in the following sections as an example we compute the flow importance index of f 3 there are four different sets of two flows that contain f 3 and one other flow f 1 f 3 f 2 f 3 f 3 f 4 f 3 f 5 considering the invertibility of the resulting matrices formed by each of these sets eqs 6 and 7 we see that all five flows in the model can be computed by measuring the flows in any of these sets except for f 3 f 4 in other words out of the four flow sets that have the potential to compute all five flows only three of them can be used to do so therefore we define the flow importance index of f 3 as 3 4 we can compute the importance indices of all flows by first making a list of all sets containing two different flows the two cross outs contain redundant flows one can use the two flows in any of the eight remaining sets to compute all five flows we define the link importance index linx m f i as the fraction of sets with two elements including f i that can be used to quantify all flows previously we observed that out of the four sets containing f 3 only three can be used to quantify all flows the same statement is valid for f 1 f 4 and f 5 however f 2 is different in that all four groups containing f 2 can be used to quantify all flows the flow importance indices for fig 2 model are m f 1 m f 3 m f 4 m f 5 3 4 0 75 m f 2 4 4 1 the value of m f 2 is higher than the others because once we know the value of f 2 quantifying any one of the remaining four flows is sufficient to derive all flows this is not true for any of the other flows implying that quantifying f 2 before others or more accurately than others is advantageous which is the goal of linx this section has illustrated how linx is computed for a simple model in the next section we derive a preliminary general mathematical formulation for linx and in section 6 we improve this formulation to give the full definition of linx 5 linx preliminary general formulation to construct a preliminary general formulation for linx we need first to find out the minimum number of flows needed to determine all flows in multi compartment models in general for an n compartment model which always contains k n flows including environmental inputs and outputs at least k n flows must be quantified to determine all flows this is because the steady state condition forms a linear system with k variables and n equations leaving k n degrees of freedom see appendix a for proof however as in the above example not every set of k n flows will work this observation leads to our preliminary formulation that is given by 8 m f i number of flow sets of size k n including f i sufficient to determine all flows number of flow sets of size k n including f i we formulate the denominator first the number of sets containing k n distinct flows is the number of subsets of the set of all flows f f 1 f k containing k n elements this in turn is the number of combinations of k flows taken k n at a time k k n k k n n since f i has to be one of the elements we need to count the combinations of the remaining k 1 flows taken k n 1 at a time which is k 1 k n 1 n k 1 n next we formulate the numerator which is the number of the sets counted in the denominator that can be used to determine all the flows for instance to compute m f 3 for the model depicted in fig 2 of the previous section first identify all sets containing two flows one of which is f 3 f 1 f 3 f 2 f 3 f 3 f 4 f 3 f 5 the number of such sets is 5 1 3 4 which is the denominator of the linx definition in eq 8 the numerator of the preliminary linx definition only counts the sets that allow the computation of all flows to find out if this is the case for a given set we need to form the linear equations that express the undetermined flows in terms of the quantified flows as we did in eqs 6 and 7 for example let us derive the system of linear equations for the first set a f 1 f 3 the steady state condition s f 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 f 1 f 2 f 3 f 4 f 5 0 0 0 this linear equation should express the unknown flows f 2 f 4 f 5 in terms of the known flows f 1 f 3 and test if a unique solution exists in order to do so we need to divide the matrix s and vector f into two parts accordingly 9 1 1 0 0 0 1 f 1 f 3 1 0 0 1 1 1 0 1 0 f 2 f 4 f 5 0 0 0 in general let f f 1 f 2 f k represent the set of all flows for a subset a f we define f a as a vector containing only the flows in a similarly we define s a as a matrix formed by the columns of s that correspond to the flows in a then a general version of eq 9 is s a f a s f a f f a 0 where the set f a contains all flows that are not in a i e the complement of a for our specific example f a f 2 f 4 f 5 then the general form of the linear equation that represents the unknown flows f a in terms of the known flows a is f f a s f a 1 s a f a for our specific example this equation is f 2 f 4 f 5 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 f 1 f 3 which can be simplified as f 2 f 4 f 5 1 0 0 1 1 1 0 1 0 1 f 1 f 3 0 f 3 the flows f 1 f 3 can be used to determine all five flows if this equation is solvable which only possible if the shown 3 3 matrix is invertible in general the flows in set a can be used to uniquely determine all flows f if and only if the matrix s f a is invertible in other words det s f a 0 this observation is all we need to construct a general formula for m f i which is the proportion of all flow sets of size k n containing f i which can be used to compute all flows 10 m f i 1 k 1 n a f f i a a k n sgn det s f a in the expression under the sum the symbol a represents the number of elements cardinality of a thus the sum is over the subsets a of f that have k n elements and contain the i th flow f i the number of such sets is k 1 n which is the denominator for the numerator we should count only the sets a with det s f a 0 we do that by taking the absolute value and sign function of the determinant so that it equals one if this condition is satisfied and zero otherwise 11 sgn det s f a 1 if s f a is invertible 0 otherwise by this preliminary definition linx always takes values between zero and one if the linx value of a flow is one then it would certainly be a good idea to quantify that flow first whereas flows with low linx values should be the last ones to be quantified for most well connected ecosystem models linx will usually be less than one half unlike the values we observed for the simple models we used for demonstration 6 linx improved formulation computation of unknown flows based on quantified flows requires the solution of a linear system of equations u x v where v is a vector based on quantified flows u is a matrix based on the model s network structure and x represents the unknown flows to be determined for the three compartment model shown in fig 2 one of these linear equations is 12 1 0 0 1 1 1 0 1 0 u s f a f 2 f 4 f 5 x f 3 f 1 0 f 3 v one issue with the solution of such equations is the propagation of error from the quantified flows f 1 f 3 to the computed solution f 2 f 4 f 5 in ideal conditions one might naively expect that 10 error in f 1 f 3 would imply a 10 error on f 2 f 4 f 5 in reality the error in f 2 f 4 f 5 can be as high as but cannot exceed 37 3 this error amplification factor of 3 73 of a linear equation system is determined by the condition number of its matrix u which is computed as the ratio of its maximum singular value to its minimum singular value beezer 2015 an n n linear system will be under determined and will not have a unique solution if the corresponding matrix u is not invertible the condition number of a non invertible matrix is meaning that any small error can be infinitely magnified matrices that are very close to being non invertible have high condition numbers meaning that the solutions they provide have high potential to magnify the errors in quantified empirical data this fact is relevant to linx as it is based on the ability of a given flow when combined with others to determine all the flows in a model determining the entire flow data in a model requires the solution of a linear system like eq 12 the preliminary definition of linx 10 is entirely based on whether or not this linear system is solvable 11 but not how accurate the resulting numerical approximation is for the whole system in other words it does not factor in how the existing error in quantified flows will propagate to the computed flows if for a given set of flows the condition number is high the computed flows will not be nearly as reliable decreasing the value of the additional information provided by quantifying those focal flows accurate assessment of this additional information is precisely the goal of linx therefore the additional information provided by the condition number should be factored into its formulation this is relatively straightforward to accomplish original m f i 1 k 1 n a f f i a a k n sgn det s f a 13 revised m f i 1 m a f f i a a k n 1 κ s f a m max j a f f j a a k n 1 κ s f a the improved formulation is different in two ways first the expression inside the sum is replaced with the multiplicative inverse of the condition number κ of the relevant matrix the expression sgn det s f a takes the value 1 or 0 depending on the matrix s f a being invertible or not respectively if s f a is not invertible then 1 κ s f a 1 0 as well matching the preliminary definition unlike the preliminary definition however 1 κ s f a often takes values significantly lower than one if s f a is invertible for example table 1 shows these values for the three compartment model in fig 2 for the three sets with non zero determinants the inverse of the condition numbers are 0 25 0 28 and 0 38 this poses a problem as just replacing the expression inside the sum the improved importance index would take the value 0 28 0 38 0 0 25 4 0 23 whereas the preliminary formulation gives 1 1 0 1 4 0 75 to compensate for the characteristic low values of the improved formulation we do not divide the sum by the number of sets k 1 n but by the maximum such sum this way the preliminary and improved importance indices achieve comparable values as shown on table 2 2 2 actually for large systems the preliminary linx will also generally produce importance indices significantly less than one thus in general to compare the preliminary index with the improved index it would be best to scale both of them so that the largest importance value is 1 while the proportion of the values are roughly preserved the additional information provided by the condition numbers placed the flows f 3 and f 4 ahead of f 1 and f 5 whereas all four flows had the same importance value according to the older definition fig 3 shows a more realistic comparison of the improved versus preliminary linx formulations using a classical ecological energy flow model odum 1957 kemp and boynton 2004 this model has n 5 compartments and k 14 flows so there exist 14 1 5 1287 flow groups containing any specific flow in fig 3 the thickness of a flow line indicates its importance the clear difference between the two diagrams demonstrates the significant effect of condition numbers on the importance of flows which was not observed in the simple three compartment model table 1 even though the silver springs model is small in size its slightly more complicated network topology compared to the three compartment model causes a difference significant enough to change the order of importance of flows when the improved formulation is used for example according to the preliminary definition the environmental output of the carnivores compartment carnivores environment is more important than the flow from carnivores to detritus carnivores detritus on the other hand it is exactly the opposite when the improved formulation is used even though the improved formulation is slightly more resource intensive to compute from here on we will employ it by default as it provides a more accurate measure of link importance the following two sections discuss several issues that may arise in using linx for model building refining or tuning we address some of these with a view to effective application of the methodology 7 practical considerations data availability quantifying a flow not only informs about its value but values of other flows as well because of the steady state assumption linx simply computes the amount of this additional information to determine the importance of each link this information cannot be accurately computed for flows about which prior information exists from previous observations experiments or literature such empirical information will render linx information partially useless also known flow values will provide information on other flows as well which may significantly alter linx values it is not just known flow values that affect linx values if a model contains flows that are difficult to measure accurately 3 3 an ecological example of a difficult to measure flow would be a feeding flow for a rare predator that is known to exist in the model but is so rare that obtaining a consumption rate for it is nearly impossible even considering the range of empirical methods available such as direct observation or gut content analysis or indirect methods such as metabolic estimates or stable isotope analysis then any additional information that can help quantify those flows will be commensurately valuable such difficult to measure flows can have a significant effect on the linx values as well this section explains how linx is computed in these two situations using the three compartment model of fig 2 and illustrates the ideas using an intertidal oyster reef ecosystem model fig 4 we consider three scenarios that can arise about flow f 1 in determining linx values in fig 2 model 1 f 1 is already known via pre existing data or because it is the focus of the research question and so have a high importance by default 2 it is not feasible to determine f 1 empirically and no data for it exists and 3 f 1 is known and it is not feasible to determine f 2 accurately combination of the first two cases for simplicity in this example only we will use the preliminary linx formulation it is possible to compute the linx values using the preliminary formulation simply by counting whereas the condition numbers of several matrices need to be computed in the case of the improved formulation in practice the improved formulation is preferred due to its accuracy therefore it will be used for the intertidal oyster reef model analysis presented at the end of this section fig 4 recall that we defined m f i for the three compartment model in section 4 as the fraction of sets with two flows k n 5 3 2 including f i that can be used to quantify all flows listing all flow sets with two elements and striking out the two unusable ones section 4 we computed all five linx values to be m f 2 1 and m f i 0 75 i 1 3 4 5 for the three scenarios listed above the importance index can be constructed using this same basic idea table 3 lists the relevant sets for each of the three different scenarios as well as the importance indices computed using them for the first scenario f 1 is already known so it should participate in all flow sets of size two which reduces the number of such sets from ten to only four it is exactly the opposite in the case of the second scenario where f 1 cannot participate in any of the sets since it is assumed that it is not feasible to quantify f 1 using direct methods such as field experiments or literature review scenario 3 is a combination of the previous cases so f 1 should be in all sets and f 2 cannot be in any of the sets the computation of importance indices of all flows is done according to the same definition above using the preliminary formulation and results are shown on table 3 comparing the linx values for each of the three scenarios we observe some similarities as well as significant differences for example the linx value f 3 originally m f 3 0 75 changes to 1 0 67 and 1 for each of the three scenarios respectively the linx value of f 5 on the other hand originally m f 5 0 75 changes to 1 0 and 1 for each of the three scenarios respectively drastic changes in m f 5 under different scenarios show that prior information and data availability can affect importance of flows significantly and hence should be taken into account this observation is not specific to this small and simple model and the preliminary formulation of linx fig 4 shows four different prior information and data availability scenarios for an intertidal oyster reef ecosystem using the improved formulation of linx 8 practical considerations computational feasibility in this section we describe how to compute the linx values using matlab and then discuss issues of performance and feasibility a matlab code that automatically computes the linx values given a model s stoichiometric matrix is available at github kazanci and black 2020 and matlab central file exchange kazanci 2020 this code is compatible with the freely available gnu octave software eaton et al 2014 as well as matlab for instance one can compute the linx values for the three compartment model of fig 2 using only its stoichiometric matrix eq 5 as follows the results match the ones provided in table 2 use of the stoichiometric matrix for ecosystem modeling is not as common as the adjacency matrix a second matlab code named a2s m is available with linx m that automatically converts a given adjacency matrix that includes environmental inputs and outputs to the stoichiometric matrix therefore the same results shown above can be obtained by executing the command linx a2s a where a is the adjacency matrix of the model it is also possible to use the same code in case of prior information or if some flows are difficult to obtain for instance the linx values corresponding to the three scenarios provided on table 3 can be computed as follows by simply listing the flows that are known or are infeasible to compute these results are slightly different than provided on table 3 because in the latter case the preliminary formulation of linx was used for its simplicity the results presented here use the improved formulation figs 3 and 4 are automatically generated using graphviz graphviz uses a text file with dot extension to create images in multiple formats a matlab file named linxdiagram m that automatically generates this text file which then can be used to create a network diagram is provided at github kazanci and black 2020 and matlab central file exchange kazanci 2020 one limitation of this code is its slowness for large models this is because all possible combinations of flows must be examined which is a computationally expensive process it is possible to make use of the row reduced echelon form rref of the stoichiometric matrix to limit the number of combinations to be tested this more complicated algorithm scales better for larger models and further details are provided in appendix b a faster c code based on this algorithm is provided along with the matlab code at github kazanci and black 2020 in the case of extremely large models even the faster c code might not be feasibly utilized it is not possible to provide a specific threshold however as network topology and prior flow information section 7 have a significant effect on speed in general models with high connectances dunne et al 2002 are expected to run slower and models with prior information will definitely run faster for example the number of flow combinations that needs to be tested for the oyster reef ecosystem model shown in fig 4 is 27132 under the three different scenarios shown on the same figure the number of combinations decrease to 5005 105 and 1365 for cases b c and d respectively in other words knowledge that four of the nineteen flows are unlikely to be determined empirically makes the algorithm about 270 times faster in theory in practice the oyster reef model is small enough that all cases run almost instantly on a regular personal computer this happens to be case for models of similar size the difference can be drastic for large or complex models however model size and complexity are decidedly limiting factors in the applicability of the present linx methodology to realistically complex systems studies such as those undertaken in large continuing studies like the us national science foundation s long term ecological research lter and national ecological observatory network neon programs even at such ambitious research scales our linx methodology could provide guidance for targeting empirical research to quantify flows in incomplete model flow networks as they are created quantified and revised over the life of projects in most cases large complex ecosystem models are the result of long term team based research projects and empirical research would provide prior information on some of the model flows since prior information often provides for feasible application of the linx algorithm many of these models could use this methodology large models typically contain model sectors which are groups of compartments that share many characteristics such as microautotrophs microheterotrophs or macroheterotrophs since teams of researchers are often responsible for a certain model sector such sectors might have some flows quantified and could be analyzed with the linx algorithm as sub system matrices as well as the full model being analyzed at various stages in the model building process 9 conclusion computational methods serve as an essential part of compartmental network flow modeling their utilization however usually occurs after parameters have been determined through empirical data collection missing data methods and literature search the computational method of this paper is applied early in the parametrization phase of modeling after some flow data but not all have been acquired exploiting conservation laws and network topology linx enables modelers to make informed decisions that guide the data collection helping to build a more accurate model while minimizing resources necessary to determine flow values the usefulness of linx is not just limited to before data collection as fig 4 demonstrates quantifying four flows produces system wide large changes in the linx values of the subsequent partially quantified flow network in other words the quantification of a few focal flows can dramatically change the linx values of other flows the process described in section 7 can be utilized repeatedly during the model building process and can identify which remaining flows are going to be redundant eliminating further costly and potentially unnecessary empirical measurements furthermore modelers may utilize redundant flows to increase the accuracy of other flows that may contain relatively greater errors in other words they can use linx to identify which specific flows will help increase the accuracy of the others finally the significant effect of prior knowledge about specific flows on the linx values of other flows reflects on the organization of large complex self assembling systems like ecosystems from the way our present algorithm works it is possible to infer that the importance structure of flows in ecosystems is very fluid and changing indeed in response to other network flows extant or not extant at any given time or place credit authorship contribution statement caner kazanci conceived the ideas developed and refined the formulation software writing original draft malcolm r adams conceived the ideas developed and refined the formulation writing original draft aladeen al basheer conceived the ideas developed and refined the formulation writing original draft kelly j black conceived the ideas developed and refined the formulation software writing original draft nicholas lindell contribution to the proof of the theorem writing original draft bernard c patten writing original draft stuart j whipple conceived the ideas developed and refined the formulation writing original draft acknowledgment all authors participated in the preparation of the publication contributed critically to the drafts and gave final approval for publication appendix a minimum number of flows required to determine all flows theorem 1 the rank of the n k stoichiometric matrix s of any open and connected ecosystem model equals n the number of its compartments in other words its rows are linearly independent and so are n of its columns proof first we note that n k i e there are at least as many flows as there are compartments since the system is connected and open also rank s min n k n by definition assume for contradiction that rank s n then there is a nontrivial linear combination of the rows r m such that a 1 m 1 n c m r m 0 but c m 0 for some m note that each of the k flows is attached to either one or two compartments by construction then each column of s has either one or two nonzero entries since the system is open there is at least one column with only one nonzero entry let the i th column be one of these it corresponds to an environmental flow into or out of some compartment s corresponding to the s th row r s of the matrix this flow is not connected to any other compartment we use induction on the length of the shortest path from compartment b to s to arrive at a contradiction as follows the i th entry of every row but r s is zero necessitating c s 0 inductively assume any compartment a with shortest path length l from s has c a 0 let compartment b have distance l 1 to compartment s certainly b must be connected via flow j to some a which satisfies the inductive hypothesis only r a and r b have nonzero entries in column j so that c a 0 c b 0 also the system is connected whence every coefficient is zero by the induction above contradicting c m 0 for some m thus rank s n remark this theorem shows that there exist collections of n flows such that the corresponding columns of s are linearly independent and thus the n n matrix formed by these columns is invertible however as evident from examples in the paper it is often not the case that every collection of n columns yields an invertible matrix appendix b a faster algorithm the minimum number of flows required to determine all flows is established within the main body of the primary paper one immediate task is to determine an algorithm to determine which combinations of the flows determine the feasible sets in other words which set of n flows can be used to calculate all of the remaining flows we will call these sets acceptable here we define two methods for searching the acceptable sets of flows the first is a brute force method and the second makes use of the row reduced echelon form rref of the stoichiometric matrix to limit the number of combinations that are tested codes for both approaches are publicly available kazanci and black 2020 the first method is to simply examine all possible n element sets of flows an advantage of this method is it makes use of standard libraries readily available to calculate the combinations to test another advantage is that it is complete and robust every combination is tested and none will be missed the shortcoming is that the algorithm does not scale well to larger systems and a system with a larger number of nodes may require a long time to search through all possible combinations the approach to examine all possible combinations is a computationally expensive process so another approach is explored that is designed to reduce the operation count the primary disadvantage of the alternate method is that its implementation is more complicated the basic idea is that the rref of the stoichiometric matrix can be used to more narrowly define which columns of the stoichiometric matrix are linearly dependent to better describe the approach we first describe the problem given in section 3 of the original paper as a reminder the linear system for the example can be expressed in the form b 1 s 1 1 0 0 0 1 1 0 0 0 1 1 f 1 f 2 f 3 f 4 0 we note that eq b 1 represents a system in the form b 2 s f 0 in general the matrix s has n rows and k columns the vector f has k rows and the zero vector on the right hand side has n rows an alternate representation for the system is to express s in terms of its columns and the vector f as b 3 s 1 s 2 s k f 1 f 2 f k 0 here the column vector s i represents the entries in the stoichiometric matrix corresponding to whether or not there is an inflow or outflow for node i for a given flow the system can be expanded to express it in an equivalent form b 4 f 1 s 1 f 2 s 2 f k s k 0 the goal is to test all sets of n linearly independent vectors from s i the linear system given in eq b 2 can be put in row reduced echelon form this will transform the system where the s i can be more easily compared as a way to rule out which columns might be expressed in terms of another column we first examine the example system given in eq b 1 the system with the rref of the matrix is given by 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 f 0 the task is to determine all sets that contain three column vectors that represent a linearly independent set the first thing to notice is that the first row in the rref matrix implies that s 1 and s 5 must be equal unless there is a nonzero entry in s 5 after the first row then there is no need to check a set of vectors that have both s 1 and s 5 the search for the first vector in the candidate sets can be conducted by using the first row and searching all sets where the first vector is s 1 or the first vector is s 5 given these two initial vectors for a candidate set the second row can then be searched because there are zeros in the second row for columns 1 and 3 a set of candidates for the second vector in the acceptable sets include the vectors s 2 s 4 or s 5 finally based on the third row the only non zero entries are in columns three and four so candidates for the acceptable sets include those sets where the third vector is either vectors s 3 or s 4 the general algorithm makes direct use of the row reduced echelon form of eq b 2 the recursive algorithm proceeds by first testing sets where the first vector is one of columns that has a non zero coefficient in the first row of the rref form of the stoichiometric matrix each column in the second row that has a non zero coefficient is then included for the second element of the possible sets the algorithm then proceeds through each of the following rows until all possible sets of the columns have been tested compared to the exhaustive method described above this algorithm reduces the number of columns to test the expense though is the use of a recursive algorithm that adds additional overhead another downside of the algorithm is that it requires additional book keeping and checking to insure that previously used combinations are not reused the c code requires that the stoichiometric matrix be defined in a regular text file the first lines in the text file consist of the stoichiometric matrix each line is a row in the matrix and the numbers are separated by spaces two optional lines can be appended below the stoichiometric matrix one line can be used to define which flows are known in advance known the other line can be used to specify which flows cannot be measured unknown an example of a file is given below where it is assumed that the flows for nodes 1 3 and 6 are known while the flows for nodes 2 5 and 11 cannot be determined when the c program is run the name of the program optimalflow is used to start the program the name of the file to read is provided on the command line after the program name for example if the file shown above is named oyster txt then the command to run the program is opimtimalflow oyster txt the program will read the file and determine approximations to the impact for each node an example of the output for the text file above is given below 
25967,hurricanes in the southeast united states are infrequent disturbances that affect large areas and have a large effect on forest succession in order to understand and quantify this effect we added a new module to the landis ii landscape change model focusing on the southeast coast of the united states we simulated stochastic hurricanes for 50 years for each simulated storm the new model extension generates the maximum sustained wind speed over the region and uses the resulting parameter surface to compute maximum sustained wind speed for each cohort cell in a raster grid mortality is estimated for each species and age cohort in each cell based on the maximum sustained wind speed altering forest succession results indicate that hurricanes reduce average aboveground biomass by 20 over 50 years on a landscape in fort bragg north carolina usa compared to a scenario without hurricanes and increased uncertainty of projected succession keywords hurricanes disturbance forest mortality landis ii 1 introduction forest disturbances cause mortality chambers et al 2007 hicke et al 2012 change the direction of succession turner et al 1998 alter ecosystem service delivery thom and seidl 2016 and create substantial uncertainty that challenges forest management predicated on consistent growth and yield kurz et al 2008 in the southeastern region of united states hurricanes are an infrequent but catastrophic disturbance that alter forest succession in the western united states wildfire and insects hicke et al 2015 cause the most mortality and have recently surged due to climate change related increases in temperatures and reductions in snowpack westerling et al 2006 in boreal regions wildfires have been increasing in size and intensity over the past 20 years walker et al 2019 across the southeastern us harvesting of saw timber is the dominant source of recurring mortality fagan et al 2018 although hurricanes generate substantial localized mortality for example in south carolina in 1989 hurricane hugo damaged 1 8 million ha of forest an area larger than the state of connecticut hook et al 1991 in addition hurricane frequency and intensity bender et al 2010 bacmeister et al 2018 may increase as the climate warms adding additional uncertainty to forest change and management dale et al 2001 chambers et al 2007 the effects of hurricanes are similar to that generated by downbursts derechos and tornados across the midwestern and northeastern us peterson 2000 frelich 2002 lucash et al 2018 although vastly exceeding those events in their spatial extent typically a hurricane that makes landfall in the southeastern us will disturb 50 100 000 km2 foster et al 1998 hurricanes play a critical ecological role in maintaining structure function and fuel dynamics in longleaf and other fire dominated coastal systems myers et al 1998 chambers et al 2007 xi et al 2008a b o brien et al 2008 cannon et al 2017 similar to other wind events the tallest trees are most vulnerable to uprooting or snapping boose et al 1994 interspecific differences also exist although they are less well quantified busing et al 2009 cely 1989 hook et al 1991 sharitz et al 1992 xi et al 2008b within the zone of hurricane mortality the effects can vary from near total mortality for example the areas adjacent to the landfall of hurricane michael in 2018 beven et al 2019 to sparse mortality of a few trees per ha where mortality is greatest there will be a loss of economic value haight et al 1995 long term release of forest carbon as dead trees decay mcnulty 2002 and changes to long term successional trends for example if a site is dominated by conifers with a hardwood understory a common situation where fire has been excluded for extended periods in the southeast glitzenstein et al 2003 moderate hurricane winds may accelerate succession towards hardwood dominance on the contrary if a site dominated by shade intolerant conifers experiences near total mortality conifers will likely reestablish given high light conditions we developed a hurricane model the base hurricane extension that is an extension to the landscape change model landis ii scheller et al 2007 to incorporate the simulation of hurricane wind mortalities into landis ii our goal was to forecast long term interactions between forest disturbance and prescribed fire across large extents previous work has emphasized fine scale interactions of hurricanes and forest mortality boose et al 1994 we therefore developed a model of hurricane mortality and hurricanes effects that interfaces with existing models of succession management and other natural disturbances e g insect mortality we focused on ft bragg north carolina as it is broadly representative of southeastern forests falls within the zone of hurricanes and forest managers are concerned about the potential for hurricanes to interfere with their long term management goals fort bragg is a us army military reservation in the sandhills region of north carolina a us commerce department shapefile indicates the area of the base is 61 728 4 ha the raster model used in this research included 45 947 ha of forested area 2 methods first we describe the conceptual hurricane model next we describe how the model was specifically parameterized for our study area 2 1 conceptual hurricane model base hurricane is a new extension to the landscape change model which simulates stand level mortalities due to hurricane wind forces in this context base indicates that this extension is compatible with all other extensions landis ii simulates succession disturbance and management across large and heterogeneous landscapes typically 50 000 5 000 000 ha in size incorporating our hurricane model into an existing model of succession disturbance and management allows us to leverage prior efforts to integrate multiple processes across a landscape e g scheller et al 2011 lucash et al 2018 serra diaz et al 2018 all of our code is open source https github com landis ii foundation extension base hurricane landis ii represents trees and shrubs on the landscape as species age cohorts not individual trees each cohort represents multiple individual trees and typically has other associated attributes the landscape is represented as a grid of spatially interacting cells each cell can contain one or more cohorts with an unlimited number of species the processes of succession disturbance and management interact through their effects on cohort composition within each cell the base hurricane extension runs on an annual time step in which it randomly determines the number of storms that make landfall on the southeastern coast of the us for that year this hurricane count is in the range 0 to n where n may be adjusted by the user for our simulations we used a maximum of 3 hurricanes per year the location of each storm is controlled by three parameters landfall latitude storm track direction and maximum sustained wind speed at landfall for each storm the particular value of each of these three is set from a random number generator landfall latitude is the latitude along the coast line where the storm makes landfall see fig 1 storm track direction is an azimuth value between 280 and 360 maximum sustained windspeed at landfall is determined from a log normal distribution as shown in fig 4 the geometry resulting from these three parameters define the parametric surface of maximum sustained wind speed msws see fig 7 base hurricane then computes msws for each site in the study area treating the entire duration of the event as a single moment in model time mortalities are generated stochastically for each raster cell based on the maximum sustained wind speed of the site see below one may consult the base hurricane user guide for complete explanation of all user parameters https github com landis ii foundation extension base hurricane blob master docs landis ii 20base 20hurricane 20v0 1 20user 20guide docx for any given location within a hurricane impact area surface wind speeds vary over the duration of the event in both sustained wind speed and gust speed a storm whose center passes two hundred kilometers away from a location may potentially cause mortality given these factors we elected to represent the wind field as a parametric surface representing msws over the duration of the storm wind induced mortality is computed as a function of the msws cohort age older cohorts are taller and therefore more vulnerable and species some species are more vulnerable than others maximum wind gust and duration of exposure are included implicitly in the species mortality table table 3 without being explicitly modeled this approach reduces computation time compared to a more precise model e g boose et al 1994 specifically actual historic msws is available in the hurdat2 dataset but hectare scale maximum wind gusts are not as the present model is stochastic and not deterministic we elected to model overall probability of mortality with a single random number comparison where a hurricane strikes we anticipate the structural resistance of the soil to overturning is reduced by rainfall saturation for all scenarios likewise where there is a given maximum sustained wind speed wind gusts will be approximately 25 35 higher yet gust direction and length have not been researched and modelling them is not in our scope the general effect is that high winds blow trees down we have synthesized all of these factors to simplify our model in a way which we believe does not reduce accuracy over a large area simulated with a computerized random number generator the base hurricane extension simulates the msws at the center of the storm track the storm track direction is modeled as a straight line at a random azimuth direction proceeding inland from the shoreline the distance from the storm track to the study site is determined from landfall latitude and track direction for each hurricane simulated fig 1 the coastline is represented as linear with a fixed axis after a storm makes landfall and moves inland msws at the center of the storm declines over distance along the storm track as well as perpendicular to the storm track this creates a wind field the geographic distribution of the msws relative to the storm track we used the second derivative of the hyperbola as shown in equation 1 to estimate the msws centerline profile eq 1 m s w s a 2 s l s b a 2 x 2 x 2 a 2 1 s b where msws is the maximum sustained wind speed along the center track km hr 1 a 2 is the distance from landfall to the inflection point of the profile curve km x is the track distance from landfall km sb is the wind speed at final reading km hr 1 sl is the maximum sustained wind speed at landfall km hr 1 the profile shown in fig 2 represents msws along the line in fig 1 labeled x distance along storm track to represent a parametric field surface over a region we also modeled maximum sustained wind speed perpendicular to the storm track again using equation 1 for the cross section i e wind speeds perpendicular to the center sl is taken from msws along the centerline from equation 1 because atlantic hurricane wind speeds are asymmetrical the wind speed decline as a distance from center differs on the left and right sides of the hurricane relative to their primary direction this is expressed as differing values of a for the left and right sides while x represents the distance from the centerline or y in cartesian space the combination of equations produces a parametric surface of the maximum sustained wind speed over the duration of the event the resulting parametric surface is oriented with the x axis following the storm track with its origin at the landfall location the y axis of the parametric surface is perpendicular to the storm track see fig 1 after sustained wind speed is calculated for each raster cell for each hurricane the final step is the determination of mortality of tree species by age each tree species and each cohort age binned into user specified age ranges is assigned a probability of mortality given the wind speed as set by the user in table 3 the model msws is computed according to location using equation 1 in x and again in y see fig 1 this probability is compared against a random number generated for each cohort if a cohort is selected for mortality the entire cohort is killed partial mortality of an individual cohort is not modeled 2 2 parameterization for the southeastern us our study landscape is located in southeast north carolina approximately 90 km from the atlantic coast fort bragg is dominated by longleaf pine pinus palustris loblolly pine p taeda and contains hardwoods quercus spp liquidambar styraciflua lirodendron tulipifera and shortleaf pine p echinata ft bragg is an active military base with large areas directly affected by training exercises including tank maneuvers and shelling in addition fort bragg is carefully managed with scheduled prescribed fire and harvest rotations forested areas are managed for red cockaded woodpecker rcw dryobates borealis habitat a federally listed endangered species rcw prefers older longleaf pine for nesting hooper et al 1995 and therefore ft bragg natural resource managers actively encourage this habitat due to their proximity to the coastline forests in this region are susceptible to incurring high mortality if a hurricane passes over or close to them therefore rcw habitat inside fort bragg s limits is vulnerable to hurricanes we simulated hurricanes on a baseline maps of 100 100 m cells 1 ha each across fort bragg for 50 years from 2019 to 2069 the baseline map consisted of species composition and species age at a resolution of 1 ha by imputing fia data from north carolina south carolina and virginia onto a land cover map provided by fbdpwed since the fia database contains diameter and height for each tree but not tree age as required for cohort simulation we used site index curves to calculate age for every tree in the fia database for va sc and nc we also added old growth longleaf and loblolly pine to the landscape based on the maps of old pines provided by fbdpwed in addition to the new hurricane extension we simulated forest succession and disturbance with the necn extension v6 2 to simulate forest succession scheller et al 2011 the biomass harvest extension v4 3 to simulate harvesting gustafson et al 2000 and the scrpple extension v2 3 to simulate prescribed fires scheller et al 2019 growth rates in necn were calibrated to forest inventory and analysis bechtold patterson 2005 data for plots in north carolina south carolina and virginia lucash et al in review simulated harvesting thinning and removal of slash pine followed by replanting with longleaf and prescribed fires were calibrated to match the spatial temporal and target species as indicated by current forest management plans ft bragg natural resources division personal communication slash pine was removed at 0 25 every year for the next 20 years and 1500 2000 acres of thinning was performed annually approximately 450 prescribed fires occur on fort bragg each year across the landscape prescribed fire return intervals vary between one to three years to parameterize the new hurricane extension for our study we first determined how many storms occur in a given year on the east coast of the united states we only included storms that made landfall on the east coast from 30 7 n to 38 45 n which includes the coast from georgia to virginia and that have a reasonable likelihood of causing forest mortality on our landscape the coastline was represented as a straight line with a heading of 45 to approximate the southeast coast of the united states to determine the historic landfall incident counts per year and the historic percentages of storm wind speed at landfall we used the national hurricane center s best track data hurdat2 https www nhc noaa gov data hurdat hurdat2 contains data for all tropical cyclones in the atlantic basin since 1851 hurdat2 data were recorded every six hours and include location latitude and longitude of the center of the eye msws knots central pressure millibars and wind field size in nautical miles from the center we processed the hurdat2 data using python see https github com paulschrum hurdat2fc we estimated the number of storm occurrences per year using the hurdat2 dataset from 1969 to 2018 occurring on the coast of georgia south carolina north carolina and virginia approximately 1125 km of coastline only storms with landfall wind speed greater than or equal to 80 kph 43 knots were included in the count we selected 80 kph based on our judgement for the minimum speed that would generate wind induced mortalities this value is slightly less than the minimum msws for mortality as found in table 3 for the period the number of occurrences in a given year varied from 0 to 2 table 1 we also estimated the maximum recorded storm center msws for the lifetime of the storm and the size of each storm we then determined the statistical distribution of maximum sustained wind speed of storms at landfall restricted to this range finally we characterized the wind field size to facilitate parameterization of our suitable maximum sustained wind field model the latitude of simulated storms varied from 30 7 to 38 45 and was generated using a uniform random number generator the storm track direction was simulated from a random azimuth direction between 280 and 360 using a uniform random generator maximum sustained wind speed at landfall sl was determined stochastically we estimated sl by comparison against the actual distribution of historic landfalls using the hurdat2 dataset we looked at all tropical storm landfalls on the east coast from georgia to virginia from 1971 to 2018 the period of record with the most complete data we found 26 storms by these criteria and plotted the count of storms that made landfall at given wind speeds fig 3 we fit a lognormal distribution to these data when base hurricane stochastically determines sl for each individual hurricane the distribution is scaled and translated via minimum maximum and mode wind speeds fig 4 we parameterized the distribution of hurricane wind speeds along a storm centerline track from the southeast us based on hurricane katrina 2005 from the hurdat2 data fig 5 although katrina did not make landfall on the east coast we selected it because it is recent enough that the hurdat2 data for the wind field was complete by contrast hurricanes hugo 1989 and fran 1996 affected our landscape but occurred at a time when the wind field values away from the centerline track were not included in the database we elected to use katrina and sandy as exemplar hurricanes for parameterization we used katrina for the lateral parameterization because katrina and sandy were the only storms with straight paths in which the hurdat dataset also included the wind field information we set the values for a in equation 1 according to katrina then compared to sandy and found similar results we determined parameters to equation 1 table 2 to approximate the katrina centerline msws values fig 5 we sets values for a and sb that were used for all hurricanes we also used hurricane katrina to determine a values lateral to the storm track because the hurdat2 dataset contains wind field extents for this hurricane that are not present other recent hurricanes for each sampled point for hurricane katrina we plotted the wind speeds to the left and right of the centerline we developed wind speed profiles perpendicular to the storm track fig 5 and found that those profiles have characteristics similar to equation 1 based on this study the msws extends perpendicular to the storm track direction of katrina were higher for a on the right side of the storm 240 km than on the left 162 km the profile of katrina s maximum sustained wind speed plotted against its distance inland follows a logarithmic decline approaching an asymptote we compared these equations and parameters to historic values found in the hurdat2 database for hurricanes katrina hugo and fran fig 6 when a simulated storm is spawned landfall latitude storm track direction and sl are stochastically generated and the msws field is determined by the wind speed equation equation 1 thus msws values are computed according to the geographic location of each cell relative to the storm track and cohort mortalities are simulated based on the msws of each cell to illustrate how the parametric surface relates to the landscape we estimated the wind speed parameter field set to the values for hurricane hugo fig 7 the base hurricane extension uses an input table table 3 to determine cohort mortality probabilities cohort mortality probability was binned according to species and age and wind speed we derived mortality estimates by species age and wind speed from empirical data busing et al 2009 cely 1989 hook et al 1991 sharitz et al 1992 xi et al 2008b where species data were missing we used taxonomic analogs e g substituting sugar maple to estimate sweetgum mortality risk given their similarities cohort mortality includes all causes of death including snapping uprooting neighbor tree falls and delayed mortality we did not include experimental studies of tree mortality vulnerabilities e g garms and dean 2019 because winching studies do not capture the structural fatigue due to storm duration stress micro bursts e g isolated tornados and soil saturation 3 results and discussion previous research has focused on hurricane effects on local diversity xi et al 2008 2019 singular hurricane effects zimmerman et al 1995 busby et al 2008 or simulated localized hurricane wind speeds without forest mortality boose et al 2004 by contrast our objective was to simulate many hurricanes over a broad extent and duration within a modeling framework that also integrates other disturbances and management our simulations demonstrate the substantial effects and large stochastic variation of hurricanes on tree biomass fig 8 at the start of the simulation average aboveground live biomass agb across the ft bragg landscape was 5700 g m 2 the baseline scenario no hurricanes resulted in a 400 increase in average agb to 21 600 g m 2 over 50 years simulated hurricanes across the landscape resulted in average agb of 16 800 g m 2 at the end of the 50 year simulation fig 9 in addition there was a strong legacy effect of simulated hurricanes that is agb never fully recovered back to no hurricane scenario levels on average though two simulations with early hurricanes did come close to recovering to the baseline scenario before the end of the fifty years there is also an effect on the distribution of species at the clade level between agb of conifers versus angiosperms in the baseline scenario agb is 95 6 conifer with the remainder being angiosperms and increases to 96 8 over the 50 year span of the model when simulations are run to include hurricanes the average conifer abg decreases to 93 7 fig 10 simulated hurricanes altered the ratio of coniferous and angiosperm agb in favor of angiosperms due to their lower probability of hurricane mortality with conifers declining from 95 dominance to 93 although this particular landscape is intensively managed for rcw habitat and therefore large deviations in successional trajectories are limited we would expect neighboring areas that are unmanaged to experience larger shifts in forest composition and structure after hurricanes zimmerman et al 1995 hurricanes substantially increased overall model abg variability whereas the baseline scenario was essentially deterministic simulated fires and harvest instances follow a proscribed schedule and species are broadly intermixed minimizing stochastic variation due to seed dispersal the simulation with hurricanes highlight larger inherent variability of final abg in addition the new extension adds further parameter uncertainty which is exacerbated by uncertainty in climate change effects on hurricane frequency and intensity walsh et al 2016 there will also be many subsequent effects and uncertainties not explored here variation in rcw habitat variation in fuel loading that would likely alter prescribed burning planning extrapolated to the broader southeastern us there are large economic implications for any given hurricane haight et al 1995 and these effects will be cumulative over time given this uncertainty land managers may be motivated to shorten harvest rotation times to minimize economic risks reed 1984 as climate change amplifies hurricane frequency and or intensity these effects may increase webster et al 2005 but also see walsh et al 2016 and these potential consequences should be evaluated as landscape change model such as landis ii offer the possibility to include climate scenarios in simulations a future manuscript will explore these potential consequences 3 1 limitations our goal was to simulate forest mortality due to tropical storm and hurricane strength winds over a multi decadal time horizon as a result there are several aspects of hurricanes which we have not included we do not model rainfall inland flooding due to rainfall soil saturation coastal flooding due to storm surge or the resulting saltwater damage delaune et al 1987 although we did not explicitly model soil saturation and wind gusts they are implicitly accounted for by the forest mortalities derived from empirical data further we did not consider wind gusts hurricane rotation embedded tornados or terrain boose et al 1994 we only modeled the maximum sustained wind speed at each cell over the duration of a simulated storm local spatial variation in damage identified by xi et al 2008a was therefore not modeled because we are considering hurricane effects decades into the future the downscaling necessary to include wind gusts and wind direction would require extensive additional parameterization and computation and would improve precision but not accuracy that is not parsimonious with our goals of long term forecasting of trends and uncertainty finally the base hurricane extension provides no way to model specific historic storms which could be used to compare model results with field measurements such as cely 1989 and xi et al 2008a despite these acknowledged limitations we have successfully demonstrated the efficacy of simulating large area high winds from hurricanes altering landscape succession 4 conclusions hurricanes are infrequent and large disturbance events foster et al 1998 that have the potential to alter the successional trajectories lugo 2008 of large areas across the southeastern us though hurricane damage takes place on the time scale of a single day the influence of a single storm on ecosystem wide high mortality alters the succession trajectory for decades afterwards as seen in fig 8 further the selection bias of very high winds preferring taller trees tends to leave younger stands with lower mortality rates further altering the succession trajectory in ways that differ from several other kinds of disturbances our extension to an existing landscape modeling framework enables hurricane effects to be integrated into other research on the effects of prescribed fire e g krofcheck et al 2019 forest management successional trajectories and recovery planning of threatened species e g cadieux et al 2019 for example lucash et al in review used the base hurricane extension to assess rcw habitat changes at fort bragg due to multiple disturbances including hurricanes incorporating the hurricane effects described herein will allow a more complete assessment of long term uncertainty across large areas funding this research was funded by the department of defense environmental security technology certification program estcp project rc 201702 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements jeff cannon provided valuable input to the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104833 
25967,hurricanes in the southeast united states are infrequent disturbances that affect large areas and have a large effect on forest succession in order to understand and quantify this effect we added a new module to the landis ii landscape change model focusing on the southeast coast of the united states we simulated stochastic hurricanes for 50 years for each simulated storm the new model extension generates the maximum sustained wind speed over the region and uses the resulting parameter surface to compute maximum sustained wind speed for each cohort cell in a raster grid mortality is estimated for each species and age cohort in each cell based on the maximum sustained wind speed altering forest succession results indicate that hurricanes reduce average aboveground biomass by 20 over 50 years on a landscape in fort bragg north carolina usa compared to a scenario without hurricanes and increased uncertainty of projected succession keywords hurricanes disturbance forest mortality landis ii 1 introduction forest disturbances cause mortality chambers et al 2007 hicke et al 2012 change the direction of succession turner et al 1998 alter ecosystem service delivery thom and seidl 2016 and create substantial uncertainty that challenges forest management predicated on consistent growth and yield kurz et al 2008 in the southeastern region of united states hurricanes are an infrequent but catastrophic disturbance that alter forest succession in the western united states wildfire and insects hicke et al 2015 cause the most mortality and have recently surged due to climate change related increases in temperatures and reductions in snowpack westerling et al 2006 in boreal regions wildfires have been increasing in size and intensity over the past 20 years walker et al 2019 across the southeastern us harvesting of saw timber is the dominant source of recurring mortality fagan et al 2018 although hurricanes generate substantial localized mortality for example in south carolina in 1989 hurricane hugo damaged 1 8 million ha of forest an area larger than the state of connecticut hook et al 1991 in addition hurricane frequency and intensity bender et al 2010 bacmeister et al 2018 may increase as the climate warms adding additional uncertainty to forest change and management dale et al 2001 chambers et al 2007 the effects of hurricanes are similar to that generated by downbursts derechos and tornados across the midwestern and northeastern us peterson 2000 frelich 2002 lucash et al 2018 although vastly exceeding those events in their spatial extent typically a hurricane that makes landfall in the southeastern us will disturb 50 100 000 km2 foster et al 1998 hurricanes play a critical ecological role in maintaining structure function and fuel dynamics in longleaf and other fire dominated coastal systems myers et al 1998 chambers et al 2007 xi et al 2008a b o brien et al 2008 cannon et al 2017 similar to other wind events the tallest trees are most vulnerable to uprooting or snapping boose et al 1994 interspecific differences also exist although they are less well quantified busing et al 2009 cely 1989 hook et al 1991 sharitz et al 1992 xi et al 2008b within the zone of hurricane mortality the effects can vary from near total mortality for example the areas adjacent to the landfall of hurricane michael in 2018 beven et al 2019 to sparse mortality of a few trees per ha where mortality is greatest there will be a loss of economic value haight et al 1995 long term release of forest carbon as dead trees decay mcnulty 2002 and changes to long term successional trends for example if a site is dominated by conifers with a hardwood understory a common situation where fire has been excluded for extended periods in the southeast glitzenstein et al 2003 moderate hurricane winds may accelerate succession towards hardwood dominance on the contrary if a site dominated by shade intolerant conifers experiences near total mortality conifers will likely reestablish given high light conditions we developed a hurricane model the base hurricane extension that is an extension to the landscape change model landis ii scheller et al 2007 to incorporate the simulation of hurricane wind mortalities into landis ii our goal was to forecast long term interactions between forest disturbance and prescribed fire across large extents previous work has emphasized fine scale interactions of hurricanes and forest mortality boose et al 1994 we therefore developed a model of hurricane mortality and hurricanes effects that interfaces with existing models of succession management and other natural disturbances e g insect mortality we focused on ft bragg north carolina as it is broadly representative of southeastern forests falls within the zone of hurricanes and forest managers are concerned about the potential for hurricanes to interfere with their long term management goals fort bragg is a us army military reservation in the sandhills region of north carolina a us commerce department shapefile indicates the area of the base is 61 728 4 ha the raster model used in this research included 45 947 ha of forested area 2 methods first we describe the conceptual hurricane model next we describe how the model was specifically parameterized for our study area 2 1 conceptual hurricane model base hurricane is a new extension to the landscape change model which simulates stand level mortalities due to hurricane wind forces in this context base indicates that this extension is compatible with all other extensions landis ii simulates succession disturbance and management across large and heterogeneous landscapes typically 50 000 5 000 000 ha in size incorporating our hurricane model into an existing model of succession disturbance and management allows us to leverage prior efforts to integrate multiple processes across a landscape e g scheller et al 2011 lucash et al 2018 serra diaz et al 2018 all of our code is open source https github com landis ii foundation extension base hurricane landis ii represents trees and shrubs on the landscape as species age cohorts not individual trees each cohort represents multiple individual trees and typically has other associated attributes the landscape is represented as a grid of spatially interacting cells each cell can contain one or more cohorts with an unlimited number of species the processes of succession disturbance and management interact through their effects on cohort composition within each cell the base hurricane extension runs on an annual time step in which it randomly determines the number of storms that make landfall on the southeastern coast of the us for that year this hurricane count is in the range 0 to n where n may be adjusted by the user for our simulations we used a maximum of 3 hurricanes per year the location of each storm is controlled by three parameters landfall latitude storm track direction and maximum sustained wind speed at landfall for each storm the particular value of each of these three is set from a random number generator landfall latitude is the latitude along the coast line where the storm makes landfall see fig 1 storm track direction is an azimuth value between 280 and 360 maximum sustained windspeed at landfall is determined from a log normal distribution as shown in fig 4 the geometry resulting from these three parameters define the parametric surface of maximum sustained wind speed msws see fig 7 base hurricane then computes msws for each site in the study area treating the entire duration of the event as a single moment in model time mortalities are generated stochastically for each raster cell based on the maximum sustained wind speed of the site see below one may consult the base hurricane user guide for complete explanation of all user parameters https github com landis ii foundation extension base hurricane blob master docs landis ii 20base 20hurricane 20v0 1 20user 20guide docx for any given location within a hurricane impact area surface wind speeds vary over the duration of the event in both sustained wind speed and gust speed a storm whose center passes two hundred kilometers away from a location may potentially cause mortality given these factors we elected to represent the wind field as a parametric surface representing msws over the duration of the storm wind induced mortality is computed as a function of the msws cohort age older cohorts are taller and therefore more vulnerable and species some species are more vulnerable than others maximum wind gust and duration of exposure are included implicitly in the species mortality table table 3 without being explicitly modeled this approach reduces computation time compared to a more precise model e g boose et al 1994 specifically actual historic msws is available in the hurdat2 dataset but hectare scale maximum wind gusts are not as the present model is stochastic and not deterministic we elected to model overall probability of mortality with a single random number comparison where a hurricane strikes we anticipate the structural resistance of the soil to overturning is reduced by rainfall saturation for all scenarios likewise where there is a given maximum sustained wind speed wind gusts will be approximately 25 35 higher yet gust direction and length have not been researched and modelling them is not in our scope the general effect is that high winds blow trees down we have synthesized all of these factors to simplify our model in a way which we believe does not reduce accuracy over a large area simulated with a computerized random number generator the base hurricane extension simulates the msws at the center of the storm track the storm track direction is modeled as a straight line at a random azimuth direction proceeding inland from the shoreline the distance from the storm track to the study site is determined from landfall latitude and track direction for each hurricane simulated fig 1 the coastline is represented as linear with a fixed axis after a storm makes landfall and moves inland msws at the center of the storm declines over distance along the storm track as well as perpendicular to the storm track this creates a wind field the geographic distribution of the msws relative to the storm track we used the second derivative of the hyperbola as shown in equation 1 to estimate the msws centerline profile eq 1 m s w s a 2 s l s b a 2 x 2 x 2 a 2 1 s b where msws is the maximum sustained wind speed along the center track km hr 1 a 2 is the distance from landfall to the inflection point of the profile curve km x is the track distance from landfall km sb is the wind speed at final reading km hr 1 sl is the maximum sustained wind speed at landfall km hr 1 the profile shown in fig 2 represents msws along the line in fig 1 labeled x distance along storm track to represent a parametric field surface over a region we also modeled maximum sustained wind speed perpendicular to the storm track again using equation 1 for the cross section i e wind speeds perpendicular to the center sl is taken from msws along the centerline from equation 1 because atlantic hurricane wind speeds are asymmetrical the wind speed decline as a distance from center differs on the left and right sides of the hurricane relative to their primary direction this is expressed as differing values of a for the left and right sides while x represents the distance from the centerline or y in cartesian space the combination of equations produces a parametric surface of the maximum sustained wind speed over the duration of the event the resulting parametric surface is oriented with the x axis following the storm track with its origin at the landfall location the y axis of the parametric surface is perpendicular to the storm track see fig 1 after sustained wind speed is calculated for each raster cell for each hurricane the final step is the determination of mortality of tree species by age each tree species and each cohort age binned into user specified age ranges is assigned a probability of mortality given the wind speed as set by the user in table 3 the model msws is computed according to location using equation 1 in x and again in y see fig 1 this probability is compared against a random number generated for each cohort if a cohort is selected for mortality the entire cohort is killed partial mortality of an individual cohort is not modeled 2 2 parameterization for the southeastern us our study landscape is located in southeast north carolina approximately 90 km from the atlantic coast fort bragg is dominated by longleaf pine pinus palustris loblolly pine p taeda and contains hardwoods quercus spp liquidambar styraciflua lirodendron tulipifera and shortleaf pine p echinata ft bragg is an active military base with large areas directly affected by training exercises including tank maneuvers and shelling in addition fort bragg is carefully managed with scheduled prescribed fire and harvest rotations forested areas are managed for red cockaded woodpecker rcw dryobates borealis habitat a federally listed endangered species rcw prefers older longleaf pine for nesting hooper et al 1995 and therefore ft bragg natural resource managers actively encourage this habitat due to their proximity to the coastline forests in this region are susceptible to incurring high mortality if a hurricane passes over or close to them therefore rcw habitat inside fort bragg s limits is vulnerable to hurricanes we simulated hurricanes on a baseline maps of 100 100 m cells 1 ha each across fort bragg for 50 years from 2019 to 2069 the baseline map consisted of species composition and species age at a resolution of 1 ha by imputing fia data from north carolina south carolina and virginia onto a land cover map provided by fbdpwed since the fia database contains diameter and height for each tree but not tree age as required for cohort simulation we used site index curves to calculate age for every tree in the fia database for va sc and nc we also added old growth longleaf and loblolly pine to the landscape based on the maps of old pines provided by fbdpwed in addition to the new hurricane extension we simulated forest succession and disturbance with the necn extension v6 2 to simulate forest succession scheller et al 2011 the biomass harvest extension v4 3 to simulate harvesting gustafson et al 2000 and the scrpple extension v2 3 to simulate prescribed fires scheller et al 2019 growth rates in necn were calibrated to forest inventory and analysis bechtold patterson 2005 data for plots in north carolina south carolina and virginia lucash et al in review simulated harvesting thinning and removal of slash pine followed by replanting with longleaf and prescribed fires were calibrated to match the spatial temporal and target species as indicated by current forest management plans ft bragg natural resources division personal communication slash pine was removed at 0 25 every year for the next 20 years and 1500 2000 acres of thinning was performed annually approximately 450 prescribed fires occur on fort bragg each year across the landscape prescribed fire return intervals vary between one to three years to parameterize the new hurricane extension for our study we first determined how many storms occur in a given year on the east coast of the united states we only included storms that made landfall on the east coast from 30 7 n to 38 45 n which includes the coast from georgia to virginia and that have a reasonable likelihood of causing forest mortality on our landscape the coastline was represented as a straight line with a heading of 45 to approximate the southeast coast of the united states to determine the historic landfall incident counts per year and the historic percentages of storm wind speed at landfall we used the national hurricane center s best track data hurdat2 https www nhc noaa gov data hurdat hurdat2 contains data for all tropical cyclones in the atlantic basin since 1851 hurdat2 data were recorded every six hours and include location latitude and longitude of the center of the eye msws knots central pressure millibars and wind field size in nautical miles from the center we processed the hurdat2 data using python see https github com paulschrum hurdat2fc we estimated the number of storm occurrences per year using the hurdat2 dataset from 1969 to 2018 occurring on the coast of georgia south carolina north carolina and virginia approximately 1125 km of coastline only storms with landfall wind speed greater than or equal to 80 kph 43 knots were included in the count we selected 80 kph based on our judgement for the minimum speed that would generate wind induced mortalities this value is slightly less than the minimum msws for mortality as found in table 3 for the period the number of occurrences in a given year varied from 0 to 2 table 1 we also estimated the maximum recorded storm center msws for the lifetime of the storm and the size of each storm we then determined the statistical distribution of maximum sustained wind speed of storms at landfall restricted to this range finally we characterized the wind field size to facilitate parameterization of our suitable maximum sustained wind field model the latitude of simulated storms varied from 30 7 to 38 45 and was generated using a uniform random number generator the storm track direction was simulated from a random azimuth direction between 280 and 360 using a uniform random generator maximum sustained wind speed at landfall sl was determined stochastically we estimated sl by comparison against the actual distribution of historic landfalls using the hurdat2 dataset we looked at all tropical storm landfalls on the east coast from georgia to virginia from 1971 to 2018 the period of record with the most complete data we found 26 storms by these criteria and plotted the count of storms that made landfall at given wind speeds fig 3 we fit a lognormal distribution to these data when base hurricane stochastically determines sl for each individual hurricane the distribution is scaled and translated via minimum maximum and mode wind speeds fig 4 we parameterized the distribution of hurricane wind speeds along a storm centerline track from the southeast us based on hurricane katrina 2005 from the hurdat2 data fig 5 although katrina did not make landfall on the east coast we selected it because it is recent enough that the hurdat2 data for the wind field was complete by contrast hurricanes hugo 1989 and fran 1996 affected our landscape but occurred at a time when the wind field values away from the centerline track were not included in the database we elected to use katrina and sandy as exemplar hurricanes for parameterization we used katrina for the lateral parameterization because katrina and sandy were the only storms with straight paths in which the hurdat dataset also included the wind field information we set the values for a in equation 1 according to katrina then compared to sandy and found similar results we determined parameters to equation 1 table 2 to approximate the katrina centerline msws values fig 5 we sets values for a and sb that were used for all hurricanes we also used hurricane katrina to determine a values lateral to the storm track because the hurdat2 dataset contains wind field extents for this hurricane that are not present other recent hurricanes for each sampled point for hurricane katrina we plotted the wind speeds to the left and right of the centerline we developed wind speed profiles perpendicular to the storm track fig 5 and found that those profiles have characteristics similar to equation 1 based on this study the msws extends perpendicular to the storm track direction of katrina were higher for a on the right side of the storm 240 km than on the left 162 km the profile of katrina s maximum sustained wind speed plotted against its distance inland follows a logarithmic decline approaching an asymptote we compared these equations and parameters to historic values found in the hurdat2 database for hurricanes katrina hugo and fran fig 6 when a simulated storm is spawned landfall latitude storm track direction and sl are stochastically generated and the msws field is determined by the wind speed equation equation 1 thus msws values are computed according to the geographic location of each cell relative to the storm track and cohort mortalities are simulated based on the msws of each cell to illustrate how the parametric surface relates to the landscape we estimated the wind speed parameter field set to the values for hurricane hugo fig 7 the base hurricane extension uses an input table table 3 to determine cohort mortality probabilities cohort mortality probability was binned according to species and age and wind speed we derived mortality estimates by species age and wind speed from empirical data busing et al 2009 cely 1989 hook et al 1991 sharitz et al 1992 xi et al 2008b where species data were missing we used taxonomic analogs e g substituting sugar maple to estimate sweetgum mortality risk given their similarities cohort mortality includes all causes of death including snapping uprooting neighbor tree falls and delayed mortality we did not include experimental studies of tree mortality vulnerabilities e g garms and dean 2019 because winching studies do not capture the structural fatigue due to storm duration stress micro bursts e g isolated tornados and soil saturation 3 results and discussion previous research has focused on hurricane effects on local diversity xi et al 2008 2019 singular hurricane effects zimmerman et al 1995 busby et al 2008 or simulated localized hurricane wind speeds without forest mortality boose et al 2004 by contrast our objective was to simulate many hurricanes over a broad extent and duration within a modeling framework that also integrates other disturbances and management our simulations demonstrate the substantial effects and large stochastic variation of hurricanes on tree biomass fig 8 at the start of the simulation average aboveground live biomass agb across the ft bragg landscape was 5700 g m 2 the baseline scenario no hurricanes resulted in a 400 increase in average agb to 21 600 g m 2 over 50 years simulated hurricanes across the landscape resulted in average agb of 16 800 g m 2 at the end of the 50 year simulation fig 9 in addition there was a strong legacy effect of simulated hurricanes that is agb never fully recovered back to no hurricane scenario levels on average though two simulations with early hurricanes did come close to recovering to the baseline scenario before the end of the fifty years there is also an effect on the distribution of species at the clade level between agb of conifers versus angiosperms in the baseline scenario agb is 95 6 conifer with the remainder being angiosperms and increases to 96 8 over the 50 year span of the model when simulations are run to include hurricanes the average conifer abg decreases to 93 7 fig 10 simulated hurricanes altered the ratio of coniferous and angiosperm agb in favor of angiosperms due to their lower probability of hurricane mortality with conifers declining from 95 dominance to 93 although this particular landscape is intensively managed for rcw habitat and therefore large deviations in successional trajectories are limited we would expect neighboring areas that are unmanaged to experience larger shifts in forest composition and structure after hurricanes zimmerman et al 1995 hurricanes substantially increased overall model abg variability whereas the baseline scenario was essentially deterministic simulated fires and harvest instances follow a proscribed schedule and species are broadly intermixed minimizing stochastic variation due to seed dispersal the simulation with hurricanes highlight larger inherent variability of final abg in addition the new extension adds further parameter uncertainty which is exacerbated by uncertainty in climate change effects on hurricane frequency and intensity walsh et al 2016 there will also be many subsequent effects and uncertainties not explored here variation in rcw habitat variation in fuel loading that would likely alter prescribed burning planning extrapolated to the broader southeastern us there are large economic implications for any given hurricane haight et al 1995 and these effects will be cumulative over time given this uncertainty land managers may be motivated to shorten harvest rotation times to minimize economic risks reed 1984 as climate change amplifies hurricane frequency and or intensity these effects may increase webster et al 2005 but also see walsh et al 2016 and these potential consequences should be evaluated as landscape change model such as landis ii offer the possibility to include climate scenarios in simulations a future manuscript will explore these potential consequences 3 1 limitations our goal was to simulate forest mortality due to tropical storm and hurricane strength winds over a multi decadal time horizon as a result there are several aspects of hurricanes which we have not included we do not model rainfall inland flooding due to rainfall soil saturation coastal flooding due to storm surge or the resulting saltwater damage delaune et al 1987 although we did not explicitly model soil saturation and wind gusts they are implicitly accounted for by the forest mortalities derived from empirical data further we did not consider wind gusts hurricane rotation embedded tornados or terrain boose et al 1994 we only modeled the maximum sustained wind speed at each cell over the duration of a simulated storm local spatial variation in damage identified by xi et al 2008a was therefore not modeled because we are considering hurricane effects decades into the future the downscaling necessary to include wind gusts and wind direction would require extensive additional parameterization and computation and would improve precision but not accuracy that is not parsimonious with our goals of long term forecasting of trends and uncertainty finally the base hurricane extension provides no way to model specific historic storms which could be used to compare model results with field measurements such as cely 1989 and xi et al 2008a despite these acknowledged limitations we have successfully demonstrated the efficacy of simulating large area high winds from hurricanes altering landscape succession 4 conclusions hurricanes are infrequent and large disturbance events foster et al 1998 that have the potential to alter the successional trajectories lugo 2008 of large areas across the southeastern us though hurricane damage takes place on the time scale of a single day the influence of a single storm on ecosystem wide high mortality alters the succession trajectory for decades afterwards as seen in fig 8 further the selection bias of very high winds preferring taller trees tends to leave younger stands with lower mortality rates further altering the succession trajectory in ways that differ from several other kinds of disturbances our extension to an existing landscape modeling framework enables hurricane effects to be integrated into other research on the effects of prescribed fire e g krofcheck et al 2019 forest management successional trajectories and recovery planning of threatened species e g cadieux et al 2019 for example lucash et al in review used the base hurricane extension to assess rcw habitat changes at fort bragg due to multiple disturbances including hurricanes incorporating the hurricane effects described herein will allow a more complete assessment of long term uncertainty across large areas funding this research was funded by the department of defense environmental security technology certification program estcp project rc 201702 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements jeff cannon provided valuable input to the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104833 
25968,in this study an open source google earth engine based gee system is proposed surnamed sebaligee based on the surface energy balance for land improved sebali model sebaligee has the ability to produce monthly evapotranspiration et assessments with an automatic fetching of the required inputs based on satellite datasets already hosted within the gee platform calibrations followed by validations were made in five different countries i e united states china italy germany and belgium between 2013 and 2017 it was followed by the application of the sebaligee over a 307 000 km2 in the plain region in the state of california united states between 2017 and 2019 as an average in these selected years aquaculture appeared to be consuming the largest amount of water with 1109 72 215 07 mm year the proposed open source system aims at improving the assessment of et and water productivity towards better management of water resources keywords sebali google earth engine open source water productivity farm scale 1 introduction within a global trend towards improving national food security the assessment of water needs and demands are of high priority this could be done through the retrieval of evapotranspiration et rates in the agricultural lands nasrallah et al 2019 usually predicted to be consuming 70 of the total available water resources zurayk and dirar 2019 mcnabb 2019 such direction is coinciding as well with another global trend related to the increase of water productivity without affecting farmer s economic situation and crop performance mhawej et al 2020a nasrallah et al 2020 while several ground based approaches were proposed in the literature e g aquastat eliasson et al 2003 cropsyst stöckle et al 2003 and cropwatch bingfang and qiangzi 2004 nowadays remote sensing and modeling based methods is providing the most adequate option for the retrieval of the et related information more particularly the advancement of satellite sensors in terms of better spatial temporal and spectral resolutions coupled with the free availability of different data sources enabled a much reduction in time personal and resources faour et al 2013 2018 ghoussein et al 2018 in this context several surface energy balance seb models were proposed since the 1990 including among others the surface energy balance algorithm for land sebal bastiaanssen et al 1998a 1998b the mapping evapotranspiration at high resolution using internalized calibration metric allen et al 2005 2007 the surface energy balance index sebi menenti and choudhury 1993 the surface energy balance system sebs su 2002a 2002b and lately the sebal improved or sebali mhawej et al 2020a 2020b sebali has tried and succeeded in filling the gaps and in reducing the standard generated biases in the previous seb mhawej et al 2020a 2020b particularly in data lacking countries still the basic concept in these models reamains the retrieval of et which is considered as a residual of the surface energy balance liou and kar 2014 to facilitate access to such much needed et information three free to access et estimations platforms stand out first sebal open source python script or py sebal is available at https github com wateraccounting sebal with further documentations accessible on the ihe delft website i e www un ihe org its installation requires a specialized software i e anaconda as many libraries are missing in a standard python version furthermore users have to include the required remote sensing climatic and other datasets manually within the system before generating et related information which could entail many errors when running this python script second the fao water productivity open access portal wapor could be accessed at https wapor apps fao org home wapor 2 1 providing et ready estimations at 250 m 100 m and 30 m scales still users do not have an option to process new data or other locations nor the website will provide such datasets as it is related to a fao project which has ended by the end of 2019 more recently the coarse resolution 250 m wapor datasets were hosted on the google earth engine gee platform e g https developers google com earth engine datasets catalog fao wapor 2 l1 npp d third an earth engine evapotranspiration flux eeflux developed by the consortium of university of nebraska lincoln desert research institute and university of idaho is also available it is a version of metric that operates on the gee platform i e https eeflux level1 appspot com full documentation of the product could be found in irmak and allen 2012 eeflux is produced in a user friendly interface however users cannot alter or amend the available code and such have a limited control represented by a selection of the study site only as any seb model requires different set of information from diverse sources a calibration and a validation of these proposed automated and semi automated systems are particularly needed unfortunately this was not produced in the previous set of free to access et estimations platforms only eeflux tried to compensate by including a questioned user defined adjusted low and high end potential evapotranspiration values the calibration validation process is then mandatory for two facts one related to the good estimation of hot and cold pixels e g timmermans et al 2007 senay et al 2011 silva and marques da silva 2019 the other to the accuracy and biases from the usage of climatic and ground based information e g smith and katz 2013 cammarano et al 2017 lei et al 2019 which could be lacking in many regions and countries even though that sebali has reduced the number of the needed factors and was validated in diverse climatic zones in different countries it could not be migrated as it is into the gee platform unfortunately it appears to be the case for the other available free to access et estimation platforms previously discussed thus in this paper the sebali google earth engine sebaligee is proposed enabling professional and non specialist to asses any region across the globe in an automated and user friendly manners et rates were calibrated and validated in different climatic regions and countries across the globe including united states china italy germany and belgium between 2013 and 2017 the proposed sebaligee enables users to include other agricultural based models maybe the most prominent is the crop identification and mapping alter and even improve the existent system based on their preferences following the proposal of sebaligee a thorough assessment of the large california plain was produced with more than 80 crops type considered the crop water consumption at pixel level and for the studied region was identified classifying the available crops and providing pivotal information for future references 2 materials and methods 2 1 study area the study area fig 1 corresponds to a major plain in the state of california extending over an area of 307 000 km2 the prevailing climate is hot summer mediterranean climate according to the koppen classification with an average annual precipitation of 745 62 mm between 2017 and 2019 based on the north american land data assimilation system nldas data the mean altitude is 919 m a s l agriculture is the main economic activity in the plain with 83 different crop types patterns available almond plantation is widespread over 5045 km2 followed by alfalfa and grapes with 3290 km2 and 2974 km2 respectively according to the united states department of agricultural national agricultural statistics service usda nass averaged between 2017 and 2019 2 2 data background 2 2 1 satellite data six sources of satellite datasets were used in this study with its ability to assess vegetation health and land surface temperature the 30 m landsat 8 satellite data were collected launched on february 11 2013 the eighth landsat satellite provides a revisit time of 16 day with a 30 m spatial resolution for visible and infrared bands as well as two thermal infrared bands at 100 m i e bands 10 and 11 level 1 top of atmosphere toa and level 2 bottom of atmosphere boa images could be acquired from usgs official website i e https earthexplorer usgs gov the processed landsat 8 level 2 datasets which were used in this study correspond to atmospherically corrected satellite images generated from the land surface reflectance code lasrc algorithm this latter uses the first band i e coastal aerosol of landsat 8 level 1 along auxiliary climate data from the moderate resolution imaging spectroradiometer modis and a unique radiative transfer model within the gee platform landsat 8 level 2 could be accessed on https developers google com earth engine datasets catalog landsat lc08 c01 t1 sr hl en as the intention was to work on globally available weather satellites the climate related datasets i e temperature relative humidity and wind speed were acquired from the era5 or the fifth generation ecmwf atmospheric reanalysis of the global climate such reanalysis corresponds to the years 1950 onwards with a total size of about 9 petabytes combining model data with observations from across the world into a globally complete hourly based and consistent dataset using the laws of physics along this product an uncertainty estimate is included providing guidance on where products are expected to be more or less accurate within gee era5 climatic datasets could be accessed on https developers google com earth engine datasets catalog ecmwf era5 monthly bands aimed to improve sunshine duration calculation a product of terra modis satellite was used this latter orbit around the earth from north to south at 10 30 a m local time having coarse spatial resolutions i e between 250 m and 1 km terra modis is viewing the entire earth s surface every one to two days with 36 spectral bands the used product is the 1 km daily snow cover version 6 designated as mod10a1 where cloud cover could be deduced each day and aggregated monthly over the entire region it could be accessed on https developers google com earth engine datasets catalog modis 006 mod10a1 within the gee platform another product of modis was used for the identification of the agricultural lands available at https developers google com earth engine datasets catalog modis 006 mcd12q1 this was used to improve the selection of hot cold pixels as suggested by mhawej et al 2020a 2020b the land cover type i e mcd12q1 1 km version 6 product is a combination of the two terra and aqua satellites providing yearly global land cover types derived from six diverse classification schemes between 2001 and 2018 the main used approach is the application of a supervised classification for both satellites based on their reflectance data the elevation and slope required in sebali were collected from the freely available nasa shuttle radar topographic mission srtm version 4 providing digital elevation data dem for over 80 of the globe the spatial resolution of the data is approximately 90 m and available on gee at https developers google com earth engine datasets catalog cgiar srtm90 v4 finally usda nass cropland data were used for the identification of diverse crops type within the study area between 2017 and 2019 these datasets utilized satellite imagery from the landsat 8 oli tirs sensor the disaster monitoring constellation dmc deimos 1 and uk2 the isro resourcesat 2 liss 3 and the esa sentinel 2 sensors collected during the correspondent growing seasons outputs are 30 m for the continental united states it can be accessed at https developers google com earth engine datasets catalog usda nass cdl 2 2 2 ground data with the effort of several scientists and technicians around the world the fluxnet2015 dataset were established from multiple regional flux networks at more than 200 different sites between 1991 and 2014 the fluxnet2015 dataset includes several data quality control protocols and a data processing pipeline more precisely close interaction with the tower teams were made to improve data quality to propose novel methods for uncertainty quantification and to use of reanalysis data to fill long gaps of meteorological variable records in this study eddy covariance et datasets were used to validate sebaligee et values fig 2 eight different parcels in five main regions corresponding to five different countries were considered based on the availability of ground based et tower datasets after 2013 which coincided with the launch of landsat 8 these parcels correspond to two sites in the united states of america three sites in italy one in china one in germany and one in belgium with flux ground based data between september 2013 and december 2017 the missing months correspond to missing values from the measured ground based et or missing landsat 8 images due to bad weather conditions part of these data were downloaded from the official data portal of the fluxnet community available at https fluxnet fluxdata org the other source of data is based on an eddy covariance et measurement located in donghuayuan town china at 115 7880e 40 3491n between january 2016 and december 2017 outputs are 30 min data obtained by post processing using eddypro software data doi for each site is available in table 1 2 3 the sebaligee system while unlocking the power of the supercomputers and using some of the twenty petabytes of geospatial data hosted within the gee platform gorelick et al 2017 the combination of both gee and remote sensing techniques push remote sensing modeling studies to a new level this is particularly what was produced in this study as the already proposed 30 m et retrieval sebali system mhawej et al 2020a 2020b was migrated into the gee platform for a more global crop water consumption assessment this script is available at https code earthengine google com 48200ed2b76ff4acc530c618bb047635 as sebali requires different set of inputs remote sensing datasets available within gee were used fig 3 more precisely normalized difference vegetation index ndvi albedo and land surface temperature lst were calculated based on the 30 m landsat 8 satellite following the commonly known approaches e g faour and mario 2015 2016 2018 era 5 reanalysis datasets were used to retrieve mean monthly air temperature relative humidity and wind speed it is important to note that the usage of monthly instead of instantaneous climatic values in sebali model was thoroughly described and validated in allam et al 2021 modis snow cover product was integrated to assess monthly cloud coverage in order to better estimate sunshine duration with value ranging between 0 for cloudy areas to 1 for clear conditions agricultural areas were extracted from modis land cover type datasets with these areas corresponding to a value of 1 while the other values were set to null altitude and slope were calculated based on the 90 m srtm global product without any further processing sebaligee fig 4 requires users to only input their studied month and year for instance the month 10 of the year 2019 as well as the study region which is preferred to be larger than 50 km2 as discussed in previous papers e g mhawej et al 2020b users has also the option to include one or multiple parcels for basic statistical analysis sebaligee monthly outputs include several layers that can be exported into google drive including albedo ndvi lst averaged land cover lc for the past three years et and the crop coefficient kc still as different set of data inputs with different sensors were used particularly in terms of reanalyzed climatic data sebaligee et rates were linearly calibrated using randomly selected 30 of the available flux et eddy covariance ground datasets as suggested by many previous research e g singh et al 2012 mhawej et al 2016 2020c whereas 70 of these latter data were used for validations root mean square error rmse absolut mean error ame and r squared values were retrieved it is important to note that five countries with diverse climatic regions were considered i e continental semi arid monsoon influenced warm summer humid continental mediterranean hot summer and oceanic climates among them one in the selected study area furthermore in order to retrieve the crops type in the study region usda nass cropland data were used as it is with over 130 different classes statistical analysis was followed in each of these categories generating the average sum standard deviation variance among others across the 2017 2018 and 2019 years and at monthly basis 3 results 3 1 calibrations and validations without calibration sebaligee showed good results with an rmse of 16 26 mm month an ame of 14 54 mm month and an r squared value of 66 1 still when calibrated and validated using the 30 70 percent approach accuracy has improved even further with an rmse of 11 53 mm month an ame of 9 56 and an r squared value of 80 23 table 2 and fig 5 3 2 crops type water consumption based on the three consecutive years 2017 2018 and 2019 average annual et rates per crop type as well as the total annual water consumption were retrieved and shown in table 3 it was found that aquaculture was consuming the largest amount of water with 1109 72 215 07 mm year per crop type it was followed by pumpkin cabbage and sweet corn with values over 1060 mm year inversely the crops with the least average water consumed at pixel level are barley i e 837 81 225 71 mm year millet i e 814 09 76 21 mm year mint i e 780 61 163 mm year and rye i e 735 60 257 81 mm year over the total basin shrubland and grassland pasture accounted for 41 11 of the total consumed water even though they corresponded to the least consumption of water per pixel i e 733 96 mm year and 841 03 mm year respectively almonds with a total area of nearly 5045 km2 averaged between 2017 and 2019 showed an approximate consumption of 9 of the total consumed water within the considered plain alfalafa for instance constituted a staggering 5 39 of the total water consumption it was followed by grapes walnuts pistachios and winter wheat with approximate values of 5 1 4 3 and 2 9 respectively a violin like distribution of the total et values found in the study area is shown in fig 6 as the average precipitation for the study area is 745 62 mm most of the values exist above this threshold perennial ice and barren lands which were included for references presented et values of 153 93 51 89 mm year and 509 51 326 27 mm year respectively well below the annual precipitation rates 4 discussion sebaligee initial et outputs compared to eddy covariance ground et values yielded acceptable accuracy with a bias of approximately 0 48 mm day still because of the usage of different satellite platforms and the reanalysis climate datasets a further calibration was held the bias has decreased to nearly 0 32 mm day such result is promising and enables the implementation of sebaligee in four different climatic regions i e continental semi arid monsoon influenced warm summer humid continental mediterranean hot summer and oceanic climates across the globe more importantly with an r squared value of nearly 80 users from any background being professionals or non experts in the modeling or remote sensing techniques could use such system to alter their water consumption at parcel regional or national levels predicting future trends and proposing water productivity plans in this context sebaligee was applied in a vast region i e 307 000 km2 in the state of california between 2017 and 2019 the objective was to retrieve the average annual et rates per crop type for more than 80 crops as well as their total annual water consumption and its correspondent percentage of the total considered study area the use of a large study area with a huge number of parcels in each crop type enabled to indirectly include diverse agricultural management plans and existent microclimates within the analysis our findings indicated that aquaculture plantation is consuming the largest amount of water which is aligned with many previous studies e g verdegem et al 2006 2009 calone et al 2019 the low amount of consumed water in some of the crops e g barley millet mint and rye designates limited irrigation application as discussed in the literature e g singh and singh 1995 jamieson et al 1995 chen et al 2016 martel et al 2018 hashemi 2020 in parallel the high consumption of shrubland and grassland pasture over the total considered study area is alarming and should be addressed this could be related to deserted agricultural fields due to diminishing economic returns and stability faour and mario 2014 nasrallah et al 2020 furthermore as the area cultivated for each crop greatly differed crops with the largest areas shown the highest percentage of the consumed water within the considered plain and vice versa this could be highlighted with almonds cultivated in an area of 5045 km2 and having an average 9 of the consumed water between 2017 and 2019 on the other hand squash cultivated over a limited area of 19 km2 represented nearly 1 of the total consumed water nonetheless most of the annual et values appears to be above the annual precipitation rates which could signal that farmers tend to irrigate their fields for better yields still further investigations should be held as other influxes source of could be available within the selected plain including ground water and aquifers among others anyhow the proposal of sebaligee with 30 m et values retrieval over agricultural areas followed two global trends the first aims at increasing water productivity through the reduction of water consumption at farm and regional scales the second is related to the simplification and easiness of access to complex models where any non expert personal can be part of the solution at these two levels sebaligee succeeded by providing 30 m et rates within a user friendly easily accessible platform i e through any device having a web browsing capability such as any desktop tablet or mobile and under different operating systems but within a massive computation power and geospatial databases provided by the gee platform users will save on time and resources and greatly reduce biases and errors generated from human errors or through the extrapolation of climatic datasets among others more importantly they will have the ability to store information on the cloud and access it in a timely manner furthermore such information shall help decision makers and local governance bodies as well as farmers and other concerned parties or stakeholders to sustainably manage water resources at any level ultimately sebaligee will contribute towards improving the surface energy balance modelling where any user with prior knowledge of java script programming language could directly and freely alter or improve any component e g leaf area index albedo crop height temperature difference between two heights sensible heat flux etc of the existent code this shall enhance the available environmental and agricultural models requiring accurate 30 m et values while being a stepping stone towards a migration of such models into the freely accessible gee platform future studies could focus on applying sebaligee with different available sensors e g landsat 5 landsat 7 advanced spaceborne thermal emission and reflectance radiometer aster to assess historical et rates and trends satellite datasets used as inputs could be improved whenever better spatial spectral or temporal resolutions are available on the gee platform such as a better lc or dem maps moreover calibrating and validating sebaligee in other climatic regions is recommended finally including sebaligee within a water accounting system such as water accounting plus wa is also advisable for a comprehensive water assessment approach 5 conclusion sebaligee is an automated user friendly flexible open access 30 m et retrieval system its main features are the estimations of et and kc in diverse climatic regions i e continental semi arid monsoon influenced warm summer humid continental mediterranean hot summer and oceanic climates with the readily available landsat 8 30 m satellite images the importance of implementing such model is to acquire a much needed et related information particularly in regions missing critical inputs for running surface energy balance models including soil and climate related datasets this valuable information shall assist in improving water productivity at any scale and destined for any concerned party it perfectly fits within the sustainable development goals sdg and the 2030 agenda proposed by the united nation un sebaligee was calibrated and validated in different climatic zones with an accuracy over 80 when implemented over a large plain in the state of california between 2017 and 2019 crop water consumptions were assessed and the total annual water consumed in the selected region were deduced aquaculture was consuming the largest amount of water with 1109 72 215 07 mm year among all crops type mint i e 780 61 163 mm year and rye i e 735 60 257 81 mm year crops showed the least average water consumed at pixel level what was alarming is that over the total basin shrubland and grassland pasture account for 41 11 of the total consumed water sebaligee is freely available from the authors for research and educational purposes at https code earthengine google com 48200ed2b76ff4acc530c618bb047635 new releases will be progressively mentioned on the official national council for scientific research website http www cnrs edu lb as new methods for crop classification soil related datasets retrieval and the application for other sensors e g landsat 5 landsat 7 advanced spaceborne thermal emission and reflectance radiometer aster and sentinel 2 will be implemented in this on going project users are welcome to send their feedbacks concerning sebaligee by contacting the corresponding author 6 software availability name of the software sebaligee phone 961 4 409 845 e mail rsensing cnrs edu lb first available 2020 minimum requirements any device with a web browsing capability platform any but with a web browsing capability availability through gee platform at https code earthengine google com 48200ed2b76ff4acc530c618bb047635 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25968,in this study an open source google earth engine based gee system is proposed surnamed sebaligee based on the surface energy balance for land improved sebali model sebaligee has the ability to produce monthly evapotranspiration et assessments with an automatic fetching of the required inputs based on satellite datasets already hosted within the gee platform calibrations followed by validations were made in five different countries i e united states china italy germany and belgium between 2013 and 2017 it was followed by the application of the sebaligee over a 307 000 km2 in the plain region in the state of california united states between 2017 and 2019 as an average in these selected years aquaculture appeared to be consuming the largest amount of water with 1109 72 215 07 mm year the proposed open source system aims at improving the assessment of et and water productivity towards better management of water resources keywords sebali google earth engine open source water productivity farm scale 1 introduction within a global trend towards improving national food security the assessment of water needs and demands are of high priority this could be done through the retrieval of evapotranspiration et rates in the agricultural lands nasrallah et al 2019 usually predicted to be consuming 70 of the total available water resources zurayk and dirar 2019 mcnabb 2019 such direction is coinciding as well with another global trend related to the increase of water productivity without affecting farmer s economic situation and crop performance mhawej et al 2020a nasrallah et al 2020 while several ground based approaches were proposed in the literature e g aquastat eliasson et al 2003 cropsyst stöckle et al 2003 and cropwatch bingfang and qiangzi 2004 nowadays remote sensing and modeling based methods is providing the most adequate option for the retrieval of the et related information more particularly the advancement of satellite sensors in terms of better spatial temporal and spectral resolutions coupled with the free availability of different data sources enabled a much reduction in time personal and resources faour et al 2013 2018 ghoussein et al 2018 in this context several surface energy balance seb models were proposed since the 1990 including among others the surface energy balance algorithm for land sebal bastiaanssen et al 1998a 1998b the mapping evapotranspiration at high resolution using internalized calibration metric allen et al 2005 2007 the surface energy balance index sebi menenti and choudhury 1993 the surface energy balance system sebs su 2002a 2002b and lately the sebal improved or sebali mhawej et al 2020a 2020b sebali has tried and succeeded in filling the gaps and in reducing the standard generated biases in the previous seb mhawej et al 2020a 2020b particularly in data lacking countries still the basic concept in these models reamains the retrieval of et which is considered as a residual of the surface energy balance liou and kar 2014 to facilitate access to such much needed et information three free to access et estimations platforms stand out first sebal open source python script or py sebal is available at https github com wateraccounting sebal with further documentations accessible on the ihe delft website i e www un ihe org its installation requires a specialized software i e anaconda as many libraries are missing in a standard python version furthermore users have to include the required remote sensing climatic and other datasets manually within the system before generating et related information which could entail many errors when running this python script second the fao water productivity open access portal wapor could be accessed at https wapor apps fao org home wapor 2 1 providing et ready estimations at 250 m 100 m and 30 m scales still users do not have an option to process new data or other locations nor the website will provide such datasets as it is related to a fao project which has ended by the end of 2019 more recently the coarse resolution 250 m wapor datasets were hosted on the google earth engine gee platform e g https developers google com earth engine datasets catalog fao wapor 2 l1 npp d third an earth engine evapotranspiration flux eeflux developed by the consortium of university of nebraska lincoln desert research institute and university of idaho is also available it is a version of metric that operates on the gee platform i e https eeflux level1 appspot com full documentation of the product could be found in irmak and allen 2012 eeflux is produced in a user friendly interface however users cannot alter or amend the available code and such have a limited control represented by a selection of the study site only as any seb model requires different set of information from diverse sources a calibration and a validation of these proposed automated and semi automated systems are particularly needed unfortunately this was not produced in the previous set of free to access et estimations platforms only eeflux tried to compensate by including a questioned user defined adjusted low and high end potential evapotranspiration values the calibration validation process is then mandatory for two facts one related to the good estimation of hot and cold pixels e g timmermans et al 2007 senay et al 2011 silva and marques da silva 2019 the other to the accuracy and biases from the usage of climatic and ground based information e g smith and katz 2013 cammarano et al 2017 lei et al 2019 which could be lacking in many regions and countries even though that sebali has reduced the number of the needed factors and was validated in diverse climatic zones in different countries it could not be migrated as it is into the gee platform unfortunately it appears to be the case for the other available free to access et estimation platforms previously discussed thus in this paper the sebali google earth engine sebaligee is proposed enabling professional and non specialist to asses any region across the globe in an automated and user friendly manners et rates were calibrated and validated in different climatic regions and countries across the globe including united states china italy germany and belgium between 2013 and 2017 the proposed sebaligee enables users to include other agricultural based models maybe the most prominent is the crop identification and mapping alter and even improve the existent system based on their preferences following the proposal of sebaligee a thorough assessment of the large california plain was produced with more than 80 crops type considered the crop water consumption at pixel level and for the studied region was identified classifying the available crops and providing pivotal information for future references 2 materials and methods 2 1 study area the study area fig 1 corresponds to a major plain in the state of california extending over an area of 307 000 km2 the prevailing climate is hot summer mediterranean climate according to the koppen classification with an average annual precipitation of 745 62 mm between 2017 and 2019 based on the north american land data assimilation system nldas data the mean altitude is 919 m a s l agriculture is the main economic activity in the plain with 83 different crop types patterns available almond plantation is widespread over 5045 km2 followed by alfalfa and grapes with 3290 km2 and 2974 km2 respectively according to the united states department of agricultural national agricultural statistics service usda nass averaged between 2017 and 2019 2 2 data background 2 2 1 satellite data six sources of satellite datasets were used in this study with its ability to assess vegetation health and land surface temperature the 30 m landsat 8 satellite data were collected launched on february 11 2013 the eighth landsat satellite provides a revisit time of 16 day with a 30 m spatial resolution for visible and infrared bands as well as two thermal infrared bands at 100 m i e bands 10 and 11 level 1 top of atmosphere toa and level 2 bottom of atmosphere boa images could be acquired from usgs official website i e https earthexplorer usgs gov the processed landsat 8 level 2 datasets which were used in this study correspond to atmospherically corrected satellite images generated from the land surface reflectance code lasrc algorithm this latter uses the first band i e coastal aerosol of landsat 8 level 1 along auxiliary climate data from the moderate resolution imaging spectroradiometer modis and a unique radiative transfer model within the gee platform landsat 8 level 2 could be accessed on https developers google com earth engine datasets catalog landsat lc08 c01 t1 sr hl en as the intention was to work on globally available weather satellites the climate related datasets i e temperature relative humidity and wind speed were acquired from the era5 or the fifth generation ecmwf atmospheric reanalysis of the global climate such reanalysis corresponds to the years 1950 onwards with a total size of about 9 petabytes combining model data with observations from across the world into a globally complete hourly based and consistent dataset using the laws of physics along this product an uncertainty estimate is included providing guidance on where products are expected to be more or less accurate within gee era5 climatic datasets could be accessed on https developers google com earth engine datasets catalog ecmwf era5 monthly bands aimed to improve sunshine duration calculation a product of terra modis satellite was used this latter orbit around the earth from north to south at 10 30 a m local time having coarse spatial resolutions i e between 250 m and 1 km terra modis is viewing the entire earth s surface every one to two days with 36 spectral bands the used product is the 1 km daily snow cover version 6 designated as mod10a1 where cloud cover could be deduced each day and aggregated monthly over the entire region it could be accessed on https developers google com earth engine datasets catalog modis 006 mod10a1 within the gee platform another product of modis was used for the identification of the agricultural lands available at https developers google com earth engine datasets catalog modis 006 mcd12q1 this was used to improve the selection of hot cold pixels as suggested by mhawej et al 2020a 2020b the land cover type i e mcd12q1 1 km version 6 product is a combination of the two terra and aqua satellites providing yearly global land cover types derived from six diverse classification schemes between 2001 and 2018 the main used approach is the application of a supervised classification for both satellites based on their reflectance data the elevation and slope required in sebali were collected from the freely available nasa shuttle radar topographic mission srtm version 4 providing digital elevation data dem for over 80 of the globe the spatial resolution of the data is approximately 90 m and available on gee at https developers google com earth engine datasets catalog cgiar srtm90 v4 finally usda nass cropland data were used for the identification of diverse crops type within the study area between 2017 and 2019 these datasets utilized satellite imagery from the landsat 8 oli tirs sensor the disaster monitoring constellation dmc deimos 1 and uk2 the isro resourcesat 2 liss 3 and the esa sentinel 2 sensors collected during the correspondent growing seasons outputs are 30 m for the continental united states it can be accessed at https developers google com earth engine datasets catalog usda nass cdl 2 2 2 ground data with the effort of several scientists and technicians around the world the fluxnet2015 dataset were established from multiple regional flux networks at more than 200 different sites between 1991 and 2014 the fluxnet2015 dataset includes several data quality control protocols and a data processing pipeline more precisely close interaction with the tower teams were made to improve data quality to propose novel methods for uncertainty quantification and to use of reanalysis data to fill long gaps of meteorological variable records in this study eddy covariance et datasets were used to validate sebaligee et values fig 2 eight different parcels in five main regions corresponding to five different countries were considered based on the availability of ground based et tower datasets after 2013 which coincided with the launch of landsat 8 these parcels correspond to two sites in the united states of america three sites in italy one in china one in germany and one in belgium with flux ground based data between september 2013 and december 2017 the missing months correspond to missing values from the measured ground based et or missing landsat 8 images due to bad weather conditions part of these data were downloaded from the official data portal of the fluxnet community available at https fluxnet fluxdata org the other source of data is based on an eddy covariance et measurement located in donghuayuan town china at 115 7880e 40 3491n between january 2016 and december 2017 outputs are 30 min data obtained by post processing using eddypro software data doi for each site is available in table 1 2 3 the sebaligee system while unlocking the power of the supercomputers and using some of the twenty petabytes of geospatial data hosted within the gee platform gorelick et al 2017 the combination of both gee and remote sensing techniques push remote sensing modeling studies to a new level this is particularly what was produced in this study as the already proposed 30 m et retrieval sebali system mhawej et al 2020a 2020b was migrated into the gee platform for a more global crop water consumption assessment this script is available at https code earthengine google com 48200ed2b76ff4acc530c618bb047635 as sebali requires different set of inputs remote sensing datasets available within gee were used fig 3 more precisely normalized difference vegetation index ndvi albedo and land surface temperature lst were calculated based on the 30 m landsat 8 satellite following the commonly known approaches e g faour and mario 2015 2016 2018 era 5 reanalysis datasets were used to retrieve mean monthly air temperature relative humidity and wind speed it is important to note that the usage of monthly instead of instantaneous climatic values in sebali model was thoroughly described and validated in allam et al 2021 modis snow cover product was integrated to assess monthly cloud coverage in order to better estimate sunshine duration with value ranging between 0 for cloudy areas to 1 for clear conditions agricultural areas were extracted from modis land cover type datasets with these areas corresponding to a value of 1 while the other values were set to null altitude and slope were calculated based on the 90 m srtm global product without any further processing sebaligee fig 4 requires users to only input their studied month and year for instance the month 10 of the year 2019 as well as the study region which is preferred to be larger than 50 km2 as discussed in previous papers e g mhawej et al 2020b users has also the option to include one or multiple parcels for basic statistical analysis sebaligee monthly outputs include several layers that can be exported into google drive including albedo ndvi lst averaged land cover lc for the past three years et and the crop coefficient kc still as different set of data inputs with different sensors were used particularly in terms of reanalyzed climatic data sebaligee et rates were linearly calibrated using randomly selected 30 of the available flux et eddy covariance ground datasets as suggested by many previous research e g singh et al 2012 mhawej et al 2016 2020c whereas 70 of these latter data were used for validations root mean square error rmse absolut mean error ame and r squared values were retrieved it is important to note that five countries with diverse climatic regions were considered i e continental semi arid monsoon influenced warm summer humid continental mediterranean hot summer and oceanic climates among them one in the selected study area furthermore in order to retrieve the crops type in the study region usda nass cropland data were used as it is with over 130 different classes statistical analysis was followed in each of these categories generating the average sum standard deviation variance among others across the 2017 2018 and 2019 years and at monthly basis 3 results 3 1 calibrations and validations without calibration sebaligee showed good results with an rmse of 16 26 mm month an ame of 14 54 mm month and an r squared value of 66 1 still when calibrated and validated using the 30 70 percent approach accuracy has improved even further with an rmse of 11 53 mm month an ame of 9 56 and an r squared value of 80 23 table 2 and fig 5 3 2 crops type water consumption based on the three consecutive years 2017 2018 and 2019 average annual et rates per crop type as well as the total annual water consumption were retrieved and shown in table 3 it was found that aquaculture was consuming the largest amount of water with 1109 72 215 07 mm year per crop type it was followed by pumpkin cabbage and sweet corn with values over 1060 mm year inversely the crops with the least average water consumed at pixel level are barley i e 837 81 225 71 mm year millet i e 814 09 76 21 mm year mint i e 780 61 163 mm year and rye i e 735 60 257 81 mm year over the total basin shrubland and grassland pasture accounted for 41 11 of the total consumed water even though they corresponded to the least consumption of water per pixel i e 733 96 mm year and 841 03 mm year respectively almonds with a total area of nearly 5045 km2 averaged between 2017 and 2019 showed an approximate consumption of 9 of the total consumed water within the considered plain alfalafa for instance constituted a staggering 5 39 of the total water consumption it was followed by grapes walnuts pistachios and winter wheat with approximate values of 5 1 4 3 and 2 9 respectively a violin like distribution of the total et values found in the study area is shown in fig 6 as the average precipitation for the study area is 745 62 mm most of the values exist above this threshold perennial ice and barren lands which were included for references presented et values of 153 93 51 89 mm year and 509 51 326 27 mm year respectively well below the annual precipitation rates 4 discussion sebaligee initial et outputs compared to eddy covariance ground et values yielded acceptable accuracy with a bias of approximately 0 48 mm day still because of the usage of different satellite platforms and the reanalysis climate datasets a further calibration was held the bias has decreased to nearly 0 32 mm day such result is promising and enables the implementation of sebaligee in four different climatic regions i e continental semi arid monsoon influenced warm summer humid continental mediterranean hot summer and oceanic climates across the globe more importantly with an r squared value of nearly 80 users from any background being professionals or non experts in the modeling or remote sensing techniques could use such system to alter their water consumption at parcel regional or national levels predicting future trends and proposing water productivity plans in this context sebaligee was applied in a vast region i e 307 000 km2 in the state of california between 2017 and 2019 the objective was to retrieve the average annual et rates per crop type for more than 80 crops as well as their total annual water consumption and its correspondent percentage of the total considered study area the use of a large study area with a huge number of parcels in each crop type enabled to indirectly include diverse agricultural management plans and existent microclimates within the analysis our findings indicated that aquaculture plantation is consuming the largest amount of water which is aligned with many previous studies e g verdegem et al 2006 2009 calone et al 2019 the low amount of consumed water in some of the crops e g barley millet mint and rye designates limited irrigation application as discussed in the literature e g singh and singh 1995 jamieson et al 1995 chen et al 2016 martel et al 2018 hashemi 2020 in parallel the high consumption of shrubland and grassland pasture over the total considered study area is alarming and should be addressed this could be related to deserted agricultural fields due to diminishing economic returns and stability faour and mario 2014 nasrallah et al 2020 furthermore as the area cultivated for each crop greatly differed crops with the largest areas shown the highest percentage of the consumed water within the considered plain and vice versa this could be highlighted with almonds cultivated in an area of 5045 km2 and having an average 9 of the consumed water between 2017 and 2019 on the other hand squash cultivated over a limited area of 19 km2 represented nearly 1 of the total consumed water nonetheless most of the annual et values appears to be above the annual precipitation rates which could signal that farmers tend to irrigate their fields for better yields still further investigations should be held as other influxes source of could be available within the selected plain including ground water and aquifers among others anyhow the proposal of sebaligee with 30 m et values retrieval over agricultural areas followed two global trends the first aims at increasing water productivity through the reduction of water consumption at farm and regional scales the second is related to the simplification and easiness of access to complex models where any non expert personal can be part of the solution at these two levels sebaligee succeeded by providing 30 m et rates within a user friendly easily accessible platform i e through any device having a web browsing capability such as any desktop tablet or mobile and under different operating systems but within a massive computation power and geospatial databases provided by the gee platform users will save on time and resources and greatly reduce biases and errors generated from human errors or through the extrapolation of climatic datasets among others more importantly they will have the ability to store information on the cloud and access it in a timely manner furthermore such information shall help decision makers and local governance bodies as well as farmers and other concerned parties or stakeholders to sustainably manage water resources at any level ultimately sebaligee will contribute towards improving the surface energy balance modelling where any user with prior knowledge of java script programming language could directly and freely alter or improve any component e g leaf area index albedo crop height temperature difference between two heights sensible heat flux etc of the existent code this shall enhance the available environmental and agricultural models requiring accurate 30 m et values while being a stepping stone towards a migration of such models into the freely accessible gee platform future studies could focus on applying sebaligee with different available sensors e g landsat 5 landsat 7 advanced spaceborne thermal emission and reflectance radiometer aster to assess historical et rates and trends satellite datasets used as inputs could be improved whenever better spatial spectral or temporal resolutions are available on the gee platform such as a better lc or dem maps moreover calibrating and validating sebaligee in other climatic regions is recommended finally including sebaligee within a water accounting system such as water accounting plus wa is also advisable for a comprehensive water assessment approach 5 conclusion sebaligee is an automated user friendly flexible open access 30 m et retrieval system its main features are the estimations of et and kc in diverse climatic regions i e continental semi arid monsoon influenced warm summer humid continental mediterranean hot summer and oceanic climates with the readily available landsat 8 30 m satellite images the importance of implementing such model is to acquire a much needed et related information particularly in regions missing critical inputs for running surface energy balance models including soil and climate related datasets this valuable information shall assist in improving water productivity at any scale and destined for any concerned party it perfectly fits within the sustainable development goals sdg and the 2030 agenda proposed by the united nation un sebaligee was calibrated and validated in different climatic zones with an accuracy over 80 when implemented over a large plain in the state of california between 2017 and 2019 crop water consumptions were assessed and the total annual water consumed in the selected region were deduced aquaculture was consuming the largest amount of water with 1109 72 215 07 mm year among all crops type mint i e 780 61 163 mm year and rye i e 735 60 257 81 mm year crops showed the least average water consumed at pixel level what was alarming is that over the total basin shrubland and grassland pasture account for 41 11 of the total consumed water sebaligee is freely available from the authors for research and educational purposes at https code earthengine google com 48200ed2b76ff4acc530c618bb047635 new releases will be progressively mentioned on the official national council for scientific research website http www cnrs edu lb as new methods for crop classification soil related datasets retrieval and the application for other sensors e g landsat 5 landsat 7 advanced spaceborne thermal emission and reflectance radiometer aster and sentinel 2 will be implemented in this on going project users are welcome to send their feedbacks concerning sebaligee by contacting the corresponding author 6 software availability name of the software sebaligee phone 961 4 409 845 e mail rsensing cnrs edu lb first available 2020 minimum requirements any device with a web browsing capability platform any but with a web browsing capability availability through gee platform at https code earthengine google com 48200ed2b76ff4acc530c618bb047635 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25969,estimation of exposure to air pollution using land use regression lur models often focuses on spatial variation in annual average concentration however temporal variability is known to be an important factor for exposure to estimate the short term street level exposure to black carbon bc we build a spatio temporal lur model by including time dependent variables as predictor variables we developed and evaluated the model based on data from an opportunistic mobile monitoring campaign in which city employees measured black carbon bc during their surveillance tours exposure estimates based on the hourly lur model are more accurate than those based on a fixed site monitoring station or on a spatial lur model and can be used to estimate exposure of cyclists or pedestrians to traffic related pollution based on a gps track we demonstrate the potential of building a real time dynamic pollution map based on unstructured opportunistic measurements to provide personalized exposure information graphical abstract image 1 keywords land use regression spatio temporal cross validation mobile measurements opportunistic monitoring black carbon urban air quality 1 introduction land use regression lur modelling is used to assess spatial variation in air pollution for example to estimate exposure to air pollution in epidemiological studies jerrett et al 2005 hoek et al 2008 brauer et al 2008 beelen et al 2014 in most cases the lur models focus on the spatial variation in annual average concentration and do not include a temporal dimension however in many applications temporal variability is an important factor for exposure for example spatio temporal models can be used in epidemiological studies focusing on an individual specific period such as pregnancy ghosh et al 2012 or days before death maynard et al 2007 in short term exposure studies or to provide personalized exposure information based on gps tracks to incorporate the temporal dimension in lur models different approaches are used in literature one approach is temporal adjustment of annual average model output brauer et al 2008 wu et al 2011 de nazelle et al 2013 in which the annual average exposure at each location is adjusted to temporal variations in air pollution concentrations another approach is to develop separate models for each typical hour dons et al 2013 or for each time period hasenfratz et al 2015 mueller et al 2016 a third approach is to include time dependent data that are related to the temporal variability of the air quality in the model maynard et al 2007 szpiro et al 2010 li et al 2013 patton et al 2014 gryparis et al 2014 montagne et al 2015 ragettli et al 2014 possible time dependent variables include meteorological variables wind speed and direction temperature and air pollution measurements at fixed site monitoring stations in addition to lur models also dispersion models can provide spatially and temporally resolved air pollution estimates for example specifically for the study area the hybrid rio ifdm street canyon model is used to assess high resolution street level concentrations over the city lefebvre et al 2013 dedicated studies about the comparison of lur and dispersion models exist e g de hoogh et al 2014 generally speaking dispersion models have the potential advantage of incorporating both spatial and temporal variation of air pollution without the need for dense monitoring networks on the other hand these models need relatively costly and detailed input data more costly than the input data for lur models require a relatively high level of programming expertise coupled with expensive hardware requirements and make a lot of assumptions about dispersion patterns jerrett et al 2005 beelen et al 2009 2013 in this study we used the third approach to develop a spatio temporal lur model for predicting street level exposure by incorporating time dependent predictor variables in addition to the spatial predictor variables to this end we built upon the spatial lur models described in van den bossche et al 2018 which are based on an opportunistic mobile monitoring campaign in which city wardens gathered data during their surveillance tasks van den bossche et al 2016 but instead of predicting the average concentration of all measurements at each location in this paper we aimed to predict hourly bc concentrations by using the temporal dimension of the mobile measurements the model is validated by predicting the exposure during trips made by the city wardens and is compared to other methods such as temporally adjusted spatial lur models to assess this exposure the potential of using opportunistic measurements to develop a dynamic real time pollution map is discussed 2 materials and methods 2 1 mobile monitoring campaign an opportunistic mobile measurement campaign was carried out with the collaboration of city wardens in the city of antwerp belgium from july 2012 until june 2013 these wardens are city employees with a surveillance task they are outdoors for a large part of the day carrying out surveillance tours on foot or by bicycle these tours do not follow fixed routes or times the wardens measured black carbon bc during their surveillance tasks using the vito airqmap platform 1 1 http www airqmap com the measurement unit consists of a micro aethalometer microaeth model ae51 aethlabs i e a lightweight sensor that allows to measure bc at a high 1 s frequency and a gps locosys genie gt 31 gps three teams of two city wardens each were equipped with a measurement unit and 393 h of raw 1 s measurements were recorded for the three teams combined spread over 110 days most of the measurements were done between 10 a m and 4 p m during working days and performed both on foot and by bike more details on data collection processing and quality control of the micro aethalometer bc measurements can be found in van den bossche et al 2016 the study site the city of antwerp belgium is a medium sized city of 480 000 inhabitants 51 12 n 4 26 e 985 inhabitants km 2 the inner city within the ring road has an area of approximately 16 km2 the study area where measurements were gathered comprises a quarter of this region approximately 3 7 km2 a map indicating the study area is included in the supplementary material figure s1 this region consists mainly of residential and commercial areas including main traffic roads and green areas a highway the ring road is located at the border of the study area additional stationary measurements from the official monitoring stations of the flemish environmental agency vmm were available at these stations bc is measured with multi angle absorption photometry instruments maap petzold and schonlinner 2004 petzold et al 2005 one location with two monitoring stations betr802 and betr801 at the street side and 30 m away from traffic respectively is situated in the study area as indicated infigure s1 2 2 aggregated bc concentrations as described in more detail in van den bossche et al 2016 the roads were divided in segments of approximately 50 m each continuous period of time during which bc measurements are performed within one segment is called a passage the measurement points of each passage were averaged and assigned to the midpoint of that segment i e sampling location if multiple passages occurred within the same segment during a 1 h time frame the passages were averaged because the model will be built at an hourly resolution this resulted in a dataset in which each data point is characterized by x y t with x y the midpoint of a segment and t the timestamp with hourly resolution the number of data points totalled 30 099 because no fixed routes were followed the number of data points was not identical for all segments interquartile range of 9 27 for the validation of the model the data were also aggregated to trips a trip is the path that one team of city wardens follows on their surveillance tour during one day spanning one to multiple hours for each trip the average bc concentration was calculated in total for the three teams combined there were 134 trips spread over 110 days see figure s3 in the supplementary material 2 3 hourly lur model and predictor variables the spatio temporal hourly lur model uses the averaged bc measurements per segment and per hour as the dependent target variable to build the model a non linear regression technique i e support vector regression svr using a radial basis function rbf kernel smola and schölkopf 2004 is used the same spatial predictor variables as in van den bossche et al 2018 are used they include traffic variables traffic intensity road length distance to roads land use population density and street geometry sky view factor defined in the supplementary material those predictor variables are calculated for all sampling locations no time resolution in addition time dependent predictor variables are included in the model in this study these variables are all obtained from monitoring stations of the flemish environmental agency vmm and comprise wind speed and temperature at one location 5 km north of the study area station betm802 and bc concentrations at several stations in the region stations betr801 and betr802 urban traffic located within the study area belal01 and betm802 suburban background located about 4 km and 6 km from the study area respectively betn016 and betn029 rural background located about 80 km and 200 km from the study area respectively all time dependent variables have an hourly resolution a detailed overview of all predictor variables is included in the supplementary material in van den bossche et al 2018 different model building techniques were tested for the spatial lur model including linear and non linear techniques with and without predictor variable selection when using predictor variable selection variables were selected in an iterative manner based on model performance while computationally more intensive this did not yield better results compared to the methods with built in regularization therefore in the present study the non linear regularized svr algorithm using a radial basis function rbf kernel smola and schölkopf 2004 is used with all predictor variables included in the model no explicit predictor variable selection is performed the model itself is evaluated using spatio temporal cross validation see the next section for the different cross validation schemes the hyperparameters of the model are the regularization parameter c and epsilon in the epsilon svr model pedregosa et al 2011 due to computational constraints the hyperparameters are optimized once based on all data using spatial cross validation and not for each fold during cross validation separately further all predictor variables are scaled by subtracting the mean and scaling to unit variance during model building all regression analyses are performed using the python package scikit learn pedregosa et al 2011 2 4 model evaluation spatio temporal cross validation model performance is evaluated by comparing predicted bc concentrations with measured concentrations at different aggregation levels as detailed in the next section the following metrics are used to evaluate model performance coefficient of determination r 2 explained variance ev root mean squared error rmse and bias rmse and bias are expressed in μ g m 3 exact definitions are taken from van den bossche et al 2018 and included in the supplementary material obviously we need to guarantee that data used for model evaluation are spatially and temporally independent from data used for model building in van den bossche et al 2018 a spatial cross validation scheme was used to deal with the spatial autocorrelation in bc concentrations here we also have to deal with the temporal autocorrelation in the bc concentrations therefore the following cross validation cv schemes are distinguished fig 1 spatial cross validation the study area is divided in six zones and to guarantee spatially independent data the model is built based on data of five of the zones and evaluated for the sixth zone see figure s2 in the supplementary material for the zones temporal cross validation similarly as for the spatial cv but now the dataset is divided in time to have temporally independent validation data in practice when evaluating the model for the data of day t the data of day t 2 until day t 2 are left out to build the model spatio temporal cross validation combination of the spatial and temporal cv to evaluate the model for the data of one zone of day t the data of that zone for all days and the data of days t 2 to t 2 for all zones are left out this is illustrated in fig 1 the hatched area is excluded from model building when evaluating for the black area minimal cross validation to evaluate the model for the data of one zone of day t only the data of that spatial zone for days t 2 to t 2 are excluded from model building and not the data of all zones during those days and of all days in that zone in fig 1 the excluded data is represented by the doubly hatched area the time window of two days is based on a temporal autocorrelation analysis of the measurements at the fixed site vmm monitoring station the mobile measurements had no full temporal coverage and consequently it was not possible to calculate the temporal autocorrelation based on these measurements the autocorrelation decreases dramatically with increasing time lags and for a time lag of three days the autocorrelation has dropped to the noise level 0 2 therefore a time window of two days is used to ensure temporally independent data 2 5 evaluation strategies in a first step the model is evaluated using the spatio temporal cross validation scheme this was done at different spatial and temporal aggregation levels the aggregation per segment and per hour as used for the model input a spatial aggregation and a temporal aggregation for the spatial evaluation the predicted and the measured bc concentrations per passage are both aggregated over time to an overall average per segment in the same way as the passages were treated in van den bossche et al 2018 to obtain the data for the spatial lur model i e only segments with a minimum of five passages were used further and using the trimmed mean to calculate the overall average for the temporal evaluation the predicted and measured bc concentrations per passage are now aggregated over all segments to daily averages in a second step the spatio temporal lur model is evaluated on a trip basis as defined above a trip is the path that one team of city wardens follows on their surveillance tour during one day the predicted trip exposure is calculated by relating each point of the gps track to the enveloping segment and hourly time frame and taking the average of the corresponding predictions of the model note that the predictions within one trip can originate from different folds of the cross validation and thus from different models this evaluation is performed for each of the cross validation schemes in addition to the spatio temporal lur model the average trip exposure is also estimated with some other methods i based on a fixed site monitoring station ii based on a spatial lur model and iii based on a spatial lur model with temporal adjustment for the first method the hourly bc concentration at stations betr801 and betr802 is used as the city wide concentration estimate these stations are located within the study area at a traffic site betr802 at the street side and betr801 30 m away from the street at the same site for the method based on a spatial lur the prediction of the svr model using all predictor variables and with spatial cross validation obtained in van den bossche et al 2018 is used to estimate the bc concentration for each location during the trip constant concentration level over time in the third method a temporal adjustment is used to convert the annual average bc concentration levels of the spatial lur model to hourly concentration levels based on the hourly measurements at the monitoring station betr801 the adjusted values are calculated using an additive method as 1 b c adj i j b c i b c ref b c ref j with b c adj i j the adjusted bc concentration level at location i and time j b c i the bc concentration predicted at location i b c ref j the measurement at the monitoring station at time j and b c ref the mean concentration at the station for the full period for each of those additional methods hourly concentration levels are obtained at all locations and from this the predicted trip exposure is calculated in the same way as for the spatio temporal lur model 3 results and discussion 3 1 spatio temporal lur model the predictions from the spatio temporal lur model are illustrated in fig 2 when evaluating the model performance at the aggregation level per segment and per hour a low performance is obtained an r 2 of 0 08 ev of 0 09 using spatio temporal cross validation table 1 also the model r 2 without cross validation is low r 2 of 0 15 however a good performance should not be expected at this level the passages do not cover the full hour and thus do not represent hourly means at a specific location rather they are a momentary concentration level that can be influenced by many other factors and events e g a passing car that cannot be included in the predictor variables and therefore cannot be predicted by the model to get a more meaningful evaluation the data are further aggregated at both a spatial and temporal level to average out part of the inexplicable variance the model is also evaluated based on trips and this is discussed in section 3 2 3 1 1 spatial and temporal evaluation the spatial evaluation yields an r 2 of 0 29 and ev of 0 34 table 1 figure these results are comparable to those of the spatial lur model obtained in van den bossche et al 2018 which used the same bc dataset but with all data spatially aggregated in that paper the equivalent spatial lur model using the same svr model building technique yielded an r 2 and ev of 0 30 it should be noted that this r 2 is the result of the use of a more rigorous spatial cross validation scheme than leave one out cross validation loocv or random hold out folds which are often used in lur studies and which lead to overly optimistic r 2 values this is discussed extensively in van den bossche et al 2018 this comparison to the spatial lur model confirms that the spatio temporal model captures the spatial differences in bc concentrations at least as well as the spatial model the explained variance is even slightly higher although the increased complexity of the model higher number of data points and higher variability in the data points could lead to a lower r 2 some reasons can explain the comparable performance first more data are used to build the spatio temporal model because no passages are excluded the criterion to have at least five passages for each sampling location is not imposed here second more predictor variables i e the time dependent variables are included in the model in the spatial lur model which uses temporally aggregated data as input part of the variability in the input data is related to temporal variations in bc concentrations in the spatio temporal lur model this variation is also modelled to some extent the daily averages for the temporal evaluation are shown in fig 3 b comparison of measured and predicted daily averages resulted in an r 2 of 0 49 and ev of 0 57 table 1 this temporal evaluation shows that the spatio temporal model captures the larger daily temporal variations in the bc concentration levels rather well of course this daily temporal variation is also present in the measured bc concentrations at the monitoring stations that are used as input of the model so this good performance was to be expected when the bc concentrations at the monitoring stations are left out as predictor variables the performance of the model is similar when looking at the spatial evaluation but the temporal variability is not captured well r 2 of 0 14 ev of 0 29 for the temporal evaluation this confirms the importance of including the fixed site measurements in the model to take into account the temporal variability of the urban or regional background in the model rose et al 2011 saraswat et al 2013 patton et al 2014 ragettli et al 2014 montagne et al 2015 for the spatial variation the average daytime traffic intensity is an important variable van den bossche et al 2018 however the actual traffic intensity also shows a temporal variability the inclusion of measurements of fixed site traffic stations could provide a proxy for this temporal variability although the evaluation is done at an aggregated spatial and temporal level the hourly resolution of the model is still essential to enable applications such as the average trip exposure discussed in section 3 2 3 1 2 comparison to other studies comparing the model performance obtained in this study to literature is difficult given the specific set up of the measurement campaign model building and evaluation approach a frequently used approach to obtain temporally resolved air pollution estimates is based on the temporal adjustment of annual average concentrations brauer et al 2008 wu et al 2011 ghosh et al 2012 de nazelle et al 2013 dons et al 2014 typically the temporal variability is introduced by scaling the annual concentrations based on the temporal pattern in concentrations at a fixed site monitoring station when the model is built using temporally integrated data e g measurements with passive no2 samplers temporal adjustment is a straightforward method to introduce temporal variability however given that in those cases the model is not built with temporally resolved data but as a spatial lur model the evaluation results are also not reported based on temporally resolved data hence it is not possible to compare our model performance when focusing on the spatio temporal aspect of the model in this study we included the temporal variability in the lur modelling approach by using time dependent predictor variables in combination with the spatial predictor variables in literature several studies used the same approach to build spatio temporal lur models most of these studies conducted repeated short term measurements at several locations to obtain the data to build the model for example rose et al 2011 built a spatio temporal lur model based on the measurements of 12 two week periods over three years with passive samplers at 42 locations montagne et al 2015 measured bc for 30 min at 80 locations in three different seasons for their spatio temporal lur model they reported a validation ev of 0 21 0 26 mobile monitoring can provide data with a higher temporal resolution although mobile monitoring campaigns are increasingly used as a basis to build lur models most studies are building spatial lur models hankey and marshall 2015 van den bossche et al 2018 kerckhoffs et al 2017 hatzopoulou et al 2017 xu et al 2017 minet et al 2018 patton et al 2014 2015 on the other hand also built a spatio temporal lur model based on mobile measurements based on 1 s resolution measurements of ultrafine particles using a spatial resolution of 20 m and an hourly resolution for the time dependent variables they obtained r 2 values of 0 38 0 53 patton et al 2014 however the reported r 2 values are for the training dataset and not for the validation dataset it is therefore not possible to compare the r 2 values with the performance obtained in the present study another approach to construct a spatio temporal lur model is building separate models for each time period dons et al 2013 built a separate lur model for each hour of the day i e 24 different models the hourly lur models can be considered as annual average models with an hourly resolution and thus only focus on diurnal variability instantaneous models on the other hand only use the measurements performed during a specific time period to predict concentrations at other locations in that same time period thus the model is rebuilt every time period hasenfratz et al 2015 and mueller et al 2016 built such models based on mobile measurements using a tram based sensor network mueller et al 2016 developed models with a temporal resolution of 30 min one model was built for each time period of 30 min throughout the year using all tram based measurements with a resolution of 5 s made during those 30 min they report correlations in the range of 0 67 0 77 however in this evaluation the mobile measurement data were aggregated for 15 min time intervals instead of using the actual data with a resolution of 10 m and 5 s as included in the model during this time interval the tram has already travelled a distance of several hundred meters up to a few kilometres therefore this kind of evaluation is rather comparable to a trip based evaluation as discussed in section 3 2 compared to the approach in this study a much more dense network of measurements is needed to cover many locations within a single time period to build these instantaneous models 3 2 predicting street level exposure during trips the model is evaluated for its ability to predict the average exposure during trips making use of the trips of the city wardens those trips also form the basis of the data to build the model but cross validation is used to ensure the evaluation is performed with independent data the 134 trips have an average duration of 2 8 h and bc concentration of 3 3 μ g m 3 but there is a large variability among the trips see figure s3 in the supplementary material when comparing the predicted and the measured trip exposures an r 2 of 0 72 is obtained using the minimal cv scheme fig 4 a the other cross validation schemes result in slightly higher or lower r 2 values ranging between 0 58 and 0 75 table 2 to benchmark these results some other methods to estimate the exposure during the trips are implemented for comparison first the exposure is estimated based on the hourly measurements from a fixed monitoring station this resulted in an r 2 of 0 23 fig 4b for station betr801 when using station betr802 at the same traffic site but at the street side the same r 2 but a lower ev 0 34 instead of 0 47 is obtained this method shows a high negative bias indicating an underprediction of the average exposure second a spatial lur is used this will only explain the spatial differences in the trip exposure due to the followed route using only this spatial lur an ev of 0 15 is obtained fig 4c third a temporally adjusted spatial lur model yielded an r 2 of 0 56 fig 4d an overview of the results is given in table 2 the results show that the spatio temporal model can predict the trip exposure well the spatio temporal lur model also performs better compared to the other methods it is clear that the temporal variations in the bc concentration levels are the largest determinant of the trip exposure the spatial lur model can only explain 15 of the variance while the prediction based on the fixed monitoring station yields an explained variance of 47 but gives larger errors leading to a lower r 2 of 0 23 the temporally adjusted spatial lur model also gives good results but the spatio temporal lur model yields higher r 2 values for all cross validation schemes the temporal adjustment is constant for the full study area while the spatio temporal lur model can show different temporal profiles at different locations difference in sampling methodology can also contribute to this difference in performance for the spatio temporal lur model both the predicted and the target trip exposures are based on the mobile micro aethalometer data however the prediction based on the fixed monitoring station is based on measurement data from that station although the micro aethalometers have been colocated at the monitoring station resulting in an overall bias of 1 4 compared to the measurements of the station see van den bossche et al 2016 for details difference in measurement equipment and sampling protocol can introduce bias dekoninck et al 2013 used a similar trip based approach to validate a spatio temporal model they predicted the bc exposure of cyclists using an instantaneous model based on mobile noise measurements the average trip exposure was predicted with a correlation of 0 78 pearson coefficient however this model used simultaneous noise measurements to predict the bc concentration this has the advantage of including real time estimates of the traffic intensity in our model no measurements need to be made by the person for whom the trip exposure is predicted and therefore our model is more generally applicable dekoninck et al 2017 obtained a correlation of 0 65 using a similar approach but with the simultaneous noise measurements replaced with a noise map and for different micro environments combined in this paper the surveillance tours of the city wardens were used as trips to estimate the exposure based on the model but this can be generalized to gps tracks from any travel on foot or by bike e g from commuting 3 3 limitations of this study the lur model is limited to certain hours of the day and is applied in a relatively small study area this is a consequence of the available data this paper shows a proof of concept of building a spatio temporal lur model based on unstructured opportunistic mobile monitoring data but ideally this should be tested with data that cover the full day similarly the question of how well the model would perform at a larger scale e g including the periphery and not only the city centre of antwerp remains unanswered as there are no data to validate outside of the city centre in addition to the mobile monitoring data the spatio temporal lur model developed in this study also requires the presence of stationary monitoring stations which might limit the applicability of a similar model in other areas it could be investigated whether other data sources such as large scale models satellite imagery meteorological variables such as boundary layer height etc could serve as alternative sources of temporal predictor variables further dispersion models can also provide spatio temporal estimates of air pollution and could be another alternative to estimate the trip exposures in the comparison with our lur model however we had no high resolution model output readily available for period of the case study an interesting avenue for further research would be the combination of both where the dispersion model results could be augmented with mobile monitoring data 3 4 real time dynamic pollution maps the spatio temporal model can also provide a dynamic pollution map consider a situation where data are collected continuously in an unstructured way a model can be constructed that is continuously updated with these new data the r 2 and ev of the different cv schemes can then be regarded as the predictive ability of the model under different circumstances the minimal cv scheme gives the predictive ability of such a model at a location where previously measurements were performed while measurements are performed during the same period at other locations in the city to update the model in real time however as in the case of the city wardens data collection can be confined to specific areas of the city for other areas no data will be available at all the spatial cv scheme then reflects the predictive ability of the model for locations where no measurements were performed at all but measurements can still be conducted during the same time period at other locations in the city data collection can be confined in time as well e g the city wardens did not measure each day the temporal cv scheme then reflects a situation where no measurements are available for the time period for which the prediction is made but measurements have been performed during other time periods at the same location the most stringent scheme the spatio temporal cv relates to a situation where one wants to predict the bc exposure at a location in the city where no measurements were made and at a time when no measurements are performed at other locations the predictive ability of the model varies for the different cv schemes r 2 of 0 58 0 75 table 2 the highest r 2 values are obtained for the spatial and minimal cv scheme thus adding simultaneous information from similar locations in the other zones increases the predictive ability of the model real time dynamic pollution maps can provide personalized exposure information for example they can be used to estimate exposure of cyclists or pedestrians to traffic related pollution based on a gps track or to find the least polluted route the situations outlined above demonstrate the potential of developing a dynamic real time pollution map based on measurements that are continuously made throughout the city by performing continuous but unstructured opportunistic measurements we are able to dynamically estimate trip exposure with good performance potentially when more devices are deployed and measuring simultaneously or when more measurements are included over time the model can continue to improve note that the spatio temporal lur model in this study did not result in an actual real time map as all data of the full measurement campaign were used not only the past data but also the future data relative to the time period that is being predicted however this work demonstrates the potential of the spatio temporal lur model as an approach to obtain a real time map some other studies on dynamic pollution maps exist for example in the studies of hasenfratz et al 2015 and mueller et al 2016 mentioned above particle number concentration pollution maps were developed based on a tram based mobile network as part of the opensense project further research could explore more intelligent ways to update the model compared to just adding the new measurements to the dataset e g attributing more weight to recent measurements selective use of historical measurements or how many measurements are required to obtain a model with good performance it could also be investigated if data fusion with other data sources such as dispersion models can provide added value comparable with the approach of schneider et al 2017 for real time measurements from a stationary sensor network this is however beyond the scope of this study in the present study we showed that unstructured and opportunistic mobile measurements can be used to estimate exposure of cyclists or pedestrians to traffic related pollution based on a gps track and with additional effort could be used for building real time dynamic pollution maps declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors thank the city of antwerp environmental services and especially the city wardens dienst samen leven stadstoezicht for the collaboration in the measurement campaign we would also like to thank the province of antwerp and the fietsersbond flemish cyclists association for the biking lane data appendix a supplementary data the following is are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104837 
25969,estimation of exposure to air pollution using land use regression lur models often focuses on spatial variation in annual average concentration however temporal variability is known to be an important factor for exposure to estimate the short term street level exposure to black carbon bc we build a spatio temporal lur model by including time dependent variables as predictor variables we developed and evaluated the model based on data from an opportunistic mobile monitoring campaign in which city employees measured black carbon bc during their surveillance tours exposure estimates based on the hourly lur model are more accurate than those based on a fixed site monitoring station or on a spatial lur model and can be used to estimate exposure of cyclists or pedestrians to traffic related pollution based on a gps track we demonstrate the potential of building a real time dynamic pollution map based on unstructured opportunistic measurements to provide personalized exposure information graphical abstract image 1 keywords land use regression spatio temporal cross validation mobile measurements opportunistic monitoring black carbon urban air quality 1 introduction land use regression lur modelling is used to assess spatial variation in air pollution for example to estimate exposure to air pollution in epidemiological studies jerrett et al 2005 hoek et al 2008 brauer et al 2008 beelen et al 2014 in most cases the lur models focus on the spatial variation in annual average concentration and do not include a temporal dimension however in many applications temporal variability is an important factor for exposure for example spatio temporal models can be used in epidemiological studies focusing on an individual specific period such as pregnancy ghosh et al 2012 or days before death maynard et al 2007 in short term exposure studies or to provide personalized exposure information based on gps tracks to incorporate the temporal dimension in lur models different approaches are used in literature one approach is temporal adjustment of annual average model output brauer et al 2008 wu et al 2011 de nazelle et al 2013 in which the annual average exposure at each location is adjusted to temporal variations in air pollution concentrations another approach is to develop separate models for each typical hour dons et al 2013 or for each time period hasenfratz et al 2015 mueller et al 2016 a third approach is to include time dependent data that are related to the temporal variability of the air quality in the model maynard et al 2007 szpiro et al 2010 li et al 2013 patton et al 2014 gryparis et al 2014 montagne et al 2015 ragettli et al 2014 possible time dependent variables include meteorological variables wind speed and direction temperature and air pollution measurements at fixed site monitoring stations in addition to lur models also dispersion models can provide spatially and temporally resolved air pollution estimates for example specifically for the study area the hybrid rio ifdm street canyon model is used to assess high resolution street level concentrations over the city lefebvre et al 2013 dedicated studies about the comparison of lur and dispersion models exist e g de hoogh et al 2014 generally speaking dispersion models have the potential advantage of incorporating both spatial and temporal variation of air pollution without the need for dense monitoring networks on the other hand these models need relatively costly and detailed input data more costly than the input data for lur models require a relatively high level of programming expertise coupled with expensive hardware requirements and make a lot of assumptions about dispersion patterns jerrett et al 2005 beelen et al 2009 2013 in this study we used the third approach to develop a spatio temporal lur model for predicting street level exposure by incorporating time dependent predictor variables in addition to the spatial predictor variables to this end we built upon the spatial lur models described in van den bossche et al 2018 which are based on an opportunistic mobile monitoring campaign in which city wardens gathered data during their surveillance tasks van den bossche et al 2016 but instead of predicting the average concentration of all measurements at each location in this paper we aimed to predict hourly bc concentrations by using the temporal dimension of the mobile measurements the model is validated by predicting the exposure during trips made by the city wardens and is compared to other methods such as temporally adjusted spatial lur models to assess this exposure the potential of using opportunistic measurements to develop a dynamic real time pollution map is discussed 2 materials and methods 2 1 mobile monitoring campaign an opportunistic mobile measurement campaign was carried out with the collaboration of city wardens in the city of antwerp belgium from july 2012 until june 2013 these wardens are city employees with a surveillance task they are outdoors for a large part of the day carrying out surveillance tours on foot or by bicycle these tours do not follow fixed routes or times the wardens measured black carbon bc during their surveillance tasks using the vito airqmap platform 1 1 http www airqmap com the measurement unit consists of a micro aethalometer microaeth model ae51 aethlabs i e a lightweight sensor that allows to measure bc at a high 1 s frequency and a gps locosys genie gt 31 gps three teams of two city wardens each were equipped with a measurement unit and 393 h of raw 1 s measurements were recorded for the three teams combined spread over 110 days most of the measurements were done between 10 a m and 4 p m during working days and performed both on foot and by bike more details on data collection processing and quality control of the micro aethalometer bc measurements can be found in van den bossche et al 2016 the study site the city of antwerp belgium is a medium sized city of 480 000 inhabitants 51 12 n 4 26 e 985 inhabitants km 2 the inner city within the ring road has an area of approximately 16 km2 the study area where measurements were gathered comprises a quarter of this region approximately 3 7 km2 a map indicating the study area is included in the supplementary material figure s1 this region consists mainly of residential and commercial areas including main traffic roads and green areas a highway the ring road is located at the border of the study area additional stationary measurements from the official monitoring stations of the flemish environmental agency vmm were available at these stations bc is measured with multi angle absorption photometry instruments maap petzold and schonlinner 2004 petzold et al 2005 one location with two monitoring stations betr802 and betr801 at the street side and 30 m away from traffic respectively is situated in the study area as indicated infigure s1 2 2 aggregated bc concentrations as described in more detail in van den bossche et al 2016 the roads were divided in segments of approximately 50 m each continuous period of time during which bc measurements are performed within one segment is called a passage the measurement points of each passage were averaged and assigned to the midpoint of that segment i e sampling location if multiple passages occurred within the same segment during a 1 h time frame the passages were averaged because the model will be built at an hourly resolution this resulted in a dataset in which each data point is characterized by x y t with x y the midpoint of a segment and t the timestamp with hourly resolution the number of data points totalled 30 099 because no fixed routes were followed the number of data points was not identical for all segments interquartile range of 9 27 for the validation of the model the data were also aggregated to trips a trip is the path that one team of city wardens follows on their surveillance tour during one day spanning one to multiple hours for each trip the average bc concentration was calculated in total for the three teams combined there were 134 trips spread over 110 days see figure s3 in the supplementary material 2 3 hourly lur model and predictor variables the spatio temporal hourly lur model uses the averaged bc measurements per segment and per hour as the dependent target variable to build the model a non linear regression technique i e support vector regression svr using a radial basis function rbf kernel smola and schölkopf 2004 is used the same spatial predictor variables as in van den bossche et al 2018 are used they include traffic variables traffic intensity road length distance to roads land use population density and street geometry sky view factor defined in the supplementary material those predictor variables are calculated for all sampling locations no time resolution in addition time dependent predictor variables are included in the model in this study these variables are all obtained from monitoring stations of the flemish environmental agency vmm and comprise wind speed and temperature at one location 5 km north of the study area station betm802 and bc concentrations at several stations in the region stations betr801 and betr802 urban traffic located within the study area belal01 and betm802 suburban background located about 4 km and 6 km from the study area respectively betn016 and betn029 rural background located about 80 km and 200 km from the study area respectively all time dependent variables have an hourly resolution a detailed overview of all predictor variables is included in the supplementary material in van den bossche et al 2018 different model building techniques were tested for the spatial lur model including linear and non linear techniques with and without predictor variable selection when using predictor variable selection variables were selected in an iterative manner based on model performance while computationally more intensive this did not yield better results compared to the methods with built in regularization therefore in the present study the non linear regularized svr algorithm using a radial basis function rbf kernel smola and schölkopf 2004 is used with all predictor variables included in the model no explicit predictor variable selection is performed the model itself is evaluated using spatio temporal cross validation see the next section for the different cross validation schemes the hyperparameters of the model are the regularization parameter c and epsilon in the epsilon svr model pedregosa et al 2011 due to computational constraints the hyperparameters are optimized once based on all data using spatial cross validation and not for each fold during cross validation separately further all predictor variables are scaled by subtracting the mean and scaling to unit variance during model building all regression analyses are performed using the python package scikit learn pedregosa et al 2011 2 4 model evaluation spatio temporal cross validation model performance is evaluated by comparing predicted bc concentrations with measured concentrations at different aggregation levels as detailed in the next section the following metrics are used to evaluate model performance coefficient of determination r 2 explained variance ev root mean squared error rmse and bias rmse and bias are expressed in μ g m 3 exact definitions are taken from van den bossche et al 2018 and included in the supplementary material obviously we need to guarantee that data used for model evaluation are spatially and temporally independent from data used for model building in van den bossche et al 2018 a spatial cross validation scheme was used to deal with the spatial autocorrelation in bc concentrations here we also have to deal with the temporal autocorrelation in the bc concentrations therefore the following cross validation cv schemes are distinguished fig 1 spatial cross validation the study area is divided in six zones and to guarantee spatially independent data the model is built based on data of five of the zones and evaluated for the sixth zone see figure s2 in the supplementary material for the zones temporal cross validation similarly as for the spatial cv but now the dataset is divided in time to have temporally independent validation data in practice when evaluating the model for the data of day t the data of day t 2 until day t 2 are left out to build the model spatio temporal cross validation combination of the spatial and temporal cv to evaluate the model for the data of one zone of day t the data of that zone for all days and the data of days t 2 to t 2 for all zones are left out this is illustrated in fig 1 the hatched area is excluded from model building when evaluating for the black area minimal cross validation to evaluate the model for the data of one zone of day t only the data of that spatial zone for days t 2 to t 2 are excluded from model building and not the data of all zones during those days and of all days in that zone in fig 1 the excluded data is represented by the doubly hatched area the time window of two days is based on a temporal autocorrelation analysis of the measurements at the fixed site vmm monitoring station the mobile measurements had no full temporal coverage and consequently it was not possible to calculate the temporal autocorrelation based on these measurements the autocorrelation decreases dramatically with increasing time lags and for a time lag of three days the autocorrelation has dropped to the noise level 0 2 therefore a time window of two days is used to ensure temporally independent data 2 5 evaluation strategies in a first step the model is evaluated using the spatio temporal cross validation scheme this was done at different spatial and temporal aggregation levels the aggregation per segment and per hour as used for the model input a spatial aggregation and a temporal aggregation for the spatial evaluation the predicted and the measured bc concentrations per passage are both aggregated over time to an overall average per segment in the same way as the passages were treated in van den bossche et al 2018 to obtain the data for the spatial lur model i e only segments with a minimum of five passages were used further and using the trimmed mean to calculate the overall average for the temporal evaluation the predicted and measured bc concentrations per passage are now aggregated over all segments to daily averages in a second step the spatio temporal lur model is evaluated on a trip basis as defined above a trip is the path that one team of city wardens follows on their surveillance tour during one day the predicted trip exposure is calculated by relating each point of the gps track to the enveloping segment and hourly time frame and taking the average of the corresponding predictions of the model note that the predictions within one trip can originate from different folds of the cross validation and thus from different models this evaluation is performed for each of the cross validation schemes in addition to the spatio temporal lur model the average trip exposure is also estimated with some other methods i based on a fixed site monitoring station ii based on a spatial lur model and iii based on a spatial lur model with temporal adjustment for the first method the hourly bc concentration at stations betr801 and betr802 is used as the city wide concentration estimate these stations are located within the study area at a traffic site betr802 at the street side and betr801 30 m away from the street at the same site for the method based on a spatial lur the prediction of the svr model using all predictor variables and with spatial cross validation obtained in van den bossche et al 2018 is used to estimate the bc concentration for each location during the trip constant concentration level over time in the third method a temporal adjustment is used to convert the annual average bc concentration levels of the spatial lur model to hourly concentration levels based on the hourly measurements at the monitoring station betr801 the adjusted values are calculated using an additive method as 1 b c adj i j b c i b c ref b c ref j with b c adj i j the adjusted bc concentration level at location i and time j b c i the bc concentration predicted at location i b c ref j the measurement at the monitoring station at time j and b c ref the mean concentration at the station for the full period for each of those additional methods hourly concentration levels are obtained at all locations and from this the predicted trip exposure is calculated in the same way as for the spatio temporal lur model 3 results and discussion 3 1 spatio temporal lur model the predictions from the spatio temporal lur model are illustrated in fig 2 when evaluating the model performance at the aggregation level per segment and per hour a low performance is obtained an r 2 of 0 08 ev of 0 09 using spatio temporal cross validation table 1 also the model r 2 without cross validation is low r 2 of 0 15 however a good performance should not be expected at this level the passages do not cover the full hour and thus do not represent hourly means at a specific location rather they are a momentary concentration level that can be influenced by many other factors and events e g a passing car that cannot be included in the predictor variables and therefore cannot be predicted by the model to get a more meaningful evaluation the data are further aggregated at both a spatial and temporal level to average out part of the inexplicable variance the model is also evaluated based on trips and this is discussed in section 3 2 3 1 1 spatial and temporal evaluation the spatial evaluation yields an r 2 of 0 29 and ev of 0 34 table 1 figure these results are comparable to those of the spatial lur model obtained in van den bossche et al 2018 which used the same bc dataset but with all data spatially aggregated in that paper the equivalent spatial lur model using the same svr model building technique yielded an r 2 and ev of 0 30 it should be noted that this r 2 is the result of the use of a more rigorous spatial cross validation scheme than leave one out cross validation loocv or random hold out folds which are often used in lur studies and which lead to overly optimistic r 2 values this is discussed extensively in van den bossche et al 2018 this comparison to the spatial lur model confirms that the spatio temporal model captures the spatial differences in bc concentrations at least as well as the spatial model the explained variance is even slightly higher although the increased complexity of the model higher number of data points and higher variability in the data points could lead to a lower r 2 some reasons can explain the comparable performance first more data are used to build the spatio temporal model because no passages are excluded the criterion to have at least five passages for each sampling location is not imposed here second more predictor variables i e the time dependent variables are included in the model in the spatial lur model which uses temporally aggregated data as input part of the variability in the input data is related to temporal variations in bc concentrations in the spatio temporal lur model this variation is also modelled to some extent the daily averages for the temporal evaluation are shown in fig 3 b comparison of measured and predicted daily averages resulted in an r 2 of 0 49 and ev of 0 57 table 1 this temporal evaluation shows that the spatio temporal model captures the larger daily temporal variations in the bc concentration levels rather well of course this daily temporal variation is also present in the measured bc concentrations at the monitoring stations that are used as input of the model so this good performance was to be expected when the bc concentrations at the monitoring stations are left out as predictor variables the performance of the model is similar when looking at the spatial evaluation but the temporal variability is not captured well r 2 of 0 14 ev of 0 29 for the temporal evaluation this confirms the importance of including the fixed site measurements in the model to take into account the temporal variability of the urban or regional background in the model rose et al 2011 saraswat et al 2013 patton et al 2014 ragettli et al 2014 montagne et al 2015 for the spatial variation the average daytime traffic intensity is an important variable van den bossche et al 2018 however the actual traffic intensity also shows a temporal variability the inclusion of measurements of fixed site traffic stations could provide a proxy for this temporal variability although the evaluation is done at an aggregated spatial and temporal level the hourly resolution of the model is still essential to enable applications such as the average trip exposure discussed in section 3 2 3 1 2 comparison to other studies comparing the model performance obtained in this study to literature is difficult given the specific set up of the measurement campaign model building and evaluation approach a frequently used approach to obtain temporally resolved air pollution estimates is based on the temporal adjustment of annual average concentrations brauer et al 2008 wu et al 2011 ghosh et al 2012 de nazelle et al 2013 dons et al 2014 typically the temporal variability is introduced by scaling the annual concentrations based on the temporal pattern in concentrations at a fixed site monitoring station when the model is built using temporally integrated data e g measurements with passive no2 samplers temporal adjustment is a straightforward method to introduce temporal variability however given that in those cases the model is not built with temporally resolved data but as a spatial lur model the evaluation results are also not reported based on temporally resolved data hence it is not possible to compare our model performance when focusing on the spatio temporal aspect of the model in this study we included the temporal variability in the lur modelling approach by using time dependent predictor variables in combination with the spatial predictor variables in literature several studies used the same approach to build spatio temporal lur models most of these studies conducted repeated short term measurements at several locations to obtain the data to build the model for example rose et al 2011 built a spatio temporal lur model based on the measurements of 12 two week periods over three years with passive samplers at 42 locations montagne et al 2015 measured bc for 30 min at 80 locations in three different seasons for their spatio temporal lur model they reported a validation ev of 0 21 0 26 mobile monitoring can provide data with a higher temporal resolution although mobile monitoring campaigns are increasingly used as a basis to build lur models most studies are building spatial lur models hankey and marshall 2015 van den bossche et al 2018 kerckhoffs et al 2017 hatzopoulou et al 2017 xu et al 2017 minet et al 2018 patton et al 2014 2015 on the other hand also built a spatio temporal lur model based on mobile measurements based on 1 s resolution measurements of ultrafine particles using a spatial resolution of 20 m and an hourly resolution for the time dependent variables they obtained r 2 values of 0 38 0 53 patton et al 2014 however the reported r 2 values are for the training dataset and not for the validation dataset it is therefore not possible to compare the r 2 values with the performance obtained in the present study another approach to construct a spatio temporal lur model is building separate models for each time period dons et al 2013 built a separate lur model for each hour of the day i e 24 different models the hourly lur models can be considered as annual average models with an hourly resolution and thus only focus on diurnal variability instantaneous models on the other hand only use the measurements performed during a specific time period to predict concentrations at other locations in that same time period thus the model is rebuilt every time period hasenfratz et al 2015 and mueller et al 2016 built such models based on mobile measurements using a tram based sensor network mueller et al 2016 developed models with a temporal resolution of 30 min one model was built for each time period of 30 min throughout the year using all tram based measurements with a resolution of 5 s made during those 30 min they report correlations in the range of 0 67 0 77 however in this evaluation the mobile measurement data were aggregated for 15 min time intervals instead of using the actual data with a resolution of 10 m and 5 s as included in the model during this time interval the tram has already travelled a distance of several hundred meters up to a few kilometres therefore this kind of evaluation is rather comparable to a trip based evaluation as discussed in section 3 2 compared to the approach in this study a much more dense network of measurements is needed to cover many locations within a single time period to build these instantaneous models 3 2 predicting street level exposure during trips the model is evaluated for its ability to predict the average exposure during trips making use of the trips of the city wardens those trips also form the basis of the data to build the model but cross validation is used to ensure the evaluation is performed with independent data the 134 trips have an average duration of 2 8 h and bc concentration of 3 3 μ g m 3 but there is a large variability among the trips see figure s3 in the supplementary material when comparing the predicted and the measured trip exposures an r 2 of 0 72 is obtained using the minimal cv scheme fig 4 a the other cross validation schemes result in slightly higher or lower r 2 values ranging between 0 58 and 0 75 table 2 to benchmark these results some other methods to estimate the exposure during the trips are implemented for comparison first the exposure is estimated based on the hourly measurements from a fixed monitoring station this resulted in an r 2 of 0 23 fig 4b for station betr801 when using station betr802 at the same traffic site but at the street side the same r 2 but a lower ev 0 34 instead of 0 47 is obtained this method shows a high negative bias indicating an underprediction of the average exposure second a spatial lur is used this will only explain the spatial differences in the trip exposure due to the followed route using only this spatial lur an ev of 0 15 is obtained fig 4c third a temporally adjusted spatial lur model yielded an r 2 of 0 56 fig 4d an overview of the results is given in table 2 the results show that the spatio temporal model can predict the trip exposure well the spatio temporal lur model also performs better compared to the other methods it is clear that the temporal variations in the bc concentration levels are the largest determinant of the trip exposure the spatial lur model can only explain 15 of the variance while the prediction based on the fixed monitoring station yields an explained variance of 47 but gives larger errors leading to a lower r 2 of 0 23 the temporally adjusted spatial lur model also gives good results but the spatio temporal lur model yields higher r 2 values for all cross validation schemes the temporal adjustment is constant for the full study area while the spatio temporal lur model can show different temporal profiles at different locations difference in sampling methodology can also contribute to this difference in performance for the spatio temporal lur model both the predicted and the target trip exposures are based on the mobile micro aethalometer data however the prediction based on the fixed monitoring station is based on measurement data from that station although the micro aethalometers have been colocated at the monitoring station resulting in an overall bias of 1 4 compared to the measurements of the station see van den bossche et al 2016 for details difference in measurement equipment and sampling protocol can introduce bias dekoninck et al 2013 used a similar trip based approach to validate a spatio temporal model they predicted the bc exposure of cyclists using an instantaneous model based on mobile noise measurements the average trip exposure was predicted with a correlation of 0 78 pearson coefficient however this model used simultaneous noise measurements to predict the bc concentration this has the advantage of including real time estimates of the traffic intensity in our model no measurements need to be made by the person for whom the trip exposure is predicted and therefore our model is more generally applicable dekoninck et al 2017 obtained a correlation of 0 65 using a similar approach but with the simultaneous noise measurements replaced with a noise map and for different micro environments combined in this paper the surveillance tours of the city wardens were used as trips to estimate the exposure based on the model but this can be generalized to gps tracks from any travel on foot or by bike e g from commuting 3 3 limitations of this study the lur model is limited to certain hours of the day and is applied in a relatively small study area this is a consequence of the available data this paper shows a proof of concept of building a spatio temporal lur model based on unstructured opportunistic mobile monitoring data but ideally this should be tested with data that cover the full day similarly the question of how well the model would perform at a larger scale e g including the periphery and not only the city centre of antwerp remains unanswered as there are no data to validate outside of the city centre in addition to the mobile monitoring data the spatio temporal lur model developed in this study also requires the presence of stationary monitoring stations which might limit the applicability of a similar model in other areas it could be investigated whether other data sources such as large scale models satellite imagery meteorological variables such as boundary layer height etc could serve as alternative sources of temporal predictor variables further dispersion models can also provide spatio temporal estimates of air pollution and could be another alternative to estimate the trip exposures in the comparison with our lur model however we had no high resolution model output readily available for period of the case study an interesting avenue for further research would be the combination of both where the dispersion model results could be augmented with mobile monitoring data 3 4 real time dynamic pollution maps the spatio temporal model can also provide a dynamic pollution map consider a situation where data are collected continuously in an unstructured way a model can be constructed that is continuously updated with these new data the r 2 and ev of the different cv schemes can then be regarded as the predictive ability of the model under different circumstances the minimal cv scheme gives the predictive ability of such a model at a location where previously measurements were performed while measurements are performed during the same period at other locations in the city to update the model in real time however as in the case of the city wardens data collection can be confined to specific areas of the city for other areas no data will be available at all the spatial cv scheme then reflects the predictive ability of the model for locations where no measurements were performed at all but measurements can still be conducted during the same time period at other locations in the city data collection can be confined in time as well e g the city wardens did not measure each day the temporal cv scheme then reflects a situation where no measurements are available for the time period for which the prediction is made but measurements have been performed during other time periods at the same location the most stringent scheme the spatio temporal cv relates to a situation where one wants to predict the bc exposure at a location in the city where no measurements were made and at a time when no measurements are performed at other locations the predictive ability of the model varies for the different cv schemes r 2 of 0 58 0 75 table 2 the highest r 2 values are obtained for the spatial and minimal cv scheme thus adding simultaneous information from similar locations in the other zones increases the predictive ability of the model real time dynamic pollution maps can provide personalized exposure information for example they can be used to estimate exposure of cyclists or pedestrians to traffic related pollution based on a gps track or to find the least polluted route the situations outlined above demonstrate the potential of developing a dynamic real time pollution map based on measurements that are continuously made throughout the city by performing continuous but unstructured opportunistic measurements we are able to dynamically estimate trip exposure with good performance potentially when more devices are deployed and measuring simultaneously or when more measurements are included over time the model can continue to improve note that the spatio temporal lur model in this study did not result in an actual real time map as all data of the full measurement campaign were used not only the past data but also the future data relative to the time period that is being predicted however this work demonstrates the potential of the spatio temporal lur model as an approach to obtain a real time map some other studies on dynamic pollution maps exist for example in the studies of hasenfratz et al 2015 and mueller et al 2016 mentioned above particle number concentration pollution maps were developed based on a tram based mobile network as part of the opensense project further research could explore more intelligent ways to update the model compared to just adding the new measurements to the dataset e g attributing more weight to recent measurements selective use of historical measurements or how many measurements are required to obtain a model with good performance it could also be investigated if data fusion with other data sources such as dispersion models can provide added value comparable with the approach of schneider et al 2017 for real time measurements from a stationary sensor network this is however beyond the scope of this study in the present study we showed that unstructured and opportunistic mobile measurements can be used to estimate exposure of cyclists or pedestrians to traffic related pollution based on a gps track and with additional effort could be used for building real time dynamic pollution maps declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors thank the city of antwerp environmental services and especially the city wardens dienst samen leven stadstoezicht for the collaboration in the measurement campaign we would also like to thank the province of antwerp and the fietsersbond flemish cyclists association for the biking lane data appendix a supplementary data the following is are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104837 
