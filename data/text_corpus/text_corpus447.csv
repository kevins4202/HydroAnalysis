index,text
2235,to timely prompt anticipatory operations and relieve prolonged droughts effectively multiple hydrological forecasts that provide thorough information about future streamflow need to be employed however it is challenging to integrate multiple forecasts in reservoir optimization and obtain a proper trading off between current and future water supply benefits this study proposes a novel model predictive control mpc that can not only utilize the streamflow forecast but also utilize other two categorical forecasts including the regime state forecast to capture the long term persistence of the streamflow process and the annual streamflow volume state forecast in the novel mpc a discount factor depending on the annual streamflow volume state forecast weighs up the current deficit cost decided by the streamflow forecast and the future cost function determined by the regime state forecast taking the biliuhe reservoir in china as a case study the value of each forecast in drought mitigation is evaluated results show that with an appropriate discount factor using seasonal streamflow forecast can relieve intense deficits for extreme droughts and avoid unnecessary intense limitations for slight droughts utilizing the annual streamflow volume state forecast contributes to guaranteeing a reliable water supply and employing the regime state forecast relieves the intensity of severe water deficit by incorporating these hydrological forecasts a 31 86 performance gain can be obtained with only a 5 03 reliability decrease compared to the baseline stochastic dynamic programming informed by no forecast information nevertheless forecast value is beset by forecast uncertainty for streamflow forecast forecast value decreases with increasing forecast uncertainty but for the other two categorical forecasts their forecast value depends on not only the forecast accuracy but also the hydrologic conditions and forecast bias keywords model predictive control regime state forecast annual streamflow volume state forecast forecast informed reservoir operations drought management forecast uncertainty data availability the authors do not have permission to share data 1 introduction drought is one of the most devastating natural hazards around the world borgomeo et al 2015 veldkamp et al 2015 zhang and zhou 2015 in recent years changing hydro climatic and socio economic conditions are aggravating water scarcity anaraki et al 2021 farrokhi et al 2021 kadkhodazadeh et al 2022 trindade et al 2017 reservoirs that regulate runoff play an important role in satisfying water demands scientific reservoir operating policies provide rational water allocation over time mohammadi et al 2019 yaseen et al 2019 which is of great importance during droughts restricting the water supply mildly at the beginning of the drought with a hedging rule helps to avoid the happening of severe water deficits later however it is challenging to devise drought management strategies that weigh up current and future water supply benefits appropriately traditional reservoir operating policies are conditioned on simple information including time of the year and water storage taghian et al 2014 tu et al 2008 they are devised to maximize overall reservoir operation benefits with historical streamflow sequences since their decisions are not conditioned on the latest hydrometeorological information these traditional policies do not adapt to hydrological variability well and fail to inform timely water curtailments during droughts denaro et al 2017 giuliani et al 2015 libisch lehner et al 2019 to prompt anticipatory operations significant research efforts have been devoted to utilizing hydrologic information in developing management policies denaro et al 2017 giuliani et al 2015 giuliani et al 2019 libisch lehner et al 2019 streamflow forecast that depicts the streamflow sequence over the forecast horizon is one of the widely used hydrologic information it has been employed to improve reservoir operations in flood control ding et al 2015 zhu et al 2017 hydropower generation anghileri et al 2019 xu et al 2014 water supply anghileri et al 2016 turner et al 2017 and multi objective operations ficchì et al 2016 pianosi and soncini sessa 2009 while utilizing streamflow forecasts to inform reservoir operation the spatial and temporal correlation between multisite streamflow forecasts and between streamflow forecasts and other forecast variables is taken into consideration using copula functions mo et al 2021 xu et al 2019 xu et al 2022 for reservoirs with large storage mainly operated for long term purposes such as water supply long term forecasts are beneficial for reducing severe water scarcity due to recent advancements in forecast techniques seasonal streamflow forecast services become available in some specific regions that are influenced by strong climate signals such as el nino southern oscillation or have particular hydrologic characteristics such as snow dominance areas their utility in improving water supply performance during droughts has been demonstrated anghileri et al 2016 li et al 2014 anghileri et al 2016 draw on seasonally skillful long term streamflow forecasts to relieve intensive water deficits during prolonged dry spells li et al 2014 took advantage of season ahead inflow forecasts to inform inter basin water transfers and improve the overall water supply reliability in the context of increasing water demand nonetheless for water supply systems susceptible to multiyear drought the streamflow forecast with seasonal lead time falls short of providing enough hydrologic information turner and galelli 2016 broke this limitation by incorporating the regime like behavior in streamflow time series in a novel stochastic dynamic programming sdp model to derive reservoir water supply policies offline characterized by long term persistence and temporal clustering of extreme events the regime states depict the hydrologic condition i e dry normal and wet over months or years bracken et al 2014 tan et al 2017 turner and galelli 2016 they are driven by large scale climate signals such as el niño southern oscillation gelati et al 2010 and pacific decadal oscillation akintug and rasmussen 2005 incorporating the regime states in the reservoir operation model can capture low frequency climate variability without knowing the underlying drivers of hydrologic processes comprehensively turner s research shows that regime like behavior is a major cause of the suboptimality of traditional sdp water supply operations exploiting the regime state information allows for initiating a crucial hedge at the beginning of a multi year drought which avoids the happening of severe water shortages in the future notwithstanding the above sdp method incorporating the regime state is beset by excessive hedges for instance when the long term regime state is dry water limitation would be suggested but the short term inflow could be high at the same time under this circumstance the water limitation is unbeneficial due to soon refilled reservoir storage model predictive control mpc is a promising way to remedy this problem it implements real time dynamic optimization with the latest streamflow forecast to revise the offline policy considering both current benefits and future benefits on top of this the offline sdp model dependent on the regime state may guide inappropriate water curtailments during short dry regime episodes when water storage is ample to satisfy water demand in this case the annual streamflow volume state that depicts the hydrologic state i e dry normal and wet over a year would be a complement to the long run hydrologic information the addition of it allows for depicting the characteristics of the hydrologic process more completely and suggests flexible water supply decisions giuliani et al 2019 libisch lehner et al 2019 turner and galelli 2016 yet to our knowledge the method to employ all the above hydrologic forecasts including the streamflow forecast the regime state forecast and the annual streamflow volume state forecast in water supply operations has not been investigated yet in this research we propose a series of novel optimization models to not only accommodate the streamflow forecast but also employ the regime state forecast and the annual streamflow volume state forecast based on the formulation of model predictive control mpc the reservoir operation under mpc models with and without each hydrologic forecast is compared to quantify potential improvements owing to incorporating forecast information and to investigate the role of each hydrologic forecast in drought management furthermore numerical experiments are devised to analyze the effect of forecast uncertainty on water supply performance for each piece of forecast information this promotes a better understanding of decision making under uncertainties and facilitates the efficient use of forecast information with the proposed models 2 methodology fig 1 illustrates the process of modeling the forecast informed real time reservoir optimal operation and analyzing the effect of forecast uncertainty to construct the novel forecast based reservoir optimization model different hydrologic forecasts should be simulated first synthetic streamflow forecasts with evolving forecast uncertainty are simulated with the martingale model of forecasting evolution mmfe the categorical forecasts including the regime state forecast and the annual streamflow volume state forecast are synthesized with assumed forecast accuracy rate forecast bias level and the true states the true regime states of a streamflow process are identified and characterized with a hidden markov model hmm the true annual streamflow volume states are determined by the quantiles of the annual streamflow volumes yang et al 2020 then a series of model predictive control mpc models are constructed to take advantage of the streamflow forecast the regime state forecast and the annual streamflow volume state forecast the seasonal streamflow forecast is utilized to calculate water deficit cost over the current operation horizon the regime state is integrated into the future cost function and the annual streamflow volume state is employed to determine the discount factor in this process it is assumed that these forecasts can be known deterministically the contribution of each forecast information to drought mitigation is demonstrated by comparing reservoir water supply operations with and without each hydrologic forecast relative performance gain in relieving water deficit intensity is benchmarked against the conventional sdp model informed by no forecast and the dynamic programming dp model fed by a perfect forecast water supply reliability variation compared to the baseline sdp model is also utilized as a performance index finally the effect of forecast uncertainty on reservoir performance is investigated with several numerical experiments 2 1 hidden markov model hidden markov model hmm a statistical model can label the individual observations of a data sequence as different regime states i e dry normal and wet bracken et al 2014 turner and galelli 2016 zucchini and macdonald 2009 in this model fig 2 a the system is assumed to switch between β hidden regime states which are modeled as a β state markov chain as shown in equation 1 each state determines a probability distribution from which the observed sequence is drawn as shown in equation 2 in other words the modeled observed sequence here the monthly streamflow time sequence comprises β independent probability distributions and the distribution of the observed sequence depends only on the current regime state and not on the previous regime state or previous observations for an observed time series qt t 1 2 t t is the length of observed sequence hmm can be formulated as 1 pr φ t φ t 1 pr φ t φ t 1 φ t 1 β t 2 3 t 2 pr q t q t 1 φ t pr q t φ t t n where φt and φt 1 are the hidden regime state at time t and t 1 respectively φ t represents the regime sequence φ 1 φ 2 φt q t 1 represents the observed sequence q 1 q 2 qt 1 an hmm is described by the transition probability matrix p of the regime sequence the initial distribution of the markov chain δ and the parameters in the probability distribution for the observed sequence the elements of matrix p are the transition probabilities from one regime state to another as shown in equation 3 utilizing a specific gaussian probability distribution to describe the observed sequence under each regime state the parameters to be estimated are the mean μ 1 μ 2 μβ and standard deviation σ 1 σ 2 σβ the set of parameters in hmm to be estimated is θ µ σ p δ 3 p ij pr φ t j φ t 1 i i j 1 β fig 2 b illustrates the process of fitting hmm before fitting the hmm the raw data should be pre processed the log transform is utilized to normalize the observed sequence 4 q trans ln q a where qtrans is the normalized streamflow a is the location parameter and is estimated as follows boswell et al 1979 5 a q min q max q med 2 q min q max 2 q med where q min q max and q med are the minimum maximum and median values of the observed streamflow sequence respectively then the normalized data are deseasonalized by subtracting the monthly means it guarantees long term excursions rather than within year periodicity being recognized in the fitting procedure parameters in hmm are fitted with an r package depmixs4 visser and speekenbrink 2010 by the expectation maximization algorithm em algorithm dempster et al 1977 this algorithm begins with an expectation step to estimate the expected data log likelihood with given hmm parameters θ the expectation step is followed by a maximization step to maximize the expected data log likelihood with respect to the parameters θ these two steps are repeated until an acceptable convergence of parameter values is achieved 2 2 martingale model of forecast evolution the martingale model of forecast evolution mmfe zhao et al 2011 zhao and zhao 2014 is a framework depicting the dynamic evolution of forecast uncertainty that is as the forecast time approaches and more hydrometeorological information is available forecast uncertainty decreases this method can be utilized to generate synthetic streamflow forecasts with specified statistic characteristics based on forecast updates let fw t denote the period w s forecast for the period t s streamflow w represents the period when the forecast is made and the streamflow of the period t is forecasted t w i i 0 1 2 fh fh is the forecast horizon forecast update uw t is the improvement of period w s forecast fw t from the previous period w 1 s forecast fw 1 t 6 u w t f w t f w 1 t assuming the forecast for the current period s streamflow is perfect i e ft t qt the process of forecast uncertainty evolution can be described as follows 7 f t t q t f t 1 t q t u t t f t 2 t q t u t t u t 1 t f t f h t q t i 1 fh u t f h i t where qt is the observed streamflow at period t mmfe simulates the forecast improvement process with the following assumptions for the sequence of forecast update uw w i 1 i 1 2 fh made at period w denoted by u w u w uw w uw w 1 uw w fh 1 1 the mean value of uw w i 1 equals zero which means the sequence of the forecast made at period w denoted by f w f w fw w fw w 1 fw w fh is unbiased 2 uw w i 1 are normally distributed 3 uw 1 w 1 i 1 are independent of uw 2 w 2 i 1 w1 and w2 denote two different periods at which forecasts are made 4 the distribution of uw w i 1 is stationary under these assumptions the statistical properties of forecast updates are captured by a variance covariance matrix vcv 8 vcv var 1 cov 1 2 cov 1 f h cov 2 1 var 2 cov 2 f h cov fh 1 cov fh 2 var fh where var i i 1 2 fh is the variance of uw w i 1 and cov i j i j 1 2 fh and i j is the covariance of uw w i 1 and uw w j 1 since matrix vcv is positive semidefinite it can be decomposed via cholesky decomposition 9 vcv v v t then u ts can be simulated as 10 u w w u w w 1 u w w f h 1 ε 1 ε 2 ε fh v t where εi i 1 2 fh are independent random numbers following identically standard gaussian distribution subsequently the synthetic deterministic streamflow forecast can be generated by incorporating the simulated forecast updates into the equation 7 in this research the vcv matrix is simplified as a triple diagonal matrix 11 vcv σ 2 ρ error σ 2 0 0 ρ error σ 2 σ 2 ρ error σ 2 0 0 ρ error σ 2 σ 2 0 0 0 0 σ 2 where σ represents the standard deviation of the forecast updates ρerror represents the temporal correlation of the forecast updates at two adjacent periods 2 3 model predictive control model predictive control mpc is a flexible and adaptive optimization technique that has been shown to be efficient in real time anticipatory reservoir operations bertsekas 1976 castelletti et al 2008 ficchì et al 2016 pianosi and soncini sessa 2009 based on the sequential online resolution of optimization problems it exploits the up to date streamflow forecasts to improve water supply decisions step by step at each decision period t an optimization problem fed by the latest streamflow forecast is formulated over a finite operation horizon t t h the optimal decisions throughout the operation horizon are obtained but only the decision for the first period is implemented as time moves to the next period t 1 as the streamflow forecast is updated with newly available hydrometeorological information a new optimization problem is built over the horizon t t h 1 and the decision is updated the objective of the online optimization problem is to minimize the sum of the current cost over the operation horizon and the possible future cost which are weighed up by a discount factor as shown in equation 12 it allows for taking both the short term effect and the long term effects of current decisions into consideration 12 min r t 1 t 2 t h τ t t h 1 c τ r τ s τ q τ α f t h s t h the online optimization problem is subject to 13 s τ 1 s τ q τ e τ r τ τ t t h 1 14 z τ min z τ z τ max τ t t h 1 15 0 r τ d τ τ t t h 1 where cτ is the water deficit cost in period τ st h is reservoir storage at the end of the operation horizon which is the carryover storage ft h is the future cost function that sums up all the future costs related to different carryover storages beyond the control horizon h α is the discount factor pianosi and soncini sessa 2009 turner and galelli 2016 qτ is the streamflow forecast in period τ rτ is the average release in period τ sτ and sτ 1 are reservoir storage at the beginning and ending of period τ respectively eτ is average reservoir evaporation in period τ zτ min and zτ max are minimum water level and maximum water level in period τ respectively zτ is reservoir water level at the beginning of the period τ dτ is the water demand in period τ usually one single large deficit has more damaging consequences than several small shortages when evaluating the cost of water deficit a higher penalty should be placed on large water shortages thus the squared shortage ratio is utilized to evaluate the step cost as shown in equation 16 the optimization objective over the entire time horizon is denoted as the sum of the squared shortage ratio sssr this function creates an impetus to cut back the current water supply mildly to distribute the water supply evenly over time and reduce the risk of major deficits 16 c τ r τ s τ q τ max 1 r τ d τ 0 2 the formulation of the future cost function in the mpc model i e ft h equation 12 is essential for making rational water supply decisions if the future cost is overvalued it is more likely to supply less water to retain water for the future and the current hedging may be excessive if the future cost is undervalued it is more likely to supply more water and retain less water for the future and current hedging may be insufficient solving an off line infinite horizon optimization problem via stochastic dynamic programming sdp is a promising way to determine the future cost function castelletti et al 2008 pianosi and soncini sessa 2009 it provides the optimal cost to go functions related to each state and time instant which helps to quantify the future cost related to different carryover storages scientifically the offline optimization problem takes the following form 17 h t 1 s t 1 e q t 1 h t 1 s t 1 q t 1 h t 1 s t 1 q t 1 min r t 1 c t 1 s t 1 q t 1 r t 1 e q t 1 1 q t 1 h t 1 1 s t 1 1 q t 1 1 subject to 13 15 time period τ replaced by time period t 1 t 1 1 2 where ht 1 is the optimal cost to go function at time period t 1 all other variables have the same meaning as the previous context except for time period τ being replaced by time period t 1 the solution of the sdp model provides t 1 optimal cost to go functions h 1 h 2 ht 1 set the future cost function in mpc to the optimal cost to go function ft h hi i mod t h t 1 and mod means the modulus of t h divided by t 1 namely the formulation of ft h is 18 f t h s t h f i s i e q i min r i c i s i q i r i e q i 1 q i f i 1 s i 1 q i 1 2 4 novel model predictive control seasonal streamflow forecast is not enough to guide appropriate anticipatory actions for multi year droughts to make more flexible water supply decisions more information describing the characteristics of streamflow should be incorporated into reservoir operations the regime state depicts the long term persistence and abrupt shifts behavior in streamflow this information helps to inform timely water curtailments during droughts however in short dry regime episodes the hedging policies guided by the regime state could be excessive as short dry regime episodes share the same water supply policy or future cost with long dry regime episodes turner and galelli 2016 the addition of the annual streamflow volume state forecast which contains longer information contributes to remedying the problem the excessive hedging guided by short dry regime episodes can be modified with a neutral or wet annual streamflow volume state in this research a series of novel model predictive control models is proposed to not only utilize the streamflow forecast but also take advantage of the regime state forecast and the annual streamflow volume state forecast in the following context the conventional mpc model employing streamflow forecasts is referred to as mpc based on mpc the novel mpc model that incorporates regime state forecast is referred to as mpc r the optimization objective in mpc r is 19 min r t 1 t 2 t h τ t t h 1 c τ r τ s τ q τ α f t h s t h φ t h where φt h is the discrete regime state variable in period t h the streamflow time series can be classified into two regime states denoted by wet and dry or three regime states denoted by wet dry and neutral and so on based on mpc the novel mpc model that incorporates the annual streamflow volume state forecast is referred to as mpc a mpc a utilizes the annual streamflow volume state by conditioning the discount factor on it the corresponding formulation of the discount factor α vt is determined by analyzing the effect of the discount factor on reservoir performance under various hydrologic conditions the optimization objective in mpc a formulates as follows 20 min r t 1 t 2 t h τ t t h 1 c τ r τ s τ q τ α v ty f t h s t h where vty is the annual streamflow volume state of the year ty which is a categorical forecast it can be classified into two annual streamflow volume states denoted by wet and dry or three states denoted by wet dry and neutral and so on the novel mpc model that exploits streamflow forecasts regime state forecasts and the annual streamflow volume state forecast is referred to as mpc r a the optimization objective in mpc r a formulates as follows 21 min r t 1 t 2 t h τ t t h 1 c τ r τ s τ q τ α v ty f t h s t h φ t h to derive the future cost function in mpc r and mpc r a a regime state variable is integrated into the sdp model in the new sdp model the cost expectation term depends on two transition probability matrixes both of which are obtained from the fitted hmm the outer expectation term utilizes the probabilities of transitioning between regime states the inner expectation term employs the probabilities of transitioning between flow classes under each regime state these probabilities are supplied by a set of first order periodic markov chains constructed with streamflow data belonging to each specific regime state the offline optimization problem takes the following form 22 h t 1 s t 1 φ t 1 e q t 1 h t 1 s t 1 q t 1 φ t 1 h t 1 s t 1 q t 1 φ t 1 min r t 1 c t 1 s t 1 q t 1 r t 1 φ t 1 α e φ t 1 1 φ t 1 e q t 1 1 q t 1 φ t 1 h t 1 1 s t 1 1 q t 1 1 φ t 1 1 subject to 13 15 time period τ replaced by time period t 1 2 5 performance indexes two metrics are utilized to evaluate the performance improvement obtained by utilizing hydrologic forecast information in new mpc models the first metric is the standardized measure of performance gain in the optimization objective function namely minimizing the sum of the squared shortage ratio sssr it is utilized to evaluate to what extent the water deficit intensity is relieved with proposed mpc models in this metric water supply performance under sdp depicted in equation 17 is taken as the lower benchmark because in sdp the operation policy is determined by historical data not considering the latest forecast information water supply performance under dynamic programming dp is taken as the upper benchmark because in dp future inflows are assumed to be perfect forecasts and the related water supply decisions are globally optimal at certain discrete accuracy 23 p e r f o r m a n c e g a i n s s s r c sdp c mpc c sdp c dp where c sdp c mpc and c dp represent the sum of the squared shortage ratio sssr under sdp mpc and dp respectively the performance gain in sssr is positive when the water shortage intensity under mpc is relieved compared to that under sdp the performance gain is negative when the water shortage under mpc is more severe than that under sdp which means the information utilization with mpc is unbeneficial the performance gain is 1 when mpc outperforms sdp and the water shortage intensity reaches the best situation as can be obtained by dp the performance gain is 0 when the water shortage intensity under mpc equals that under sdp when the mpc models considering more forecast information mitigate water deficit intensity in comparison with sdp they would decrease water supply reliability which is the primary water supply request in practice zhang et al 2017 to fully assess reservoir performance water supply reliability variation benchmarked against sdp is introduced as the second metric 24 v r r mpc r sdp r sdp where r sdp and r mpc are the water supply reliability over the entire simulation horizon obtained with sdp and mpc respectively the reliability variation is positive when the water supply under mpc is more reliable than that under sdp the reliability variation is negative when the water supply under mpc is less reliable than that under sdp the reliability variation is 0 when the water supply reliability under mpc equals that under sdp 3 case study the biliuhe reservoir fig 3 located in northeast china is the most important water resource for dalian city liaoning province it collects water from a catchment of 2 085 km2 and has an active storage capacity of 644 million m3 the water resource is unevenly distributed in time with an annual streamflow volume of 528 million m3 and a standard deviation of 289 million m3 75 of the streamflow centralizes in the flood season between june and september fig 4 shows the annual inflow volumes from 1951 to 2019 it can be seen that the reservoir has experienced three multiyear droughts 1989 1993 1999 2003 and 2014 2017 which are recognized by the quantiles of the annual streamflow volumes among them 1999 2003 is the most severe drought followed by 2014 2017 a 69 year 1951 2019 inflow with the time interval being a period of ten days obtained from the basin management administration is used for analysis the mean value of the ten day streamflow volume is 15 million m3 and the standard deviation is 38 million m3 biliuhe reservoir is mainly operated for water supply and flood control and has an over year regulating capacity it serves to meet the water demand of the public and ecology in dalian with a total of 315 million m3 each year since the proportion of ecological demands is quite small they are not considered here for simplicity the public water demand is evenly distributed throughout the year 4 results and discussion 4 1 the value of hydrologic forecast information to evaluate the value of different hydrological forecast information in drought mitigation with the proposed models the reservoir water supply performance of different models is compared new forecast information is introduced between two adjacent models as follows a sdp only utilizes the observations including time of the year and reservoir storage b the mpc model utilizes streamflow forecast other than the simple observations in sdp c the mpc a model utilizes streamflow forecast and annual streamflow volume state forecast other than the simple observations the annual streamflow volume state forecast is utilized to decide the discount factor d the mpc r a model employs the streamflow forecast annual streamflow volume state forecast and regime state forecast other than the simple observations the regime state forecast is incorporated in the cost to go function in this section the forecasts are assumed to be perfect or near perfect to explore the potential of forecast utilization the categorical forecasts including the season ahead regime state forecast and annual streamflow volume state forecast are assumed to be perfect the regime states are calculated with the monthly streamflow sequence of the biliuhe reservoir by the hidden markov model hmm the optimal model order for hmm is 4 according to the bayesian information criteria thus the streamflow sequence is divided into 4 regime states namely extremely dry slightly dry neutral and wet the regimes states are labeled based on the mean value of the streamflow distribution that relates to each state fig 5 shows the fitted regime states and the monthly inflow anomaly it can be seen that the most severe drought 1999 2003 and the second severe multiyear drought 2014 2017 is dominated by extremely dry slightly dry and neutral states the annual streamflow volume states are obtained by classifying biliuhe reservoir annual streamflow sequence into different states the number of states is determined by trial and error the seasonal streamflow forecast with a ten day time step is simulated by mmfe with temporal correlation and the variance of forecast update being 0 and 0 1 m3 s 2 respectively compared to the variance of biliuhe reservoir s ten day inflow sequence of 1808 m3 s 2 the seasonal streamflow forecast error that is the accumulation of forecast updates can be neglected the online optimization for the mpc models is carried out with dynamic programming that provides the globally optimal solution given the level that the decision variable is discretized to in the calculation of sdp which also determines the future cost function of mpc the storage is discretized into 1000 states and the inflow for each month is discretized according to bounding quantiles of 1 00 0 95 0 7125 0 4750 0 2375 and 0 00 stedinger et al 1984 table 1 illustrates the performance gain and the reliability variation of each model each operation period starts from the end of the main flood season when the active storage capacity in flood season is filled and the subsequent operation will not be influenced by the previous hydrologic condition in the mpc model the system performance varies with the discount ratio when the discount ratio is 2 the performance gain in sssr under mpc reaches its peak of 24 02 with a 21 82 decrease in system reliability even though this mpc model alleviates water shortage severity dramatically it also leads to much more frequent water deficits so this mpc model with a constant discount ratio is impractical when the discount ratio is 3 or 5 the performance gain in sssr is negative it indicates that without an appropriate constant discount ratio employing a streamflow forecast would impair reservoir drought mitigation performance in turn after analyzing the influence of the discount ratio on system performance described in the following paragraph and conditioning the discount ratio on the annual streamflow volume state forecast the new mpc a model obtains a performance gain in sssr of 21 04 with only a 0 87 decrease in water supply reliability this demonstrates that the annual streamflow volume state forecast helps to avoid unacceptable water shortages during droughts while guaranteeing a reliable water supply in normal years taking advantage of the regime state forecast the mpc r a model obtains another 10 82 increase in performance gain in sssr compared to the mpc a model with an acceptable decrease in reliability so the regime state forecast contributes to relieving severe water deficits further fig 6 illustrates the effect of the discount ratio on mpc performance apart from the optimization objective which is sssr over the entire time horizon we also calculate sssr over two representative multiyear droughts to provide more insights it can be seen that the relationship between the discount ratio and performance gain in sssr over the entire time horizon is not remarkable however with a rising discount ratio the performance gain in sssr1 over the extreme drought increases the performance gain in sssr2 over the slight drought decreases and the system reliability decrease this is because greater importance is attached to future demand with a larger discount ratio it is more likely to curtail the present water supply to save water for future use more mild water deficits are produced to prevent the happening of one severe deficit this hedging strategy is beneficial for extreme drought and helps to improve reservoir performance in sssr1 but it is excessive for slight drought and worsens reservoir performance in sssr2 it can be concluded that a big discount ratio should be applied when the year is forecast to be extremely dry and a small discount ratio should be applied when the year is forecast to be less dry taking advantage of this principle we determine the value of the discount factor in mpc a according to the annual streamflow volume state forecast by trial and error a high performance gain in sssr is supposed to be obtained without impairing water supply reliability excessively the system performance in mpc a is satisfactory with the discount factor being 4 in extremely dry years 1 in slightly dry years and 0 in not dry years these annual streamflow volume states are determined according to bounding quantiles of 1 0 0 9 0 7 0 yang et al 2020 thus the role of the annual streamflow volume state forecast is to regulate hedging intensity while this contribution is clear the streamflow forecast and the regime state forecast influence water supply decisions in a more complex way their contribution to drought mitigation is illustrated in the following paragraphs 4 1 1 the value of the streamflow forecast to better understand the contribution of streamflow forecast we compare the release and storage trajectories in the two representative multiyear droughts under sdp and the specific mpc model whose discount factor is most valuable to a certain drought as shown in fig 7 it can be seen that for extreme drought represented by 1 9 1998 1 8 2004 mpc generally informs implementing more water supply restrictions than sdp from the beginning of the drought to 8 2002 even when water storage is not quite low and this strategy in mpc helps to reserve more water in storage and decrease the largest water deficit ratio from 85 under sdp to 60 it means that utilizing a seasonal streamflow forecast helps to save more water in advance to relieve severe future water shortages during extreme droughts for slight drought represented by 1 9 2013 1 8 2018 when instructed by sdp sharp supply restrictions are imposed on 7 2017 and 8 2018 when the current water level is low but incoming inflow is high these inappropriate large limitations are avoided after utilizing a seasonal streamflow forecast in the mpc model 4 1 2 the value of the regime state forecast to reveal the contribution of regime state forecast in drought mitigation we first analyze how it influences the future cost function and determines the choice of optimal water supply decisions and then explore its impacts on the release and storage process during a typical multiyear drought fig 8 illustrates the future cost under mpc a and the different regime states of mpc r a as can be seen that the future cost depends only on the within year period and storage state in mpc a while it is also influenced by the regime state in mpc r a for the same water storage and period future costs are higher under a dry state than that under a wet state generally the impact of seasonality in the streamflow on future cost is pronounced in both models it is demonstrated by the smallest future cost during the flood season for the same storage and regime state however among all the regime states the impact of seasonality under extremely dry and slightly dry is less noticeable as the difference in future cost among different months is smaller this reflects that during a dry regime state water supply decisions under mpc r a are more dominated by large scale climate fluctuations rather than seasonality in mpc the future cost influences water supply decisions by influencing the carryover storage which trades off current cost and future cost with simple derivation given in appendix a it can be concluded that when the absolute marginal value of the future cost brought by carryover storage is higher higher carryover storage is better and more hedging is imposed fig 9 compares the relationships between future cost and storage state in each month under mpc r a and mpc a the steeper line indicates a higher absolute marginal value and suggests more hedging among all the lines of mpc r a the solid red lines representing the extremely dry state are the steepest while the blue lines representing the wet state are the flattest the dashed red lines representing the slightly dry state and the neutral state have similar steepness this demonstrates that more hedging is implemented when the regime state is drier under mpc r a in general moreover the difference in steepness between the lines of mpc r a and the black lines of mpc r varies with storage states and months as shown in fig 9 when reservoir storages are high the slopes of the black lines are smaller than that of red lines in all months and are smaller than that of blue lines in months other than june and july this means that under high reservoir storages mpc a suggests less hedging than mpc r a except for the wet state in june and july which belong to flood season when reservoir storages are low the slope of black lines is not smaller than that of red lines in months other than june and july and is greater than that of blue lines in all months this shows that under low reservoir storages mpc a suggests more or similar hedging than mpc r a except for dry and neutral states in july and august which belong to flood season in other words to tackle the same drought mpc r a generally starts to save more water earlier than mpc a which is beneficial for avoiding the happening of extreme water deficits later during droughts and provides better temporal water allocation the exceptional situation in flood season is because the hydrological condition is influenced by both long term excursions indicated by the regime state and the within year seasonality the streamflow in flood season is quite variable and influence water supply decision more when it is in a wet state in flood season the water resource is abundant and it is unnecessary for mpc r a to suggest high hedging under high water storage when it is in dry and neutral states state even in flood season the water resource is deficient and it is beneficial for mpc r a to suggest high hedging under low water storage fig 10 compares the water supply decisions in a typical multiyear drought under mpc a and mpc r a consistent with the previous analysis from 4 1999 to 3 2001 when reservoir storage is relatively high the mpc a model with a single state suggests less hedging than the season ahead neutral extremely dry and slightly dry state of mpc r a and releases more water from 7 2003 to 10 2003 when reservoir storage is low and the season ahead regime state is neutral and beyond the flood season the mpc a model suggests excessive hedging and produces a more severe water shortage compared to it the mpc r a model with the regime state forecast distributes water more evenly throughout the multiyear drought and decreases the largest water deficit ratio from 80 to 60 this result is consistent with previous studies espanmanesh and tilmant 2022 turner and galelli 2016 and show that the incorporation of the regime information can improve the performance of a water resources system 4 2 the effect of forecast uncertainty on water supply performance 4 2 1 numerical experiment setting even though utilizing forecast information via proposed models contributes to improving reservoir water supply performance effectively and is more economical than constructing new infrastructures their value is largely dependent on forecast skill turner et al 2017 yang et al 2020 zhao et al 2011 zhao and zhao 2014 to improve the understanding of forecast utility with proposed models the effect of forecast uncertainty on water supply performance should be explored we devise five numerical experiments the parameter settings of which are shown in table 2 experiment 0 shows the parameters in section 4 1 and it is the baseline in experiment 1 and experiment 2 the impact of streamflow forecast uncertainty is discussed with the key parameters in mmfe experiment 1 investigates the impact of the standard deviation of forecast updates which represents the streamflow forecast uncertainty level experiment 2 investigates the temporal correlation of forecast updates ρerror which represents the forecast improvement correlation for categorical forecasts including the annual streamflow volume state forecast and the regime state forecast the effect of forecast uncertainty is explored by building typical scenarios with forecast accuracy rate pa and pr and forecast bias level la and lr in experiment 3 and experiment 4 the forecast accuracy rate pa and pr evaluates the probability of obtaining an accurate forecast the forecast bias level la and lr represents whether the forecast is wetter or drier than the true value since both categorical forecasts influence the future cost of the mpc model we use new mpc models that involve only one of them namely mpc r and mpc a in the numerical experiments to prevent the effect of categorical forecasts correlation these numerical experiments are simulated with typical droughts when the water storage drop to a certain level and drought management models inform critical decisions turner et al 2017 each numerical experiment is conducted with 30 randomly generated forecasts to eliminate the influence of forecast generation uncertainty 4 2 2 effect of streamflow forecast uncertainty experiment 1 and experiment 2 are performed during the extreme multiyear drought 1 9 1998 1 8 2004 fig 11 illustrates the relationship between performance gain and forecast uncertainty level which is the result of experiment 1 the mean value and standard deviation of performance gain are computed with 30 samples as the forecast uncertainty level increases the mean value of performance gain exhibits a decreasing trend and the standard deviation of performance gain shows an increasing trend it indicates that when the streamflow forecast uncertainty is higher the performance gain is smaller and less stable which makes the information utilization less valuable these results are in line with the conclusions of previous studies that investigate the influence of streamflow forecast uncertainty on real time reservoir flood control operation zhu et al 2017 and on real time reservoir operation with a general utility objective zhao et al 2011 specifically there is no performance gain or even negative gain when the forecast is quite uncertain the critical value is σ 9 in this case σ 9 is about one fifth of the streamflow standard deviation of 42 5 m3 s when σ 9 utilizing streamflow forecast is valuable when σ 9 the system performance is better without utilizing streamflow forecast fig 12 illustrates the relationship between performance gain and forecast improvement correlation which is the result of experiment 2 as the forecast improvement correlation increases the mean value of performance gain exhibits a decreasing trend and the standard deviation of performance gain shows an increasing trend as in experiment 1 this is because when the forecast improvement correlation is negative the overestimated forecast errors are more likely to be balanced by the underestimated errors corresponding to a lower total forecast uncertainty level when the forecast improvement correlation is positive it implies a higher forecast uncertainty level and these results are in accordance with the conclusions of the previous studies zhao et al 2011 zhu et al 2017 4 2 3 effect of regime state forecast uncertainty to fully reveal the impact of different forecast biases in different hydrologic conditions experiment 3 is conducted in the two typical multiyear droughts which are the extreme drought 1 9 1998 1 8 2004 and the slight drought 1 9 2013 1 8 2018 fig 13 illustrates the relationships between the performance gain and forecast accuracy of the regime state forecast under two biased situations the same forecast bias has an opposite effect on the relationship between water supply performance and forecast accuracy for the two droughts when the forecast is wetter than reality performance gain increases with increasing forecast accuracy during 1 9 1998 1 8 2004 but performance gain decreases with increasing forecast accuracy during 1 9 2013 1 8 2018 when the forecast is drier than reality there presents an opposite relationship to our general knowledge a more accurate forecast helps to enhance system performance the unexpected phenomenon here that a less accurate forecast leads to better performance can be attributed to the limitation of the forecast informed optimization model it can be seen by looking into water supply decisions instructed by a perfect regime state with the mpc r model fig 14 compares the release time series guided by the mpc r model with perfect regime forecast sdp and dp during the extreme drought 1 9 1998 1 8 2004 compared to dp which is the ideal upper bound of relieving water shortage severity the mpc r model imposes less hedging and supplies more water before 11 2002 and encounters more severe water shortages from 4 2003 to 6 2004 it can be concluded that the hedging intensity of the mpc r model should be enhanced for better performance when the regime forecast is less accurate and drier than reality more hedging is promoted and it is beneficial for improving system performance during 1 9 2013 1 8 2018 the mpc r model generally supplies less water than dp throughout the drought it implies that the hedging in mpc r is excessive in the slight drought when the forecast is less accurate and wetter than reality less hedging is imposed which helps reduce unbeneficial water curtailments and the hedging in mpc r is more excessive than sdp and the performance gain is negative during this period 4 2 4 effect of annual streamflow volume state forecast uncertainty considering the state of annual streamflow volume influences water supply decisions for a whole year the effect of its forecast uncertainty is investigated with the two representative sequences of ten years including wet years neutral years and dry years i e 1 9 1994 1 8 2004 and 1 9 2008 1 8 2018 the first one includes the typical extreme drought and the second one includes the slight drought fig 15 illustrates the relationships between the performance gain and forecast accuracy of the annual streamflow volume state forecast under two bias situations for 1 9 1994 1 8 2004 when the forecast is wetter than the true value performance gain increases with increasing forecast accuracy which is in line with our general knowledge however when the forecast is drier than reality the relationship between performance gain and forecast accuracy is complex this is because as analyzed in section 4 2 3 a drier forecast is beneficial for making decisions in a dry year but not beneficial for a wet year and a neutral year for 1 9 2008 1 8 2018 whether the forecast is drier or wetter than reality performance gain increases with increasing forecast accuracy this is because when the forecast is perfect with the accuracy being 100 the performance gain reaches 94 9 as shown in fig 16 the water supply process guided by the mpc a model is quite close to the optimal one guided by dp a forecast bias is likely to aggravate reservoir operation performance it can be concluded that for these two categorical forecasts the effect of forecast uncertainty does not only relate to forecast accuracy but also relates to hydrologic conditions and forecast bias in proposed models other than that for both the two periods drier forecast bias in the annual streamflow volume state has less effect on water supply performance than wetter forecast bias as can be seen from fig 16 better performance of mpc a over the lower benchmark sdp relates to reasonable earlier hedge behavior and it is consistent with the effect of drier forecast bias 5 conclusions utilizing future streamflow information in reservoir operation is essential to make proactive drought management decisions and mitigate the negative effects of droughts while skillful streamflow forecast that describes the seasonal streamflow process is relatively short for prolonged droughts and categorical forecast information containing longer term streamflow information is available however it is challenging to integrate multiple forecasts in reservoir optimization and obtain a proper trading off between current and future water supply benefits with hedging rules this study proposed a novel mpc model to employ the streamflow forecast regime state forecast that captures the long term persistence in streamflow process and the annual streamflow volume state forecast that depicts the annual wet or dry state in this model the streamflow forecast determines the current water deficit cost the regime state forecast determines the future cost function and the annual streamflow volume state forecast determines the discount factor that weighs up the current cost against the future cost with the proposed model the value of each forecast in drought mitigation is evaluated and the effect of forecast uncertainty on water supply performance is investigated the application in biliuhe reservoir in china shows 1 with an appropriate discount factor seasonal streamflow forecast can relieve intense deficits for extreme droughts and avoid unnecessary intense limitations for slight droughts 2 comparing the relationship between performance gain and the discount factor under different hydrologic conditions we find that a big discount ratio should be applied during extremely dry years and a small discount ratio should be applied during slightly dry years utilizing the annual streamflow volume state forecast to derive a flexible discount factor aids in guaranteeing a high water supply reliability while alleviating severe water deficits 3 incorporating regime state forecast suggests more hedging under high reservoir storages except for the wet state in flood season and suggests less hedging under low reservoir storages except for the dry and neutral state in flood season it helps to further alleviate severe water shortages during multi year droughts 4 utilizing all the above forecasts with the proposed novel mpc model can obtain a performance gain of 31 86 in relieving the water deficit intensity with only a 5 03 decrease in water supply reliability compared to sdp informed by no forecast this performance gain is 7 84 higher than the maximum performance gain that can be obtained by the conventional mpc and this water supply reliability is 16 79 higher than that under the same conventional mpc the value of the forecast is closely related to the forecast uncertainty performance gain of streamflow forecast decreases and becomes less stable with a more uncertain forecast but for the other two categorical forecasts i e the regime state forecast and annual streamflow volume state forecast forecast value depends on not only the forecast accuracy but also the hydrologic conditions and forecast bias when the regime state forecast is drier than reality during an extreme drought or when the regime state forecast is wetter than reality during a slight drought lower accuracy leads to better drought management instead moreover a drier bias in annual streamflow volume state forecasts always has smaller detrimental effects on system performance than a wetter bias these insights about the effect of forecast uncertainty help to find the most detrimental forecast bias situation under which the threshold of forecast accuracy to determine the forecast utility can be quantified it is also affirmed that forecast value is influenced by not only forecast skill but also how the forecast information transfers into reservoir operation s performance which emphasizes the importance of a flexible reservoir optimization model the significant performance gain attained by utilizing multiple forecasts in novel mpc illustrates an opportunity to develop a practical optimization procedure when all the forecasts are prepared and demonstrated to be able to improve reservoir operation with the forecast precision the novel mpc can be put into practical use this model is flexibility to employ other information according to the actual available forecasts of the study area furthermore applying the new mpc models to other catchments in diverse hydroclimatic regimes with distinct management challenges helps to understand the forecast value better and in a multi reservoir system forecast utilization would be more valuable and more complex which is futher work credit authorship contribution statement chengxin luo conceptualization methodology software data curation writing original draft wei ding supervision writing review editing funding acquisition chi zhang supervision project administration funding acquisition xuan yang visualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research is supported by the national key research and development program of china no 2021yfc3000205 and the national natural science foundation of china no 51925902 appendix a the expression of the optimization objective considered in this study can be simplified as minimizing the sum of current water deficit cost cc sh 1 and future water deficit cost cf sh 1 related to carryover storage a1 min c s h 1 c c s h 1 c f s h 1 assume carryover storage sh 1 1 is one unit higher than sh 1 2 a2 s h 1 1 s h 1 2 s h 1 1 s h 1 2 δ s h 1 then the current cost of sh 1 1 is higher than that of sh 1 2 with less water being supplied currently and the future cost of sh 1 1 is lower than that of sh 1 2 with more water being supplied in the future a3 c c s h 1 1 c c s h 1 2 c f s h 1 1 c f s h 1 2 equation a 4 shows the difference in the optimization objective for two carryover storages among all the mpc models developed in this research the current cost function is the same namely the sum of the squared shortage ratio its value depends on current water availability and how the water deficits are distributed assuming the current deficit distribution is perfect the current cost depends only on current water availability which is determined by the carryover storage so the difference in the current cost for two carryover storages could be constant then the difference in the optimization objective for two carryover storages is determined by the marginal future cost of carryover storage δcf sh 1 δsh 1 if the absolute value of marginal future cost brought by carryover storage is higher then the optimization objective c sh 1 1 is smaller than c sh 1 2 which means that the choice of sh 1 1 is better therefore it can be concluded that a higher absolute value of the marginal future cost helps to choose higher carryover storage a4 c s h 1 1 c s h 1 2 c c s h 1 1 c f s h 1 1 c c s h 1 2 c f s h 1 2 c f s h 1 1 c f s h 1 2 c c s h 1 1 c c s h 1 2 δ c f s h 1 δ s h 1 δ s h 1 c c s h 1 1 c c s h 1 2 
2235,to timely prompt anticipatory operations and relieve prolonged droughts effectively multiple hydrological forecasts that provide thorough information about future streamflow need to be employed however it is challenging to integrate multiple forecasts in reservoir optimization and obtain a proper trading off between current and future water supply benefits this study proposes a novel model predictive control mpc that can not only utilize the streamflow forecast but also utilize other two categorical forecasts including the regime state forecast to capture the long term persistence of the streamflow process and the annual streamflow volume state forecast in the novel mpc a discount factor depending on the annual streamflow volume state forecast weighs up the current deficit cost decided by the streamflow forecast and the future cost function determined by the regime state forecast taking the biliuhe reservoir in china as a case study the value of each forecast in drought mitigation is evaluated results show that with an appropriate discount factor using seasonal streamflow forecast can relieve intense deficits for extreme droughts and avoid unnecessary intense limitations for slight droughts utilizing the annual streamflow volume state forecast contributes to guaranteeing a reliable water supply and employing the regime state forecast relieves the intensity of severe water deficit by incorporating these hydrological forecasts a 31 86 performance gain can be obtained with only a 5 03 reliability decrease compared to the baseline stochastic dynamic programming informed by no forecast information nevertheless forecast value is beset by forecast uncertainty for streamflow forecast forecast value decreases with increasing forecast uncertainty but for the other two categorical forecasts their forecast value depends on not only the forecast accuracy but also the hydrologic conditions and forecast bias keywords model predictive control regime state forecast annual streamflow volume state forecast forecast informed reservoir operations drought management forecast uncertainty data availability the authors do not have permission to share data 1 introduction drought is one of the most devastating natural hazards around the world borgomeo et al 2015 veldkamp et al 2015 zhang and zhou 2015 in recent years changing hydro climatic and socio economic conditions are aggravating water scarcity anaraki et al 2021 farrokhi et al 2021 kadkhodazadeh et al 2022 trindade et al 2017 reservoirs that regulate runoff play an important role in satisfying water demands scientific reservoir operating policies provide rational water allocation over time mohammadi et al 2019 yaseen et al 2019 which is of great importance during droughts restricting the water supply mildly at the beginning of the drought with a hedging rule helps to avoid the happening of severe water deficits later however it is challenging to devise drought management strategies that weigh up current and future water supply benefits appropriately traditional reservoir operating policies are conditioned on simple information including time of the year and water storage taghian et al 2014 tu et al 2008 they are devised to maximize overall reservoir operation benefits with historical streamflow sequences since their decisions are not conditioned on the latest hydrometeorological information these traditional policies do not adapt to hydrological variability well and fail to inform timely water curtailments during droughts denaro et al 2017 giuliani et al 2015 libisch lehner et al 2019 to prompt anticipatory operations significant research efforts have been devoted to utilizing hydrologic information in developing management policies denaro et al 2017 giuliani et al 2015 giuliani et al 2019 libisch lehner et al 2019 streamflow forecast that depicts the streamflow sequence over the forecast horizon is one of the widely used hydrologic information it has been employed to improve reservoir operations in flood control ding et al 2015 zhu et al 2017 hydropower generation anghileri et al 2019 xu et al 2014 water supply anghileri et al 2016 turner et al 2017 and multi objective operations ficchì et al 2016 pianosi and soncini sessa 2009 while utilizing streamflow forecasts to inform reservoir operation the spatial and temporal correlation between multisite streamflow forecasts and between streamflow forecasts and other forecast variables is taken into consideration using copula functions mo et al 2021 xu et al 2019 xu et al 2022 for reservoirs with large storage mainly operated for long term purposes such as water supply long term forecasts are beneficial for reducing severe water scarcity due to recent advancements in forecast techniques seasonal streamflow forecast services become available in some specific regions that are influenced by strong climate signals such as el nino southern oscillation or have particular hydrologic characteristics such as snow dominance areas their utility in improving water supply performance during droughts has been demonstrated anghileri et al 2016 li et al 2014 anghileri et al 2016 draw on seasonally skillful long term streamflow forecasts to relieve intensive water deficits during prolonged dry spells li et al 2014 took advantage of season ahead inflow forecasts to inform inter basin water transfers and improve the overall water supply reliability in the context of increasing water demand nonetheless for water supply systems susceptible to multiyear drought the streamflow forecast with seasonal lead time falls short of providing enough hydrologic information turner and galelli 2016 broke this limitation by incorporating the regime like behavior in streamflow time series in a novel stochastic dynamic programming sdp model to derive reservoir water supply policies offline characterized by long term persistence and temporal clustering of extreme events the regime states depict the hydrologic condition i e dry normal and wet over months or years bracken et al 2014 tan et al 2017 turner and galelli 2016 they are driven by large scale climate signals such as el niño southern oscillation gelati et al 2010 and pacific decadal oscillation akintug and rasmussen 2005 incorporating the regime states in the reservoir operation model can capture low frequency climate variability without knowing the underlying drivers of hydrologic processes comprehensively turner s research shows that regime like behavior is a major cause of the suboptimality of traditional sdp water supply operations exploiting the regime state information allows for initiating a crucial hedge at the beginning of a multi year drought which avoids the happening of severe water shortages in the future notwithstanding the above sdp method incorporating the regime state is beset by excessive hedges for instance when the long term regime state is dry water limitation would be suggested but the short term inflow could be high at the same time under this circumstance the water limitation is unbeneficial due to soon refilled reservoir storage model predictive control mpc is a promising way to remedy this problem it implements real time dynamic optimization with the latest streamflow forecast to revise the offline policy considering both current benefits and future benefits on top of this the offline sdp model dependent on the regime state may guide inappropriate water curtailments during short dry regime episodes when water storage is ample to satisfy water demand in this case the annual streamflow volume state that depicts the hydrologic state i e dry normal and wet over a year would be a complement to the long run hydrologic information the addition of it allows for depicting the characteristics of the hydrologic process more completely and suggests flexible water supply decisions giuliani et al 2019 libisch lehner et al 2019 turner and galelli 2016 yet to our knowledge the method to employ all the above hydrologic forecasts including the streamflow forecast the regime state forecast and the annual streamflow volume state forecast in water supply operations has not been investigated yet in this research we propose a series of novel optimization models to not only accommodate the streamflow forecast but also employ the regime state forecast and the annual streamflow volume state forecast based on the formulation of model predictive control mpc the reservoir operation under mpc models with and without each hydrologic forecast is compared to quantify potential improvements owing to incorporating forecast information and to investigate the role of each hydrologic forecast in drought management furthermore numerical experiments are devised to analyze the effect of forecast uncertainty on water supply performance for each piece of forecast information this promotes a better understanding of decision making under uncertainties and facilitates the efficient use of forecast information with the proposed models 2 methodology fig 1 illustrates the process of modeling the forecast informed real time reservoir optimal operation and analyzing the effect of forecast uncertainty to construct the novel forecast based reservoir optimization model different hydrologic forecasts should be simulated first synthetic streamflow forecasts with evolving forecast uncertainty are simulated with the martingale model of forecasting evolution mmfe the categorical forecasts including the regime state forecast and the annual streamflow volume state forecast are synthesized with assumed forecast accuracy rate forecast bias level and the true states the true regime states of a streamflow process are identified and characterized with a hidden markov model hmm the true annual streamflow volume states are determined by the quantiles of the annual streamflow volumes yang et al 2020 then a series of model predictive control mpc models are constructed to take advantage of the streamflow forecast the regime state forecast and the annual streamflow volume state forecast the seasonal streamflow forecast is utilized to calculate water deficit cost over the current operation horizon the regime state is integrated into the future cost function and the annual streamflow volume state is employed to determine the discount factor in this process it is assumed that these forecasts can be known deterministically the contribution of each forecast information to drought mitigation is demonstrated by comparing reservoir water supply operations with and without each hydrologic forecast relative performance gain in relieving water deficit intensity is benchmarked against the conventional sdp model informed by no forecast and the dynamic programming dp model fed by a perfect forecast water supply reliability variation compared to the baseline sdp model is also utilized as a performance index finally the effect of forecast uncertainty on reservoir performance is investigated with several numerical experiments 2 1 hidden markov model hidden markov model hmm a statistical model can label the individual observations of a data sequence as different regime states i e dry normal and wet bracken et al 2014 turner and galelli 2016 zucchini and macdonald 2009 in this model fig 2 a the system is assumed to switch between β hidden regime states which are modeled as a β state markov chain as shown in equation 1 each state determines a probability distribution from which the observed sequence is drawn as shown in equation 2 in other words the modeled observed sequence here the monthly streamflow time sequence comprises β independent probability distributions and the distribution of the observed sequence depends only on the current regime state and not on the previous regime state or previous observations for an observed time series qt t 1 2 t t is the length of observed sequence hmm can be formulated as 1 pr φ t φ t 1 pr φ t φ t 1 φ t 1 β t 2 3 t 2 pr q t q t 1 φ t pr q t φ t t n where φt and φt 1 are the hidden regime state at time t and t 1 respectively φ t represents the regime sequence φ 1 φ 2 φt q t 1 represents the observed sequence q 1 q 2 qt 1 an hmm is described by the transition probability matrix p of the regime sequence the initial distribution of the markov chain δ and the parameters in the probability distribution for the observed sequence the elements of matrix p are the transition probabilities from one regime state to another as shown in equation 3 utilizing a specific gaussian probability distribution to describe the observed sequence under each regime state the parameters to be estimated are the mean μ 1 μ 2 μβ and standard deviation σ 1 σ 2 σβ the set of parameters in hmm to be estimated is θ µ σ p δ 3 p ij pr φ t j φ t 1 i i j 1 β fig 2 b illustrates the process of fitting hmm before fitting the hmm the raw data should be pre processed the log transform is utilized to normalize the observed sequence 4 q trans ln q a where qtrans is the normalized streamflow a is the location parameter and is estimated as follows boswell et al 1979 5 a q min q max q med 2 q min q max 2 q med where q min q max and q med are the minimum maximum and median values of the observed streamflow sequence respectively then the normalized data are deseasonalized by subtracting the monthly means it guarantees long term excursions rather than within year periodicity being recognized in the fitting procedure parameters in hmm are fitted with an r package depmixs4 visser and speekenbrink 2010 by the expectation maximization algorithm em algorithm dempster et al 1977 this algorithm begins with an expectation step to estimate the expected data log likelihood with given hmm parameters θ the expectation step is followed by a maximization step to maximize the expected data log likelihood with respect to the parameters θ these two steps are repeated until an acceptable convergence of parameter values is achieved 2 2 martingale model of forecast evolution the martingale model of forecast evolution mmfe zhao et al 2011 zhao and zhao 2014 is a framework depicting the dynamic evolution of forecast uncertainty that is as the forecast time approaches and more hydrometeorological information is available forecast uncertainty decreases this method can be utilized to generate synthetic streamflow forecasts with specified statistic characteristics based on forecast updates let fw t denote the period w s forecast for the period t s streamflow w represents the period when the forecast is made and the streamflow of the period t is forecasted t w i i 0 1 2 fh fh is the forecast horizon forecast update uw t is the improvement of period w s forecast fw t from the previous period w 1 s forecast fw 1 t 6 u w t f w t f w 1 t assuming the forecast for the current period s streamflow is perfect i e ft t qt the process of forecast uncertainty evolution can be described as follows 7 f t t q t f t 1 t q t u t t f t 2 t q t u t t u t 1 t f t f h t q t i 1 fh u t f h i t where qt is the observed streamflow at period t mmfe simulates the forecast improvement process with the following assumptions for the sequence of forecast update uw w i 1 i 1 2 fh made at period w denoted by u w u w uw w uw w 1 uw w fh 1 1 the mean value of uw w i 1 equals zero which means the sequence of the forecast made at period w denoted by f w f w fw w fw w 1 fw w fh is unbiased 2 uw w i 1 are normally distributed 3 uw 1 w 1 i 1 are independent of uw 2 w 2 i 1 w1 and w2 denote two different periods at which forecasts are made 4 the distribution of uw w i 1 is stationary under these assumptions the statistical properties of forecast updates are captured by a variance covariance matrix vcv 8 vcv var 1 cov 1 2 cov 1 f h cov 2 1 var 2 cov 2 f h cov fh 1 cov fh 2 var fh where var i i 1 2 fh is the variance of uw w i 1 and cov i j i j 1 2 fh and i j is the covariance of uw w i 1 and uw w j 1 since matrix vcv is positive semidefinite it can be decomposed via cholesky decomposition 9 vcv v v t then u ts can be simulated as 10 u w w u w w 1 u w w f h 1 ε 1 ε 2 ε fh v t where εi i 1 2 fh are independent random numbers following identically standard gaussian distribution subsequently the synthetic deterministic streamflow forecast can be generated by incorporating the simulated forecast updates into the equation 7 in this research the vcv matrix is simplified as a triple diagonal matrix 11 vcv σ 2 ρ error σ 2 0 0 ρ error σ 2 σ 2 ρ error σ 2 0 0 ρ error σ 2 σ 2 0 0 0 0 σ 2 where σ represents the standard deviation of the forecast updates ρerror represents the temporal correlation of the forecast updates at two adjacent periods 2 3 model predictive control model predictive control mpc is a flexible and adaptive optimization technique that has been shown to be efficient in real time anticipatory reservoir operations bertsekas 1976 castelletti et al 2008 ficchì et al 2016 pianosi and soncini sessa 2009 based on the sequential online resolution of optimization problems it exploits the up to date streamflow forecasts to improve water supply decisions step by step at each decision period t an optimization problem fed by the latest streamflow forecast is formulated over a finite operation horizon t t h the optimal decisions throughout the operation horizon are obtained but only the decision for the first period is implemented as time moves to the next period t 1 as the streamflow forecast is updated with newly available hydrometeorological information a new optimization problem is built over the horizon t t h 1 and the decision is updated the objective of the online optimization problem is to minimize the sum of the current cost over the operation horizon and the possible future cost which are weighed up by a discount factor as shown in equation 12 it allows for taking both the short term effect and the long term effects of current decisions into consideration 12 min r t 1 t 2 t h τ t t h 1 c τ r τ s τ q τ α f t h s t h the online optimization problem is subject to 13 s τ 1 s τ q τ e τ r τ τ t t h 1 14 z τ min z τ z τ max τ t t h 1 15 0 r τ d τ τ t t h 1 where cτ is the water deficit cost in period τ st h is reservoir storage at the end of the operation horizon which is the carryover storage ft h is the future cost function that sums up all the future costs related to different carryover storages beyond the control horizon h α is the discount factor pianosi and soncini sessa 2009 turner and galelli 2016 qτ is the streamflow forecast in period τ rτ is the average release in period τ sτ and sτ 1 are reservoir storage at the beginning and ending of period τ respectively eτ is average reservoir evaporation in period τ zτ min and zτ max are minimum water level and maximum water level in period τ respectively zτ is reservoir water level at the beginning of the period τ dτ is the water demand in period τ usually one single large deficit has more damaging consequences than several small shortages when evaluating the cost of water deficit a higher penalty should be placed on large water shortages thus the squared shortage ratio is utilized to evaluate the step cost as shown in equation 16 the optimization objective over the entire time horizon is denoted as the sum of the squared shortage ratio sssr this function creates an impetus to cut back the current water supply mildly to distribute the water supply evenly over time and reduce the risk of major deficits 16 c τ r τ s τ q τ max 1 r τ d τ 0 2 the formulation of the future cost function in the mpc model i e ft h equation 12 is essential for making rational water supply decisions if the future cost is overvalued it is more likely to supply less water to retain water for the future and the current hedging may be excessive if the future cost is undervalued it is more likely to supply more water and retain less water for the future and current hedging may be insufficient solving an off line infinite horizon optimization problem via stochastic dynamic programming sdp is a promising way to determine the future cost function castelletti et al 2008 pianosi and soncini sessa 2009 it provides the optimal cost to go functions related to each state and time instant which helps to quantify the future cost related to different carryover storages scientifically the offline optimization problem takes the following form 17 h t 1 s t 1 e q t 1 h t 1 s t 1 q t 1 h t 1 s t 1 q t 1 min r t 1 c t 1 s t 1 q t 1 r t 1 e q t 1 1 q t 1 h t 1 1 s t 1 1 q t 1 1 subject to 13 15 time period τ replaced by time period t 1 t 1 1 2 where ht 1 is the optimal cost to go function at time period t 1 all other variables have the same meaning as the previous context except for time period τ being replaced by time period t 1 the solution of the sdp model provides t 1 optimal cost to go functions h 1 h 2 ht 1 set the future cost function in mpc to the optimal cost to go function ft h hi i mod t h t 1 and mod means the modulus of t h divided by t 1 namely the formulation of ft h is 18 f t h s t h f i s i e q i min r i c i s i q i r i e q i 1 q i f i 1 s i 1 q i 1 2 4 novel model predictive control seasonal streamflow forecast is not enough to guide appropriate anticipatory actions for multi year droughts to make more flexible water supply decisions more information describing the characteristics of streamflow should be incorporated into reservoir operations the regime state depicts the long term persistence and abrupt shifts behavior in streamflow this information helps to inform timely water curtailments during droughts however in short dry regime episodes the hedging policies guided by the regime state could be excessive as short dry regime episodes share the same water supply policy or future cost with long dry regime episodes turner and galelli 2016 the addition of the annual streamflow volume state forecast which contains longer information contributes to remedying the problem the excessive hedging guided by short dry regime episodes can be modified with a neutral or wet annual streamflow volume state in this research a series of novel model predictive control models is proposed to not only utilize the streamflow forecast but also take advantage of the regime state forecast and the annual streamflow volume state forecast in the following context the conventional mpc model employing streamflow forecasts is referred to as mpc based on mpc the novel mpc model that incorporates regime state forecast is referred to as mpc r the optimization objective in mpc r is 19 min r t 1 t 2 t h τ t t h 1 c τ r τ s τ q τ α f t h s t h φ t h where φt h is the discrete regime state variable in period t h the streamflow time series can be classified into two regime states denoted by wet and dry or three regime states denoted by wet dry and neutral and so on based on mpc the novel mpc model that incorporates the annual streamflow volume state forecast is referred to as mpc a mpc a utilizes the annual streamflow volume state by conditioning the discount factor on it the corresponding formulation of the discount factor α vt is determined by analyzing the effect of the discount factor on reservoir performance under various hydrologic conditions the optimization objective in mpc a formulates as follows 20 min r t 1 t 2 t h τ t t h 1 c τ r τ s τ q τ α v ty f t h s t h where vty is the annual streamflow volume state of the year ty which is a categorical forecast it can be classified into two annual streamflow volume states denoted by wet and dry or three states denoted by wet dry and neutral and so on the novel mpc model that exploits streamflow forecasts regime state forecasts and the annual streamflow volume state forecast is referred to as mpc r a the optimization objective in mpc r a formulates as follows 21 min r t 1 t 2 t h τ t t h 1 c τ r τ s τ q τ α v ty f t h s t h φ t h to derive the future cost function in mpc r and mpc r a a regime state variable is integrated into the sdp model in the new sdp model the cost expectation term depends on two transition probability matrixes both of which are obtained from the fitted hmm the outer expectation term utilizes the probabilities of transitioning between regime states the inner expectation term employs the probabilities of transitioning between flow classes under each regime state these probabilities are supplied by a set of first order periodic markov chains constructed with streamflow data belonging to each specific regime state the offline optimization problem takes the following form 22 h t 1 s t 1 φ t 1 e q t 1 h t 1 s t 1 q t 1 φ t 1 h t 1 s t 1 q t 1 φ t 1 min r t 1 c t 1 s t 1 q t 1 r t 1 φ t 1 α e φ t 1 1 φ t 1 e q t 1 1 q t 1 φ t 1 h t 1 1 s t 1 1 q t 1 1 φ t 1 1 subject to 13 15 time period τ replaced by time period t 1 2 5 performance indexes two metrics are utilized to evaluate the performance improvement obtained by utilizing hydrologic forecast information in new mpc models the first metric is the standardized measure of performance gain in the optimization objective function namely minimizing the sum of the squared shortage ratio sssr it is utilized to evaluate to what extent the water deficit intensity is relieved with proposed mpc models in this metric water supply performance under sdp depicted in equation 17 is taken as the lower benchmark because in sdp the operation policy is determined by historical data not considering the latest forecast information water supply performance under dynamic programming dp is taken as the upper benchmark because in dp future inflows are assumed to be perfect forecasts and the related water supply decisions are globally optimal at certain discrete accuracy 23 p e r f o r m a n c e g a i n s s s r c sdp c mpc c sdp c dp where c sdp c mpc and c dp represent the sum of the squared shortage ratio sssr under sdp mpc and dp respectively the performance gain in sssr is positive when the water shortage intensity under mpc is relieved compared to that under sdp the performance gain is negative when the water shortage under mpc is more severe than that under sdp which means the information utilization with mpc is unbeneficial the performance gain is 1 when mpc outperforms sdp and the water shortage intensity reaches the best situation as can be obtained by dp the performance gain is 0 when the water shortage intensity under mpc equals that under sdp when the mpc models considering more forecast information mitigate water deficit intensity in comparison with sdp they would decrease water supply reliability which is the primary water supply request in practice zhang et al 2017 to fully assess reservoir performance water supply reliability variation benchmarked against sdp is introduced as the second metric 24 v r r mpc r sdp r sdp where r sdp and r mpc are the water supply reliability over the entire simulation horizon obtained with sdp and mpc respectively the reliability variation is positive when the water supply under mpc is more reliable than that under sdp the reliability variation is negative when the water supply under mpc is less reliable than that under sdp the reliability variation is 0 when the water supply reliability under mpc equals that under sdp 3 case study the biliuhe reservoir fig 3 located in northeast china is the most important water resource for dalian city liaoning province it collects water from a catchment of 2 085 km2 and has an active storage capacity of 644 million m3 the water resource is unevenly distributed in time with an annual streamflow volume of 528 million m3 and a standard deviation of 289 million m3 75 of the streamflow centralizes in the flood season between june and september fig 4 shows the annual inflow volumes from 1951 to 2019 it can be seen that the reservoir has experienced three multiyear droughts 1989 1993 1999 2003 and 2014 2017 which are recognized by the quantiles of the annual streamflow volumes among them 1999 2003 is the most severe drought followed by 2014 2017 a 69 year 1951 2019 inflow with the time interval being a period of ten days obtained from the basin management administration is used for analysis the mean value of the ten day streamflow volume is 15 million m3 and the standard deviation is 38 million m3 biliuhe reservoir is mainly operated for water supply and flood control and has an over year regulating capacity it serves to meet the water demand of the public and ecology in dalian with a total of 315 million m3 each year since the proportion of ecological demands is quite small they are not considered here for simplicity the public water demand is evenly distributed throughout the year 4 results and discussion 4 1 the value of hydrologic forecast information to evaluate the value of different hydrological forecast information in drought mitigation with the proposed models the reservoir water supply performance of different models is compared new forecast information is introduced between two adjacent models as follows a sdp only utilizes the observations including time of the year and reservoir storage b the mpc model utilizes streamflow forecast other than the simple observations in sdp c the mpc a model utilizes streamflow forecast and annual streamflow volume state forecast other than the simple observations the annual streamflow volume state forecast is utilized to decide the discount factor d the mpc r a model employs the streamflow forecast annual streamflow volume state forecast and regime state forecast other than the simple observations the regime state forecast is incorporated in the cost to go function in this section the forecasts are assumed to be perfect or near perfect to explore the potential of forecast utilization the categorical forecasts including the season ahead regime state forecast and annual streamflow volume state forecast are assumed to be perfect the regime states are calculated with the monthly streamflow sequence of the biliuhe reservoir by the hidden markov model hmm the optimal model order for hmm is 4 according to the bayesian information criteria thus the streamflow sequence is divided into 4 regime states namely extremely dry slightly dry neutral and wet the regimes states are labeled based on the mean value of the streamflow distribution that relates to each state fig 5 shows the fitted regime states and the monthly inflow anomaly it can be seen that the most severe drought 1999 2003 and the second severe multiyear drought 2014 2017 is dominated by extremely dry slightly dry and neutral states the annual streamflow volume states are obtained by classifying biliuhe reservoir annual streamflow sequence into different states the number of states is determined by trial and error the seasonal streamflow forecast with a ten day time step is simulated by mmfe with temporal correlation and the variance of forecast update being 0 and 0 1 m3 s 2 respectively compared to the variance of biliuhe reservoir s ten day inflow sequence of 1808 m3 s 2 the seasonal streamflow forecast error that is the accumulation of forecast updates can be neglected the online optimization for the mpc models is carried out with dynamic programming that provides the globally optimal solution given the level that the decision variable is discretized to in the calculation of sdp which also determines the future cost function of mpc the storage is discretized into 1000 states and the inflow for each month is discretized according to bounding quantiles of 1 00 0 95 0 7125 0 4750 0 2375 and 0 00 stedinger et al 1984 table 1 illustrates the performance gain and the reliability variation of each model each operation period starts from the end of the main flood season when the active storage capacity in flood season is filled and the subsequent operation will not be influenced by the previous hydrologic condition in the mpc model the system performance varies with the discount ratio when the discount ratio is 2 the performance gain in sssr under mpc reaches its peak of 24 02 with a 21 82 decrease in system reliability even though this mpc model alleviates water shortage severity dramatically it also leads to much more frequent water deficits so this mpc model with a constant discount ratio is impractical when the discount ratio is 3 or 5 the performance gain in sssr is negative it indicates that without an appropriate constant discount ratio employing a streamflow forecast would impair reservoir drought mitigation performance in turn after analyzing the influence of the discount ratio on system performance described in the following paragraph and conditioning the discount ratio on the annual streamflow volume state forecast the new mpc a model obtains a performance gain in sssr of 21 04 with only a 0 87 decrease in water supply reliability this demonstrates that the annual streamflow volume state forecast helps to avoid unacceptable water shortages during droughts while guaranteeing a reliable water supply in normal years taking advantage of the regime state forecast the mpc r a model obtains another 10 82 increase in performance gain in sssr compared to the mpc a model with an acceptable decrease in reliability so the regime state forecast contributes to relieving severe water deficits further fig 6 illustrates the effect of the discount ratio on mpc performance apart from the optimization objective which is sssr over the entire time horizon we also calculate sssr over two representative multiyear droughts to provide more insights it can be seen that the relationship between the discount ratio and performance gain in sssr over the entire time horizon is not remarkable however with a rising discount ratio the performance gain in sssr1 over the extreme drought increases the performance gain in sssr2 over the slight drought decreases and the system reliability decrease this is because greater importance is attached to future demand with a larger discount ratio it is more likely to curtail the present water supply to save water for future use more mild water deficits are produced to prevent the happening of one severe deficit this hedging strategy is beneficial for extreme drought and helps to improve reservoir performance in sssr1 but it is excessive for slight drought and worsens reservoir performance in sssr2 it can be concluded that a big discount ratio should be applied when the year is forecast to be extremely dry and a small discount ratio should be applied when the year is forecast to be less dry taking advantage of this principle we determine the value of the discount factor in mpc a according to the annual streamflow volume state forecast by trial and error a high performance gain in sssr is supposed to be obtained without impairing water supply reliability excessively the system performance in mpc a is satisfactory with the discount factor being 4 in extremely dry years 1 in slightly dry years and 0 in not dry years these annual streamflow volume states are determined according to bounding quantiles of 1 0 0 9 0 7 0 yang et al 2020 thus the role of the annual streamflow volume state forecast is to regulate hedging intensity while this contribution is clear the streamflow forecast and the regime state forecast influence water supply decisions in a more complex way their contribution to drought mitigation is illustrated in the following paragraphs 4 1 1 the value of the streamflow forecast to better understand the contribution of streamflow forecast we compare the release and storage trajectories in the two representative multiyear droughts under sdp and the specific mpc model whose discount factor is most valuable to a certain drought as shown in fig 7 it can be seen that for extreme drought represented by 1 9 1998 1 8 2004 mpc generally informs implementing more water supply restrictions than sdp from the beginning of the drought to 8 2002 even when water storage is not quite low and this strategy in mpc helps to reserve more water in storage and decrease the largest water deficit ratio from 85 under sdp to 60 it means that utilizing a seasonal streamflow forecast helps to save more water in advance to relieve severe future water shortages during extreme droughts for slight drought represented by 1 9 2013 1 8 2018 when instructed by sdp sharp supply restrictions are imposed on 7 2017 and 8 2018 when the current water level is low but incoming inflow is high these inappropriate large limitations are avoided after utilizing a seasonal streamflow forecast in the mpc model 4 1 2 the value of the regime state forecast to reveal the contribution of regime state forecast in drought mitigation we first analyze how it influences the future cost function and determines the choice of optimal water supply decisions and then explore its impacts on the release and storage process during a typical multiyear drought fig 8 illustrates the future cost under mpc a and the different regime states of mpc r a as can be seen that the future cost depends only on the within year period and storage state in mpc a while it is also influenced by the regime state in mpc r a for the same water storage and period future costs are higher under a dry state than that under a wet state generally the impact of seasonality in the streamflow on future cost is pronounced in both models it is demonstrated by the smallest future cost during the flood season for the same storage and regime state however among all the regime states the impact of seasonality under extremely dry and slightly dry is less noticeable as the difference in future cost among different months is smaller this reflects that during a dry regime state water supply decisions under mpc r a are more dominated by large scale climate fluctuations rather than seasonality in mpc the future cost influences water supply decisions by influencing the carryover storage which trades off current cost and future cost with simple derivation given in appendix a it can be concluded that when the absolute marginal value of the future cost brought by carryover storage is higher higher carryover storage is better and more hedging is imposed fig 9 compares the relationships between future cost and storage state in each month under mpc r a and mpc a the steeper line indicates a higher absolute marginal value and suggests more hedging among all the lines of mpc r a the solid red lines representing the extremely dry state are the steepest while the blue lines representing the wet state are the flattest the dashed red lines representing the slightly dry state and the neutral state have similar steepness this demonstrates that more hedging is implemented when the regime state is drier under mpc r a in general moreover the difference in steepness between the lines of mpc r a and the black lines of mpc r varies with storage states and months as shown in fig 9 when reservoir storages are high the slopes of the black lines are smaller than that of red lines in all months and are smaller than that of blue lines in months other than june and july this means that under high reservoir storages mpc a suggests less hedging than mpc r a except for the wet state in june and july which belong to flood season when reservoir storages are low the slope of black lines is not smaller than that of red lines in months other than june and july and is greater than that of blue lines in all months this shows that under low reservoir storages mpc a suggests more or similar hedging than mpc r a except for dry and neutral states in july and august which belong to flood season in other words to tackle the same drought mpc r a generally starts to save more water earlier than mpc a which is beneficial for avoiding the happening of extreme water deficits later during droughts and provides better temporal water allocation the exceptional situation in flood season is because the hydrological condition is influenced by both long term excursions indicated by the regime state and the within year seasonality the streamflow in flood season is quite variable and influence water supply decision more when it is in a wet state in flood season the water resource is abundant and it is unnecessary for mpc r a to suggest high hedging under high water storage when it is in dry and neutral states state even in flood season the water resource is deficient and it is beneficial for mpc r a to suggest high hedging under low water storage fig 10 compares the water supply decisions in a typical multiyear drought under mpc a and mpc r a consistent with the previous analysis from 4 1999 to 3 2001 when reservoir storage is relatively high the mpc a model with a single state suggests less hedging than the season ahead neutral extremely dry and slightly dry state of mpc r a and releases more water from 7 2003 to 10 2003 when reservoir storage is low and the season ahead regime state is neutral and beyond the flood season the mpc a model suggests excessive hedging and produces a more severe water shortage compared to it the mpc r a model with the regime state forecast distributes water more evenly throughout the multiyear drought and decreases the largest water deficit ratio from 80 to 60 this result is consistent with previous studies espanmanesh and tilmant 2022 turner and galelli 2016 and show that the incorporation of the regime information can improve the performance of a water resources system 4 2 the effect of forecast uncertainty on water supply performance 4 2 1 numerical experiment setting even though utilizing forecast information via proposed models contributes to improving reservoir water supply performance effectively and is more economical than constructing new infrastructures their value is largely dependent on forecast skill turner et al 2017 yang et al 2020 zhao et al 2011 zhao and zhao 2014 to improve the understanding of forecast utility with proposed models the effect of forecast uncertainty on water supply performance should be explored we devise five numerical experiments the parameter settings of which are shown in table 2 experiment 0 shows the parameters in section 4 1 and it is the baseline in experiment 1 and experiment 2 the impact of streamflow forecast uncertainty is discussed with the key parameters in mmfe experiment 1 investigates the impact of the standard deviation of forecast updates which represents the streamflow forecast uncertainty level experiment 2 investigates the temporal correlation of forecast updates ρerror which represents the forecast improvement correlation for categorical forecasts including the annual streamflow volume state forecast and the regime state forecast the effect of forecast uncertainty is explored by building typical scenarios with forecast accuracy rate pa and pr and forecast bias level la and lr in experiment 3 and experiment 4 the forecast accuracy rate pa and pr evaluates the probability of obtaining an accurate forecast the forecast bias level la and lr represents whether the forecast is wetter or drier than the true value since both categorical forecasts influence the future cost of the mpc model we use new mpc models that involve only one of them namely mpc r and mpc a in the numerical experiments to prevent the effect of categorical forecasts correlation these numerical experiments are simulated with typical droughts when the water storage drop to a certain level and drought management models inform critical decisions turner et al 2017 each numerical experiment is conducted with 30 randomly generated forecasts to eliminate the influence of forecast generation uncertainty 4 2 2 effect of streamflow forecast uncertainty experiment 1 and experiment 2 are performed during the extreme multiyear drought 1 9 1998 1 8 2004 fig 11 illustrates the relationship between performance gain and forecast uncertainty level which is the result of experiment 1 the mean value and standard deviation of performance gain are computed with 30 samples as the forecast uncertainty level increases the mean value of performance gain exhibits a decreasing trend and the standard deviation of performance gain shows an increasing trend it indicates that when the streamflow forecast uncertainty is higher the performance gain is smaller and less stable which makes the information utilization less valuable these results are in line with the conclusions of previous studies that investigate the influence of streamflow forecast uncertainty on real time reservoir flood control operation zhu et al 2017 and on real time reservoir operation with a general utility objective zhao et al 2011 specifically there is no performance gain or even negative gain when the forecast is quite uncertain the critical value is σ 9 in this case σ 9 is about one fifth of the streamflow standard deviation of 42 5 m3 s when σ 9 utilizing streamflow forecast is valuable when σ 9 the system performance is better without utilizing streamflow forecast fig 12 illustrates the relationship between performance gain and forecast improvement correlation which is the result of experiment 2 as the forecast improvement correlation increases the mean value of performance gain exhibits a decreasing trend and the standard deviation of performance gain shows an increasing trend as in experiment 1 this is because when the forecast improvement correlation is negative the overestimated forecast errors are more likely to be balanced by the underestimated errors corresponding to a lower total forecast uncertainty level when the forecast improvement correlation is positive it implies a higher forecast uncertainty level and these results are in accordance with the conclusions of the previous studies zhao et al 2011 zhu et al 2017 4 2 3 effect of regime state forecast uncertainty to fully reveal the impact of different forecast biases in different hydrologic conditions experiment 3 is conducted in the two typical multiyear droughts which are the extreme drought 1 9 1998 1 8 2004 and the slight drought 1 9 2013 1 8 2018 fig 13 illustrates the relationships between the performance gain and forecast accuracy of the regime state forecast under two biased situations the same forecast bias has an opposite effect on the relationship between water supply performance and forecast accuracy for the two droughts when the forecast is wetter than reality performance gain increases with increasing forecast accuracy during 1 9 1998 1 8 2004 but performance gain decreases with increasing forecast accuracy during 1 9 2013 1 8 2018 when the forecast is drier than reality there presents an opposite relationship to our general knowledge a more accurate forecast helps to enhance system performance the unexpected phenomenon here that a less accurate forecast leads to better performance can be attributed to the limitation of the forecast informed optimization model it can be seen by looking into water supply decisions instructed by a perfect regime state with the mpc r model fig 14 compares the release time series guided by the mpc r model with perfect regime forecast sdp and dp during the extreme drought 1 9 1998 1 8 2004 compared to dp which is the ideal upper bound of relieving water shortage severity the mpc r model imposes less hedging and supplies more water before 11 2002 and encounters more severe water shortages from 4 2003 to 6 2004 it can be concluded that the hedging intensity of the mpc r model should be enhanced for better performance when the regime forecast is less accurate and drier than reality more hedging is promoted and it is beneficial for improving system performance during 1 9 2013 1 8 2018 the mpc r model generally supplies less water than dp throughout the drought it implies that the hedging in mpc r is excessive in the slight drought when the forecast is less accurate and wetter than reality less hedging is imposed which helps reduce unbeneficial water curtailments and the hedging in mpc r is more excessive than sdp and the performance gain is negative during this period 4 2 4 effect of annual streamflow volume state forecast uncertainty considering the state of annual streamflow volume influences water supply decisions for a whole year the effect of its forecast uncertainty is investigated with the two representative sequences of ten years including wet years neutral years and dry years i e 1 9 1994 1 8 2004 and 1 9 2008 1 8 2018 the first one includes the typical extreme drought and the second one includes the slight drought fig 15 illustrates the relationships between the performance gain and forecast accuracy of the annual streamflow volume state forecast under two bias situations for 1 9 1994 1 8 2004 when the forecast is wetter than the true value performance gain increases with increasing forecast accuracy which is in line with our general knowledge however when the forecast is drier than reality the relationship between performance gain and forecast accuracy is complex this is because as analyzed in section 4 2 3 a drier forecast is beneficial for making decisions in a dry year but not beneficial for a wet year and a neutral year for 1 9 2008 1 8 2018 whether the forecast is drier or wetter than reality performance gain increases with increasing forecast accuracy this is because when the forecast is perfect with the accuracy being 100 the performance gain reaches 94 9 as shown in fig 16 the water supply process guided by the mpc a model is quite close to the optimal one guided by dp a forecast bias is likely to aggravate reservoir operation performance it can be concluded that for these two categorical forecasts the effect of forecast uncertainty does not only relate to forecast accuracy but also relates to hydrologic conditions and forecast bias in proposed models other than that for both the two periods drier forecast bias in the annual streamflow volume state has less effect on water supply performance than wetter forecast bias as can be seen from fig 16 better performance of mpc a over the lower benchmark sdp relates to reasonable earlier hedge behavior and it is consistent with the effect of drier forecast bias 5 conclusions utilizing future streamflow information in reservoir operation is essential to make proactive drought management decisions and mitigate the negative effects of droughts while skillful streamflow forecast that describes the seasonal streamflow process is relatively short for prolonged droughts and categorical forecast information containing longer term streamflow information is available however it is challenging to integrate multiple forecasts in reservoir optimization and obtain a proper trading off between current and future water supply benefits with hedging rules this study proposed a novel mpc model to employ the streamflow forecast regime state forecast that captures the long term persistence in streamflow process and the annual streamflow volume state forecast that depicts the annual wet or dry state in this model the streamflow forecast determines the current water deficit cost the regime state forecast determines the future cost function and the annual streamflow volume state forecast determines the discount factor that weighs up the current cost against the future cost with the proposed model the value of each forecast in drought mitigation is evaluated and the effect of forecast uncertainty on water supply performance is investigated the application in biliuhe reservoir in china shows 1 with an appropriate discount factor seasonal streamflow forecast can relieve intense deficits for extreme droughts and avoid unnecessary intense limitations for slight droughts 2 comparing the relationship between performance gain and the discount factor under different hydrologic conditions we find that a big discount ratio should be applied during extremely dry years and a small discount ratio should be applied during slightly dry years utilizing the annual streamflow volume state forecast to derive a flexible discount factor aids in guaranteeing a high water supply reliability while alleviating severe water deficits 3 incorporating regime state forecast suggests more hedging under high reservoir storages except for the wet state in flood season and suggests less hedging under low reservoir storages except for the dry and neutral state in flood season it helps to further alleviate severe water shortages during multi year droughts 4 utilizing all the above forecasts with the proposed novel mpc model can obtain a performance gain of 31 86 in relieving the water deficit intensity with only a 5 03 decrease in water supply reliability compared to sdp informed by no forecast this performance gain is 7 84 higher than the maximum performance gain that can be obtained by the conventional mpc and this water supply reliability is 16 79 higher than that under the same conventional mpc the value of the forecast is closely related to the forecast uncertainty performance gain of streamflow forecast decreases and becomes less stable with a more uncertain forecast but for the other two categorical forecasts i e the regime state forecast and annual streamflow volume state forecast forecast value depends on not only the forecast accuracy but also the hydrologic conditions and forecast bias when the regime state forecast is drier than reality during an extreme drought or when the regime state forecast is wetter than reality during a slight drought lower accuracy leads to better drought management instead moreover a drier bias in annual streamflow volume state forecasts always has smaller detrimental effects on system performance than a wetter bias these insights about the effect of forecast uncertainty help to find the most detrimental forecast bias situation under which the threshold of forecast accuracy to determine the forecast utility can be quantified it is also affirmed that forecast value is influenced by not only forecast skill but also how the forecast information transfers into reservoir operation s performance which emphasizes the importance of a flexible reservoir optimization model the significant performance gain attained by utilizing multiple forecasts in novel mpc illustrates an opportunity to develop a practical optimization procedure when all the forecasts are prepared and demonstrated to be able to improve reservoir operation with the forecast precision the novel mpc can be put into practical use this model is flexibility to employ other information according to the actual available forecasts of the study area furthermore applying the new mpc models to other catchments in diverse hydroclimatic regimes with distinct management challenges helps to understand the forecast value better and in a multi reservoir system forecast utilization would be more valuable and more complex which is futher work credit authorship contribution statement chengxin luo conceptualization methodology software data curation writing original draft wei ding supervision writing review editing funding acquisition chi zhang supervision project administration funding acquisition xuan yang visualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research is supported by the national key research and development program of china no 2021yfc3000205 and the national natural science foundation of china no 51925902 appendix a the expression of the optimization objective considered in this study can be simplified as minimizing the sum of current water deficit cost cc sh 1 and future water deficit cost cf sh 1 related to carryover storage a1 min c s h 1 c c s h 1 c f s h 1 assume carryover storage sh 1 1 is one unit higher than sh 1 2 a2 s h 1 1 s h 1 2 s h 1 1 s h 1 2 δ s h 1 then the current cost of sh 1 1 is higher than that of sh 1 2 with less water being supplied currently and the future cost of sh 1 1 is lower than that of sh 1 2 with more water being supplied in the future a3 c c s h 1 1 c c s h 1 2 c f s h 1 1 c f s h 1 2 equation a 4 shows the difference in the optimization objective for two carryover storages among all the mpc models developed in this research the current cost function is the same namely the sum of the squared shortage ratio its value depends on current water availability and how the water deficits are distributed assuming the current deficit distribution is perfect the current cost depends only on current water availability which is determined by the carryover storage so the difference in the current cost for two carryover storages could be constant then the difference in the optimization objective for two carryover storages is determined by the marginal future cost of carryover storage δcf sh 1 δsh 1 if the absolute value of marginal future cost brought by carryover storage is higher then the optimization objective c sh 1 1 is smaller than c sh 1 2 which means that the choice of sh 1 1 is better therefore it can be concluded that a higher absolute value of the marginal future cost helps to choose higher carryover storage a4 c s h 1 1 c s h 1 2 c c s h 1 1 c f s h 1 1 c c s h 1 2 c f s h 1 2 c f s h 1 1 c f s h 1 2 c c s h 1 1 c c s h 1 2 δ c f s h 1 δ s h 1 δ s h 1 c c s h 1 1 c c s h 1 2 
2236,hydraulic tomography ht has been proven to be an effective approach in mapping the heterogeneity of hydraulic parameters the travel time based inversion tti and geostatistical inversion gi approaches are two of several interpretation methods for ht data in particular gi is used to estimate heterogeneous hydraulic conductivity k and specific storage ss tomograms while tti yields the diffusivity d k ss tomogram the main objective of this paper is to compare the performances of these two methods through a synthetic study two cases are designed based on different monitoring configurations in each case two independent scenarios are designed as 1 providing the same dataset and 2 providing datasets based on recommended strategies for each inversion approach then the estimated tomograms are evaluated by direct comparison of estimated parameter distributions and assessment of model validation results comparison results show that the advantages of tti are 1 imaging of structural features representing high d zones and 2 requirement of less data for inverse modeling in contrast the advantages of gi are 1 estimation of parameters throughout the simulation domain 2 better characterization of low d zones and 3 generating the best estimated tomogram leading to best drawdown predictions collectively our study suggests that tti is suitable for rapid coarse characterization of heterogeneity that could be utilized for providing hydrogeological structures for gi as an initial model gi is more robust and preferable to applications that require higher accuracy in parameter estimation over a larger volume keywords subsurface heterogeneity hydraulic tomography travel time based inversion geostatistical inverse modeling data availability data will be made available on request 1 introduction the accurate characterization of hydraulic properties is critical to groundwater resources management contaminant transport and groundwater flow modeling traditionally the aquifer is treated to be homogeneous and effective hydraulic parameters such as hydraulic conductivity k and specific storage ss are estimated using analytical solutions e g cooper jacob 1946 theis 1935 however the accurate prediction of groundwater flow and contaminant transport requires more accurate descriptions of subsurface heterogeneity e g maier et al 2009 zheng gorelick 2003 in comparison to homogeneous models well characterized heterogeneous models yield better results in predicting groundwater flow and solute transport e g luo et al 2017 2020 zhao illman 2021 in order to characterize subsurface heterogeneity a novel approach termed hydraulic tomography ht has been developed gottlieb dietrich 1995 considered ht to be a special application of impedance tomography nowadays ht has been developed into a method for highly parameterized estimation of heterogeneous hydraulic properties the robust performance of ht in revealing subsurface heterogeneity has been validated through numerous synthetic bohling et al 2002 qiu et al 2019 yeh liu 2000 zhu yeh 2006 laboratory e g brauchler et al 2003 illman et al 2007 2010 2012a 2015 liu et al 2002 liu et al 2007 yin illman 2009 and field studies e g berg illman 2011 bohling et al 2007 brauchler et al 2013 cardiff et al 2020 zha et al 2015 zhao illman 2018 over the past few decades different interpretation methods have been proposed for subsurface heterogeneity characterization using data obtained from ht surveys geostatistical inversion e g snodgrass kitanidis 1998 travel time inversion brauchler et al 2003 steady shape inversion bohling et al 2007 and pilot point de marsily et al 1978 are common inverse methods based on different algorithms but all aim to reconstruct the subsurface heterogeneity of hydraulic parameters geostatistical inversion gi approach forms the backbone of a majority of ht algorithms illman et al 2015 this method estimates hydraulic parameters as well as their uncertainties at all elements or cells in the model domain by adopting a highly parameterized heterogeneous conceptual model e g kitanidis vomvoris 1983 yeh liu 2000 successive linear estimator sle is a geostatistical inverse algorithm yeh et al 1995 1996 that led to the development of a code for ht data analysis yeh and liu 2000 zhu and yeh 2005 with the development of the gi approach xiang et al 2009 developed the simultaneous successive linear estimator simsle which can analyze transient hydraulic data from different pumping tests simultaneously compared to other inverse methods the gi approach has advantages on its ability 1 to provide uncertainty estimates 2 for data fusion zha et al 2017 and 3 to yield hydraulic parameter tomograms that are able to provide robust validation results luo et al 2017 however there are some drawbacks to the gi approach such as 1 the highly parameterized geostatistical model is restricted for large scale site characterization due to its computational intensity and that 2 results may rely heavily on the accuracy of prior information when there is a lack of hydraulic data for inversion luo et al 2017 the steady shape analysis of ht data has been evaluated as an efficient and robust approach for subsurface heterogeneity characterization bohling et al 2002 2007 specifically this approach interprets transient hydraulic head data under steady shape condition in which hydraulic head data continuously change over time while the hydraulic gradient remains constant bohling et al 2002 the advantages associated with steady shape analysis of ht data can be summarized as high computational efficiency and low impact of uncertain boundary conditions on inversion results however steady shape analysis can only estimate the spatial distribution of k meanwhile steady shape analysis cannot analyze drawdown data collected prior to the stabilization of the hydraulic gradient the pilot point approach was first proposed by de marsily et al 1984 and then adopted for subsurface heterogeneity characterization through many studies e g lavenue de marsily 2001 vesselinov et al 2001 in this approach hydraulic property values are estimated at a set of selected points that are distributed throughout the model domain and subsequently interpolated through kriging doherty 2003 the approach is amenable to regularization which allows for the incorporation of structure data from available measurements to reduce the issue of nonuniqueness jiménez et al 2013 however the number of pilot points and their locations are considered to have effects on the characterization of subsurface heterogeneity lavenue pickens 1992 suggested that pilot points should be positioned at locations where model sensitivity to the observation data are highest some scholars have also attempted to guide the distribution of pilot points through geophysical or tracer tests showing promising results poduri kambhammettu 2020 sanchez león et al 2016 hydraulic travel time based inversion tti approach is based on solving an eikonal equation which is derived from the groundwater flow equation vasco et al 2000 brauchler et al 2003 vasco karasaki 2006 the simultaneous iterative reconstruction technique trampert leveque 1990 and ray tracing technique moser 1991 are commonly used to reconstruct the diffusivity d of an aquifer using tti tti is known to have some attractive advantages over other inversion methods which are 1 computational efficiency 2 less data required for inversion and 3 less dependency on initial conditions however the eikonal equation which forms the basis of tti is built on many assumptions which limit the range of its application the limitations include 1 variation of aquifer properties and natural hydraulic gradient should be smooth 2 the inversion accuracy decreases with lower ray density and 3 effects of boundary conditions are ignored to overcome these limitations 1 vasco 2018 developed an extended trajectory mechanics approach which is valid for arbitrary spatial variations of aquifer properties 2 brauchler et al 2013 and jiménez et al 2013 utilized null space energy map to extract the area with high reliability and 3 vasco et al 2019 presented a travel time based inversion to estimate d iteratively by coupling with forward simulation resulting from coupling groundwater forward simulation the estimation of d also considers boundary and initial conditions there are some suggestions for improving performance brauchler et al 2007 suggested using earlier time data rather than at peak arrival time to reconstruct d tomograms and hu et al 2011 suggested selecting data with constrained angle between pumping and monitoring intervals to improve the imaging of layer structures the major drawback of the tti approach is that the estimated parameter is diffusivity d which is the ratio of hydraulic conductivity k to specific storage s s to obtain the distribution of s s directly brauchler et al 2013 presented a hydraulic attenuation tomography based on the eikonal equation several other studies have been published that have attempted to separate k and s s from the estimated d values brauchler et al 2011 2013 hu et al 2011 jiménez et al 2013 however it is still common to assume a homogeneous ss when conducting site characterizations using the tti vasco et al 2019 other than the inverse methods mentioned above ht data can also be interpreted using a radial flow model coupled with a levenberg marquardt inversion paradis et al 2016 hybrid inverse method wang et al 2017 nelder mead simplex algorithm cardiff et al 2020 and gaussian mixtures minutti et al 2020 for subsurface heterogeneity characterization to improve the estimation of hydraulic parameters the joint inversion of hydrogeological and geophysical data is a promising idea and has been investigated by several researchers kowalsky et al 2006 lochbühler et al 2013 johnson et al 2009 however one difficulty resides in the non unique relationship between hydrogeological and geophysical attributes rubin and hubbard 2005 the hydraulic travel time based inversion approach based on seismic travel time inversion brauchler et al 2003 avoids this difficulty because it has a direct relationship to the hydraulic diffusivity d which is the ratio of k to s s for estimating k several hydrogeological inversion approaches were combined with tti i e steady shape inversion hu et al 2011 or pilot points inversion jiménez et al 2013 the gi approach which has robust performance on estimating both heterogeneous k and s s has also the capability for incorporating geological information for further improvement of inversion results luo et al 2017 zha et al 2017 zhao et al 2016 zhao and illman 2017 2018 the tti and gi approaches have some attractive and complementary characteristics on several aspects the first is that geological information can be incorporated to gi while tti has been used to provide the structure information jiménez et al 2013 both tti and gi approaches can use hydraulic response data for their inversions specifically gi requires hydraulic head data from pumping slug tests while tti requires the arrival time of the peak derivative of drawdowns this means that the tti only requires early time pumping or slug test data which is lower in cost in terms of field data collection more importantly tti require significantly less computational power than gi due to its inversion algorithm however gi is able to estimate hydraulic parameters throughout the simulation domain yeh liu 2000 while the tti can only reconstruct the volume between pumping and observation wells brauchler et al 2003 based on the above observations it is reasonable to think that tti and gi have a promising future to be used together for joint inverse modeling to date no research on joint inversion or combination of tti and gi has been presented before conducting such a study a comparison study is necessary to investigate the advantages disadvantages of them by using different algorithms to estimate the hydraulic parameters of the same aquifer the advantages of each code and differences between them can be found for example zimmerman et al 1998 found that results from the different gi codes exhibited little difference hendricks franssen et al 2009 compared several inverse techniques and found the differences among them was not large however the tti technique was not included in the above studies to best of our knowledge there is no study which objectively compares the performance of gi and tti approaches with data collected from the same aquifer in this study a comprehensive comparison between the gi and tti is performed to evaluate their performances in revealing subsurface heterogeneities through synthetic experiments the utilization of a synthetic aquifer with known heterogeneity patterns no measurement error and controlled initial and boundary conditions is of critical importance in assessing the comparison results since the temporal sampling strategies for gi and tti are different the comparison is performed under two scenarios based on fairness and preferred sampling strategies for each approach to investigate the impact of well selection on the final characterization results two configuration cases with different pumping and observation wells are utilized for each approach the comparison results are then assessed through visual comparison of the estimated hydraulic parameter tomograms and statistical analysis of validation results this study makes a comprehensive comparison to identify advantages disadvantages of tti and gi which can be utilized in future studies on application at a given site and combining the strengths of each approach for joint inverse modeling 2 inverse modeling approaches the fundamental information investigated approaches is introduced briefly in this section additional details on both tti and gi approaches can be found in brauchler et al 2003 and xiang et al 2009 respectively 2 1 transient groundwater flow the transient groundwater flow equation can be described by 1 k x h q x p s s x h t with the following initial and boundary conditions 2 h t 0 h 0 h γ 1 h 1 a n d k x h n γ 2 q where k x is hydraulic conductivity l t h is hydraulic head l q x p is the volumetric flux per unit volume 1 t at location x p and s s x is specific storage 1 l in eqn 2 h 0 represents the initial hydraulic head l h 1 is the constant head l at boundary γ 1 q is the specific discharge l t at neumann boundary γ 2 and n is a unit vector normal to γ 2 yeh et al 2015 here l and t represent dimensions of length and time respectively tti and gi are approaches that estimate hydraulic parameters through the interpretation of transient hydraulic data the 3d finite element model mmoc3 yeh et al 1993 is used to generate transient hydraulic data based on equations 1 and 2 which are then subjected to ht analysis for heterogeneity characterization using the tti and gi approaches 2 2 travel time based inversion the start of the tti is a line integral equation which describes the relationship between the peak arrival time and diffusivity of a dirac signal kulkarni et al 2001 vasco et al 2000 3 t peak x 2 1 6 x 1 x 2 ds d s where t peak x 2 is the peak arrival time of hydraulic signal from a source x 1 pumping well to a receiver x 2 observation well s is the trajectory path of the signal and d s is the diffusivity along the path the hydraulic diffusivity is the quotient of k to s s it provides a quantitative measure of response rate of the groundwater heads during transient flow and is a key consideration for the predictive simulations of groundwater contaminant transport models shepley and taylor 2003 the eikonal equation can be solved with ray tracing techniques which allow for the calculation of pressure propagation along trajectories technically eqn 3 is only valid for a dirac pulse in a homogeneous medium vasco et al 2000 demonstrates that eqn 3 is also valid with a heaviside source stated another way they conclude that eqn 3 is also valid for cases with constant pumping or injecting rates in this study the eikonal solver geotom3d and tomogo jackson tweeton 1996 qiu et al 2019 are applied for the tti approach in which the sirt simultaneous iteration reconstruction technique and ray tracing algorithms are utilized to reconstruct the d distribution of the investigated aquifer the eikonal solver has been widely used in previous studies for subsurface heterogeneity characterization using tti brauchler et al 2003 2007 2011 2013 ray tracing is used to solve the trajectory or the propagation path of the hydraulic signal as it travels through the porous medium when applying the tti the following strategies and operations are considered to improve the inversion results 1 utilization of earlier travel time than initially considered peak arrival time and 2 omitting data collected at observation wells forming large angles between the horizontal line and trajectory to the pumping well brauchler et al 2007 found that the utilization of travel time prior to the arrival of peak derivatives early time yielded improved tti results in terms of revealing preferential flow paths eqn 3 was modified by including a transformation factor to approximate the early time as 4 t α d 1 6 f α d x 1 x 2 ds d s where t α d is the travel time of a selected point of hydraulic signal and f α d t peak t α d is the related transformation factor fig s1a shows an example of early time data in which the arrival time of the 20 amplitude of peak derivative is considered as t 20 however utilization of early time data for tti approach may not be appliable to cases with closely distributed sources and receivers in a high d zone in which the derivative of drawdown data may yield the peak arrival time as the first observation point as shown in fig s2b using the early travel time is called travel time diagnostic for example the t 20 diagnostic is the time at which the derivative of drawdown rises to 20 of its peak value fig s1a hu et al 2011 showed that the selection of observation wells with low angles to the pumping well was beneficial for imaging horizontal and layered structures using tti a schematic diagram showing the consideration of all observation wells or only observation wells with low angles to the pumping wells is illustrated in fig s2 vasco 2018 presented an extended trajectory mechanics approach to calculate the path of hydraulic signal which enlarged the application scope for tti 2 3 geostatistical inversion the gi approach estimates hydraulic parameters through the interpretation of hydraulic head data in this study the estimation of hydraulic parameters using gi is carried out by the simultaneous successive linear estimator simsle developed by xiang et al 2009 simsle characterizes the heterogeneous k and s s fields of aquifers by analyzing transient hydraulic heads a brief description of simsle is provided below the geostatistical inversion of transient hydraulic data using simsle treats the natural logarithm of k and ss as multi gaussian second order stationary and stochastic processes with given unconditional means variances and correlation lengths of k and ss simsle starts with the cokriging of all observation data and the initial k and ss values to create the first estimate of heterogeneous lnk and lnss maps the hydraulic parameter fields are then updated using the successive linear estimator sle yeh et al 1996 built in simsle by comparing the differences between the simulated and observed hydraulic heads at observation points in which the covariances of hydraulic parameters and the cross covariances between the head measurements and estimated parameters are evaluated and updated as the inversion progresses the iteration stops if 1 the spatial variance of the estimated parameters stabilizes 2 the differences between simulated and observed heads are closer than the prescribed tolerance and 3 iteration steps reach a user defined maximum value for transient analysis of ht data using the gi approach observation data at different time stages early intermediate and late are commonly selected to fully capture the drawdown curve however such a temporal sampling strategy may require a great number of observation data which in turn would increase the computational load for inversion hence it is important to balance the number of input data with the computational cost for gi 3 experimental setup 3 1 model setup for the comparison of tti and gi approaches we created a synthetic aquifer based on a real laboratory sandbox which was utilized previously for ht studies berg illman 2011 illman et al 2010 2015 luo et al 2017 zhao et al 2016 the dimension of the synthetic aquifer is 161 0 cm in length 75 6 cm in height and 10 2 cm in width the true field of the synthetic aquifer is constructed based on estimated parameters of a real sandbox hydraulic parameters estimated from the exact geological based zonation model are utilized as the true fields for the analysis presented in this study details to the geological zonation model can be found on luo et al 2017 fig 1 a 1b and 1c show the spatial distributions of hydraulic conductivity k specific storage s s and diffusivity d respectively fig 1d shows ports of the synthetic experiments which mimic the real experimental setup the exact values of k s s and d of different layers are provided on table s1 two constant head boundaries are set at left and right boundaries the front back top and bottom are treated as no flow boundaries each pumping test continues for 20 secs to ensure that the aquifer system has reached steady state condition for groundwater flow simulation the synthetic aquifer is discretized into 6 669 elements and 13 688 nodes the reason for using a fine grid to generate hydraulic head data is to capture the small differences of travel times the small sandbox scale leads to small differences of travel times at different ports to capture this small difference the time interval of observation data is set as 0 01 sec while utilizing this short time interval requires a fine grid for inversion tomograms are obtained with 741 elements for both methods to ensure fairness in addition using a coarse grid for inversion can decrease computational costs using a coarse grid is common for both tti and gi approaches especially for field scale studies the single cuboid element of the uniform grid of the inversion model has dimensions of 4 13 cm 3 98 cm 10 2 cm from a previous study luo et al 2017 it is known that the performance of gi changes with different densities of hydraulic test data hence two separate well configuration cases are designed to assess and compare the performances of the tti and gi approaches for case 1 c1 a reference configuration is set to characterize the aquifer with a single pumping well with multiple ports at the right side and an observation well with multiple ports at the left side of the sandbox to record hydraulic data fig 2 a specifically eight ports 6 12 18 24 30 36 42 48 are used for pumping while eight ports 1 7 13 19 25 31 37 43 are used for monitoring for case 2 c2 eight pumping ports are placed at the middle of the aquifer while two columns of wells on both sides are used as observation ports fig 2b eight ports 4 10 16 22 28 34 40 46 are set as pumping locations while 16 ports 1 6 7 12 13 18 19 24 25 30 31 36 37 42 43 48 are set as monitoring locations c2 is designed for comparing the effect of using different well configurations the gi approach can use wells and ports placed at arbitrary locations and estimate hydraulic parameters within and outside the well area while tti is only applicable for the region between wells hence although there are many other possible well configurations for this case an arbitrary well configuration for the tti approach is not suggested a uniform pumping rate is set at 8 ml secs for all tests for laboratory and field experiments signal strength should be strong enough so that it can be detected under the presence of various noises in previous studies the magnitude of pumping rates for the gi and tti approaches have been proven to have negligible impacts for synthetic experiments brauchler et al 2003 yeh liu 2000 all inversion codes were run on the same pc with intel core i7 7700 cpu and 16 gb random access memory 3 2 data used for inverse modeling based on previous tti related studies hu et al 2011 brauchler et al 2013 we know that using tti requires constraining the angle between pumping and observation ports which is introduced in section 2 considering the small angle constraint hydraulic head response data from three observation ports that have relatively small angles to the pumping port are utilized for tti fig s2 to maintain fairness the selection of observation ports for each pumping test is the same for both approaches for each well configuration case two scenarios with different selections of observation data for inverse modeling are considered in the analyses presented in this study to construct a fair comparison between two approaches scenario 1 s1 is designed for providing the same dataset which contains same number of observation data points for both approaches to ensure both approaches work for s1 the selection of observation data mainly was based on the requirement of using tti since the proper data selection for gi does not work with tti according to the definition of travel time there is one travel time value in a drawdown curve generated from one constant rate pumping test hence for a given pumping test only one head data at every observation well would be selected to the dataset for s1 however when using hydraulic head based on gi it is suggested that several hydraulic head data that capture the entire drawdown curve are selected this means that the dataset for s1 maybe not be appropriate for gi and the s1 dataset will undoubtedly generate a less satisfactory result for the gi approach both approaches in s1 include 24 and 48 observation data for inverse modeling for cases 1 and 2 respectively for the second scenario s2 the main factor to be considered is to construct an appropriate dataset for both approaches during a common application of a ht survey users would use the most suitable sampling strategy for the best performance with the most satisfying inversion results to mimic this situation different observation data are selected and utilized for inverse modeling using two approaches based on their corresponding optimal temporal sampling strategies for tti the operation involves selecting early time data for inversion i e the time when the derivative of head time curve rises to 20 of its maximum amplitude which is also called t 20 diagnostic for gi the operation involves selecting several data points from each head observation curve which can describe the salient features of the transient drawdown behavior i e five head data points from each drawdown observation curve are selected for the inversion these five head data are 10 20 60 80 and 99 of the maximum drawdown with their recorded times respectively keeping the same observation ports as s1 tti in s2 includes 24 and 48 observation data for inverse modeling for cases 1 and 2 respectively while gi in s2 involves 144 and 288 observation data for inverse modeling for cases 1 and 2 respectively to ensure fairness input parameters for tti or gi for different cases and scenarios to be the same the information of input dataset is summarized in table 1 the overall workflow is illustrated in fig 3 initially the forward simulation code mmoc3 is utilized to generate synthetic hydraulic head data then several datasets are constructed based on the various cases and scenarios tti and gi approaches are then used to analyze the dataset to estimate parameter fields finally d tomograms from tti while k and ss tomograms from gi are compared and evaluated quantitatively in this study a calibrated effective parameter model is used to provide homogeneous k and ss estimates for different approaches for tti during model validation a homogeneous s s is used to separate k from the estimated d d k s s for generating drawdown data by mmoc3 for the gi approach the effective k and ss estimates are used as initial values the provided dataset for effective parameter estimation also follows various of cases and scenarios summarized in table 1 4 results 4 1 travel time based inversion fig 4 shows the reconstructed d tomograms from different comparison scenarios and cases along with the true d distribution of the synthetic model fig 4e these estimated tomograms have the same color scale in each tomogram the pumping and observation ports are represented by red circles and white circles respectively for classifying results briefly the abbreviation of cx sx is used to represent case x scenario x in the following figure for example c1 s1 indicates case 1 scenario 1 comparing with the true d distribution estimated d tomograms successfully reveal significant structural features of the aquifer the horizontal layer with high d on the top and bottom of aquifer can be well captured in most situations fig 4a 4b 4d when dataset construction criteria of t 20 diagnostics are considered the estimated d tomograms fig 4b and 4d reveal distinct horizontal layers with high d at the top and middle of the aquifer this result shows that the tti can locate high d areas well which is consistent with previous studies brauchler et al 2003 2007 for low d characterization fig 4d has the best performance which reveals two low d zones successfully while on other tomograms fig 4a 4b and 4c the middle of aquifer with low d is blurry however there are two main obvious drawbacks to the tti approach according to these results firstly the estimated d values at the top high d zone of the inversion domain are far from the true values for some areas fig 4b and 4d this drawback is similar with the finding from brauchler et al 2013 based on the eikonal equation eqn 3 and the assumption of homogeneous aquifer brauchler et al 2003 the travel time is thought only to be related with d however when using tti there are some obvious abnormal data points which do not follow the above equation the normal method is to remove these data points from inverse modeling while this study retains them based on the consideration of fairness based on the research by vasco et al 2019 we speculate it is also related to the fact that the eikonal solver neglects the effect of hydraulic head field on hydraulic signal propagation the second drawback is obvious on fig 4c which uses the c2 configuration and peak arrival time for s1 this figure shows that both the left half and the right half of tomogram can reveal the top high d zone however the estimated d on the right half of tomogram is smaller than the left which is different from the true d distribution the same phenomenon also appears in other areas of fig 4c yet it is more obvious for the top layer this phenomenon is caused by the fact that the sensitivity of the peak arrival time decreases strongly with an increase in the distance between wells used for pumping and observation which can be also found in brauchler et al 2007 fortunately using the early travel time data s2 overcomes this drawback fig 4d 4 2 geostatistical inversion in this study the calibrated effective parameter model is utilized as initial k and ss fields homogeneous for the gi approach following the suggestions by xiang et al 2009 inversion results are selected when the l 2 norm stabilizes here stabilization of the l 2 norm indicates that the variation of adjacent iterations is smaller than 1 10 4 however the gi result for c1 s1 is an exception because the l 2 norm of its initial step is too small the gi result for c1 s1 is selected when r 2 yields a value of 0 995 indicating that the model is well calibrated fig 5 shows the estimated k tomograms for two cases under two comparison scenarios along with the true k fields fig s3 shows the s s tomograms for the above situations along with the true s s fields fig 5b and 5d show that the estimated k tomograms characterize the heterogeneity of aquifer well when the dataset utilized for inverse modeling follows the suggested strategy the comparison between fig 5b and 5d shows the c2 configuration results in a k tomogram that reveals more details in heterogeneity on fig s3 a similar phenomenon is seen in which the s s tomogram for c2 s2 yields more details to heterogeneity than c1 s2 however the estimated k tomogram fig 5a 5c shows that s1 limits the inversion performance for the gi approach which utilized head data points at peak arrival times only the blurry pattern of fig 5a shows that the inversion result of gi on c1 s1 is unsatisfactory that is the outer area of fig 5c has much higher values that are not consistent with the true distribution this is because the k distribution is known to be more sensitive to intermediate to late time data while ss is most sensitive to early time data sun et al 2013 hence it is reasonable to expect that the gi approach can provide k and s s estimates for s1 but cannot yield satisfactory results to compare the results obtained from tti d tomograms are calculated based on the estimated k and ss values d k s s from the gi approach fig 6 a and 6c show the d tomograms are not good for s1 the distribution of the high d zone at the top of fig 6a is well reconstructed but the value of this zone is far from the true value whereas at the middle depth of fig 6a the low d area is not well described according to the true d distribution in particular the outer area of fig 6c reveals a uniform high d which indicates that the outer area is not adjusted sufficiently enough through inverse modeling these observations inform us that the s1 is not appropriate for gi fig 6b and 6d reveal that the d tomograms obtained from gi for s2 capture the heterogeneity patterns although gi is not developed for a direct estimation of d the reason for this result is that the variability of the estimated s s tomogram fig s3 is much smaller than estimated k tomograms fig 5 hence the well estimated k tomogram dominates the d distribution in terms of computational costs the tti has an obvious advantage over gi table 2 generally tti converges within several seconds while gi requires several hours for s2 with the same pc obviously the computational time for gi increases with the number of input data while the computational time for tti remains small for cases with different amounts of data 4 3 direct comparison to make a quantitative assessment of estimated parameters scatterplots of the estimated versus true l n d values are plotted for both tti and gi approaches quantitative indices such as mean absolute error l 1 mean square error l 2 and the coefficient of determination r 2 are used to assess the results in each scatterplot a linear model that fits all data and a 45 line are included along with the computed l 1 and l 2 norms as well as r 2 values l 1 l 2 and r 2 are defined as 8 l 1 1 n i 1 n x i x i 9 l 2 1 n i 1 n x i x i 2 10 r 2 1 n i 1 n x i μ x x i μ x i 1 n x i μ x 2 1 n i 1 n x i μ x 2 2 where n is total number of data i indicates the data number x i and x i represent the value from estimated and true models respectively μ x and μ x represent averaged values from the inversion and true models respectively as mentioned previously the tti approach only reconstructs the d tomogram between pumping and observation wells while gi can reveal the heterogeneity throughout the simulation domain thus the same area between pumping and observation wells is adopted for the comparison fig 7 shows scatterplots of the estimated versus true l n d values of the selected area the slope intercept r 2 of the linear model as well as l 1 and l 2 norms are quantitative indices utilized for the judgement of inversion performance an inversion with good performance yields a slope and r 2 of scatterplot close to one which means that the estimated parameter is close to the true value and has a good fit with the true distribution on the scatterplot the higher the intercepts as well as l 1 and l 2 norms are the larger is the difference between the estimated and true parameters the lower are the intercepts as well as l 1 and l 2 norms the better is the inversion performance there are some similarities between the tti and gi approaches d tomograms of gi have the ability to characterize the heterogeneity of the aquifer for s2 which is supported by the good performance on fig 7f and 7 h for tti d tomograms on s2 fig 7b 7d also have obvious improvements on slope and r 2 indices than d tomograms on s1 fig 7a 7c this means for s2 using more observation data from additional locations can improve the visual quality of tomograms for both approaches but gi has more obvious improvement for s2 than tti which is supported by that the performance indices for gi on c1 s2 and c2 s2 that are obviously better than c1 s1 and c2 s2 respectively finally fig 7c and 7 g show that tti and gi both yield poor results for c2 s1 this means that c2 s1 is not an appropriate group for both approaches examination of fig 7 shows that the gi approach for c2 s2 fig 7h yields the best estimated d field while the 2nd best is gi for c1 s2 fig 7f then the tti approach for c1 s2 fig 7b and c2 s2 fig 7d yields acceptable results poor results are obtained for tti in terms of c1 s1 fig 7a c2 s1 fig 7c and gi for c1 s1 fig 7e c2 s1 fig 7g for s1 both tti and gi yield poorly estimated d tomograms but the gi result for c1 s1 fig 7e is relatively better than the other three situations the slope and r 2 of fig 7e are better than other results for s1 yet its intercept l 1 and l 2 norms are too high to be considered as a good result comparisons of various scatterplots in fig 7 show that gi under c2 s2 yields the best estimated d field with respect to quantitative indices however when the dataset contains less data the gi approach yields poor inversion results according to figs 7e and 7 g these results reveal that for gi an appropriate dataset with sufficient data points can strongly improve its performance to obtain the best performance the gi approach needs more observation data than tti the improvement of the amount of data from ports has little effect on tti for s2 which is different with the gi approach this observation is supported by the similar performance seen on figs 7b and 7d on s2 using the early travel time the well configuration seems to have little effect on tti performance however for gi on s2 the slope in fig 7h shows obvious improvement compared to that in fig 7f it shows the well configuration has an obvious effect on gi performance on fig 8 the vertical variation of ln d from tti and gi approaches as well as the true ln d distribution are shown to highlight the performance of tti and gi along the vertical direction this figure displays the depth variation of the maximum mean and minimum of l n d which is calculated by l n d of elements with the same horizontal coordinate as mentioned before the compared area is adjusted to the area between the pumping and observation wells the vertical variations of ln d demonstrate that all tomograms from tti can reveal the high d zones at on the top and bottom of the aquifer tti has the stable performance of distinguishing different magnitudes of d at different depths however the variation along depth is smooth this is due to the assumption of the eikonal equation eqn 3 which assumes a smooth heterogeneous distribution of aquifer parameters as shown in fig 8 it is obvious that the gi approach for c2 s2 yields the best performance it is worth mentioning that the ability of gi to characterize the middle low d zone is much better than tti for the area between the depths of 20 and 40 cm the variation of all tti tomograms approximates a straight line while gi for c2 s2 characterizes a similar variation tendency compared to the true d distribution fig 8g shows that a high d zone is found by gi at about 30 cm from the bottom and this high d zone is even higher than the top and bottom of sandbox which differs with the true d distribution from previous analysis the reason has been already known that the dataset for c2 s1 contains only few very early time data but for tti using early time data is its characteristic and it is very possible to have better performance with the earlier travel time data 4 4 model calibration and validation for the tti approach the root mean square rms residual is used to characterize the performance of model calibration for each case and scenario rms residual of tti model is lower than 1 10 2 sec fig s4 shows the rms residual of tti with iteration steps for the gi approach fig s5 shows the variation of the l 2 norm with the iteration steps computed by gi approach fig s6 shows the simulated drawdowns versus true drawdowns at the selected iteration step for model calibration figs s4 s5 and s6 show that both tti and gi models are well calibrated this confirms that the inversion of both approaches is conducted correctly ensuring the meaningful evaluation of inversion results for model validation the observation time interval is 0 5 sec and the simulation period is set as 20 sec to ensure that the drawdown reaches a steady state condition the pumping tests are conducted at 16 ports ports 3 5 9 11 15 17 21 23 27 29 33 35 39 41 45 47 for each pumping test the remaining 47 ports are utilized to record hydraulic head variations the configuration of ports used for model validation is shown in fig 9 as mmoc3 does not utilize d to generate head data an effective homogeneous ss is provided for tti to separate k from d then the heterogeneous k and homogeneous ss are used to generate drawdown data using mmoc3 it is worth mentioning that the heterogenous d distribution is only utilized for the area covered by the pumping and observation wells which is the area of travel time inversion for the outer area of the sandbox a homogenous d value is utilized for the modelling which is calculated as mean travel time based on eqn 3 for the gi approach the drawdown data for validation is generated using the corresponding estimated heterogeneous k and s s validation scatterplots of the true and simulated drawdowns for tti and gi are shown on fig 10 and the quantitative indices are also provided in each sub figure the l 1 and l 2 norms of each scatterplot are summarized and shown in table s2 these scatterplots indicate that gi yields better validation results than tti for the following cases and scenarios c1 s2 c2 s1 c2 s2 generally the gi approach yields better and more stable validation results than tti especially for s2 in contrast the comparison of fig 10a and 10e reveals that tti yields better validation results than gi for c1 s1 the scatterplot shown as fig 10c indicates that the tti approach yields poor validation results for c2 s1 as explained earlier tti has disadvantages due to sensitivity issue when using peak arrival times for inverse modeling hence the utilization of c2 should be viewed with caution because without using early travel time data tti yields poor validation results when distances between pumping and observation wells are different the comparison of figs 10c and 10d shows obvious improvement of tti by using the t 20 diagnostics for c2 the slopes 0 56 0 59 of figs 10b and 10d reveals a smaller predicted drawdown than true drawdown for tti on s2 this means that while using the early time data improves the performance of tti it causes a general overestimation of d over the entire aquifer in general the gi approach predicts biased drawdown data for s1 fig 10e and 10 g the biased drawdown indicates that the datasets from s1 is not appropriate for gi scatterplots of fig 10f and 10 h are much better than that of fig 10e and 10 g for s2 the gi approach yields better results according to the quantitative indices it shows the importance of utilizing an appropriate dataset for gi i e sufficient observation data points should be selected for capturing the hydraulic behavior of drawdown curves 5 discussion in this study performances of the tti and gi approaches are compared with each other based on the direct comparison of computed tomograms as well as through the validation of the estimated parameters through forward simulations of tests not used in the calibration effort it is worth noting that the fairness of comparison has been partly compromised to ensure the correct implementations of both approaches for s1 datasets with the same amount of data points are provided for the two approaches for gi it uses the head data at the time when the derivative of head data reaches a peak yet the peak time of the derivative of drawdown curve for tti is calculated with head data over a time interval rather than with a single data point hence some may consider that the condition of s1 is more beneficial for tti than gi however as mentioned earlier tti utilizes a high frequency asymptotic solution to solve the diffusion equation therefore some assumptions of the tti approach are not met for the sandbox for example tti assumes that boundary conditions have minor effects on inversion results and the spatial variation of the estimated d is assumed to be smooth these assumptions hamper the performance of tti for the sandbox study with a much smaller scale and larger boundary effect hence the use of a sandbox to conduct the comparison may be more advantageous for gi than tti for this study we find that the tti and gi approaches have a complementary relationship rather than a competitive relationship it is hard to find one approach with overwhelming advantages in all situations that is each one of them has advantages for specific situations generally tti may be used to derive an initial model for gi because it is not as sensitive to starting model with initial guesses as gi on the other hand the gi approach is more suitable for estimating hydraulic parameters necessary for groundwater modeling with higher accuracy as an initial study we started with only two factors to investigate the difference between two approaches specifically we focused on the important effects of data density and well configuration for the two inversions with one boundary condition setting noise free dataset and one synthetic aquifer based on a laboratory sandbox other aspects such as the variations in boundary conditions the inclusion of noise with different kinds and levels as well as variations of geology e g geological settings degree of heterogeneity correlation lengths etc are also important for a more comprehensive comparison study these variables will likely affect the performance of both tti and gi particularly gi considers the effect of boundary conditions while the eikonal equation based tti assumes that boundary conditions are negligible for early stage hydraulic data actually the boundary condition does affect the hydraulic signal propagation especially for the late stage hydraulic data and sensitivities of travel times for different stage data are different vasco 2018 hence the performance of eikonal equation based tti will deteriorate with the increase of boundary condition effects on travel times although the current study has above mentioned limitations the findings of advantages disadvantages of the two approaches learned are informative in deciding when to apply the two approaches under real world situations firstly both approaches have been tested using a synthetic aquifer constructed based on a real sandbox study consisting of an interfingering fluvial deposit found commonly at many real field sites secondly the well configuration considered in this study with few wells c1 is a common situation at real sites thus the comparison of two inverse modeling approaches based on few wells is relevant to real field situations with only a few available wells and monitoring ports thirdly the various scenarios and cases examined clearly showed when each approach yields improved hydraulic parameter estimate for a given dataset which should be useful for field studies fourthly with the use of an appropriate dataset for each inverse modeling approach in s2 the comparison study revealed the capabilities of each approach in estimating the hydraulic parameters for the same aquifer which should allow researchers and practitioners to make better decisions on the approach to utilize in real field situations finally based on the comparison of the two inverse modeling approaches and the results obtained we found that the combination of tti and gi approaches appears to be promising and feasible the tti approach can be initially applied to early time data to quickly obtain the prior model for the gi approach at a low computational cost then data from different ports and well locations as well as from different stages early intermediate late of the drawdown curves could be incorporated into the gi approach yielding k and ss tomograms of higher accuracy that is the initial d tomogram estimated from tti can provide structural information and an initial k estimate both of which are expected to assist the gi approach in achieving more accurate results while accelerating the speed of model convergence 6 summary conclusions and outlook through various studies estimation of heterogeneous aquifer parameter reconstruction from ht surveys have demonstrated significant advantages over traditional characterization approaches which include 1 the ability of characterizing aquifer heterogeneity including the connectivity of flow transport pathways between boreholes 2 the improved performance of predicting groundwater flow and solute transport illman et al 2012 jiménez et al 2015 luo et al 2020 3 providing information on uncertainty or reliability of estimated tomograms brauchler et al 2013 illman et al 2010 and 4 providing valuable information on hydraulic parameter heterogeneity at an acceptable cost when compared to site heterogeneity characterization based on a large number of small scale samples illman et al 2012b yeh liu 2000 in this study numerical experiments are conducted by using a synthetic heterogeneous aquifer which is derived from a laboratory sandbox aquifer to compare two kinds of transient hydraulic tomography tht algorithms based on tti and gi approaches two configuration cases of pumping and observation wells are designed with two scenarios for each case for case 1 eight pumping and eight observation ports are set at opposing sides of the sandbox for case 2 eight pumping ports are set in the middle part of the aquifer while eight observation ports are set at each side with a total of sixteen observation ports the two scenarios for each case are designed to compare the estimation performance and effect of using different selected data for inverse modeling for s1 the same dataset is provided to both approaches for tti it uses the travel time which is the peak arrival time of the drawdown derivative curve for gi it uses the head data at the peak arrival time for s2 the construction of dataset is based on the requirement for each approach for the tti approach the optimal data selection strategy involves the early travel time selection which is also called the travel time diagnostic for the gi approach the second scenario provides sufficient data points to capture the salient features of drawdown curves that contain information on both k and ss heterogeneity model calibration results confirm the correct utilization of both approaches the estimated tomograms from both approaches are compared through direct comparisons and model validation for model validation sixteen pumping tests are conducted at ports which are not used for calibration purposes when one port is used for a pumping test the rest of the 47 ports are set as observation locations our study leads to the following findings and conclusions 1 in terms of direct comparison both approaches can characterize subsurface heterogeneity with regards to the hydraulic parameter distribution for c2 s2 the gi approach shows good reconstruction of heterogeneous k and s s consistent with the true distribution while the tti approach can roughly recover the variations of d it is shown with this comparison that gi has larger estimated range than tti the latter focuses on the area between pumping and observation wells the ln d from estimated tomograms is compared with true ln d distribution by scatterplots these scatterplots show that tti has better result for scenario 1 while gi has better performance for scenario 2 the vertical variation plot shows that tti performs better in locating the high d zone than gi while gi has better characterization capabilities for low d zone 2 in terms of model validation and head prediction which is based on the numerical modelling with the estimated tomograms from gi and tti as inputs the gi approach yields the best validation result under case 2 scenario 2 gi has the obvious advantage if the dataset meets the requirement of the suggested strategy i e gi has better result when the dataset has sufficient data for capturing the shape of the drawdown curves in contrast tti has an advantage on cost because it only needs data during the early stage of the pumping test and 3d inversions are affordable on a personal computer however tti generates the worst validation result under case 2 scenario 1 the peak arrival time data is not appropriate for characterizing the d of aquifer with an asymmetrical well configuration the early travel time diagnostic is necessary for this configuration 3 the early travel time diagnostic of tti are found helpful in obtaining good tomograms and validation results yet it cannot solve the problem that the reconstructed d values differ significantly from real values in certain areas of the simulation domain this phenomenon is due to the tti algorithm that ignores both initial and boundary conditions which are crucial factors for ht data collected in an extremely small domain such as the small scaled sandbox utilized in this study 4 this comparison study also led to the idea of sequential inversion of tti and gi to be a promising concept firstly the similar pattern is shown on tomograms of tti and gi for s2 which is also consistent with true parameter distribution secondly the time cost of tti is negligible compared with gi therefore tti can be used initially with early time data to rapidly generate a d tomogram that can then be used as prior information for the gi approach the gi approach can then take advantage of information contained in the entire drawdown curve to more accurately map the k and ss distributions for a future study an attractive topic is the combination of these two methods which is currently being studied with the establishment of an initial model for the gi with the structure information that is based on the d tomogram from the tti this combination could increase gi performance by using early stage data of the hydraulic test and to decrease the time cost i e the time for collecting data and head recovery between tests while conducting a ht survey we believe that with a successful combination of these two methods ht surveys can be more efficient and accurate for aquifer characterization at a higher spatial resolution credit authorship contribution statement huiyang qiu methodology writing original draft validation funding acquisition rui hu project administration funding acquisition writing review editing ning luo writing review editing walter a illman supervision resources writing review editing xiaolan hou writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the ministry of science and technology of china through the program driving process and mechanism of three dimensional spatial distribution of high risk organic pollutants in multi field coupled sites project code 2019yfc1804303 walter a illman acknowledges the partial support from the discovery grant funded by the natural sciences and engineering research council of canada nserc which facilitated this collaboration the first author acknowledges the financial support of china scholarship council 201906710028 open research there is an open source version of tomogo via github https github com wichniarek sirt variants git the datasets utilized for developing the tti and gi models are provided as a supplement as dataset zip appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129247 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2236,hydraulic tomography ht has been proven to be an effective approach in mapping the heterogeneity of hydraulic parameters the travel time based inversion tti and geostatistical inversion gi approaches are two of several interpretation methods for ht data in particular gi is used to estimate heterogeneous hydraulic conductivity k and specific storage ss tomograms while tti yields the diffusivity d k ss tomogram the main objective of this paper is to compare the performances of these two methods through a synthetic study two cases are designed based on different monitoring configurations in each case two independent scenarios are designed as 1 providing the same dataset and 2 providing datasets based on recommended strategies for each inversion approach then the estimated tomograms are evaluated by direct comparison of estimated parameter distributions and assessment of model validation results comparison results show that the advantages of tti are 1 imaging of structural features representing high d zones and 2 requirement of less data for inverse modeling in contrast the advantages of gi are 1 estimation of parameters throughout the simulation domain 2 better characterization of low d zones and 3 generating the best estimated tomogram leading to best drawdown predictions collectively our study suggests that tti is suitable for rapid coarse characterization of heterogeneity that could be utilized for providing hydrogeological structures for gi as an initial model gi is more robust and preferable to applications that require higher accuracy in parameter estimation over a larger volume keywords subsurface heterogeneity hydraulic tomography travel time based inversion geostatistical inverse modeling data availability data will be made available on request 1 introduction the accurate characterization of hydraulic properties is critical to groundwater resources management contaminant transport and groundwater flow modeling traditionally the aquifer is treated to be homogeneous and effective hydraulic parameters such as hydraulic conductivity k and specific storage ss are estimated using analytical solutions e g cooper jacob 1946 theis 1935 however the accurate prediction of groundwater flow and contaminant transport requires more accurate descriptions of subsurface heterogeneity e g maier et al 2009 zheng gorelick 2003 in comparison to homogeneous models well characterized heterogeneous models yield better results in predicting groundwater flow and solute transport e g luo et al 2017 2020 zhao illman 2021 in order to characterize subsurface heterogeneity a novel approach termed hydraulic tomography ht has been developed gottlieb dietrich 1995 considered ht to be a special application of impedance tomography nowadays ht has been developed into a method for highly parameterized estimation of heterogeneous hydraulic properties the robust performance of ht in revealing subsurface heterogeneity has been validated through numerous synthetic bohling et al 2002 qiu et al 2019 yeh liu 2000 zhu yeh 2006 laboratory e g brauchler et al 2003 illman et al 2007 2010 2012a 2015 liu et al 2002 liu et al 2007 yin illman 2009 and field studies e g berg illman 2011 bohling et al 2007 brauchler et al 2013 cardiff et al 2020 zha et al 2015 zhao illman 2018 over the past few decades different interpretation methods have been proposed for subsurface heterogeneity characterization using data obtained from ht surveys geostatistical inversion e g snodgrass kitanidis 1998 travel time inversion brauchler et al 2003 steady shape inversion bohling et al 2007 and pilot point de marsily et al 1978 are common inverse methods based on different algorithms but all aim to reconstruct the subsurface heterogeneity of hydraulic parameters geostatistical inversion gi approach forms the backbone of a majority of ht algorithms illman et al 2015 this method estimates hydraulic parameters as well as their uncertainties at all elements or cells in the model domain by adopting a highly parameterized heterogeneous conceptual model e g kitanidis vomvoris 1983 yeh liu 2000 successive linear estimator sle is a geostatistical inverse algorithm yeh et al 1995 1996 that led to the development of a code for ht data analysis yeh and liu 2000 zhu and yeh 2005 with the development of the gi approach xiang et al 2009 developed the simultaneous successive linear estimator simsle which can analyze transient hydraulic data from different pumping tests simultaneously compared to other inverse methods the gi approach has advantages on its ability 1 to provide uncertainty estimates 2 for data fusion zha et al 2017 and 3 to yield hydraulic parameter tomograms that are able to provide robust validation results luo et al 2017 however there are some drawbacks to the gi approach such as 1 the highly parameterized geostatistical model is restricted for large scale site characterization due to its computational intensity and that 2 results may rely heavily on the accuracy of prior information when there is a lack of hydraulic data for inversion luo et al 2017 the steady shape analysis of ht data has been evaluated as an efficient and robust approach for subsurface heterogeneity characterization bohling et al 2002 2007 specifically this approach interprets transient hydraulic head data under steady shape condition in which hydraulic head data continuously change over time while the hydraulic gradient remains constant bohling et al 2002 the advantages associated with steady shape analysis of ht data can be summarized as high computational efficiency and low impact of uncertain boundary conditions on inversion results however steady shape analysis can only estimate the spatial distribution of k meanwhile steady shape analysis cannot analyze drawdown data collected prior to the stabilization of the hydraulic gradient the pilot point approach was first proposed by de marsily et al 1984 and then adopted for subsurface heterogeneity characterization through many studies e g lavenue de marsily 2001 vesselinov et al 2001 in this approach hydraulic property values are estimated at a set of selected points that are distributed throughout the model domain and subsequently interpolated through kriging doherty 2003 the approach is amenable to regularization which allows for the incorporation of structure data from available measurements to reduce the issue of nonuniqueness jiménez et al 2013 however the number of pilot points and their locations are considered to have effects on the characterization of subsurface heterogeneity lavenue pickens 1992 suggested that pilot points should be positioned at locations where model sensitivity to the observation data are highest some scholars have also attempted to guide the distribution of pilot points through geophysical or tracer tests showing promising results poduri kambhammettu 2020 sanchez león et al 2016 hydraulic travel time based inversion tti approach is based on solving an eikonal equation which is derived from the groundwater flow equation vasco et al 2000 brauchler et al 2003 vasco karasaki 2006 the simultaneous iterative reconstruction technique trampert leveque 1990 and ray tracing technique moser 1991 are commonly used to reconstruct the diffusivity d of an aquifer using tti tti is known to have some attractive advantages over other inversion methods which are 1 computational efficiency 2 less data required for inversion and 3 less dependency on initial conditions however the eikonal equation which forms the basis of tti is built on many assumptions which limit the range of its application the limitations include 1 variation of aquifer properties and natural hydraulic gradient should be smooth 2 the inversion accuracy decreases with lower ray density and 3 effects of boundary conditions are ignored to overcome these limitations 1 vasco 2018 developed an extended trajectory mechanics approach which is valid for arbitrary spatial variations of aquifer properties 2 brauchler et al 2013 and jiménez et al 2013 utilized null space energy map to extract the area with high reliability and 3 vasco et al 2019 presented a travel time based inversion to estimate d iteratively by coupling with forward simulation resulting from coupling groundwater forward simulation the estimation of d also considers boundary and initial conditions there are some suggestions for improving performance brauchler et al 2007 suggested using earlier time data rather than at peak arrival time to reconstruct d tomograms and hu et al 2011 suggested selecting data with constrained angle between pumping and monitoring intervals to improve the imaging of layer structures the major drawback of the tti approach is that the estimated parameter is diffusivity d which is the ratio of hydraulic conductivity k to specific storage s s to obtain the distribution of s s directly brauchler et al 2013 presented a hydraulic attenuation tomography based on the eikonal equation several other studies have been published that have attempted to separate k and s s from the estimated d values brauchler et al 2011 2013 hu et al 2011 jiménez et al 2013 however it is still common to assume a homogeneous ss when conducting site characterizations using the tti vasco et al 2019 other than the inverse methods mentioned above ht data can also be interpreted using a radial flow model coupled with a levenberg marquardt inversion paradis et al 2016 hybrid inverse method wang et al 2017 nelder mead simplex algorithm cardiff et al 2020 and gaussian mixtures minutti et al 2020 for subsurface heterogeneity characterization to improve the estimation of hydraulic parameters the joint inversion of hydrogeological and geophysical data is a promising idea and has been investigated by several researchers kowalsky et al 2006 lochbühler et al 2013 johnson et al 2009 however one difficulty resides in the non unique relationship between hydrogeological and geophysical attributes rubin and hubbard 2005 the hydraulic travel time based inversion approach based on seismic travel time inversion brauchler et al 2003 avoids this difficulty because it has a direct relationship to the hydraulic diffusivity d which is the ratio of k to s s for estimating k several hydrogeological inversion approaches were combined with tti i e steady shape inversion hu et al 2011 or pilot points inversion jiménez et al 2013 the gi approach which has robust performance on estimating both heterogeneous k and s s has also the capability for incorporating geological information for further improvement of inversion results luo et al 2017 zha et al 2017 zhao et al 2016 zhao and illman 2017 2018 the tti and gi approaches have some attractive and complementary characteristics on several aspects the first is that geological information can be incorporated to gi while tti has been used to provide the structure information jiménez et al 2013 both tti and gi approaches can use hydraulic response data for their inversions specifically gi requires hydraulic head data from pumping slug tests while tti requires the arrival time of the peak derivative of drawdowns this means that the tti only requires early time pumping or slug test data which is lower in cost in terms of field data collection more importantly tti require significantly less computational power than gi due to its inversion algorithm however gi is able to estimate hydraulic parameters throughout the simulation domain yeh liu 2000 while the tti can only reconstruct the volume between pumping and observation wells brauchler et al 2003 based on the above observations it is reasonable to think that tti and gi have a promising future to be used together for joint inverse modeling to date no research on joint inversion or combination of tti and gi has been presented before conducting such a study a comparison study is necessary to investigate the advantages disadvantages of them by using different algorithms to estimate the hydraulic parameters of the same aquifer the advantages of each code and differences between them can be found for example zimmerman et al 1998 found that results from the different gi codes exhibited little difference hendricks franssen et al 2009 compared several inverse techniques and found the differences among them was not large however the tti technique was not included in the above studies to best of our knowledge there is no study which objectively compares the performance of gi and tti approaches with data collected from the same aquifer in this study a comprehensive comparison between the gi and tti is performed to evaluate their performances in revealing subsurface heterogeneities through synthetic experiments the utilization of a synthetic aquifer with known heterogeneity patterns no measurement error and controlled initial and boundary conditions is of critical importance in assessing the comparison results since the temporal sampling strategies for gi and tti are different the comparison is performed under two scenarios based on fairness and preferred sampling strategies for each approach to investigate the impact of well selection on the final characterization results two configuration cases with different pumping and observation wells are utilized for each approach the comparison results are then assessed through visual comparison of the estimated hydraulic parameter tomograms and statistical analysis of validation results this study makes a comprehensive comparison to identify advantages disadvantages of tti and gi which can be utilized in future studies on application at a given site and combining the strengths of each approach for joint inverse modeling 2 inverse modeling approaches the fundamental information investigated approaches is introduced briefly in this section additional details on both tti and gi approaches can be found in brauchler et al 2003 and xiang et al 2009 respectively 2 1 transient groundwater flow the transient groundwater flow equation can be described by 1 k x h q x p s s x h t with the following initial and boundary conditions 2 h t 0 h 0 h γ 1 h 1 a n d k x h n γ 2 q where k x is hydraulic conductivity l t h is hydraulic head l q x p is the volumetric flux per unit volume 1 t at location x p and s s x is specific storage 1 l in eqn 2 h 0 represents the initial hydraulic head l h 1 is the constant head l at boundary γ 1 q is the specific discharge l t at neumann boundary γ 2 and n is a unit vector normal to γ 2 yeh et al 2015 here l and t represent dimensions of length and time respectively tti and gi are approaches that estimate hydraulic parameters through the interpretation of transient hydraulic data the 3d finite element model mmoc3 yeh et al 1993 is used to generate transient hydraulic data based on equations 1 and 2 which are then subjected to ht analysis for heterogeneity characterization using the tti and gi approaches 2 2 travel time based inversion the start of the tti is a line integral equation which describes the relationship between the peak arrival time and diffusivity of a dirac signal kulkarni et al 2001 vasco et al 2000 3 t peak x 2 1 6 x 1 x 2 ds d s where t peak x 2 is the peak arrival time of hydraulic signal from a source x 1 pumping well to a receiver x 2 observation well s is the trajectory path of the signal and d s is the diffusivity along the path the hydraulic diffusivity is the quotient of k to s s it provides a quantitative measure of response rate of the groundwater heads during transient flow and is a key consideration for the predictive simulations of groundwater contaminant transport models shepley and taylor 2003 the eikonal equation can be solved with ray tracing techniques which allow for the calculation of pressure propagation along trajectories technically eqn 3 is only valid for a dirac pulse in a homogeneous medium vasco et al 2000 demonstrates that eqn 3 is also valid with a heaviside source stated another way they conclude that eqn 3 is also valid for cases with constant pumping or injecting rates in this study the eikonal solver geotom3d and tomogo jackson tweeton 1996 qiu et al 2019 are applied for the tti approach in which the sirt simultaneous iteration reconstruction technique and ray tracing algorithms are utilized to reconstruct the d distribution of the investigated aquifer the eikonal solver has been widely used in previous studies for subsurface heterogeneity characterization using tti brauchler et al 2003 2007 2011 2013 ray tracing is used to solve the trajectory or the propagation path of the hydraulic signal as it travels through the porous medium when applying the tti the following strategies and operations are considered to improve the inversion results 1 utilization of earlier travel time than initially considered peak arrival time and 2 omitting data collected at observation wells forming large angles between the horizontal line and trajectory to the pumping well brauchler et al 2007 found that the utilization of travel time prior to the arrival of peak derivatives early time yielded improved tti results in terms of revealing preferential flow paths eqn 3 was modified by including a transformation factor to approximate the early time as 4 t α d 1 6 f α d x 1 x 2 ds d s where t α d is the travel time of a selected point of hydraulic signal and f α d t peak t α d is the related transformation factor fig s1a shows an example of early time data in which the arrival time of the 20 amplitude of peak derivative is considered as t 20 however utilization of early time data for tti approach may not be appliable to cases with closely distributed sources and receivers in a high d zone in which the derivative of drawdown data may yield the peak arrival time as the first observation point as shown in fig s2b using the early travel time is called travel time diagnostic for example the t 20 diagnostic is the time at which the derivative of drawdown rises to 20 of its peak value fig s1a hu et al 2011 showed that the selection of observation wells with low angles to the pumping well was beneficial for imaging horizontal and layered structures using tti a schematic diagram showing the consideration of all observation wells or only observation wells with low angles to the pumping wells is illustrated in fig s2 vasco 2018 presented an extended trajectory mechanics approach to calculate the path of hydraulic signal which enlarged the application scope for tti 2 3 geostatistical inversion the gi approach estimates hydraulic parameters through the interpretation of hydraulic head data in this study the estimation of hydraulic parameters using gi is carried out by the simultaneous successive linear estimator simsle developed by xiang et al 2009 simsle characterizes the heterogeneous k and s s fields of aquifers by analyzing transient hydraulic heads a brief description of simsle is provided below the geostatistical inversion of transient hydraulic data using simsle treats the natural logarithm of k and ss as multi gaussian second order stationary and stochastic processes with given unconditional means variances and correlation lengths of k and ss simsle starts with the cokriging of all observation data and the initial k and ss values to create the first estimate of heterogeneous lnk and lnss maps the hydraulic parameter fields are then updated using the successive linear estimator sle yeh et al 1996 built in simsle by comparing the differences between the simulated and observed hydraulic heads at observation points in which the covariances of hydraulic parameters and the cross covariances between the head measurements and estimated parameters are evaluated and updated as the inversion progresses the iteration stops if 1 the spatial variance of the estimated parameters stabilizes 2 the differences between simulated and observed heads are closer than the prescribed tolerance and 3 iteration steps reach a user defined maximum value for transient analysis of ht data using the gi approach observation data at different time stages early intermediate and late are commonly selected to fully capture the drawdown curve however such a temporal sampling strategy may require a great number of observation data which in turn would increase the computational load for inversion hence it is important to balance the number of input data with the computational cost for gi 3 experimental setup 3 1 model setup for the comparison of tti and gi approaches we created a synthetic aquifer based on a real laboratory sandbox which was utilized previously for ht studies berg illman 2011 illman et al 2010 2015 luo et al 2017 zhao et al 2016 the dimension of the synthetic aquifer is 161 0 cm in length 75 6 cm in height and 10 2 cm in width the true field of the synthetic aquifer is constructed based on estimated parameters of a real sandbox hydraulic parameters estimated from the exact geological based zonation model are utilized as the true fields for the analysis presented in this study details to the geological zonation model can be found on luo et al 2017 fig 1 a 1b and 1c show the spatial distributions of hydraulic conductivity k specific storage s s and diffusivity d respectively fig 1d shows ports of the synthetic experiments which mimic the real experimental setup the exact values of k s s and d of different layers are provided on table s1 two constant head boundaries are set at left and right boundaries the front back top and bottom are treated as no flow boundaries each pumping test continues for 20 secs to ensure that the aquifer system has reached steady state condition for groundwater flow simulation the synthetic aquifer is discretized into 6 669 elements and 13 688 nodes the reason for using a fine grid to generate hydraulic head data is to capture the small differences of travel times the small sandbox scale leads to small differences of travel times at different ports to capture this small difference the time interval of observation data is set as 0 01 sec while utilizing this short time interval requires a fine grid for inversion tomograms are obtained with 741 elements for both methods to ensure fairness in addition using a coarse grid for inversion can decrease computational costs using a coarse grid is common for both tti and gi approaches especially for field scale studies the single cuboid element of the uniform grid of the inversion model has dimensions of 4 13 cm 3 98 cm 10 2 cm from a previous study luo et al 2017 it is known that the performance of gi changes with different densities of hydraulic test data hence two separate well configuration cases are designed to assess and compare the performances of the tti and gi approaches for case 1 c1 a reference configuration is set to characterize the aquifer with a single pumping well with multiple ports at the right side and an observation well with multiple ports at the left side of the sandbox to record hydraulic data fig 2 a specifically eight ports 6 12 18 24 30 36 42 48 are used for pumping while eight ports 1 7 13 19 25 31 37 43 are used for monitoring for case 2 c2 eight pumping ports are placed at the middle of the aquifer while two columns of wells on both sides are used as observation ports fig 2b eight ports 4 10 16 22 28 34 40 46 are set as pumping locations while 16 ports 1 6 7 12 13 18 19 24 25 30 31 36 37 42 43 48 are set as monitoring locations c2 is designed for comparing the effect of using different well configurations the gi approach can use wells and ports placed at arbitrary locations and estimate hydraulic parameters within and outside the well area while tti is only applicable for the region between wells hence although there are many other possible well configurations for this case an arbitrary well configuration for the tti approach is not suggested a uniform pumping rate is set at 8 ml secs for all tests for laboratory and field experiments signal strength should be strong enough so that it can be detected under the presence of various noises in previous studies the magnitude of pumping rates for the gi and tti approaches have been proven to have negligible impacts for synthetic experiments brauchler et al 2003 yeh liu 2000 all inversion codes were run on the same pc with intel core i7 7700 cpu and 16 gb random access memory 3 2 data used for inverse modeling based on previous tti related studies hu et al 2011 brauchler et al 2013 we know that using tti requires constraining the angle between pumping and observation ports which is introduced in section 2 considering the small angle constraint hydraulic head response data from three observation ports that have relatively small angles to the pumping port are utilized for tti fig s2 to maintain fairness the selection of observation ports for each pumping test is the same for both approaches for each well configuration case two scenarios with different selections of observation data for inverse modeling are considered in the analyses presented in this study to construct a fair comparison between two approaches scenario 1 s1 is designed for providing the same dataset which contains same number of observation data points for both approaches to ensure both approaches work for s1 the selection of observation data mainly was based on the requirement of using tti since the proper data selection for gi does not work with tti according to the definition of travel time there is one travel time value in a drawdown curve generated from one constant rate pumping test hence for a given pumping test only one head data at every observation well would be selected to the dataset for s1 however when using hydraulic head based on gi it is suggested that several hydraulic head data that capture the entire drawdown curve are selected this means that the dataset for s1 maybe not be appropriate for gi and the s1 dataset will undoubtedly generate a less satisfactory result for the gi approach both approaches in s1 include 24 and 48 observation data for inverse modeling for cases 1 and 2 respectively for the second scenario s2 the main factor to be considered is to construct an appropriate dataset for both approaches during a common application of a ht survey users would use the most suitable sampling strategy for the best performance with the most satisfying inversion results to mimic this situation different observation data are selected and utilized for inverse modeling using two approaches based on their corresponding optimal temporal sampling strategies for tti the operation involves selecting early time data for inversion i e the time when the derivative of head time curve rises to 20 of its maximum amplitude which is also called t 20 diagnostic for gi the operation involves selecting several data points from each head observation curve which can describe the salient features of the transient drawdown behavior i e five head data points from each drawdown observation curve are selected for the inversion these five head data are 10 20 60 80 and 99 of the maximum drawdown with their recorded times respectively keeping the same observation ports as s1 tti in s2 includes 24 and 48 observation data for inverse modeling for cases 1 and 2 respectively while gi in s2 involves 144 and 288 observation data for inverse modeling for cases 1 and 2 respectively to ensure fairness input parameters for tti or gi for different cases and scenarios to be the same the information of input dataset is summarized in table 1 the overall workflow is illustrated in fig 3 initially the forward simulation code mmoc3 is utilized to generate synthetic hydraulic head data then several datasets are constructed based on the various cases and scenarios tti and gi approaches are then used to analyze the dataset to estimate parameter fields finally d tomograms from tti while k and ss tomograms from gi are compared and evaluated quantitatively in this study a calibrated effective parameter model is used to provide homogeneous k and ss estimates for different approaches for tti during model validation a homogeneous s s is used to separate k from the estimated d d k s s for generating drawdown data by mmoc3 for the gi approach the effective k and ss estimates are used as initial values the provided dataset for effective parameter estimation also follows various of cases and scenarios summarized in table 1 4 results 4 1 travel time based inversion fig 4 shows the reconstructed d tomograms from different comparison scenarios and cases along with the true d distribution of the synthetic model fig 4e these estimated tomograms have the same color scale in each tomogram the pumping and observation ports are represented by red circles and white circles respectively for classifying results briefly the abbreviation of cx sx is used to represent case x scenario x in the following figure for example c1 s1 indicates case 1 scenario 1 comparing with the true d distribution estimated d tomograms successfully reveal significant structural features of the aquifer the horizontal layer with high d on the top and bottom of aquifer can be well captured in most situations fig 4a 4b 4d when dataset construction criteria of t 20 diagnostics are considered the estimated d tomograms fig 4b and 4d reveal distinct horizontal layers with high d at the top and middle of the aquifer this result shows that the tti can locate high d areas well which is consistent with previous studies brauchler et al 2003 2007 for low d characterization fig 4d has the best performance which reveals two low d zones successfully while on other tomograms fig 4a 4b and 4c the middle of aquifer with low d is blurry however there are two main obvious drawbacks to the tti approach according to these results firstly the estimated d values at the top high d zone of the inversion domain are far from the true values for some areas fig 4b and 4d this drawback is similar with the finding from brauchler et al 2013 based on the eikonal equation eqn 3 and the assumption of homogeneous aquifer brauchler et al 2003 the travel time is thought only to be related with d however when using tti there are some obvious abnormal data points which do not follow the above equation the normal method is to remove these data points from inverse modeling while this study retains them based on the consideration of fairness based on the research by vasco et al 2019 we speculate it is also related to the fact that the eikonal solver neglects the effect of hydraulic head field on hydraulic signal propagation the second drawback is obvious on fig 4c which uses the c2 configuration and peak arrival time for s1 this figure shows that both the left half and the right half of tomogram can reveal the top high d zone however the estimated d on the right half of tomogram is smaller than the left which is different from the true d distribution the same phenomenon also appears in other areas of fig 4c yet it is more obvious for the top layer this phenomenon is caused by the fact that the sensitivity of the peak arrival time decreases strongly with an increase in the distance between wells used for pumping and observation which can be also found in brauchler et al 2007 fortunately using the early travel time data s2 overcomes this drawback fig 4d 4 2 geostatistical inversion in this study the calibrated effective parameter model is utilized as initial k and ss fields homogeneous for the gi approach following the suggestions by xiang et al 2009 inversion results are selected when the l 2 norm stabilizes here stabilization of the l 2 norm indicates that the variation of adjacent iterations is smaller than 1 10 4 however the gi result for c1 s1 is an exception because the l 2 norm of its initial step is too small the gi result for c1 s1 is selected when r 2 yields a value of 0 995 indicating that the model is well calibrated fig 5 shows the estimated k tomograms for two cases under two comparison scenarios along with the true k fields fig s3 shows the s s tomograms for the above situations along with the true s s fields fig 5b and 5d show that the estimated k tomograms characterize the heterogeneity of aquifer well when the dataset utilized for inverse modeling follows the suggested strategy the comparison between fig 5b and 5d shows the c2 configuration results in a k tomogram that reveals more details in heterogeneity on fig s3 a similar phenomenon is seen in which the s s tomogram for c2 s2 yields more details to heterogeneity than c1 s2 however the estimated k tomogram fig 5a 5c shows that s1 limits the inversion performance for the gi approach which utilized head data points at peak arrival times only the blurry pattern of fig 5a shows that the inversion result of gi on c1 s1 is unsatisfactory that is the outer area of fig 5c has much higher values that are not consistent with the true distribution this is because the k distribution is known to be more sensitive to intermediate to late time data while ss is most sensitive to early time data sun et al 2013 hence it is reasonable to expect that the gi approach can provide k and s s estimates for s1 but cannot yield satisfactory results to compare the results obtained from tti d tomograms are calculated based on the estimated k and ss values d k s s from the gi approach fig 6 a and 6c show the d tomograms are not good for s1 the distribution of the high d zone at the top of fig 6a is well reconstructed but the value of this zone is far from the true value whereas at the middle depth of fig 6a the low d area is not well described according to the true d distribution in particular the outer area of fig 6c reveals a uniform high d which indicates that the outer area is not adjusted sufficiently enough through inverse modeling these observations inform us that the s1 is not appropriate for gi fig 6b and 6d reveal that the d tomograms obtained from gi for s2 capture the heterogeneity patterns although gi is not developed for a direct estimation of d the reason for this result is that the variability of the estimated s s tomogram fig s3 is much smaller than estimated k tomograms fig 5 hence the well estimated k tomogram dominates the d distribution in terms of computational costs the tti has an obvious advantage over gi table 2 generally tti converges within several seconds while gi requires several hours for s2 with the same pc obviously the computational time for gi increases with the number of input data while the computational time for tti remains small for cases with different amounts of data 4 3 direct comparison to make a quantitative assessment of estimated parameters scatterplots of the estimated versus true l n d values are plotted for both tti and gi approaches quantitative indices such as mean absolute error l 1 mean square error l 2 and the coefficient of determination r 2 are used to assess the results in each scatterplot a linear model that fits all data and a 45 line are included along with the computed l 1 and l 2 norms as well as r 2 values l 1 l 2 and r 2 are defined as 8 l 1 1 n i 1 n x i x i 9 l 2 1 n i 1 n x i x i 2 10 r 2 1 n i 1 n x i μ x x i μ x i 1 n x i μ x 2 1 n i 1 n x i μ x 2 2 where n is total number of data i indicates the data number x i and x i represent the value from estimated and true models respectively μ x and μ x represent averaged values from the inversion and true models respectively as mentioned previously the tti approach only reconstructs the d tomogram between pumping and observation wells while gi can reveal the heterogeneity throughout the simulation domain thus the same area between pumping and observation wells is adopted for the comparison fig 7 shows scatterplots of the estimated versus true l n d values of the selected area the slope intercept r 2 of the linear model as well as l 1 and l 2 norms are quantitative indices utilized for the judgement of inversion performance an inversion with good performance yields a slope and r 2 of scatterplot close to one which means that the estimated parameter is close to the true value and has a good fit with the true distribution on the scatterplot the higher the intercepts as well as l 1 and l 2 norms are the larger is the difference between the estimated and true parameters the lower are the intercepts as well as l 1 and l 2 norms the better is the inversion performance there are some similarities between the tti and gi approaches d tomograms of gi have the ability to characterize the heterogeneity of the aquifer for s2 which is supported by the good performance on fig 7f and 7 h for tti d tomograms on s2 fig 7b 7d also have obvious improvements on slope and r 2 indices than d tomograms on s1 fig 7a 7c this means for s2 using more observation data from additional locations can improve the visual quality of tomograms for both approaches but gi has more obvious improvement for s2 than tti which is supported by that the performance indices for gi on c1 s2 and c2 s2 that are obviously better than c1 s1 and c2 s2 respectively finally fig 7c and 7 g show that tti and gi both yield poor results for c2 s1 this means that c2 s1 is not an appropriate group for both approaches examination of fig 7 shows that the gi approach for c2 s2 fig 7h yields the best estimated d field while the 2nd best is gi for c1 s2 fig 7f then the tti approach for c1 s2 fig 7b and c2 s2 fig 7d yields acceptable results poor results are obtained for tti in terms of c1 s1 fig 7a c2 s1 fig 7c and gi for c1 s1 fig 7e c2 s1 fig 7g for s1 both tti and gi yield poorly estimated d tomograms but the gi result for c1 s1 fig 7e is relatively better than the other three situations the slope and r 2 of fig 7e are better than other results for s1 yet its intercept l 1 and l 2 norms are too high to be considered as a good result comparisons of various scatterplots in fig 7 show that gi under c2 s2 yields the best estimated d field with respect to quantitative indices however when the dataset contains less data the gi approach yields poor inversion results according to figs 7e and 7 g these results reveal that for gi an appropriate dataset with sufficient data points can strongly improve its performance to obtain the best performance the gi approach needs more observation data than tti the improvement of the amount of data from ports has little effect on tti for s2 which is different with the gi approach this observation is supported by the similar performance seen on figs 7b and 7d on s2 using the early travel time the well configuration seems to have little effect on tti performance however for gi on s2 the slope in fig 7h shows obvious improvement compared to that in fig 7f it shows the well configuration has an obvious effect on gi performance on fig 8 the vertical variation of ln d from tti and gi approaches as well as the true ln d distribution are shown to highlight the performance of tti and gi along the vertical direction this figure displays the depth variation of the maximum mean and minimum of l n d which is calculated by l n d of elements with the same horizontal coordinate as mentioned before the compared area is adjusted to the area between the pumping and observation wells the vertical variations of ln d demonstrate that all tomograms from tti can reveal the high d zones at on the top and bottom of the aquifer tti has the stable performance of distinguishing different magnitudes of d at different depths however the variation along depth is smooth this is due to the assumption of the eikonal equation eqn 3 which assumes a smooth heterogeneous distribution of aquifer parameters as shown in fig 8 it is obvious that the gi approach for c2 s2 yields the best performance it is worth mentioning that the ability of gi to characterize the middle low d zone is much better than tti for the area between the depths of 20 and 40 cm the variation of all tti tomograms approximates a straight line while gi for c2 s2 characterizes a similar variation tendency compared to the true d distribution fig 8g shows that a high d zone is found by gi at about 30 cm from the bottom and this high d zone is even higher than the top and bottom of sandbox which differs with the true d distribution from previous analysis the reason has been already known that the dataset for c2 s1 contains only few very early time data but for tti using early time data is its characteristic and it is very possible to have better performance with the earlier travel time data 4 4 model calibration and validation for the tti approach the root mean square rms residual is used to characterize the performance of model calibration for each case and scenario rms residual of tti model is lower than 1 10 2 sec fig s4 shows the rms residual of tti with iteration steps for the gi approach fig s5 shows the variation of the l 2 norm with the iteration steps computed by gi approach fig s6 shows the simulated drawdowns versus true drawdowns at the selected iteration step for model calibration figs s4 s5 and s6 show that both tti and gi models are well calibrated this confirms that the inversion of both approaches is conducted correctly ensuring the meaningful evaluation of inversion results for model validation the observation time interval is 0 5 sec and the simulation period is set as 20 sec to ensure that the drawdown reaches a steady state condition the pumping tests are conducted at 16 ports ports 3 5 9 11 15 17 21 23 27 29 33 35 39 41 45 47 for each pumping test the remaining 47 ports are utilized to record hydraulic head variations the configuration of ports used for model validation is shown in fig 9 as mmoc3 does not utilize d to generate head data an effective homogeneous ss is provided for tti to separate k from d then the heterogeneous k and homogeneous ss are used to generate drawdown data using mmoc3 it is worth mentioning that the heterogenous d distribution is only utilized for the area covered by the pumping and observation wells which is the area of travel time inversion for the outer area of the sandbox a homogenous d value is utilized for the modelling which is calculated as mean travel time based on eqn 3 for the gi approach the drawdown data for validation is generated using the corresponding estimated heterogeneous k and s s validation scatterplots of the true and simulated drawdowns for tti and gi are shown on fig 10 and the quantitative indices are also provided in each sub figure the l 1 and l 2 norms of each scatterplot are summarized and shown in table s2 these scatterplots indicate that gi yields better validation results than tti for the following cases and scenarios c1 s2 c2 s1 c2 s2 generally the gi approach yields better and more stable validation results than tti especially for s2 in contrast the comparison of fig 10a and 10e reveals that tti yields better validation results than gi for c1 s1 the scatterplot shown as fig 10c indicates that the tti approach yields poor validation results for c2 s1 as explained earlier tti has disadvantages due to sensitivity issue when using peak arrival times for inverse modeling hence the utilization of c2 should be viewed with caution because without using early travel time data tti yields poor validation results when distances between pumping and observation wells are different the comparison of figs 10c and 10d shows obvious improvement of tti by using the t 20 diagnostics for c2 the slopes 0 56 0 59 of figs 10b and 10d reveals a smaller predicted drawdown than true drawdown for tti on s2 this means that while using the early time data improves the performance of tti it causes a general overestimation of d over the entire aquifer in general the gi approach predicts biased drawdown data for s1 fig 10e and 10 g the biased drawdown indicates that the datasets from s1 is not appropriate for gi scatterplots of fig 10f and 10 h are much better than that of fig 10e and 10 g for s2 the gi approach yields better results according to the quantitative indices it shows the importance of utilizing an appropriate dataset for gi i e sufficient observation data points should be selected for capturing the hydraulic behavior of drawdown curves 5 discussion in this study performances of the tti and gi approaches are compared with each other based on the direct comparison of computed tomograms as well as through the validation of the estimated parameters through forward simulations of tests not used in the calibration effort it is worth noting that the fairness of comparison has been partly compromised to ensure the correct implementations of both approaches for s1 datasets with the same amount of data points are provided for the two approaches for gi it uses the head data at the time when the derivative of head data reaches a peak yet the peak time of the derivative of drawdown curve for tti is calculated with head data over a time interval rather than with a single data point hence some may consider that the condition of s1 is more beneficial for tti than gi however as mentioned earlier tti utilizes a high frequency asymptotic solution to solve the diffusion equation therefore some assumptions of the tti approach are not met for the sandbox for example tti assumes that boundary conditions have minor effects on inversion results and the spatial variation of the estimated d is assumed to be smooth these assumptions hamper the performance of tti for the sandbox study with a much smaller scale and larger boundary effect hence the use of a sandbox to conduct the comparison may be more advantageous for gi than tti for this study we find that the tti and gi approaches have a complementary relationship rather than a competitive relationship it is hard to find one approach with overwhelming advantages in all situations that is each one of them has advantages for specific situations generally tti may be used to derive an initial model for gi because it is not as sensitive to starting model with initial guesses as gi on the other hand the gi approach is more suitable for estimating hydraulic parameters necessary for groundwater modeling with higher accuracy as an initial study we started with only two factors to investigate the difference between two approaches specifically we focused on the important effects of data density and well configuration for the two inversions with one boundary condition setting noise free dataset and one synthetic aquifer based on a laboratory sandbox other aspects such as the variations in boundary conditions the inclusion of noise with different kinds and levels as well as variations of geology e g geological settings degree of heterogeneity correlation lengths etc are also important for a more comprehensive comparison study these variables will likely affect the performance of both tti and gi particularly gi considers the effect of boundary conditions while the eikonal equation based tti assumes that boundary conditions are negligible for early stage hydraulic data actually the boundary condition does affect the hydraulic signal propagation especially for the late stage hydraulic data and sensitivities of travel times for different stage data are different vasco 2018 hence the performance of eikonal equation based tti will deteriorate with the increase of boundary condition effects on travel times although the current study has above mentioned limitations the findings of advantages disadvantages of the two approaches learned are informative in deciding when to apply the two approaches under real world situations firstly both approaches have been tested using a synthetic aquifer constructed based on a real sandbox study consisting of an interfingering fluvial deposit found commonly at many real field sites secondly the well configuration considered in this study with few wells c1 is a common situation at real sites thus the comparison of two inverse modeling approaches based on few wells is relevant to real field situations with only a few available wells and monitoring ports thirdly the various scenarios and cases examined clearly showed when each approach yields improved hydraulic parameter estimate for a given dataset which should be useful for field studies fourthly with the use of an appropriate dataset for each inverse modeling approach in s2 the comparison study revealed the capabilities of each approach in estimating the hydraulic parameters for the same aquifer which should allow researchers and practitioners to make better decisions on the approach to utilize in real field situations finally based on the comparison of the two inverse modeling approaches and the results obtained we found that the combination of tti and gi approaches appears to be promising and feasible the tti approach can be initially applied to early time data to quickly obtain the prior model for the gi approach at a low computational cost then data from different ports and well locations as well as from different stages early intermediate late of the drawdown curves could be incorporated into the gi approach yielding k and ss tomograms of higher accuracy that is the initial d tomogram estimated from tti can provide structural information and an initial k estimate both of which are expected to assist the gi approach in achieving more accurate results while accelerating the speed of model convergence 6 summary conclusions and outlook through various studies estimation of heterogeneous aquifer parameter reconstruction from ht surveys have demonstrated significant advantages over traditional characterization approaches which include 1 the ability of characterizing aquifer heterogeneity including the connectivity of flow transport pathways between boreholes 2 the improved performance of predicting groundwater flow and solute transport illman et al 2012 jiménez et al 2015 luo et al 2020 3 providing information on uncertainty or reliability of estimated tomograms brauchler et al 2013 illman et al 2010 and 4 providing valuable information on hydraulic parameter heterogeneity at an acceptable cost when compared to site heterogeneity characterization based on a large number of small scale samples illman et al 2012b yeh liu 2000 in this study numerical experiments are conducted by using a synthetic heterogeneous aquifer which is derived from a laboratory sandbox aquifer to compare two kinds of transient hydraulic tomography tht algorithms based on tti and gi approaches two configuration cases of pumping and observation wells are designed with two scenarios for each case for case 1 eight pumping and eight observation ports are set at opposing sides of the sandbox for case 2 eight pumping ports are set in the middle part of the aquifer while eight observation ports are set at each side with a total of sixteen observation ports the two scenarios for each case are designed to compare the estimation performance and effect of using different selected data for inverse modeling for s1 the same dataset is provided to both approaches for tti it uses the travel time which is the peak arrival time of the drawdown derivative curve for gi it uses the head data at the peak arrival time for s2 the construction of dataset is based on the requirement for each approach for the tti approach the optimal data selection strategy involves the early travel time selection which is also called the travel time diagnostic for the gi approach the second scenario provides sufficient data points to capture the salient features of drawdown curves that contain information on both k and ss heterogeneity model calibration results confirm the correct utilization of both approaches the estimated tomograms from both approaches are compared through direct comparisons and model validation for model validation sixteen pumping tests are conducted at ports which are not used for calibration purposes when one port is used for a pumping test the rest of the 47 ports are set as observation locations our study leads to the following findings and conclusions 1 in terms of direct comparison both approaches can characterize subsurface heterogeneity with regards to the hydraulic parameter distribution for c2 s2 the gi approach shows good reconstruction of heterogeneous k and s s consistent with the true distribution while the tti approach can roughly recover the variations of d it is shown with this comparison that gi has larger estimated range than tti the latter focuses on the area between pumping and observation wells the ln d from estimated tomograms is compared with true ln d distribution by scatterplots these scatterplots show that tti has better result for scenario 1 while gi has better performance for scenario 2 the vertical variation plot shows that tti performs better in locating the high d zone than gi while gi has better characterization capabilities for low d zone 2 in terms of model validation and head prediction which is based on the numerical modelling with the estimated tomograms from gi and tti as inputs the gi approach yields the best validation result under case 2 scenario 2 gi has the obvious advantage if the dataset meets the requirement of the suggested strategy i e gi has better result when the dataset has sufficient data for capturing the shape of the drawdown curves in contrast tti has an advantage on cost because it only needs data during the early stage of the pumping test and 3d inversions are affordable on a personal computer however tti generates the worst validation result under case 2 scenario 1 the peak arrival time data is not appropriate for characterizing the d of aquifer with an asymmetrical well configuration the early travel time diagnostic is necessary for this configuration 3 the early travel time diagnostic of tti are found helpful in obtaining good tomograms and validation results yet it cannot solve the problem that the reconstructed d values differ significantly from real values in certain areas of the simulation domain this phenomenon is due to the tti algorithm that ignores both initial and boundary conditions which are crucial factors for ht data collected in an extremely small domain such as the small scaled sandbox utilized in this study 4 this comparison study also led to the idea of sequential inversion of tti and gi to be a promising concept firstly the similar pattern is shown on tomograms of tti and gi for s2 which is also consistent with true parameter distribution secondly the time cost of tti is negligible compared with gi therefore tti can be used initially with early time data to rapidly generate a d tomogram that can then be used as prior information for the gi approach the gi approach can then take advantage of information contained in the entire drawdown curve to more accurately map the k and ss distributions for a future study an attractive topic is the combination of these two methods which is currently being studied with the establishment of an initial model for the gi with the structure information that is based on the d tomogram from the tti this combination could increase gi performance by using early stage data of the hydraulic test and to decrease the time cost i e the time for collecting data and head recovery between tests while conducting a ht survey we believe that with a successful combination of these two methods ht surveys can be more efficient and accurate for aquifer characterization at a higher spatial resolution credit authorship contribution statement huiyang qiu methodology writing original draft validation funding acquisition rui hu project administration funding acquisition writing review editing ning luo writing review editing walter a illman supervision resources writing review editing xiaolan hou writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the ministry of science and technology of china through the program driving process and mechanism of three dimensional spatial distribution of high risk organic pollutants in multi field coupled sites project code 2019yfc1804303 walter a illman acknowledges the partial support from the discovery grant funded by the natural sciences and engineering research council of canada nserc which facilitated this collaboration the first author acknowledges the financial support of china scholarship council 201906710028 open research there is an open source version of tomogo via github https github com wichniarek sirt variants git the datasets utilized for developing the tti and gi models are provided as a supplement as dataset zip appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129247 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2237,selection of the number and which of multisource precipitation datasets is crucially important for precipitation fusion considering the effects of different inputs this study proposes a new framework based on the bayesian model averaging bma algorithm to integrate precipitation information from gauge based analysis cpc reanalysis derived dataset era5 and satellite retrieval products imerg e and gsmap rt the bma weights were optimized for the period 2001 2010 using daily measurements and then applied to the period 2011 2015 for model validation seven bma merged precipitation products i e mce mci mcg mcei mceg mcig and mceig were thoroughly evaluated across mainland china and then compared against the state of the art ensemble based product mswep the results indicate that the bma predictions performed substantially better than the reanalysis and satellite precipitation datasets in both daily statistics and seasonal analyses mce mci and mceg demonstrated better performances relative to cpc in terms of individual metrics moreover mci mcg and mcei generally outperformed mswep over the entire study area particularly in local regions such as southwestern china and the eastern tibetan plateau during typhoon rammasun in 2014 mcg and mceg provided greater detail for heavy rainfall events than the four ensemble members and the mswep product thus the performance of the bma predictions exhibited evident differences because of various input sources cpc was the major internal influencing factor with the highest weight score meanwhile the increased input cpc dataset into the bma based schemes exerted a significant influence on the precipitation estimates which markedly facilitated the performance improvement of the fusion model and its improved degree greater than 14 was obtained using a changed initial comparison method our results demonstrate that the developed modifiable bma framework is useful for analyzing the impacts of ensemble members on bma predictions and suggests that it is considerate in the use of different input sources for generating ensemble based precipitation products keywords precipitation precipitation fusion bayesian model averaging ensemble based precipitation product performance improvement mainland china 1 introduction precipitation is an essential component in the water cycle therefore high quality precipitation estimations are imperative for the assessment monitoring and prevention of natural disasters in hydrological meteorological and agricultural fields duan and bastiaanssen 2013 yang et al 2020 yin et al 2021 conventional ground stations are the most reliable way to document precipitation records at specific locations however these ground networks are usually unevenly and sparsely distributed in many developing countries particularly over complex topography lanza and stagi 2008 baez villanueva et al 2020 therefore it is challenging to obtain accurate spatiotemporal information of precipitation patterns using only in situ measurements fortunately grid based gauges reanalysis models and satellite sensors provide viable alternatives that can provide open access and near global coverage precipitation estimates at fine spatial and temporal resolutions currently there are three categories of global gridded precipitation datasets 1 gauge based precipitation products that are constructed from globally available rain stations using different interpolation techniques the accuracy of the datasets is strongly dependent on the density and number of rain stations chen et al 2008 shen and xiong 2016 schneider et al 2016 these widely used gauge based precipitation datasets comprise climate prediction center cpc global unified gauge based analysis of daily precipitation xie et al 2007 2 reanalysis model based precipitation products that have been developed based on various sources using data assimilation algorithms and state of the art numerical analysis and forecasting climate systems dee et al 2011 jiang et al 2021a which provide an array of vertical atmospheric information containing precipitation variable the fifth generation of the european centre for medium range weather forecasts ecmwf atmospheric reanalysis i e era5 is a representative multi year global reanalysis dataset graham et al 2019 3 satellite precipitation estimates that are produced with intermittent advances in sensor resolutions and retrieval algorithms that have great opportunities to observe continuous precipitation patterns particularly over areas with inadequate stations huffman et al 2007 hou et al 2014 presently the mainstream satellite precipitation products include the following integrated multi satellite retrievals for global precipitation measurement imerg huffman et al 2020 and global satellite mapping of precipitation gsmap ushio et al 2009 these gauge reanalysis and satellite based precipitation products significantly facilitate precipitation mapping for a broad range of applications however these products lack coherence relative to ground observations with varying accuracies in different areas duan et al 2016 jiang et al 2018 beck et al 2019a satgé et al 2020 tang et al 2020 amjad et al 2020 wei et al 2021 therefore it is suggested to improve the accuracy of precipitation estimates by taking the best of the multiple sources in recent years many efforts have been made to utilize or propose different methods to merge various gauge reanalysis and satellite precipitation datasets to obtain better precipitation estimates beck et al 2017 beck et al 2019b xu et al 2020 baez villanueva et al 2020 wu et al 2020 for example yumnam et al 2022 considered three satellite precipitation datasets to run a quantile based bayesian model averaging bma algorithm during the monsoon season the validation results verified that bias corrected quantile based bma products outperformed the imerg product over the india s vamsadhara river basin in terms of the average error index ma et al 2018 established an ensemble based dynamic bma framework to integrate four satellite precipitation products by optimizing the relative product weights the evaluation results validated that the dynamic bma merged precipitation product reduced the estimation error and showed better performance than the individual members over the tibetan plateau in terms of some metrics yin et al 2021 established a three stage bma approach to combine five precipitation datasets by incorporating gauge reanalysis and satellite products over mainland china statistical metrics also indicated that the three stage bma method performed better than individual members these studies primarily focused on the development of fusion algorithms which are of immense importance for use in precipitation integration schemes the performance of any precipitation product is primarily affected by the algorithm and input sources chen et al 2019 beck et al 2019a chen et al 2020 satgé et al 2020 tang et al 2020 baez villanueva et al 2020 it is equally important to blend precipitation products to analyze the influences of selecting the number and type of multisource precipitation information as model inputs before the fusion procedure chen et al 2019 had proven that the performance of gsmap product has connections to the error characteristics of the input sources from several sensors in the data fusion the input sources play a pivotal role for the development of blended precipitation estimates however previous studies have rarely investigated the influences of different model inputs on the precipitation fusion scheme in this study the bma algorithm as the core of a new framework is proposed to investigate and simultaneously compare the performance differences of blended precipitation products under different input sources and to quantitatively analyze the influences of ensemble members on the bma based fusion schemes the design experiment was performed on four original daily precipitation datasets comprising unified gauge based analysis cpc atmospheric reanalysis model data era5 and multisource satellite retrieved products imerg and gsmap from 2001 to 2015 across mainland china subsequently the bma merged precipitation products were synthetically assessed using an array of evaluation metrics and compared to the publicly available ensemble based precipitation product mswep the effects of input members on the bma predictions were quantified by using the optimal weights and a changed initial comparison method the results of this study will provide a new perspective of error sources for merging multisource precipitation information into a fusion algorithm 2 study area and data 2 1 study area the study area was mainland china situated in eastern asia and on the western bank of the pacific ocean falling in the spatial field between longitudes of 72 136 e and latitudes of 18 54 n the area has monsoon climate characteristics and complex topography which includes low altitude plains and hills in the east sichuan basin and loess plateaus in the central and high altitude mountains and plateaus in the west thus the precipitation of the region exhibits a significantly uneven spatiotemporal distribution increasing gradually from the northwest inland to the southeast coast with high frequencies and levels in summer the chinese mainland is commonly divided into eight subregions according to the average annual precipitation distribution elevation and mountain chain information chen et al 2013 these subregion divisions consist of northeastern china ne northern china nc the middle and lower reaches of the yangtze river cj southeastern china se southwestern china sw northwestern china nw xinjiang xj and tibetan plateau tp as shown in fig 1 a in this study these subregions were adopted to analyze the spatial heterogeneity of the blended precipitation products and further compare their regional evaluation statistics 2 2 ground observations the ground observed daily precipitation dataset for the period 2001 2015 was collected based on 824 meteorological stations from the china meteorological administration cma the quality i e extreme check and consistency test of all data is strictly controlled by a series of criteria before availability on the website https data cma cn shen et al 2010 shen and xiong 2016 moreover to ensure the integrity of the precipitation data and maintain consistency with satellite precipitation products utc day definition 807 effective meteorological stations were handled under certain rules wei et al 2022 and selected as the reference dataset in this study there is a ground network of meteorological stations that cover mainland china as shown in fig 1b with a dense group in eastern china and sparse distribution over the western region particularly in the western tp 2 3 gauge reanalysis and satellite based precipitation products cpc is a unified gauge based daily precipitation product from 1979 to present developed by the national oceanic and atmospheric administration climate prediction center using the optimal interpolation objective analysis technique and more than 30 000 gauge records which includes approximately 17 000 near real time gauges xie et al 2007 more specific information on this product can be found in previous literature xie et al 2007 chen et al 2008 the near real time cpc product mostly covers all latitudes and longitudes of land with a spatial resolution of 0 5 since the release of the gauge based cpc daily precipitation data on the website https psl noaa gov data gridded data cpc globalprecip html it has been employed to correct the bias of satellite precipitation products i e cmorph and gsmap and blend multisource precipitation datasets i e mswep to obtain higher precision precipitation tian et al 2010 mega et al 2014 beck et al 2017 therefore the cpc global daily precipitation product from 2001 to 2015 was chosen for this study for multiple precipitation data fusion era5 was produced by the ecmwf as the successor to the era interim to offer a new generation of global atmospheric reanalysis products era5 provides much higher quality estimations of atmospheric ocean wave and land surface parameters with a finer 0 25 1 h spatiotemporal resolution than the era interim 0 75 6 h and has a similar model and input to the era interim dee et al 2011 xu et al 2022 this contributes major improvements to the atmospheric model and data assimilation technology over the era interim this study used the total precipitation parameter of the era5 reanalysis precipitation product for the period 2001 2015 the native 0 25 resolution and 1 hourly precipitation product era5 was downloaded from the climate data store website https cds climate copernicus eu search text era5 type dataset and converted to a daily precipitation dataset to guarantee temporal consistency with satellite precipitation estimates the national aeronautics and space administration and japan aerospace exploration agency jointly established the global precipitation measurement gpm and generated representative open access satellite precipitation products namely the imerg and gsmap series the gpm imerg products inter calibrated merged and interpolated all available data from the gpm constellation satellite estimates gauges and other precipitation estimators based on the imerg algorithm huffman et al 2019 imerg provides global precipitation estimates with a relatively superior spatiotemporal resolution of 0 1 30 min and spatial coverage of full 60 s n latitudes the imerg suite incorporates the near real time early run satellite hereafter referred to as imerg e and late run multi satellite and post real time final run satellite gauge precipitation estimates imerg e is available approximately 4 h after retrieval using only forward morphing and is designed for near real time applications in this study the daily precipitation product imerg e from 2001 to 2015 with a daily record from 0 00 to 24 00 utc was utilized and downloaded from the website https disc gsfc nasa gov the gpm gsmap is a high spatiotemporal resolution 0 1 1h global precipitation product with a latitude range of 60 s n developed by employing a microwave infrared combined algorithm including the backward forward morphing and kalman filtering techniques to integrate the use of passive microwave estimates and infrared ir satellite retrievals ushio et al 2009 similar to the imerg products the multi satellite gsmap suite also contains three types of products near real time gsmap microwave ir reanalysis gsmap mvk and gauge calibrated standard gsmap gauge the near real time gsmap utilizes available microwave imagers and sounders based on the gsmap mvk algorithm for near real time precipitation estimates chen et al 2020 which comprises the gsmap nrt and calibrated gsmap gauge nrt rainfall data hereafter referred to as gsmap rt duo to the significantly poor performance of gsmap nrt relative to imerg e zhou et al 2020 chen et al 2020 the adjusted near real time daily precipitation product gsmap rt for the study period 2001 2015 was selected in this study and obtained online https sharaku eorc jaxa jp 2 4 mswep mswep version 2 8 was generated by hylke beck taking advantage of the global gauge based datasets atmospheric reanalysis data and satellite remote sensing retrievals based on the optimal integration procedure to obtain accurate precipitation estimates beck et al 2017 beck et al 2019b these datasets comprise state of the art gpcc ghcn d gsod worldclim era interim jra 55 gsmap tmpa 3b42rt cmorph gridsat and other datasets beck et al 2019a the fundamental fusion program involved in the development of mswep products can be found in existing literature beck et al 2019b although the mswep version will be updated intermittently to improve product performance the mswep product provides a truly global coverage historic precipitation product with a fine spatial resolution of 0 1 and a high temporal resolution of 3 h beginning from 1979 the systematic bias of terrestrial precipitation data from mswep product was adjusted by using more than 14 000 river discharge data across the globe and incorporation of daily precipitation from approximately 77 000 stations worldwide yang et al 2020 owing to the superior performance of mswep in comparison with other precipitation datasets in some areas mswep product is widely employed or evaluated in extreme precipitation drought monitoring and runoff simulations therefore the daily precipitation product mswep for the period 2001 2015 was regarded as the benchmark in this study to reflect the effect of data fusion behaviors and was downloaded from the network https www gloh2o org mswep 3 methodology 3 1 framework the overall structure of the modifiable bma based framework for multisource precipitation product fusion is shown in fig 2 there were five main steps 1 preprocessing the cpc era5 imerg e and gsmap rt precipitation datasets in terms of the nearest grid to point method to ensure spatial correspondence with the cma stations 2 separating the study period into the training period 2001 2010 and the validation period 2011 2015 3 constructing the diverse precipitation datasets for the different input sources taking the cpc data as the core of bma fusion 4 determining the optimal bma weight for each original precipitation member under various scenarios and 5 blending multisource precipitation datasets to generate the bma merged precipitation products under the different inputs for the period 2011 2015 and the spatiotemporal resolution of point 1 d across mainland china the bma scheme that was run using two original precipitation datasets were abbreviated as bma2 similarly three and four original datasets were used to drive the bma method denoted as bma3 and bma4 respectively seven blended precipitation product series were developed by using the bma technique these were mce merged cpc and era5 mci merged cpc and imerg e mcg merged cpc and gsmap rt mcei merged cpc era5 and imerg e mceg merged cpc era5 and gsmap rt mcig merged cpc imerg e and gsmap rt and mceig merged cpc era5 imerg e and gsmap rt 3 2 bayesian model averaging bma bma a statistical ensemble model based on the bayesian theory integrates inferences and predictions to obtain a more robust posterior probability using the prior probability of ensemble members and likelihood function duan et al 2007 in this study the bma approach was implemented to merge multisource precipitation datasets by assigning weights based on probabilistic skills to obtain a good fit to the in situ observations the procedure for estimating the optimal bma weights is described below in the training period according to the total probability formula the posterior probability density function pdf of the bma merged precipitation product is expressed as 1 p y g s 1 h p f s g p s y f s g where y and g are the blended precipitation product and cma observed precipitation data respectively h denotes the number of gauge and or reanalysis and or satellite based precipitation datasets inputted into the bma fusion p f s g represents the posterior probability of the inputted precipitation dataset f s and identified as the likelihood of ensemble members the term p s y f s g denotes the posterior distribution of y derived based on the input precipitation estimates f s and ground observations the posterior probability of each member is computed corresponding to the same in situ observations thus making the sum of the weights equal to one if the weight w s represents p f s g and s 1 s p f s g s 1 h w s 1 then eq 1 can be updated by substituting the posterior probability p f s g with the weight w s and is then given by 2 p y g s 1 h w s p s y f s g moreover the posterior mean e y g and variance v a r y g of the bma merged precipitation product can be approximated as 3 e y g s 1 h w s f s 4 v a r y g s 1 h w s f s e y g 2 s 1 h w s σ s 2 where σ s 2 is the variability of the input precipitation dataset f s for the observed g dataset usually the prior pdf of precipitation does not conform to the gaussian assumption with a normal distribution the original precipitation datasets and cma data were preprocessed using the box cox transformation before performing the bma algorithm raftery et al 2005 the box cox function from the matlab software was employed to achieve data transformation ma et al 2018 this can be used to obtain the approximate gaussian distribution of conditional probability p s y f s g the weight w s of the input precipitation datasets are efficiently estimated based on the maximum log likelihood function which is expressed as 5 l w s σ s log s 1 h w s p s y f s g log s 1 h w s g y f s σ s 2 where g represents the gaussian distribution for the analytical solution to obtain the optimal bma parameters the expectation maximization e m algorithm was used by an iterative way to maximize the log likelihood equation raftery et al 2005 jiang et al 2012 during the iteration the prior bma weight of per individual member was set to 1 h the e m algorithm includes two steps an e or expectation step and an m or maximization step a more detailed formulation of the e m algorithm is provided in previous literature mclachlan and krishnan 2007 bma probabilistic prediction can reflect the better reliability of input precipitation datasets against ground observations with a larger weight ma et al 2018 3 3 evaluation metrics the performances of the original bma merged and mswep precipitation products against the cma ground observations were verified and compared using the following statistical metrics which incorporated the continuous and detection indices 1 the correlation coefficient cc relative bias rb root mean square error rmse and the modified kling gupta efficiency kge gupta et al 2009 are continuous metrics used to measure the linear correlation systematic bias total error level and comprehensive accuracy of the evaluated precipitation products jiang et al 2021b tang et al 2020 kge balances the contributions of the cc bias and variability components high kge and cc and low absolute rb and rmse values indicate better performance 2 the probability of detection pod false alarm ratio far and critical success index csi are detection metrics that reflect the correct detection false detection and overall detection capabilities of the evaluated precipitation products to identify rainfall events jiang et al 2021a the csi is expressed as an integrated function of pod and far the calculations of pod far and csi require a threshold which was set as 1 mm day to determine rain no rain events a large pod and csi and small far imply a more satisfactory prediction ability some information of the aforementioned statistical metrics is listed in table 1 3 4 quantifying the impacts of different input precipitation datasets on the bma based fusion scheme the accuracy characteristics of all bma merged precipitation products can be determined based on the abovementioned evaluation metrics to quantitatively determine the relative effect or contribution denoted by a percentage change of each individual member to the bma precipitation fusion under different inputs the values of the comprehensive performance metric kge for bma3 vs bma2 bma4 vs bma3 and bma4 vs bma2 were compared by considering a control variable method and the changed initial comparison method as following equation woolmer et al 2008 wang et al 2020 6 r x change x initial x initial 100 where r represents the percentage variation caused by increasing the input precipitation datasets into the original variables group of the bma based scheme x change and x initial are the kge values of the changed and initial blended precipitation products respectively for example r is the relative effect of the increased imerg e precipitation dataset which is the increasing input from the initial mce to the changed mcei if r 0 it implies that the increased input precipitation dataset promotes the quality of the blended precipitation product if r 0 it implies that the increased input precipitation datasets degrade the quality of the blended precipitation product 4 results 4 1 general accuracy of different precipitation products at a daily scale table 2 summarizes the overall performance of the evaluated precipitation products using regional average metrics over mainland china regarding the original precipitation datasets cpc had a significantly preferable performance compared to that of the other members in terms of both continuous and detection metrics with the highest kge 0 644 cc 0 776 and csi 0 633 and the smallest rb 1 54 rmse 4 6 mm d and far 0 308 era5 imerg e and gsmap rt had substantial errors and exhibited a significantly lower performance than mswep while each dataset had its own unique advantages for example era5 had relatively high pod whereas gsmap rt had relatively good rb after the bma data fusion one blended precipitation product always performed best excluding cpc and its reliability was slightly better than that of mswep according to single metrics namely mci with kge and far mcei with cc rmse and pod mcg with rb and mce with csi moreover all blended precipitation products outperformed mswep in estimating the magnitude of rainfall in terms of the cc rmse csi and far indices the mci tended to display the overall best capability for most metrics and blended precipitation products have evident improvements in the reanalysis and satellite precipitation datasets for that all some merged products except for mcg presented better performance than cpc in terms of cc and others i e mce mci mcei mceg and mceig for the rmse and pod metrics fig 3 shows the boxplots of six statistical metrics kge cc rb csi pod and far for the evaluated precipitation products over mainland china the performance of era5 imerg e and gsmap rt was unsatisfactory with wide ranges and or poor metric values which were excluded when the median rb was close to 0 for the bias adjustment gsmap rt and median pod exceeded 0 9 for era5 in contrast the cpc had considerably high reliability for all evaluation indicators compared to mswep the blended precipitation products except for mce exhibited a more robust performance with a higher value of kge which almost corresponded to the three quantiles fig 3a it is worth noting that all blended precipitation products had significantly good correlations i e most cc values in the range of 0 7 0 9 with cma observations and outperformed mswep and cpc in capturing the temporal variability of rainfall fig 3b meanwhile the blended precipitation products slightly overestimated the rainfall amounts with most rb varying from 10 to 15 fig 3c in which mcei had positive biases and the worst performance whereas mcg performed the best when comparing the detection abilities the reliability of blended precipitation products performed relatively well and most displayed outstanding performances compared to that of mswep for example all blended precipitation products presented a higher csi at the 75th percentile fig 3d a larger pod at the median line except for mcg and mcig fig 3e and a slightly lower far at the 25th percentile excluding mcei fig 3f moreover mce and mcei outperformed cpc in terms of the pod metric in general there were some apparent differences particularly for cc and pod metrics among the seven blended precipitation products because of the various inputs in the bma data fusion which likely determined whether the blended precipitation product performed better than mswep these characteristics of the bma merged precipitation products are further analyzed in the subsequent sections fig 4 shows the spatial distribution of kge for different precipitation products over mainland china among the original precipitation datasets each had significant spatial variabilities and regional characteristics notably there were also clear differences between the precipitation datasets the cpc primarily exhibited the highest kge value over the entire continent with a strongly stable spatial pattern although it was lower in some parts of xj era5 performed better in ne nc cj and nw than in sw xj and tp however diverse results were obtained for imerg e with relatively larger kge values in cj se and sw than in xj and parts of ne nw and tp essentially gsmap rt was not the same as the former the kge variation decreased from the east to xj and tp these behaviors fig 4a b c and d are important factors that determine the performances of bma merged precipitation products under different input sources the blended precipitation products obtained from the bma scheme with various input groups at the same input number exhibited remarkable performance differences in most regions this can be directly observed from the spatial patterns of kge for the mce mci and mcg which were homologous to those of era5 imerg e and gsmap rt respectively accompanied by the cpc as the input number increased from bma2 to bma4 the reliability of the blended precipitation product tended to reach a relatively steady state and similar spatial characteristics among the mcei and mceg versus mceig this indicates that inputting more datasets can reduce the spatial heterogeneity of the bma merged precipitation product to a certain extent however increasing inputs may not continuously improve the performance of blended precipitation products which was expressed in the decreased kge in some regions such as the mce versus mceig in nw and mci versus mcig in tp compared to mswep it is evident that the different blended precipitation products displayed their respective superiority and performance closer to the cma observations over various regions this stresses the need to carefully select the original data sources for the generation of merged precipitation products fig 5 shows the spatial distribution of csi for the different precipitation products over mainland china one commonality among the precipitation products is the higher csi values concentrated in the east compared to those in the west a descending rank in the detection ability was observed clearly by cpc era5 gsmap rt and imerg e era5 gsmap rt and imerg e had relatively low csi values less than 0 4 in most stations particularly in xj indicating low reliability to detect rainfall occurrence events for the blended precipitation products that significantly heightened the performance of detecting rainfall events mce had the strongest performance with a relatively wide range csi of stations that were greater than 0 65 followed by mci the spatial patterns of csi exhibited notable differences i e in ne and nw in the rainfall capture capability it is important to note that the bma merged precipitation products outperformed mswep in some regions such as sw and the eastern tp table 3 presents the regional average results of kge and csi for the different precipitation products in each subregion the discrepancies among all precipitation products in figs 4 and 5 were quantified for various subregions and clearly distinguished to determine the most applicable products in each subregion with the exception of cpc there were two situations for the best performance as follows 1 in terms of the kge index the following was chosen mswep in ne nc nw and xj where some bma merged precipitation products were extremely close to the best reliability mcig in cj se and sw and mci in tp 2 in terms of the csi metric the following was chosen mce in ne nc cj nw and xj mcg in se and mci in sw and tp where the best performance was not mswep for the overall detection capability it is important to note that the blended precipitation products had significant improvements of different magnitudes over the era5 imerg e and gsmap rt and some performed better than cpc in cj namely mcig se except for mce and sw except for mce and mcei in terms of the kge metric and similarly mce for xj in terms of the csi metric 4 2 seasonal analysis of different precipitation products fig 6 shows the seasonal average statistical outcomes of the seven metrics i e kge cc rb rmse csi pod and far for the different precipitation products over mainland china the metrics were computed based on the seasonal datasets that were obtained at a monthly time step and divided into four seasons spring march may summer june august autumn september november and winter december february the substantial improvements and potential differences in blended precipitation products are presented by the statistical metrics in all seasons specifically the bma method effectively improved rainfall estimations from era5 which initially had significant overestimations imerg e which previously had a poor csi and gsmap rt which initially had the worse kge during the four seasons where era5 imerg e and gsmap rt presented low precision large mean error level and low comprehensive detection ability products had relatively high usability in summer and autumn than in spring and winter especially where the kge metric was less than 0 notably in terms of most metrics mce mci and mcei performed better than mswep in spring especially for the cc csi and far indicators fig 6a in summer compared to the mswep all blended precipitation products exhibited a better capability in retrieving rainfall estimates in terms of the cc rmse csi and pod metrics whereas the opposite was true for the rb statistics fig 6b some similar behaviors were also observed in autumn for the cc and rmse indicators although the accuracy of each blended precipitation product showed some changes because of the reduction in rainfall from summer to autumn as indicated by the rb and rmse indices fig 6c in winter the mswep significantly overestimated the rainfall amount with more positive rb values above 80 thus incorrectly coinciding with rainfall occurrence events with far values exceeding 0 35 mswep also achieved a poor performance indicated by a lower kge compared to that of the blended precipitation products fig 6d the lower suitability of mswep also occurred relative to the individual blended precipitation products in terms of other metrics such as mcei s cc and mce s rmse the above results demonstrate that different blended precipitation products have seasonal variabilities and differences in rainfall estimations owing to the impacts of various input sources for the bma approach 4 3 extreme precipitation assessment typhoons rainstorm events and floods are destructive natural disasters that can cause tremendous economic losses this study investigated the performance of different precipitation products during typhoons and examined whether they can provide high quality rainfall information for typhoon associated extreme rainfall events the ninth typhoon in 2014 super typhoon rammasun was selected in this study for the extreme precipitation assessment typhoon rammasun occurred in the hainan and guangdong provinces on july 18 2014 with a maximum wind speed above 60 m s a threshold of 25 mm day was set to determine the heavy rainfall event for the detection capability fig 7 shows the spatial distribution maps of the daily rainfall amount of the cma data versus the different precipitation products over se during typhoon rammasun the corresponding results for the evaluation metrics are listed in table 4 heavy rainfall was primarily concentrated in the southwest of se and decreased to the northeast of se generally all precipitation products agreed well with the spatial pattern of the cma observations with a cc value greater than 0 83 but overestimated light rainfall and underestimated the typhoon rainfall volume at the center which is indicated by the spatial maps and negative rb metrics particularly for era5 rb of 25 2 and mswep rb of 20 1 of these products some bma merged precipitation products such as mcg and mceg provided much more detail compared to the original precipitation datasets and the mswep product for example cpc exhibited the worse performance in detecting heavy rainfall with a comparatively lower csi of 0 542 and obviously underestimated rainfall volumes which was indicated by an rb of 15 1 the kge of era5 was the smallest with the largest error level of 35 5 mm day compared to that of the other products evidently mswep had a poor comprehensive capability with a low kge of 0 693 and csi of 0 567 from these aspects the multisource data fusion schemes have shown to increase the reliability of the estimations of heavy rainfall events during the typhoon in addition owing to the different input sources of the bma model there were apparent differences among the blended precipitation products for capturing both the spatial patterns and volumes of rainstorm events such as the best mcg and the worst mce 4 4 weight variations of bma ensemble members fig 8 shows the spatial distribution of the relative optimal weights for the ensemble members under various scenarios over mainland china and table 5 presents the corresponding average weights the weights of the four original precipitation datasets with certain spatial heterogeneity were significantly influenced by the different inputs the relative proportions of the weights of the original datasets were almost unchanged that cpc always occupies the highest proportion whereas the optimal weights of the same ensemble member varied substantially under different data sources with respect to bma2 cpc accounted for more than 50 of the relative weight across the landmasses and contributed approximately three quarters of the average weight score for the mce and mci with the highest weight appearing in tp in addition cpc in mcg exhibited a slightly higher weight skill than gsmap rt thus there was a decreasing relationship among gsmap rt imerg e and era5 in terms of the independent weights for the bma3 cpc had the highest weight score among the components although it attenuated the ratio of the relative weights in most regions relative to bma2 era5 had a lower weight than imerg e in the mcei regarding the bma4 the individual weights of mceig displayed similar spatial patterns to those of mceg and mcig the average weights of cpc era5 imerg e and gsmap rt were 44 7 9 6 8 5 and 37 2 respectively implying that cpc still ranked first gsmap rt had a small deviation based on the previous analysis and thus received medium weight skills it was noteworthy that the relationship between era5 and imerg e was different from that between mcei and mceig as a result of increasing the gsmap rt into the bma scheme generally these notable discrepancies in the relative weights also highlight the necessity of carefully considering different input precipitation datasets for use in multisource data fusion the optimal weights reflect the internal impacts of ensemble members on bma predictions under different input groups 4 5 contributions of individual precipitation datasets to the bma based fusion scheme revealing the impacts of ensemble members on different blended precipitation products can distinguish important information of individual datasets that better serve as input in precipitation fusion methods the internal contributions of the original dataset to each bma merged precipitation product are shown in fig 8 and table 5 to further quantify the effect of different input numbers on the bma schemes performance comparisons of blended precipitation products were designed for the comprehensive kge metric which is shown in table 2 four added bma merged precipitation products were developed to reflect the role of the cpc dataset these were the mei merged era5 and imerg e meg merged era5 and gsmap rt mig merged imerg e and gsmap rt and meig product merged era5 imerg e and gsmap rt and the corresponding average kge values were 0 483 0 521 0 517 and 0 532 respectively fig 9 shows the quantitative estimation of the impact of the four original precipitation datasets on the bma model the input number exerted a considerable influence on the bma based precipitation estimates the cpc precipitation dataset was the dominant factor which strengthened more than 14 of the percentage variation in the reliability of the blended precipitation products the highest contributions of increased cpc were 26 3 for the single factor and 25 7 for the joint utility factors this is because cpc had a high accuracy as detailed in section 4 1 among the reanalysis and satellite precipitation datasets the increased imerg e slightly enhanced the reliability from mcg to mcig and from mceg to mceig although the percentage variations were low in contrast the increased input of era5 and or gsmap rt could hinder the performance improvement of the fusion model which would be indicated by a negative rate below 0 such as the percentage of 2 9 obtained from mci to mcei and the ratio of 0 49 from the mcei to mceig these results illustrate that increased inputs may not necessarily improve the accuracy of the bma merged precipitation products 5 discussion 5 1 rationality analysis under the background of multisource precipitation data it is necessary to optimize the advantages of different precipitation datasets to improve the spatiotemporal representation of rainfall by merging algorithms before these processes the impact of various precipitation datasets on the fusion method should be identified to understand the error sources of the fusion model in this study a modifiable framework was established based on the bma model to integrate different input precipitation datasets and the changed and initial performance comparison approach was used the bma framework was trained and verified using the original precipitation datasets and cma observations in the training 2001 2010 and testing 2011 2015 periods this is same with the literature yumnam et al 2022 and similar to the research period division of hydrological models for parameter calibration and model validation the ensemble based bma algorithm has variability and can mechanically apply bayes theorem and the conditional probability definition of two variables at multiple times thus the bma algorithm is applicable to the case of more than two variables and has been widely used in previous studies jiang et al 2012 ma et al 2018 more importantly the ensemble method can synthesize multisource information and reduce estimation errors against ground observations by optimizing weights yin et al 2021 generally a better performing member contributes a higher relative weight to the bma predictions duan et al 2007 ma et al 2018 this implies that the relative weight reflects the intrinsic effects of the input precipitation datasets on the bma schemes to a certain extent the changed initial comparison method was implemented based on the control variable method and was used only to analyze the impact of the input number on the bma fusion scheme in this case mcei to mceig it is difficult to separate the effects of increased or decreased members on the accuracy of the bma merged precipitation products xiong et al 2021 thus the comparative approach may be a good choice in summary the case outcomes of this study over mainland china demonstrated that the proposed modifiable framework and comparison method were effective tools for the impact assessment of input sources on precipitation fusion and provided valuable information on the improvement of precipitation estimates 5 2 influence of different inputs on bma fusion seven blended daily precipitation products were generated using the different input sources in the modifiable bma algorithm and their performance was systematically validated and compared in sections 4 1 4 2 and 4 3 the bma merged precipitation products substantially improved the performance of the reanalysis and near real time satellite precipitation retrievals interestingly some of the blended precipitation products had better reliability relative to that of mswep and cpc in terms of single or multiple statistical metrics in three aspects the daily statistics seasonal analysis and typhoon related rainfall assessment moreover there were apparent discrepancies in the three aspects among all the blended precipitation products owing to different inputs including different groups and various numbers the different groups had a significant and direct impact on the ability of bma predictions as shown in figs 4 and 5 the reliability of mce mci and mcg was significantly different and closely associated with the spatial patterns of performances of individual members over mainland china however this connection was not easily observed in the mceig because it balanced the strength of more individual members to further explore the influencing factors the variations in the optimal weights internal factors are presented in fig 8 for each blended precipitation product the relative weight of each member varied in the different groups however cpc was the major contributor with the highest proportion of weight and the main intrinsic influencing factor in the blended precipitation product followed by gsmap rt imerg e and era5 this is consistent with the order of their biases table 2 in addition the original precipitation datasets had clear capability differences thus different groups could guide the bma prediction to achieve a different performance the various number also affected the performance of the blended precipitation products more input sources reduced the spatial heterogeneity of the bma prediction fig 4 whereas the accuracy was not improved in some regions table 3 and fig 9 in the comparison the increased input cpc dataset significantly promoted the persistent improvement of the performance of the initial bma scheme and yielded a large percentage above 14 the kge metric ascended from a value of 0 483 for extra mei to 0 61 for mcei although its proportion was small increasing the imerg e dataset into the bma scheme also promoted performance these suggested that the cpc and imerg e generally presented some positive effects on the blended precipitation products thus the two near real time products particularly for the cpc dataset should be considered in the precipitation fusion in contrast the changed bma scheme inputted by the increased era5 and or gsmap rt datasets did not show better performances than the initial blended precipitation product this implies that increasing the input of the bma model can cause a relatively lower accuracy than that of the initial scheme in general selecting input groups and numbers of algorithms require careful consideration when used in the development of ensemble based precipitation products in the future it can select appropriate group and number precipitation products for data fusion at different climatic regions so that a best final precipitation product with high accuracy can be produced on the other hand high quality hourly precipitation products can be then developed based on the final precipitation product and linear scaling method these studies will further improve the application accuracy of merged precipitation product in flood forecasting 5 3 limitations of this study considering the impacts of different inputs the modifiable bma framework in this study is proposed for blending four daily precipitation datasets the results demonstrated that the developed framework including the changed initial comparison method could be successfully applied to analyze the performance differences of various blended precipitation products however some uncertainties and limitations remain and should be noted first the nearest grid to point evaluation used in this study could not adequately represent the details in some areas without meteorological stations or with sparse networks especially in the xj and tp regions with varied topography and complex climate the results of the evaluation approach can be controlled by a network of meteorological stations tang et al 2018 despite that some studies have proven that the evaluation results of the grid to point approach were generally consistent with those of the grid to grid analysis jiang et al 2021a lei et al 2022 second there were some overlapping stations between the cma stations against the cpc unified gauge analysis and the mswep product corrected by the daily gauge and monthly gpcc dataset which can introduce uncertainties into our evaluation results previous studies have evaluated cpc and gpcc adjusted satellite precipitation products using 778 meteorological stations and proved that the evaluation results were credible over mainland china zhou et al 2020 in addition the evaluation results of mswep in this study are generally consistent with those of previous studies at the point scale including independent and dependent meteorological stations as shown in table 6 third the comparison method had a certain available range for quantifying the effects of individual members on bma based schemes for example this method is not applicable for discerning the impacts of ensemble members on the change from merged mce to mcig moreover this study only utilized the comprehensive kge metric in the calculation of this method which does not denote the rainfall detection performance fourth the bma framework could not well remove the false alarmed events from different sources although the cpc gauge based dataset urges the blended precipitation product to obtain a better false alarmed ratio in capturing the rainfall or typhoon related precipitation events as shown in table 2 and table 4 a potentially improved bma framework that classifies the rainfall and snowfall lyu et al 2020 or rain and no rain events dong et al 2022 can be developed to further reduce the false alarmed ratio or enhance the detection of precipitation finally the bma framework should be further compared by using triple collocation tc xu et al 2020 machine learning ml baez villanueva et al 2020 and other approaches to expand the applicability of our schemes xiong et al 2021 although the bma method is more robust than the traditional ensemble based simple model averaging sma and one outlier removed oor yin et al 2021 yumnam et al 2022 regarding the manufacturing process of merged precipitation products a grid based bma dataset can be generated by interpolating and normalizing the optimal weights of ensemble members for each pixel cell ma et al 2018 this study did not consider the interpolated grid weights over the non station areas because the different interpolation techniques could have introduced some uncertainties into the analysis pradhan et al 2022 in addition bias adjustment is an important link and should be considered into the modifiable bma framework for greater improvements in future research such as the use of dynamic more stages and quantile based bma models yin et al 2021 yumnam et al 2022 6 conclusions this study established a modifiable bma framework to merge four precipitation datasets gauge based cpc reanalysis model era5 multi satellite imerg e and gsmap rt by considering different inputs over mainland china this framework was trained on a daily scale from 2001 to 2010 and validated for the period 2011 2015 the bma predictions mce mci mcg mcei mceg mcig and mceig were systematically evaluated and compared with the original precipitation members and the representative ensemble based precipitation product mswep subsequently the effects of different inputs on the bma predictions were quantitatively separated using the optimal weight and changed initial comparison method our major findings are summarized as follows 1 of the four members cpc exhibited the highest reliability in the daily statistics and seasonal analysis but had a poor detection performance during typhoon rammasun in 2014 where it underestimated rainfall volumes era5 imerg e and gsmap rt had substantial errors but better detection ability in extreme rainfall events 2 after the bma fusion the blended precipitation products showed significant improvements relative to the reanalysis and satellite precipitation datasets and some e g mceg performed better than cpc in terms of individual metrics evidently during the typhoon more importantly some bma merged precipitation products e g mci and mcg better reflected the spatial and temporal characteristics of rainfall patterns compared to mswep particularly in the sw and eastern tp regions and during the period of typhoon rammasun 3 the bma predictions had evident performance differences because of the different input sources the various groups of inputs showed significant impacts on the bma schemes which determined whether or not the blended precipitation product was better than mswep increasing the input variables might not continuously enhance the performance of blended precipitation products although it can reduce spatial heterogeneity 4 the bma optimal weights of the ensemble members exhibited clear variations for different groups in these cases cpc had the highest weight and was the main influencing factor by comparison the increased input cpc dataset evidently facilitated the performance improvement more than 14 of the fusion model over the initial bma scheme in terms of the kge metric similarly increasing the imerg e dataset showed a slight improvement whereas the opposite held true for era5 and gsmap rt overall the developed modifiable bma framework is viable over mainland china and enables the full consideration of the selection of different input sources to merge multiple precipitation datasets credit authorship contribution statement linyong wei conceptualization methodology software writing original draft shanhu jiang conceptualization project administration jianzhi dong writing review editing liliang ren funding acquisition yi liu methodology linqi zhang methodology menghao wang data curation zheng duan writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was financially supported by the national natural science foundation of china u2243203 51979069 the key science and technology program of the ministry of water resources skr 2022034 the national natural science foundation of jiangsu province china bk20211202 postgraduate research practice innovation program of jiangsu province kycx22 0635 
2237,selection of the number and which of multisource precipitation datasets is crucially important for precipitation fusion considering the effects of different inputs this study proposes a new framework based on the bayesian model averaging bma algorithm to integrate precipitation information from gauge based analysis cpc reanalysis derived dataset era5 and satellite retrieval products imerg e and gsmap rt the bma weights were optimized for the period 2001 2010 using daily measurements and then applied to the period 2011 2015 for model validation seven bma merged precipitation products i e mce mci mcg mcei mceg mcig and mceig were thoroughly evaluated across mainland china and then compared against the state of the art ensemble based product mswep the results indicate that the bma predictions performed substantially better than the reanalysis and satellite precipitation datasets in both daily statistics and seasonal analyses mce mci and mceg demonstrated better performances relative to cpc in terms of individual metrics moreover mci mcg and mcei generally outperformed mswep over the entire study area particularly in local regions such as southwestern china and the eastern tibetan plateau during typhoon rammasun in 2014 mcg and mceg provided greater detail for heavy rainfall events than the four ensemble members and the mswep product thus the performance of the bma predictions exhibited evident differences because of various input sources cpc was the major internal influencing factor with the highest weight score meanwhile the increased input cpc dataset into the bma based schemes exerted a significant influence on the precipitation estimates which markedly facilitated the performance improvement of the fusion model and its improved degree greater than 14 was obtained using a changed initial comparison method our results demonstrate that the developed modifiable bma framework is useful for analyzing the impacts of ensemble members on bma predictions and suggests that it is considerate in the use of different input sources for generating ensemble based precipitation products keywords precipitation precipitation fusion bayesian model averaging ensemble based precipitation product performance improvement mainland china 1 introduction precipitation is an essential component in the water cycle therefore high quality precipitation estimations are imperative for the assessment monitoring and prevention of natural disasters in hydrological meteorological and agricultural fields duan and bastiaanssen 2013 yang et al 2020 yin et al 2021 conventional ground stations are the most reliable way to document precipitation records at specific locations however these ground networks are usually unevenly and sparsely distributed in many developing countries particularly over complex topography lanza and stagi 2008 baez villanueva et al 2020 therefore it is challenging to obtain accurate spatiotemporal information of precipitation patterns using only in situ measurements fortunately grid based gauges reanalysis models and satellite sensors provide viable alternatives that can provide open access and near global coverage precipitation estimates at fine spatial and temporal resolutions currently there are three categories of global gridded precipitation datasets 1 gauge based precipitation products that are constructed from globally available rain stations using different interpolation techniques the accuracy of the datasets is strongly dependent on the density and number of rain stations chen et al 2008 shen and xiong 2016 schneider et al 2016 these widely used gauge based precipitation datasets comprise climate prediction center cpc global unified gauge based analysis of daily precipitation xie et al 2007 2 reanalysis model based precipitation products that have been developed based on various sources using data assimilation algorithms and state of the art numerical analysis and forecasting climate systems dee et al 2011 jiang et al 2021a which provide an array of vertical atmospheric information containing precipitation variable the fifth generation of the european centre for medium range weather forecasts ecmwf atmospheric reanalysis i e era5 is a representative multi year global reanalysis dataset graham et al 2019 3 satellite precipitation estimates that are produced with intermittent advances in sensor resolutions and retrieval algorithms that have great opportunities to observe continuous precipitation patterns particularly over areas with inadequate stations huffman et al 2007 hou et al 2014 presently the mainstream satellite precipitation products include the following integrated multi satellite retrievals for global precipitation measurement imerg huffman et al 2020 and global satellite mapping of precipitation gsmap ushio et al 2009 these gauge reanalysis and satellite based precipitation products significantly facilitate precipitation mapping for a broad range of applications however these products lack coherence relative to ground observations with varying accuracies in different areas duan et al 2016 jiang et al 2018 beck et al 2019a satgé et al 2020 tang et al 2020 amjad et al 2020 wei et al 2021 therefore it is suggested to improve the accuracy of precipitation estimates by taking the best of the multiple sources in recent years many efforts have been made to utilize or propose different methods to merge various gauge reanalysis and satellite precipitation datasets to obtain better precipitation estimates beck et al 2017 beck et al 2019b xu et al 2020 baez villanueva et al 2020 wu et al 2020 for example yumnam et al 2022 considered three satellite precipitation datasets to run a quantile based bayesian model averaging bma algorithm during the monsoon season the validation results verified that bias corrected quantile based bma products outperformed the imerg product over the india s vamsadhara river basin in terms of the average error index ma et al 2018 established an ensemble based dynamic bma framework to integrate four satellite precipitation products by optimizing the relative product weights the evaluation results validated that the dynamic bma merged precipitation product reduced the estimation error and showed better performance than the individual members over the tibetan plateau in terms of some metrics yin et al 2021 established a three stage bma approach to combine five precipitation datasets by incorporating gauge reanalysis and satellite products over mainland china statistical metrics also indicated that the three stage bma method performed better than individual members these studies primarily focused on the development of fusion algorithms which are of immense importance for use in precipitation integration schemes the performance of any precipitation product is primarily affected by the algorithm and input sources chen et al 2019 beck et al 2019a chen et al 2020 satgé et al 2020 tang et al 2020 baez villanueva et al 2020 it is equally important to blend precipitation products to analyze the influences of selecting the number and type of multisource precipitation information as model inputs before the fusion procedure chen et al 2019 had proven that the performance of gsmap product has connections to the error characteristics of the input sources from several sensors in the data fusion the input sources play a pivotal role for the development of blended precipitation estimates however previous studies have rarely investigated the influences of different model inputs on the precipitation fusion scheme in this study the bma algorithm as the core of a new framework is proposed to investigate and simultaneously compare the performance differences of blended precipitation products under different input sources and to quantitatively analyze the influences of ensemble members on the bma based fusion schemes the design experiment was performed on four original daily precipitation datasets comprising unified gauge based analysis cpc atmospheric reanalysis model data era5 and multisource satellite retrieved products imerg and gsmap from 2001 to 2015 across mainland china subsequently the bma merged precipitation products were synthetically assessed using an array of evaluation metrics and compared to the publicly available ensemble based precipitation product mswep the effects of input members on the bma predictions were quantified by using the optimal weights and a changed initial comparison method the results of this study will provide a new perspective of error sources for merging multisource precipitation information into a fusion algorithm 2 study area and data 2 1 study area the study area was mainland china situated in eastern asia and on the western bank of the pacific ocean falling in the spatial field between longitudes of 72 136 e and latitudes of 18 54 n the area has monsoon climate characteristics and complex topography which includes low altitude plains and hills in the east sichuan basin and loess plateaus in the central and high altitude mountains and plateaus in the west thus the precipitation of the region exhibits a significantly uneven spatiotemporal distribution increasing gradually from the northwest inland to the southeast coast with high frequencies and levels in summer the chinese mainland is commonly divided into eight subregions according to the average annual precipitation distribution elevation and mountain chain information chen et al 2013 these subregion divisions consist of northeastern china ne northern china nc the middle and lower reaches of the yangtze river cj southeastern china se southwestern china sw northwestern china nw xinjiang xj and tibetan plateau tp as shown in fig 1 a in this study these subregions were adopted to analyze the spatial heterogeneity of the blended precipitation products and further compare their regional evaluation statistics 2 2 ground observations the ground observed daily precipitation dataset for the period 2001 2015 was collected based on 824 meteorological stations from the china meteorological administration cma the quality i e extreme check and consistency test of all data is strictly controlled by a series of criteria before availability on the website https data cma cn shen et al 2010 shen and xiong 2016 moreover to ensure the integrity of the precipitation data and maintain consistency with satellite precipitation products utc day definition 807 effective meteorological stations were handled under certain rules wei et al 2022 and selected as the reference dataset in this study there is a ground network of meteorological stations that cover mainland china as shown in fig 1b with a dense group in eastern china and sparse distribution over the western region particularly in the western tp 2 3 gauge reanalysis and satellite based precipitation products cpc is a unified gauge based daily precipitation product from 1979 to present developed by the national oceanic and atmospheric administration climate prediction center using the optimal interpolation objective analysis technique and more than 30 000 gauge records which includes approximately 17 000 near real time gauges xie et al 2007 more specific information on this product can be found in previous literature xie et al 2007 chen et al 2008 the near real time cpc product mostly covers all latitudes and longitudes of land with a spatial resolution of 0 5 since the release of the gauge based cpc daily precipitation data on the website https psl noaa gov data gridded data cpc globalprecip html it has been employed to correct the bias of satellite precipitation products i e cmorph and gsmap and blend multisource precipitation datasets i e mswep to obtain higher precision precipitation tian et al 2010 mega et al 2014 beck et al 2017 therefore the cpc global daily precipitation product from 2001 to 2015 was chosen for this study for multiple precipitation data fusion era5 was produced by the ecmwf as the successor to the era interim to offer a new generation of global atmospheric reanalysis products era5 provides much higher quality estimations of atmospheric ocean wave and land surface parameters with a finer 0 25 1 h spatiotemporal resolution than the era interim 0 75 6 h and has a similar model and input to the era interim dee et al 2011 xu et al 2022 this contributes major improvements to the atmospheric model and data assimilation technology over the era interim this study used the total precipitation parameter of the era5 reanalysis precipitation product for the period 2001 2015 the native 0 25 resolution and 1 hourly precipitation product era5 was downloaded from the climate data store website https cds climate copernicus eu search text era5 type dataset and converted to a daily precipitation dataset to guarantee temporal consistency with satellite precipitation estimates the national aeronautics and space administration and japan aerospace exploration agency jointly established the global precipitation measurement gpm and generated representative open access satellite precipitation products namely the imerg and gsmap series the gpm imerg products inter calibrated merged and interpolated all available data from the gpm constellation satellite estimates gauges and other precipitation estimators based on the imerg algorithm huffman et al 2019 imerg provides global precipitation estimates with a relatively superior spatiotemporal resolution of 0 1 30 min and spatial coverage of full 60 s n latitudes the imerg suite incorporates the near real time early run satellite hereafter referred to as imerg e and late run multi satellite and post real time final run satellite gauge precipitation estimates imerg e is available approximately 4 h after retrieval using only forward morphing and is designed for near real time applications in this study the daily precipitation product imerg e from 2001 to 2015 with a daily record from 0 00 to 24 00 utc was utilized and downloaded from the website https disc gsfc nasa gov the gpm gsmap is a high spatiotemporal resolution 0 1 1h global precipitation product with a latitude range of 60 s n developed by employing a microwave infrared combined algorithm including the backward forward morphing and kalman filtering techniques to integrate the use of passive microwave estimates and infrared ir satellite retrievals ushio et al 2009 similar to the imerg products the multi satellite gsmap suite also contains three types of products near real time gsmap microwave ir reanalysis gsmap mvk and gauge calibrated standard gsmap gauge the near real time gsmap utilizes available microwave imagers and sounders based on the gsmap mvk algorithm for near real time precipitation estimates chen et al 2020 which comprises the gsmap nrt and calibrated gsmap gauge nrt rainfall data hereafter referred to as gsmap rt duo to the significantly poor performance of gsmap nrt relative to imerg e zhou et al 2020 chen et al 2020 the adjusted near real time daily precipitation product gsmap rt for the study period 2001 2015 was selected in this study and obtained online https sharaku eorc jaxa jp 2 4 mswep mswep version 2 8 was generated by hylke beck taking advantage of the global gauge based datasets atmospheric reanalysis data and satellite remote sensing retrievals based on the optimal integration procedure to obtain accurate precipitation estimates beck et al 2017 beck et al 2019b these datasets comprise state of the art gpcc ghcn d gsod worldclim era interim jra 55 gsmap tmpa 3b42rt cmorph gridsat and other datasets beck et al 2019a the fundamental fusion program involved in the development of mswep products can be found in existing literature beck et al 2019b although the mswep version will be updated intermittently to improve product performance the mswep product provides a truly global coverage historic precipitation product with a fine spatial resolution of 0 1 and a high temporal resolution of 3 h beginning from 1979 the systematic bias of terrestrial precipitation data from mswep product was adjusted by using more than 14 000 river discharge data across the globe and incorporation of daily precipitation from approximately 77 000 stations worldwide yang et al 2020 owing to the superior performance of mswep in comparison with other precipitation datasets in some areas mswep product is widely employed or evaluated in extreme precipitation drought monitoring and runoff simulations therefore the daily precipitation product mswep for the period 2001 2015 was regarded as the benchmark in this study to reflect the effect of data fusion behaviors and was downloaded from the network https www gloh2o org mswep 3 methodology 3 1 framework the overall structure of the modifiable bma based framework for multisource precipitation product fusion is shown in fig 2 there were five main steps 1 preprocessing the cpc era5 imerg e and gsmap rt precipitation datasets in terms of the nearest grid to point method to ensure spatial correspondence with the cma stations 2 separating the study period into the training period 2001 2010 and the validation period 2011 2015 3 constructing the diverse precipitation datasets for the different input sources taking the cpc data as the core of bma fusion 4 determining the optimal bma weight for each original precipitation member under various scenarios and 5 blending multisource precipitation datasets to generate the bma merged precipitation products under the different inputs for the period 2011 2015 and the spatiotemporal resolution of point 1 d across mainland china the bma scheme that was run using two original precipitation datasets were abbreviated as bma2 similarly three and four original datasets were used to drive the bma method denoted as bma3 and bma4 respectively seven blended precipitation product series were developed by using the bma technique these were mce merged cpc and era5 mci merged cpc and imerg e mcg merged cpc and gsmap rt mcei merged cpc era5 and imerg e mceg merged cpc era5 and gsmap rt mcig merged cpc imerg e and gsmap rt and mceig merged cpc era5 imerg e and gsmap rt 3 2 bayesian model averaging bma bma a statistical ensemble model based on the bayesian theory integrates inferences and predictions to obtain a more robust posterior probability using the prior probability of ensemble members and likelihood function duan et al 2007 in this study the bma approach was implemented to merge multisource precipitation datasets by assigning weights based on probabilistic skills to obtain a good fit to the in situ observations the procedure for estimating the optimal bma weights is described below in the training period according to the total probability formula the posterior probability density function pdf of the bma merged precipitation product is expressed as 1 p y g s 1 h p f s g p s y f s g where y and g are the blended precipitation product and cma observed precipitation data respectively h denotes the number of gauge and or reanalysis and or satellite based precipitation datasets inputted into the bma fusion p f s g represents the posterior probability of the inputted precipitation dataset f s and identified as the likelihood of ensemble members the term p s y f s g denotes the posterior distribution of y derived based on the input precipitation estimates f s and ground observations the posterior probability of each member is computed corresponding to the same in situ observations thus making the sum of the weights equal to one if the weight w s represents p f s g and s 1 s p f s g s 1 h w s 1 then eq 1 can be updated by substituting the posterior probability p f s g with the weight w s and is then given by 2 p y g s 1 h w s p s y f s g moreover the posterior mean e y g and variance v a r y g of the bma merged precipitation product can be approximated as 3 e y g s 1 h w s f s 4 v a r y g s 1 h w s f s e y g 2 s 1 h w s σ s 2 where σ s 2 is the variability of the input precipitation dataset f s for the observed g dataset usually the prior pdf of precipitation does not conform to the gaussian assumption with a normal distribution the original precipitation datasets and cma data were preprocessed using the box cox transformation before performing the bma algorithm raftery et al 2005 the box cox function from the matlab software was employed to achieve data transformation ma et al 2018 this can be used to obtain the approximate gaussian distribution of conditional probability p s y f s g the weight w s of the input precipitation datasets are efficiently estimated based on the maximum log likelihood function which is expressed as 5 l w s σ s log s 1 h w s p s y f s g log s 1 h w s g y f s σ s 2 where g represents the gaussian distribution for the analytical solution to obtain the optimal bma parameters the expectation maximization e m algorithm was used by an iterative way to maximize the log likelihood equation raftery et al 2005 jiang et al 2012 during the iteration the prior bma weight of per individual member was set to 1 h the e m algorithm includes two steps an e or expectation step and an m or maximization step a more detailed formulation of the e m algorithm is provided in previous literature mclachlan and krishnan 2007 bma probabilistic prediction can reflect the better reliability of input precipitation datasets against ground observations with a larger weight ma et al 2018 3 3 evaluation metrics the performances of the original bma merged and mswep precipitation products against the cma ground observations were verified and compared using the following statistical metrics which incorporated the continuous and detection indices 1 the correlation coefficient cc relative bias rb root mean square error rmse and the modified kling gupta efficiency kge gupta et al 2009 are continuous metrics used to measure the linear correlation systematic bias total error level and comprehensive accuracy of the evaluated precipitation products jiang et al 2021b tang et al 2020 kge balances the contributions of the cc bias and variability components high kge and cc and low absolute rb and rmse values indicate better performance 2 the probability of detection pod false alarm ratio far and critical success index csi are detection metrics that reflect the correct detection false detection and overall detection capabilities of the evaluated precipitation products to identify rainfall events jiang et al 2021a the csi is expressed as an integrated function of pod and far the calculations of pod far and csi require a threshold which was set as 1 mm day to determine rain no rain events a large pod and csi and small far imply a more satisfactory prediction ability some information of the aforementioned statistical metrics is listed in table 1 3 4 quantifying the impacts of different input precipitation datasets on the bma based fusion scheme the accuracy characteristics of all bma merged precipitation products can be determined based on the abovementioned evaluation metrics to quantitatively determine the relative effect or contribution denoted by a percentage change of each individual member to the bma precipitation fusion under different inputs the values of the comprehensive performance metric kge for bma3 vs bma2 bma4 vs bma3 and bma4 vs bma2 were compared by considering a control variable method and the changed initial comparison method as following equation woolmer et al 2008 wang et al 2020 6 r x change x initial x initial 100 where r represents the percentage variation caused by increasing the input precipitation datasets into the original variables group of the bma based scheme x change and x initial are the kge values of the changed and initial blended precipitation products respectively for example r is the relative effect of the increased imerg e precipitation dataset which is the increasing input from the initial mce to the changed mcei if r 0 it implies that the increased input precipitation dataset promotes the quality of the blended precipitation product if r 0 it implies that the increased input precipitation datasets degrade the quality of the blended precipitation product 4 results 4 1 general accuracy of different precipitation products at a daily scale table 2 summarizes the overall performance of the evaluated precipitation products using regional average metrics over mainland china regarding the original precipitation datasets cpc had a significantly preferable performance compared to that of the other members in terms of both continuous and detection metrics with the highest kge 0 644 cc 0 776 and csi 0 633 and the smallest rb 1 54 rmse 4 6 mm d and far 0 308 era5 imerg e and gsmap rt had substantial errors and exhibited a significantly lower performance than mswep while each dataset had its own unique advantages for example era5 had relatively high pod whereas gsmap rt had relatively good rb after the bma data fusion one blended precipitation product always performed best excluding cpc and its reliability was slightly better than that of mswep according to single metrics namely mci with kge and far mcei with cc rmse and pod mcg with rb and mce with csi moreover all blended precipitation products outperformed mswep in estimating the magnitude of rainfall in terms of the cc rmse csi and far indices the mci tended to display the overall best capability for most metrics and blended precipitation products have evident improvements in the reanalysis and satellite precipitation datasets for that all some merged products except for mcg presented better performance than cpc in terms of cc and others i e mce mci mcei mceg and mceig for the rmse and pod metrics fig 3 shows the boxplots of six statistical metrics kge cc rb csi pod and far for the evaluated precipitation products over mainland china the performance of era5 imerg e and gsmap rt was unsatisfactory with wide ranges and or poor metric values which were excluded when the median rb was close to 0 for the bias adjustment gsmap rt and median pod exceeded 0 9 for era5 in contrast the cpc had considerably high reliability for all evaluation indicators compared to mswep the blended precipitation products except for mce exhibited a more robust performance with a higher value of kge which almost corresponded to the three quantiles fig 3a it is worth noting that all blended precipitation products had significantly good correlations i e most cc values in the range of 0 7 0 9 with cma observations and outperformed mswep and cpc in capturing the temporal variability of rainfall fig 3b meanwhile the blended precipitation products slightly overestimated the rainfall amounts with most rb varying from 10 to 15 fig 3c in which mcei had positive biases and the worst performance whereas mcg performed the best when comparing the detection abilities the reliability of blended precipitation products performed relatively well and most displayed outstanding performances compared to that of mswep for example all blended precipitation products presented a higher csi at the 75th percentile fig 3d a larger pod at the median line except for mcg and mcig fig 3e and a slightly lower far at the 25th percentile excluding mcei fig 3f moreover mce and mcei outperformed cpc in terms of the pod metric in general there were some apparent differences particularly for cc and pod metrics among the seven blended precipitation products because of the various inputs in the bma data fusion which likely determined whether the blended precipitation product performed better than mswep these characteristics of the bma merged precipitation products are further analyzed in the subsequent sections fig 4 shows the spatial distribution of kge for different precipitation products over mainland china among the original precipitation datasets each had significant spatial variabilities and regional characteristics notably there were also clear differences between the precipitation datasets the cpc primarily exhibited the highest kge value over the entire continent with a strongly stable spatial pattern although it was lower in some parts of xj era5 performed better in ne nc cj and nw than in sw xj and tp however diverse results were obtained for imerg e with relatively larger kge values in cj se and sw than in xj and parts of ne nw and tp essentially gsmap rt was not the same as the former the kge variation decreased from the east to xj and tp these behaviors fig 4a b c and d are important factors that determine the performances of bma merged precipitation products under different input sources the blended precipitation products obtained from the bma scheme with various input groups at the same input number exhibited remarkable performance differences in most regions this can be directly observed from the spatial patterns of kge for the mce mci and mcg which were homologous to those of era5 imerg e and gsmap rt respectively accompanied by the cpc as the input number increased from bma2 to bma4 the reliability of the blended precipitation product tended to reach a relatively steady state and similar spatial characteristics among the mcei and mceg versus mceig this indicates that inputting more datasets can reduce the spatial heterogeneity of the bma merged precipitation product to a certain extent however increasing inputs may not continuously improve the performance of blended precipitation products which was expressed in the decreased kge in some regions such as the mce versus mceig in nw and mci versus mcig in tp compared to mswep it is evident that the different blended precipitation products displayed their respective superiority and performance closer to the cma observations over various regions this stresses the need to carefully select the original data sources for the generation of merged precipitation products fig 5 shows the spatial distribution of csi for the different precipitation products over mainland china one commonality among the precipitation products is the higher csi values concentrated in the east compared to those in the west a descending rank in the detection ability was observed clearly by cpc era5 gsmap rt and imerg e era5 gsmap rt and imerg e had relatively low csi values less than 0 4 in most stations particularly in xj indicating low reliability to detect rainfall occurrence events for the blended precipitation products that significantly heightened the performance of detecting rainfall events mce had the strongest performance with a relatively wide range csi of stations that were greater than 0 65 followed by mci the spatial patterns of csi exhibited notable differences i e in ne and nw in the rainfall capture capability it is important to note that the bma merged precipitation products outperformed mswep in some regions such as sw and the eastern tp table 3 presents the regional average results of kge and csi for the different precipitation products in each subregion the discrepancies among all precipitation products in figs 4 and 5 were quantified for various subregions and clearly distinguished to determine the most applicable products in each subregion with the exception of cpc there were two situations for the best performance as follows 1 in terms of the kge index the following was chosen mswep in ne nc nw and xj where some bma merged precipitation products were extremely close to the best reliability mcig in cj se and sw and mci in tp 2 in terms of the csi metric the following was chosen mce in ne nc cj nw and xj mcg in se and mci in sw and tp where the best performance was not mswep for the overall detection capability it is important to note that the blended precipitation products had significant improvements of different magnitudes over the era5 imerg e and gsmap rt and some performed better than cpc in cj namely mcig se except for mce and sw except for mce and mcei in terms of the kge metric and similarly mce for xj in terms of the csi metric 4 2 seasonal analysis of different precipitation products fig 6 shows the seasonal average statistical outcomes of the seven metrics i e kge cc rb rmse csi pod and far for the different precipitation products over mainland china the metrics were computed based on the seasonal datasets that were obtained at a monthly time step and divided into four seasons spring march may summer june august autumn september november and winter december february the substantial improvements and potential differences in blended precipitation products are presented by the statistical metrics in all seasons specifically the bma method effectively improved rainfall estimations from era5 which initially had significant overestimations imerg e which previously had a poor csi and gsmap rt which initially had the worse kge during the four seasons where era5 imerg e and gsmap rt presented low precision large mean error level and low comprehensive detection ability products had relatively high usability in summer and autumn than in spring and winter especially where the kge metric was less than 0 notably in terms of most metrics mce mci and mcei performed better than mswep in spring especially for the cc csi and far indicators fig 6a in summer compared to the mswep all blended precipitation products exhibited a better capability in retrieving rainfall estimates in terms of the cc rmse csi and pod metrics whereas the opposite was true for the rb statistics fig 6b some similar behaviors were also observed in autumn for the cc and rmse indicators although the accuracy of each blended precipitation product showed some changes because of the reduction in rainfall from summer to autumn as indicated by the rb and rmse indices fig 6c in winter the mswep significantly overestimated the rainfall amount with more positive rb values above 80 thus incorrectly coinciding with rainfall occurrence events with far values exceeding 0 35 mswep also achieved a poor performance indicated by a lower kge compared to that of the blended precipitation products fig 6d the lower suitability of mswep also occurred relative to the individual blended precipitation products in terms of other metrics such as mcei s cc and mce s rmse the above results demonstrate that different blended precipitation products have seasonal variabilities and differences in rainfall estimations owing to the impacts of various input sources for the bma approach 4 3 extreme precipitation assessment typhoons rainstorm events and floods are destructive natural disasters that can cause tremendous economic losses this study investigated the performance of different precipitation products during typhoons and examined whether they can provide high quality rainfall information for typhoon associated extreme rainfall events the ninth typhoon in 2014 super typhoon rammasun was selected in this study for the extreme precipitation assessment typhoon rammasun occurred in the hainan and guangdong provinces on july 18 2014 with a maximum wind speed above 60 m s a threshold of 25 mm day was set to determine the heavy rainfall event for the detection capability fig 7 shows the spatial distribution maps of the daily rainfall amount of the cma data versus the different precipitation products over se during typhoon rammasun the corresponding results for the evaluation metrics are listed in table 4 heavy rainfall was primarily concentrated in the southwest of se and decreased to the northeast of se generally all precipitation products agreed well with the spatial pattern of the cma observations with a cc value greater than 0 83 but overestimated light rainfall and underestimated the typhoon rainfall volume at the center which is indicated by the spatial maps and negative rb metrics particularly for era5 rb of 25 2 and mswep rb of 20 1 of these products some bma merged precipitation products such as mcg and mceg provided much more detail compared to the original precipitation datasets and the mswep product for example cpc exhibited the worse performance in detecting heavy rainfall with a comparatively lower csi of 0 542 and obviously underestimated rainfall volumes which was indicated by an rb of 15 1 the kge of era5 was the smallest with the largest error level of 35 5 mm day compared to that of the other products evidently mswep had a poor comprehensive capability with a low kge of 0 693 and csi of 0 567 from these aspects the multisource data fusion schemes have shown to increase the reliability of the estimations of heavy rainfall events during the typhoon in addition owing to the different input sources of the bma model there were apparent differences among the blended precipitation products for capturing both the spatial patterns and volumes of rainstorm events such as the best mcg and the worst mce 4 4 weight variations of bma ensemble members fig 8 shows the spatial distribution of the relative optimal weights for the ensemble members under various scenarios over mainland china and table 5 presents the corresponding average weights the weights of the four original precipitation datasets with certain spatial heterogeneity were significantly influenced by the different inputs the relative proportions of the weights of the original datasets were almost unchanged that cpc always occupies the highest proportion whereas the optimal weights of the same ensemble member varied substantially under different data sources with respect to bma2 cpc accounted for more than 50 of the relative weight across the landmasses and contributed approximately three quarters of the average weight score for the mce and mci with the highest weight appearing in tp in addition cpc in mcg exhibited a slightly higher weight skill than gsmap rt thus there was a decreasing relationship among gsmap rt imerg e and era5 in terms of the independent weights for the bma3 cpc had the highest weight score among the components although it attenuated the ratio of the relative weights in most regions relative to bma2 era5 had a lower weight than imerg e in the mcei regarding the bma4 the individual weights of mceig displayed similar spatial patterns to those of mceg and mcig the average weights of cpc era5 imerg e and gsmap rt were 44 7 9 6 8 5 and 37 2 respectively implying that cpc still ranked first gsmap rt had a small deviation based on the previous analysis and thus received medium weight skills it was noteworthy that the relationship between era5 and imerg e was different from that between mcei and mceig as a result of increasing the gsmap rt into the bma scheme generally these notable discrepancies in the relative weights also highlight the necessity of carefully considering different input precipitation datasets for use in multisource data fusion the optimal weights reflect the internal impacts of ensemble members on bma predictions under different input groups 4 5 contributions of individual precipitation datasets to the bma based fusion scheme revealing the impacts of ensemble members on different blended precipitation products can distinguish important information of individual datasets that better serve as input in precipitation fusion methods the internal contributions of the original dataset to each bma merged precipitation product are shown in fig 8 and table 5 to further quantify the effect of different input numbers on the bma schemes performance comparisons of blended precipitation products were designed for the comprehensive kge metric which is shown in table 2 four added bma merged precipitation products were developed to reflect the role of the cpc dataset these were the mei merged era5 and imerg e meg merged era5 and gsmap rt mig merged imerg e and gsmap rt and meig product merged era5 imerg e and gsmap rt and the corresponding average kge values were 0 483 0 521 0 517 and 0 532 respectively fig 9 shows the quantitative estimation of the impact of the four original precipitation datasets on the bma model the input number exerted a considerable influence on the bma based precipitation estimates the cpc precipitation dataset was the dominant factor which strengthened more than 14 of the percentage variation in the reliability of the blended precipitation products the highest contributions of increased cpc were 26 3 for the single factor and 25 7 for the joint utility factors this is because cpc had a high accuracy as detailed in section 4 1 among the reanalysis and satellite precipitation datasets the increased imerg e slightly enhanced the reliability from mcg to mcig and from mceg to mceig although the percentage variations were low in contrast the increased input of era5 and or gsmap rt could hinder the performance improvement of the fusion model which would be indicated by a negative rate below 0 such as the percentage of 2 9 obtained from mci to mcei and the ratio of 0 49 from the mcei to mceig these results illustrate that increased inputs may not necessarily improve the accuracy of the bma merged precipitation products 5 discussion 5 1 rationality analysis under the background of multisource precipitation data it is necessary to optimize the advantages of different precipitation datasets to improve the spatiotemporal representation of rainfall by merging algorithms before these processes the impact of various precipitation datasets on the fusion method should be identified to understand the error sources of the fusion model in this study a modifiable framework was established based on the bma model to integrate different input precipitation datasets and the changed and initial performance comparison approach was used the bma framework was trained and verified using the original precipitation datasets and cma observations in the training 2001 2010 and testing 2011 2015 periods this is same with the literature yumnam et al 2022 and similar to the research period division of hydrological models for parameter calibration and model validation the ensemble based bma algorithm has variability and can mechanically apply bayes theorem and the conditional probability definition of two variables at multiple times thus the bma algorithm is applicable to the case of more than two variables and has been widely used in previous studies jiang et al 2012 ma et al 2018 more importantly the ensemble method can synthesize multisource information and reduce estimation errors against ground observations by optimizing weights yin et al 2021 generally a better performing member contributes a higher relative weight to the bma predictions duan et al 2007 ma et al 2018 this implies that the relative weight reflects the intrinsic effects of the input precipitation datasets on the bma schemes to a certain extent the changed initial comparison method was implemented based on the control variable method and was used only to analyze the impact of the input number on the bma fusion scheme in this case mcei to mceig it is difficult to separate the effects of increased or decreased members on the accuracy of the bma merged precipitation products xiong et al 2021 thus the comparative approach may be a good choice in summary the case outcomes of this study over mainland china demonstrated that the proposed modifiable framework and comparison method were effective tools for the impact assessment of input sources on precipitation fusion and provided valuable information on the improvement of precipitation estimates 5 2 influence of different inputs on bma fusion seven blended daily precipitation products were generated using the different input sources in the modifiable bma algorithm and their performance was systematically validated and compared in sections 4 1 4 2 and 4 3 the bma merged precipitation products substantially improved the performance of the reanalysis and near real time satellite precipitation retrievals interestingly some of the blended precipitation products had better reliability relative to that of mswep and cpc in terms of single or multiple statistical metrics in three aspects the daily statistics seasonal analysis and typhoon related rainfall assessment moreover there were apparent discrepancies in the three aspects among all the blended precipitation products owing to different inputs including different groups and various numbers the different groups had a significant and direct impact on the ability of bma predictions as shown in figs 4 and 5 the reliability of mce mci and mcg was significantly different and closely associated with the spatial patterns of performances of individual members over mainland china however this connection was not easily observed in the mceig because it balanced the strength of more individual members to further explore the influencing factors the variations in the optimal weights internal factors are presented in fig 8 for each blended precipitation product the relative weight of each member varied in the different groups however cpc was the major contributor with the highest proportion of weight and the main intrinsic influencing factor in the blended precipitation product followed by gsmap rt imerg e and era5 this is consistent with the order of their biases table 2 in addition the original precipitation datasets had clear capability differences thus different groups could guide the bma prediction to achieve a different performance the various number also affected the performance of the blended precipitation products more input sources reduced the spatial heterogeneity of the bma prediction fig 4 whereas the accuracy was not improved in some regions table 3 and fig 9 in the comparison the increased input cpc dataset significantly promoted the persistent improvement of the performance of the initial bma scheme and yielded a large percentage above 14 the kge metric ascended from a value of 0 483 for extra mei to 0 61 for mcei although its proportion was small increasing the imerg e dataset into the bma scheme also promoted performance these suggested that the cpc and imerg e generally presented some positive effects on the blended precipitation products thus the two near real time products particularly for the cpc dataset should be considered in the precipitation fusion in contrast the changed bma scheme inputted by the increased era5 and or gsmap rt datasets did not show better performances than the initial blended precipitation product this implies that increasing the input of the bma model can cause a relatively lower accuracy than that of the initial scheme in general selecting input groups and numbers of algorithms require careful consideration when used in the development of ensemble based precipitation products in the future it can select appropriate group and number precipitation products for data fusion at different climatic regions so that a best final precipitation product with high accuracy can be produced on the other hand high quality hourly precipitation products can be then developed based on the final precipitation product and linear scaling method these studies will further improve the application accuracy of merged precipitation product in flood forecasting 5 3 limitations of this study considering the impacts of different inputs the modifiable bma framework in this study is proposed for blending four daily precipitation datasets the results demonstrated that the developed framework including the changed initial comparison method could be successfully applied to analyze the performance differences of various blended precipitation products however some uncertainties and limitations remain and should be noted first the nearest grid to point evaluation used in this study could not adequately represent the details in some areas without meteorological stations or with sparse networks especially in the xj and tp regions with varied topography and complex climate the results of the evaluation approach can be controlled by a network of meteorological stations tang et al 2018 despite that some studies have proven that the evaluation results of the grid to point approach were generally consistent with those of the grid to grid analysis jiang et al 2021a lei et al 2022 second there were some overlapping stations between the cma stations against the cpc unified gauge analysis and the mswep product corrected by the daily gauge and monthly gpcc dataset which can introduce uncertainties into our evaluation results previous studies have evaluated cpc and gpcc adjusted satellite precipitation products using 778 meteorological stations and proved that the evaluation results were credible over mainland china zhou et al 2020 in addition the evaluation results of mswep in this study are generally consistent with those of previous studies at the point scale including independent and dependent meteorological stations as shown in table 6 third the comparison method had a certain available range for quantifying the effects of individual members on bma based schemes for example this method is not applicable for discerning the impacts of ensemble members on the change from merged mce to mcig moreover this study only utilized the comprehensive kge metric in the calculation of this method which does not denote the rainfall detection performance fourth the bma framework could not well remove the false alarmed events from different sources although the cpc gauge based dataset urges the blended precipitation product to obtain a better false alarmed ratio in capturing the rainfall or typhoon related precipitation events as shown in table 2 and table 4 a potentially improved bma framework that classifies the rainfall and snowfall lyu et al 2020 or rain and no rain events dong et al 2022 can be developed to further reduce the false alarmed ratio or enhance the detection of precipitation finally the bma framework should be further compared by using triple collocation tc xu et al 2020 machine learning ml baez villanueva et al 2020 and other approaches to expand the applicability of our schemes xiong et al 2021 although the bma method is more robust than the traditional ensemble based simple model averaging sma and one outlier removed oor yin et al 2021 yumnam et al 2022 regarding the manufacturing process of merged precipitation products a grid based bma dataset can be generated by interpolating and normalizing the optimal weights of ensemble members for each pixel cell ma et al 2018 this study did not consider the interpolated grid weights over the non station areas because the different interpolation techniques could have introduced some uncertainties into the analysis pradhan et al 2022 in addition bias adjustment is an important link and should be considered into the modifiable bma framework for greater improvements in future research such as the use of dynamic more stages and quantile based bma models yin et al 2021 yumnam et al 2022 6 conclusions this study established a modifiable bma framework to merge four precipitation datasets gauge based cpc reanalysis model era5 multi satellite imerg e and gsmap rt by considering different inputs over mainland china this framework was trained on a daily scale from 2001 to 2010 and validated for the period 2011 2015 the bma predictions mce mci mcg mcei mceg mcig and mceig were systematically evaluated and compared with the original precipitation members and the representative ensemble based precipitation product mswep subsequently the effects of different inputs on the bma predictions were quantitatively separated using the optimal weight and changed initial comparison method our major findings are summarized as follows 1 of the four members cpc exhibited the highest reliability in the daily statistics and seasonal analysis but had a poor detection performance during typhoon rammasun in 2014 where it underestimated rainfall volumes era5 imerg e and gsmap rt had substantial errors but better detection ability in extreme rainfall events 2 after the bma fusion the blended precipitation products showed significant improvements relative to the reanalysis and satellite precipitation datasets and some e g mceg performed better than cpc in terms of individual metrics evidently during the typhoon more importantly some bma merged precipitation products e g mci and mcg better reflected the spatial and temporal characteristics of rainfall patterns compared to mswep particularly in the sw and eastern tp regions and during the period of typhoon rammasun 3 the bma predictions had evident performance differences because of the different input sources the various groups of inputs showed significant impacts on the bma schemes which determined whether or not the blended precipitation product was better than mswep increasing the input variables might not continuously enhance the performance of blended precipitation products although it can reduce spatial heterogeneity 4 the bma optimal weights of the ensemble members exhibited clear variations for different groups in these cases cpc had the highest weight and was the main influencing factor by comparison the increased input cpc dataset evidently facilitated the performance improvement more than 14 of the fusion model over the initial bma scheme in terms of the kge metric similarly increasing the imerg e dataset showed a slight improvement whereas the opposite held true for era5 and gsmap rt overall the developed modifiable bma framework is viable over mainland china and enables the full consideration of the selection of different input sources to merge multiple precipitation datasets credit authorship contribution statement linyong wei conceptualization methodology software writing original draft shanhu jiang conceptualization project administration jianzhi dong writing review editing liliang ren funding acquisition yi liu methodology linqi zhang methodology menghao wang data curation zheng duan writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was financially supported by the national natural science foundation of china u2243203 51979069 the key science and technology program of the ministry of water resources skr 2022034 the national natural science foundation of jiangsu province china bk20211202 postgraduate research practice innovation program of jiangsu province kycx22 0635 
2238,hydrological ensemble forecasting plays a critical role in decision making and water resource management the skill of an ensemble forecasting system is limited by input data model parameters and model structure and the precision of the system depends mainly on the ensemble size the main contradiction is to ensure the forecast performance and ensemble size under considering the uncertainties of inputs and models in this study a hybrid decomposition based multi model and multi parameter dmp ensemble streamflow forecast method is proposed the proposed method couples the signal decomposition and artificial intelligence ai forecast method in order to provide a more efficient and effective streamflow forecast the decomposition method is used to reduce the uncertainty of inputs for the forecast model and ai models are used to improve the performance of forecasting the results illustrate that the dmp ensemble streamflow forecast method not only extracts the characteristic periodic term and trend term of hydrological series but also improves the forecasting accuracy and reduces the ensemble forecast uncertainty at the same time it greatly expands the ensemble size which solves the problem of insufficient ensemble size in order to be convenient for further water resources analysis and decision making the findings also show that high frequency subseries are highly sensitive to ensemble forecast and it is recommended that more than 2 layers of high frequency series should be used for ensemble forecast the proposed approach in this study is more suitable for nonlinear and non stationary hydrological series forecasting and it is of reference significance for real reservoir operation keywords ensemble streamflow forecast singal decomposition method artificial neural networks ann least squares support vector machines ls svm uncertainty data availability the authors do not have permission to share data 1 introduction hydrological time series is a complex synthesis of meteorological activities and is affected by atmospheric cycle sea surface temperature underlying surface human activities and so on wang et al 2020 as such it is not an easy task if one needs to forecast such time series as a technical science to find the law behind the hydrological phenomenon a skillful monthly hydrological forecast plays an important role in decision making of water related engineering operations agriculture and risk management of water resources especially during drought conditions and flood periods chen et al 2021 harrigan et al 2018 in recent decades hydrological forecasting models have made great progress including statistics based hydrological models and physically based hydrological models in addition many studies focused on the improvement of model accuracy but the actual application of hydrological forecast indicates that model precision and hydrological uncertainty are also important issues that need to be addressed bourgin et al 2014 kasiviswanathan et al 2016 hydrological forecasting ability depends on adequate historical input data forecast model and the skill of weather forecast which introduce varying degrees of uncertainties thus degrading the quality and limiting the operational utility of streamflow forecasting demirel et al 2013 mazrooei et al 2015 montanari and grossi 2008 paiva et al 2012 meanwhile these uncertainties couldn t be shown in a deterministic or single valued forecast and such forecasts couldn t fully reflect the range of future streamflow possibilities demargne et al 2014 it is significant that hydrological forecasts can be used for water resource management and natural hazards prevention only if a measure of their reliability is attached to each predicted value taormina and chau 2015 in this context lorenz 1963 first put forward the concept of ensemble forecast which marked the beginning of ensemble forecast research ensemble streamflow forecasting is a probabilistic forecasting method that can not only obtain the deterministic forecast value but also give the uncertain information of hydrological forecast since the 1990 s with the improvement of computer computing ability and the development of meteorology an ensemble forecasting system has been used by european centre for medium range weather forecasts ecmwf molteni et al 1996 and national centers for environmental prediction ncep toth and kalnay 1997 based on the idea of ensemble forecast ensemble streamflow forecast was first introduced by the national weather service nws and has since attracted much attention day 1985 compared to a single deterministic forecast of medium and long term forecasting the ensembles of the deterministic forecast has been proved to be beneficial which provides a series of reasonable answers and produces greater consistency and reliability of the true probability distribution of forecasts hagedorn et al 2005 pappenberger et al 2008 xu et al 2020 the ensemble forecast results can also extract single value deterministic forecast results corresponding to the best forecast by fusing multiple forecast modes which performs better than single model simulations did arsenault et al 2015 najafi et al 2012 troin et al 2021 ensemble forecasts can be composed of multiple sets of deterministic forecast results or generated randomly through interval forecasting the difference between the ensemble forecast method is that the uncertainty of the former depends on the deterministic forecast accuracy and the uncertainty interval of the latter depends on the specific algorithm model for example regonda et al 2006 proposed a multi model ensemble forecast framework for streamflow forecasts taormina and chau 2015 used the lower upper bound estimation lube method and multi objective fully informed particle swarm mofips optimization algorithm for ann based interval forecasting of streamflow discharges according to the sources of uncertainties in the hydrological forecasting process ensemble streamflow forecast methods can be divided into multi input multi model and multi parameter ensemble forecasting results however this interval forecasting method is greatly affected by given models and algorithms and can not take into account the uncertainty of inputs and parameters for streamflow forecast models limited by the data requirements complexities and intensive computation of physical models kaya et al 2019 data driven statistically based models are becoming popular especially in the medium and long term zhang et al 2015 in general the methods are simple computing rapidly and capable of forecasting non linear streamflow processes without a full understanding of the physical mechanism involved awchi 2014 wu and chau 2013 various data driven methods using artificial intelligence ai techniques such as artificial neural networks ann support vector machine svm random forest rf and long short term memory lstm have been used to model complex hydrological studies fu et al 2020 le et al 2021 shijun et al 2020 wu et al 2009 although many such models have been researched for hydrological forecasting no conclusion has been reached on which model is optimal for streamflow forecast at the same time complex hydrological processes are presented by relatively simple parameters which cause distortion and inevitably lead to systematically overestimating or underestimating flows during the dry or wet seasons hashino et al 2007 wang et al 2019 regardless biases from the forecast model could degrade the ensemble by fusing multiple model characteristics therefore it is widely acknowledged that an ensemble forecast is much better in providing insight into the uncertainty of any forecast beven 1993 regonda et al 2006 we also noted that an ensemble streamflow forecast can consist of multiple models or models with the different ensemble of parameters bourgin et al 2014 herr and krzysztofowicz 2010 kasiviswanathan et al 2016 regonda et al 2006 wang et al 2019 georgakakos et al 2004 attributed the advantage of multi model ensemble forecasting to considering the uncertainty of model structure in addition to multi model and multi parameter the performance of the ensemble streamflow forecast is still affected by the input data statistics based streamflow forecast takes observed streamflow as the only predictor to forecast the hydrological process in essence this model adapts itself better to the nuances of historical data troin et al 2021 however as the only input data the historical runoff streamflow series shares the characteristics of nonlinear and non stationary under the disturbance of multiple uncertainties similar to natural signals with multiple cycles amplitude and phases qian et al 2019 therefore it is necessary to introduce signal processing techniques for preprocessing the streamflow time series which is suitable for analyzing hydrological time series due to the combined stochastic and non stationary features nourani et al 2014 the common signal processing techniques in hydrology are mainly developed based on fourier transform ft such as wavelet analysis wa singular spectrum analysis ssa and empirical mode decomposition emd these decomposition methods can reveal the local characteristics of the time series from time domain and frequency domain gaucherel 2002 wa has high resolution and its essence is fourier transform with an adjustable window while it is required that the signal in the window must be stable which often produces false harmonics tewfik et al 1992 ssa is modeless which neither requires a stable time series nor assumes a parametric model vautard and ghil 1989 it faces the challenge of how determining a threshold to distinguish between signal components and noise components emd is an experience based adaptive data processing method with a high signal to noise ratio huang et al 1998 subsequently emd derivative methods were proposed to overcome the mode mixing problem such as ensemble empirical mode decomposition eemd and complete ensemble empirical mode decomposition with adaptive noise ceemdan ceemdan is a new noise assisted data analysis algorithm which effectively compensates for the missing scale of the original signal for non linear and non stationary time series torres et al 2011a in the existing research there are some coupling models of decomposition and regression for hydrology simulation and forecast for example rajagopalan et al 1998 kwon et al 2007 and erkyihun et al 2016 adopted a multivariate frequency domain approach autoregressive wavelet decomposition and wavelet based time series bootstrap model for climatic forecasting rainfall and streamflow simulating respectively wu et al 2009 and wu and chau 2013 combined ssa with auto regressive moving average arma k nearest neighbors knn ann crisp distributed artificial neural networks cdann and support vectors regression svr for prediction of streamflow and rainfall wang et al 2021 proposed an ensemble hybrid forecasting model for annual runoff based on sample entropy secondary decomposition and lstm which includes extreme point symmetric mode decomposition esmd and wa zhu et al 2016 wang et al 2018 wang et al 2020 ghasempour et al 2021 combined support vector regression svr autoregressive integrated moving average arima ann gaussian process regression gpr forecast models and emd eemd for streamflow forecasting respectively the results show that the decomposition based forecast models exhibited better statistical performance the techniques by decomposing time series into multiple layers the trend and harmonics of hydrological time series may be extracted and the signal noise eliminated from the series huang et al 1998 torres et al 2011a on the one hand it can provide cleaner input for ensemble streamflow forecast to reduce the uncertainty of input data on the other hand it could expand the ensemble size of ensemble streamflow forecast the effectiveness and accuracy of an ensemble forecasting system are affected by both input and model uncertainties a good ensemble forecasting model needs to consider the uncertainty of input model structure and parameters with good performance in accuracy less uncertainty and less uncertainty good precision and objective set quantity ensemble size existing ensemble forecasting models face the following challenges i improve the forecast accuracy to some extent ii control the range of uncertainty and iii expand the ensemble size meanwhile three key problems need to be solved how to provide effective input how to build reasonable forecast models and how to select suitable model parameters therefore it is necessary to propose a new method to improve the effect of ensemble streamflow forecast the main goal of this study is to develop a novel ensemble forecasting method for hydrological forecasting in this regard we have set 4 objectives for this study which are 1 to provide clear inputs which present significant cycle and trend characteristics without noise for ensemble streamflow forecast by signal decomposition method which transforms non stationary original streamflow to stationary sub time series 2 to test the applicability of different artificial intelligence algorithms with different parameters in forecasting decomposed subseries 3 to compare the accuracy and uncertainty of developed ensemble model with single models and traditional ensemble model and 4 to explore the sensitivity of different decomposed components for ensemble streamflow forecast model its practicability is then verified through a case study of the monthly streamflow ensemble forecast of the yalong river basin it should be noted that in this study the classical forecast models ann and ls svm are adopted as benchmark methods in forecasting decomposed subseries which may not be the optimal choice the purpose of the choice is to get rid of the influence of forecast accuracy improvement due to the advantages of the algorithm itself the novelty of this new model lies in the inclusion of ensemble forecast based on streamflow periodic components that contribute to obtaining more accurate forecast results with less uncertainty and effectively expanding the ensemble size of streamflow 2 methodology 2 1 ensemble streamflow forecast framework based on decomposition in this study a decomposition ensemble forecast framework is developed to increase the accuracy of streamflow forecast and overcome the limitation of traditional multi model ensemble forecast as shown in fig 1 the developed decomposition based multi model and multi parameter dmp ensemble streamflow forecast framework is summarized in the following steps step 1 streamflow series decomposition using the decomposition method the original streamflow time series are composed into several stationary subseries and one residue trend series which have different frequencies step 2 forecast modes selection in this study the multi modes involved multi model and multi parameter ann and ls svm are selected as multi models to forecast decomposed components of streamflow because each model involves many parameters the results are computed for multiple possible combinations of parameters moreover the results of all combinations within a prescribed performance are selected as admissible constituting the pool of candidate parameters step 3 dmp forecast of decomposes series different models with candidate parameters are used to forecast the decomposed streamflow subsequences and multiple sets of forecast results are obtained for each subsequence as subseries forecast sets step 4 dmp ensemble forecast select a forecast result from each layer subseries forecast sets to combine as an ultimate streamflow forecast results and then set all the combined results as the final dpm ensemble forecast results step 5 forecast performance evaluation traditional statistical modeling involves picking up the best model to forecast however it neglects model inadequacy and increases the associated risk of a model regonda et al 2006 the model averaging which combines candidate models into a deterministic forecast result by mean value is proposed to alleviate this problem reid 1968 the accuracy of ensemble forecast is based on averaging result which is evaluated by the taylor diagram the uncertainty of the ensemble forecast based on ensemble result intervals which is assessed by three uncertainty criteria in this dmp ensemble forecast framework non stationary streamflow is decomposed into stationary series with various frequencies by decomposition method multi forecast modes are used for decomposed sub streamflow series forecast integrating multiple forecast results of every subseries as integrated streamflow ensemble forecast and forecast analysis of the subseries are performed using multi evaluation criteria 2 2 the decomposition method ceemdan is a spatio temporal analysis method the theory of it is that additional finite amplitude white noise in pairs are uniformly distributed in the entire time frequency space of the original signal and the space is composed of varied scale components in different frequencies the method contributes to the reduction of residue error in the reconstruction process by adding white paired noise which includes positive and negative signals and obtains components with less noise and more physical meaning yu et al 2013 compared with emd eemd and ceemd ceemdan not only solves the problem of the mode mixing but also increases the stability and reduces the residue noise and computational cost moreover it achieves a same number of ensembles under a tolerable level of noise colominas et al 2014 torres et al 2011a it should be noted that the final decomposition results can be obtained by adding enough white noise is added into the original data to each test and eliminating the noise by calculating the average value of entire tests then remaining components as part of the original signal which contain correct physical significance the same frequent components are gathered into one intrinsic mode functions imfs this new method makes full use of the white noise statistical characteristics to disturb the original signal and then eliminates its influence the processes of decomposition a given signal x t are described shown as follows torres et al 2011a in the steps assume ek t is the kth mode obtained by emd w i is defined as additional white gaussian noise with a mean value equal to 0 and variance 1 ε k ε 0 s t d r k k 1 ε k ε 0 s t d x s t d e 1 w i 0 ε 0 is the reciprocal of the initial noise and the ideal signal to noise ratio snr of the original series then x i x w i step 1 decompose x i x t ε 0 w i t by emd nr realizations to obtain the first mode and compute using eq 1 1 im f 1 t 1 nr i 1 nr im f 1 i t where i is ith realization nr is the maximum number of realizations step 2 at the first stage k 1 the first residue r 1 t presents as in eq 2 2 r 1 t x t i m f 1 t step 3 decompose r 1 t ε 1 e 1 w i t i 1 2 n r until the second mode is defined as eq 3 3 im f 2 t 1 nr i 1 nr e 1 r 1 t ε 1 e 1 w i t step 4 for k 2 3 k the kth residue calculated by eq 4 4 r k t r k 1 t i m f k t step 5 decompose realizations r k t ε k e k w i t i 1 2 n r the second mode is presented as eq 5 5 im f k 1 t 1 nr i 1 nr e 1 r k t ε k e k w i t step 6 go back to step 4 for the next k until the residual meets one of the following conditions 1 the residual is no longer to be decomposed by emd 2 the imf condition is satisfied 3 the number of local extreme points is less than 3 after reconstruction the final residuals satisfy the following requirements 6 r t x t k 1 k im f k t 2 3 forecast framework 2 3 1 artificial neural networks ann ann is a common artificial intelligence technology that is a biologically motivated algorithm maier and dandy 1996 mcculloch and pitts 1943 it is also an abstract simplified and simulated mathematical model which is interconnected by multiple neurons ann has characteristics of large scale parallel processing distributed information storage good self organization and self learning ability the advantage of ann is that it has the ability to recognize the signal distortion and noise without consideration of the complex underlying surface information and accurate description of the hydrological process mechanism only through the training of samples it can effectively predict the complex input output relationship in the hydrological time series process therefore it plays an important role in hydrological forecast fields including reservoir inflow forecasts yang et al 2017 water quality parameter prediction maier and dandy 1996 flood forecasting elsafi 2014 precipitation prediction luk et al 2000 in particular the error back propagation training algorithm bpnn as a three layer feed forward neural network is one of the most widely used neural network models rumelhart et al 1986 bpnn can learn and store a large number of nonlinear and densely mapping relationships between inputs and output the topological structure of the bpnn model includes input layer hide layer and output layer hecht nielsen 1988 has proved that the three layer network can simulate any complex nonlinear process the functions f and g are used to connect the inputs to hidden neurons and the hidden layer to the outputs as shown in equations 7 and 8 respectively 7 h j f i 1 n w ij x i b j 8 o k g j 1 m v jk h j c k where hj is the jth neuron of the hidden layer n is the total number of inputs m is the number of neurons xi is the ith input wij is the weight assigned to xi to calculate the jth hidden neuron bj is the bias of jth hidden layer i 1 2 n and j 1 2 m ok is the kth result of the output layer vjk is the weight assigned to hi ck is the bias of the kth output layer l is the number of outputs j 1 2 m and k 1 2 l in this study the activation functions f and g were tangent sigmoid and linear functions respectively moreover the levenberg marquardt algorithm was used in the training model by adjusting the weight and bias hagan and menhaj 1994 wang et al 2021 it is noted that in order to improve the efficiency of the training network the input variables should be normalized to ensure that the magnitude of the variables is consistent the normalization formula is as follows 9 9 x i n o r m a l i z e d x i x i min x i max x i min where xi normalized is the result of input variables normalization xi min and xi max are the minimum and maximum value in input variables xi respectively 2 3 2 least squares support vector machines ls svm the ls svm model is reformulation to the standard svm vapnik 1995 which was proposed by suykens and vandewalle 1999 and is one of the most widely used svm methods it is closely related to regularization networks and gaussian processes and alleviates the computational burden of svm balabin and lomakina 2011 a set of linear equations by ls svm instead of a quadratic programming problem it has better accuracy for nonlinear signal processing zhu et al 2010 compared with ann it can overcome the shortcomings of long training time the randomness of training results and overfitting the key to ls svm is finding a linear function f x which could describe the relationship between the given inputs vector x x 1 x 2 x n and target series y y 1 y 2 y n as follows 10 f x w φ x b where w and b are the weight vector and bias term respectively φ x is the nonlinear mapping function by transforming the input vector into a high dimensional feature space maity et al 2010 the optimization equation is a quadratic programming problem which can be estimated by a lagrangian multiplier and the ls svm model can be expressed as 11 f x sign i 1 n y i a i k x x i b where a i is the lagrange multipliers k x x i is the kernel function and used to change the dimensionality of the input space which is critical to the performance of ls svm the common kernel functions include the linear polynomial and radial basis function rbf as shown 12 k x x i x x i k x x i γ x x i r d k x x i exp γ x x i 2 where d represents the degree of the polynomial terms r is a residual term and γ the kernel specific parameter 2 4 forecast skill evaluation by statistical methods to conceptualize and abstractly describe the hydrological processes it inevitably leads to distortion and uncertainty in ensemble forecast which integrates multiple subseries multiple models and multiple parameter sets how to effectively assess the ensemble forecast uncertainty is key for analysis rationality and availability of forecast results the evaluation of the model performance has often been conducted using intuitive graphical representation as well as statistical measures in this study nash sutcliffe model efficiency nse relative error re root mean square error observations standard deviation ratio rsr and taylor diagram are employed to evaluate the accuracy of streamflow forecast containing ratio cr average band width bw average asymmetry degree ad continuous rank probability score crps and brier score and skill score bss are as criteria for measure the forecast uncertainty 2 4 1 nash sutcliffe model efficiency nse nse is the generalized version of r squared from parametric regression which indicates the relative magnitude of residual variance and observation data variance nash and sutcliffe 1970 13 nse 1 i 1 n q i obs q i f 2 i 1 n q i obs q ave obs 2 where q i obs and q i f represent the observed and forecast inflow series respectively during the ith month q ave obs is the means of the observed values and n is the total number of original observations as well as the number of months the closer nse is to 1 indicates less bias and a better fit 2 4 2 relative error re re is known as the relative percentage error which is computed by the following normalized average 14 re 1 n i 1 n q i obs q i f q i obs 100 it is valuable for estimating over or under simulation typically offering an indication of bias he et al 2017 the closer re is to 0 indicates less bias and a better fit 2 4 3 root mean square error observations standard deviation ratio rsr rsr is a model evaluation statistic which uses observations standard deviation sd to standardize root mean square error rmse rsr is the ratio of the rmse and the standard deviation of measured data and is calculated with the following equation moriasi et al 2007 15 rsr i 1 n q i obs q i f 2 i 1 n q i obs q ave obs 2 rsr includes a scaling normalization factor and incorporates the benefits of error index statistics which can be applied to various constituents chen et al 2012 rsr ranges from 0 to the lower the better the model simulation performance 2 4 4 taylor diagram taylor diagram is devised by taylor 2001 which is a visual and concise statistical framework for quantifying the differences the degree of uncertainty and expected agreement between model simulated results and observations in terms of their correlation coefficient corr sd and rmse the diagram is suitable for comparing the relative skill and advantages of an ensemble of various models or evaluating complex models with different parameter sets there are some points representing the results of different models and a reference point corr 1 sd 1 and rmse 0 in the taylor diagram the distance between the reference point and model results points indicate the performance of simulation results the closer the points to the reference point the more satisfactory performance in this study the taylor diagram is employed to compare and analyze the characteristics and uncertainty of the single models the multi model multi parameter ensemble models and the multi subseries multi model multi parameter ensemble models 2 4 5 containing ratio cr the containing ratio cr is an index that describes the ratio of the number of observed values enveloped by uncertainty intervals of total forecast streamflow value expressed as a percentage it is widely used for quantifying the goodness of the forecast intervals since the glue method emergence beven and binley 1992 freer et al 1996 the larger the cr value is the greater proportion of observation values fall within the forecast uncertain intervals is the detailed formula is as follows 16 cr n in n 100 where n in is the number of observed points falling within forecast uncertainty interval 2 4 6 average band width bw the average band width is defined as the average bounds of uncertainty intervals which is the difference between the upper and lower values of forecast value sets blasone et al 2008 olsson and lindström 2008 the narrower the bw the less the uncertainty which means the most important information is captured and the forecast result is credible as shown in the equation 17 17 bw 1 n i 1 n q i upper q i lower where q i upper and q i lower are the upper and lower values of the ith forecast points respectively 2 4 7 average asymmetry degree ad average asymmetry degree which is denoted by ad is an accurate and robust estimator of streamflow forecast uncertainty wang et al 2019 xiong et al 2010 due to the nonlinearity of the hydrology process it is satisfactory that the asymmetry degree of streamflow forecast intervals is as small as possible which means the closer the average value of the upper and lower forecast points at each time step are to the observed value the index is defined as 18 ad 1 n i 1 n q i upper q i obs q i upper q i lower 0 5 2 4 8 brier score and skill score bss bss is the proportion of improvement of accuracy over the benchmark forecast using some measure of accuracy brier score bs given generically by wilks 2011 19 bss bs b s bench b s bench where bs is the accuracy measure of the ensemble forecasting system qf against observations bsbench is the accuracy measure of the benchmark forecast q bench f against qf 20 bs 1 n i 1 n q i f q i obs 2 4 9 continuous rank probability score crps the crps is one of the most recommended scores for the evaluation of overall hydrological ensemble forecast performance harrigan et al 2018 it is the integral of the squared difference between the cumulative distribution function cdf of the forecast ensemble and the observation 21 crps 1 n i 1 n f i q i f i q i f q i obs d x where f is the cdf of forecasts at time step i th thepfcst forecasts at time step t the heaviside function h is calculated as follows 22 h 1 0 q i f q i obs q i f q i obs 3 data and parameters description 3 1 study area yalong river watershed which is the largest tributary of the jinsha river is a typical mountain valley river it is characterized by rich water resources good regulation performance less submergence loss and superior economic indicators especially in the middle and lower reaches of the watershed jinping first stage hydropower station jp ⅰ is the headwater reservoir for controlling cascade hydropower stations the middle and lower reach of yalong river as shown in fig 2 whose operation level can directly determine the water supply guarantee benefit and flood risk management of the whole downstream cascade reservoirs xu et al 2016 jp ⅰ reservoir is the annual regulating reservoir was selected as the study area because large inflow forecast errors inherent have resulted in the water level of the reservoir often fluctuating beyond the normal range ma et al 2021 the basin above the reservoir covers an area of 1 03 105 km2 accounting for 75 4 of the yalong river watershed jp ⅰ reservoir has an annual mean inflow of 1220 m3 s accounting for 8 6 of the total water in the upper reaches of the yangtze river its normal storage water level is 1880 m and corresponds to capacity of 7 76 109 m3 and the storage water is primarily employed for hydropower generation it also has comprehensive benefits of flood control sand blocking and tourism its annual runoff mainly comes from the flood season june october comprising above 75 of the total runoff and the rest from the non flood season november may the monthly streamflow which is the inflow data of jp ⅰ reservoir covers the period of january 1953 december 2011 59 years or 708 months and was retrieved from the yalong river hydropower development company ltd 3 2 model inputs and parameter settings it is common that long term streamflow forecast base on the same period data which means that the previous years of one month series as inputs of that month forecast model therefore 12 sub models need to be built for the months of the year for each model the lead time is one month the comprehensive overall lead time can be up to one year 12 months in these sub models the data of 1953 2001 approximately 80 of data 2002 2011 about 20 of data are set as calibration and validation periods respectively in the ceemdan model nr was set to 100 the maximum number of sifting iterations was set to 500 and the amplitude of white noise was set to 0 2 times the standard deviation of the sample data which is optimal parameters set determined by trial and error these settings are similar to those employed by torres et al 2011b dmp forecast models are used in ensemble forecast and ls svm and ann models are mainly used in the model for ls svm model the rbf linear and polynomial kernel functions with different penalty coefficients c and gamma γ combine multiple parameter sets at the same time the hidden nodes of ann from 4 to 20 combine different iterations to make trial and error finally several groups of parameters with satisfied fitting performance are selected such as table 1 moreover the same parameter sets are also used for all decomposition series 4 results in this section the developed ensemble forecast framework was applied to forecast the inflow of jp ⅰ reservoir and the result are demonstrated the periodic characteristics and trend of inflow of jp ⅰ reservoir are analyzed by ceemdan method then the simulated reservoir inflow are compared with observation under multi model with different parameters moreover the influence analysis of different components of decomposition on ensemble forecast results are described the detailed results are as follows 4 1 the decomposed results ceemdan was employed to decompose the integrated 12 monthly inflow series into five components of periodic oscillations imf1 imf5 and a residue trend the decomposed levels with different cycles and amplitudes are arranged from the highest to the lowest frequency and become more and more stable there is the greatest amplitude and frequency in first layer on the contrary the residue has the slowest change and shows the trend of the original time series fig 3 shows an example of the decomposed components by ceemdan of january low water period and july high water period as shown in fig 3 the decomposed series has obvious periodic and the frequency gradually decreasing which provide clear inputs for forecast models the periods represent different levels of streamflow from interannual to multidecadal from this figure the distance between two adjacent highest points or lowest points is a period and a time subseries contains multiple cycles thus the average cycle can be calculated according to the time series length for example the imf1 series in fig 3 a contains 22 complete cycles from 1955 to 2010 so the average cycle of imf1 is 2 5 years in the same way the inflow decomposed series of january include about average 2 5 year 6 year 10 year 14 year and 40 year cycles and the trend is to go down first 1950 s 1980 s and then up 1980 s 2010 s for july in fig 3 b imf1 to imf5 have obvious periodic variations with 2 5 year 6 year 10 year 20 year and 40 year periods and a gradual downward trend is evident for all 60 years the inflow of the other months is also decomposed into similar cyclical characteristics therefore the monthly inflow series of jp ⅰ reservoir includes 2 3 years 5 6 years 7 10 years 12 20 years and 35 40 years periodic variations which is basically consistent with the existing research results for the trend term of jp ⅰ reservoir inflow the overall trend is slow growth the trends of july and august are continuously declining from 1953 2010 a steady growth for march may june and october trends and other month trends reveal decrease to the 1980 s and then increase overall the inflow series has no significant change during the period and the result is consistent with li et al 2015 xu et al 2008 at the same time it is found that the variation range of inflow trend in flood season is large especially from may to september and the trend change in other months is not significant according to the literature research the runoff of the yalong river is mainly driven by precipitation snow melting and groundwater which in the tibetan plateau li et al 2015 li et al 2014 especially in the wet season the runoff is primarily affected by rainfall factors precipitation change is roughly the same as the runoff and the intensity distribution of each time scale is also consistent precipitation and snowmelt are related to solar activities sunspot number ssn sea surface temperature sst north atlantic oscillation nao etc which is the external driving force of climatic change li et al 2020 zhang et al 2007 at the same time the existing research points out that besides rainfall temperature and human activities play an auxiliary and promoting role especially in winter li et al 2018 sun et al 2017 4 2 the results of single method and decomposed forecast method in this study ann and ls svm with different parameter sets are employed in original and decomposed inflow series forecasting nse re rsr and taylor diagram is used to evaluate the accuracy of different forecast models the forecast period of these forecast modes from january 2002 to december 2011 the lead time is 12 months the comparison of single and decomposition forecast results and observation values is shown in fig 4 and fig 5 and the evaluation of forecast results is in table 2 and fig 6 fig 4 a and b respectively illustrate the comparison between the forecast results of the original inflow series and the decomposed series by ann and ls svm methods with different parameter sets and observation of 2002 2011 jp ⅰ monthly inflow from the figures it is obvious that the simulated results are basically consistent with the observed value however ls svm has better performance than ann for different parameter sets ls svm and ann forecast both overestimate in flood season and underestimate in non flood season while the ann method forecast performance is more random from fig 5 a and b the typical scatter plots depict the forecast quality of modes under different inflow magnitude conditions as displayed under low water conditions 1000 m3 s the forecast quality of ann and ls svm forecast modes are high with small deviation especially the ls svm method obviously the forecast results have and the forecast points of ann based forecast modes are distributed evenly on both sides of the 45 line with larger deviations than the results of ls svm forecast modes which means the under forecast of flow whereas most forecast points of the flood period of ls svm method fall above the 1 1 line which confirms the inflow in this period is overestimated in addition the points of decomposition based forecast modes match with the red line the hybrid method produced slightly better performance compared to the single forecast modes from table 2 it is easily found that the hybrid models with ceemdan perform well in nse and rsr indicators except for ann1 and ann4 on the contrary the hybrid svm models show larger deviation with higher re than single svm models in the taylor diagram fig 6 the performances of multi forecast modes are evaluated by corr sd and rmse it is found that the decomposed forecast modes result points closer to the reference point than single forecast models which mean the forecast modes base on the decomposition method have more satisfactory result than single forecast models similarly ls svm forecast modes outperform the ann forecast modes at the same time the result points of ls svm with different parameters are relatively concentrated so the parameter sets insensitive to ls svm results in summary the decomposition method of ceemdan contributes to improving the forecast accuracy 4 3 dmp ensemble forecast results in this study two ensemble forecast methods are involved which are based on the selected multi model multi parameter and multi subseries multi model multi parameter modes respectively the uncertain results of the monthly forecast inflow of the jp ⅰ reservoir can be seen in fig 7 the black solid line and red dotted line represent the monthly observed and mean simulated inflow of ensemble forecast modes the green polygons indicate the uncertainty intervals and the wider the interval the greater the uncertainty table 3 further presents the forecast skill evaluations of two ensemble forecast results cr is a measure of the ability of the forecast interval to contain real information the larger cr the more real information the prediction interval contains as illustrated in table 3 the cr of traditional multi model and multi parameter ensemble forecast models tmp is 47 while the cr of the dmp models is 55 the dmp model contains more real information and has a stronger forecasting ability dmp is superior to tmp in crps which can measure the reliability and concentration of prediction the smaller the crps is the better the probability forecast performance is although the bw is smaller by tmp there are higher cr 55 lower ad 0 69 bbs 0 15 and crps 2 1 1 for the dmp method overall dmp could yield more accurate forecast results with smaller uncertainty the low cr less 50 produced by tmp is primarily due to the obvious under forecast of the peak flow of flood period as demonstrated in fig 7 a tmp has slight advantages in bw nevertheless the dmp ensemble size has increased by orders of magnitude compared with the tmp method reaching 2 2 1042 forecast results set which solves the problem of insufficient ensemble size by closely observing fig 7 a and fig 7 b it is not difficult to find that the dmp ensemble forecast mode has better coverage of the observations with comparable forecast intervals from table 2 we can see that the dmp has advantages in nse re and rsr performance over the traditional ensemble forecast method it can also be verified that the mean ensemble forecast results perform better than the single model in forecasting and the ceemdan contributes to obtaining more accurate ensemble forecast results fig 6 further shows the blue dot dmp is closer to the reference point red dot than the purple dot tmp which indicates that the dmp can yield appreciable results in corr sd and rmse in general these results support that dmp outperforms the tmp method in terms of uncertainty see green polygons in fig 7 and accuracy of mean ensemble forecast inflow see red lines in fig 7 in a wide range of flow conditions the intervals of the two methods vary with the magnitude of the inflow the flood periods have larger uncertainty than the low flow zone during the flood periods the flow is overestimated and underestimated by dmp and tmp but dmp is slightly better than tmp in the expression of double flow peaks of high flow conditions 5 discussion 5 1 the influence of involved different subseries dmp ensemble forecast it is well understood from the above analysis that the dmp ensemble forecast method could achieve high precision and low uncertainty forecast results due to the number of ensemble reaching 2 2 1042 groups based on composed six subseries the computation burden is heavy therefore this paper explores the influence of the number of involved decomposed subseries on dmp ensemble forecast results to release calculation computing complexity building model test and setting up different involved subseries to analysis due to the large amplitude and high frequency of the first layer subseries imf1 the number of subseries involved in dmp forecast is increased based on the imf1 and compared with the dmp without subseries involving random combination thus 7 schemes are set respectively to analysis influence of ensemble subseries from the single layer of imf1 to total six imfs involved in the ensemble forecast the results are illustrated in table 4 and fig 7 the results show that compared with the tmp the accuracy and uncertainty of the different schemes of dmp ensemble forecast are relatively satisfactory except for ad the scheme nd dmp outperforms the tmp in uncertainty with higher cr less bw and the same ensemble size at the same time when the number of involved subseries in dmp are greater than 2 the uncertainty indicators cr bw ad and the ensemble size are significantly better than tmp the advantages of cr ad and ensemble size become more and more obvious with the increase of the number of imfs involved in dmp in contrast bw will also increase schemes 5d dmp and 6d dmp have comparable uncertainties however the ensemble size varies by orders of magnitude it can be inferred that different subseries have different effects on forecast uncertainty of dmp which will be discussed in section 4 5 therefore the results suggest that dmp might be more efficient in more accurate and lower uncertainty forecasting than tmp when the subseries involved in the ensemble are greater than 2 5 2 sensitivity analysis according to the above results the sensitivity of different subseries to dmp ensemble forecast results is further analyzed the sensitivity analysis is mainly through comparing the different decomposed subseries forecast results by multi model and multi parameter forecast modes if the deviation of the subseries forecast results is large it shows that forecast models have a significant influence on the decomposed subseries at the same time it can be explained that the decomposed subseries is more sensitive to dmp ensemble forecast results by analyzing the dmp ensemble forecast results of different involved subseries it can be seen from fig 8 that the difference of imf1 forecast results is the largest which using multi model and multi parameter forecast modes followed by imf2 imf3 and the forecast results of imf5 and residue item are basically no difference from fig 8 a c the higher the frequency and the greater the amplitude of the subseries are sensitive to the forecast model and the deviation of the forecast results are larger as shown in fig 8 a c while the dmp ensemble forecast results with lower frequency subseries trend to be consistent as shown in fig 7 d f combined with table 4 it can be concluded that the high frequency subseries are highly sensitive to ensemble forecast and when the top 2 layers of high frequency series are involved in ensemble forecast more satisfactory results can be expected 5 3 ensemble forecast analysis in flood season according to the analysis in section 4 it can not be seen that the forecast effect of either tmp or dmp method is limited mainly due to the poor peak forecasting in flood season in this study the ensemble forecast framework is based on monthly simultaneous forecasting model which is common in long term streamflow forecast the streamflow of non flood season is mainly affected by the climate change which varies by years on the contrary the streamflow in the flood season is mainly controlled by the previous streamflow with frequent changes and large amplitude this forecast mode is no longer applicable therefore we attempt to use the dmp method to continuously forecast the streamflow which puts the previous streamflow as input the accuracy and uncertainty of continuous ensemble forecast are shown in fig 5 there is satisfactory performance in deterministic and ensemble forecast compared with the monthly forecast table 5 the continuous forecast has a higher accuracy and its nse is as high as 0 95 re is 18 63 slightly inferior to the performance of monthly forecast meanwhile continuous ensemble forecasting has less uncertainty especially the bss is small to approximately 0 it can be seen that in the actual forecasting work it is necessary to adopt different methods to forecast in wet and dry periods 5 4 limitations of the present model although the dmp framework successfully tested the performance in accuracy uncertainty and ensemble size of ensemble forecasting but several questions still remain to be answered firstly the research has shown that different models with different parameters have great differences in forecast performance in the present model the contribution of multi models with different parameters to ensemble forecasting is the same therefore it is necessary to give weights to different model weights in the follow up research secondly the performance of different decomposition methods forecast models and parameters on the ensemble forecast are quite different the proposed dmp method is not limited to the methods and parameter settings given in this study it deserves further study and be customized according to the characteristics of different study areas meanwhile streamflow is also related to climate change weather conditions and human activities which should be taken into account in the dmp framework thirdly according to the results of this paper the ensemble size is quite large by dmp method however the random combination of high frequency subseries have little significance for ensemble forecasting in the face of a large ensemble size whether there is redundancy in the ensemble forecast results and how to release the computing burden are worth discussing in deeply 6 conclusions ensemble forecasting plays a significant role in improving the accuracy of hydrological forecasting reducing the uncertainty of forecasting and increasing the availability of hydrological forecasting results however the ability of ensemble forecasting is affected by the uncertainty of input data and model parameters and the limited ensemble size in this study a decomposition based multi model and multi parameter ensemble forecast framework for monthly streamflow forecasting is proposed which integrated ceemdan method with ann and svm models with different four parameter sets the aim of the present research was to examine the effect of multi models and multi parameters coupled with data preprocessing techniques in improving the accuracy of monthly streamflow ensemble forecasting results show that the dmp was superior to the tmp in accuracy and uncertainty the inflow of jp ⅰ of yalong river was used as testing cases with the help of the ceemdan multi models with multi parameter showed the best performance when compared with the tmd and single models which as baseline models accuracy monthly streamflow estimates were also obtained from dmp and tmp methods however the methods underestimated the peak flow during flood periods although dry seasons were mostly well forecast moreover this dmp method can increase the ensemble size in the order of magnitude by stochastic integration of forecast subseries so as to effectively solve the problem of the insufficient number of ensemble series some discussions were presented 1 on the influence of involved different subseries dmp ensemble forecast 2 on the sensitiveness of decomposed subseries to dmp 3 on improvement the ensemble forecast in flood season first comparing with tmp the forecast significant advantage when more than 2 high frequency components subseries are involved in dmp ensemble processes second the high frequency subseries with great amplitude were sensitive to the forecast models third continuous ensemble forecast outperformed monthly forecast with higher accuracy and less uncertainties during flood periods the dmp ensemble forecast method in this study is novel in assisting ensemble streamflow forecast on the one hand dmp introduces signal decomposition method and multi model multi parameter forecast modes to reduce the uncertainty of input data and improve forecasting performance and on the other hand dmp contributes to expanding the ensemble size future research should be undertaken to explore better performance in ensemble forecast by advanced decomposition methods and forecast models moreover it is necessary to validate the dmp framework in more watersheds and short scale forecast systems credit authorship contribution statement jia wang conceptualization methodology software validation formal analysis investigation writing original draft visualization xu wang conceptualization investigation resources data curation supervision project administration funding acquisition soon thiam khu resources writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was partially supported by the national natural science foundation of china 52079144 
2238,hydrological ensemble forecasting plays a critical role in decision making and water resource management the skill of an ensemble forecasting system is limited by input data model parameters and model structure and the precision of the system depends mainly on the ensemble size the main contradiction is to ensure the forecast performance and ensemble size under considering the uncertainties of inputs and models in this study a hybrid decomposition based multi model and multi parameter dmp ensemble streamflow forecast method is proposed the proposed method couples the signal decomposition and artificial intelligence ai forecast method in order to provide a more efficient and effective streamflow forecast the decomposition method is used to reduce the uncertainty of inputs for the forecast model and ai models are used to improve the performance of forecasting the results illustrate that the dmp ensemble streamflow forecast method not only extracts the characteristic periodic term and trend term of hydrological series but also improves the forecasting accuracy and reduces the ensemble forecast uncertainty at the same time it greatly expands the ensemble size which solves the problem of insufficient ensemble size in order to be convenient for further water resources analysis and decision making the findings also show that high frequency subseries are highly sensitive to ensemble forecast and it is recommended that more than 2 layers of high frequency series should be used for ensemble forecast the proposed approach in this study is more suitable for nonlinear and non stationary hydrological series forecasting and it is of reference significance for real reservoir operation keywords ensemble streamflow forecast singal decomposition method artificial neural networks ann least squares support vector machines ls svm uncertainty data availability the authors do not have permission to share data 1 introduction hydrological time series is a complex synthesis of meteorological activities and is affected by atmospheric cycle sea surface temperature underlying surface human activities and so on wang et al 2020 as such it is not an easy task if one needs to forecast such time series as a technical science to find the law behind the hydrological phenomenon a skillful monthly hydrological forecast plays an important role in decision making of water related engineering operations agriculture and risk management of water resources especially during drought conditions and flood periods chen et al 2021 harrigan et al 2018 in recent decades hydrological forecasting models have made great progress including statistics based hydrological models and physically based hydrological models in addition many studies focused on the improvement of model accuracy but the actual application of hydrological forecast indicates that model precision and hydrological uncertainty are also important issues that need to be addressed bourgin et al 2014 kasiviswanathan et al 2016 hydrological forecasting ability depends on adequate historical input data forecast model and the skill of weather forecast which introduce varying degrees of uncertainties thus degrading the quality and limiting the operational utility of streamflow forecasting demirel et al 2013 mazrooei et al 2015 montanari and grossi 2008 paiva et al 2012 meanwhile these uncertainties couldn t be shown in a deterministic or single valued forecast and such forecasts couldn t fully reflect the range of future streamflow possibilities demargne et al 2014 it is significant that hydrological forecasts can be used for water resource management and natural hazards prevention only if a measure of their reliability is attached to each predicted value taormina and chau 2015 in this context lorenz 1963 first put forward the concept of ensemble forecast which marked the beginning of ensemble forecast research ensemble streamflow forecasting is a probabilistic forecasting method that can not only obtain the deterministic forecast value but also give the uncertain information of hydrological forecast since the 1990 s with the improvement of computer computing ability and the development of meteorology an ensemble forecasting system has been used by european centre for medium range weather forecasts ecmwf molteni et al 1996 and national centers for environmental prediction ncep toth and kalnay 1997 based on the idea of ensemble forecast ensemble streamflow forecast was first introduced by the national weather service nws and has since attracted much attention day 1985 compared to a single deterministic forecast of medium and long term forecasting the ensembles of the deterministic forecast has been proved to be beneficial which provides a series of reasonable answers and produces greater consistency and reliability of the true probability distribution of forecasts hagedorn et al 2005 pappenberger et al 2008 xu et al 2020 the ensemble forecast results can also extract single value deterministic forecast results corresponding to the best forecast by fusing multiple forecast modes which performs better than single model simulations did arsenault et al 2015 najafi et al 2012 troin et al 2021 ensemble forecasts can be composed of multiple sets of deterministic forecast results or generated randomly through interval forecasting the difference between the ensemble forecast method is that the uncertainty of the former depends on the deterministic forecast accuracy and the uncertainty interval of the latter depends on the specific algorithm model for example regonda et al 2006 proposed a multi model ensemble forecast framework for streamflow forecasts taormina and chau 2015 used the lower upper bound estimation lube method and multi objective fully informed particle swarm mofips optimization algorithm for ann based interval forecasting of streamflow discharges according to the sources of uncertainties in the hydrological forecasting process ensemble streamflow forecast methods can be divided into multi input multi model and multi parameter ensemble forecasting results however this interval forecasting method is greatly affected by given models and algorithms and can not take into account the uncertainty of inputs and parameters for streamflow forecast models limited by the data requirements complexities and intensive computation of physical models kaya et al 2019 data driven statistically based models are becoming popular especially in the medium and long term zhang et al 2015 in general the methods are simple computing rapidly and capable of forecasting non linear streamflow processes without a full understanding of the physical mechanism involved awchi 2014 wu and chau 2013 various data driven methods using artificial intelligence ai techniques such as artificial neural networks ann support vector machine svm random forest rf and long short term memory lstm have been used to model complex hydrological studies fu et al 2020 le et al 2021 shijun et al 2020 wu et al 2009 although many such models have been researched for hydrological forecasting no conclusion has been reached on which model is optimal for streamflow forecast at the same time complex hydrological processes are presented by relatively simple parameters which cause distortion and inevitably lead to systematically overestimating or underestimating flows during the dry or wet seasons hashino et al 2007 wang et al 2019 regardless biases from the forecast model could degrade the ensemble by fusing multiple model characteristics therefore it is widely acknowledged that an ensemble forecast is much better in providing insight into the uncertainty of any forecast beven 1993 regonda et al 2006 we also noted that an ensemble streamflow forecast can consist of multiple models or models with the different ensemble of parameters bourgin et al 2014 herr and krzysztofowicz 2010 kasiviswanathan et al 2016 regonda et al 2006 wang et al 2019 georgakakos et al 2004 attributed the advantage of multi model ensemble forecasting to considering the uncertainty of model structure in addition to multi model and multi parameter the performance of the ensemble streamflow forecast is still affected by the input data statistics based streamflow forecast takes observed streamflow as the only predictor to forecast the hydrological process in essence this model adapts itself better to the nuances of historical data troin et al 2021 however as the only input data the historical runoff streamflow series shares the characteristics of nonlinear and non stationary under the disturbance of multiple uncertainties similar to natural signals with multiple cycles amplitude and phases qian et al 2019 therefore it is necessary to introduce signal processing techniques for preprocessing the streamflow time series which is suitable for analyzing hydrological time series due to the combined stochastic and non stationary features nourani et al 2014 the common signal processing techniques in hydrology are mainly developed based on fourier transform ft such as wavelet analysis wa singular spectrum analysis ssa and empirical mode decomposition emd these decomposition methods can reveal the local characteristics of the time series from time domain and frequency domain gaucherel 2002 wa has high resolution and its essence is fourier transform with an adjustable window while it is required that the signal in the window must be stable which often produces false harmonics tewfik et al 1992 ssa is modeless which neither requires a stable time series nor assumes a parametric model vautard and ghil 1989 it faces the challenge of how determining a threshold to distinguish between signal components and noise components emd is an experience based adaptive data processing method with a high signal to noise ratio huang et al 1998 subsequently emd derivative methods were proposed to overcome the mode mixing problem such as ensemble empirical mode decomposition eemd and complete ensemble empirical mode decomposition with adaptive noise ceemdan ceemdan is a new noise assisted data analysis algorithm which effectively compensates for the missing scale of the original signal for non linear and non stationary time series torres et al 2011a in the existing research there are some coupling models of decomposition and regression for hydrology simulation and forecast for example rajagopalan et al 1998 kwon et al 2007 and erkyihun et al 2016 adopted a multivariate frequency domain approach autoregressive wavelet decomposition and wavelet based time series bootstrap model for climatic forecasting rainfall and streamflow simulating respectively wu et al 2009 and wu and chau 2013 combined ssa with auto regressive moving average arma k nearest neighbors knn ann crisp distributed artificial neural networks cdann and support vectors regression svr for prediction of streamflow and rainfall wang et al 2021 proposed an ensemble hybrid forecasting model for annual runoff based on sample entropy secondary decomposition and lstm which includes extreme point symmetric mode decomposition esmd and wa zhu et al 2016 wang et al 2018 wang et al 2020 ghasempour et al 2021 combined support vector regression svr autoregressive integrated moving average arima ann gaussian process regression gpr forecast models and emd eemd for streamflow forecasting respectively the results show that the decomposition based forecast models exhibited better statistical performance the techniques by decomposing time series into multiple layers the trend and harmonics of hydrological time series may be extracted and the signal noise eliminated from the series huang et al 1998 torres et al 2011a on the one hand it can provide cleaner input for ensemble streamflow forecast to reduce the uncertainty of input data on the other hand it could expand the ensemble size of ensemble streamflow forecast the effectiveness and accuracy of an ensemble forecasting system are affected by both input and model uncertainties a good ensemble forecasting model needs to consider the uncertainty of input model structure and parameters with good performance in accuracy less uncertainty and less uncertainty good precision and objective set quantity ensemble size existing ensemble forecasting models face the following challenges i improve the forecast accuracy to some extent ii control the range of uncertainty and iii expand the ensemble size meanwhile three key problems need to be solved how to provide effective input how to build reasonable forecast models and how to select suitable model parameters therefore it is necessary to propose a new method to improve the effect of ensemble streamflow forecast the main goal of this study is to develop a novel ensemble forecasting method for hydrological forecasting in this regard we have set 4 objectives for this study which are 1 to provide clear inputs which present significant cycle and trend characteristics without noise for ensemble streamflow forecast by signal decomposition method which transforms non stationary original streamflow to stationary sub time series 2 to test the applicability of different artificial intelligence algorithms with different parameters in forecasting decomposed subseries 3 to compare the accuracy and uncertainty of developed ensemble model with single models and traditional ensemble model and 4 to explore the sensitivity of different decomposed components for ensemble streamflow forecast model its practicability is then verified through a case study of the monthly streamflow ensemble forecast of the yalong river basin it should be noted that in this study the classical forecast models ann and ls svm are adopted as benchmark methods in forecasting decomposed subseries which may not be the optimal choice the purpose of the choice is to get rid of the influence of forecast accuracy improvement due to the advantages of the algorithm itself the novelty of this new model lies in the inclusion of ensemble forecast based on streamflow periodic components that contribute to obtaining more accurate forecast results with less uncertainty and effectively expanding the ensemble size of streamflow 2 methodology 2 1 ensemble streamflow forecast framework based on decomposition in this study a decomposition ensemble forecast framework is developed to increase the accuracy of streamflow forecast and overcome the limitation of traditional multi model ensemble forecast as shown in fig 1 the developed decomposition based multi model and multi parameter dmp ensemble streamflow forecast framework is summarized in the following steps step 1 streamflow series decomposition using the decomposition method the original streamflow time series are composed into several stationary subseries and one residue trend series which have different frequencies step 2 forecast modes selection in this study the multi modes involved multi model and multi parameter ann and ls svm are selected as multi models to forecast decomposed components of streamflow because each model involves many parameters the results are computed for multiple possible combinations of parameters moreover the results of all combinations within a prescribed performance are selected as admissible constituting the pool of candidate parameters step 3 dmp forecast of decomposes series different models with candidate parameters are used to forecast the decomposed streamflow subsequences and multiple sets of forecast results are obtained for each subsequence as subseries forecast sets step 4 dmp ensemble forecast select a forecast result from each layer subseries forecast sets to combine as an ultimate streamflow forecast results and then set all the combined results as the final dpm ensemble forecast results step 5 forecast performance evaluation traditional statistical modeling involves picking up the best model to forecast however it neglects model inadequacy and increases the associated risk of a model regonda et al 2006 the model averaging which combines candidate models into a deterministic forecast result by mean value is proposed to alleviate this problem reid 1968 the accuracy of ensemble forecast is based on averaging result which is evaluated by the taylor diagram the uncertainty of the ensemble forecast based on ensemble result intervals which is assessed by three uncertainty criteria in this dmp ensemble forecast framework non stationary streamflow is decomposed into stationary series with various frequencies by decomposition method multi forecast modes are used for decomposed sub streamflow series forecast integrating multiple forecast results of every subseries as integrated streamflow ensemble forecast and forecast analysis of the subseries are performed using multi evaluation criteria 2 2 the decomposition method ceemdan is a spatio temporal analysis method the theory of it is that additional finite amplitude white noise in pairs are uniformly distributed in the entire time frequency space of the original signal and the space is composed of varied scale components in different frequencies the method contributes to the reduction of residue error in the reconstruction process by adding white paired noise which includes positive and negative signals and obtains components with less noise and more physical meaning yu et al 2013 compared with emd eemd and ceemd ceemdan not only solves the problem of the mode mixing but also increases the stability and reduces the residue noise and computational cost moreover it achieves a same number of ensembles under a tolerable level of noise colominas et al 2014 torres et al 2011a it should be noted that the final decomposition results can be obtained by adding enough white noise is added into the original data to each test and eliminating the noise by calculating the average value of entire tests then remaining components as part of the original signal which contain correct physical significance the same frequent components are gathered into one intrinsic mode functions imfs this new method makes full use of the white noise statistical characteristics to disturb the original signal and then eliminates its influence the processes of decomposition a given signal x t are described shown as follows torres et al 2011a in the steps assume ek t is the kth mode obtained by emd w i is defined as additional white gaussian noise with a mean value equal to 0 and variance 1 ε k ε 0 s t d r k k 1 ε k ε 0 s t d x s t d e 1 w i 0 ε 0 is the reciprocal of the initial noise and the ideal signal to noise ratio snr of the original series then x i x w i step 1 decompose x i x t ε 0 w i t by emd nr realizations to obtain the first mode and compute using eq 1 1 im f 1 t 1 nr i 1 nr im f 1 i t where i is ith realization nr is the maximum number of realizations step 2 at the first stage k 1 the first residue r 1 t presents as in eq 2 2 r 1 t x t i m f 1 t step 3 decompose r 1 t ε 1 e 1 w i t i 1 2 n r until the second mode is defined as eq 3 3 im f 2 t 1 nr i 1 nr e 1 r 1 t ε 1 e 1 w i t step 4 for k 2 3 k the kth residue calculated by eq 4 4 r k t r k 1 t i m f k t step 5 decompose realizations r k t ε k e k w i t i 1 2 n r the second mode is presented as eq 5 5 im f k 1 t 1 nr i 1 nr e 1 r k t ε k e k w i t step 6 go back to step 4 for the next k until the residual meets one of the following conditions 1 the residual is no longer to be decomposed by emd 2 the imf condition is satisfied 3 the number of local extreme points is less than 3 after reconstruction the final residuals satisfy the following requirements 6 r t x t k 1 k im f k t 2 3 forecast framework 2 3 1 artificial neural networks ann ann is a common artificial intelligence technology that is a biologically motivated algorithm maier and dandy 1996 mcculloch and pitts 1943 it is also an abstract simplified and simulated mathematical model which is interconnected by multiple neurons ann has characteristics of large scale parallel processing distributed information storage good self organization and self learning ability the advantage of ann is that it has the ability to recognize the signal distortion and noise without consideration of the complex underlying surface information and accurate description of the hydrological process mechanism only through the training of samples it can effectively predict the complex input output relationship in the hydrological time series process therefore it plays an important role in hydrological forecast fields including reservoir inflow forecasts yang et al 2017 water quality parameter prediction maier and dandy 1996 flood forecasting elsafi 2014 precipitation prediction luk et al 2000 in particular the error back propagation training algorithm bpnn as a three layer feed forward neural network is one of the most widely used neural network models rumelhart et al 1986 bpnn can learn and store a large number of nonlinear and densely mapping relationships between inputs and output the topological structure of the bpnn model includes input layer hide layer and output layer hecht nielsen 1988 has proved that the three layer network can simulate any complex nonlinear process the functions f and g are used to connect the inputs to hidden neurons and the hidden layer to the outputs as shown in equations 7 and 8 respectively 7 h j f i 1 n w ij x i b j 8 o k g j 1 m v jk h j c k where hj is the jth neuron of the hidden layer n is the total number of inputs m is the number of neurons xi is the ith input wij is the weight assigned to xi to calculate the jth hidden neuron bj is the bias of jth hidden layer i 1 2 n and j 1 2 m ok is the kth result of the output layer vjk is the weight assigned to hi ck is the bias of the kth output layer l is the number of outputs j 1 2 m and k 1 2 l in this study the activation functions f and g were tangent sigmoid and linear functions respectively moreover the levenberg marquardt algorithm was used in the training model by adjusting the weight and bias hagan and menhaj 1994 wang et al 2021 it is noted that in order to improve the efficiency of the training network the input variables should be normalized to ensure that the magnitude of the variables is consistent the normalization formula is as follows 9 9 x i n o r m a l i z e d x i x i min x i max x i min where xi normalized is the result of input variables normalization xi min and xi max are the minimum and maximum value in input variables xi respectively 2 3 2 least squares support vector machines ls svm the ls svm model is reformulation to the standard svm vapnik 1995 which was proposed by suykens and vandewalle 1999 and is one of the most widely used svm methods it is closely related to regularization networks and gaussian processes and alleviates the computational burden of svm balabin and lomakina 2011 a set of linear equations by ls svm instead of a quadratic programming problem it has better accuracy for nonlinear signal processing zhu et al 2010 compared with ann it can overcome the shortcomings of long training time the randomness of training results and overfitting the key to ls svm is finding a linear function f x which could describe the relationship between the given inputs vector x x 1 x 2 x n and target series y y 1 y 2 y n as follows 10 f x w φ x b where w and b are the weight vector and bias term respectively φ x is the nonlinear mapping function by transforming the input vector into a high dimensional feature space maity et al 2010 the optimization equation is a quadratic programming problem which can be estimated by a lagrangian multiplier and the ls svm model can be expressed as 11 f x sign i 1 n y i a i k x x i b where a i is the lagrange multipliers k x x i is the kernel function and used to change the dimensionality of the input space which is critical to the performance of ls svm the common kernel functions include the linear polynomial and radial basis function rbf as shown 12 k x x i x x i k x x i γ x x i r d k x x i exp γ x x i 2 where d represents the degree of the polynomial terms r is a residual term and γ the kernel specific parameter 2 4 forecast skill evaluation by statistical methods to conceptualize and abstractly describe the hydrological processes it inevitably leads to distortion and uncertainty in ensemble forecast which integrates multiple subseries multiple models and multiple parameter sets how to effectively assess the ensemble forecast uncertainty is key for analysis rationality and availability of forecast results the evaluation of the model performance has often been conducted using intuitive graphical representation as well as statistical measures in this study nash sutcliffe model efficiency nse relative error re root mean square error observations standard deviation ratio rsr and taylor diagram are employed to evaluate the accuracy of streamflow forecast containing ratio cr average band width bw average asymmetry degree ad continuous rank probability score crps and brier score and skill score bss are as criteria for measure the forecast uncertainty 2 4 1 nash sutcliffe model efficiency nse nse is the generalized version of r squared from parametric regression which indicates the relative magnitude of residual variance and observation data variance nash and sutcliffe 1970 13 nse 1 i 1 n q i obs q i f 2 i 1 n q i obs q ave obs 2 where q i obs and q i f represent the observed and forecast inflow series respectively during the ith month q ave obs is the means of the observed values and n is the total number of original observations as well as the number of months the closer nse is to 1 indicates less bias and a better fit 2 4 2 relative error re re is known as the relative percentage error which is computed by the following normalized average 14 re 1 n i 1 n q i obs q i f q i obs 100 it is valuable for estimating over or under simulation typically offering an indication of bias he et al 2017 the closer re is to 0 indicates less bias and a better fit 2 4 3 root mean square error observations standard deviation ratio rsr rsr is a model evaluation statistic which uses observations standard deviation sd to standardize root mean square error rmse rsr is the ratio of the rmse and the standard deviation of measured data and is calculated with the following equation moriasi et al 2007 15 rsr i 1 n q i obs q i f 2 i 1 n q i obs q ave obs 2 rsr includes a scaling normalization factor and incorporates the benefits of error index statistics which can be applied to various constituents chen et al 2012 rsr ranges from 0 to the lower the better the model simulation performance 2 4 4 taylor diagram taylor diagram is devised by taylor 2001 which is a visual and concise statistical framework for quantifying the differences the degree of uncertainty and expected agreement between model simulated results and observations in terms of their correlation coefficient corr sd and rmse the diagram is suitable for comparing the relative skill and advantages of an ensemble of various models or evaluating complex models with different parameter sets there are some points representing the results of different models and a reference point corr 1 sd 1 and rmse 0 in the taylor diagram the distance between the reference point and model results points indicate the performance of simulation results the closer the points to the reference point the more satisfactory performance in this study the taylor diagram is employed to compare and analyze the characteristics and uncertainty of the single models the multi model multi parameter ensemble models and the multi subseries multi model multi parameter ensemble models 2 4 5 containing ratio cr the containing ratio cr is an index that describes the ratio of the number of observed values enveloped by uncertainty intervals of total forecast streamflow value expressed as a percentage it is widely used for quantifying the goodness of the forecast intervals since the glue method emergence beven and binley 1992 freer et al 1996 the larger the cr value is the greater proportion of observation values fall within the forecast uncertain intervals is the detailed formula is as follows 16 cr n in n 100 where n in is the number of observed points falling within forecast uncertainty interval 2 4 6 average band width bw the average band width is defined as the average bounds of uncertainty intervals which is the difference between the upper and lower values of forecast value sets blasone et al 2008 olsson and lindström 2008 the narrower the bw the less the uncertainty which means the most important information is captured and the forecast result is credible as shown in the equation 17 17 bw 1 n i 1 n q i upper q i lower where q i upper and q i lower are the upper and lower values of the ith forecast points respectively 2 4 7 average asymmetry degree ad average asymmetry degree which is denoted by ad is an accurate and robust estimator of streamflow forecast uncertainty wang et al 2019 xiong et al 2010 due to the nonlinearity of the hydrology process it is satisfactory that the asymmetry degree of streamflow forecast intervals is as small as possible which means the closer the average value of the upper and lower forecast points at each time step are to the observed value the index is defined as 18 ad 1 n i 1 n q i upper q i obs q i upper q i lower 0 5 2 4 8 brier score and skill score bss bss is the proportion of improvement of accuracy over the benchmark forecast using some measure of accuracy brier score bs given generically by wilks 2011 19 bss bs b s bench b s bench where bs is the accuracy measure of the ensemble forecasting system qf against observations bsbench is the accuracy measure of the benchmark forecast q bench f against qf 20 bs 1 n i 1 n q i f q i obs 2 4 9 continuous rank probability score crps the crps is one of the most recommended scores for the evaluation of overall hydrological ensemble forecast performance harrigan et al 2018 it is the integral of the squared difference between the cumulative distribution function cdf of the forecast ensemble and the observation 21 crps 1 n i 1 n f i q i f i q i f q i obs d x where f is the cdf of forecasts at time step i th thepfcst forecasts at time step t the heaviside function h is calculated as follows 22 h 1 0 q i f q i obs q i f q i obs 3 data and parameters description 3 1 study area yalong river watershed which is the largest tributary of the jinsha river is a typical mountain valley river it is characterized by rich water resources good regulation performance less submergence loss and superior economic indicators especially in the middle and lower reaches of the watershed jinping first stage hydropower station jp ⅰ is the headwater reservoir for controlling cascade hydropower stations the middle and lower reach of yalong river as shown in fig 2 whose operation level can directly determine the water supply guarantee benefit and flood risk management of the whole downstream cascade reservoirs xu et al 2016 jp ⅰ reservoir is the annual regulating reservoir was selected as the study area because large inflow forecast errors inherent have resulted in the water level of the reservoir often fluctuating beyond the normal range ma et al 2021 the basin above the reservoir covers an area of 1 03 105 km2 accounting for 75 4 of the yalong river watershed jp ⅰ reservoir has an annual mean inflow of 1220 m3 s accounting for 8 6 of the total water in the upper reaches of the yangtze river its normal storage water level is 1880 m and corresponds to capacity of 7 76 109 m3 and the storage water is primarily employed for hydropower generation it also has comprehensive benefits of flood control sand blocking and tourism its annual runoff mainly comes from the flood season june october comprising above 75 of the total runoff and the rest from the non flood season november may the monthly streamflow which is the inflow data of jp ⅰ reservoir covers the period of january 1953 december 2011 59 years or 708 months and was retrieved from the yalong river hydropower development company ltd 3 2 model inputs and parameter settings it is common that long term streamflow forecast base on the same period data which means that the previous years of one month series as inputs of that month forecast model therefore 12 sub models need to be built for the months of the year for each model the lead time is one month the comprehensive overall lead time can be up to one year 12 months in these sub models the data of 1953 2001 approximately 80 of data 2002 2011 about 20 of data are set as calibration and validation periods respectively in the ceemdan model nr was set to 100 the maximum number of sifting iterations was set to 500 and the amplitude of white noise was set to 0 2 times the standard deviation of the sample data which is optimal parameters set determined by trial and error these settings are similar to those employed by torres et al 2011b dmp forecast models are used in ensemble forecast and ls svm and ann models are mainly used in the model for ls svm model the rbf linear and polynomial kernel functions with different penalty coefficients c and gamma γ combine multiple parameter sets at the same time the hidden nodes of ann from 4 to 20 combine different iterations to make trial and error finally several groups of parameters with satisfied fitting performance are selected such as table 1 moreover the same parameter sets are also used for all decomposition series 4 results in this section the developed ensemble forecast framework was applied to forecast the inflow of jp ⅰ reservoir and the result are demonstrated the periodic characteristics and trend of inflow of jp ⅰ reservoir are analyzed by ceemdan method then the simulated reservoir inflow are compared with observation under multi model with different parameters moreover the influence analysis of different components of decomposition on ensemble forecast results are described the detailed results are as follows 4 1 the decomposed results ceemdan was employed to decompose the integrated 12 monthly inflow series into five components of periodic oscillations imf1 imf5 and a residue trend the decomposed levels with different cycles and amplitudes are arranged from the highest to the lowest frequency and become more and more stable there is the greatest amplitude and frequency in first layer on the contrary the residue has the slowest change and shows the trend of the original time series fig 3 shows an example of the decomposed components by ceemdan of january low water period and july high water period as shown in fig 3 the decomposed series has obvious periodic and the frequency gradually decreasing which provide clear inputs for forecast models the periods represent different levels of streamflow from interannual to multidecadal from this figure the distance between two adjacent highest points or lowest points is a period and a time subseries contains multiple cycles thus the average cycle can be calculated according to the time series length for example the imf1 series in fig 3 a contains 22 complete cycles from 1955 to 2010 so the average cycle of imf1 is 2 5 years in the same way the inflow decomposed series of january include about average 2 5 year 6 year 10 year 14 year and 40 year cycles and the trend is to go down first 1950 s 1980 s and then up 1980 s 2010 s for july in fig 3 b imf1 to imf5 have obvious periodic variations with 2 5 year 6 year 10 year 20 year and 40 year periods and a gradual downward trend is evident for all 60 years the inflow of the other months is also decomposed into similar cyclical characteristics therefore the monthly inflow series of jp ⅰ reservoir includes 2 3 years 5 6 years 7 10 years 12 20 years and 35 40 years periodic variations which is basically consistent with the existing research results for the trend term of jp ⅰ reservoir inflow the overall trend is slow growth the trends of july and august are continuously declining from 1953 2010 a steady growth for march may june and october trends and other month trends reveal decrease to the 1980 s and then increase overall the inflow series has no significant change during the period and the result is consistent with li et al 2015 xu et al 2008 at the same time it is found that the variation range of inflow trend in flood season is large especially from may to september and the trend change in other months is not significant according to the literature research the runoff of the yalong river is mainly driven by precipitation snow melting and groundwater which in the tibetan plateau li et al 2015 li et al 2014 especially in the wet season the runoff is primarily affected by rainfall factors precipitation change is roughly the same as the runoff and the intensity distribution of each time scale is also consistent precipitation and snowmelt are related to solar activities sunspot number ssn sea surface temperature sst north atlantic oscillation nao etc which is the external driving force of climatic change li et al 2020 zhang et al 2007 at the same time the existing research points out that besides rainfall temperature and human activities play an auxiliary and promoting role especially in winter li et al 2018 sun et al 2017 4 2 the results of single method and decomposed forecast method in this study ann and ls svm with different parameter sets are employed in original and decomposed inflow series forecasting nse re rsr and taylor diagram is used to evaluate the accuracy of different forecast models the forecast period of these forecast modes from january 2002 to december 2011 the lead time is 12 months the comparison of single and decomposition forecast results and observation values is shown in fig 4 and fig 5 and the evaluation of forecast results is in table 2 and fig 6 fig 4 a and b respectively illustrate the comparison between the forecast results of the original inflow series and the decomposed series by ann and ls svm methods with different parameter sets and observation of 2002 2011 jp ⅰ monthly inflow from the figures it is obvious that the simulated results are basically consistent with the observed value however ls svm has better performance than ann for different parameter sets ls svm and ann forecast both overestimate in flood season and underestimate in non flood season while the ann method forecast performance is more random from fig 5 a and b the typical scatter plots depict the forecast quality of modes under different inflow magnitude conditions as displayed under low water conditions 1000 m3 s the forecast quality of ann and ls svm forecast modes are high with small deviation especially the ls svm method obviously the forecast results have and the forecast points of ann based forecast modes are distributed evenly on both sides of the 45 line with larger deviations than the results of ls svm forecast modes which means the under forecast of flow whereas most forecast points of the flood period of ls svm method fall above the 1 1 line which confirms the inflow in this period is overestimated in addition the points of decomposition based forecast modes match with the red line the hybrid method produced slightly better performance compared to the single forecast modes from table 2 it is easily found that the hybrid models with ceemdan perform well in nse and rsr indicators except for ann1 and ann4 on the contrary the hybrid svm models show larger deviation with higher re than single svm models in the taylor diagram fig 6 the performances of multi forecast modes are evaluated by corr sd and rmse it is found that the decomposed forecast modes result points closer to the reference point than single forecast models which mean the forecast modes base on the decomposition method have more satisfactory result than single forecast models similarly ls svm forecast modes outperform the ann forecast modes at the same time the result points of ls svm with different parameters are relatively concentrated so the parameter sets insensitive to ls svm results in summary the decomposition method of ceemdan contributes to improving the forecast accuracy 4 3 dmp ensemble forecast results in this study two ensemble forecast methods are involved which are based on the selected multi model multi parameter and multi subseries multi model multi parameter modes respectively the uncertain results of the monthly forecast inflow of the jp ⅰ reservoir can be seen in fig 7 the black solid line and red dotted line represent the monthly observed and mean simulated inflow of ensemble forecast modes the green polygons indicate the uncertainty intervals and the wider the interval the greater the uncertainty table 3 further presents the forecast skill evaluations of two ensemble forecast results cr is a measure of the ability of the forecast interval to contain real information the larger cr the more real information the prediction interval contains as illustrated in table 3 the cr of traditional multi model and multi parameter ensemble forecast models tmp is 47 while the cr of the dmp models is 55 the dmp model contains more real information and has a stronger forecasting ability dmp is superior to tmp in crps which can measure the reliability and concentration of prediction the smaller the crps is the better the probability forecast performance is although the bw is smaller by tmp there are higher cr 55 lower ad 0 69 bbs 0 15 and crps 2 1 1 for the dmp method overall dmp could yield more accurate forecast results with smaller uncertainty the low cr less 50 produced by tmp is primarily due to the obvious under forecast of the peak flow of flood period as demonstrated in fig 7 a tmp has slight advantages in bw nevertheless the dmp ensemble size has increased by orders of magnitude compared with the tmp method reaching 2 2 1042 forecast results set which solves the problem of insufficient ensemble size by closely observing fig 7 a and fig 7 b it is not difficult to find that the dmp ensemble forecast mode has better coverage of the observations with comparable forecast intervals from table 2 we can see that the dmp has advantages in nse re and rsr performance over the traditional ensemble forecast method it can also be verified that the mean ensemble forecast results perform better than the single model in forecasting and the ceemdan contributes to obtaining more accurate ensemble forecast results fig 6 further shows the blue dot dmp is closer to the reference point red dot than the purple dot tmp which indicates that the dmp can yield appreciable results in corr sd and rmse in general these results support that dmp outperforms the tmp method in terms of uncertainty see green polygons in fig 7 and accuracy of mean ensemble forecast inflow see red lines in fig 7 in a wide range of flow conditions the intervals of the two methods vary with the magnitude of the inflow the flood periods have larger uncertainty than the low flow zone during the flood periods the flow is overestimated and underestimated by dmp and tmp but dmp is slightly better than tmp in the expression of double flow peaks of high flow conditions 5 discussion 5 1 the influence of involved different subseries dmp ensemble forecast it is well understood from the above analysis that the dmp ensemble forecast method could achieve high precision and low uncertainty forecast results due to the number of ensemble reaching 2 2 1042 groups based on composed six subseries the computation burden is heavy therefore this paper explores the influence of the number of involved decomposed subseries on dmp ensemble forecast results to release calculation computing complexity building model test and setting up different involved subseries to analysis due to the large amplitude and high frequency of the first layer subseries imf1 the number of subseries involved in dmp forecast is increased based on the imf1 and compared with the dmp without subseries involving random combination thus 7 schemes are set respectively to analysis influence of ensemble subseries from the single layer of imf1 to total six imfs involved in the ensemble forecast the results are illustrated in table 4 and fig 7 the results show that compared with the tmp the accuracy and uncertainty of the different schemes of dmp ensemble forecast are relatively satisfactory except for ad the scheme nd dmp outperforms the tmp in uncertainty with higher cr less bw and the same ensemble size at the same time when the number of involved subseries in dmp are greater than 2 the uncertainty indicators cr bw ad and the ensemble size are significantly better than tmp the advantages of cr ad and ensemble size become more and more obvious with the increase of the number of imfs involved in dmp in contrast bw will also increase schemes 5d dmp and 6d dmp have comparable uncertainties however the ensemble size varies by orders of magnitude it can be inferred that different subseries have different effects on forecast uncertainty of dmp which will be discussed in section 4 5 therefore the results suggest that dmp might be more efficient in more accurate and lower uncertainty forecasting than tmp when the subseries involved in the ensemble are greater than 2 5 2 sensitivity analysis according to the above results the sensitivity of different subseries to dmp ensemble forecast results is further analyzed the sensitivity analysis is mainly through comparing the different decomposed subseries forecast results by multi model and multi parameter forecast modes if the deviation of the subseries forecast results is large it shows that forecast models have a significant influence on the decomposed subseries at the same time it can be explained that the decomposed subseries is more sensitive to dmp ensemble forecast results by analyzing the dmp ensemble forecast results of different involved subseries it can be seen from fig 8 that the difference of imf1 forecast results is the largest which using multi model and multi parameter forecast modes followed by imf2 imf3 and the forecast results of imf5 and residue item are basically no difference from fig 8 a c the higher the frequency and the greater the amplitude of the subseries are sensitive to the forecast model and the deviation of the forecast results are larger as shown in fig 8 a c while the dmp ensemble forecast results with lower frequency subseries trend to be consistent as shown in fig 7 d f combined with table 4 it can be concluded that the high frequency subseries are highly sensitive to ensemble forecast and when the top 2 layers of high frequency series are involved in ensemble forecast more satisfactory results can be expected 5 3 ensemble forecast analysis in flood season according to the analysis in section 4 it can not be seen that the forecast effect of either tmp or dmp method is limited mainly due to the poor peak forecasting in flood season in this study the ensemble forecast framework is based on monthly simultaneous forecasting model which is common in long term streamflow forecast the streamflow of non flood season is mainly affected by the climate change which varies by years on the contrary the streamflow in the flood season is mainly controlled by the previous streamflow with frequent changes and large amplitude this forecast mode is no longer applicable therefore we attempt to use the dmp method to continuously forecast the streamflow which puts the previous streamflow as input the accuracy and uncertainty of continuous ensemble forecast are shown in fig 5 there is satisfactory performance in deterministic and ensemble forecast compared with the monthly forecast table 5 the continuous forecast has a higher accuracy and its nse is as high as 0 95 re is 18 63 slightly inferior to the performance of monthly forecast meanwhile continuous ensemble forecasting has less uncertainty especially the bss is small to approximately 0 it can be seen that in the actual forecasting work it is necessary to adopt different methods to forecast in wet and dry periods 5 4 limitations of the present model although the dmp framework successfully tested the performance in accuracy uncertainty and ensemble size of ensemble forecasting but several questions still remain to be answered firstly the research has shown that different models with different parameters have great differences in forecast performance in the present model the contribution of multi models with different parameters to ensemble forecasting is the same therefore it is necessary to give weights to different model weights in the follow up research secondly the performance of different decomposition methods forecast models and parameters on the ensemble forecast are quite different the proposed dmp method is not limited to the methods and parameter settings given in this study it deserves further study and be customized according to the characteristics of different study areas meanwhile streamflow is also related to climate change weather conditions and human activities which should be taken into account in the dmp framework thirdly according to the results of this paper the ensemble size is quite large by dmp method however the random combination of high frequency subseries have little significance for ensemble forecasting in the face of a large ensemble size whether there is redundancy in the ensemble forecast results and how to release the computing burden are worth discussing in deeply 6 conclusions ensemble forecasting plays a significant role in improving the accuracy of hydrological forecasting reducing the uncertainty of forecasting and increasing the availability of hydrological forecasting results however the ability of ensemble forecasting is affected by the uncertainty of input data and model parameters and the limited ensemble size in this study a decomposition based multi model and multi parameter ensemble forecast framework for monthly streamflow forecasting is proposed which integrated ceemdan method with ann and svm models with different four parameter sets the aim of the present research was to examine the effect of multi models and multi parameters coupled with data preprocessing techniques in improving the accuracy of monthly streamflow ensemble forecasting results show that the dmp was superior to the tmp in accuracy and uncertainty the inflow of jp ⅰ of yalong river was used as testing cases with the help of the ceemdan multi models with multi parameter showed the best performance when compared with the tmd and single models which as baseline models accuracy monthly streamflow estimates were also obtained from dmp and tmp methods however the methods underestimated the peak flow during flood periods although dry seasons were mostly well forecast moreover this dmp method can increase the ensemble size in the order of magnitude by stochastic integration of forecast subseries so as to effectively solve the problem of the insufficient number of ensemble series some discussions were presented 1 on the influence of involved different subseries dmp ensemble forecast 2 on the sensitiveness of decomposed subseries to dmp 3 on improvement the ensemble forecast in flood season first comparing with tmp the forecast significant advantage when more than 2 high frequency components subseries are involved in dmp ensemble processes second the high frequency subseries with great amplitude were sensitive to the forecast models third continuous ensemble forecast outperformed monthly forecast with higher accuracy and less uncertainties during flood periods the dmp ensemble forecast method in this study is novel in assisting ensemble streamflow forecast on the one hand dmp introduces signal decomposition method and multi model multi parameter forecast modes to reduce the uncertainty of input data and improve forecasting performance and on the other hand dmp contributes to expanding the ensemble size future research should be undertaken to explore better performance in ensemble forecast by advanced decomposition methods and forecast models moreover it is necessary to validate the dmp framework in more watersheds and short scale forecast systems credit authorship contribution statement jia wang conceptualization methodology software validation formal analysis investigation writing original draft visualization xu wang conceptualization investigation resources data curation supervision project administration funding acquisition soon thiam khu resources writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was partially supported by the national natural science foundation of china 52079144 
2239,data related to river velocity and discharge are important for water resource management non intrusive image measurement techniques based on direct cross correlation dcc algorithms such as particle image velocimetry piv are widely used to measure velocity and discharge in the field however environmental noise is highly complex and uncontrollable in the field significantly reducing the accuracy of dcc based methods convolutional neural networks cnns are commonly used in image recognition because of their outstanding accuracy which far exceeds that of conventional imaging methods however these accuracy levels cannot be directly extrapolated for flow movement estimation therefore in this study we developed an innovative sub pixel correction technique that allows cnn based methods to obtain stable measurements in the piv framework this is the first study that successfully applied the concept of cnn to measure velocity data using images the hamel oseen vortex flow uniform steady flow and plane laminar jet flow models are established as benchmark vector fields non uniform illumination and gaussian noise with varying degrees of interference are added to the synthetic data to evaluate the performance of the cnn based method in piv for noiseless images the dcc based and cnn based methods achieve lower measurement errors than benchmark errors for noisy images the dcc suffers a fold error increase ranging from 2 77 to 31 13 whereas the cnn suffers only a fold error increase ranging from 1 25 to 1 68 a 30 m long flume is then used in an uncontrolled environment to mimic real world flow measurement the dispersion of the instantaneous velocity measurements for the cnn is more concentrated than that for the dcc the acoustic doppler velocimetry yields an error of only 7 87 in discharge estimation using cnn these results indicate that the cnn based method is more robust than conventional methods and has the potential to be effectively applied to measurements in the field keywords flow measurement lspiv direct cross correlation convolutional neural network image technique data availability data will be made available on request 1 introduction in situ water data are important for water resource management such as urban planning flood control construction river restoration and flood monitoring field data must be obtained to analyze the discharge for flood monitoring particularly considering the recent increase in the frequency of extreme rainfall events lipper et al 2017 kouadri et al 2021 unfortunately conventional approaches for the measurement of hydrological phenomena have been hindered by insufficient data resolution inability to transmit data in real time and inability to obtain reliable data during storms imaging methods have been developed for flow measurement and their efficacy has been demonstrated in flume experiments adrian 1991 hann and greated 1997 creutin et al 2002 young et al 2015 kim et al 2016 a popular non intrusive imaging method is the large scale particle image velocimetry lspiv which derives flow rates from flow velocity measurements at the surface of river channels fujita and komura 1994 fujita et al 1998 bradley et al 2002 hauet et al 2008 muste et al 2014 ostad ali askari and shayan 2021 this method involves setting up a camera on a riverbank to capture a continuous stream of images of the river surface and derive hydrological data image matching is then performed using direct cross correlation dcc cross correlation matrices can be calculated directly in the spatial or frequency domain using the fast fourier transform fft the fact that the dcc algorithm is used to correlate images in space means that the algorithm imposes no limitations on the size of the sub images interrogation area ia to be compared nonetheless the size of the sub images can have a significant impact on the computation time and accuracy of the corresponding estimates the fft provides notable benefits in terms of computational overhead and suppression of noise related to the inter correlation coefficients fft also increases the density of the velocity field through multiple channels willert and gharib 1991 stamhuis 2006 thielicke and stamhuis 2014 the matching accuracy of the dcc method is significantly influenced by the quality of successive images in indoor flume experiments it is relatively easy to obtain reliable velocity measurements because the experimental conditions can be easily controlled and high quality images can be captured however environmental conditions e g light wind and shadows cannot be controlled in the field despite extensive efforts to eliminate these effects chaves 2012 zhang et al 2013 xu et al 2017 zhen et al 2017 ghashghaie et al 2022 most acquired images contain noise that cannot be entirely removed it is difficult to obtain uniform seeding conditions for particle tracking the inability to control environmental conditions inevitably undermines the accuracy of the data and limits the applicability of imaging techniques for flow measurement there is a pressing need for an algorithm that considers not only brightness but also geometric and abstract features there is also a need for image processing algorithms that do not rely on a stable environment artificial intelligence ai involves using computers to perform tasks that are beyond human cognitive capability in 1958 rosenblatt 1958 published a perceptron machine learning algorithm that was a linear binary classification model the linearity of the perceptron algorithm was a significant drawback that caused the development of machine learning to be hindered until the 1980s which was when the multi layer perceptron algorithm enabled the processing of multiple messages within a network makino et al 1983 hinton 1986 watrous and shastri 1987 morgan and bourlard 1990 in recent years ai and machine learning have been revived with the advent of convolutional neural networks cnns starting with the lenet 5 network in 1995 lecun et al 1998 cnns use images as a direct input thereby eliminating the need for pre processing image features cnns also allow the users to correct the weights of convolutional filters for automatically producing convolutional kernels that enable image recognition neural network weights are combined with kernels to reduce the number and complexity of the parameters while enhancing the tolerance for image distortion rotation and panning cnns have been widely used in transportation agriculture and medicine kagaya et al 2014 hao et al 2018 shang et al 2018 traore et al 2018 özyurt 2020 ma et al 2021 ostad ali askari and shayannejad 2021 xiong et al 2021 the cnn concept was also implemented to predict discharge and velocity based on empirical and numerical models chiang et al 2017 forghani et al 2021 deng et al 2022 karpatne et al 2022 however there is no application for in situ measurements of discharge and velocity with non intrusive imaging methods in this study we employed a cnn as an alternative to the dcc algorithm for the recognition of granular images representing a flow field lspiv is a non intrusive approach for hydrologic flow measurement however it is highly susceptible to environmental conditions cnns have proven to be highly effective in resisting the effects of environmental interference however no previous study has applied cnns in the context of lspiv herein we explored the possibility of replacing the dcc method with the cnn method in lspiv to allow direct extraction of abstract or geometric features for image matching in the simulations we compared both approaches in terms of their accuracy in estimating the flow velocity for various flow scenarios we also compared the approaches in a full scale flume experiment in which the surface velocity fields obtained using the two methods were compared with those obtained using acoustic doppler velocimetry adv for a given cross sectional surface 2 experimental methods 2 1 concepts underlying the dcc method the standard operating procedure for particle image velocimetry piv is to create a grid point on the image sequence to be analyzed as the central location of the extracted ia the size of the ia is determined by the density of the particles in the window to ensure that this sub image contains sufficient feature related information for comparison the search area sa is then defined such that the algorithm can find the ia of highest similarity at time t within the periphery of the grid points at t 1 where the velocity is calculated with sub pixel correction the details of the dcc method used to estimate the velocity can be found in appendix a 2 2 design of the proposed cnn scheme piv analysis in the current study was performed using the standard procedure for the definition of regions of interest rois in images and cutting out sub images of a specific size with fixed spaced grid points at time t in the current study these sub images were used as input data for the cnn in accordance with the following definition x 64 64 g t g 1 ñ each sub image was defined as a different object class y g the output of the network was encoded in the form of one hot see fig 1 to generate an array representing class y g the data input to the network must be restricted within a specified range through normalization to avoid slow convergence prevent gradient explosion due to a large difference in data order and ensure trainability we stipulated that the intensity of the pixels in an 8 bit grayscale image must take values between 0 and 255 we also normalized the raw image data to 0 s and 1 s before cutting the sub images in the definition of the network structure for the proposed cnn the characteristics of the cutout sub images included only the arrangement and shape of the particles and the time interval between successive images was short this ensured that the two images did not diverge excessively to ensure that the network met the requirements in terms of piv analysis and computation time we discarded existing network architectures such as vgg net google inception or resnet instead we built the self defined convolutional structure shown in fig 2 we examined two convolutional layers net1 versus four convolutional layers net2 as well as maximum pooling layers and a fully connected layer connected to the output layer the depth of the convolutional layer was used to determine the diversity of the extracted features essentially insufficient depth would lead to the extraction of an insufficient number of features to perform recognition tasks whereas excessive depth would increase the training time we eventually opted for a two layer architecture net1 with various depth combinations as follows 8 16 16 32 32 64 64 128 and 128 256 the number of neurons in the fully connected layer was also varied as follows 256 512 1024 2048 and 4096 we formulated a total of 25 depth number combinations in the fully connected layer and then selected the most robust one based on a comparison with the benchmark we eventually established a convolutional depth of 16 32 with 4096 neurons in the fully connected layer for all subsequent analyses the depth of the first convolution layer was 16 net1 and the depth of the second convolution layer was 32 net2 the depths of the third and fourth convolution layers in net2 were 32 and 64 respectively the last connection layer included 4096 nodes in the output layer after the fully connected layer in the convolution layer we employed a 3 3 convolutional kernel and a 2 2 pooling kernel in step 1 which were connected to the fully connected layer for feature classification the effects of four activation functions were assessed the rectified linear unit relu leaky relu lrelu parametric rectified linear unit prelu and exponential linear unit elu the details of the four activation functions are provided in appendix b a loss function is used to determine the difference between the output and target values in the current study the highly popular cross entropy l i was used to represent the amount of information between the target array y 1 n and network output array y 1 n de boer et al 2005 based on the following equation 1 l i n 1 n y n i log y n i l total i l i where y is the element of the array y n is the array index and number of classes n is the total number of classes and i is the number of arrays a small cross entropy value indicates a small difference and vice versa once the loss function is determined it is necessary to select an optimization algorithm to correct the weights such that the best solution can be achieved with a minimal discrepancy between the target and network output the most common methods employed for deep learning or neural networks are the adam method and stochastic gradient descent sgd in the current study we opted for the adam method to avoid the shortcomings of sgd in optimization the adam method combines the momentum algorithm qian 1999 with the adagrad duchi et al 2011 and rmsprop algorithms thus it corrects errors by adopting a high learning rate at the beginning of training and a lower learning rate in later stages moreover the concept of momentum allows the adam method to increase or decrease the current error gradient based on the previous error gradient thereby enabling more rapid correction in the desired direction and slower corrections in other directions the adam method has proven to be effective in most general cases and provides rapid convergence exceeding that of other optimization algorithms kingma and ba 2014 ostad ali askari et al 2017 weight initialization also affects the training and performance of cnns setting a poor initialization can lead to an uneven distribution of data values transmitted in the neural layer such that the neuron outputs tend to be concentrated around a certain value a recommended weight initialization suitable for different activation functions was applied in this study glorot and bengio 2010 he et al 2015 to unify the dimensions of the network input data the sub images were scaled to the following sizes 32 32 64 64 96 96 and 128 128 in all convolution layers the movement step of the convolutional kernel was 1 px and the initial weights were randomly generated using a normal distribution with a mean value of 0 a standard deviation of 0 1 and an initial bias of 0 01 the softmax function was applied to the output layer for the proposed cnn model we also employed batch training with batch regularization to keep the weights in each convolution layer active thereby stabilizing the performance of the loss function during training and suppressing the network dependence on default values ioffe and szegedy 2015 after training was completed using all the sub images at time t an sa was defined at the corresponding grid point x g t 1 y g t 1 for time t 1 to identify sub image candidates x 64 64 g k t 1 with k in the sa using the sliding window approach all the candidates were input to the network to obtain the output array y 1 n g k as calculated by the cnn for use in obtaining the loss matrix l g the minimum loss value of the loss matrix was based on the position of the matching sub image this made it possible to detect pixel level movement x g t 1 m y g t 1 m for use in the calculation of velocity 2 3 sub pixel correction for cnn the distance represented by a pixel can be scaled for specific applications for imaging applications using large scale photographic images of flowing rivers e g lspiv the length unit corresponding to a pixel in the image is usually on the centimeter scale under these conditions the measured velocity vector is far from the true value unless the measurement resolution is very high sub pixel correction is an approach for enhancing the measurement resolution in conventional piv sub pixel precision corrections are generally obtained via gaussian curve fitting or 2d gaussian fitting willert and gharib 1991 nobach and honkanen 2005 once the peak z max of the correlation matrix r g is found the peak of the curve is located through curve fitting using the values at 1 pixels adjacent to z max which is used to achieve sub pixel accuracy however for the cnn in this study we replaced the correlation matrix r g with a loss matrix l g the distribution characteristics of the two matrices are not the same the peak of the loss matrix cannot be fitted using gaussian curve fitting see fig 3 theoretically matching sub images x g m should be similar to the original image x g such that if one image were laid over the other then only minor particle movements would be observed see fig 4 in the current study we aimed to improve the accuracy at sub pixel level using the average movement of all particles in the sub image at microscopic scale as the displacement of sub pixels particle tracking velocimetry ptv was used to capture the particle coordinates and noise in the images was smoothed out using binomial and binomial laplacian filters with the contrast between the bright center and edge of the particle enhanced as shown in fig 5 this smoothed the noise and suppressed the particle edge intensity to enhance edge performance a quadratic curve was then fitted to the peaks with a post filtering intensity exceeding the threshold value jähne 1995 capart et al 2002 using the following formula 2 x p x i 1 2 i i j 1 i i j 1 i i j 1 2 i i j i i j 1 y p y i 1 2 i i 1 j i i 1 j i i 1 j 2 i i j i i 1 j where i and j represent the coordinates of the pixel with peak intensity after filtering and i is the intensity of that pixel the offset between the quadratic maximum and maximum pixel intensity was calculated to obtain the accuracy of the particle coordinates x p y p at sub pixel level thus by marking the particle coordinates x p y p and x p y p of the matching sub pixel pairs the average difference in particle coordinates in the two images can be calculated at microscopic scale and by adding this movement to the initial results obtained using the cnn the final movement vector δ x g δ y g can be obtained with sub pixel accuracy this approach to particle detection was also applied to determine whether sub images on grid points included particles or features for either method cnn or dcc if a sub image were found to contain no particles or distinctive features that sub image would not undergo image matching regardless of the type of method used it would be helpful to screen out sub images without particles or features numerous sub images in the cnn contained evident particles or features however some sub images did not contain particles or evident features because the grid dots fell in areas with an indiscernible particle distribution allowing such sub images to be trained in the network as separate categories would slow down the speed and effectiveness of training featureless sub images would also tend to generate unreliable or erroneous results when using the dcc method therefore we subjected all sub images to pre screening 2 4 evaluation of measurement results 2 4 1 root mean square error rmse the rmse was used to determine the discrepancy between the estimates of the velocity magnitude and corresponding benchmark values as follows 3 rmse 1 k k 1 k y k y k 2 where y k represents the velocity vectors calculated using the dcc or cnn methods and y k represents the corresponding benchmark vector at each grid point 2 4 2 vector correlation coefficient vcc the velocity vectors estimated using the cnn and dcc methods were compared to the benchmark velocity results using the same grid in order to evaluate the accuracy of both methods based on the vcc assuming that the two dimensional velocity vector obtained using cnn or dcc is w 1 u 1 i v 1 j and that the corresponding benchmark vector is w 2 u 2 i v 2 j then the covariance matrix between both vector fields and the vcc ρ v 2 can be calculated vector fields w 1 and w 2 are completely uncorrelated if ρ v 2 0 however these vector fields present complete linear correlation if ρ v 2 2 when the number of samples in the vector exceeds 64 the distribution of n ρ v 2 approximates a cardinal distribution with four degrees of freedom this makes it necessary to determine whether the chi square test values x 2 9 488 and α 0 05 for the four degrees of freedom are sufficient details of the calculation for the covariance matrix and the vcc ρ v 2 can be found in appendix a 2 5 discharge estimation the goal of using imaging techniques for flow measurement while operating in the field e g lspiv implies to obtain an estimate of discharge quickly and easily the velocity index method commonly used for discharge estimation involves multiplying the depth averaged velocity converted from the surface velocity using an index α and the cross section of the channel in this study we conducted a flume experiment to measure discharge and vertical velocity distributions in order to obtain α from adv measurements the log law velocity profile function was used to fit the vertical velocity data for estimating α the surface flow velocities obtained using the dcc and cnn methods were converted using this index and multiplied by the cross section to estimate the rate of discharge 3 experimental setup 3 1 benchmark images the efficacy of the dcc and cnn methods in measuring flow rates was evaluated under various flow fields and environmental conditions using artificial images as a benchmark generated using the open source software program pivlab based on known true speed values and pixel size thielicke and stamhuis 2014 the first flow field was set as the hamel oseen vortex saffman 1992 using the following equation 4 v θ r t γ 2 π r 1 exp r 2 r c 2 t 5 r c t 4 ν t r c 2 0 where γ refers to the circulation r is the distance from the center of the vortex ν is the kinematic viscosity and r c is the vortex radius as shown in fig 6 a this describes an up down left right symmetrical twin turbine velocity field the use of this velocity field equation made it possible to derive the true velocity vector at each grid point as a benchmark value for comparison in this study we set the pixel velocity magnitude to a value two orders of magnitude smaller than 3 and 5 pix frame the second type of flow field was a plane laminar jet flow see fig 6b the velocity distribution and flow rate were derived using the following equations schlichting 1933 6 u 3 8 π k ν x 1 1 ξ 2 4 2 7 v 1 4 3 π k x ξ ξ 3 4 1 ξ 2 4 2 where k j ρ ξ 3 16 π k v y x j is the momentum flux ρ is the fluid density and ν is the kinematic viscosity the last flow field was a uniform steady flow in which the velocity and flow direction did not change with time or location in this study the velocity was 2 pix frame in the u direction and 5 pix frame in the v direction the flow field is shown in fig 6c 3 2 artificial image conditions the artificial images of particles generated using the aforementioned equations and pivlab presented a high contrast between the particles and a uniform background which means that the dcc and cnn methods should both be able to obtain precise estimates of particle movement however the environmental conditions encountered in the field would likely introduce unexpected errors in flow measurements images obtained in the field are subjected to a range of environmental conditions including uneven brightness and particle density distributions in this study we employed particles of two sizes variations in particle density and particle movement three light sources and noisy signals to mimic the interferences typically encountered in the field the stability and robustness of the dcc and cnn methods were evaluated under these conditions 3 2 1 particle distribution effects the diameter and number of particles were varied to assess the effect of particle density on flow measurement performance the objective here was to determine whether the cnn method could reduce the negative effects of variations in particle density distribution note that even under ideal environmental conditions images captured in the field are affected by the distance between the camera and water surface as well as the specifications of the recording equipment moreover the pixel area occupied by the particles or features in the image is not fixed in most field applications drones are used to capture water surface features thus the resolution of the images is determined by the specifications of the recording equipment and the altitude of the drone above the water i e ground sample distance gsd which can be defined using the following formula 8 gsd h s f where h is the altitude of the drone s is the pixel size in the sensor and f is the optical focal length of the lens we consider the dji phantom 3 drone 4 k version as an example its sensor size is 1 2 3 the optical focal length of its lens is 3 57 mm full high definition recording is performed at 1080 1920 px and the image element size is 0 0037 m the correspondence between altitude h and gsd is presented in table 1 when measurements are performed on a river presenting a range of 30 50 m in width the ground resolution decreases inversely with the altitude for example at an altitude of 30 m the gsd would be 31 24 mm pixel such that an object measuring 3 1 cm would occupy only one pixel in the image note that the leaves are commonly used as a seed for imaging in the range of 5 15 cm2 thus in these simulations the diameter of the particles must be 3 5 pixels to ensure that the artificial images accurately represent those obtained in the real world in terms of seeding density the number of particles determines the degree of sparseness in an image we generated two cases involving 100 000 or 3 000 particles to mimic small scale indoor experiments and large scale field experiments respectively in the high density case 100 000 particles to mimic well controlled conditions the particle diameter was set to 3 pixels and the illumination was uniform in the low density case 3 000 particles we aimed to mimic uneven illumination of the water surface by adding noise in the form of an uneven distribution of the background brightness a total of 22 cases were used to examine the performance of the cnn and dcc methods see table 2 we then conducted experiments involving two particle velocities 3 or 5 px frame in successive images cases v3r1 and v4r1 involved high density particle distributions simulating small scale applications whereas cases v1r1 and v2r2 involved low density particle distributions simulating field conditions in the field uneven distribution of brightness on the water surface can be largely attributed to cloud cover shadowing and water depth overcoming this issue generally requires the removal of background noise through tedious image filtering the results of which are not usually satisfactory therefore we included cases v1r1n and v2r1n which added uneven lighting conditions to the conditions in cases v1r1 and v2r1 table 2 presents the simulation cases 3 2 2 illumination and signal effects we simulated three illumination conditions representing uniform and non uniform backgrounds see fig 7 uniform image illumination involved brightening the background evenly to simulate a lack of reflection in the field non uniform illumination involved adjusting the background brightness to simulate uneven reflections note that the same contrast was used for all artificial images the signal interference caused by the camera sensor and circuitry was replicated by adding gaussian noise at three densities see fig 8 3 3 flume experimental setup the flume experiment was designed to assess the performance of the cnn and dcc methods based on ground truth data collected using a sontek 10 mhz micro adv the channel used in this study was 30 m in length and 1 m in width the camera captured an area of x 8 9 9 6 m the cross section was captured using the adv measured at x 9 27 m see fig 9 a ten ground reference points grps were placed on both sides to facilitate image ortho rectification images were recorded using a 16 megapixel camera sony nex vg30 at a height of approximately 1 5 m from the bottom of the flume without auxiliary lighting video recording was performed using full high definition resolution at 50 fps with a field of view fov of 1 2 0 7 m2 following ortho rectification the image size was reduced to 917 501 px with a resolution of 1 2 mm pixel when analyzing the dcc and cnn methods image pre processing was not conducted the ia was fixed at 64 px and the sa was 12 px extended to the ia see fig 9c the adv sampling frequency was set to 25 hz the water depth of the flume was 0 249 m the measurement positions for adv were at x 9 27 m and y 0 15 0 42 0 58 and 0 85 m see fig 9b for each position we measured 10 points vertically from the bottom to the water surface using the adv to capture vertical velocity profiles for each point we derived the average velocity from the data obtained over a period of 3 min 4 results and discussion 4 1 evaluation of cnn models the framework and settings of the cnn were established using vortex images captured under uniform illumination conditions we then compared the results obtained using the dcc method and various cnn architectures with the benchmark values as shown in table 3 cases v1r1 and v1r2 with ias of 64 64 presented no significant differences between the results obtained using the cnn and dcc methods and the vcc results ρ v 2 2 indicates values that are indistinguishable from the benchmark both cnn architectures presented promising results however net 1 with fewer convolution layers extracted sufficient feature related data for analysis note also that the benefits of adding convolutional layers diminished beyond a certain point even as the computational overhead increased thus we selected the net 1 cnn architecture for subsequent analysis to assess the effects of activation functions we compared cases v1r1 v1lr1 v1pr1 and v1el1 see table 2 with various functions overall the relu function outperformed the others by providing the lowest rmse compared to the benchmark for the u or v velocity components see fig 10 the relu function provides four advantages for deriving flow measurements first it is in line with the transmission of signals by biological neurons which do not respond until a stimulus has reached a certain intensity and sends only weak signals when a given threshold is exceeded glorot et al 2011 second in error transmission the error gradient must be calculated in terms of the bias and transmitted back note that when the relu function is used for the output the error gradient does not disappear third the relu function causes the output of some neurons those with input less than 0 to be zero thereby increasing the sparseness of active neurons in the network which is consistent with biological mechanisms attwell and laughlin 2001 glorot et al 2011 finally relu is simple to compute and imposes a reasonable computational overhead therefore we used relu as the activation function in the cnn architecture for all subsequent analyses the effect of ia was evaluated in cases v1r1 v1i32 v1i96 and v1i128 see table 2 our results revealed that the window size certainly affected the performance of the dcc and cnn see fig 11 the appropriate size for the ia depends on the particle density particle diameter and flow pattern the sensitivity of the dcc method to ia resulted in a wide range of measurement values the cnn method was largely robust to variations in ia however the best results were obtained for a window size of 64 64 the proposed cnn was assessed using various architectures activation functions and window sizes in the following sections we consider a cnn with fewer convolution layers net 1 a relu active function and a 64 64 ia 4 2 impact of environmental noise on cnn and dcc the illumination conditions and particle density are difficult to control in the field thus we conducted experiments to evaluate the robustness of the cnn and dcc methods when using images captured under uniform and non uniform illumination conditions case v1r1 was compared with case v1r1n under a slow flow field whereas case v2r1 was compared with case v2r1n under a fast flow field the results are shown in fig 12 and table 4 overall the cnn generated similar results regardless of illumination under uneven illumination the dcc method was able to capture the vortex but was unable to estimate the velocity magnitude with accuracy particularly when the vectors were close to the center of the vortex when the particle movement was increased v2r1 the results obtained using the dcc method were profoundly affected by the illumination conditions see fig 13 the dcc could barely capture the vortex and the estimated velocities significantly differed from the benchmark values when using the dcc method to analyze a slow flow field the discrepancy between results obtained under uniform or non uniform illumination was 6 43 however when applied to a fast flow field the discrepancy increased to 18 14 see tables 4 and 5 these results indicate that the dcc method is strongly affected by image quality the cnn method proved to be highly robust to image contamination our results indicate that the proposed cnn should be able to overcome the effects of ambient light noise thereby capturing flow fields close to ground truth however under well controlled lighting conditions the cnn provided no advantage over the dcc method in deriving an accurate velocity field the effects of seeding density were assessed in cases v1r1 v2r1 v3r1 and v4r1 table 6 lists the measurements obtained using the methods versus the benchmark values a high seeding density increased the accuracy of the measurements regardless of the computation method overall dcc was more sensitive than cnn to seeding density however both methods provided reasonable measurements regardless of seeding density 4 3 impact of gaussian noise gaussian noise was used to simulate the noise in the images captured under low light conditions these results revealed a positive correlation between the amount of gaussian noise and the discrepancy between the dcc velocity results and ground truth values see table 7 the cnn method was significantly less sensitive to gaussian noise the addition of 35 gaussian noise increased the measurement error as follows dcc 2 77 to 31 13 fold increase cnn 1 25 to 1 68 fold increase overall the dcc method proved to be highly vulnerable to the imaging conditions whereas the cnn method proved to be robust thus it is reasonable to assume that the proposed cnn scheme outperforms the dcc method when it comes to overcoming environmental noise to obtain reliable flow measurements in practical situations 4 4 impact of flow field characteristics the capability of the proposed cnn method was demonstrated in instances of uniform flow and jet flow which are difficult to manage for the dcc method owing to problems associated with ia as shown in table 7 the cnn results were relatively stable despite the less satisfactory performance in modeling jet flow the tolerance for noise and ia makes the cnn method well suited for flow measurement under uncontrolled environmental conditions 4 5 computation time table 8 lists the computation times for all cases overall the main factor affecting the computation time was the size of the ia the computational overhead for the proposed cnn scheme was insufficient to counterbalance the advantages in fact the proposed cnn should be able to provide results in real time 4 6 flume experiments flume experiments were performed to assess the efficacy of the cnn and dcc methods and the results were compared with measurements obtained using adv instantaneous velocities were derived using 200 successive images the statistical dispersion of these velocities is illustrated in fig 14 which shows the velocity components in the x and y directions the instantaneous velocities obtained using the dcc method included a large number of outliers indicating evident errors in the opposite direction the cnn method exhibited stable performance presenting only one outlier estimates of the discharge from the surface flow measurements were derived from the mean velocities in the flume experiment the main flow ran in the x direction such that the average velocity of only the u component was used to compare both methods to improve the dcc results we eliminated outlier measurements larger than their two corresponding standard deviations and manually deleted the opposite velocity values before deriving average surface velocities once these outliers were removed the cnn and dcc methods presented similar results in terms of vector fields and velocity magnitude contours see fig 15 overall the flume experiment corroborated the capability of the cnn method to overcome low seeding density and surface reflection induced by standing waves in the flume the surface velocity was estimated from adv measurements by fitting vertical adv data to log law profiles at positions x 9 27 m and y 0 1 0 42 0 58 and 0 85 m the r2 value for all positions was 0 94 at least 5 min of data measurements were collected for each point under a signal to noise ratio larger than 80 for each position the surface and depth averaged velocities were calculated from the fitted vertical velocity profile function the velocity index was estimated using the depth averaged velocity divided by the surface velocity see table 9 using the average velocity it was possible to estimate the discharge in each zone to obtain a total discharge of 0 0752 m3 s for the cross section at x 9 27 m cnn and dcc estimates of discharge were obtained by multiplying surface flow velocities at positions x 9 27 m and y 0 15 0 42 0 58 and 0 85 m by the corresponding velocity index derived from adv measurements in order to obtain the depth averaged unit width flowrate which was then multiplied by the respective cross section areas to obtain the total flow rate cnn and dcc provided similar estimates with an error ranging from 7 87 to 8 09 with respect to adv measurements see table 10 in this experiment the illumination was not controlled using an additional light source however the brightness distribution on the water surface was fairly uniform and the color of the image background was distinctly different from that of the particles the diameter of the particles in the analyzed images was approximately 10 px and the particle density was low similar to the artificial images previously discussed despite the fact that the imaging conditions were not precisely the same as those of the simulations the same conclusions in terms of velocity measurement can be drawn the robustness of the proposed cnn method made it possible to obtain reliable measurements of velocity which makes it a viable alternative to current techniques used in the field 5 conclusions imaging technology is a highly useful non intrusive approach for flow measurement based on surface velocity fields in other words imaging technology makes it easier to achieve large scale velocity measurements at high sampling rates nonetheless image noise introduced by environmental conditions e g variable illumination and low light artifacts seeding distribution and the effects of image matching algorithms can have a profound impact on flow measurements the cnn method proposed in this study is a feasible approach for obtaining flow measurements in the field using conventional imaging techniques such as lspiv velocity measurements obtained using successive artificial images revealed that non uniform illumination can significantly degrade image matching however the cnn method proved to be robust to this type of environmental noise non uniform illumination was shown to reduce the accuracy of dcc measurements by 18 14 while reducing the accuracy of cnn measurements by only 0 34 the dcc method could not delineate the flow features of the vortex we also examined the influence of gaussian noise on various flow fields vortices jets and uniform flows the addition of 35 gaussian noise increased the measurement error in terms of rmse of the dcc by 2 77 to 31 13 fold whereas it increased the error of cnn by only 1 25 to 1 68 fold flume experiments were also conducted to assess the capability of the cnn and dcc in estimating discharge when imaging parameters were well controlled and outliers were filtered out both methods performed similarly diverging from ground truth values adv in the range of 7 87 to 8 09 for discharge when measuring instantaneous velocity the proposed cnn scheme generated far fewer outliers the network architecture activation functions and depth of the convolution layer can significantly affect the estimation results here we opted for a two layer framework in conjunction with a convolutional layer 16 32 fully connected layer 4096 and relu for flow measurement the robustness of the proposed cnn method makes it possible to obtain reliable measurements of velocity under conditions of high environmental noise thereby demonstrating its viability as an alternative to current field techniques credit authorship contribution statement hao che ho conceptualization methodology software formal analysis investigation resources writing original draft writing review editing yu wei chiu software validation investigation visualization ting yu chen software investigation data curation yen cheng lin data curation writing review editing visualization declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests hao che ho reports financial support was provided by national science and technology council appendix a the dcc method calculates the correlation coefficient r ab between the ia at time t and the intensity of all possible ias in the sa at time t 1 see fig a 1 a using the following equation fujita et al 1998 a 1 r ab i 1 m j 1 n a ij a ij b ij b ij i 1 m j 1 n a ij a ij 2 i 1 m j 1 n b ij b ij 2 where i and j represent the coordinates of the elements in image matrices a and b respectively once the correlation coefficients of sub image a and all candidate sub images b are calculated the correlation coefficient matrix r g is obtained using the above equation for sub image a at a fixed grid point g between the images at times t and t 1 generally if the image quality is suitably controlled then there should be only one significant peak z max see fig a 1b thus z max corresponding to sub image b is defined as the one that matches sub image a at t 1 and the difference between the positions of both sub images a and b is obtained as a movement vector δ x g δ y g with pixel level accuracy at grid point g the velocity is then calculated in accordance with this movement vector appendix b we assessed the effects of four activation functions the rectified linear unit relu leaky relu lrelu parametric rectified linear unit prelu and exponential linear unit elu as shown in fig b 1 relu presents no saturation region or gradient dissipation and its computational overhead is low nair and hinton 2010 furthermore feedback is provided only when x 0 which is in line with the activation mechanism of biological nerves wherein a neuron is deactivated when x 0 to avoid updating parameters during training relu requires a small gradient assigned by a fixed value that is normally smaller than zero prelu requires a variable a which is learned from the slope of the data when x 0 he et al 2015 if a is small then the formula is the same as that of lrelu whereas if a 0 then the formula will be the same as that of relu prelu has been shown to approach human classification ability in the imagenet 2012 classification dataset he et al 2015 krizhevsky et al 2017 compared to relu elu requires an extra constant α to define the smoothness function when the inputs are negative by reducing the bias shift effect elu makes it possible to reduce the training time and enhance accuracy clevert et al 2015 appendix c the covariance matrix between two vector fields is given by the following equation crosby et al 1993 c 1 σ w σ 11 σ 12 σ 21 σ 22 where c 2 σ 11 σ u 1 u 1 σ u 1 v 1 σ v 1 u 1 σ v 1 v 1 c 3 σ 12 σ u 1 u 2 σ u 1 v 2 σ v 1 u 2 σ v 1 v 2 c 4 σ 21 σ u 2 u 1 σ u 2 v 1 σ v 2 u 1 σ v 2 v 1 c 5 σ 22 σ u 2 u 2 σ u 2 v 2 σ v 2 u 2 σ v 2 v 2 the vcc ρ v 2 is calculated as follows c 6 ρ v 2 t r σ 11 1 σ 12 σ 22 1 σ 21 where tr is the sum of matrix diagonal elements the calculation of vcc can be facilitated by expanding eq c 6 as follows c 7 ρ v 2 f g where f σ u 1 v 1 σ u 2 v 2 σ v 1 v 2 2 σ v 2 v 2 σ v 1 u 2 2 σ v 1 v 1 σ u 2 v 2 σ u 1 v 2 2 σ v 2 v 2 σ u 1 u 2 2 2 σ u 1 v 1 σ u 1 v 2 σ v 1 u 2 σ u 2 v 2 2 σ u 1 v 1 σ u 1 u 2 σ v 1 v 2 σ u 2 v 2 2 σ u 1 u 1 σ v 1 u 2 σ v 1 v 2 σ u 2 v 2 2 σ v 1 v 1 σ u 1 u 2 σ u 1 v 2 σ u 2 v 2 2 σ u 2 u 2 σ u 1 v 1 σ u 1 v 2 σ v 1 v 2 2 σ v 2 v 2 σ u 1 v 1 σ u 1 u 2 σ v 2 u 2 g σ u 1 u 1 σ v 1 v 1 σ u 1 v 1 2 σ u 2 u 2 σ v 2 v 2 σ u 2 v 2 2 
2239,data related to river velocity and discharge are important for water resource management non intrusive image measurement techniques based on direct cross correlation dcc algorithms such as particle image velocimetry piv are widely used to measure velocity and discharge in the field however environmental noise is highly complex and uncontrollable in the field significantly reducing the accuracy of dcc based methods convolutional neural networks cnns are commonly used in image recognition because of their outstanding accuracy which far exceeds that of conventional imaging methods however these accuracy levels cannot be directly extrapolated for flow movement estimation therefore in this study we developed an innovative sub pixel correction technique that allows cnn based methods to obtain stable measurements in the piv framework this is the first study that successfully applied the concept of cnn to measure velocity data using images the hamel oseen vortex flow uniform steady flow and plane laminar jet flow models are established as benchmark vector fields non uniform illumination and gaussian noise with varying degrees of interference are added to the synthetic data to evaluate the performance of the cnn based method in piv for noiseless images the dcc based and cnn based methods achieve lower measurement errors than benchmark errors for noisy images the dcc suffers a fold error increase ranging from 2 77 to 31 13 whereas the cnn suffers only a fold error increase ranging from 1 25 to 1 68 a 30 m long flume is then used in an uncontrolled environment to mimic real world flow measurement the dispersion of the instantaneous velocity measurements for the cnn is more concentrated than that for the dcc the acoustic doppler velocimetry yields an error of only 7 87 in discharge estimation using cnn these results indicate that the cnn based method is more robust than conventional methods and has the potential to be effectively applied to measurements in the field keywords flow measurement lspiv direct cross correlation convolutional neural network image technique data availability data will be made available on request 1 introduction in situ water data are important for water resource management such as urban planning flood control construction river restoration and flood monitoring field data must be obtained to analyze the discharge for flood monitoring particularly considering the recent increase in the frequency of extreme rainfall events lipper et al 2017 kouadri et al 2021 unfortunately conventional approaches for the measurement of hydrological phenomena have been hindered by insufficient data resolution inability to transmit data in real time and inability to obtain reliable data during storms imaging methods have been developed for flow measurement and their efficacy has been demonstrated in flume experiments adrian 1991 hann and greated 1997 creutin et al 2002 young et al 2015 kim et al 2016 a popular non intrusive imaging method is the large scale particle image velocimetry lspiv which derives flow rates from flow velocity measurements at the surface of river channels fujita and komura 1994 fujita et al 1998 bradley et al 2002 hauet et al 2008 muste et al 2014 ostad ali askari and shayan 2021 this method involves setting up a camera on a riverbank to capture a continuous stream of images of the river surface and derive hydrological data image matching is then performed using direct cross correlation dcc cross correlation matrices can be calculated directly in the spatial or frequency domain using the fast fourier transform fft the fact that the dcc algorithm is used to correlate images in space means that the algorithm imposes no limitations on the size of the sub images interrogation area ia to be compared nonetheless the size of the sub images can have a significant impact on the computation time and accuracy of the corresponding estimates the fft provides notable benefits in terms of computational overhead and suppression of noise related to the inter correlation coefficients fft also increases the density of the velocity field through multiple channels willert and gharib 1991 stamhuis 2006 thielicke and stamhuis 2014 the matching accuracy of the dcc method is significantly influenced by the quality of successive images in indoor flume experiments it is relatively easy to obtain reliable velocity measurements because the experimental conditions can be easily controlled and high quality images can be captured however environmental conditions e g light wind and shadows cannot be controlled in the field despite extensive efforts to eliminate these effects chaves 2012 zhang et al 2013 xu et al 2017 zhen et al 2017 ghashghaie et al 2022 most acquired images contain noise that cannot be entirely removed it is difficult to obtain uniform seeding conditions for particle tracking the inability to control environmental conditions inevitably undermines the accuracy of the data and limits the applicability of imaging techniques for flow measurement there is a pressing need for an algorithm that considers not only brightness but also geometric and abstract features there is also a need for image processing algorithms that do not rely on a stable environment artificial intelligence ai involves using computers to perform tasks that are beyond human cognitive capability in 1958 rosenblatt 1958 published a perceptron machine learning algorithm that was a linear binary classification model the linearity of the perceptron algorithm was a significant drawback that caused the development of machine learning to be hindered until the 1980s which was when the multi layer perceptron algorithm enabled the processing of multiple messages within a network makino et al 1983 hinton 1986 watrous and shastri 1987 morgan and bourlard 1990 in recent years ai and machine learning have been revived with the advent of convolutional neural networks cnns starting with the lenet 5 network in 1995 lecun et al 1998 cnns use images as a direct input thereby eliminating the need for pre processing image features cnns also allow the users to correct the weights of convolutional filters for automatically producing convolutional kernels that enable image recognition neural network weights are combined with kernels to reduce the number and complexity of the parameters while enhancing the tolerance for image distortion rotation and panning cnns have been widely used in transportation agriculture and medicine kagaya et al 2014 hao et al 2018 shang et al 2018 traore et al 2018 özyurt 2020 ma et al 2021 ostad ali askari and shayannejad 2021 xiong et al 2021 the cnn concept was also implemented to predict discharge and velocity based on empirical and numerical models chiang et al 2017 forghani et al 2021 deng et al 2022 karpatne et al 2022 however there is no application for in situ measurements of discharge and velocity with non intrusive imaging methods in this study we employed a cnn as an alternative to the dcc algorithm for the recognition of granular images representing a flow field lspiv is a non intrusive approach for hydrologic flow measurement however it is highly susceptible to environmental conditions cnns have proven to be highly effective in resisting the effects of environmental interference however no previous study has applied cnns in the context of lspiv herein we explored the possibility of replacing the dcc method with the cnn method in lspiv to allow direct extraction of abstract or geometric features for image matching in the simulations we compared both approaches in terms of their accuracy in estimating the flow velocity for various flow scenarios we also compared the approaches in a full scale flume experiment in which the surface velocity fields obtained using the two methods were compared with those obtained using acoustic doppler velocimetry adv for a given cross sectional surface 2 experimental methods 2 1 concepts underlying the dcc method the standard operating procedure for particle image velocimetry piv is to create a grid point on the image sequence to be analyzed as the central location of the extracted ia the size of the ia is determined by the density of the particles in the window to ensure that this sub image contains sufficient feature related information for comparison the search area sa is then defined such that the algorithm can find the ia of highest similarity at time t within the periphery of the grid points at t 1 where the velocity is calculated with sub pixel correction the details of the dcc method used to estimate the velocity can be found in appendix a 2 2 design of the proposed cnn scheme piv analysis in the current study was performed using the standard procedure for the definition of regions of interest rois in images and cutting out sub images of a specific size with fixed spaced grid points at time t in the current study these sub images were used as input data for the cnn in accordance with the following definition x 64 64 g t g 1 ñ each sub image was defined as a different object class y g the output of the network was encoded in the form of one hot see fig 1 to generate an array representing class y g the data input to the network must be restricted within a specified range through normalization to avoid slow convergence prevent gradient explosion due to a large difference in data order and ensure trainability we stipulated that the intensity of the pixels in an 8 bit grayscale image must take values between 0 and 255 we also normalized the raw image data to 0 s and 1 s before cutting the sub images in the definition of the network structure for the proposed cnn the characteristics of the cutout sub images included only the arrangement and shape of the particles and the time interval between successive images was short this ensured that the two images did not diverge excessively to ensure that the network met the requirements in terms of piv analysis and computation time we discarded existing network architectures such as vgg net google inception or resnet instead we built the self defined convolutional structure shown in fig 2 we examined two convolutional layers net1 versus four convolutional layers net2 as well as maximum pooling layers and a fully connected layer connected to the output layer the depth of the convolutional layer was used to determine the diversity of the extracted features essentially insufficient depth would lead to the extraction of an insufficient number of features to perform recognition tasks whereas excessive depth would increase the training time we eventually opted for a two layer architecture net1 with various depth combinations as follows 8 16 16 32 32 64 64 128 and 128 256 the number of neurons in the fully connected layer was also varied as follows 256 512 1024 2048 and 4096 we formulated a total of 25 depth number combinations in the fully connected layer and then selected the most robust one based on a comparison with the benchmark we eventually established a convolutional depth of 16 32 with 4096 neurons in the fully connected layer for all subsequent analyses the depth of the first convolution layer was 16 net1 and the depth of the second convolution layer was 32 net2 the depths of the third and fourth convolution layers in net2 were 32 and 64 respectively the last connection layer included 4096 nodes in the output layer after the fully connected layer in the convolution layer we employed a 3 3 convolutional kernel and a 2 2 pooling kernel in step 1 which were connected to the fully connected layer for feature classification the effects of four activation functions were assessed the rectified linear unit relu leaky relu lrelu parametric rectified linear unit prelu and exponential linear unit elu the details of the four activation functions are provided in appendix b a loss function is used to determine the difference between the output and target values in the current study the highly popular cross entropy l i was used to represent the amount of information between the target array y 1 n and network output array y 1 n de boer et al 2005 based on the following equation 1 l i n 1 n y n i log y n i l total i l i where y is the element of the array y n is the array index and number of classes n is the total number of classes and i is the number of arrays a small cross entropy value indicates a small difference and vice versa once the loss function is determined it is necessary to select an optimization algorithm to correct the weights such that the best solution can be achieved with a minimal discrepancy between the target and network output the most common methods employed for deep learning or neural networks are the adam method and stochastic gradient descent sgd in the current study we opted for the adam method to avoid the shortcomings of sgd in optimization the adam method combines the momentum algorithm qian 1999 with the adagrad duchi et al 2011 and rmsprop algorithms thus it corrects errors by adopting a high learning rate at the beginning of training and a lower learning rate in later stages moreover the concept of momentum allows the adam method to increase or decrease the current error gradient based on the previous error gradient thereby enabling more rapid correction in the desired direction and slower corrections in other directions the adam method has proven to be effective in most general cases and provides rapid convergence exceeding that of other optimization algorithms kingma and ba 2014 ostad ali askari et al 2017 weight initialization also affects the training and performance of cnns setting a poor initialization can lead to an uneven distribution of data values transmitted in the neural layer such that the neuron outputs tend to be concentrated around a certain value a recommended weight initialization suitable for different activation functions was applied in this study glorot and bengio 2010 he et al 2015 to unify the dimensions of the network input data the sub images were scaled to the following sizes 32 32 64 64 96 96 and 128 128 in all convolution layers the movement step of the convolutional kernel was 1 px and the initial weights were randomly generated using a normal distribution with a mean value of 0 a standard deviation of 0 1 and an initial bias of 0 01 the softmax function was applied to the output layer for the proposed cnn model we also employed batch training with batch regularization to keep the weights in each convolution layer active thereby stabilizing the performance of the loss function during training and suppressing the network dependence on default values ioffe and szegedy 2015 after training was completed using all the sub images at time t an sa was defined at the corresponding grid point x g t 1 y g t 1 for time t 1 to identify sub image candidates x 64 64 g k t 1 with k in the sa using the sliding window approach all the candidates were input to the network to obtain the output array y 1 n g k as calculated by the cnn for use in obtaining the loss matrix l g the minimum loss value of the loss matrix was based on the position of the matching sub image this made it possible to detect pixel level movement x g t 1 m y g t 1 m for use in the calculation of velocity 2 3 sub pixel correction for cnn the distance represented by a pixel can be scaled for specific applications for imaging applications using large scale photographic images of flowing rivers e g lspiv the length unit corresponding to a pixel in the image is usually on the centimeter scale under these conditions the measured velocity vector is far from the true value unless the measurement resolution is very high sub pixel correction is an approach for enhancing the measurement resolution in conventional piv sub pixel precision corrections are generally obtained via gaussian curve fitting or 2d gaussian fitting willert and gharib 1991 nobach and honkanen 2005 once the peak z max of the correlation matrix r g is found the peak of the curve is located through curve fitting using the values at 1 pixels adjacent to z max which is used to achieve sub pixel accuracy however for the cnn in this study we replaced the correlation matrix r g with a loss matrix l g the distribution characteristics of the two matrices are not the same the peak of the loss matrix cannot be fitted using gaussian curve fitting see fig 3 theoretically matching sub images x g m should be similar to the original image x g such that if one image were laid over the other then only minor particle movements would be observed see fig 4 in the current study we aimed to improve the accuracy at sub pixel level using the average movement of all particles in the sub image at microscopic scale as the displacement of sub pixels particle tracking velocimetry ptv was used to capture the particle coordinates and noise in the images was smoothed out using binomial and binomial laplacian filters with the contrast between the bright center and edge of the particle enhanced as shown in fig 5 this smoothed the noise and suppressed the particle edge intensity to enhance edge performance a quadratic curve was then fitted to the peaks with a post filtering intensity exceeding the threshold value jähne 1995 capart et al 2002 using the following formula 2 x p x i 1 2 i i j 1 i i j 1 i i j 1 2 i i j i i j 1 y p y i 1 2 i i 1 j i i 1 j i i 1 j 2 i i j i i 1 j where i and j represent the coordinates of the pixel with peak intensity after filtering and i is the intensity of that pixel the offset between the quadratic maximum and maximum pixel intensity was calculated to obtain the accuracy of the particle coordinates x p y p at sub pixel level thus by marking the particle coordinates x p y p and x p y p of the matching sub pixel pairs the average difference in particle coordinates in the two images can be calculated at microscopic scale and by adding this movement to the initial results obtained using the cnn the final movement vector δ x g δ y g can be obtained with sub pixel accuracy this approach to particle detection was also applied to determine whether sub images on grid points included particles or features for either method cnn or dcc if a sub image were found to contain no particles or distinctive features that sub image would not undergo image matching regardless of the type of method used it would be helpful to screen out sub images without particles or features numerous sub images in the cnn contained evident particles or features however some sub images did not contain particles or evident features because the grid dots fell in areas with an indiscernible particle distribution allowing such sub images to be trained in the network as separate categories would slow down the speed and effectiveness of training featureless sub images would also tend to generate unreliable or erroneous results when using the dcc method therefore we subjected all sub images to pre screening 2 4 evaluation of measurement results 2 4 1 root mean square error rmse the rmse was used to determine the discrepancy between the estimates of the velocity magnitude and corresponding benchmark values as follows 3 rmse 1 k k 1 k y k y k 2 where y k represents the velocity vectors calculated using the dcc or cnn methods and y k represents the corresponding benchmark vector at each grid point 2 4 2 vector correlation coefficient vcc the velocity vectors estimated using the cnn and dcc methods were compared to the benchmark velocity results using the same grid in order to evaluate the accuracy of both methods based on the vcc assuming that the two dimensional velocity vector obtained using cnn or dcc is w 1 u 1 i v 1 j and that the corresponding benchmark vector is w 2 u 2 i v 2 j then the covariance matrix between both vector fields and the vcc ρ v 2 can be calculated vector fields w 1 and w 2 are completely uncorrelated if ρ v 2 0 however these vector fields present complete linear correlation if ρ v 2 2 when the number of samples in the vector exceeds 64 the distribution of n ρ v 2 approximates a cardinal distribution with four degrees of freedom this makes it necessary to determine whether the chi square test values x 2 9 488 and α 0 05 for the four degrees of freedom are sufficient details of the calculation for the covariance matrix and the vcc ρ v 2 can be found in appendix a 2 5 discharge estimation the goal of using imaging techniques for flow measurement while operating in the field e g lspiv implies to obtain an estimate of discharge quickly and easily the velocity index method commonly used for discharge estimation involves multiplying the depth averaged velocity converted from the surface velocity using an index α and the cross section of the channel in this study we conducted a flume experiment to measure discharge and vertical velocity distributions in order to obtain α from adv measurements the log law velocity profile function was used to fit the vertical velocity data for estimating α the surface flow velocities obtained using the dcc and cnn methods were converted using this index and multiplied by the cross section to estimate the rate of discharge 3 experimental setup 3 1 benchmark images the efficacy of the dcc and cnn methods in measuring flow rates was evaluated under various flow fields and environmental conditions using artificial images as a benchmark generated using the open source software program pivlab based on known true speed values and pixel size thielicke and stamhuis 2014 the first flow field was set as the hamel oseen vortex saffman 1992 using the following equation 4 v θ r t γ 2 π r 1 exp r 2 r c 2 t 5 r c t 4 ν t r c 2 0 where γ refers to the circulation r is the distance from the center of the vortex ν is the kinematic viscosity and r c is the vortex radius as shown in fig 6 a this describes an up down left right symmetrical twin turbine velocity field the use of this velocity field equation made it possible to derive the true velocity vector at each grid point as a benchmark value for comparison in this study we set the pixel velocity magnitude to a value two orders of magnitude smaller than 3 and 5 pix frame the second type of flow field was a plane laminar jet flow see fig 6b the velocity distribution and flow rate were derived using the following equations schlichting 1933 6 u 3 8 π k ν x 1 1 ξ 2 4 2 7 v 1 4 3 π k x ξ ξ 3 4 1 ξ 2 4 2 where k j ρ ξ 3 16 π k v y x j is the momentum flux ρ is the fluid density and ν is the kinematic viscosity the last flow field was a uniform steady flow in which the velocity and flow direction did not change with time or location in this study the velocity was 2 pix frame in the u direction and 5 pix frame in the v direction the flow field is shown in fig 6c 3 2 artificial image conditions the artificial images of particles generated using the aforementioned equations and pivlab presented a high contrast between the particles and a uniform background which means that the dcc and cnn methods should both be able to obtain precise estimates of particle movement however the environmental conditions encountered in the field would likely introduce unexpected errors in flow measurements images obtained in the field are subjected to a range of environmental conditions including uneven brightness and particle density distributions in this study we employed particles of two sizes variations in particle density and particle movement three light sources and noisy signals to mimic the interferences typically encountered in the field the stability and robustness of the dcc and cnn methods were evaluated under these conditions 3 2 1 particle distribution effects the diameter and number of particles were varied to assess the effect of particle density on flow measurement performance the objective here was to determine whether the cnn method could reduce the negative effects of variations in particle density distribution note that even under ideal environmental conditions images captured in the field are affected by the distance between the camera and water surface as well as the specifications of the recording equipment moreover the pixel area occupied by the particles or features in the image is not fixed in most field applications drones are used to capture water surface features thus the resolution of the images is determined by the specifications of the recording equipment and the altitude of the drone above the water i e ground sample distance gsd which can be defined using the following formula 8 gsd h s f where h is the altitude of the drone s is the pixel size in the sensor and f is the optical focal length of the lens we consider the dji phantom 3 drone 4 k version as an example its sensor size is 1 2 3 the optical focal length of its lens is 3 57 mm full high definition recording is performed at 1080 1920 px and the image element size is 0 0037 m the correspondence between altitude h and gsd is presented in table 1 when measurements are performed on a river presenting a range of 30 50 m in width the ground resolution decreases inversely with the altitude for example at an altitude of 30 m the gsd would be 31 24 mm pixel such that an object measuring 3 1 cm would occupy only one pixel in the image note that the leaves are commonly used as a seed for imaging in the range of 5 15 cm2 thus in these simulations the diameter of the particles must be 3 5 pixels to ensure that the artificial images accurately represent those obtained in the real world in terms of seeding density the number of particles determines the degree of sparseness in an image we generated two cases involving 100 000 or 3 000 particles to mimic small scale indoor experiments and large scale field experiments respectively in the high density case 100 000 particles to mimic well controlled conditions the particle diameter was set to 3 pixels and the illumination was uniform in the low density case 3 000 particles we aimed to mimic uneven illumination of the water surface by adding noise in the form of an uneven distribution of the background brightness a total of 22 cases were used to examine the performance of the cnn and dcc methods see table 2 we then conducted experiments involving two particle velocities 3 or 5 px frame in successive images cases v3r1 and v4r1 involved high density particle distributions simulating small scale applications whereas cases v1r1 and v2r2 involved low density particle distributions simulating field conditions in the field uneven distribution of brightness on the water surface can be largely attributed to cloud cover shadowing and water depth overcoming this issue generally requires the removal of background noise through tedious image filtering the results of which are not usually satisfactory therefore we included cases v1r1n and v2r1n which added uneven lighting conditions to the conditions in cases v1r1 and v2r1 table 2 presents the simulation cases 3 2 2 illumination and signal effects we simulated three illumination conditions representing uniform and non uniform backgrounds see fig 7 uniform image illumination involved brightening the background evenly to simulate a lack of reflection in the field non uniform illumination involved adjusting the background brightness to simulate uneven reflections note that the same contrast was used for all artificial images the signal interference caused by the camera sensor and circuitry was replicated by adding gaussian noise at three densities see fig 8 3 3 flume experimental setup the flume experiment was designed to assess the performance of the cnn and dcc methods based on ground truth data collected using a sontek 10 mhz micro adv the channel used in this study was 30 m in length and 1 m in width the camera captured an area of x 8 9 9 6 m the cross section was captured using the adv measured at x 9 27 m see fig 9 a ten ground reference points grps were placed on both sides to facilitate image ortho rectification images were recorded using a 16 megapixel camera sony nex vg30 at a height of approximately 1 5 m from the bottom of the flume without auxiliary lighting video recording was performed using full high definition resolution at 50 fps with a field of view fov of 1 2 0 7 m2 following ortho rectification the image size was reduced to 917 501 px with a resolution of 1 2 mm pixel when analyzing the dcc and cnn methods image pre processing was not conducted the ia was fixed at 64 px and the sa was 12 px extended to the ia see fig 9c the adv sampling frequency was set to 25 hz the water depth of the flume was 0 249 m the measurement positions for adv were at x 9 27 m and y 0 15 0 42 0 58 and 0 85 m see fig 9b for each position we measured 10 points vertically from the bottom to the water surface using the adv to capture vertical velocity profiles for each point we derived the average velocity from the data obtained over a period of 3 min 4 results and discussion 4 1 evaluation of cnn models the framework and settings of the cnn were established using vortex images captured under uniform illumination conditions we then compared the results obtained using the dcc method and various cnn architectures with the benchmark values as shown in table 3 cases v1r1 and v1r2 with ias of 64 64 presented no significant differences between the results obtained using the cnn and dcc methods and the vcc results ρ v 2 2 indicates values that are indistinguishable from the benchmark both cnn architectures presented promising results however net 1 with fewer convolution layers extracted sufficient feature related data for analysis note also that the benefits of adding convolutional layers diminished beyond a certain point even as the computational overhead increased thus we selected the net 1 cnn architecture for subsequent analysis to assess the effects of activation functions we compared cases v1r1 v1lr1 v1pr1 and v1el1 see table 2 with various functions overall the relu function outperformed the others by providing the lowest rmse compared to the benchmark for the u or v velocity components see fig 10 the relu function provides four advantages for deriving flow measurements first it is in line with the transmission of signals by biological neurons which do not respond until a stimulus has reached a certain intensity and sends only weak signals when a given threshold is exceeded glorot et al 2011 second in error transmission the error gradient must be calculated in terms of the bias and transmitted back note that when the relu function is used for the output the error gradient does not disappear third the relu function causes the output of some neurons those with input less than 0 to be zero thereby increasing the sparseness of active neurons in the network which is consistent with biological mechanisms attwell and laughlin 2001 glorot et al 2011 finally relu is simple to compute and imposes a reasonable computational overhead therefore we used relu as the activation function in the cnn architecture for all subsequent analyses the effect of ia was evaluated in cases v1r1 v1i32 v1i96 and v1i128 see table 2 our results revealed that the window size certainly affected the performance of the dcc and cnn see fig 11 the appropriate size for the ia depends on the particle density particle diameter and flow pattern the sensitivity of the dcc method to ia resulted in a wide range of measurement values the cnn method was largely robust to variations in ia however the best results were obtained for a window size of 64 64 the proposed cnn was assessed using various architectures activation functions and window sizes in the following sections we consider a cnn with fewer convolution layers net 1 a relu active function and a 64 64 ia 4 2 impact of environmental noise on cnn and dcc the illumination conditions and particle density are difficult to control in the field thus we conducted experiments to evaluate the robustness of the cnn and dcc methods when using images captured under uniform and non uniform illumination conditions case v1r1 was compared with case v1r1n under a slow flow field whereas case v2r1 was compared with case v2r1n under a fast flow field the results are shown in fig 12 and table 4 overall the cnn generated similar results regardless of illumination under uneven illumination the dcc method was able to capture the vortex but was unable to estimate the velocity magnitude with accuracy particularly when the vectors were close to the center of the vortex when the particle movement was increased v2r1 the results obtained using the dcc method were profoundly affected by the illumination conditions see fig 13 the dcc could barely capture the vortex and the estimated velocities significantly differed from the benchmark values when using the dcc method to analyze a slow flow field the discrepancy between results obtained under uniform or non uniform illumination was 6 43 however when applied to a fast flow field the discrepancy increased to 18 14 see tables 4 and 5 these results indicate that the dcc method is strongly affected by image quality the cnn method proved to be highly robust to image contamination our results indicate that the proposed cnn should be able to overcome the effects of ambient light noise thereby capturing flow fields close to ground truth however under well controlled lighting conditions the cnn provided no advantage over the dcc method in deriving an accurate velocity field the effects of seeding density were assessed in cases v1r1 v2r1 v3r1 and v4r1 table 6 lists the measurements obtained using the methods versus the benchmark values a high seeding density increased the accuracy of the measurements regardless of the computation method overall dcc was more sensitive than cnn to seeding density however both methods provided reasonable measurements regardless of seeding density 4 3 impact of gaussian noise gaussian noise was used to simulate the noise in the images captured under low light conditions these results revealed a positive correlation between the amount of gaussian noise and the discrepancy between the dcc velocity results and ground truth values see table 7 the cnn method was significantly less sensitive to gaussian noise the addition of 35 gaussian noise increased the measurement error as follows dcc 2 77 to 31 13 fold increase cnn 1 25 to 1 68 fold increase overall the dcc method proved to be highly vulnerable to the imaging conditions whereas the cnn method proved to be robust thus it is reasonable to assume that the proposed cnn scheme outperforms the dcc method when it comes to overcoming environmental noise to obtain reliable flow measurements in practical situations 4 4 impact of flow field characteristics the capability of the proposed cnn method was demonstrated in instances of uniform flow and jet flow which are difficult to manage for the dcc method owing to problems associated with ia as shown in table 7 the cnn results were relatively stable despite the less satisfactory performance in modeling jet flow the tolerance for noise and ia makes the cnn method well suited for flow measurement under uncontrolled environmental conditions 4 5 computation time table 8 lists the computation times for all cases overall the main factor affecting the computation time was the size of the ia the computational overhead for the proposed cnn scheme was insufficient to counterbalance the advantages in fact the proposed cnn should be able to provide results in real time 4 6 flume experiments flume experiments were performed to assess the efficacy of the cnn and dcc methods and the results were compared with measurements obtained using adv instantaneous velocities were derived using 200 successive images the statistical dispersion of these velocities is illustrated in fig 14 which shows the velocity components in the x and y directions the instantaneous velocities obtained using the dcc method included a large number of outliers indicating evident errors in the opposite direction the cnn method exhibited stable performance presenting only one outlier estimates of the discharge from the surface flow measurements were derived from the mean velocities in the flume experiment the main flow ran in the x direction such that the average velocity of only the u component was used to compare both methods to improve the dcc results we eliminated outlier measurements larger than their two corresponding standard deviations and manually deleted the opposite velocity values before deriving average surface velocities once these outliers were removed the cnn and dcc methods presented similar results in terms of vector fields and velocity magnitude contours see fig 15 overall the flume experiment corroborated the capability of the cnn method to overcome low seeding density and surface reflection induced by standing waves in the flume the surface velocity was estimated from adv measurements by fitting vertical adv data to log law profiles at positions x 9 27 m and y 0 1 0 42 0 58 and 0 85 m the r2 value for all positions was 0 94 at least 5 min of data measurements were collected for each point under a signal to noise ratio larger than 80 for each position the surface and depth averaged velocities were calculated from the fitted vertical velocity profile function the velocity index was estimated using the depth averaged velocity divided by the surface velocity see table 9 using the average velocity it was possible to estimate the discharge in each zone to obtain a total discharge of 0 0752 m3 s for the cross section at x 9 27 m cnn and dcc estimates of discharge were obtained by multiplying surface flow velocities at positions x 9 27 m and y 0 15 0 42 0 58 and 0 85 m by the corresponding velocity index derived from adv measurements in order to obtain the depth averaged unit width flowrate which was then multiplied by the respective cross section areas to obtain the total flow rate cnn and dcc provided similar estimates with an error ranging from 7 87 to 8 09 with respect to adv measurements see table 10 in this experiment the illumination was not controlled using an additional light source however the brightness distribution on the water surface was fairly uniform and the color of the image background was distinctly different from that of the particles the diameter of the particles in the analyzed images was approximately 10 px and the particle density was low similar to the artificial images previously discussed despite the fact that the imaging conditions were not precisely the same as those of the simulations the same conclusions in terms of velocity measurement can be drawn the robustness of the proposed cnn method made it possible to obtain reliable measurements of velocity which makes it a viable alternative to current techniques used in the field 5 conclusions imaging technology is a highly useful non intrusive approach for flow measurement based on surface velocity fields in other words imaging technology makes it easier to achieve large scale velocity measurements at high sampling rates nonetheless image noise introduced by environmental conditions e g variable illumination and low light artifacts seeding distribution and the effects of image matching algorithms can have a profound impact on flow measurements the cnn method proposed in this study is a feasible approach for obtaining flow measurements in the field using conventional imaging techniques such as lspiv velocity measurements obtained using successive artificial images revealed that non uniform illumination can significantly degrade image matching however the cnn method proved to be robust to this type of environmental noise non uniform illumination was shown to reduce the accuracy of dcc measurements by 18 14 while reducing the accuracy of cnn measurements by only 0 34 the dcc method could not delineate the flow features of the vortex we also examined the influence of gaussian noise on various flow fields vortices jets and uniform flows the addition of 35 gaussian noise increased the measurement error in terms of rmse of the dcc by 2 77 to 31 13 fold whereas it increased the error of cnn by only 1 25 to 1 68 fold flume experiments were also conducted to assess the capability of the cnn and dcc in estimating discharge when imaging parameters were well controlled and outliers were filtered out both methods performed similarly diverging from ground truth values adv in the range of 7 87 to 8 09 for discharge when measuring instantaneous velocity the proposed cnn scheme generated far fewer outliers the network architecture activation functions and depth of the convolution layer can significantly affect the estimation results here we opted for a two layer framework in conjunction with a convolutional layer 16 32 fully connected layer 4096 and relu for flow measurement the robustness of the proposed cnn method makes it possible to obtain reliable measurements of velocity under conditions of high environmental noise thereby demonstrating its viability as an alternative to current field techniques credit authorship contribution statement hao che ho conceptualization methodology software formal analysis investigation resources writing original draft writing review editing yu wei chiu software validation investigation visualization ting yu chen software investigation data curation yen cheng lin data curation writing review editing visualization declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests hao che ho reports financial support was provided by national science and technology council appendix a the dcc method calculates the correlation coefficient r ab between the ia at time t and the intensity of all possible ias in the sa at time t 1 see fig a 1 a using the following equation fujita et al 1998 a 1 r ab i 1 m j 1 n a ij a ij b ij b ij i 1 m j 1 n a ij a ij 2 i 1 m j 1 n b ij b ij 2 where i and j represent the coordinates of the elements in image matrices a and b respectively once the correlation coefficients of sub image a and all candidate sub images b are calculated the correlation coefficient matrix r g is obtained using the above equation for sub image a at a fixed grid point g between the images at times t and t 1 generally if the image quality is suitably controlled then there should be only one significant peak z max see fig a 1b thus z max corresponding to sub image b is defined as the one that matches sub image a at t 1 and the difference between the positions of both sub images a and b is obtained as a movement vector δ x g δ y g with pixel level accuracy at grid point g the velocity is then calculated in accordance with this movement vector appendix b we assessed the effects of four activation functions the rectified linear unit relu leaky relu lrelu parametric rectified linear unit prelu and exponential linear unit elu as shown in fig b 1 relu presents no saturation region or gradient dissipation and its computational overhead is low nair and hinton 2010 furthermore feedback is provided only when x 0 which is in line with the activation mechanism of biological nerves wherein a neuron is deactivated when x 0 to avoid updating parameters during training relu requires a small gradient assigned by a fixed value that is normally smaller than zero prelu requires a variable a which is learned from the slope of the data when x 0 he et al 2015 if a is small then the formula is the same as that of lrelu whereas if a 0 then the formula will be the same as that of relu prelu has been shown to approach human classification ability in the imagenet 2012 classification dataset he et al 2015 krizhevsky et al 2017 compared to relu elu requires an extra constant α to define the smoothness function when the inputs are negative by reducing the bias shift effect elu makes it possible to reduce the training time and enhance accuracy clevert et al 2015 appendix c the covariance matrix between two vector fields is given by the following equation crosby et al 1993 c 1 σ w σ 11 σ 12 σ 21 σ 22 where c 2 σ 11 σ u 1 u 1 σ u 1 v 1 σ v 1 u 1 σ v 1 v 1 c 3 σ 12 σ u 1 u 2 σ u 1 v 2 σ v 1 u 2 σ v 1 v 2 c 4 σ 21 σ u 2 u 1 σ u 2 v 1 σ v 2 u 1 σ v 2 v 1 c 5 σ 22 σ u 2 u 2 σ u 2 v 2 σ v 2 u 2 σ v 2 v 2 the vcc ρ v 2 is calculated as follows c 6 ρ v 2 t r σ 11 1 σ 12 σ 22 1 σ 21 where tr is the sum of matrix diagonal elements the calculation of vcc can be facilitated by expanding eq c 6 as follows c 7 ρ v 2 f g where f σ u 1 v 1 σ u 2 v 2 σ v 1 v 2 2 σ v 2 v 2 σ v 1 u 2 2 σ v 1 v 1 σ u 2 v 2 σ u 1 v 2 2 σ v 2 v 2 σ u 1 u 2 2 2 σ u 1 v 1 σ u 1 v 2 σ v 1 u 2 σ u 2 v 2 2 σ u 1 v 1 σ u 1 u 2 σ v 1 v 2 σ u 2 v 2 2 σ u 1 u 1 σ v 1 u 2 σ v 1 v 2 σ u 2 v 2 2 σ v 1 v 1 σ u 1 u 2 σ u 1 v 2 σ u 2 v 2 2 σ u 2 u 2 σ u 1 v 1 σ u 1 v 2 σ v 1 v 2 2 σ v 2 v 2 σ u 1 v 1 σ u 1 u 2 σ v 2 u 2 g σ u 1 u 1 σ v 1 v 1 σ u 1 v 1 2 σ u 2 u 2 σ v 2 v 2 σ u 2 v 2 2 
