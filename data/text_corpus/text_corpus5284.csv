index,text
26420,in the context of climate adaptation planning there are relationships between adaptation drivers and adaptation measures which makes the selection and implementation of the adaptation measures a challenging task this challenge may be addressed by structuring the adaptation problem using a multiple perspective adaptation framework and applying a context specific precedence grammar logic for selecting and evaluating adaptation measures precedence grammar logic is a set of rule based algorithms grammar that are based on the relationships in a local adaptation context this paper demonstrates the application of a context specific precedence grammar logic in an adaptation context in can tho vietnam adaptation pathways comprising flood adaptation measures i e dike heightening for this case were generated using rule based algorithms based on the relationships between the drivers and the adaptation measures the results show that complex adaptation issues that are structured can be resolved using a context specific adaptation grammar approach keywords adaptation pathways climate adaptation flexibility precedent grammar context specific adaptation 1 introduction urban areas which are home to more than half the world s population and composed of complex interdependent systems are a major challenge for climate change adaptation planning revi et al 2014 the complexity is due to the interactions between social economic and environmental stressors where all or any can exacerbate risk to individual and to the households wellbeing radhakrishnan et al 2017 the economic capacity and ability to make comprehensive decisions in deploying adaptation measures are seen as the key factors in determining the sustainability of deltas where the urbanisation and economic activities are concentrated tessler et al 2015 the current frameworks on risk assessment and adaptation call for accounting of all significant natural and anthropogenic drivers in adaptation related decision making ipcc 2014 un 2015 this can improve the long term resilience of cities against climate change decision making at a programme or project level is beset with uncertainties associated with the multiple drivers buurman and babovic 2016 also there are uncertainties related to system performance in the range of scenarios anticipated in the future and uncertainty regarding the ability of any strategy to adapt to future scenarios maier et al 2016 hence it can be concluded that adaptation related decision making in urban areas should take into account i the complexity of adapting urban systems to climate change ii the need for the consideration of multiple drivers especially socio economic e g population urbanisation gross domestic product gdp etc iii uncertainties associated with the drivers and iv approaches set out in extant enabling frameworks for carrying out risk assessment and development of adaptation plans dittrich et al 2016 maier et al 2016 matteo et al 2016 young and hall 2015 expertise on climate change socio economic drivers that increase vulnerability and impacts integrated assessment modelling for assessing impacts and vulnerability is becoming increasingly sophisticated hallegatte et al 2011 ipcc 2013 o neill et al 2015 however at the municipality level the level which matters most for urban adaptation there is a lack of enabling conditions and frameworks to support the timely evaluation of emerging urban adaptation measures that operate across a range of scales timelines and how these are rooted in local contexts revi et al 2014 there are recent decision supporting frameworks such as dynamic adaptation policy pathways haasnoot et al 2013 real options de neufville and scholtes 2011 woodward et al 2014 and robust decision making under uncertainty which was used in the planning of thames estuary 2100 project sayers et al 2012 there are also frameworks that approach adaptation from an investment perspective young and hall 2015 which consider the performance of measures across multiple scenarios scales and timelines in addition to dealing with uncertainty the strength of real options approaches applied in infrastructure domains is the consideration of path dependency gersonius et al 2013 path dependency is the dependability of the decisions made in the present on the decisions made in the past and or the decisions that would be made in the future that are always likely to affect the current decision however inclusion of path dependency in a multiple driver or multiple adaptation context and the inclusion of inter relationships is lacking in adaptation pathways and real options approaches also these frameworks do not address the complexity arising out of the relationships between multiple drivers and the interaction between adaptation measures at a finer scale such as at household level hence in order to help decision makers to choose and implement adaptation measures at the municipal scale it is essential to develop an evaluation framework that is i broad enough to accommodate the complexities arising out of multiple drivers ii sufficiently detailed to model the interactions at finer scale iii easy to understand and modifiable with a simple logical structure and iv context specific i e represents the inter relationships between the drivers and adaptation measures for the local adaptation context recently devised adaptation frameworks can be used to address the concerns regarding the difficulties of including multiple drivers and adaptation across scales e g radhakrishnan et al 2017 however the detailed analysis required is always likely to be complex the aim of this paper is to address this by showing how to overcome the challenge in modelling and evaluating a complex adaptation problem that has been structured by using a multiple perspective adaptation framework the paper demonstrates the application of a context specific modelling and evaluation approach islam 2016 in an urban climate adaptation context where there are multiple drivers complex interactions between the drivers relationships between the adaptation measures and multiple possible futures the paper is structured as follows a a review of relevant literature on flexible adaptation approaches and making the case for a context specific modelling framework b methodology describing the context specific modelling and evaluation framework c application of the framework in can tho city vietnam which is currently adapting to floods due to multiple drivers d discussion and evaluation of the results and e conclusions of the findings specific to can tho and how the approach can be applied in other contexts 2 the need for context specific adaptation grammar modelling of path dependencies in a multiple driver context requires the understanding of various drivers and adaptation measures in a system that is undergoing adaptation a majority of such systems are complex systems where there are components such as variables concepts relationships and evaluation metrics hinkel et al 2014 ostrom 2009 the essential features of complex systems are non linear feedback strategic interactions heterogeneity and varying time scales levin et al 2012 hence complex systems cannot be explained described predicted or modelled accurately cilliers 2001 for example urban water systems can be considered as complex adaptive systems kanta and zechman 2014 urban water systems such as urban flood risk management are comprised of variables such quantity of rainfall river discharge and are based on concepts such as satisfying basic services water sensitive cities sustainable development goals e g un 2015 they also include relationships between variables based on deterministic relationships such as rainfall runoff equations or non deterministic relationships such as how the residents of the city react to flooding e g garschagen 2015 as well as evaluation metrics such as service level bench marks e g min of urban development 2017 in the context of cities adaptation being anthropogenic i e initiated by stakeholders where a rigid system can be made adaptive through adaptation measures initiated by stakeholders hence adaptive systems in an urban contexts are adaptable systems systems whose components can be modified by decision makers for adapting to changing circumstances are termed as adaptable systems oppermann 1994 therefore complex adaptable systems can be defined as systems comprising variables concepts and components that can be changed the relationships among the variables and among the concepts can be established but cannot be fully explained described or predicted accurately understanding the relationships between the adaptation measures enables the decision makers to tailor any adaptation according to the needs and emergence of variables especially the drivers a common appraisal framework based on a system of systems approach capable of comparing combining and appraising adaptation measures across sectors is essential to achieve effective or efficient adaptation outcomes young and hall 2015 in addition to the system of system approach other approaches and perspectives can enhance the understanding of interaction between the adaptation measures and the drivers for example multiple perspectives for structuring climate adaptation radhakrishnan et al 2017 and integrating pathways zeff et al 2016 are some of the recent approaches that can be used to ascertain the relationship between adaptation measures and drivers for enhancing the effectiveness of adaptation measures there are challenges in developing an overall modelling and evaluation framework for modelling path dependencies and inter relationships in a multiple driver context firstly integrating a system of systems approach into a modelling framework is a challenge where a small system change can have a large overall systems effect maier et al 2016 secondly developing a generic evaluation framework which considers change at a local scale is also a challenge as the number nature and relationships between the drivers vary in every local context sayers et al 2015 tessler et al 2015 therefore fitting together the uncertainties and system performance in all possible scenarios is essential for evaluating adaptation measures in a multiple perspective maier et al 2016 scenarios relate entirely to changed environmental drivers stressors such as sea level rainfall intensity temperature e g ipcc 2013 scenarios and changed socio economic stressors such as gdp population growth rate rate of urbanisation e g shared socio economic pathways o neill et al 2015 evaluating the collective robustness of various adaptation measures together rather than of the individual robustness of a particular adaptation measure has been proposed as a way to better include uncertainty and to ensure robust system performance maier et al 2016 decision making methods to define and select robust measures are being recommended for determining the lowest level of trade off between optimising returns efficiency and robustness sustainability although the generic toolkits for detailed analysis are still in development dittrich et al 2016 young and hall 2015 for example have proposed such a generic method based on temporal and operational interdependence in a local context however this method needs to better include precise path dependencies inter relationships between drivers and the performance of adaptation measures in all plausible scenarios that are considered zeff et al 2016 have proposed a risk based framework based on dynamic adaption policy pathways haasnoot et al 2013 to evaluate the performance of adaptation measures in all possible scenarios this framework uses the concept of path dependencies and risk of failure as a trigger which is the a condition to switch to an adaptation measure that will prevent the failure or the occurrence of tipping point zeff et al 2016 however in this approach the inter relationships between the drivers and the relationship between the adaptation measures are not sufficiently detailed the level of detailing in the inter relationships between the drivers and adaptation i e granularity can be enhanced by means of using a context specific adaptation pathway generation and evaluation approach as developed by islam 2016 this is further discussed in the next paragraph islam 2016 has used a set of context specific rules similar to the language grammar rules in english french or any language that guide the formation of a sentence for generating adaptation pathways precedent grammar a term which is generally used in computer programming is a set of production rules for generating a combination of words or symbols based upon the relationship between the words in a given set of words or symbols floyd 1963 similarly a set of rule based algorithms grammar that are based on the relationship among the drivers adaptation measures and between the drivers and adaptation measures in a local adaptation context can be generated islam 2016 approach termed context specific precedence adaptation grammar approach comprises i context specific rules for generating adaptation pathways that are based on the relationship between the adaptation measures ii scenario generators for creating scenario combinations based on the range of drivers under consideration and iii trigger functions that are based on the adaptation objectives for switching between adaptation measures or generating adaptation pathways in a local context with multiple drivers this paper demonstrates the application of this context specific precedent grammar approach for the generation and evaluation of adaptation pathways in an urban flood risk management system 3 methodology the context specific adaptation grammar approach islam 2016 was chosen to generate and evaluate pathways as it has the following advantages i builds upon the dynamic adaptation policy pathway approach of haasnoot et al 2013 ii includes path dependencies and inter relationships unlike the generic method used by young and hall 2015 and iii is able to enhance granularity in considering the inter relationships between the drivers and adaptation measures unlike the risk based framework developed by zeff et al 2016 the context specific grammar methodology for modelling of modular systems under uncertainty developed by islam 2016 comprises the following four components i the exogenous scenario space sp influencing the system along with the number of possible scenario combinations s ii the possible modules i e the adaptation measures and their interrelations r iii the trigger g i e the control logic connecting sp and r and iv the evaluation functions e based on r and triggers which are used for evaluating the adaptation pathways along with the components the system time window t time step δ t and number of scenarios n should also be specified by understanding the components in a complex adaptable urban flood risk management system adaptation grammars scenarios trigger and evaluation functions for assessing adaptation measures can be generated the terminologies and abbreviations used in the context specific algorithm are presented in table 1 3 1 exogenous scenario space the total scenario space is defined as s p u 1 u 2 u n where the space is described as a collection of drivers u such as rainfall increase sea level increase and gdp growth along with their associated uncertainties the scenario space sp together with the planning horizon t time step δ t and number of scenarios n are used to generate a number of possible scenario combinations s s 1 s 2 s n where s i u 1 i u 2 i u n i in local contexts where necessary separate exogenous scenario combinations s trig comprising trigger drivers and evaluation scenario combinations s eval comprising evaluation drivers can be generated s trig are the scenario combinations that are used to trigger or implement the adaptation measures whereas s eval are the scenario combinations used in the final evaluation of pathways that are based on a user defined criteria 3 2 adaptation modules the system modules i e adaptation measures and their interactions are defined using the rule set r r 1 r 2 r n where each rule r1 to rn comprises 5 determinants r f c q p x a composed of a module id c pre requisites q pre exclusions p post exclusions x and attributes a let us consider for example an adaptation measure such as constructing the superstructure for a 50 cm high dike as a module with module id c 4 0 the names of adaptation measures are deliberately assigned to decimal number such as 4 0 and not as an integer or as character such as 50 dike or d50 because the context specific algorithm uses the name for generating temporary names based on the position of the measure in the pathway the prerequisite q for this module is a condition where the foundation for the 50 cm dike is already in place or a smaller dike say a 20 cm high dike is in place the pre exclusion p is the module modules that should not be in place for constructing a 50 cm dike i e if a 70 cm high dike is in place then a 50 cm dike need not be constructed the post exclusion x is the module modules that cannot be implemented once the module c is in place for example once the 50 cm high dike is in place a 20 cm dike cannot be implemented the attributes a comprise the characteristics which are specific to the 50 cm high structure such as cost of the dike time for construction lifetime of the dike etc these determinants in the rule set ensure that the irreversibility and path dependency of adaptation pathways that are encountered in reality are taken into account using rule set r a set of adaptation pathways w containing paths w0 w1 wi can be generated for example the path w0 is pathway comprising a sequence of adaptation measures such as 20 cm high dike followed by a 50 cm high dike w1 is a pathway starting with a 50 cm high dike followed by a 70 cm high dike and w2 is a pathway starting with a 20 cm high dike followed by a 70 cm dike 3 3 adaptation trigger the trigger g is the condition which determines the switch from one pathway to another in other words trigger g is a condition leading to the tipping point and hence the switch to a pathway where there is a likelihood for delay in tipping in the future for example the trigger for implementing a dike could be a certain percentage of an area submerged for once in a 100 year river level the objective of flood risk management can be to limit the area of submergence to less than 2 percent of the entire city area for instance a 50 cm high dike might be sufficient under current circumstances to fulfil the objectives but increasing river levels might enlarge the area submerged to 4 in another 5 years and hence triggers the construction of 70 cm high dike in order to keep the area submerged to less than 2 of the total the trigger can also be based on a risk based trade off i e based on cost and benefits the cartesian product w s trig of all the scenarios s comprising generated scenarios s 1 s 2 s n and pathways comprising w 1 w 2 w n is evaluated using the trigger function g g f w i s i w i s i in s t r i g x w the trigger function g determines the system state of adaptation modules which is either 1 or 0 in pathways in all scenario combinations system state of an adaptation measure is an attribute of the adaptation measures in the pathway which indicates if these measures were already in place at the start of the planning period or if these were implemented during the planning range in order to postpone the tipping point for example the system state is 0 for any adaptation pathway at the beginning of the analysis let us consider a pathway w i comprising a 50 cm dike which is later upgraded to a 70 cm dike when the increasing water levels do not necessitate the switch from the 50 cm dike to a 70 cm dike under a scenario s i the system state 0 is taken as the system state for scenario s i the system state of pathway w i becomes 1 for scenario s i where the increasing water levels trigger the switch to a 70 cm dike this comparison is done at every time step for every pathway and the result are aggregated as a value m m value is obtained for all pathways and scenarios h is a list containing the results for the pathways evaluated using the trigger function in all possible exogenous scenario combinations s t r i g and the result of the evaluation m at every time step t in the planning range t where t s is the starting year and t e is the ending year m f g w i s i t t s t o t e s 1 t o n i n t x s t r i g h m w i i n w for example a pathway containing a 50 cm high dike and a 70 cm high dike could be tested in time steps of 5 years for the next 100 years for four different sea level rise scenarios and evaluated as to when the switch occurs to the next path containing a 120 cm dike using a trigger keeping the submerged area less than 2 in this case there will be four results for the same pathway as there are four different scenarios 3 4 pathway evaluation function lastly the evaluation function e f h f f m w i s i i n s e v a l w can be any summation and or product function as every pathway has a value m for every evaluation scenario combination s e v a l the pathway can be evaluated based on the value of m for example if the pathway containing two incremental dike increases in elevation such as 50 cm first then 70 cm next are capable of maintaining the flooded extent to less than 2 across all scenarios then this pathway can be evaluated based on the timing of these two increments and the flood damages avoided by these measures user defined comparison criteria like present value of total cost along the pathway comprising implementation cost and flood damages can be used to assess the appropriate timing of modules for all possible trigger timings in a pathway in order to interpret or present the results of evaluation for the ease of understanding a ranking criteria e based on the present value of total cost can be calculated for each pathway so that the performance of the pathways can be compared across all possible scenarios islam 2016 gives further details regarding definitions descriptions about the sub components of the four main components exogenous scenario space adaptation modules adaptation triggers and pathway evaluation function comprising the context specific grammar architecture and for the logical structure behind the rules here the context specific grammar outlined above has been applied for the evaluation of climate adaptation responses for can tho city in vietnam where there is current formulation and implementation of adaptation planning for addressing changing urban flood risks and socio economic changes mdp 2013 world bank 2012 4 case study can tho the biggest and a fast growing city in the mekong delta is located in south western vietnam on the right bank of the hau river some 80 km from the sea the rapid urban development of can tho has led to unplanned growth increase in real estate prices widespread water pollution and flooding issues it also suffers prevailing social disparities in terms of the availability of housing stocks and access to services among the residents garschagen 2014 the city is likely to be affected by an increase in river water levels due to sea level rise an increase in river discharge urban runoff from rainfall and effects of land use change apel et al 2016 huong and pathirana 2013 smajgl et al 2015 van et al 2012 wassmann et al 2004 comprehensive analysis and structuring of the adaptation context in can tho using a multiple perspective adaptation framework is discussed in detail by radhakrishnan et al 2017 which provides the overall guidance for the application of context specific grammar in can tho 4 1 structuring the climate adaptation context in can tho flooding is a recurrent phenomenon in the mekong delta and the people living there have long experiences of living with floods wesselink et al 2016 the social and socio economic aspects of flood risk and its management in can tho are evident from the community experiences of living with water tolerance to flooding coping measures being undertaken at household level direct and indirect damage to the households that can be reduced because of the coping measures and preparedness due to living with water and factors that trigger the households to implement household measures chinh et al 2016a 2016b dwf 2011 garschagen 2014 2015 sce 2013 there are multiple flood risk management plans being prepared and considered for implementation in can tho by various governmental and donor agencies who focus on avoiding floods by means of i dike rings ii improvements to drainage systems iii increasing the freeboard i e the vertical allowance added to the standard flood design levels to allow for uncertainty in flood levels caused by infilling or debris accumulation of flood defence systems roads and critical infrastructure above the maximum anticipated flood level sce 2013 siwrp 2011 viap suip 2013 although can tho is making efforts at a city level and household level to adapt in the context of changing climate and changing socio economic conditions the true effectiveness of these measures is uncertain due to the uncertainties associated with the climate and socio economic forcings the uncertainties in the climate related drivers such as sea level rise and changes in rainfall are evident from global and regional level studies such as ipcc 2013 and delta level studies apel et al 2016 mdp 2013 smajgl et al 2015 the uncertainties in socio economic forcings such as gdp growth and urbanisation are evident from national level studies such as o neill et al 2015 and delta level plans such as the mekong delta plan mdp 2013 being as sure as possible about system performance across a range of scenarios where there is an interplay between various drivers is essential to tackle uncertainty maier et al 2016 the dike elevation measures 20 cm 50 cm and 70 cm increase in dike heights and household retrofitting such as raising the floor levels by up to 50 cm and beyond 50 cm are the adaptation measures that are most relevant to the local context in can tho birkmann et al 2012 sce 2013 siwrp 2011 viap suip 2013 the dike elevation measures and the corresponding construction cost of the adaptation measures have been obtained from the existing planning documents sce 2013 siwrp 2011 viap suip 2013 through consultations with the can tho city engineering department and from literature e g jonkman et al 2013 the dike elevation measures that are planned to be delivered by the city council are in response to rising river levels based on what is expected once in a hundred years sce 2013 the increase in water level of the hau river has a strong correlation with the increasing sea levels under the various climate scenarios apel et al 2016 hence the dike elevation measures can be considered as adaptation modules in the context specific adaptation grammar approach the household adaptation measures in can tho such as increasing the floor levels and the construction of temporary dikes are autonomous measures birkmann et al 2012 according to garschagen 2014 the correlation of adaptation actions at household level with flood depths as the driver is strong whereas the correlation with household incomes is weak table 6 5 garschagen 2014 also a spike in the number of houses adapting is noticed after a major flood event these inferences are based on the responses from the households during a survey conducted in 2011 however a trend could not be established between the recorded river levels reported by sce 2013 and the increase in the number of households elevating the floor levels reported by garschagen 2014 hence household measures are excluded here as an adaptation module in the pathways the increase in floor levels of houses are considered for evaluating the adaptation pathways the avoided flood damages due to increase in floor levels and the cost incurred for increasing the floor levels are substantial and has to be considered while evaluating the adaptation pathways although there is a weak correlation in the relationship of household level measures to household incomes the increase of household income and wealth is suggested as a means to increase adaptive capacity garschagen 2014 however the construction cost and flood damage costs at the households can vary depending upon the socio economic scenarios this is a context specific relationship hence to accommodate this context specific relationship the elevation of floor levels at households can be considered at the stage of evaluation of the adaptation pathways it is in this context of multiple futures of adaptation measures and interrelationships between the drivers that the flood risk management systems of can tho are considered here as a complex adaptable system under uncertainty hinkel et al 2014 oppermann 1994 ostrom 2009 4 2 application of context specific grammar in can tho the ninh kieu district situated along the hau river is the primary administrative and business area of can tho and this has been chosen as an important study area due to its economic importance 4 2 1 exogenous scenario space four climate scenarios have been considered for sea level rise slr from ipcc 2013 based on representative concentration pathways rcp 2 6 8 5 table 2 the shared socio economic pathways ssp which comprise five scenarios for gdp leimbach et al 2015 scenarios ssp1 through ssp5 and nine scenarios for urbanisation jiang and o neill 2015 scenarios fast fast ff to slow slow ss for vietnam are considered here as socio economic scenarios from table 2 it can be seen that the sea level increase can vary between 0 44 m and 0 74 m by 2100 the annual gdp growth rate which was 6 4 in 2010 is expected to decrease and is between 0 21 and 0 96 in the year 2090 based on the ssp s leimbach et al 2015 the urbanised population which was 30 4 of the total population of vietnam in 2010 is expected to increase and is likely to be between 48 43 and 91 37 in the year 2100 based on the combination of fast central and slow rates of urbanisation jiang and o neill 2015 hence the scenario space sp for can tho considered here comprises the three uncertain drivers sea level rise gdp and urbanisation sp slr rcp 2 6 slr rcp 4 5 slr rcp 6 0 slr rcp 8 5 gdp ssp 1 gdp ssp 2 gdp ssp 3 gdp ssp 4 gdp ssp 5 urb ff urb fc urb fs urb cf urb cc urb cs urb s f urb sc urb ss in urbanised population i e urb f pertains to fast growth c pertains to central growth and s pertains to slow growth the first suffix pertain to urbanisation up to year 2045 and the second suffix pertains to urbanisation after 2045 urbff is a scenario where the urbanisation is fast throughout whereas urbfs is a scenario where the growth is fast till year 2045 and slow after 2045 a large number of multiple driver combinations based on the progression of drivers within the lower and upper bounds of the uncertainty range can be generated up to the year 2100 by analysis using monthly interval time steps however in order to reduce the computation effort the number of combinations n were limited to 100 at every time step small time steps such as hourly or daily time steps would increase the computational load whereas longer time steps such as yearly will exclude the monthly and seasonal variations hence the assessment time step δ t considered was fixed as one month so that the water levels in combinations reflect the monthly variations in water levels the time window i e the planning duration is from year 2016 up to year 2100 since projections on sea level gdp and urbanisation are available up to the year 2100 for the reasons explained in section 4 1 i sea level rise has been considered as the only trigger driver due to the strong correlation between sea level rise and increase in river levels that triggers any need to increase dike height and ii gdp and urbanisation have been considered as evaluation drivers in addition to sea level rise hence the trigger scenario space s trig will comprise 100 scenario combinations comprising only sea levels whereas the evaluation scenario space s eval will comprise 100 scenario combinations comprising sea levels gdp and urbanisation s t r i g s l r 1 s l r 2 s l r 100 s l r i δ t i i n s p t s e v a l s l r 1 g d p 1 u r b 1 s l r 100 g d p 100 u r b 100 s l r i g d p i u r b i δ t i i n s p t 4 2 2 adaptation modules the adaptation measures i e modules for flood risk management for can tho and their interrelationships such as pre requisites pre exclusions and post exclusions are presented in table 3 it shows the rule set r comprising seven individual rules which represent the range of adaptation measures from a no adaptation measure state r 0 to a state where a 70 cm high dike is constructed r 6 r r 0 r 1 r 2 r 3 r 4 r 5 r 6 i e r r 0 0 0 r 1 1 0 0 0 2 0 3 0 4 0 5 0 6 0 3 0 5 0 19600 r 6 6 0 5 0 2 0 4 0 204000 for example r 1 the rule set for foundation of a 20 cm dike comprises i name or identification number c of the adaptation measure such as 1 0 ii pre requisite q 0 0 i e the no adaptation measure condition which is a necessary condition for implementing 1 0 iii pre exclusions p 2 0 3 0 4 0 5 0 6 0 i e the adaptation measures that when implemented do not necessitate the implementation of 1 0 in the future iv post exclusions x 3 0 5 0 i e adaptation measures such as complete foundation for 50 cm and 70 cm dikes that cannot be implemented when 1 0 is already in place and v attribute a 19600 million vnd which is the construction cost for a 20 cm dike for a kilometre by using the determinants for adaptation measures table 3 an adaptation pathway set w comprising pathways have been generated using the rule set r which complies with the context specific adaptation grammar algorithm although there are only six adaptation modules excluding the base case where there is no adaptation measures it is possible to generate multiple combinations that satisfy the rules in rule set r for example to implement a 70 cm dike module id 6 0 the pre requisite is the foundation module id 5 0 when a foundation module id 5 0 is in place it is possible to implement a 20 cm dike id 2 0 or a 50 cm dike id 4 0 or a 70 cm dike id 6 0 hence the pathway to 70 cm dike module id 6 0 can be a pathway with no smaller dikes or one smaller dike or two of the smaller dikes thus the adaptation pathway set w comprising 78 possible pathways has been generated a number of the adaptation pathways generated are shown in fig 1 as examples the next step after generating the pathways set w comprising all the pathways is to determine the timing of the switch from a lower dike to a higher dike for all possible scenario combinations s trig 4 2 3 adaptation trigger the dikes can be implemented for the prevention of flooding at any point in time in the planning window for adapting to the sea level increase in can tho sea level rise which leads to increased river levels necessitates the heightening of dikes to prevent flooding sce 2013 hence sea level rise is a trigger driver an anticipated increase of 40 cm of sea level by year 2090 under rcp 2 6 will trigger implementation of a 50 cm high dike whereas a 62 cm sea level rise by year 2090 under rcp 8 5 will trigger the implementation of a further increase to a 70 cm high dike hence the trigger g for switching to other measures is a condition when the river level is above the crest level of a dike that is currently in place the river level estimated is the 1 in a 100 year level based on the projected river level in the scenario combination which is influenced by the sea level rise g 0 r i v e r l e v e l c r e s t o f d i k e 1 r i v e r l e v e l c r e s t o f d i k e the trigger condition g is checked at time step i e every month however it is not practical to revise the decision to build a dike every month in this case the trigger condition g is checked for every month in a year and the decision to switch to a suitable adaptation measure has been based on the highest water level anticipated within that year all the 78 adaptation pathways in w were evaluated across 100 scenario combinations s trig between the year 2016 and year 2100 at monthly intervals the system state is 0 if g is true i e if river level less than crest level of the dike or 1 if g is false i e if river level is not less than crest level of the dike at end the of the adaptation pathway m which is the value based on the system state g for a pathway at every monthly time step between 2016 and 2100 for all scenario combinations has been determined m f g w i s i t 2016 t o 2100 s 1 t o 100 i n t x s t r i g the m for 78 pathways across 100 scenario combinations at yearly intervals between years 2016 2100 were stored in a matrix h h m w 1 t o 78 i n w the pathways which satisfy the performance requirement i e no overtopping of the dike at any point in time during the planned for sea level increase were then selected for further assessment the set of pathways that satisfy the no overtopping performance requirement for can tho are presented in fig 1 it can be seen that out of the 78 pathways there are ten possible pathways numbered 0 1 9 for implementing the various dike heightening options in can tho there are pathways with a single measure such as a 70 cm high dike pathway 2 and also pathways which contain multiple measures such as 20 50 70 cm dikes pathways 1 5 6 7 4 2 4 evaluation of adaptation pathways once the pathways which satisfy the performance requirement are selected the pathways can be ranked by user defined criteria such a relative ranking of pathways will help the decision makers to select a pathway for implementation relative ranking can be done with any criterion or a combination of criteria such as economic cost benefits eco system benefits travel time lost etc in can tho the present value of total cost along the adaptation pathway was considered in evaluating the pathways total cost comprised flood damages and cost of implementing the adaptation measures the pathways were ranked based on the expected present value of total costs along a pathway fig 2 the flood damages along the ten adaptation pathways fig 1 which satisfy the performance requirement were estimated using the modelling results from the pc swmm 1d 2d coupled hydraulic model chi 2017 of ninh kieu district developed by quan et al 2014 and radhakrishnan et al accepted the hydraulic simulations were run multiple times for all the pathways for all the river water levels corresponding to the scenario combinations in s trig and at all time steps δ t using these model runs the flood depth in the case study area ninh kieu district was determined the flood damages were estimated using the depth damage relationships sce 2013 the flood damages were estimated at monthly time steps throughout the planning horizon for all 100 scenario combinations in s eval as the flood damages depend up on the flood depth number of houses properties and the floor levels of households the construction cost of the adaptation measures consists of cost of dikes and household floor elevations whereas the damages cost comprises the road damages and household damages despite the weak correlation of the household measures with gdp and river levels the construction and damage cost for households were assumed to vary depending upon the socio economic scenario which comprises the gdp growth rate and urbanisation in order to consider the local adaptation context discussed in detail in section 4 1 the rate of urbanisation determines the number of households at any given time whereas the gdp growth rate determines the households that undertake adaptation measures as household adaptation measures are directly linked to household income garschagen 2014 depending income and exposure to flooding many households elevate their floor levels by 0 2 m 1 5 m garschagen 2014 hence gdp growth and urbanisation were considered as evaluation drivers as opposed to triggering drivers the estimated numbers of households at any given time that do not raise the floor levels that raise the levels by 0 5 m and those that raise the floor levels above 0 5 m have been used in order to compute the total construction cost and flood damages at household level the classification of the flood adapting households into two categories i e up to 0 5 m and above 0 5 m is essential to capture the dynamics of adaptation at a household level as any increase in floor levels above 0 5 m is five times more expensive than increasing floor levels by 0 5 m garschagen 2014 the pathways were evaluated based on the total flood damage and implementation cost for all the 100 evaluation scenario combinations s eval e f h f f m w 1 t o 10 s 1 t o 100 i n s e v a l w the pathways are ranked in ascending order so that those with low total cost across the majority of the scenarios are the preferred path whereas the pathways with highest total costs are the least preferred path fig 2 the pathways 0 9 are depicted along the x axis from left to right whereas the relative ranking of the pathways are presented on the y axis the least preferred pathway with lower rank denoted by a bigger number is at the bottom whereas the most preferred pathway with the highest relative rank is at the top of the y axis the coloured circles represent the evaluation scenario combination s eval and the probability of occurrence of the scenario combination the colour of the circle represents a particular scenario combination s i whereas the size of the circle represents the probability of occurrence of a particular scenario the bigger the circle the more likely the scenario is to occur for example from table 2 it can be seen that in the year 2080 the probability for sea level rise being less than or equal to 0 4 m is 0 75 i e sea level rise is less than 0 4 m in three of the four rcp s whereas the probability of sea level being greater than 0 4 is 0 25 based on the values of the three drivers in sp the probabilities for occurrence for the 100 scenario combinations in s eval are calculated from fig 2 it can be seen that the pathways 0 5 have higher ranking across most of the scenarios whereas pathways 6 9 have lower ranking across most of the scenarios hence the decision makers in can tho can focus on pathways 0 5 to select the desired pathway the other decision criteria could be the performance based on the likelihood of scenarios among pathways 0 5 the pathways 0 1 and 4 have higher ranking for scenarios that are more likely to occur pathways 5 9 also rank low in the scenario that is most likely to occur as the size of the dark green circles in the pathways are bigger and also ranks lower hence the focus can be narrowed down to the pathways 0 1 and 4 pathway 1 has the highest relative rank 1 in a scenario that is most likely to occur 5 discussion a performance analysis of adaptation pathways has been carried out for multiple scenario combinations using triggers such as sea level rise in order to arrive at a set of pathways that satisfy the performance criteria fig 1 the drivers such as sea level rise s trig which has a strong correlation with river level increase can be used as a trigger to implement adaptation measures and generate pathways the other drivers which influence the local context albeit with a weak correlation with adaptation measures can be used to evaluate the pathways based on user defined criteria in can tho there are weak correlations between gdp growth rate and urbanisation and with household adaptation measures hence these drivers have been used to generate scenario combinations s eval for the evaluation of pathways which are based on the total cost incurred along the pathways this reflects the flexibility in applying the context specific grammar as i the algorithm may be modified to consider drivers with strong correlations as triggers to generate pathways and drivers with weak correlations as evaluation criteria ii there is no compromise in level of detail for the relationships between drivers and adaptation measures in spite of the weak correlation as a strong correlation could not be established between household incomes and socio economic drivers such as gdp growth rate and urbanisation these were not considered as trigger drivers that determine the presence of adaptation measures along the pathways a context specific shared socio economic pathway ssp analysis of can tho based on specific socio economic outcomes can be undertaken by i changing the rate of poverty reduction and or economic development in can tho ii considering the effects of the socio economic development plan pm 2013 iii changing the rate of urbanisation rate and type of industrialization recommended in mekong delta plan mdp 2013 iv consideration of vulnerability birkmann et al 2012 and climate change impacts ipcc 2013 the results for pathways illustrated in fig 2 are based on the relative ranking derived using the expected present value of construction cost and flood damages that are likely to occur along a pathway for all possible trigger timings the ranking of pathways when presented together with the likelihood of the scenario combinations will help the decision maker to understand the results more easily although not presented here a combination of criteria such as minimal construction cost avoided flood damages and eco system benefits would convert the climate adaptation problem into a multiple objective problem and help in choosing pathways that satisfy all these objectives approaches followed in benefit evaluation tools such best horton et al 2016 which does not rely only on economic inputs could be used for creating the composite relative ranking criteria this will also resolve the arrow s paradox of narrowing down on solutions based on a single objective where there are also other objectives to be achieved kasprzyk et al 2016 the major limitation of this methodology is that the large number of adaptation measures in a major adaptation planning process have to be neatly quantified into smaller discrete actions that can be selected at different times hence a careful pre planning is necessary for applying this methodology also this methodology can be extended to improve flood warning times and determine the setback distances in flood plains however this needs further research as the inter relationships and stakeholders involved in flood warning and in improving setback distances are different the key towards operationalising the modelling and evaluation approaches is to account for stakeholders different beliefs regarding the levels of uncertainty as this leads to better understanding of causal relationships and preference of management solutions dewulf et al 2005 the challenge of integrating multiple perspectives system approaches and involvement of stakeholders can be overcome by applying context specific precedence grammar logic for developing an overall modelling and evaluation approach thereby enhancing the effectiveness of adaptation across the sectors 6 conclusions a context specific adaptation grammar has been used to generate and analyse climate adaptation pathways to manage flooding in can tho using the methodology adaptation pathways have been generated based on relationships between the measures that were shortlisted based on the performance of measures across scenarios and further narrowed down based upon the criteria preferred by the decision makers or stakeholders table 3 these relationships have been used for creating a set of rules r based on the precedence grammar logic to generate adaptation pathways and to assess the collective performance of the adaptation measures pathways over a range of scenarios as set out by islam 2016 decision makers at various levels households city council provincial national and international levels may use changes such as rising river water levels damage to households and increasing household incomes to trigger adaptation measures e g dike heightening drainage improvements spatial development plans and retrofitting of houses it has also been demonstrated that the relationship and interdependencies between the adaptation measures can be captured using determinants such as pre requisites pre exclusions and post exclusions table 3 these determinants have been used to generate pathways using rule based algorithms which is the core of the context specific adaptation grammar approach an example of how the inter dependencies can be used for generating pathways has been presented in section 4 2 2 for a case of building a 20 cm dike followed by a 70 cm dike the aim of this paper was to overcome the challenge in modelling and evaluating a complex adaptation problem that has been structured by using a multiple perspective adaptation framework thus this paper demonstrates the application of a context specific modelling and evaluation framework islam 2016 in the urban climate adaptation context where there are multiple drivers complex interactions between the drivers adaptation measures and multiple futures application of the context specific adaptation grammar islam 2016 in can tho reveals that it is possible to generate adaptation pathways using rule based algorithms based upon the relationship between the drivers and the adaptation measures it also shows that a context specific categorisation of drivers is possible i e the various drivers used for generating scenarios can also be segregated into trigger drivers e g sea level in can tho and evaluation drivers e g gdp growth rate and urbanisation in can tho finally there is scope for assessing adaptation pathways based on user defined criteria such as economic objectives or total costs along the pathways or non economic objectives in combination with other assessment tools eg best horton et al 2016 the results from the case study show that the complex adaptation problems structured using a generic framework can be addressed using a context specific adaptation grammar approach data software availability currently the software is in an experimental state and no separately executable version exists it is composed of a set of python scripts which run on 64 bit python 2 7 11 using the anaconda distribution 2 1 0 also the code can be made available upon on request through email from dr tushith islam who is the coauthor of this paper at tushithi yahoo com mailto tushithi yahoo com acknowledgements this technical paper is an outcome of an ongoing research funded by a cooperative research centre for water sensitive cities crc an initiative of the australian government b proacc post doctoral programme on climate change adaptation in the mekong river basin programme by the netherlands ministry of development cooperation dgis through the unesco ihe partnership research fund c hydropraxis for granting educational license for using pc swmm software appendix a supplementary data the following are the supplementary data related to this article appendix appendix dataprofile dataprofile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 016 
26420,in the context of climate adaptation planning there are relationships between adaptation drivers and adaptation measures which makes the selection and implementation of the adaptation measures a challenging task this challenge may be addressed by structuring the adaptation problem using a multiple perspective adaptation framework and applying a context specific precedence grammar logic for selecting and evaluating adaptation measures precedence grammar logic is a set of rule based algorithms grammar that are based on the relationships in a local adaptation context this paper demonstrates the application of a context specific precedence grammar logic in an adaptation context in can tho vietnam adaptation pathways comprising flood adaptation measures i e dike heightening for this case were generated using rule based algorithms based on the relationships between the drivers and the adaptation measures the results show that complex adaptation issues that are structured can be resolved using a context specific adaptation grammar approach keywords adaptation pathways climate adaptation flexibility precedent grammar context specific adaptation 1 introduction urban areas which are home to more than half the world s population and composed of complex interdependent systems are a major challenge for climate change adaptation planning revi et al 2014 the complexity is due to the interactions between social economic and environmental stressors where all or any can exacerbate risk to individual and to the households wellbeing radhakrishnan et al 2017 the economic capacity and ability to make comprehensive decisions in deploying adaptation measures are seen as the key factors in determining the sustainability of deltas where the urbanisation and economic activities are concentrated tessler et al 2015 the current frameworks on risk assessment and adaptation call for accounting of all significant natural and anthropogenic drivers in adaptation related decision making ipcc 2014 un 2015 this can improve the long term resilience of cities against climate change decision making at a programme or project level is beset with uncertainties associated with the multiple drivers buurman and babovic 2016 also there are uncertainties related to system performance in the range of scenarios anticipated in the future and uncertainty regarding the ability of any strategy to adapt to future scenarios maier et al 2016 hence it can be concluded that adaptation related decision making in urban areas should take into account i the complexity of adapting urban systems to climate change ii the need for the consideration of multiple drivers especially socio economic e g population urbanisation gross domestic product gdp etc iii uncertainties associated with the drivers and iv approaches set out in extant enabling frameworks for carrying out risk assessment and development of adaptation plans dittrich et al 2016 maier et al 2016 matteo et al 2016 young and hall 2015 expertise on climate change socio economic drivers that increase vulnerability and impacts integrated assessment modelling for assessing impacts and vulnerability is becoming increasingly sophisticated hallegatte et al 2011 ipcc 2013 o neill et al 2015 however at the municipality level the level which matters most for urban adaptation there is a lack of enabling conditions and frameworks to support the timely evaluation of emerging urban adaptation measures that operate across a range of scales timelines and how these are rooted in local contexts revi et al 2014 there are recent decision supporting frameworks such as dynamic adaptation policy pathways haasnoot et al 2013 real options de neufville and scholtes 2011 woodward et al 2014 and robust decision making under uncertainty which was used in the planning of thames estuary 2100 project sayers et al 2012 there are also frameworks that approach adaptation from an investment perspective young and hall 2015 which consider the performance of measures across multiple scenarios scales and timelines in addition to dealing with uncertainty the strength of real options approaches applied in infrastructure domains is the consideration of path dependency gersonius et al 2013 path dependency is the dependability of the decisions made in the present on the decisions made in the past and or the decisions that would be made in the future that are always likely to affect the current decision however inclusion of path dependency in a multiple driver or multiple adaptation context and the inclusion of inter relationships is lacking in adaptation pathways and real options approaches also these frameworks do not address the complexity arising out of the relationships between multiple drivers and the interaction between adaptation measures at a finer scale such as at household level hence in order to help decision makers to choose and implement adaptation measures at the municipal scale it is essential to develop an evaluation framework that is i broad enough to accommodate the complexities arising out of multiple drivers ii sufficiently detailed to model the interactions at finer scale iii easy to understand and modifiable with a simple logical structure and iv context specific i e represents the inter relationships between the drivers and adaptation measures for the local adaptation context recently devised adaptation frameworks can be used to address the concerns regarding the difficulties of including multiple drivers and adaptation across scales e g radhakrishnan et al 2017 however the detailed analysis required is always likely to be complex the aim of this paper is to address this by showing how to overcome the challenge in modelling and evaluating a complex adaptation problem that has been structured by using a multiple perspective adaptation framework the paper demonstrates the application of a context specific modelling and evaluation approach islam 2016 in an urban climate adaptation context where there are multiple drivers complex interactions between the drivers relationships between the adaptation measures and multiple possible futures the paper is structured as follows a a review of relevant literature on flexible adaptation approaches and making the case for a context specific modelling framework b methodology describing the context specific modelling and evaluation framework c application of the framework in can tho city vietnam which is currently adapting to floods due to multiple drivers d discussion and evaluation of the results and e conclusions of the findings specific to can tho and how the approach can be applied in other contexts 2 the need for context specific adaptation grammar modelling of path dependencies in a multiple driver context requires the understanding of various drivers and adaptation measures in a system that is undergoing adaptation a majority of such systems are complex systems where there are components such as variables concepts relationships and evaluation metrics hinkel et al 2014 ostrom 2009 the essential features of complex systems are non linear feedback strategic interactions heterogeneity and varying time scales levin et al 2012 hence complex systems cannot be explained described predicted or modelled accurately cilliers 2001 for example urban water systems can be considered as complex adaptive systems kanta and zechman 2014 urban water systems such as urban flood risk management are comprised of variables such quantity of rainfall river discharge and are based on concepts such as satisfying basic services water sensitive cities sustainable development goals e g un 2015 they also include relationships between variables based on deterministic relationships such as rainfall runoff equations or non deterministic relationships such as how the residents of the city react to flooding e g garschagen 2015 as well as evaluation metrics such as service level bench marks e g min of urban development 2017 in the context of cities adaptation being anthropogenic i e initiated by stakeholders where a rigid system can be made adaptive through adaptation measures initiated by stakeholders hence adaptive systems in an urban contexts are adaptable systems systems whose components can be modified by decision makers for adapting to changing circumstances are termed as adaptable systems oppermann 1994 therefore complex adaptable systems can be defined as systems comprising variables concepts and components that can be changed the relationships among the variables and among the concepts can be established but cannot be fully explained described or predicted accurately understanding the relationships between the adaptation measures enables the decision makers to tailor any adaptation according to the needs and emergence of variables especially the drivers a common appraisal framework based on a system of systems approach capable of comparing combining and appraising adaptation measures across sectors is essential to achieve effective or efficient adaptation outcomes young and hall 2015 in addition to the system of system approach other approaches and perspectives can enhance the understanding of interaction between the adaptation measures and the drivers for example multiple perspectives for structuring climate adaptation radhakrishnan et al 2017 and integrating pathways zeff et al 2016 are some of the recent approaches that can be used to ascertain the relationship between adaptation measures and drivers for enhancing the effectiveness of adaptation measures there are challenges in developing an overall modelling and evaluation framework for modelling path dependencies and inter relationships in a multiple driver context firstly integrating a system of systems approach into a modelling framework is a challenge where a small system change can have a large overall systems effect maier et al 2016 secondly developing a generic evaluation framework which considers change at a local scale is also a challenge as the number nature and relationships between the drivers vary in every local context sayers et al 2015 tessler et al 2015 therefore fitting together the uncertainties and system performance in all possible scenarios is essential for evaluating adaptation measures in a multiple perspective maier et al 2016 scenarios relate entirely to changed environmental drivers stressors such as sea level rainfall intensity temperature e g ipcc 2013 scenarios and changed socio economic stressors such as gdp population growth rate rate of urbanisation e g shared socio economic pathways o neill et al 2015 evaluating the collective robustness of various adaptation measures together rather than of the individual robustness of a particular adaptation measure has been proposed as a way to better include uncertainty and to ensure robust system performance maier et al 2016 decision making methods to define and select robust measures are being recommended for determining the lowest level of trade off between optimising returns efficiency and robustness sustainability although the generic toolkits for detailed analysis are still in development dittrich et al 2016 young and hall 2015 for example have proposed such a generic method based on temporal and operational interdependence in a local context however this method needs to better include precise path dependencies inter relationships between drivers and the performance of adaptation measures in all plausible scenarios that are considered zeff et al 2016 have proposed a risk based framework based on dynamic adaption policy pathways haasnoot et al 2013 to evaluate the performance of adaptation measures in all possible scenarios this framework uses the concept of path dependencies and risk of failure as a trigger which is the a condition to switch to an adaptation measure that will prevent the failure or the occurrence of tipping point zeff et al 2016 however in this approach the inter relationships between the drivers and the relationship between the adaptation measures are not sufficiently detailed the level of detailing in the inter relationships between the drivers and adaptation i e granularity can be enhanced by means of using a context specific adaptation pathway generation and evaluation approach as developed by islam 2016 this is further discussed in the next paragraph islam 2016 has used a set of context specific rules similar to the language grammar rules in english french or any language that guide the formation of a sentence for generating adaptation pathways precedent grammar a term which is generally used in computer programming is a set of production rules for generating a combination of words or symbols based upon the relationship between the words in a given set of words or symbols floyd 1963 similarly a set of rule based algorithms grammar that are based on the relationship among the drivers adaptation measures and between the drivers and adaptation measures in a local adaptation context can be generated islam 2016 approach termed context specific precedence adaptation grammar approach comprises i context specific rules for generating adaptation pathways that are based on the relationship between the adaptation measures ii scenario generators for creating scenario combinations based on the range of drivers under consideration and iii trigger functions that are based on the adaptation objectives for switching between adaptation measures or generating adaptation pathways in a local context with multiple drivers this paper demonstrates the application of this context specific precedent grammar approach for the generation and evaluation of adaptation pathways in an urban flood risk management system 3 methodology the context specific adaptation grammar approach islam 2016 was chosen to generate and evaluate pathways as it has the following advantages i builds upon the dynamic adaptation policy pathway approach of haasnoot et al 2013 ii includes path dependencies and inter relationships unlike the generic method used by young and hall 2015 and iii is able to enhance granularity in considering the inter relationships between the drivers and adaptation measures unlike the risk based framework developed by zeff et al 2016 the context specific grammar methodology for modelling of modular systems under uncertainty developed by islam 2016 comprises the following four components i the exogenous scenario space sp influencing the system along with the number of possible scenario combinations s ii the possible modules i e the adaptation measures and their interrelations r iii the trigger g i e the control logic connecting sp and r and iv the evaluation functions e based on r and triggers which are used for evaluating the adaptation pathways along with the components the system time window t time step δ t and number of scenarios n should also be specified by understanding the components in a complex adaptable urban flood risk management system adaptation grammars scenarios trigger and evaluation functions for assessing adaptation measures can be generated the terminologies and abbreviations used in the context specific algorithm are presented in table 1 3 1 exogenous scenario space the total scenario space is defined as s p u 1 u 2 u n where the space is described as a collection of drivers u such as rainfall increase sea level increase and gdp growth along with their associated uncertainties the scenario space sp together with the planning horizon t time step δ t and number of scenarios n are used to generate a number of possible scenario combinations s s 1 s 2 s n where s i u 1 i u 2 i u n i in local contexts where necessary separate exogenous scenario combinations s trig comprising trigger drivers and evaluation scenario combinations s eval comprising evaluation drivers can be generated s trig are the scenario combinations that are used to trigger or implement the adaptation measures whereas s eval are the scenario combinations used in the final evaluation of pathways that are based on a user defined criteria 3 2 adaptation modules the system modules i e adaptation measures and their interactions are defined using the rule set r r 1 r 2 r n where each rule r1 to rn comprises 5 determinants r f c q p x a composed of a module id c pre requisites q pre exclusions p post exclusions x and attributes a let us consider for example an adaptation measure such as constructing the superstructure for a 50 cm high dike as a module with module id c 4 0 the names of adaptation measures are deliberately assigned to decimal number such as 4 0 and not as an integer or as character such as 50 dike or d50 because the context specific algorithm uses the name for generating temporary names based on the position of the measure in the pathway the prerequisite q for this module is a condition where the foundation for the 50 cm dike is already in place or a smaller dike say a 20 cm high dike is in place the pre exclusion p is the module modules that should not be in place for constructing a 50 cm dike i e if a 70 cm high dike is in place then a 50 cm dike need not be constructed the post exclusion x is the module modules that cannot be implemented once the module c is in place for example once the 50 cm high dike is in place a 20 cm dike cannot be implemented the attributes a comprise the characteristics which are specific to the 50 cm high structure such as cost of the dike time for construction lifetime of the dike etc these determinants in the rule set ensure that the irreversibility and path dependency of adaptation pathways that are encountered in reality are taken into account using rule set r a set of adaptation pathways w containing paths w0 w1 wi can be generated for example the path w0 is pathway comprising a sequence of adaptation measures such as 20 cm high dike followed by a 50 cm high dike w1 is a pathway starting with a 50 cm high dike followed by a 70 cm high dike and w2 is a pathway starting with a 20 cm high dike followed by a 70 cm dike 3 3 adaptation trigger the trigger g is the condition which determines the switch from one pathway to another in other words trigger g is a condition leading to the tipping point and hence the switch to a pathway where there is a likelihood for delay in tipping in the future for example the trigger for implementing a dike could be a certain percentage of an area submerged for once in a 100 year river level the objective of flood risk management can be to limit the area of submergence to less than 2 percent of the entire city area for instance a 50 cm high dike might be sufficient under current circumstances to fulfil the objectives but increasing river levels might enlarge the area submerged to 4 in another 5 years and hence triggers the construction of 70 cm high dike in order to keep the area submerged to less than 2 of the total the trigger can also be based on a risk based trade off i e based on cost and benefits the cartesian product w s trig of all the scenarios s comprising generated scenarios s 1 s 2 s n and pathways comprising w 1 w 2 w n is evaluated using the trigger function g g f w i s i w i s i in s t r i g x w the trigger function g determines the system state of adaptation modules which is either 1 or 0 in pathways in all scenario combinations system state of an adaptation measure is an attribute of the adaptation measures in the pathway which indicates if these measures were already in place at the start of the planning period or if these were implemented during the planning range in order to postpone the tipping point for example the system state is 0 for any adaptation pathway at the beginning of the analysis let us consider a pathway w i comprising a 50 cm dike which is later upgraded to a 70 cm dike when the increasing water levels do not necessitate the switch from the 50 cm dike to a 70 cm dike under a scenario s i the system state 0 is taken as the system state for scenario s i the system state of pathway w i becomes 1 for scenario s i where the increasing water levels trigger the switch to a 70 cm dike this comparison is done at every time step for every pathway and the result are aggregated as a value m m value is obtained for all pathways and scenarios h is a list containing the results for the pathways evaluated using the trigger function in all possible exogenous scenario combinations s t r i g and the result of the evaluation m at every time step t in the planning range t where t s is the starting year and t e is the ending year m f g w i s i t t s t o t e s 1 t o n i n t x s t r i g h m w i i n w for example a pathway containing a 50 cm high dike and a 70 cm high dike could be tested in time steps of 5 years for the next 100 years for four different sea level rise scenarios and evaluated as to when the switch occurs to the next path containing a 120 cm dike using a trigger keeping the submerged area less than 2 in this case there will be four results for the same pathway as there are four different scenarios 3 4 pathway evaluation function lastly the evaluation function e f h f f m w i s i i n s e v a l w can be any summation and or product function as every pathway has a value m for every evaluation scenario combination s e v a l the pathway can be evaluated based on the value of m for example if the pathway containing two incremental dike increases in elevation such as 50 cm first then 70 cm next are capable of maintaining the flooded extent to less than 2 across all scenarios then this pathway can be evaluated based on the timing of these two increments and the flood damages avoided by these measures user defined comparison criteria like present value of total cost along the pathway comprising implementation cost and flood damages can be used to assess the appropriate timing of modules for all possible trigger timings in a pathway in order to interpret or present the results of evaluation for the ease of understanding a ranking criteria e based on the present value of total cost can be calculated for each pathway so that the performance of the pathways can be compared across all possible scenarios islam 2016 gives further details regarding definitions descriptions about the sub components of the four main components exogenous scenario space adaptation modules adaptation triggers and pathway evaluation function comprising the context specific grammar architecture and for the logical structure behind the rules here the context specific grammar outlined above has been applied for the evaluation of climate adaptation responses for can tho city in vietnam where there is current formulation and implementation of adaptation planning for addressing changing urban flood risks and socio economic changes mdp 2013 world bank 2012 4 case study can tho the biggest and a fast growing city in the mekong delta is located in south western vietnam on the right bank of the hau river some 80 km from the sea the rapid urban development of can tho has led to unplanned growth increase in real estate prices widespread water pollution and flooding issues it also suffers prevailing social disparities in terms of the availability of housing stocks and access to services among the residents garschagen 2014 the city is likely to be affected by an increase in river water levels due to sea level rise an increase in river discharge urban runoff from rainfall and effects of land use change apel et al 2016 huong and pathirana 2013 smajgl et al 2015 van et al 2012 wassmann et al 2004 comprehensive analysis and structuring of the adaptation context in can tho using a multiple perspective adaptation framework is discussed in detail by radhakrishnan et al 2017 which provides the overall guidance for the application of context specific grammar in can tho 4 1 structuring the climate adaptation context in can tho flooding is a recurrent phenomenon in the mekong delta and the people living there have long experiences of living with floods wesselink et al 2016 the social and socio economic aspects of flood risk and its management in can tho are evident from the community experiences of living with water tolerance to flooding coping measures being undertaken at household level direct and indirect damage to the households that can be reduced because of the coping measures and preparedness due to living with water and factors that trigger the households to implement household measures chinh et al 2016a 2016b dwf 2011 garschagen 2014 2015 sce 2013 there are multiple flood risk management plans being prepared and considered for implementation in can tho by various governmental and donor agencies who focus on avoiding floods by means of i dike rings ii improvements to drainage systems iii increasing the freeboard i e the vertical allowance added to the standard flood design levels to allow for uncertainty in flood levels caused by infilling or debris accumulation of flood defence systems roads and critical infrastructure above the maximum anticipated flood level sce 2013 siwrp 2011 viap suip 2013 although can tho is making efforts at a city level and household level to adapt in the context of changing climate and changing socio economic conditions the true effectiveness of these measures is uncertain due to the uncertainties associated with the climate and socio economic forcings the uncertainties in the climate related drivers such as sea level rise and changes in rainfall are evident from global and regional level studies such as ipcc 2013 and delta level studies apel et al 2016 mdp 2013 smajgl et al 2015 the uncertainties in socio economic forcings such as gdp growth and urbanisation are evident from national level studies such as o neill et al 2015 and delta level plans such as the mekong delta plan mdp 2013 being as sure as possible about system performance across a range of scenarios where there is an interplay between various drivers is essential to tackle uncertainty maier et al 2016 the dike elevation measures 20 cm 50 cm and 70 cm increase in dike heights and household retrofitting such as raising the floor levels by up to 50 cm and beyond 50 cm are the adaptation measures that are most relevant to the local context in can tho birkmann et al 2012 sce 2013 siwrp 2011 viap suip 2013 the dike elevation measures and the corresponding construction cost of the adaptation measures have been obtained from the existing planning documents sce 2013 siwrp 2011 viap suip 2013 through consultations with the can tho city engineering department and from literature e g jonkman et al 2013 the dike elevation measures that are planned to be delivered by the city council are in response to rising river levels based on what is expected once in a hundred years sce 2013 the increase in water level of the hau river has a strong correlation with the increasing sea levels under the various climate scenarios apel et al 2016 hence the dike elevation measures can be considered as adaptation modules in the context specific adaptation grammar approach the household adaptation measures in can tho such as increasing the floor levels and the construction of temporary dikes are autonomous measures birkmann et al 2012 according to garschagen 2014 the correlation of adaptation actions at household level with flood depths as the driver is strong whereas the correlation with household incomes is weak table 6 5 garschagen 2014 also a spike in the number of houses adapting is noticed after a major flood event these inferences are based on the responses from the households during a survey conducted in 2011 however a trend could not be established between the recorded river levels reported by sce 2013 and the increase in the number of households elevating the floor levels reported by garschagen 2014 hence household measures are excluded here as an adaptation module in the pathways the increase in floor levels of houses are considered for evaluating the adaptation pathways the avoided flood damages due to increase in floor levels and the cost incurred for increasing the floor levels are substantial and has to be considered while evaluating the adaptation pathways although there is a weak correlation in the relationship of household level measures to household incomes the increase of household income and wealth is suggested as a means to increase adaptive capacity garschagen 2014 however the construction cost and flood damage costs at the households can vary depending upon the socio economic scenarios this is a context specific relationship hence to accommodate this context specific relationship the elevation of floor levels at households can be considered at the stage of evaluation of the adaptation pathways it is in this context of multiple futures of adaptation measures and interrelationships between the drivers that the flood risk management systems of can tho are considered here as a complex adaptable system under uncertainty hinkel et al 2014 oppermann 1994 ostrom 2009 4 2 application of context specific grammar in can tho the ninh kieu district situated along the hau river is the primary administrative and business area of can tho and this has been chosen as an important study area due to its economic importance 4 2 1 exogenous scenario space four climate scenarios have been considered for sea level rise slr from ipcc 2013 based on representative concentration pathways rcp 2 6 8 5 table 2 the shared socio economic pathways ssp which comprise five scenarios for gdp leimbach et al 2015 scenarios ssp1 through ssp5 and nine scenarios for urbanisation jiang and o neill 2015 scenarios fast fast ff to slow slow ss for vietnam are considered here as socio economic scenarios from table 2 it can be seen that the sea level increase can vary between 0 44 m and 0 74 m by 2100 the annual gdp growth rate which was 6 4 in 2010 is expected to decrease and is between 0 21 and 0 96 in the year 2090 based on the ssp s leimbach et al 2015 the urbanised population which was 30 4 of the total population of vietnam in 2010 is expected to increase and is likely to be between 48 43 and 91 37 in the year 2100 based on the combination of fast central and slow rates of urbanisation jiang and o neill 2015 hence the scenario space sp for can tho considered here comprises the three uncertain drivers sea level rise gdp and urbanisation sp slr rcp 2 6 slr rcp 4 5 slr rcp 6 0 slr rcp 8 5 gdp ssp 1 gdp ssp 2 gdp ssp 3 gdp ssp 4 gdp ssp 5 urb ff urb fc urb fs urb cf urb cc urb cs urb s f urb sc urb ss in urbanised population i e urb f pertains to fast growth c pertains to central growth and s pertains to slow growth the first suffix pertain to urbanisation up to year 2045 and the second suffix pertains to urbanisation after 2045 urbff is a scenario where the urbanisation is fast throughout whereas urbfs is a scenario where the growth is fast till year 2045 and slow after 2045 a large number of multiple driver combinations based on the progression of drivers within the lower and upper bounds of the uncertainty range can be generated up to the year 2100 by analysis using monthly interval time steps however in order to reduce the computation effort the number of combinations n were limited to 100 at every time step small time steps such as hourly or daily time steps would increase the computational load whereas longer time steps such as yearly will exclude the monthly and seasonal variations hence the assessment time step δ t considered was fixed as one month so that the water levels in combinations reflect the monthly variations in water levels the time window i e the planning duration is from year 2016 up to year 2100 since projections on sea level gdp and urbanisation are available up to the year 2100 for the reasons explained in section 4 1 i sea level rise has been considered as the only trigger driver due to the strong correlation between sea level rise and increase in river levels that triggers any need to increase dike height and ii gdp and urbanisation have been considered as evaluation drivers in addition to sea level rise hence the trigger scenario space s trig will comprise 100 scenario combinations comprising only sea levels whereas the evaluation scenario space s eval will comprise 100 scenario combinations comprising sea levels gdp and urbanisation s t r i g s l r 1 s l r 2 s l r 100 s l r i δ t i i n s p t s e v a l s l r 1 g d p 1 u r b 1 s l r 100 g d p 100 u r b 100 s l r i g d p i u r b i δ t i i n s p t 4 2 2 adaptation modules the adaptation measures i e modules for flood risk management for can tho and their interrelationships such as pre requisites pre exclusions and post exclusions are presented in table 3 it shows the rule set r comprising seven individual rules which represent the range of adaptation measures from a no adaptation measure state r 0 to a state where a 70 cm high dike is constructed r 6 r r 0 r 1 r 2 r 3 r 4 r 5 r 6 i e r r 0 0 0 r 1 1 0 0 0 2 0 3 0 4 0 5 0 6 0 3 0 5 0 19600 r 6 6 0 5 0 2 0 4 0 204000 for example r 1 the rule set for foundation of a 20 cm dike comprises i name or identification number c of the adaptation measure such as 1 0 ii pre requisite q 0 0 i e the no adaptation measure condition which is a necessary condition for implementing 1 0 iii pre exclusions p 2 0 3 0 4 0 5 0 6 0 i e the adaptation measures that when implemented do not necessitate the implementation of 1 0 in the future iv post exclusions x 3 0 5 0 i e adaptation measures such as complete foundation for 50 cm and 70 cm dikes that cannot be implemented when 1 0 is already in place and v attribute a 19600 million vnd which is the construction cost for a 20 cm dike for a kilometre by using the determinants for adaptation measures table 3 an adaptation pathway set w comprising pathways have been generated using the rule set r which complies with the context specific adaptation grammar algorithm although there are only six adaptation modules excluding the base case where there is no adaptation measures it is possible to generate multiple combinations that satisfy the rules in rule set r for example to implement a 70 cm dike module id 6 0 the pre requisite is the foundation module id 5 0 when a foundation module id 5 0 is in place it is possible to implement a 20 cm dike id 2 0 or a 50 cm dike id 4 0 or a 70 cm dike id 6 0 hence the pathway to 70 cm dike module id 6 0 can be a pathway with no smaller dikes or one smaller dike or two of the smaller dikes thus the adaptation pathway set w comprising 78 possible pathways has been generated a number of the adaptation pathways generated are shown in fig 1 as examples the next step after generating the pathways set w comprising all the pathways is to determine the timing of the switch from a lower dike to a higher dike for all possible scenario combinations s trig 4 2 3 adaptation trigger the dikes can be implemented for the prevention of flooding at any point in time in the planning window for adapting to the sea level increase in can tho sea level rise which leads to increased river levels necessitates the heightening of dikes to prevent flooding sce 2013 hence sea level rise is a trigger driver an anticipated increase of 40 cm of sea level by year 2090 under rcp 2 6 will trigger implementation of a 50 cm high dike whereas a 62 cm sea level rise by year 2090 under rcp 8 5 will trigger the implementation of a further increase to a 70 cm high dike hence the trigger g for switching to other measures is a condition when the river level is above the crest level of a dike that is currently in place the river level estimated is the 1 in a 100 year level based on the projected river level in the scenario combination which is influenced by the sea level rise g 0 r i v e r l e v e l c r e s t o f d i k e 1 r i v e r l e v e l c r e s t o f d i k e the trigger condition g is checked at time step i e every month however it is not practical to revise the decision to build a dike every month in this case the trigger condition g is checked for every month in a year and the decision to switch to a suitable adaptation measure has been based on the highest water level anticipated within that year all the 78 adaptation pathways in w were evaluated across 100 scenario combinations s trig between the year 2016 and year 2100 at monthly intervals the system state is 0 if g is true i e if river level less than crest level of the dike or 1 if g is false i e if river level is not less than crest level of the dike at end the of the adaptation pathway m which is the value based on the system state g for a pathway at every monthly time step between 2016 and 2100 for all scenario combinations has been determined m f g w i s i t 2016 t o 2100 s 1 t o 100 i n t x s t r i g the m for 78 pathways across 100 scenario combinations at yearly intervals between years 2016 2100 were stored in a matrix h h m w 1 t o 78 i n w the pathways which satisfy the performance requirement i e no overtopping of the dike at any point in time during the planned for sea level increase were then selected for further assessment the set of pathways that satisfy the no overtopping performance requirement for can tho are presented in fig 1 it can be seen that out of the 78 pathways there are ten possible pathways numbered 0 1 9 for implementing the various dike heightening options in can tho there are pathways with a single measure such as a 70 cm high dike pathway 2 and also pathways which contain multiple measures such as 20 50 70 cm dikes pathways 1 5 6 7 4 2 4 evaluation of adaptation pathways once the pathways which satisfy the performance requirement are selected the pathways can be ranked by user defined criteria such a relative ranking of pathways will help the decision makers to select a pathway for implementation relative ranking can be done with any criterion or a combination of criteria such as economic cost benefits eco system benefits travel time lost etc in can tho the present value of total cost along the adaptation pathway was considered in evaluating the pathways total cost comprised flood damages and cost of implementing the adaptation measures the pathways were ranked based on the expected present value of total costs along a pathway fig 2 the flood damages along the ten adaptation pathways fig 1 which satisfy the performance requirement were estimated using the modelling results from the pc swmm 1d 2d coupled hydraulic model chi 2017 of ninh kieu district developed by quan et al 2014 and radhakrishnan et al accepted the hydraulic simulations were run multiple times for all the pathways for all the river water levels corresponding to the scenario combinations in s trig and at all time steps δ t using these model runs the flood depth in the case study area ninh kieu district was determined the flood damages were estimated using the depth damage relationships sce 2013 the flood damages were estimated at monthly time steps throughout the planning horizon for all 100 scenario combinations in s eval as the flood damages depend up on the flood depth number of houses properties and the floor levels of households the construction cost of the adaptation measures consists of cost of dikes and household floor elevations whereas the damages cost comprises the road damages and household damages despite the weak correlation of the household measures with gdp and river levels the construction and damage cost for households were assumed to vary depending upon the socio economic scenario which comprises the gdp growth rate and urbanisation in order to consider the local adaptation context discussed in detail in section 4 1 the rate of urbanisation determines the number of households at any given time whereas the gdp growth rate determines the households that undertake adaptation measures as household adaptation measures are directly linked to household income garschagen 2014 depending income and exposure to flooding many households elevate their floor levels by 0 2 m 1 5 m garschagen 2014 hence gdp growth and urbanisation were considered as evaluation drivers as opposed to triggering drivers the estimated numbers of households at any given time that do not raise the floor levels that raise the levels by 0 5 m and those that raise the floor levels above 0 5 m have been used in order to compute the total construction cost and flood damages at household level the classification of the flood adapting households into two categories i e up to 0 5 m and above 0 5 m is essential to capture the dynamics of adaptation at a household level as any increase in floor levels above 0 5 m is five times more expensive than increasing floor levels by 0 5 m garschagen 2014 the pathways were evaluated based on the total flood damage and implementation cost for all the 100 evaluation scenario combinations s eval e f h f f m w 1 t o 10 s 1 t o 100 i n s e v a l w the pathways are ranked in ascending order so that those with low total cost across the majority of the scenarios are the preferred path whereas the pathways with highest total costs are the least preferred path fig 2 the pathways 0 9 are depicted along the x axis from left to right whereas the relative ranking of the pathways are presented on the y axis the least preferred pathway with lower rank denoted by a bigger number is at the bottom whereas the most preferred pathway with the highest relative rank is at the top of the y axis the coloured circles represent the evaluation scenario combination s eval and the probability of occurrence of the scenario combination the colour of the circle represents a particular scenario combination s i whereas the size of the circle represents the probability of occurrence of a particular scenario the bigger the circle the more likely the scenario is to occur for example from table 2 it can be seen that in the year 2080 the probability for sea level rise being less than or equal to 0 4 m is 0 75 i e sea level rise is less than 0 4 m in three of the four rcp s whereas the probability of sea level being greater than 0 4 is 0 25 based on the values of the three drivers in sp the probabilities for occurrence for the 100 scenario combinations in s eval are calculated from fig 2 it can be seen that the pathways 0 5 have higher ranking across most of the scenarios whereas pathways 6 9 have lower ranking across most of the scenarios hence the decision makers in can tho can focus on pathways 0 5 to select the desired pathway the other decision criteria could be the performance based on the likelihood of scenarios among pathways 0 5 the pathways 0 1 and 4 have higher ranking for scenarios that are more likely to occur pathways 5 9 also rank low in the scenario that is most likely to occur as the size of the dark green circles in the pathways are bigger and also ranks lower hence the focus can be narrowed down to the pathways 0 1 and 4 pathway 1 has the highest relative rank 1 in a scenario that is most likely to occur 5 discussion a performance analysis of adaptation pathways has been carried out for multiple scenario combinations using triggers such as sea level rise in order to arrive at a set of pathways that satisfy the performance criteria fig 1 the drivers such as sea level rise s trig which has a strong correlation with river level increase can be used as a trigger to implement adaptation measures and generate pathways the other drivers which influence the local context albeit with a weak correlation with adaptation measures can be used to evaluate the pathways based on user defined criteria in can tho there are weak correlations between gdp growth rate and urbanisation and with household adaptation measures hence these drivers have been used to generate scenario combinations s eval for the evaluation of pathways which are based on the total cost incurred along the pathways this reflects the flexibility in applying the context specific grammar as i the algorithm may be modified to consider drivers with strong correlations as triggers to generate pathways and drivers with weak correlations as evaluation criteria ii there is no compromise in level of detail for the relationships between drivers and adaptation measures in spite of the weak correlation as a strong correlation could not be established between household incomes and socio economic drivers such as gdp growth rate and urbanisation these were not considered as trigger drivers that determine the presence of adaptation measures along the pathways a context specific shared socio economic pathway ssp analysis of can tho based on specific socio economic outcomes can be undertaken by i changing the rate of poverty reduction and or economic development in can tho ii considering the effects of the socio economic development plan pm 2013 iii changing the rate of urbanisation rate and type of industrialization recommended in mekong delta plan mdp 2013 iv consideration of vulnerability birkmann et al 2012 and climate change impacts ipcc 2013 the results for pathways illustrated in fig 2 are based on the relative ranking derived using the expected present value of construction cost and flood damages that are likely to occur along a pathway for all possible trigger timings the ranking of pathways when presented together with the likelihood of the scenario combinations will help the decision maker to understand the results more easily although not presented here a combination of criteria such as minimal construction cost avoided flood damages and eco system benefits would convert the climate adaptation problem into a multiple objective problem and help in choosing pathways that satisfy all these objectives approaches followed in benefit evaluation tools such best horton et al 2016 which does not rely only on economic inputs could be used for creating the composite relative ranking criteria this will also resolve the arrow s paradox of narrowing down on solutions based on a single objective where there are also other objectives to be achieved kasprzyk et al 2016 the major limitation of this methodology is that the large number of adaptation measures in a major adaptation planning process have to be neatly quantified into smaller discrete actions that can be selected at different times hence a careful pre planning is necessary for applying this methodology also this methodology can be extended to improve flood warning times and determine the setback distances in flood plains however this needs further research as the inter relationships and stakeholders involved in flood warning and in improving setback distances are different the key towards operationalising the modelling and evaluation approaches is to account for stakeholders different beliefs regarding the levels of uncertainty as this leads to better understanding of causal relationships and preference of management solutions dewulf et al 2005 the challenge of integrating multiple perspectives system approaches and involvement of stakeholders can be overcome by applying context specific precedence grammar logic for developing an overall modelling and evaluation approach thereby enhancing the effectiveness of adaptation across the sectors 6 conclusions a context specific adaptation grammar has been used to generate and analyse climate adaptation pathways to manage flooding in can tho using the methodology adaptation pathways have been generated based on relationships between the measures that were shortlisted based on the performance of measures across scenarios and further narrowed down based upon the criteria preferred by the decision makers or stakeholders table 3 these relationships have been used for creating a set of rules r based on the precedence grammar logic to generate adaptation pathways and to assess the collective performance of the adaptation measures pathways over a range of scenarios as set out by islam 2016 decision makers at various levels households city council provincial national and international levels may use changes such as rising river water levels damage to households and increasing household incomes to trigger adaptation measures e g dike heightening drainage improvements spatial development plans and retrofitting of houses it has also been demonstrated that the relationship and interdependencies between the adaptation measures can be captured using determinants such as pre requisites pre exclusions and post exclusions table 3 these determinants have been used to generate pathways using rule based algorithms which is the core of the context specific adaptation grammar approach an example of how the inter dependencies can be used for generating pathways has been presented in section 4 2 2 for a case of building a 20 cm dike followed by a 70 cm dike the aim of this paper was to overcome the challenge in modelling and evaluating a complex adaptation problem that has been structured by using a multiple perspective adaptation framework thus this paper demonstrates the application of a context specific modelling and evaluation framework islam 2016 in the urban climate adaptation context where there are multiple drivers complex interactions between the drivers adaptation measures and multiple futures application of the context specific adaptation grammar islam 2016 in can tho reveals that it is possible to generate adaptation pathways using rule based algorithms based upon the relationship between the drivers and the adaptation measures it also shows that a context specific categorisation of drivers is possible i e the various drivers used for generating scenarios can also be segregated into trigger drivers e g sea level in can tho and evaluation drivers e g gdp growth rate and urbanisation in can tho finally there is scope for assessing adaptation pathways based on user defined criteria such as economic objectives or total costs along the pathways or non economic objectives in combination with other assessment tools eg best horton et al 2016 the results from the case study show that the complex adaptation problems structured using a generic framework can be addressed using a context specific adaptation grammar approach data software availability currently the software is in an experimental state and no separately executable version exists it is composed of a set of python scripts which run on 64 bit python 2 7 11 using the anaconda distribution 2 1 0 also the code can be made available upon on request through email from dr tushith islam who is the coauthor of this paper at tushithi yahoo com mailto tushithi yahoo com acknowledgements this technical paper is an outcome of an ongoing research funded by a cooperative research centre for water sensitive cities crc an initiative of the australian government b proacc post doctoral programme on climate change adaptation in the mekong river basin programme by the netherlands ministry of development cooperation dgis through the unesco ihe partnership research fund c hydropraxis for granting educational license for using pc swmm software appendix a supplementary data the following are the supplementary data related to this article appendix appendix dataprofile dataprofile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 016 
26421,a large number of input factors are involved in mechanistic models for constructed wetlands simulation proper calibration procedure requires identifying which are the most influential ones in the simulation of main interest outputs total suspended solids concentration is an essential component in the simulation of eutrophic water treatment in this paper a global sensitivity analysis using the morris method and scaled elementary effects was performed in order to identify which are the most influential input factors in total suspended solids simulation in free water surface constructed wetlands treating eutrophic water results indicated that the most influential one was the sedimentation velocity of total suspended solids followed by parameters related to the resuspension process caused by wind which presented non linear or interaction effects keywords sensitivity analysis mathematical modelling suspended solids morris method eutrophic water 1 introduction constructed wetlands cws are complex systems for treating wastewater where several physical chemical and biological processes take place understanding these processes is needed to optimize the performance of cws but it is hindered by the large number of processes and their interdependency in this context mathematical modelling has been considered a promising tool to increase the understanding of the processes involved in cws samsó et al 2014 several mechanistic models have proven to be helpful for increasing the knowledge about the performance of cws and optimizing both their design and management bio pore samsó and garcia 2013 and hydrus wetland module langergraber and simunek 2012 are mechanistic wetlands models formed by a number of sub models that represents the different elements in a cw and their interactions among others these are the biokinetic model water flow model transport model or the wetland plants model biokinetic models describe transformation and degradation processes and are frequently highly parameterized models the model constructed wetland model number 1 cwm1 langergraber et al 2009 contains 59 parameters and the biokinetic model constructed wetlands 2d cw2d langergraber 2001 is formed by 46 parameters therefore identifying which are the most influential input factors in the simulation of each output is required for achieving properly calibration procedures however mathematical simulations are subject to uncertainty since models are an abstract approximation of reality based on a number of assumptions and sensitivity analysis are useful tools to quantify how any modification in model s input factors affect the model s output robles et al 2014 furthermore sensitivity analysis facilitates identifying the most influential input factors of a model and simplifies the calibration procedure on the other hand appropriate and rigorous sensitivity analysis allows to reduce practical and theoretical identifiability problems in mathematical models durán 2013 generally type i errors are those that identify non important parameters as significant whereas type ii errors recognise important parameters as insignificant total suspended solids tss is a fundamental component in eutrophic water since it presents a strong correlation with turbidity which is an essential factor in determining the ecological status of the water body scheffer et al 1993 in fact scheffer et al determined that switching from a eutrophic to a good ecological status or vice versa is highly dependent on water turbidity on this basis gargallo et al 2016 developed a biokinetic model for free water surface fws constructed wetlands including tss as an output this study included a local sensitivity analysis using the absolute relative sensitivity function of aquasim reichert 1994 1998 this method calculates changes in simulated components when 100 of the value of one input factor is changed while the values of all other input factors remain invariables in their calibrated values however local sensitivity analysis does not determine the effect on model output of simultaneously changing the value of different input factors global sensitivity analysis like morris method of elementary effects ees morris 1991 offers this possibility and allows to calculate the fractional contribution of each uncertain input factor to the variance of a model output haaker and verheijen 2004 in this study a global sensitivity analysis based on the morris method was implemented to a mechanistic model for total suspended solids simulation in free water surface constructed wetlands by means of the biokinetic model developed by gargallo et al 2016 this study contributes to focus calibration efforts on most influential input factors 2 methods 2 1 model description the biokinetic model for fws wetlands used in this study gargallo et al 2016 simulates tss concentration as an output xtss mg l 1 by means of 7 processes i e growth decay respiration and sedimentation of phytoplankton sedimentation of tss and sediment resuspension produced by wind and by avifauna table 1 shows stoichiometric matrix and processes rates furthermore resuspension produced by wind was not simulated as a dynamic process so addition of tss from sediment to water column by wind tssres was calculated using eq 1 1 tss res mg l 1 α w β δ w w o 1 vc k vegresus where w m s 1 is maximum daily wind speed α mg s l 1 d 1 m 1 and β are parameters to be calibrated δ w w0 is a step function that determines the periods when the wind induced waves cause resuspension δ 0 if w wo and δ 1 if w wo vc is vegetation cover kvegresus is coefficient for trapping by emergent vc in resuspension processes xp mg chla l 1 is phytoplankton biomass gmax d 1 is maximum growth rate for xp at 20 c θg is temperature coefficient for xp growth t ºc is temperature pint mg p mg chla 1 is phosphorus accumulated inside the phytoplankton cells pmin mg p mg chla 1 is minimum pint rate for xp pmax mg p mg chla 1 is maximum pint rate for xp gl is light limitation in phytoplankton growth kinetics kr d 1 is xp decay rate at 20 c θr is temperature coefficient for xp decay kresp d 1 is xp respiration rate at 20 c θresp is temperature coefficient for xp respiration do mg o2 l 1 is dissolved oxygen concentration kdo xp mg o2 l 1 is saturation coefficient for do in xp respiration vsxp m d 1 is sedimentation velocity of xp h m is water depth vstss m d 1 is sedimentation velocity of tss kvegsed is coefficient for trapping by emergent vc in sedimentation processes kavi mg m2 l 1 d 1 is coefficient of resuspension by avifauna ni is the number of birds in the group i rfi is resuspension factor of the group of birds i and a m2 is area of the fws wetland the biokinetic model of gargallo et al 2016 was properly calibrated and validated using experimental data from two full scale fws wetlands treating eutrophic water from albufera lake valència spain for three years that study revealed that tss were mainly removed by tss sedimentation while resuspension produced by both avifauna and wind introduced a significant amount of suspended material to the water column however processes related to phytoplankton i e growth death respiration and sedimentation presented little importance in tss performance within the studied fws wetlands 2 2 morris method morris method entails a properly screening strategy for identifying the most influential input factors in a highly parameterized model campolongo et al 2011 by calculating the elementary effects eei of input factors on the output of a model this method is based on a randomized one factor at a time oat design where input factors are varied individually by the same relative amount around the nominal point the output variable y y x of a model is defined by the scalar k dimensional function where k is the total number of parameters and input values this function is defined by a vector x x1 x2 xk with k components xi that can take p levels in the set 0 1 p 1 2 p 1 p 2 p 1 1 that identifies an exact point in the experimental k dimensional domain ω this assumes that a range of any k dimensional factors has been scaled to the set levels 0 1 p 1 2 p 1 p 2 p 1 1 and the region of experimentation ω is thus a k dimensional p level grid eei of the ith input factor at given value of x randomly selected in ω was defined by morris 1991 as shown in eq 2 in order to avoid committing a type i error sin and gernaey 2009 proposed to calculate scaled elementary effects seei j eq 3 for obtaining a non dimensional and normalized measurement of eei seei j are especially useful to study models where parameters value differs one or more orders of magnitude 2 ee i y y x 1 x 2 x i 1 x i δ x i 1 x k y x δ 3 see i j y y j x 1 x 2 x i 1 x i δ x i 1 x k y x δ σ i σ j where δ is the maximum magnitude of step length that can be considered so that x δ remains in ω whereas σi and σj are the standard deviations of the parameters xi and model outputs yj in this study the studied output factor is xtss morris method is based on building r random orientations in the region of experimentation ω formed by p levels the step length δ is a multiple of 1 p 1 and due to economical design construction reasons alam et al 2004 proposed the approximation of δ p 2 p 1 the r random trajectories that constitute the finite distribution fi j in the domain ω are obtained following the oat design where n r k 1 simulations are needed to be run fi j represents the finite distribution of seei j and contains r independent seei j calculated at randomly sampled points in the domain ω the mean value μi eq 4 and the standard deviation σi eq 5 of seei j in fi j provide an approximate global sensitivity measure μi indicates the global influence of the ith input factor on the output jth whereas σi points out the presence of higher order effects and identifies the non linearity or the interactions of the ith factor with other input factors sumner et al 2012 high μi values are associated to parameters with an important overall effect on the output high σi values indicate that the ith input factor presents non linear effects on the output or interactions with other parameters this analysis facilitates the classification of each parameter in i negligible effects ii lineal or additive effects and iii non linear or interaction effects but it does not allow discriminating between non linearity and interactions santiago et al 2012 this classification can be graphically represented as shown in fig 1 campolongo et al 2007 proposed to calculate the mean modified value μ in order to avoid type ii errors important factors classified as insignificant for non monotonic distributions of fi j eq 6 4 μ i n 1 r see n r 5 σ i 1 r n 1 r see n μ i 2 6 μ i n 1 r see n r 2 3 studied parameters among the 35 input factors included in the model developed by gargallo et al 2016 16 are involved in tss simulation carrying out a global sensitivity analysis for this number of input factors entails a time consuming and hard work so a smaller number of factors was selected based on results obtained in calibration and validation procedures in gargallo et al 2016 these procedures revealed that sedimentation and resuspension are the main processes in tss performance whereas processes related to phytoplankton present little importance therefore the 6 input parameters k 6 related to sedimentation of tss and resuspension processes produced by wind and by avifauna where included table 2 the study was carried out by comparing tss concentrations simulated in a fws wetland located at tancat de la pipa valència spain system details can be found in martín et al 2013 where it was named as fg1 the study was focused on the results obtained on 7th of october of 2009 whose main operating conditions are shown in table 3 simulations were executed using the software aquasim pc computer 2 4 ghz intel core i5 4210u processor 2 4 trajectories construction the oat design proposed by morris 1991 was implemented following the steps described in the study of solimeno et al 2016 this method is based on two main principles i one factor at a time is randomly moved in each simulation and ii all the parameters are moved in each trajectory trajectories were defined based on a k dimensional vector x whose values randomly varied between 0 and 1 δ so that adding the length step δ to one of the k components the vector remains in ω in table 4 the vector x for each trajectory is shown then the matrices b containing the parameter values for each trajectory were calculated by applying eq 7 where b is a m n matrix m k 1 7 b j m 1 x δ 2 2 b j m k d j m k p where j is a m k unit matrix b is a m k lower left triangle unit matrix d is a k dimensional diagonal matrix with values 1 and 1 distributed in the diagonal with the same probability whereas p is a k dimensional matrix with only one element equal to 1 in each row and column and the rest elements are zeros conclusions obtained by campolongo et al 1999 have been used to established p 4 levels and r 10 since these values have demonstrated to produce satisfactory results the maximum magnitude of the length step was set to δ p 2 p 1 2 3 therefore the p values in this study were 0 1 3 2 3 1 the total number of simulations n carried out was n r k 1 i e n 70 the points randomly sampled in the domain ω are determined by matrices d and p for k 6 matrix d presents 26 different possibilities each one with probability 1 64 and matrix p 6 720 possibilities each one with probability 1 720 rows of matrix b determine the factorization of k parameters and a matrix b was constructed for each of the ten trajectories thus the design matrix x for the entire experiment was obtained 8 x b 1 b 2 b 10 3 results and discussion the seei j of 6 input factors on the output xtss have been calculated based on the oat morris method and the mean and standard deviation values of the distribution function fi j have been calculated in order to determine their influence for the studied value of δ 2 3 and r 10 xtss concentration was mostly influenced by its sedimentation velocity vstss which was also shown to be the most influential input factor in the local sensitivity analysis carried out in gargallo et al 2016 vstss presented the highest values for μ σ and μ table 5 followed by the parameters α and β which are involved in the resuspension process caused by wind the fact that μ and μ present the same magnitude for each input factor indicates that the relationship between each factor and the model output xtss is monotonic and type ii errors are not occurring the graphical morris approach fig 2 facilitates identifying highly influential parameters on xtss concentrations μ and σ obtained for each seei as well as two lines representing μi 2 semi were plotted where semi is the standard error of the mean semi σi r only factors lying outside of the area defined by these two lines are considered in this study to be influential factors that lie inside the area obtained by the two lines are deemed as non influential or negligible morris 1991 sin et al 2009 the 6 input factors studied are divided into two different groups on the one hand settling velocity of xtss α and β are the outermost factors from the area defined by the two lines which indicates that these are the most influential input factors in xtss modelling on the other hand kvegsed kvegresus and kavi lies outside but very close to the area defined by these two lines so they are less influential than the former group analysing the type of influence of each input factor the same two groups have been obtained fig 3 vstss α and β present non linear or interaction effects whereas kavi kvegresus and kvegsed have negligible effects no input factor with linear and additive effects was identified the low impact of input factors related to the trapping effect of vegetation i e kvegresus and kvegsed was attributed to the low vegetation cover existing in the fws wetland when this study was carried out since only 10 of the area of the wetland was covered by vegetation nevertheless methodology used in this study was not able to identify whether influences of vstss α and β were due to non linear or interaction effects among the studied input factors kavi was identified as the least influential input factor on xtss modelling with a value of zero for both sensitivity measures μ and σ meaning that kavi presents negligible effect on xtss therefore the calibrated value obtained in gargallo et al 2016 can be used for modelling tss in other fws wetlands results obtained in this global sensitivity analysis for xtss modelling enhance those achieved in the local sensitivity analysis carried out in gargallo et al 2016 the global sensitivity analysis pointed out that vstss was the most influential input factor and reveals that in contrast to results obtained in the local sensitivity analysis kvegsed and kavi have negligible influences on xtss modelling 4 conclusions a global sensitivity analysis based on the scaled elementary effects and the morris method was carried out to identify which are the most influential parameters in total suspended solids simulation in free water surface constructed wetlands the influence of the 6 input factors related to the sedimentation of total suspended solids and the resuspension processes caused by wind and avifauna vstss α β kavi kvegresus and kvegsed was analysed the results pointed out that the most influential input factor was the sedimentation velocity of total suspended solids vstss followed by parameters α and β for resuspension caused by wind these input factors had non linear or interaction effects kavi kvegresus and kvegsed had negligible effects therefore maximum calibration efforts for modelling tss must be focused on vstss while input factors with negligible effects can be obtained from bibliographical sources 
26421,a large number of input factors are involved in mechanistic models for constructed wetlands simulation proper calibration procedure requires identifying which are the most influential ones in the simulation of main interest outputs total suspended solids concentration is an essential component in the simulation of eutrophic water treatment in this paper a global sensitivity analysis using the morris method and scaled elementary effects was performed in order to identify which are the most influential input factors in total suspended solids simulation in free water surface constructed wetlands treating eutrophic water results indicated that the most influential one was the sedimentation velocity of total suspended solids followed by parameters related to the resuspension process caused by wind which presented non linear or interaction effects keywords sensitivity analysis mathematical modelling suspended solids morris method eutrophic water 1 introduction constructed wetlands cws are complex systems for treating wastewater where several physical chemical and biological processes take place understanding these processes is needed to optimize the performance of cws but it is hindered by the large number of processes and their interdependency in this context mathematical modelling has been considered a promising tool to increase the understanding of the processes involved in cws samsó et al 2014 several mechanistic models have proven to be helpful for increasing the knowledge about the performance of cws and optimizing both their design and management bio pore samsó and garcia 2013 and hydrus wetland module langergraber and simunek 2012 are mechanistic wetlands models formed by a number of sub models that represents the different elements in a cw and their interactions among others these are the biokinetic model water flow model transport model or the wetland plants model biokinetic models describe transformation and degradation processes and are frequently highly parameterized models the model constructed wetland model number 1 cwm1 langergraber et al 2009 contains 59 parameters and the biokinetic model constructed wetlands 2d cw2d langergraber 2001 is formed by 46 parameters therefore identifying which are the most influential input factors in the simulation of each output is required for achieving properly calibration procedures however mathematical simulations are subject to uncertainty since models are an abstract approximation of reality based on a number of assumptions and sensitivity analysis are useful tools to quantify how any modification in model s input factors affect the model s output robles et al 2014 furthermore sensitivity analysis facilitates identifying the most influential input factors of a model and simplifies the calibration procedure on the other hand appropriate and rigorous sensitivity analysis allows to reduce practical and theoretical identifiability problems in mathematical models durán 2013 generally type i errors are those that identify non important parameters as significant whereas type ii errors recognise important parameters as insignificant total suspended solids tss is a fundamental component in eutrophic water since it presents a strong correlation with turbidity which is an essential factor in determining the ecological status of the water body scheffer et al 1993 in fact scheffer et al determined that switching from a eutrophic to a good ecological status or vice versa is highly dependent on water turbidity on this basis gargallo et al 2016 developed a biokinetic model for free water surface fws constructed wetlands including tss as an output this study included a local sensitivity analysis using the absolute relative sensitivity function of aquasim reichert 1994 1998 this method calculates changes in simulated components when 100 of the value of one input factor is changed while the values of all other input factors remain invariables in their calibrated values however local sensitivity analysis does not determine the effect on model output of simultaneously changing the value of different input factors global sensitivity analysis like morris method of elementary effects ees morris 1991 offers this possibility and allows to calculate the fractional contribution of each uncertain input factor to the variance of a model output haaker and verheijen 2004 in this study a global sensitivity analysis based on the morris method was implemented to a mechanistic model for total suspended solids simulation in free water surface constructed wetlands by means of the biokinetic model developed by gargallo et al 2016 this study contributes to focus calibration efforts on most influential input factors 2 methods 2 1 model description the biokinetic model for fws wetlands used in this study gargallo et al 2016 simulates tss concentration as an output xtss mg l 1 by means of 7 processes i e growth decay respiration and sedimentation of phytoplankton sedimentation of tss and sediment resuspension produced by wind and by avifauna table 1 shows stoichiometric matrix and processes rates furthermore resuspension produced by wind was not simulated as a dynamic process so addition of tss from sediment to water column by wind tssres was calculated using eq 1 1 tss res mg l 1 α w β δ w w o 1 vc k vegresus where w m s 1 is maximum daily wind speed α mg s l 1 d 1 m 1 and β are parameters to be calibrated δ w w0 is a step function that determines the periods when the wind induced waves cause resuspension δ 0 if w wo and δ 1 if w wo vc is vegetation cover kvegresus is coefficient for trapping by emergent vc in resuspension processes xp mg chla l 1 is phytoplankton biomass gmax d 1 is maximum growth rate for xp at 20 c θg is temperature coefficient for xp growth t ºc is temperature pint mg p mg chla 1 is phosphorus accumulated inside the phytoplankton cells pmin mg p mg chla 1 is minimum pint rate for xp pmax mg p mg chla 1 is maximum pint rate for xp gl is light limitation in phytoplankton growth kinetics kr d 1 is xp decay rate at 20 c θr is temperature coefficient for xp decay kresp d 1 is xp respiration rate at 20 c θresp is temperature coefficient for xp respiration do mg o2 l 1 is dissolved oxygen concentration kdo xp mg o2 l 1 is saturation coefficient for do in xp respiration vsxp m d 1 is sedimentation velocity of xp h m is water depth vstss m d 1 is sedimentation velocity of tss kvegsed is coefficient for trapping by emergent vc in sedimentation processes kavi mg m2 l 1 d 1 is coefficient of resuspension by avifauna ni is the number of birds in the group i rfi is resuspension factor of the group of birds i and a m2 is area of the fws wetland the biokinetic model of gargallo et al 2016 was properly calibrated and validated using experimental data from two full scale fws wetlands treating eutrophic water from albufera lake valència spain for three years that study revealed that tss were mainly removed by tss sedimentation while resuspension produced by both avifauna and wind introduced a significant amount of suspended material to the water column however processes related to phytoplankton i e growth death respiration and sedimentation presented little importance in tss performance within the studied fws wetlands 2 2 morris method morris method entails a properly screening strategy for identifying the most influential input factors in a highly parameterized model campolongo et al 2011 by calculating the elementary effects eei of input factors on the output of a model this method is based on a randomized one factor at a time oat design where input factors are varied individually by the same relative amount around the nominal point the output variable y y x of a model is defined by the scalar k dimensional function where k is the total number of parameters and input values this function is defined by a vector x x1 x2 xk with k components xi that can take p levels in the set 0 1 p 1 2 p 1 p 2 p 1 1 that identifies an exact point in the experimental k dimensional domain ω this assumes that a range of any k dimensional factors has been scaled to the set levels 0 1 p 1 2 p 1 p 2 p 1 1 and the region of experimentation ω is thus a k dimensional p level grid eei of the ith input factor at given value of x randomly selected in ω was defined by morris 1991 as shown in eq 2 in order to avoid committing a type i error sin and gernaey 2009 proposed to calculate scaled elementary effects seei j eq 3 for obtaining a non dimensional and normalized measurement of eei seei j are especially useful to study models where parameters value differs one or more orders of magnitude 2 ee i y y x 1 x 2 x i 1 x i δ x i 1 x k y x δ 3 see i j y y j x 1 x 2 x i 1 x i δ x i 1 x k y x δ σ i σ j where δ is the maximum magnitude of step length that can be considered so that x δ remains in ω whereas σi and σj are the standard deviations of the parameters xi and model outputs yj in this study the studied output factor is xtss morris method is based on building r random orientations in the region of experimentation ω formed by p levels the step length δ is a multiple of 1 p 1 and due to economical design construction reasons alam et al 2004 proposed the approximation of δ p 2 p 1 the r random trajectories that constitute the finite distribution fi j in the domain ω are obtained following the oat design where n r k 1 simulations are needed to be run fi j represents the finite distribution of seei j and contains r independent seei j calculated at randomly sampled points in the domain ω the mean value μi eq 4 and the standard deviation σi eq 5 of seei j in fi j provide an approximate global sensitivity measure μi indicates the global influence of the ith input factor on the output jth whereas σi points out the presence of higher order effects and identifies the non linearity or the interactions of the ith factor with other input factors sumner et al 2012 high μi values are associated to parameters with an important overall effect on the output high σi values indicate that the ith input factor presents non linear effects on the output or interactions with other parameters this analysis facilitates the classification of each parameter in i negligible effects ii lineal or additive effects and iii non linear or interaction effects but it does not allow discriminating between non linearity and interactions santiago et al 2012 this classification can be graphically represented as shown in fig 1 campolongo et al 2007 proposed to calculate the mean modified value μ in order to avoid type ii errors important factors classified as insignificant for non monotonic distributions of fi j eq 6 4 μ i n 1 r see n r 5 σ i 1 r n 1 r see n μ i 2 6 μ i n 1 r see n r 2 3 studied parameters among the 35 input factors included in the model developed by gargallo et al 2016 16 are involved in tss simulation carrying out a global sensitivity analysis for this number of input factors entails a time consuming and hard work so a smaller number of factors was selected based on results obtained in calibration and validation procedures in gargallo et al 2016 these procedures revealed that sedimentation and resuspension are the main processes in tss performance whereas processes related to phytoplankton present little importance therefore the 6 input parameters k 6 related to sedimentation of tss and resuspension processes produced by wind and by avifauna where included table 2 the study was carried out by comparing tss concentrations simulated in a fws wetland located at tancat de la pipa valència spain system details can be found in martín et al 2013 where it was named as fg1 the study was focused on the results obtained on 7th of october of 2009 whose main operating conditions are shown in table 3 simulations were executed using the software aquasim pc computer 2 4 ghz intel core i5 4210u processor 2 4 trajectories construction the oat design proposed by morris 1991 was implemented following the steps described in the study of solimeno et al 2016 this method is based on two main principles i one factor at a time is randomly moved in each simulation and ii all the parameters are moved in each trajectory trajectories were defined based on a k dimensional vector x whose values randomly varied between 0 and 1 δ so that adding the length step δ to one of the k components the vector remains in ω in table 4 the vector x for each trajectory is shown then the matrices b containing the parameter values for each trajectory were calculated by applying eq 7 where b is a m n matrix m k 1 7 b j m 1 x δ 2 2 b j m k d j m k p where j is a m k unit matrix b is a m k lower left triangle unit matrix d is a k dimensional diagonal matrix with values 1 and 1 distributed in the diagonal with the same probability whereas p is a k dimensional matrix with only one element equal to 1 in each row and column and the rest elements are zeros conclusions obtained by campolongo et al 1999 have been used to established p 4 levels and r 10 since these values have demonstrated to produce satisfactory results the maximum magnitude of the length step was set to δ p 2 p 1 2 3 therefore the p values in this study were 0 1 3 2 3 1 the total number of simulations n carried out was n r k 1 i e n 70 the points randomly sampled in the domain ω are determined by matrices d and p for k 6 matrix d presents 26 different possibilities each one with probability 1 64 and matrix p 6 720 possibilities each one with probability 1 720 rows of matrix b determine the factorization of k parameters and a matrix b was constructed for each of the ten trajectories thus the design matrix x for the entire experiment was obtained 8 x b 1 b 2 b 10 3 results and discussion the seei j of 6 input factors on the output xtss have been calculated based on the oat morris method and the mean and standard deviation values of the distribution function fi j have been calculated in order to determine their influence for the studied value of δ 2 3 and r 10 xtss concentration was mostly influenced by its sedimentation velocity vstss which was also shown to be the most influential input factor in the local sensitivity analysis carried out in gargallo et al 2016 vstss presented the highest values for μ σ and μ table 5 followed by the parameters α and β which are involved in the resuspension process caused by wind the fact that μ and μ present the same magnitude for each input factor indicates that the relationship between each factor and the model output xtss is monotonic and type ii errors are not occurring the graphical morris approach fig 2 facilitates identifying highly influential parameters on xtss concentrations μ and σ obtained for each seei as well as two lines representing μi 2 semi were plotted where semi is the standard error of the mean semi σi r only factors lying outside of the area defined by these two lines are considered in this study to be influential factors that lie inside the area obtained by the two lines are deemed as non influential or negligible morris 1991 sin et al 2009 the 6 input factors studied are divided into two different groups on the one hand settling velocity of xtss α and β are the outermost factors from the area defined by the two lines which indicates that these are the most influential input factors in xtss modelling on the other hand kvegsed kvegresus and kavi lies outside but very close to the area defined by these two lines so they are less influential than the former group analysing the type of influence of each input factor the same two groups have been obtained fig 3 vstss α and β present non linear or interaction effects whereas kavi kvegresus and kvegsed have negligible effects no input factor with linear and additive effects was identified the low impact of input factors related to the trapping effect of vegetation i e kvegresus and kvegsed was attributed to the low vegetation cover existing in the fws wetland when this study was carried out since only 10 of the area of the wetland was covered by vegetation nevertheless methodology used in this study was not able to identify whether influences of vstss α and β were due to non linear or interaction effects among the studied input factors kavi was identified as the least influential input factor on xtss modelling with a value of zero for both sensitivity measures μ and σ meaning that kavi presents negligible effect on xtss therefore the calibrated value obtained in gargallo et al 2016 can be used for modelling tss in other fws wetlands results obtained in this global sensitivity analysis for xtss modelling enhance those achieved in the local sensitivity analysis carried out in gargallo et al 2016 the global sensitivity analysis pointed out that vstss was the most influential input factor and reveals that in contrast to results obtained in the local sensitivity analysis kvegsed and kavi have negligible influences on xtss modelling 4 conclusions a global sensitivity analysis based on the scaled elementary effects and the morris method was carried out to identify which are the most influential parameters in total suspended solids simulation in free water surface constructed wetlands the influence of the 6 input factors related to the sedimentation of total suspended solids and the resuspension processes caused by wind and avifauna vstss α β kavi kvegresus and kvegsed was analysed the results pointed out that the most influential input factor was the sedimentation velocity of total suspended solids vstss followed by parameters α and β for resuspension caused by wind these input factors had non linear or interaction effects kavi kvegresus and kvegsed had negligible effects therefore maximum calibration efforts for modelling tss must be focused on vstss while input factors with negligible effects can be obtained from bibliographical sources 
26422,sellafield marine discharges of 14c are the largest contributor to the global collective dose from the nuclear fuel industry as such it is important to understand the fate of these discharges beyond the limitations and scope of empirical analytical investigations for this highly mobile radioactive contaminant ecopath with ecosim ewe is widely used to model anthropogenic impacts on ecosystems such as fishing although very few ewe studies have modelled the fate of bioavailable contaminants this work presents for the first time a spatial temporal 14c model utilising recent developments in ewe software to predict the ecological fate of anthropogenic 14c in the marine environment the model predicted observed trends in 14c activities between different species and through time it also provided evidence for the integration of sellafield 14c in species at higher trophic levels through time keywords radiocarbon 14c radioactive discharges sellafield ecosystem model ecopath with ecosim ecotracer software availability name of software ecopath with ecosim developer ecopath research and development consortium contact address ecopath international initiative research association barcelona spain contact email ewedevteam gmail com year first available 1991 hardware required pc software required windows xp service pack 3 or newer microsoft net framework 4 full profile availability public open source gplv2 freely available from www ecopath org version used in this study available from developer program language visual basic net c program size 16 mb basic installation 1 introduction understanding the ecological fate of anthropogenic radionuclides is necessary to determine any potentially hazardous consequences to the environment and to human populations although empirical studies are essential they are often time consuming costly and impractical to conduct particularly if basic information is required quickly e g after accidental releases of radioactive material or if radioactive contamination is spread over a large area computer modelling provides an additional tool which can be both time and cost effective the ecosystem modelling software suite ecopath with ecosim ewe christensen and walters 2004 has undergone recent developments that aid its ability to model the ecological fate of environmental contaminants spatially ewe has previously been used to model non radioactive contaminants such as methyl mercury in the faroe islands marine ecosystem booth and zeller 2005 chemical warfare agents in the baltic sea niiranen et al 2008 sanderson et al 2010 polycyclic aromatic hydrocarbons pahs in the arctic larsen et al 2016 and polychlorinated biphenyls pcb and mercury in the great lakes mcgill et al 2017 ewe has also been used to model radioactive contaminants including a hypothetical release of radiocarbon 14c in the baltic sea sandberg et al 2007 and radiocaesium 137cs in a simple coastal marine ecosystem model for fukushima walters and christensen 2017 many of these studies considered temporal change in contaminant activity but not spatial variation the ewe approach is appealing for modelling bioavailable contaminants due primarily to its ease of use there are over 400 unique ewe models published making it the most extensively applied tool for modelling marine and aquatic ecosystems colleter et al 2015 the extensive coverage of published ewe models also means that models may already exist for a specific ecosystem in which an environmental contaminant is present or subject to a recent contamination event and can be adapted for this purpose there are four components to ewe 1 ecopath which describes a static mass balanced ecosystem polovina 1984 2 ecosim a time dynamic simulation module allowing for temporal changes and impacts to an ecosystem to be investigated walters et al 1997 3 ecospace where ecosystem changes can be explored both spatially and temporally walters et al 1999 and 4 ecotracer which traces contaminants through the ecosystem walters and christensen 2017 while ecosim ecospace solve biomass dynamic equations ecotracer simultaneously models contaminant flow and or accumulation to the biological groups and environment as described in the base ecopath model ecotracer and the equations it utilises are fully described in walters and christensen 2017 the development of the spatial temporal ewe framework steenbeek et al 2013 allows physical changes to occur in the ecospace environment through time as environmental contamination is typically non ubiquitous and non static this is an appropriate development to consider for contaminant models where contaminant concentrations in the environment are both temporally and spatially variable in recent decades several studies have accumulated a large body of data regarding the fate of 14c discharged by the sellafield nuclear fuel reprocessing facility in cumbria uk to the marine environment begg et al 1992 cook et al 1995 1998 2004 gulliver et al 2001 2004 mackenzie et al 2004 muir et al 2015 2017 tierney et al 2016 2017a 2017b the most recent work has focussed on detailing the ecosystem uptake of 14c in the irish sea muir et al 2017 and the west of scotland tierney et al 2017a marine environments and has established a time series of 14c activities in marine mammals along the uk coastline tierney et al 2017b sellafield discharges 14c as low activity waste effluent via pipelines to the irish sea in the dissolved inorganic phase which is rapidly incorporated into the marine dissolved inorganic carbon dic pool begg et al 1992 cook et al 1995 in this manner sellafield 14c spreads throughout the irish sea and is largely dispersed northward through the north channel to the west of scotland marine environment by prevailing currents gulliver et al 2001 whilst contemporary environmental releases of 14c from sellafield do not pose any direct radiological risk to critical consumer groups near sellafield muir et al 2017 releases of 14c are still highly significant 14c has a long half life 5730 years is environmentally mobile highly bioavailable and marine discharges are the largest contributor to the uk european and global collective dose from sellafield nuclear decommissioning authority 2016 the aim of this study was to construct an ewe contaminant tracing model for sellafield 14c in the uk marine environment the sellafield model and to test it primarily using recent data compiled in muir et al 2017 and tierney et al 2017a 2017b this is the first time an ewe model has been tested on its ability to accurately predict the spatial ecosystem uptake and fate of radionuclides discharged routinely to the marine environment the unique biogeochemical properties of 14c in being identical in its behaviour to stable carbon on an ecosystem level coupled with its long half life make it an ideal contaminant with which to evaluate the performance and applicability of the ewe model and ecotracer utility for radioactive contamination scenarios 2 methods 2 1 ecopath input in ewe functional groups are either specific species or a group of species deemed to have sufficiently similar functionality for the purposes of the model heymans et al 2016 functional groups require a number of input parameters to satisfy the ecopath mass balance equation where consumption c production p respiration r unassimilated food u the productivity of each functional group is defined in ecopath by the equation p i y i b i m 2 i e i b a i p i 1 e e i where for group i p i is the total production rate y i is the total fishery catch rate b i is the biomass m 2 i is the total predation rate e i is the net migration rate emigration immigration b a i is the biomass accumulation rate and ee is the ecotrophic efficiency of the group christensen and walters 2004 ecotrophic efficiency is the proportion of a group s production that is explained in the model and this cannot exceed 1 typical inputs to ecopath include values for biomass b annual production biomass p b and either annual consumption biomass q b or production consumption p q as sellafield discharges 14c into the irish sea a mass balanced ewe model developed by lees and mackinson 2007 that describes the irish sea ecosystem in the early 1970s was used as the foundation for the sellafield model however aspects of this model were either beyond the complexity required for the sellafield model or not descriptive enough and a number of changes were made the lees and mackinson irish sea model contains a total of 53 functional groups which was reduced to 43 in the sellafield model as discussed below three model groups cod gadus morhua haddock melanogrammus aeglefinus and plaice pleuronectes platessa were separated into two age classes adult and juvenile these age class separations were not necessary for our purposes and these species were instead combined into single species groups in the sellafield model muir et al 2017 and tierney et al 2017a describe 14c activities in three fish species dab limanda limanda ling molva molva and herring clupea harengus which are not individually specified in the irish sea model using species information described in lees and mackinson 2007 dab was separated from the medium flatfish functional group ling extracted from other large demersals and herring from other small pelagic planktivorous fish the functional groups small medium and large flatfish were combined into the singular other flatfish group likewise a single functional group called other demersals was created by combining bass seatrout gurnards mullet other large demersals other large gadoids other small demersals and other small gadoids the irish sea model contained four zooplankton groups herbivorous omnivorous carnivorous and gelatinous which were also combined to form a single zooplankton group where groups were combined biomass values were summed and other input parameters from lees and mackinson 2007 were calculated as a proportion of the biomasses of the previously existing groups where new groups were extracted biomasses and other input parameters were taken as described in lees and mackinson 2007 tierney et al 2017b describe 14c activities in 3 marine mammal species harbour porpoise phocoena phocoena harbour common seal phoca vitulina and grey seal halichoerus grypus none of these species are specified in the irish sea model which contains the mammal functional groups baleen whale toothed whale and seals the description of marine mammals in the irish sea model was the focus of a model re structuring by hernandez milian 2014 following this the existing irish sea mammal groups were removed and five new functional groups were added for specific species bottlenose dolphin tursiops truncates harbour porpoise minke whale balaenoptera acutorostrata common seal and grey seal input parameters including biomass p b q b and diet for these functional groups were taken from hernandez milian 2014 at the base of the food web primary producers particularly phytoplankton were key functional groups in the sellafield model as 14c enters the food web through uptake by primary producers during photosynthesis phytoplankton biomass and p b were re calculated using the formula from gowen and bloomfield 1996 and primary productivity estimates from gowen et al 2000 of 97 g m2 for the coastal irish sea this resulted in an increase in biomass from 9 7 to 13 8 t km2 and a reduction in the p b value from 152 5 to 70 1 year 1 ecopath also requires diet estimates of each functional group and the diet matrix was largely carried over from the irish sea model where groups were combined new diet estimates were calculated from previous diets as a function of each group s biomass where new groups were created the diet was assumed to be the same for example herring and other small pelagic planktivorous fish have the same diet diets for the new mammal groups were taken from hernandez milian 2014 the diets for two functional groups small sharks and monkfish lophius piscatorius were edited for better definition small shark diet was updated according to ellis et al 1996 and monkfish diet was updated according to crozier 1985 the sellafield model input parameters and diet matrix are included in the appendix tables a 1 and a 2 the irish sea model contained nine fisheries which were retained in the sellafield model landings and discards of these fisheries were corrected for the new and combined functional groups but no other changes were made 2 2 prebal and balancing after the described changes to the model were made model assumptions were tested following a set of pre balance diagnostics prebal described by link 2010 prebal checks that the ecosystem model makes ecological and thermodynamic sense by checking the slopes of biomass ratios and other data input against trophic levels there were no significant issues with the sellafield model however the annual p b ratios for dab 2 4 and other flatfish 2 2 did appear to be high and conversely the p b ratios for herring 0 7 and other small pelagic planktivorous fish 0 7 appeared to be relatively low no changes were made to the input parameters for these groups following prebal however the groups were highlighted again during model balancing the ecopath model must be mass balanced after entering the input parameters heymans et al 2016 the sellafield model was initially imbalanced and several parameters were subsequently corrected as explained below increases in biomass for whiting merlangius merlangus 8 5 and lobster and large crabs 12 2 are within the biomass estimates reported by lees and mackinson 2007 the biomass increases to the epifaunal mesobenthos 0 27 and prawn and shrimp 1 8 are negligible relative to the changes in biomass made by lees and mackinson 2007 when balancing the irish sea model large differences were found in the irish sea model between the initial biomass values used and the balanced biomasses for monkfish flatfish small medium and large nephrops and zooplankton groups biomasses were estimated in ecopath for monkfish dab other flatfish nephrops and zooplankton by setting ecotrophic efficiency ee to 0 95 for these groups this assumes that the model uses all but 5 of the production of that group and ecopath can estimate a biomass based on this assumption in addition the p b ratios for dab and other flatfish were estimated in ecopath by setting the production consumption p q ratio for these groups to 0 2 these changes were made based on best practices described by heymans et al 2016 as herring p b had been identified as being relatively high it was re calculated using fishing mortality f from lees and mackinson 2007 and natural mortality m from fishbase froese and pauly 2016 as p b is equal to total mortality z mackerel q b was corrected using default values from fishbase including a mean temperature of 10 c corrections were also made to the diet matrix for model balancing and the most significant of these was the reduction in consumption of discards which was relatively high in the irish sea model e g the proportion of discards in the diet of nephrops was reduced from 0 5 to 0 03 this was balanced by increasing the proportion of particulate organic matter in diets a further important change was a substantial increase in the proportion of polychaetes in dab diet as described by gibson and ezzi 1987 all changes made to both the input parameters and the diet matrix for model balancing are shown in the appendix tables a1 and a2 2 3 ecospace to accurately predict uptake of sellafield derived 14c the sellafield model had to be spatially resolved in ecospace the biomass b of a functional group for a specific grid cell at time t can be expressed as d b d t i g c z e b where i is the total immigration rate from surrounding cells g is the net growth efficiency c is food consumption rate z is total instantaneous mortality rate and e is total instantaneous emigration rate walters et al 1999 due to the net northward dispersion of 14c and the available data for the west of scotland tierney et al 2017a 2017b the ecospace base map of the sellafield model produced in arcgis encompasses part of the west of scotland marine environment as well as the irish sea fig 1 base map grid resolution was 5 km with boundaries at approximately 56 45 n northern boundary 02 45 w eastern boundary 53 15 n southern boundary and 7 15 w western boundary the base map covers key sites in both the irish sea and west of scotland used in studies concerning 14c in the environment and biota muir et al 2017 tierney et al 2017a 2017b the recently introduced contaminant map layer in ecospace allows the user to input contaminant concentrations across the base map this can now be used to select a specific point source for a contaminant which is limited to the base map resolution a contaminant can be dispersed in ecospace using the base dispersal rate for the first detrital group listed in the model the base dispersal rate is used to set the rate with which organisms in the ecosystem will disperse due to random movements where the default is 300 km year 1 but this can also be applied to a contaminant in addition a more realistic dispersion pattern can be created using the advection map layer by inputting x east west and y north south velocity data at the base map resolution the spatial temporal framework a gis based data exchange framework built on dotspatial steenbeek et al 2013 ames et al 2012 allowed a time series of variable ocean velocity data to be input to the running model month averaged x and y velocities at 7 km grid resolution from a hind cast model of the north east atlantic for the period january 1985 to june 2014 most recent data available at the time of study were sourced from the european commission copernicus marine environment monitoring service http marine copernicus eu data for the model area were extracted and velocities were depth averaged the data were extrapolated over the base map 5 km grid resolution and a time series of map files ascii for x and y velocities on monthly time steps were produced these data files were read into ecospace using the spatial temporal framework steenbeek et al 2013 to create advection fields and the model was run on monthly time steps phytoplankton and zooplankton dispersion as well as contaminant dispersion were linked to model advection 2 4 ecotracer the iteration of ewe used for the sellafield model contained changes to the ecotracer component that will be included in the next release of ewe ewe 6 6 to be released summer 2018 when running ecotracer in ecosim non spatial contaminant modelling it is possible to link contaminant inputs to a data time series to do this for a point source in ecospace a new ecotracer function was added which allows variable contaminant input at specified grid cells limited to the base map resolution for the sellafield model this means that 14c input was in a 25 km2 area where the sellafield pipelines end this input was made as total monthly 14c discharge activity from sellafield in becquerels bq other input parameters in ecotracer include initial contaminant concentration contaminant concentration in immigrating biomass direct uptake physical decay rate proportion of contaminant excreted and metabolic decay rate table 1 initial contaminant concentrations must be set for both the environment and the functional groups sellafield 14c discharges are in addition to an existing background 14c activity from natural production and atmospheric atomic weapons testing the initial contaminant concentration in the sellafield model was set at zero and as the only input was from sellafield 14c discharges any increase in functional group contaminant concentration shows net 14c enrichment in excess of background direct uptake is the rate at which a functional group takes up the contaminant from the environment 14c is discharged to the marine environment as dic and primary producing organisms incorporate dissolved inorganic 14c through fixation of carbon during photosynthesis into soft tissue for phytoplankton direct uptake was calculated as the rate at which phytoplankton photosynthesise as it is assumed that 14c uptake is identical to stable carbon uptake following walters and christensen 2017 this can be expressed as d i r e c t u p t a k e u i b i c 0 where u i is the mass of carbon intake by primary producer i b i is the biomass of primary producer i and c 0 is the dic concentration phytoplankton 14c uptake was calculated using the primary productivity estimate of 97 g m 2 for the coastal irish sea gowen et al 2000 the calculated biomass 13 8 t km 2 and an estimated dic concentration of 30 mg l 1 taken from the upper limit of typical dic concentrations in seawater of 24 30 mg c l 1 chester 1990 primary productivity rates for the other primary producers seaweed and microflora were estimated by back calculating from the model biomass using formula from gowen and bloomfield 1996 productivity rates of 450 g m 2 seaweed and 230 g m 2 microflora were then used to calculate direct uptake for these groups contaminant uptake for consumer groups is a function of the contaminant concentration in their diet their consumption rate and their mortality walters and christensen 2017 there are two excretion parameters for each functional group the proportion of contaminant excreted is the proportion not assimilated into the biomass and so passes straight into the detritus group the default annual unassimilated consumption for consumer groups in the ecopath parameters is 0 2 and this was copied for the proportion of contaminant excreted in the sellafield model as 14c acts as a tracer of stable carbon and therefore energy transfer in an ecosystem the metabolic decay rate is the rate at which assimilated contaminant is released back into the environment see excretion in walters and christensen 2017 the carbon weight 14c activity bq per mass c of marine primary producers is in equilibrium with the dic 14c activity therefore the metabolic decay rate for primary producers can be calculated where the equilibrium ratio of 14c in a primary producer is equal to the environmental concentration for example when the dic 14c activity is at background approximately 249 bq kg 1 c the 14c activity in a primary producer is expected to be the same under these conditions the metabolic decay rate can be expressed as m e t a b o l i c d e c a y r a t e d i c 0 c i b i where d i is the direct uptake rate for the primary producer i c 0 is the 14c concentration in the environment c i is the 14c concentration in primary producer i and b i is the biomass of primary producer i the metabolic decay rate was calculated this way for all three primary producers in the sellafield model for consumer groups the metabolic decay rate was assumed to be equal to the respiration rate biomass which were calculated in the ecopath component during model balancing a contaminant physical decay rate parameter was added to the ecotracer module in the version used here this can be set for both the environment and each functional group for radionuclides this is the physical radioactive decay rate meaning that in ewe biological decay and physical radioactive decay are two separate parameters 14c has a long half life of 5730 years and the impact of radioactive decay on a model running over several decades is negligible and so physical decay rate was set at zero as the ecopath food web model contained no immigration the contaminant concentration in the immigrating biomass could also be set to zero for every functional group muir et al 2017 and tierney et al 2017a describe 14c activities at numerous sites including 4 main stations fig 1 located in the irish sea east basin eb irish sea west basin wb north channel nc and firth of lorn fol model base map grid cells at these site locations or in the case of fol as close as possible were labelled as different model regions in ecospace and data specific to these regions were extracted from ewe model data analyses were conducted and map figures were produced using r r development core team 2016 and the r package ggmap kahle and wickham 2013 used to overlay model predicted data over google satellite maps model predictions are only given where 14c enrichment is at least 1 bq kg 1 c and all observed 14c activities are given as net activities i e background subtracted 3 results and discussion 3 1 14c dispersion although advection was the dominant control on the general direction and extent of model 14c dispersion contaminant base dispersal rate had an observed impact fig 2 as 14c uptake by primary producers is limited to the 14c activity of the environment or dic 14c activity for a given cell the physical dispersion of sellafield 14c is a key mechanism studies examining the dispersion of other highly soluble radionuclides discharged by sellafield e g 134cs 137cs and 99tc have estimated transit times from sellafield to the north channel of between 3 months and 1 8 years jefferies et al 1973 kershaw and baxter 1995 kershaw et al 2004 similarly transit times of between 3 months and 1 year were predicted by the model developed by dabrowski and hartnett 2008 using a base dispersal rate of at least 100 km per month resulted in model 14c reaching the north channel within 1 year fig 2 as the distance between sellafield and the north channel is approximately 110 km it is apparent that model dispersion of 14c is limited by both advection and uptake by primary producers increasing the base dispersal rate does increase maximum dispersal extent though this increase does not appear to be substantial it does reduce the maximum dic 14c activities predicted in pockets where 14c appears to accumulate e g the solway firth this accumulation particularly at lower base dispersal rates causes activities to increase to levels which have not been observed in previous studies for example the model predicted 14c activities above 30kbq kg 1 c between 2001 and 2006 when using a base dispersal rate of 100 km per month however an increase in base dispersal rate to 200 km per month limited maximum model dic activities to less than 20 kbq kg 1 c there are no reported dic 14c activities for the period of peak predicted activity 2001 2006 and therefore no available data for dic activities in areas such as the solway firth where a significant accumulation of 14c was predicted for this period the highest reported net dic 14c activities are approximately 8550 bq kg 1 c in 1995 cook et al 1998 and 4500 bq kg 1 c in 1997 cook et al 2004 at sites relatively close to sellafield using a time series of dic 14c activities for a site in the vicinity of sellafield for the period 1989 to 1999 cook et al 2004 it is shown that the range of activities predicted by the model when using a base dispersal rate of 200 km per month was similar to the observed range through time fig 3 although the specific measured and predicted activities do not generally align it is important to recognise that model 14c activities are predicted per km2 for a 25 km2 cell per month whereas measured dic samples were taken from a specific day and location on the coastline model dispersion of dic 14c could be improved with further measurements at sites such as the solway firth to address uncertainty in dispersion to illustrate model dispersion of dissolved inorganic 14c in the environment a video component video 1 is available and accompanies the electronic version of the manuscript supplementary video related to this article can be found at https doi org 10 1016 j envsoft 2018 01 013 the following is the supplementary data related to this article video 1 video 1 in addition to the solway firth accumulation of 14c occurs in the south east irish sea and around the isle of man circulation models have described a significant seasonal southward flow in the irish sea dabrowski and hartnett 2008 dabrowski et al 2010 creating a backwater in the south east irish sea this could result in the area around liverpool bay being a significant sink for radionuclides released from sellafield dabrowski and hartnett 2008 an increase in the radionuclide inventories of saltmarsh sediments in areas including the solway firth has previously been suggested mackenzie et al 2004 however an accumulation of sellafield radionuclides in the water column of the solway firth and also around the isle of man has not been previously detected and the general northward movement of water continuously flushed the isle of man coastline in circulation models dabrowski and hartnett 2008 although accumulation of 14c in these areas could occur any accumulation in concentration may not occur to the same extent predicted by the ewe model greater retention of 14c at these sites will result in reduced dispersion to more distant areas the fact that modelled 14c dispersion does not significantly penetrate the scottish west coast suggests that model retention in the irish sea is too high previous studies have shown that a significant proportion of sellafield discharges are dispersed around the scottish coastline gulliver et al 2001 tierney et al 2016 2017a the consequence of using depth averaged advection in our study is that the necessary complex hydrodynamics to drive dispersion at this regional scale may not be well addressed in this model at coastal areas such as the solway firth dispersion will be complicated by freshwater input and non uniform current direction at different depths which would reduce the overall retention of dissolved 14c at these sites using depth averaged advection means that dissolved 14c can be trapped and accumulate exponentially at sites if advection is directed towards the coastline although this is significantly reduced by increasing the base dispersal rate dispersion is also limited by data and map resolution the velocity data used had a grid resolution of 7 km so any local physical dynamics were lost the 5 km base map grid resolution meant that many features of the uk coastline were not well defined including the loss of several islands on the scottish west coast that are connected to the mainland in the model 3 2 14c ecological fate muir et al 2017 reported 14c activities for dic and a number of species at sites in the irish sea east basin station eb and west basin station wb in june 2014 model 14c activities at eb in june 2014 were significantly over predicted compared to observed activities when using a low dispersal rate 100 km per month but a higher base dispersal rate 200 km per month brought the predicted and observed activities significantly closer fig 4 trends in the observed data were replicated by the model phytoplankton and zooplankton 14c activities were relatively low compared to benthic species and dab 14c activity was the highest although a large range in observed dab activity 499 763 bq kg 1 c meant that the average dab activity 631 bq kg 1 c was less than the infaunal macrobenthos 704 bq kg 1 c the model did not capture this high infaunal macrobenthos 14c activity relative to most other groups the observed infaunal macrobenthos activity comes from green spoon worm maxmuelleria lankesteri tissue and this species is known to have an important role in the redistribution of other sellafield derived radionuclides in bottom sediments hughes et al 1996 kershaw et al 1983 1984 1999 its inclusion as a separate species in the model was considered however this was deemed to be challenging due to limited ecological data station wb is more complex due to highly variable reported 14c activities between species muir et al 2017 typically both high and low dispersion rates under predicted the observed higher activities in polychaetes epifaunal macrobenthos and dab and over predicted the observed lower activities e g phytoplankton and zooplankton fig 5 however the main observed trends were again predicted as for eb plankton 14c activities were significantly lower than other functional groups and dab activity was again predicted to be the highest the relatively high 14c activity observed in polychaetes 405 bq kg 1 c was due to the higher observed net activity of the predatory species aphrodita aculeate 740 bq kg 1 c whereas the average activity of other polychaete species was lower 69 bq kg 1 c and similar to the model predicted activity of 59 bq kg 1 c the observed epifaunal macrobenthos activity was also relatively high 488 bq kg 1 c and not captured by the model similar to the polychaete group the model functional group epifaunal macrobenthos was made up of numerous species and the observed 14c activity was comprised from an average of starfish species only and may not accurately represent the entire functional group both these cases indicate that model functional groups were not well defined in some instances as the addition of a predatory species to a functional group is not best practice heymans et al 2016 a number of 14c activities across a range of species were reported by tierney et al 2017a for two sites in the west of scotland marine environment the north channel station nc and firth of lorn station fol due to northward dispersion of 14c being constrained in the model as a result of irish sea retention of 14c being too high the model under predicts activities at these sites relative to the observed activities additionally the connection of several islands to the scottish mainland due to the 5 km base map resolution blocked important channels in the west of scotland area including to the south of the firth of lorn preventing direct northward dispersion of 14c to this area and much of the firth of lorn itself the lack of penetrative northward dispersion of 14c resulted in the model showing no 14c enrichment at fol in 2014 although a small enrichment in dic and benthic species was observed tierney et al 2017a the model only predicted a slight enrichment 1 2 bq kg 1 c in dic and some functional groups at fol between 2005 and 2009 at station nc the observed trend of low plankton activities and higher benthic activities was again replicated in june 2014 fig 6 as observed whiting activity was predicted to be higher than other groups and repeated the theme where the group with the highest modelled trophic level also had the highest activity see dab for irish sea sites however the comparatively high activity observed in whiting at the nc station was interpreted as being likely due to northward migration of whiting which had foraged in the irish sea tierney et al 2017a the issues discussed with the model 14c dispersion meant that predicted activities for harbour porpoises did not typically align with the activities reported by tierney et al 2017b it should also be noted that although harbour porpoise is a resident species and observed 14c activities indicate a high feeding fidelity tierney et al 2017b these are animals that can traverse the modelled area and single measurements from a stranded individual is unlikely to represent the average activity across the population in that area predicted trends through time do however appear to replicate the observed trends as illustrated by comparing predicted harbour porpoise 14c activities in four different years 1993 2002 2004 and 2014 with the observed activities for those years fig 7 both predicted and observed 14c activities show very low 14c activities of between 0 and 10 bq kg 1 for west of scotland porpoises north of the north channel in 1993 with activities significantly higher in the south east irish sea peak discharges between 2001 and 2005 increased porpoise 14c activity in the north east irish sea and activities were lower in the north channel and clyde sea clyde sea 14c activities were lower in 2014 but activities in the north channel remained relatively higher and the highest activities were found in the south east irish sea the sellafield model illustrates that ecosystem uptake of 14c for a specific area is controlled by the dic 14c activity in that area and therefore the dispersion of changeable sellafield 14c discharges through time fig 8 phytoplankton and subsequently zooplankton 14c activities closely mirror changes in the dic 14c activity as 14c transfers to higher trophic levels are not immediate there is a delayed response to 14c activities which has a smoothing effect on predicted activities through time modelled 14c activities for stations eb wb and nc in june 2014 show a general trend of increasing activity with increasing trophic level figs 4 6 this is not due to bioaccumulation but rather the lag effect in 14c transfer to higher trophic levels culminating in top predators such as harbour porpoise the very low 14c discharge activity in june 2014 caused dic and plankton activities to drop at station eb whilst other functional group activities remained higher due to uptake of previously higher activities variable dispersion of 14c to station wb resulted in dic and plankton activities decreasing significantly below the 14c activities of other species in june 2014 after a peak in dic activity at station nc in 2007 the activities at higher trophic levels gradually declined but not to below the significantly reduced plankton activities this mechanism which likely caused the higher observed 14c activities in benthic species was suggested by muir et al 2017 and tierney et al 2017a who described an integrated 14c activity in older living organisms occupying higher trophic levels it was also identified through analysis of marine mammal 14c activities alone tierney et al 2017b where mammal 14c activities correlated significantly with total sellafield discharges for 24 months prior to stranding as shown by model results this means that the 14c activity of an organism is not only dependent on the discharge activity and the dispersion of 14c which can be highly variable but is also dependent on the trophic level that the organism feeds at feeding at lower trophic levels will result in a species having a highly variable 14c activity through time species that feed at higher trophic levels will have 14c activities that are not dependent on the immediate environmental activity and could be significantly more or less enriched in 14c relative to the environment they inhabit to illustrate the differences in 14c activities spatially and temporally at different trophic levels a video component video 2 is available and accompanies the electronic version of the manuscript supplementary video related to this article can be found at https doi org 10 1016 j envsoft 2018 01 013 the following is the supplementary data related to this article video 2 video 2 3 3 advantages and limitations as discussed the ewe approach accurately demonstrates a number of the observed trends in 14c activities and reproduces the observed transfer of 14c through the marine food web after initial uptake by primary producers relative to the environmental 14c activity it can therefore provide a tool which is capable of predicting the ecological uptake of radioactive contamination or other environmental contaminants i e trace metals if the environmental concentrations were accurately provided predicted activities for a specific functional group are limited by how well each functional group and their ecology are defined in the model diet is a key factor in an organisms 14c activity and diet description data should be revisited and improved where possible for the sellafield model a major advantage of ewe is that it can predict general trends for contaminant concentrations in non specific functional groups or specific contaminant concentrations in individual species for example if the aim was to determine the transfer of 14c or other radionuclides between different benthic species and the sediment then the functional groups describing these species should be further developed discrepancies between observed and predicted activities for benthic species would be better resolved by incorporation of a well defined microbial loop in the model modelling 14c dispersion within the ewe framework significantly reduces far field dispersion beyond the irish sea in comparison to observed data and appears to result from increased retention of 14c at specific areas within the irish sea as this study aimed to model the general patterns of 14c dispersion the velocity and base map resolutions are appropriate nevertheless using depth averaged advection over simplifies the localised oceanographic conditions in future work this could be overcome by using a 3 dimensional physical transport model to disperse 14c in the environment by using the same approach to which velocity data were input to ecospace in this study employing the spatial temporal framework steenbeek et al 2013 depth averaged 14c activity concentration fields predicted by the physical transport model could be applied instead this study did not consider ecosystem shifts e g changes in species biomass and the knock on effects through time however if a model contamination study for an area covers an extensive period of time then changes in the ecosystem which could affect contaminant concentration in the ecology should also be modelled in ewe the sellafield 14c model and observed 14c activities show that the 14c activity for a functional group species is dependent on the trophic level it feeds upon most ecosystems in general and the irish sea specifically have undergone significant changes over the past century due to changes in the fishing hunting pressures and climate which result in species changing their foraging behaviour and the prey they feed on this would affect the 14c activity of a species and if the contaminant was subject to bioaccumulation this could lead to additional model complexities future work should consider this and for 14c seek to address changes in ecosystem uptake due to seasonal variation in primary productivity 4 conclusions this study modelled the ecosystem uptake and ecological fate of sellafield 14c discharged to the uk marine environment using the ewe software the advantages of the ewe approach were illustrated in capturing observed trends in 14c activities for species at specific locations and through time in addition the model data aids understanding of 14c transfer processes through the food web 14c does not bio accumulate although higher activities have been observed at higher trophic levels the sellafield model illustrates that changes in environmental 14c activities will directly and immediately impact species activity at lower trophic levels whereas higher trophic level species 14c activities are integrated over time therefore species 14c activity will be strongly affected by the trophic level from which it feeds limitations in the model s ability to use advection data to disperse 14c through the marine environment meant that the specific 14c activities predicted for some areas such as the west of scotland did not compare well with observed activities further measurements of dic 14c activities such as the solway firth where the model predicts an accumulation of sellafield 14c would reduce uncertainty in dispersion patterns the effectiveness of ewe for modelling the ecological fate of contaminants in the environment has been underrepresented despite the wide use of the ewe approach to ecosystem modelling recent developments in the software were utilised in this study further refinements such as coupling this approach with better resolved contaminant dispersion could be used to help address the ecological fate of a wide range of contaminants including radionuclides acknowledgements this work was completed as part of the lo rise long lived radionuclides in surface environments ne l000202 1 consortium under the nerc rate programme radioactivity and the environment co funded by the environment agency and radioactive waste management ltd this study has been conducted using e u copernicus marine service information appendix a 1 sellafield model functional groups and balanced input parameters biomass production biomass p b consumption biomass q b ecotrophic efficiency ee production consumption p q and unassimilated consumption values used before balancing are shown in brackets where applicable group name biomass t km2 p b year q b year ee p q unassimilated consumption bottlenose dolphin 0 0016 0 2 8 67 0 2 harbour porpoise 0 0105 0 2 8 67 0 2 minke whale 0 0893 0 02 10 0 2 common seal 0 0005 0 1 14 55 0 2 grey seal 0 004 0 1 14 55 0 2 seabirds 0 0511 1 075 82 664 0 2 large sharks 0 115 0 318 3 18 0 2 small sharks 0 288 0 972 9 72 0 2 basking sharks 0 0014 0 07 3 7 0 2 skates and rays 0 103 1 6 16 0 2 cod 0 6253 1 3891 4 7051 0 2 haddock 0 2711 2 4751 8 5356 0 2 plaice 0 3425 1 3522 5 6234 0 2 whiting 0 55 0 507 0 842 2 97 0 2 sole 0 16 0 863 2 58 0 2 monkfish 0 125 1 246 1 989 0 95 0 2 0 2 dab 0 07 2 394 3 042 0 95 0 2 0 2 other flatfish 0 2404 2 1757 3 8572 0 95 0 2 0 2 dragonets 0 229 1 54 5 154 0 2 mackerel 1 623 0 414 4 4 1 73 0 2 ling 0 076 1 315 3 089 0 2 other demersals 2 4158 1 5384 4 5888 0 2 herring 1 2131 1 154 0 727 6 516 0 2 other small pelagic planktivorous fish 2 4262 0 727 6 516 0 2 sandeels 1 3 1 53 5 016 0 2 epifaunal macrobenthos 13 1 661 0 2 0 2 epifaunal mesobenthos 8 999 8 975 2 062 0 22 0 2 infaunal macrobenthos 8 007 2 695 0 2 0 2 infaunal mesobenthos 24 773 2 552 0 22 0 2 infauna polycheate 22 726 3 683 0 3 0 2 lobster and large crabs 0 11 0 098 0 783 5 22 0 2 nephrops 0 35 0 73 4 867 0 95 0 2 cephalopods 0 25 1 981 15 0 2 prawns and shrimp 4 925 4 847 0 959 6 393 0 2 sessile epifauna 7 5 2 066 0 2 0 2 meiofauna 6 314 18 45 0 3 0 2 zooplankton 48 475 15 2855 0 95 0 3 0 2 seaweed 75 60 microflora 3 92 587 phytoplankton 13 83 70 14 particulate organic matter 50 dissolved organic matter 50 discards 0 309 a 2 sellafield model diet matrix values used pre balancing are shown in parentheses where applicable prey predator bottlenose dolphin harbour porpoise minke whale common seal grey seal seabirds bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 0100 large sharks 0 0 0 0 0 0 small sharks 0 1000 0 0 0 0050 0 0100 0 0010 basking sharks 0 0 0 0 0 0 skates and rays 0 1000 0 0 0 0790 0 0300 0 0040 cod 0 0 0059 0 0 0 0 0410 haddock 0 0593 0 0135 0 0 0 0 0160 plaice 0 0027 0 0 0 0032 0 0 0050 whiting 0 0200 0 2130 0 0 0350 0 0300 0 0240 sole 0 0010 0 0 0 0200 0 0200 0 0070 monkfish 0 0 0 0 0 0 0010 dab 0 0044 0 0066 0 0 0338 0 0182 0 0073 other flatfish 0 0186 0 0304 0 0 1025 0 0292 0 0257 dragonets 0 0 0 0 0371 0 0055 0 0120 mackerel 0 0126 0 0149 0 2500 0 4000 0 0 0950 0 0390 ling 0 1805 0 0 0 1000 0 0900 0 0130 other demersals 0 4615 0 4122 0 0 3873 0 4998 0 1602 herring 0 0089 0 0563 0 2600 0 1300 0 0247 0 0290 0 0020 other planktivorous fish 0 0177 0 1126 0 2800 0 2600 0 0493 0 0579 0 0040 sandeels 0 0 1000 0 0050 0 1038 0 0160 0 1481 epifaunal macrobenthos 0 0 0 0 0 0 0521 epifaunal mesobenthos 0 0 0 0 0 0 0521 infaunal macrobenthos 0 0 0 0 0 0 infaunal mesobenthos 0 0 0 0 0 0 infauna polychaete 0 0 0 0 0 0 1051 lobster and large crabs 0 0 0 0 0 0150 0 0020 nephrops 0 0 0 0 0 0150 0 cephalopods 0 0129 0 0343 0 0900 0 0191 0 0395 0 prawns and shrimp 0 0 0 1000 0 0 0 1682 sessile epifauna 0 0 0 0 0 0 meiofauna 0 0 0 0 0 0 zooplankton 0 0 0 0150 0 0 0 seaweed 0 0 0 0 0 0 microflora 0 0 0 0 0 0 phytoplankton 0 0 0 0 0 0 particulate organic matter 0 0 0 0 0 0 0800 0 dissolved organic matter 0 0 0 0 0 0 discards 0 0 0 0 0 0 0200 0 1000 prey predator large sharks small sharks basking sharks skates and rays cod haddock bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 large sharks 0 0 0 0 0 0 small sharks 0 0 0 0 0060 0 0 basking sharks 0 0 0 0 0 0 skates and rays 0 0 0 0 0030 0 0 cod 0 0 0 0 0 0043 0 haddock 0 0 0 0 0 0108 0 0074 plaice 0 0 0 0 0 0 whiting 0 0 0 0 0080 0 0168 0 sole 0 0 0 0 0 0 monkfish 0 0 0 0 0 0 dab 0 0 0 0 0022 0 0021 0 other flatfish 0 0 0940 0 0 0018 0 0036 0 dragonets 0 0 0 0 0180 0 0057 0 0015 mackerel 0 0 0 0 0060 0 0206 0 0008 ling 0 0 0 0 0 0 other demersals 0 0 2480 0 0 0640 0 0243 0 herring 0 0 0513 0 0 0027 0 0036 0 other planktivorous fish 0 0 1026 0 0 0053 0 0072 0 sandeels 0 0 1340 0 0 0040 0 0240 0 0090 epifaunal macrobenthos 0 6650 0 3140 0 0 2510 0 3780 0 1206 epifaunal mesobenthos 0 0150 0 0 0 0310 0 0449 0 0839 infaunal macrobenthos 0 2000 0 0010 0 0 0 0 0129 infaunal mesobenthos 0 0 0010 0 0 0 0 0129 infauna polychaete 0 0350 0 0 0 0210 0 0672 0 0676 lobster and large crabs 0 0 0400 0 0 0 0 nephrops 0 0 0 0 0010 0 0280 0 cephalopods 0 0850 0 0 0 0070 0 0023 0 0015 prawns and shrimp 0 0 0010 0 0 4530 0 0529 0 0482 sessile epifauna 0 0 0 0 0 0 0008 meiofauna 0 0 0 0 0 0 zooplankton 0 0 0130 1 0000 0 1080 0 2823 0 4837 seaweed 0 0 0 0 0 0 0008 microflora 0 0 0 0 0 0 phytoplankton 0 0 0 0 0 0215 0 1484 particulate organic matter 0 0 0 0 0070 0 0 dissolved organic matter 0 0 0 0 0 0 discards 0 0 0 0 0 0 prey predator plaice whiting sole monkfish dab other flatfish bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 large sharks 0 0 0 0 0 0 small sharks 0 0 0 0 0 0 basking sharks 0 0 0 0 0 0 skates and rays 0 0 0 0 0 0 cod 0 0 0200 0 0 0110 0 0020 0 0411 haddock 0 0 0200 0 0 0055 0 0020 0 0411 plaice 0 0042 0 0 0 1044 0 0 whiting 0 0 0100 0 0 0110 0 0020 0 0002 sole 0 0 0100 0 0 0440 0 0 monkfish 0 0 0500 0 0 0 1000 0 0112 dab 0 0 0219 0 0 0778 0 0015 0 0002 other flatfish 0 0 0581 0 0 1343 0 0025 0 0003 dragonets 0 0056 0 0060 0 0 1055 0 0100 0 1000 0 0100 0 0112 mackerel 0 0 0240 0 0 1363 0 0050 0 0006 ling 0 0 0 0 0 0 other demersals 0 0056 0 1740 0 0 2000 0 2040 0 2199 0 1282 herring 0 0 0480 0 0 0351 0 0127 0 0165 other planktivorous fish 0 0 0959 0 0 0703 0 0253 0 0331 sandeels 0 0112 0 1100 0 0 0110 0 0200 0 1064 epifaunal macrobenthos 0 0562 0 1160 0 1000 0 0110 0 1540 0 1110 0 1381 epifaunal mesobenthos 0 1404 0 0170 0 2500 0 0 0020 0 0095 infaunal macrobenthos 0 1404 0 0 2500 0 0 0 0093 infaunal mesobenthos 0 1050 0 0 1500 0 0 0 infauna polychaete 0 2277 0 0120 0 2500 0 0110 0 2200 0 1140 0 1200 0 1336 lobster and large crabs 0 0 0 0 0077 0 0 nephrops 0 0 0010 0 0 0033 0 0009 0 0001 cephalopods 0 0 0160 0 0 0 0070 0 0380 prawns and shrimp 0 0624 0 0850 0 0 0209 0 1070 0 1235 sessile epifauna 0 0 0 0 0 0004 0 meiofauna 0 0 0 0 0 0 zooplankton 0 1997 0 1050 0 0 0 1220 0 1993 seaweed 0 0 0 0 0 0 microflora 0 0 0 0 0 0 phytoplankton 0 0416 0 0 0 0 0 particulate organic matter 0 0 0 0 0 0 dissolved organic matter 0 0 0 0 0 0 discards 0 0 0 0 0 0 prey predator dragonet mackerel ling other demersals herring other planktiv orous fish bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 large sharks 0 0 0 0 0 0 small sharks 0 0 0 0 0 0 basking sharks 0 0 0 0 0 0 skates and rays 0 0 0010 0 0 0 0 cod 0 0 0 0 0040 0 0 haddock 0 0 0 0 0004 0 0 plaice 0 0 0 0 0 0 whiting 0 0 0 0 0019 0 0 sole 0 0 0 0 0031 0 0 monkfish 0 0 0 0 0 0 dab 0 0 0 0 0027 0 0 other flatfish 0 0 0 0 0019 0 0 dragonets 0 0 0 0 0098 0 0 mackerel 0 0 0007 0 0 0098 0 0 ling 0 0 0 0 0 0 other demersals 0 0 0010 0 0 0325 0 0100 0 0100 herring 0 0 0030 0 0 0095 0 0007 0 0007 other planktivorous fish 0 0 0060 0 0 0190 0 0013 0 0013 sandeels 0 0 0 0 0085 0 0 epifaunal macrobenthos 0 6420 0 0280 0 5000 0 0914 0 0280 0 0280 epifaunal mesobenthos 0 0 0090 0 0 0781 0 0060 0 0060 infaunal macrobenthos 0 0 0 0 0006 0 0 infaunal mesobenthos 0 0 0 0 0003 0 0 infauna polychaete 0 2860 0 0007 0 0 0207 0 0020 0 0020 lobster and large crabs 0 0 0 0 0 0 nephrops 0 0 0 0500 0 0018 0 0 cephalopods 0 0 0010 0 0 0033 0 0005 0 0005 prawns and shrimp 0 0 0 4500 0 1538 0 0060 0 0060 sessile epifauna 0 0 0 0 0 0 meiofauna 0 0 0 0 0 0 zooplankton 0 0720 0 9373 0 0 4772 0 9455 0 9455 seaweed 0 0 0 0 0 0 microflora 0 0 0 0 0 0 phytoplankton 0 0 0003 0 0 0 0 particulate organic matter 0 0 0120 0 0 0697 0 0 dissolved organic matter 0 0 0 0 0 0 discards 0 0 0 0 0 0 prey predator sandeels epifaunal macro benthos epifaunal meso benthos infaunal macro benthos infaunal meso benthos infauna polychaete bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 large sharks 0 0 0 0 0 0 small sharks 0 0 0 0 0 0 basking sharks 0 0 0 0 0 0 skates and rays 0 0 0 0 0 0 cod 0 0 0 0 0 0 haddock 0 0 0 0 0 0 plaice 0 0 0 0 0 0 whiting 0 0 0 0 0 0 sole 0 0 0 0 0 0 monkfish 0 0 0 0 0 0 dab 0 0 0 0 0 0 other flatfish 0 0 0 0 0 0 dragonets 0 0 0 0 0 0 mackerel 0 0 0 0 0 0 ling 0 0 0 0 0 0 other demersals 0 0 0 0 0 0 herring 0 0 0 0 0 0 other planktivorous fish 0 0 0 0 0 0 sandeels 0 0 0 0 0 0 epifaunal macrobenthos 0 0 0270 0 0 0220 0 0 epifaunal mesobenthos 0 0 1040 0 0100 0 0220 0 0059 0 infaunal macrobenthos 0 0 1760 0 0 0120 0 0 infaunal mesobenthos 0 0 1770 0 3200 0 1330 0 0059 0 infauna polychaete 0 0 1780 0 3200 0 1330 0 0554 0 lobster and large crabs 0 0 0 0 0 0 nephrops 0 0 0 0 0 0 cephalopods 0 0 0 0 0 0 prawns and shrimp 0 0 0 0 0 0 sessile epifauna 0 0 0060 0 0 0 0495 0 meiofauna 0 0 0 3000 0 1120 0 1484 0 zooplankton 0 6000 0 0760 0 0 1640 0 0425 0 seaweed 0 0 0390 0 0400 0 0 0 microflora 0 0 0370 0 0100 0 0010 0 1098 0 3300 phytoplankton 0 1000 0 0190 0 0 2710 0 1098 0 particulate organic matter 0 3000 0 1420 0 1210 0 0 0650 0 1650 0 1098 0 3400 dissolved organic matter 0 0 0190 0 0 0650 0 3076 0 3300 discards 0 0 0 0210 0 0 0 0 0554 0 prey predator lobster and large crabs nephrops cephalo pods prawns and shrimp sessile epifauna meio fauna bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 large sharks 0 0 0 0 0 0 small sharks 0 0 0 0 0 0 basking sharks 0 0 0 0 0 0 skates and rays 0 0 0 0 0 0 cod 0 0 0 0098 0 0 0 haddock 0 0 0 0098 0 0 0 plaice 0 0 0 0098 0 0 0 whiting 0 0 0 0010 0 0 0 sole 0 0 0 0098 0 0 0 monkfish 0 0 0 0 0 0 dab 0 0 0 0072 0 0 0 other flatfish 0 0 0 0026 0 0 0 dragonets 0 0 0 0 0 0 mackerel 0 0 0 0 0 0 ling 0 0 0 0 0 0 other demersals 0 0 0090 0 0029 0 0 0 herring 0 0 0 0003 0 0 0 other planktivorous fish 0 0 0 0007 0 0 0 sandeels 0 0 0 0010 0 0 0 epifaunal macrobenthos 0 0500 0 0700 0 0196 0 0 0 epifaunal mesobenthos 0 0500 0 0700 0 0196 0 0 0 infaunal macrobenthos 0 0500 0 0700 0 0196 0 0 0 infaunal mesobenthos 0 0500 0 0700 0 0196 0 0 0 infauna polychaete 0 0 0500 0 0098 0 0 0 0100 lobster and large crabs 0 0300 0 0 0049 0 0 0 nephrops 0 0 0 0010 0 0 0 cephalopods 0 0 0 0010 0 0 0 prawns and shrimp 0 1700 0 1500 0 0 0098 0 0 0 sessile epifauna 0 0 1610 0 0 0 0 meiofauna 0 0 0 0 0 0 0900 zooplankton 0 0 0 6438 0 0900 0 2970 0 seaweed 0 0 0 0 0 0 microflora 0 0 0 0 0 1430 0 7000 phytoplankton 0 0 0 1963 0 0800 0 1430 0 particulate organic matter 0 6000 0 6200 0 4700 0 0 0 5200 0 1430 0 2000 dissolved organic matter 0 0 0 0 3100 0 2740 0 discards 0 0200 0 0 0300 0 5000 0 0 0 0 
26422,sellafield marine discharges of 14c are the largest contributor to the global collective dose from the nuclear fuel industry as such it is important to understand the fate of these discharges beyond the limitations and scope of empirical analytical investigations for this highly mobile radioactive contaminant ecopath with ecosim ewe is widely used to model anthropogenic impacts on ecosystems such as fishing although very few ewe studies have modelled the fate of bioavailable contaminants this work presents for the first time a spatial temporal 14c model utilising recent developments in ewe software to predict the ecological fate of anthropogenic 14c in the marine environment the model predicted observed trends in 14c activities between different species and through time it also provided evidence for the integration of sellafield 14c in species at higher trophic levels through time keywords radiocarbon 14c radioactive discharges sellafield ecosystem model ecopath with ecosim ecotracer software availability name of software ecopath with ecosim developer ecopath research and development consortium contact address ecopath international initiative research association barcelona spain contact email ewedevteam gmail com year first available 1991 hardware required pc software required windows xp service pack 3 or newer microsoft net framework 4 full profile availability public open source gplv2 freely available from www ecopath org version used in this study available from developer program language visual basic net c program size 16 mb basic installation 1 introduction understanding the ecological fate of anthropogenic radionuclides is necessary to determine any potentially hazardous consequences to the environment and to human populations although empirical studies are essential they are often time consuming costly and impractical to conduct particularly if basic information is required quickly e g after accidental releases of radioactive material or if radioactive contamination is spread over a large area computer modelling provides an additional tool which can be both time and cost effective the ecosystem modelling software suite ecopath with ecosim ewe christensen and walters 2004 has undergone recent developments that aid its ability to model the ecological fate of environmental contaminants spatially ewe has previously been used to model non radioactive contaminants such as methyl mercury in the faroe islands marine ecosystem booth and zeller 2005 chemical warfare agents in the baltic sea niiranen et al 2008 sanderson et al 2010 polycyclic aromatic hydrocarbons pahs in the arctic larsen et al 2016 and polychlorinated biphenyls pcb and mercury in the great lakes mcgill et al 2017 ewe has also been used to model radioactive contaminants including a hypothetical release of radiocarbon 14c in the baltic sea sandberg et al 2007 and radiocaesium 137cs in a simple coastal marine ecosystem model for fukushima walters and christensen 2017 many of these studies considered temporal change in contaminant activity but not spatial variation the ewe approach is appealing for modelling bioavailable contaminants due primarily to its ease of use there are over 400 unique ewe models published making it the most extensively applied tool for modelling marine and aquatic ecosystems colleter et al 2015 the extensive coverage of published ewe models also means that models may already exist for a specific ecosystem in which an environmental contaminant is present or subject to a recent contamination event and can be adapted for this purpose there are four components to ewe 1 ecopath which describes a static mass balanced ecosystem polovina 1984 2 ecosim a time dynamic simulation module allowing for temporal changes and impacts to an ecosystem to be investigated walters et al 1997 3 ecospace where ecosystem changes can be explored both spatially and temporally walters et al 1999 and 4 ecotracer which traces contaminants through the ecosystem walters and christensen 2017 while ecosim ecospace solve biomass dynamic equations ecotracer simultaneously models contaminant flow and or accumulation to the biological groups and environment as described in the base ecopath model ecotracer and the equations it utilises are fully described in walters and christensen 2017 the development of the spatial temporal ewe framework steenbeek et al 2013 allows physical changes to occur in the ecospace environment through time as environmental contamination is typically non ubiquitous and non static this is an appropriate development to consider for contaminant models where contaminant concentrations in the environment are both temporally and spatially variable in recent decades several studies have accumulated a large body of data regarding the fate of 14c discharged by the sellafield nuclear fuel reprocessing facility in cumbria uk to the marine environment begg et al 1992 cook et al 1995 1998 2004 gulliver et al 2001 2004 mackenzie et al 2004 muir et al 2015 2017 tierney et al 2016 2017a 2017b the most recent work has focussed on detailing the ecosystem uptake of 14c in the irish sea muir et al 2017 and the west of scotland tierney et al 2017a marine environments and has established a time series of 14c activities in marine mammals along the uk coastline tierney et al 2017b sellafield discharges 14c as low activity waste effluent via pipelines to the irish sea in the dissolved inorganic phase which is rapidly incorporated into the marine dissolved inorganic carbon dic pool begg et al 1992 cook et al 1995 in this manner sellafield 14c spreads throughout the irish sea and is largely dispersed northward through the north channel to the west of scotland marine environment by prevailing currents gulliver et al 2001 whilst contemporary environmental releases of 14c from sellafield do not pose any direct radiological risk to critical consumer groups near sellafield muir et al 2017 releases of 14c are still highly significant 14c has a long half life 5730 years is environmentally mobile highly bioavailable and marine discharges are the largest contributor to the uk european and global collective dose from sellafield nuclear decommissioning authority 2016 the aim of this study was to construct an ewe contaminant tracing model for sellafield 14c in the uk marine environment the sellafield model and to test it primarily using recent data compiled in muir et al 2017 and tierney et al 2017a 2017b this is the first time an ewe model has been tested on its ability to accurately predict the spatial ecosystem uptake and fate of radionuclides discharged routinely to the marine environment the unique biogeochemical properties of 14c in being identical in its behaviour to stable carbon on an ecosystem level coupled with its long half life make it an ideal contaminant with which to evaluate the performance and applicability of the ewe model and ecotracer utility for radioactive contamination scenarios 2 methods 2 1 ecopath input in ewe functional groups are either specific species or a group of species deemed to have sufficiently similar functionality for the purposes of the model heymans et al 2016 functional groups require a number of input parameters to satisfy the ecopath mass balance equation where consumption c production p respiration r unassimilated food u the productivity of each functional group is defined in ecopath by the equation p i y i b i m 2 i e i b a i p i 1 e e i where for group i p i is the total production rate y i is the total fishery catch rate b i is the biomass m 2 i is the total predation rate e i is the net migration rate emigration immigration b a i is the biomass accumulation rate and ee is the ecotrophic efficiency of the group christensen and walters 2004 ecotrophic efficiency is the proportion of a group s production that is explained in the model and this cannot exceed 1 typical inputs to ecopath include values for biomass b annual production biomass p b and either annual consumption biomass q b or production consumption p q as sellafield discharges 14c into the irish sea a mass balanced ewe model developed by lees and mackinson 2007 that describes the irish sea ecosystem in the early 1970s was used as the foundation for the sellafield model however aspects of this model were either beyond the complexity required for the sellafield model or not descriptive enough and a number of changes were made the lees and mackinson irish sea model contains a total of 53 functional groups which was reduced to 43 in the sellafield model as discussed below three model groups cod gadus morhua haddock melanogrammus aeglefinus and plaice pleuronectes platessa were separated into two age classes adult and juvenile these age class separations were not necessary for our purposes and these species were instead combined into single species groups in the sellafield model muir et al 2017 and tierney et al 2017a describe 14c activities in three fish species dab limanda limanda ling molva molva and herring clupea harengus which are not individually specified in the irish sea model using species information described in lees and mackinson 2007 dab was separated from the medium flatfish functional group ling extracted from other large demersals and herring from other small pelagic planktivorous fish the functional groups small medium and large flatfish were combined into the singular other flatfish group likewise a single functional group called other demersals was created by combining bass seatrout gurnards mullet other large demersals other large gadoids other small demersals and other small gadoids the irish sea model contained four zooplankton groups herbivorous omnivorous carnivorous and gelatinous which were also combined to form a single zooplankton group where groups were combined biomass values were summed and other input parameters from lees and mackinson 2007 were calculated as a proportion of the biomasses of the previously existing groups where new groups were extracted biomasses and other input parameters were taken as described in lees and mackinson 2007 tierney et al 2017b describe 14c activities in 3 marine mammal species harbour porpoise phocoena phocoena harbour common seal phoca vitulina and grey seal halichoerus grypus none of these species are specified in the irish sea model which contains the mammal functional groups baleen whale toothed whale and seals the description of marine mammals in the irish sea model was the focus of a model re structuring by hernandez milian 2014 following this the existing irish sea mammal groups were removed and five new functional groups were added for specific species bottlenose dolphin tursiops truncates harbour porpoise minke whale balaenoptera acutorostrata common seal and grey seal input parameters including biomass p b q b and diet for these functional groups were taken from hernandez milian 2014 at the base of the food web primary producers particularly phytoplankton were key functional groups in the sellafield model as 14c enters the food web through uptake by primary producers during photosynthesis phytoplankton biomass and p b were re calculated using the formula from gowen and bloomfield 1996 and primary productivity estimates from gowen et al 2000 of 97 g m2 for the coastal irish sea this resulted in an increase in biomass from 9 7 to 13 8 t km2 and a reduction in the p b value from 152 5 to 70 1 year 1 ecopath also requires diet estimates of each functional group and the diet matrix was largely carried over from the irish sea model where groups were combined new diet estimates were calculated from previous diets as a function of each group s biomass where new groups were created the diet was assumed to be the same for example herring and other small pelagic planktivorous fish have the same diet diets for the new mammal groups were taken from hernandez milian 2014 the diets for two functional groups small sharks and monkfish lophius piscatorius were edited for better definition small shark diet was updated according to ellis et al 1996 and monkfish diet was updated according to crozier 1985 the sellafield model input parameters and diet matrix are included in the appendix tables a 1 and a 2 the irish sea model contained nine fisheries which were retained in the sellafield model landings and discards of these fisheries were corrected for the new and combined functional groups but no other changes were made 2 2 prebal and balancing after the described changes to the model were made model assumptions were tested following a set of pre balance diagnostics prebal described by link 2010 prebal checks that the ecosystem model makes ecological and thermodynamic sense by checking the slopes of biomass ratios and other data input against trophic levels there were no significant issues with the sellafield model however the annual p b ratios for dab 2 4 and other flatfish 2 2 did appear to be high and conversely the p b ratios for herring 0 7 and other small pelagic planktivorous fish 0 7 appeared to be relatively low no changes were made to the input parameters for these groups following prebal however the groups were highlighted again during model balancing the ecopath model must be mass balanced after entering the input parameters heymans et al 2016 the sellafield model was initially imbalanced and several parameters were subsequently corrected as explained below increases in biomass for whiting merlangius merlangus 8 5 and lobster and large crabs 12 2 are within the biomass estimates reported by lees and mackinson 2007 the biomass increases to the epifaunal mesobenthos 0 27 and prawn and shrimp 1 8 are negligible relative to the changes in biomass made by lees and mackinson 2007 when balancing the irish sea model large differences were found in the irish sea model between the initial biomass values used and the balanced biomasses for monkfish flatfish small medium and large nephrops and zooplankton groups biomasses were estimated in ecopath for monkfish dab other flatfish nephrops and zooplankton by setting ecotrophic efficiency ee to 0 95 for these groups this assumes that the model uses all but 5 of the production of that group and ecopath can estimate a biomass based on this assumption in addition the p b ratios for dab and other flatfish were estimated in ecopath by setting the production consumption p q ratio for these groups to 0 2 these changes were made based on best practices described by heymans et al 2016 as herring p b had been identified as being relatively high it was re calculated using fishing mortality f from lees and mackinson 2007 and natural mortality m from fishbase froese and pauly 2016 as p b is equal to total mortality z mackerel q b was corrected using default values from fishbase including a mean temperature of 10 c corrections were also made to the diet matrix for model balancing and the most significant of these was the reduction in consumption of discards which was relatively high in the irish sea model e g the proportion of discards in the diet of nephrops was reduced from 0 5 to 0 03 this was balanced by increasing the proportion of particulate organic matter in diets a further important change was a substantial increase in the proportion of polychaetes in dab diet as described by gibson and ezzi 1987 all changes made to both the input parameters and the diet matrix for model balancing are shown in the appendix tables a1 and a2 2 3 ecospace to accurately predict uptake of sellafield derived 14c the sellafield model had to be spatially resolved in ecospace the biomass b of a functional group for a specific grid cell at time t can be expressed as d b d t i g c z e b where i is the total immigration rate from surrounding cells g is the net growth efficiency c is food consumption rate z is total instantaneous mortality rate and e is total instantaneous emigration rate walters et al 1999 due to the net northward dispersion of 14c and the available data for the west of scotland tierney et al 2017a 2017b the ecospace base map of the sellafield model produced in arcgis encompasses part of the west of scotland marine environment as well as the irish sea fig 1 base map grid resolution was 5 km with boundaries at approximately 56 45 n northern boundary 02 45 w eastern boundary 53 15 n southern boundary and 7 15 w western boundary the base map covers key sites in both the irish sea and west of scotland used in studies concerning 14c in the environment and biota muir et al 2017 tierney et al 2017a 2017b the recently introduced contaminant map layer in ecospace allows the user to input contaminant concentrations across the base map this can now be used to select a specific point source for a contaminant which is limited to the base map resolution a contaminant can be dispersed in ecospace using the base dispersal rate for the first detrital group listed in the model the base dispersal rate is used to set the rate with which organisms in the ecosystem will disperse due to random movements where the default is 300 km year 1 but this can also be applied to a contaminant in addition a more realistic dispersion pattern can be created using the advection map layer by inputting x east west and y north south velocity data at the base map resolution the spatial temporal framework a gis based data exchange framework built on dotspatial steenbeek et al 2013 ames et al 2012 allowed a time series of variable ocean velocity data to be input to the running model month averaged x and y velocities at 7 km grid resolution from a hind cast model of the north east atlantic for the period january 1985 to june 2014 most recent data available at the time of study were sourced from the european commission copernicus marine environment monitoring service http marine copernicus eu data for the model area were extracted and velocities were depth averaged the data were extrapolated over the base map 5 km grid resolution and a time series of map files ascii for x and y velocities on monthly time steps were produced these data files were read into ecospace using the spatial temporal framework steenbeek et al 2013 to create advection fields and the model was run on monthly time steps phytoplankton and zooplankton dispersion as well as contaminant dispersion were linked to model advection 2 4 ecotracer the iteration of ewe used for the sellafield model contained changes to the ecotracer component that will be included in the next release of ewe ewe 6 6 to be released summer 2018 when running ecotracer in ecosim non spatial contaminant modelling it is possible to link contaminant inputs to a data time series to do this for a point source in ecospace a new ecotracer function was added which allows variable contaminant input at specified grid cells limited to the base map resolution for the sellafield model this means that 14c input was in a 25 km2 area where the sellafield pipelines end this input was made as total monthly 14c discharge activity from sellafield in becquerels bq other input parameters in ecotracer include initial contaminant concentration contaminant concentration in immigrating biomass direct uptake physical decay rate proportion of contaminant excreted and metabolic decay rate table 1 initial contaminant concentrations must be set for both the environment and the functional groups sellafield 14c discharges are in addition to an existing background 14c activity from natural production and atmospheric atomic weapons testing the initial contaminant concentration in the sellafield model was set at zero and as the only input was from sellafield 14c discharges any increase in functional group contaminant concentration shows net 14c enrichment in excess of background direct uptake is the rate at which a functional group takes up the contaminant from the environment 14c is discharged to the marine environment as dic and primary producing organisms incorporate dissolved inorganic 14c through fixation of carbon during photosynthesis into soft tissue for phytoplankton direct uptake was calculated as the rate at which phytoplankton photosynthesise as it is assumed that 14c uptake is identical to stable carbon uptake following walters and christensen 2017 this can be expressed as d i r e c t u p t a k e u i b i c 0 where u i is the mass of carbon intake by primary producer i b i is the biomass of primary producer i and c 0 is the dic concentration phytoplankton 14c uptake was calculated using the primary productivity estimate of 97 g m 2 for the coastal irish sea gowen et al 2000 the calculated biomass 13 8 t km 2 and an estimated dic concentration of 30 mg l 1 taken from the upper limit of typical dic concentrations in seawater of 24 30 mg c l 1 chester 1990 primary productivity rates for the other primary producers seaweed and microflora were estimated by back calculating from the model biomass using formula from gowen and bloomfield 1996 productivity rates of 450 g m 2 seaweed and 230 g m 2 microflora were then used to calculate direct uptake for these groups contaminant uptake for consumer groups is a function of the contaminant concentration in their diet their consumption rate and their mortality walters and christensen 2017 there are two excretion parameters for each functional group the proportion of contaminant excreted is the proportion not assimilated into the biomass and so passes straight into the detritus group the default annual unassimilated consumption for consumer groups in the ecopath parameters is 0 2 and this was copied for the proportion of contaminant excreted in the sellafield model as 14c acts as a tracer of stable carbon and therefore energy transfer in an ecosystem the metabolic decay rate is the rate at which assimilated contaminant is released back into the environment see excretion in walters and christensen 2017 the carbon weight 14c activity bq per mass c of marine primary producers is in equilibrium with the dic 14c activity therefore the metabolic decay rate for primary producers can be calculated where the equilibrium ratio of 14c in a primary producer is equal to the environmental concentration for example when the dic 14c activity is at background approximately 249 bq kg 1 c the 14c activity in a primary producer is expected to be the same under these conditions the metabolic decay rate can be expressed as m e t a b o l i c d e c a y r a t e d i c 0 c i b i where d i is the direct uptake rate for the primary producer i c 0 is the 14c concentration in the environment c i is the 14c concentration in primary producer i and b i is the biomass of primary producer i the metabolic decay rate was calculated this way for all three primary producers in the sellafield model for consumer groups the metabolic decay rate was assumed to be equal to the respiration rate biomass which were calculated in the ecopath component during model balancing a contaminant physical decay rate parameter was added to the ecotracer module in the version used here this can be set for both the environment and each functional group for radionuclides this is the physical radioactive decay rate meaning that in ewe biological decay and physical radioactive decay are two separate parameters 14c has a long half life of 5730 years and the impact of radioactive decay on a model running over several decades is negligible and so physical decay rate was set at zero as the ecopath food web model contained no immigration the contaminant concentration in the immigrating biomass could also be set to zero for every functional group muir et al 2017 and tierney et al 2017a describe 14c activities at numerous sites including 4 main stations fig 1 located in the irish sea east basin eb irish sea west basin wb north channel nc and firth of lorn fol model base map grid cells at these site locations or in the case of fol as close as possible were labelled as different model regions in ecospace and data specific to these regions were extracted from ewe model data analyses were conducted and map figures were produced using r r development core team 2016 and the r package ggmap kahle and wickham 2013 used to overlay model predicted data over google satellite maps model predictions are only given where 14c enrichment is at least 1 bq kg 1 c and all observed 14c activities are given as net activities i e background subtracted 3 results and discussion 3 1 14c dispersion although advection was the dominant control on the general direction and extent of model 14c dispersion contaminant base dispersal rate had an observed impact fig 2 as 14c uptake by primary producers is limited to the 14c activity of the environment or dic 14c activity for a given cell the physical dispersion of sellafield 14c is a key mechanism studies examining the dispersion of other highly soluble radionuclides discharged by sellafield e g 134cs 137cs and 99tc have estimated transit times from sellafield to the north channel of between 3 months and 1 8 years jefferies et al 1973 kershaw and baxter 1995 kershaw et al 2004 similarly transit times of between 3 months and 1 year were predicted by the model developed by dabrowski and hartnett 2008 using a base dispersal rate of at least 100 km per month resulted in model 14c reaching the north channel within 1 year fig 2 as the distance between sellafield and the north channel is approximately 110 km it is apparent that model dispersion of 14c is limited by both advection and uptake by primary producers increasing the base dispersal rate does increase maximum dispersal extent though this increase does not appear to be substantial it does reduce the maximum dic 14c activities predicted in pockets where 14c appears to accumulate e g the solway firth this accumulation particularly at lower base dispersal rates causes activities to increase to levels which have not been observed in previous studies for example the model predicted 14c activities above 30kbq kg 1 c between 2001 and 2006 when using a base dispersal rate of 100 km per month however an increase in base dispersal rate to 200 km per month limited maximum model dic activities to less than 20 kbq kg 1 c there are no reported dic 14c activities for the period of peak predicted activity 2001 2006 and therefore no available data for dic activities in areas such as the solway firth where a significant accumulation of 14c was predicted for this period the highest reported net dic 14c activities are approximately 8550 bq kg 1 c in 1995 cook et al 1998 and 4500 bq kg 1 c in 1997 cook et al 2004 at sites relatively close to sellafield using a time series of dic 14c activities for a site in the vicinity of sellafield for the period 1989 to 1999 cook et al 2004 it is shown that the range of activities predicted by the model when using a base dispersal rate of 200 km per month was similar to the observed range through time fig 3 although the specific measured and predicted activities do not generally align it is important to recognise that model 14c activities are predicted per km2 for a 25 km2 cell per month whereas measured dic samples were taken from a specific day and location on the coastline model dispersion of dic 14c could be improved with further measurements at sites such as the solway firth to address uncertainty in dispersion to illustrate model dispersion of dissolved inorganic 14c in the environment a video component video 1 is available and accompanies the electronic version of the manuscript supplementary video related to this article can be found at https doi org 10 1016 j envsoft 2018 01 013 the following is the supplementary data related to this article video 1 video 1 in addition to the solway firth accumulation of 14c occurs in the south east irish sea and around the isle of man circulation models have described a significant seasonal southward flow in the irish sea dabrowski and hartnett 2008 dabrowski et al 2010 creating a backwater in the south east irish sea this could result in the area around liverpool bay being a significant sink for radionuclides released from sellafield dabrowski and hartnett 2008 an increase in the radionuclide inventories of saltmarsh sediments in areas including the solway firth has previously been suggested mackenzie et al 2004 however an accumulation of sellafield radionuclides in the water column of the solway firth and also around the isle of man has not been previously detected and the general northward movement of water continuously flushed the isle of man coastline in circulation models dabrowski and hartnett 2008 although accumulation of 14c in these areas could occur any accumulation in concentration may not occur to the same extent predicted by the ewe model greater retention of 14c at these sites will result in reduced dispersion to more distant areas the fact that modelled 14c dispersion does not significantly penetrate the scottish west coast suggests that model retention in the irish sea is too high previous studies have shown that a significant proportion of sellafield discharges are dispersed around the scottish coastline gulliver et al 2001 tierney et al 2016 2017a the consequence of using depth averaged advection in our study is that the necessary complex hydrodynamics to drive dispersion at this regional scale may not be well addressed in this model at coastal areas such as the solway firth dispersion will be complicated by freshwater input and non uniform current direction at different depths which would reduce the overall retention of dissolved 14c at these sites using depth averaged advection means that dissolved 14c can be trapped and accumulate exponentially at sites if advection is directed towards the coastline although this is significantly reduced by increasing the base dispersal rate dispersion is also limited by data and map resolution the velocity data used had a grid resolution of 7 km so any local physical dynamics were lost the 5 km base map grid resolution meant that many features of the uk coastline were not well defined including the loss of several islands on the scottish west coast that are connected to the mainland in the model 3 2 14c ecological fate muir et al 2017 reported 14c activities for dic and a number of species at sites in the irish sea east basin station eb and west basin station wb in june 2014 model 14c activities at eb in june 2014 were significantly over predicted compared to observed activities when using a low dispersal rate 100 km per month but a higher base dispersal rate 200 km per month brought the predicted and observed activities significantly closer fig 4 trends in the observed data were replicated by the model phytoplankton and zooplankton 14c activities were relatively low compared to benthic species and dab 14c activity was the highest although a large range in observed dab activity 499 763 bq kg 1 c meant that the average dab activity 631 bq kg 1 c was less than the infaunal macrobenthos 704 bq kg 1 c the model did not capture this high infaunal macrobenthos 14c activity relative to most other groups the observed infaunal macrobenthos activity comes from green spoon worm maxmuelleria lankesteri tissue and this species is known to have an important role in the redistribution of other sellafield derived radionuclides in bottom sediments hughes et al 1996 kershaw et al 1983 1984 1999 its inclusion as a separate species in the model was considered however this was deemed to be challenging due to limited ecological data station wb is more complex due to highly variable reported 14c activities between species muir et al 2017 typically both high and low dispersion rates under predicted the observed higher activities in polychaetes epifaunal macrobenthos and dab and over predicted the observed lower activities e g phytoplankton and zooplankton fig 5 however the main observed trends were again predicted as for eb plankton 14c activities were significantly lower than other functional groups and dab activity was again predicted to be the highest the relatively high 14c activity observed in polychaetes 405 bq kg 1 c was due to the higher observed net activity of the predatory species aphrodita aculeate 740 bq kg 1 c whereas the average activity of other polychaete species was lower 69 bq kg 1 c and similar to the model predicted activity of 59 bq kg 1 c the observed epifaunal macrobenthos activity was also relatively high 488 bq kg 1 c and not captured by the model similar to the polychaete group the model functional group epifaunal macrobenthos was made up of numerous species and the observed 14c activity was comprised from an average of starfish species only and may not accurately represent the entire functional group both these cases indicate that model functional groups were not well defined in some instances as the addition of a predatory species to a functional group is not best practice heymans et al 2016 a number of 14c activities across a range of species were reported by tierney et al 2017a for two sites in the west of scotland marine environment the north channel station nc and firth of lorn station fol due to northward dispersion of 14c being constrained in the model as a result of irish sea retention of 14c being too high the model under predicts activities at these sites relative to the observed activities additionally the connection of several islands to the scottish mainland due to the 5 km base map resolution blocked important channels in the west of scotland area including to the south of the firth of lorn preventing direct northward dispersion of 14c to this area and much of the firth of lorn itself the lack of penetrative northward dispersion of 14c resulted in the model showing no 14c enrichment at fol in 2014 although a small enrichment in dic and benthic species was observed tierney et al 2017a the model only predicted a slight enrichment 1 2 bq kg 1 c in dic and some functional groups at fol between 2005 and 2009 at station nc the observed trend of low plankton activities and higher benthic activities was again replicated in june 2014 fig 6 as observed whiting activity was predicted to be higher than other groups and repeated the theme where the group with the highest modelled trophic level also had the highest activity see dab for irish sea sites however the comparatively high activity observed in whiting at the nc station was interpreted as being likely due to northward migration of whiting which had foraged in the irish sea tierney et al 2017a the issues discussed with the model 14c dispersion meant that predicted activities for harbour porpoises did not typically align with the activities reported by tierney et al 2017b it should also be noted that although harbour porpoise is a resident species and observed 14c activities indicate a high feeding fidelity tierney et al 2017b these are animals that can traverse the modelled area and single measurements from a stranded individual is unlikely to represent the average activity across the population in that area predicted trends through time do however appear to replicate the observed trends as illustrated by comparing predicted harbour porpoise 14c activities in four different years 1993 2002 2004 and 2014 with the observed activities for those years fig 7 both predicted and observed 14c activities show very low 14c activities of between 0 and 10 bq kg 1 for west of scotland porpoises north of the north channel in 1993 with activities significantly higher in the south east irish sea peak discharges between 2001 and 2005 increased porpoise 14c activity in the north east irish sea and activities were lower in the north channel and clyde sea clyde sea 14c activities were lower in 2014 but activities in the north channel remained relatively higher and the highest activities were found in the south east irish sea the sellafield model illustrates that ecosystem uptake of 14c for a specific area is controlled by the dic 14c activity in that area and therefore the dispersion of changeable sellafield 14c discharges through time fig 8 phytoplankton and subsequently zooplankton 14c activities closely mirror changes in the dic 14c activity as 14c transfers to higher trophic levels are not immediate there is a delayed response to 14c activities which has a smoothing effect on predicted activities through time modelled 14c activities for stations eb wb and nc in june 2014 show a general trend of increasing activity with increasing trophic level figs 4 6 this is not due to bioaccumulation but rather the lag effect in 14c transfer to higher trophic levels culminating in top predators such as harbour porpoise the very low 14c discharge activity in june 2014 caused dic and plankton activities to drop at station eb whilst other functional group activities remained higher due to uptake of previously higher activities variable dispersion of 14c to station wb resulted in dic and plankton activities decreasing significantly below the 14c activities of other species in june 2014 after a peak in dic activity at station nc in 2007 the activities at higher trophic levels gradually declined but not to below the significantly reduced plankton activities this mechanism which likely caused the higher observed 14c activities in benthic species was suggested by muir et al 2017 and tierney et al 2017a who described an integrated 14c activity in older living organisms occupying higher trophic levels it was also identified through analysis of marine mammal 14c activities alone tierney et al 2017b where mammal 14c activities correlated significantly with total sellafield discharges for 24 months prior to stranding as shown by model results this means that the 14c activity of an organism is not only dependent on the discharge activity and the dispersion of 14c which can be highly variable but is also dependent on the trophic level that the organism feeds at feeding at lower trophic levels will result in a species having a highly variable 14c activity through time species that feed at higher trophic levels will have 14c activities that are not dependent on the immediate environmental activity and could be significantly more or less enriched in 14c relative to the environment they inhabit to illustrate the differences in 14c activities spatially and temporally at different trophic levels a video component video 2 is available and accompanies the electronic version of the manuscript supplementary video related to this article can be found at https doi org 10 1016 j envsoft 2018 01 013 the following is the supplementary data related to this article video 2 video 2 3 3 advantages and limitations as discussed the ewe approach accurately demonstrates a number of the observed trends in 14c activities and reproduces the observed transfer of 14c through the marine food web after initial uptake by primary producers relative to the environmental 14c activity it can therefore provide a tool which is capable of predicting the ecological uptake of radioactive contamination or other environmental contaminants i e trace metals if the environmental concentrations were accurately provided predicted activities for a specific functional group are limited by how well each functional group and their ecology are defined in the model diet is a key factor in an organisms 14c activity and diet description data should be revisited and improved where possible for the sellafield model a major advantage of ewe is that it can predict general trends for contaminant concentrations in non specific functional groups or specific contaminant concentrations in individual species for example if the aim was to determine the transfer of 14c or other radionuclides between different benthic species and the sediment then the functional groups describing these species should be further developed discrepancies between observed and predicted activities for benthic species would be better resolved by incorporation of a well defined microbial loop in the model modelling 14c dispersion within the ewe framework significantly reduces far field dispersion beyond the irish sea in comparison to observed data and appears to result from increased retention of 14c at specific areas within the irish sea as this study aimed to model the general patterns of 14c dispersion the velocity and base map resolutions are appropriate nevertheless using depth averaged advection over simplifies the localised oceanographic conditions in future work this could be overcome by using a 3 dimensional physical transport model to disperse 14c in the environment by using the same approach to which velocity data were input to ecospace in this study employing the spatial temporal framework steenbeek et al 2013 depth averaged 14c activity concentration fields predicted by the physical transport model could be applied instead this study did not consider ecosystem shifts e g changes in species biomass and the knock on effects through time however if a model contamination study for an area covers an extensive period of time then changes in the ecosystem which could affect contaminant concentration in the ecology should also be modelled in ewe the sellafield 14c model and observed 14c activities show that the 14c activity for a functional group species is dependent on the trophic level it feeds upon most ecosystems in general and the irish sea specifically have undergone significant changes over the past century due to changes in the fishing hunting pressures and climate which result in species changing their foraging behaviour and the prey they feed on this would affect the 14c activity of a species and if the contaminant was subject to bioaccumulation this could lead to additional model complexities future work should consider this and for 14c seek to address changes in ecosystem uptake due to seasonal variation in primary productivity 4 conclusions this study modelled the ecosystem uptake and ecological fate of sellafield 14c discharged to the uk marine environment using the ewe software the advantages of the ewe approach were illustrated in capturing observed trends in 14c activities for species at specific locations and through time in addition the model data aids understanding of 14c transfer processes through the food web 14c does not bio accumulate although higher activities have been observed at higher trophic levels the sellafield model illustrates that changes in environmental 14c activities will directly and immediately impact species activity at lower trophic levels whereas higher trophic level species 14c activities are integrated over time therefore species 14c activity will be strongly affected by the trophic level from which it feeds limitations in the model s ability to use advection data to disperse 14c through the marine environment meant that the specific 14c activities predicted for some areas such as the west of scotland did not compare well with observed activities further measurements of dic 14c activities such as the solway firth where the model predicts an accumulation of sellafield 14c would reduce uncertainty in dispersion patterns the effectiveness of ewe for modelling the ecological fate of contaminants in the environment has been underrepresented despite the wide use of the ewe approach to ecosystem modelling recent developments in the software were utilised in this study further refinements such as coupling this approach with better resolved contaminant dispersion could be used to help address the ecological fate of a wide range of contaminants including radionuclides acknowledgements this work was completed as part of the lo rise long lived radionuclides in surface environments ne l000202 1 consortium under the nerc rate programme radioactivity and the environment co funded by the environment agency and radioactive waste management ltd this study has been conducted using e u copernicus marine service information appendix a 1 sellafield model functional groups and balanced input parameters biomass production biomass p b consumption biomass q b ecotrophic efficiency ee production consumption p q and unassimilated consumption values used before balancing are shown in brackets where applicable group name biomass t km2 p b year q b year ee p q unassimilated consumption bottlenose dolphin 0 0016 0 2 8 67 0 2 harbour porpoise 0 0105 0 2 8 67 0 2 minke whale 0 0893 0 02 10 0 2 common seal 0 0005 0 1 14 55 0 2 grey seal 0 004 0 1 14 55 0 2 seabirds 0 0511 1 075 82 664 0 2 large sharks 0 115 0 318 3 18 0 2 small sharks 0 288 0 972 9 72 0 2 basking sharks 0 0014 0 07 3 7 0 2 skates and rays 0 103 1 6 16 0 2 cod 0 6253 1 3891 4 7051 0 2 haddock 0 2711 2 4751 8 5356 0 2 plaice 0 3425 1 3522 5 6234 0 2 whiting 0 55 0 507 0 842 2 97 0 2 sole 0 16 0 863 2 58 0 2 monkfish 0 125 1 246 1 989 0 95 0 2 0 2 dab 0 07 2 394 3 042 0 95 0 2 0 2 other flatfish 0 2404 2 1757 3 8572 0 95 0 2 0 2 dragonets 0 229 1 54 5 154 0 2 mackerel 1 623 0 414 4 4 1 73 0 2 ling 0 076 1 315 3 089 0 2 other demersals 2 4158 1 5384 4 5888 0 2 herring 1 2131 1 154 0 727 6 516 0 2 other small pelagic planktivorous fish 2 4262 0 727 6 516 0 2 sandeels 1 3 1 53 5 016 0 2 epifaunal macrobenthos 13 1 661 0 2 0 2 epifaunal mesobenthos 8 999 8 975 2 062 0 22 0 2 infaunal macrobenthos 8 007 2 695 0 2 0 2 infaunal mesobenthos 24 773 2 552 0 22 0 2 infauna polycheate 22 726 3 683 0 3 0 2 lobster and large crabs 0 11 0 098 0 783 5 22 0 2 nephrops 0 35 0 73 4 867 0 95 0 2 cephalopods 0 25 1 981 15 0 2 prawns and shrimp 4 925 4 847 0 959 6 393 0 2 sessile epifauna 7 5 2 066 0 2 0 2 meiofauna 6 314 18 45 0 3 0 2 zooplankton 48 475 15 2855 0 95 0 3 0 2 seaweed 75 60 microflora 3 92 587 phytoplankton 13 83 70 14 particulate organic matter 50 dissolved organic matter 50 discards 0 309 a 2 sellafield model diet matrix values used pre balancing are shown in parentheses where applicable prey predator bottlenose dolphin harbour porpoise minke whale common seal grey seal seabirds bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 0100 large sharks 0 0 0 0 0 0 small sharks 0 1000 0 0 0 0050 0 0100 0 0010 basking sharks 0 0 0 0 0 0 skates and rays 0 1000 0 0 0 0790 0 0300 0 0040 cod 0 0 0059 0 0 0 0 0410 haddock 0 0593 0 0135 0 0 0 0 0160 plaice 0 0027 0 0 0 0032 0 0 0050 whiting 0 0200 0 2130 0 0 0350 0 0300 0 0240 sole 0 0010 0 0 0 0200 0 0200 0 0070 monkfish 0 0 0 0 0 0 0010 dab 0 0044 0 0066 0 0 0338 0 0182 0 0073 other flatfish 0 0186 0 0304 0 0 1025 0 0292 0 0257 dragonets 0 0 0 0 0371 0 0055 0 0120 mackerel 0 0126 0 0149 0 2500 0 4000 0 0 0950 0 0390 ling 0 1805 0 0 0 1000 0 0900 0 0130 other demersals 0 4615 0 4122 0 0 3873 0 4998 0 1602 herring 0 0089 0 0563 0 2600 0 1300 0 0247 0 0290 0 0020 other planktivorous fish 0 0177 0 1126 0 2800 0 2600 0 0493 0 0579 0 0040 sandeels 0 0 1000 0 0050 0 1038 0 0160 0 1481 epifaunal macrobenthos 0 0 0 0 0 0 0521 epifaunal mesobenthos 0 0 0 0 0 0 0521 infaunal macrobenthos 0 0 0 0 0 0 infaunal mesobenthos 0 0 0 0 0 0 infauna polychaete 0 0 0 0 0 0 1051 lobster and large crabs 0 0 0 0 0 0150 0 0020 nephrops 0 0 0 0 0 0150 0 cephalopods 0 0129 0 0343 0 0900 0 0191 0 0395 0 prawns and shrimp 0 0 0 1000 0 0 0 1682 sessile epifauna 0 0 0 0 0 0 meiofauna 0 0 0 0 0 0 zooplankton 0 0 0 0150 0 0 0 seaweed 0 0 0 0 0 0 microflora 0 0 0 0 0 0 phytoplankton 0 0 0 0 0 0 particulate organic matter 0 0 0 0 0 0 0800 0 dissolved organic matter 0 0 0 0 0 0 discards 0 0 0 0 0 0 0200 0 1000 prey predator large sharks small sharks basking sharks skates and rays cod haddock bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 large sharks 0 0 0 0 0 0 small sharks 0 0 0 0 0060 0 0 basking sharks 0 0 0 0 0 0 skates and rays 0 0 0 0 0030 0 0 cod 0 0 0 0 0 0043 0 haddock 0 0 0 0 0 0108 0 0074 plaice 0 0 0 0 0 0 whiting 0 0 0 0 0080 0 0168 0 sole 0 0 0 0 0 0 monkfish 0 0 0 0 0 0 dab 0 0 0 0 0022 0 0021 0 other flatfish 0 0 0940 0 0 0018 0 0036 0 dragonets 0 0 0 0 0180 0 0057 0 0015 mackerel 0 0 0 0 0060 0 0206 0 0008 ling 0 0 0 0 0 0 other demersals 0 0 2480 0 0 0640 0 0243 0 herring 0 0 0513 0 0 0027 0 0036 0 other planktivorous fish 0 0 1026 0 0 0053 0 0072 0 sandeels 0 0 1340 0 0 0040 0 0240 0 0090 epifaunal macrobenthos 0 6650 0 3140 0 0 2510 0 3780 0 1206 epifaunal mesobenthos 0 0150 0 0 0 0310 0 0449 0 0839 infaunal macrobenthos 0 2000 0 0010 0 0 0 0 0129 infaunal mesobenthos 0 0 0010 0 0 0 0 0129 infauna polychaete 0 0350 0 0 0 0210 0 0672 0 0676 lobster and large crabs 0 0 0400 0 0 0 0 nephrops 0 0 0 0 0010 0 0280 0 cephalopods 0 0850 0 0 0 0070 0 0023 0 0015 prawns and shrimp 0 0 0010 0 0 4530 0 0529 0 0482 sessile epifauna 0 0 0 0 0 0 0008 meiofauna 0 0 0 0 0 0 zooplankton 0 0 0130 1 0000 0 1080 0 2823 0 4837 seaweed 0 0 0 0 0 0 0008 microflora 0 0 0 0 0 0 phytoplankton 0 0 0 0 0 0215 0 1484 particulate organic matter 0 0 0 0 0070 0 0 dissolved organic matter 0 0 0 0 0 0 discards 0 0 0 0 0 0 prey predator plaice whiting sole monkfish dab other flatfish bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 large sharks 0 0 0 0 0 0 small sharks 0 0 0 0 0 0 basking sharks 0 0 0 0 0 0 skates and rays 0 0 0 0 0 0 cod 0 0 0200 0 0 0110 0 0020 0 0411 haddock 0 0 0200 0 0 0055 0 0020 0 0411 plaice 0 0042 0 0 0 1044 0 0 whiting 0 0 0100 0 0 0110 0 0020 0 0002 sole 0 0 0100 0 0 0440 0 0 monkfish 0 0 0500 0 0 0 1000 0 0112 dab 0 0 0219 0 0 0778 0 0015 0 0002 other flatfish 0 0 0581 0 0 1343 0 0025 0 0003 dragonets 0 0056 0 0060 0 0 1055 0 0100 0 1000 0 0100 0 0112 mackerel 0 0 0240 0 0 1363 0 0050 0 0006 ling 0 0 0 0 0 0 other demersals 0 0056 0 1740 0 0 2000 0 2040 0 2199 0 1282 herring 0 0 0480 0 0 0351 0 0127 0 0165 other planktivorous fish 0 0 0959 0 0 0703 0 0253 0 0331 sandeels 0 0112 0 1100 0 0 0110 0 0200 0 1064 epifaunal macrobenthos 0 0562 0 1160 0 1000 0 0110 0 1540 0 1110 0 1381 epifaunal mesobenthos 0 1404 0 0170 0 2500 0 0 0020 0 0095 infaunal macrobenthos 0 1404 0 0 2500 0 0 0 0093 infaunal mesobenthos 0 1050 0 0 1500 0 0 0 infauna polychaete 0 2277 0 0120 0 2500 0 0110 0 2200 0 1140 0 1200 0 1336 lobster and large crabs 0 0 0 0 0077 0 0 nephrops 0 0 0010 0 0 0033 0 0009 0 0001 cephalopods 0 0 0160 0 0 0 0070 0 0380 prawns and shrimp 0 0624 0 0850 0 0 0209 0 1070 0 1235 sessile epifauna 0 0 0 0 0 0004 0 meiofauna 0 0 0 0 0 0 zooplankton 0 1997 0 1050 0 0 0 1220 0 1993 seaweed 0 0 0 0 0 0 microflora 0 0 0 0 0 0 phytoplankton 0 0416 0 0 0 0 0 particulate organic matter 0 0 0 0 0 0 dissolved organic matter 0 0 0 0 0 0 discards 0 0 0 0 0 0 prey predator dragonet mackerel ling other demersals herring other planktiv orous fish bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 large sharks 0 0 0 0 0 0 small sharks 0 0 0 0 0 0 basking sharks 0 0 0 0 0 0 skates and rays 0 0 0010 0 0 0 0 cod 0 0 0 0 0040 0 0 haddock 0 0 0 0 0004 0 0 plaice 0 0 0 0 0 0 whiting 0 0 0 0 0019 0 0 sole 0 0 0 0 0031 0 0 monkfish 0 0 0 0 0 0 dab 0 0 0 0 0027 0 0 other flatfish 0 0 0 0 0019 0 0 dragonets 0 0 0 0 0098 0 0 mackerel 0 0 0007 0 0 0098 0 0 ling 0 0 0 0 0 0 other demersals 0 0 0010 0 0 0325 0 0100 0 0100 herring 0 0 0030 0 0 0095 0 0007 0 0007 other planktivorous fish 0 0 0060 0 0 0190 0 0013 0 0013 sandeels 0 0 0 0 0085 0 0 epifaunal macrobenthos 0 6420 0 0280 0 5000 0 0914 0 0280 0 0280 epifaunal mesobenthos 0 0 0090 0 0 0781 0 0060 0 0060 infaunal macrobenthos 0 0 0 0 0006 0 0 infaunal mesobenthos 0 0 0 0 0003 0 0 infauna polychaete 0 2860 0 0007 0 0 0207 0 0020 0 0020 lobster and large crabs 0 0 0 0 0 0 nephrops 0 0 0 0500 0 0018 0 0 cephalopods 0 0 0010 0 0 0033 0 0005 0 0005 prawns and shrimp 0 0 0 4500 0 1538 0 0060 0 0060 sessile epifauna 0 0 0 0 0 0 meiofauna 0 0 0 0 0 0 zooplankton 0 0720 0 9373 0 0 4772 0 9455 0 9455 seaweed 0 0 0 0 0 0 microflora 0 0 0 0 0 0 phytoplankton 0 0 0003 0 0 0 0 particulate organic matter 0 0 0120 0 0 0697 0 0 dissolved organic matter 0 0 0 0 0 0 discards 0 0 0 0 0 0 prey predator sandeels epifaunal macro benthos epifaunal meso benthos infaunal macro benthos infaunal meso benthos infauna polychaete bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 large sharks 0 0 0 0 0 0 small sharks 0 0 0 0 0 0 basking sharks 0 0 0 0 0 0 skates and rays 0 0 0 0 0 0 cod 0 0 0 0 0 0 haddock 0 0 0 0 0 0 plaice 0 0 0 0 0 0 whiting 0 0 0 0 0 0 sole 0 0 0 0 0 0 monkfish 0 0 0 0 0 0 dab 0 0 0 0 0 0 other flatfish 0 0 0 0 0 0 dragonets 0 0 0 0 0 0 mackerel 0 0 0 0 0 0 ling 0 0 0 0 0 0 other demersals 0 0 0 0 0 0 herring 0 0 0 0 0 0 other planktivorous fish 0 0 0 0 0 0 sandeels 0 0 0 0 0 0 epifaunal macrobenthos 0 0 0270 0 0 0220 0 0 epifaunal mesobenthos 0 0 1040 0 0100 0 0220 0 0059 0 infaunal macrobenthos 0 0 1760 0 0 0120 0 0 infaunal mesobenthos 0 0 1770 0 3200 0 1330 0 0059 0 infauna polychaete 0 0 1780 0 3200 0 1330 0 0554 0 lobster and large crabs 0 0 0 0 0 0 nephrops 0 0 0 0 0 0 cephalopods 0 0 0 0 0 0 prawns and shrimp 0 0 0 0 0 0 sessile epifauna 0 0 0060 0 0 0 0495 0 meiofauna 0 0 0 3000 0 1120 0 1484 0 zooplankton 0 6000 0 0760 0 0 1640 0 0425 0 seaweed 0 0 0390 0 0400 0 0 0 microflora 0 0 0370 0 0100 0 0010 0 1098 0 3300 phytoplankton 0 1000 0 0190 0 0 2710 0 1098 0 particulate organic matter 0 3000 0 1420 0 1210 0 0 0650 0 1650 0 1098 0 3400 dissolved organic matter 0 0 0190 0 0 0650 0 3076 0 3300 discards 0 0 0 0210 0 0 0 0 0554 0 prey predator lobster and large crabs nephrops cephalo pods prawns and shrimp sessile epifauna meio fauna bottlenose dolphin 0 0 0 0 0 0 harbour porpoise 0 0 0 0 0 0 minke whale 0 0 0 0 0 0 common seal 0 0 0 0 0 0 grey seal 0 0 0 0 0 0 seabirds 0 0 0 0 0 0 large sharks 0 0 0 0 0 0 small sharks 0 0 0 0 0 0 basking sharks 0 0 0 0 0 0 skates and rays 0 0 0 0 0 0 cod 0 0 0 0098 0 0 0 haddock 0 0 0 0098 0 0 0 plaice 0 0 0 0098 0 0 0 whiting 0 0 0 0010 0 0 0 sole 0 0 0 0098 0 0 0 monkfish 0 0 0 0 0 0 dab 0 0 0 0072 0 0 0 other flatfish 0 0 0 0026 0 0 0 dragonets 0 0 0 0 0 0 mackerel 0 0 0 0 0 0 ling 0 0 0 0 0 0 other demersals 0 0 0090 0 0029 0 0 0 herring 0 0 0 0003 0 0 0 other planktivorous fish 0 0 0 0007 0 0 0 sandeels 0 0 0 0010 0 0 0 epifaunal macrobenthos 0 0500 0 0700 0 0196 0 0 0 epifaunal mesobenthos 0 0500 0 0700 0 0196 0 0 0 infaunal macrobenthos 0 0500 0 0700 0 0196 0 0 0 infaunal mesobenthos 0 0500 0 0700 0 0196 0 0 0 infauna polychaete 0 0 0500 0 0098 0 0 0 0100 lobster and large crabs 0 0300 0 0 0049 0 0 0 nephrops 0 0 0 0010 0 0 0 cephalopods 0 0 0 0010 0 0 0 prawns and shrimp 0 1700 0 1500 0 0 0098 0 0 0 sessile epifauna 0 0 1610 0 0 0 0 meiofauna 0 0 0 0 0 0 0900 zooplankton 0 0 0 6438 0 0900 0 2970 0 seaweed 0 0 0 0 0 0 microflora 0 0 0 0 0 1430 0 7000 phytoplankton 0 0 0 1963 0 0800 0 1430 0 particulate organic matter 0 6000 0 6200 0 4700 0 0 0 5200 0 1430 0 2000 dissolved organic matter 0 0 0 0 3100 0 2740 0 discards 0 0200 0 0 0300 0 5000 0 0 0 0 
26423,we present two bayesian compressive sensing bcs imputation methods bcs on signal and bcs on imf and compare to temporal and spatio temporal methods we build sparse bcs models using available data then use this sparse model for imputation most bcs applications have the sparse data distributed across the computational space in our adaptation the sparse data are outside the reconstruction space we used 30 years of temperature data and created gaps of 1 110 days 5 1 5 years 10 3 years and 20 6 years performance was not sensitive to gap size with rmse slightly above 6 c for the bcs on signal and temporal models the two best methods the methods which only required data from the target station performed as well as or better than the spatio temporal model which requires data from surrounding stations visually the bcs on imf results seem to better represent longer period random temporal fluctuations while having poorer performance metrics graphical abstract image 1 keywords environmental data imputation bayesian compressive sensing empirical mode decomposition 1 introduction various data compression or reduction methods can accurately represent full data sets using very sparse data representations to our knowledge the ability to regenerate a signal from a sparse data set has not been exploited to impute missing data in the environmental sciences although that is essentially what many of these sparsity based compression reduction methods are doing the main difference between using a sparse representation to reconstruct the original signal and using this approach for data imputation is that data used for reconstruction are distributed throughout the signal while data used for imputation are complete or dense outside gaps and no data within the gaps this paper explores whether these sparsity based approaches can be used for data imputation given this difference we propose two novel imputation methods using sparsity based compression sensing cs approach we find that one method provides predictions as accurate as commonly used imputation methods while the other provides promise in accurately capturing trends and variation in the data this work is of interest and beneficial because sparsity based compressive data reduction methods are fast rely only on the signal of interest do not need data from other locations or sources and can be implemented relatively automatically that is a user does not need to provide additional information about a signal most other commonly used methods for environmental data imputation such as the temporal and spatio temporal models compared in this paper require more application and data specific information for modeling for example we know our temperature data have a strong annual trend and use that feature in both the temporal and spatio temporal models in contrast the cs methods did not require domain specific knowledge to fit and model the data we did not need to explicitly use the annual periodic nature of the data this study provides evidence that these sparsity based methods both the cs methods presented and other compressive techniques can be of value for environmental data imputation environmental sciences use time series data such as streamflow temperature wind speed and solar radiation to describe the environment these data are used to better understand plan manage or control a wide variety of important hydrologic and environmental processes khalil et al 2001 unfortunately while there are numerous environmental data archives nearly all the available data series have gaps or periods of missing data gill et al 2007 di piazza et al 2011 gyau boakye and schultz 1994 sorjamaa et al 2010 mariethoz et al 2015 studies have shown that these missing data can affect data analysis and modeling to effectively use these data series the gaps must be filled using imputation methods i e data estimation gill et al 2007 sorjamaa et al 2010 gilroy 1970 henn et al 2012 raman et al 1995 oriani et al 2016 wang 2008 data imputation in the earth sciences has a significant amount of reported research with methods ranging from simple replacement to using spatially correlated data to various interpolation schemes to complex statistical models simple examples include battaglia and protopapas 2012 and auer et al aueret al 2007 who estimated missing temperature values using nearby stations with a simple offset craigmile and guttorp 2011 who used only data from the target site and for a single missing value averaged observed values before and after and for longer gaps used an average of the values from the previous and following year and benth et al benth et al 2007 and lemos et al lemos et al 2007 who reported similar approaches more complex approaches include spatio temporal models jeffrey et al 2001 pattern matching mariethoz et al 2015 and data modeling romanowicz et al 2006 a large part of the published imputation work addresses streamflow or precipitation data and includes approaches such as spatial correlation with nearby sites gilroy 1970 beard 1962 fiering 1962 moran 1974 giustarini et al 2016 serrano notivoli et al 2017 multivariate statistics kuczera 1987 vogel and stedinger 1985 grygier et al 1989 bayesian modeling wang 2008 and models such as neural networks khalil et al 2001 coulibaly and evora 2007 kim and ahn 2009 chaos theory elshorbagy et al 2002a and a markov chain monte carlo algorithm within a bayesian modeling framework lemos et al 2007 researchers have reported comparisons between different methods examples include beauchamp et al beauchamp et al 1989 who compared a regression approach to a time series approach and found that the time series approach performed better gyau boakye and schultz 1994 who evaluated 10 methods and reported the best method varied depending on location and data raman et al raman et al 1995 who compared regression methods and hirsch 1982 who compared two regression methods and two maintenance of variance extension move methods and found the move approaches better other researchers have extended move techniques to include other variables grygier et al 1989 researchers have reported that nearest neighbor methods were better than arma models for stream flows jayawardena and lai 1994 that ann approaches were better than arma models hsu et al 1995 and that anns performed better than linear regression elshorbagy et al 2000 and nonlinear regression elshorbagy et al 2002b besides stream flow using ann models for data imputation for other spatio temporal data types has been reported diamantopoulou 2010 this is not a comprehensive literature survey as the field is very large with a long tradition but is intended to show the range of work in this area and also that reported best methods are dependent on data specifics with no clear best method this paper presents two data imputation methods using bayesian compressive sensing bcs in the first we apply bcs to the original signal to impute data to fill gaps in the data series in the second we first decompose the signal into a series of imfs using emd then apply bcs to each imf in turn to impute data to fill the gaps then recreate the signal by summing the filled imfs we hypothesized that as imfs are less complex than the parent signal it would be easier to use bcs to model each imf individually then use the imfs to recreate the full signal we present some background on bcs and emd methods along with the formulation of our two models using three different performance metrics and a number of different gap sizes and locations we compare these bcs models to three other approaches simple linear interpolation a temporal model and a spatio temporal model these methods were selected to represent a range of complexity and data requirements for context we present information on the data series the performance metrics and the three imputation models used for comparison we discuss the results and some features of the models and present our conclusions 2 background 2 1 approach cs exploits sparsity and is capable of representing a signal using a sampling frequency significantly less than the nyquist frequency candès and wakin 2008 candès 2006 shannon 1949 bcs casts the resulting optimization problem in a bayesian framework for computational advantages cs and bcs are most often used for data reduction or compression by taking an original signal and discovering a sparse representation then using this sparse representation to recreate the original signal when required we use this process for data imputation by building a sparse cs model with existing data then recreate missing data using the sparse model gemmeke et al 2010 gemmeke and cranen 2008 commonly cs uses the sparse signal model to recreate the original signal between sparse measurement points that are distributed throughout the signal space this allows a minimal number of measurement points to either be taken reducing measurement time or stored reducing data size we adapted this approach for data imputation by retaining all the original data then recreating the data across the gap as noted above the main difference between using a sparse representation to reconstruct the original signal and using a sparse model for data imputation is that data used for reconstruction are distributed throughout the reconstruction space while data used for imputation are complete or dense outside gaps and no data within the reconstruction space or gaps empirical mode decomposition emd huang et al 1998 is a signal deconvolution method that works with non stationary non linear semi periodic data typical of environmental data series emd is data driven and does not assume or require a stationary or linear signal emd decomposes a signal into a series of intrinsic mode functions imfs each representing independent components of the signal and a residual summing the imfs and the residual exactly reproduces the original signal we evaluated these two bcs methods using historic temperature data from the salt lake city airport a long term record to characterize their behavior we created gaps of various lengths in the signal used the different methods to fill the gaps and then compared the imputed data with the original signal we compared the bcs methods with commonly used data imputation methods a temporal spline a spatial temporal spline and linear interpolation the two bcs methods and the temporal spline only require data from the target station the spatio temporal spline requires data from surrounding stations while we used temperature data for this example mostly because of the available long continuous data set for verification this approach could be applied to any semi periodic or cyclic earth science data set such as stream flow 2 2 data we acquired the data from the utah climate data set ucds the ucds contains daily measurements of environmental data at 815 utah locations with some stations starting from december 1 1887 and continuing to the present though many stations start much later than 1887 many are missing large amounts of data and some do not continue to the present for this paper we used daily maximum temperature measurements from the salt lake international airport station observed over a 30 year period between december 1 1887 and december 1 1917 while this is one of the longer and more complete stations with data until 2014 we only used the first 30 years for ease of analysis in this paper y y 1 y t represents the vector of daily temperatures at the salt lake city airport station where t is the total number of days under consideration t 10 950 the spatio temporal model used in the comparison requires data from other stations we selected 20 stations from the ucds that had continuous data during the same 30 year time period blanding deseret escalante fillmore fort duchesne heber laketown levan logan utah state university manti moab morgan power and light oak city parowan richfield radio scipio spanish fork power house saint george tooele and vernal municipal airport this selection covers the state of utah with some stations near the salt lake airport station and others ranging to several hundred miles from the target station 2 3 bayesian compressive sensing cs is widely used candès and wakin 2008 candès 2006 candes and tao 2005 here we briefly describe cs in matrix terms davenport et al 2011 take a target signal f with length t assume f is sparse with respect to some basis ψ 1 f ψ w where ψ is a matrix with dimensions t by k and w is a sparse signal with length k in addition for each individual measurements y of the signal f with assumed measurement noise n we can write 2 y φ f n where ф is the measurement matrix that indicates where measurements exist for f for simplicity if we let φ φ ψ we can write 3 y φ w n we do not know the signal f but only the observed samples y to reconstruct the signal f we solve for w in equation 3 then use equation 1 for the reconstruction this can be thought of as finding a sparse representation of the original signal davenport et al 2011 for our work f is continuous and the observed samples y are not sparse in most of the signal but have missing data or gaps of varying length with no measurements but we follow the same formulation we solve for w in equation 3 using the dense y samples then use this formulation to estimate the signal f across the gaps equation 3 is undetermined meaning there is an infinite number of potential solutions cs imposes the restriction that w is sparse and uses the ℓ 0 and ℓ 1 norms as sparsity measures davenport et al 2011 the solution is found by casting the formulation as an optimization problem that minimizes these sparsity measurements candes and tao 2005 a bayesian framework i e bcs provides a more efficient solution method raman et al 1995 with computational advantages along with error estimates and bounds for the estimated coefficients babacan et al 2010 in bayesian formulation rather than inverting the matrixes we take advantage of the fact that w is sparse with most entries being 0 we solve the problem in a reduced dimension by choosing the bayesian assumption that initially all the coefficients are 0 in one of the solution matrixes babacan et al 2010 these values are updated one at a time in the optimization algorithm eliminating the need to invert the matrixes babacan et al 2010 practically bcs decomposes the original signal using a set of basis functions and selects a subset of these basis functions to represent the signal for this work we used the fast laplace algorithm presented in babacan et al 2010 and modified code developed by matt goodwin goodwin 2017 goodwin and reese 2016 this code decomposes the measured signal into separate components with options for three basis function expansion methods fourier basis wavelets and a b spine basis see babacan et al 2010 goodwin 2017 nelson et al 2013 for details mathematical treatment and practical solution methods in this work we used the fourier basis for expansion 2 4 empirical mode decomposition emd to analyze the underlying processes of a time series or to capture an underlying trend or signal researchers often deconvolve or decompose a signal into separate periodic components using transformations based on methods such as fourier or wavelet analysis pelletier and turcotte 1997 markovic and koch 2005 kantelhardtet al 2003 kantelhardt et al 2002 these methods restrict the resulting periodic components to be regular and stationary making it more difficult to fit natural phenomenon that follow irregular cycles markovic and koch 2005 environmental data can also include trends such as temperature increases resulting from climate change many if not most environmental time series are nonlinear and nonstationary the mean of the time series changes over time i e has a trend and in a nonlinear fashion huang et al 1998 for example air pollution and temperature values rise and fall daily and seasonally but not on even intervals pelletier and turcotte 1997 as discussed emd is a signal analysis method like the fourier transform that decomposes signals into independent periodic components called imfs and a trend the imfs form a complete and nearly orthogonal basis for the signal each imf may or may not represent physical processes though in many cases physical processes can be attributed to the imfs in huang et al 2011 huang demonstrates associating specific imfs resulting from the decomposition of length of day data with el nino events and tidal variations when summed these imfs exactly represent the original data the emd process separates the riding wave of a high frequency component from the lower frequency waves this process is repeated level by level to extract n mono component waves from high to low frequencies with the residual representing the trend this can be summarized as wu and huangsssp 2014 4 x t r e j 1 n a j t e i ω u t d t r n where re represents the real part of a j t and ω j t are instantaneous amplitude and frequency of the jth imf respectively the decomposition process is data driven and does not rely on assumptions of linearity stationarity or any specific statistical distribution for a given series y we use the following process to deconvolve the signal to extract the imfs huang et al 1998 1 let m k t y and let k 1 2 identify the time series extrema m k t to create a vector of points representing the local minima and maxima of the series 3 interpolate between all the minima e 1 k t then all the maxima e 2 k t to create an envelope defined by the two interpolated boundaries as shown in fig 1 4 compute the instantaneous mean m k 1 t e 1 k t e 2 k t 2 of these two envelopes 5 extract the detail d k t m k t m k 1 by subtracting the instantaneous mean from remaining signal this is one iteration of the sifting process repeat steps 2 5 until the mean extracted in step 4 is approximately zero that is below some threshold at this point the remaining signal is the kth imf the kth imf is then subtracted from the original signal in step 1 and the result becomes the new original signal for the next imf extraction process 6 let k k 1 and repeat steps 2 6 until there is only 1 maxima and minima remaining in the time series this is the final imf or residual which represents the overall trend in the data series 7 other than the final imf the other imfs are all centered on zero the final imf or residual clearly defines the trend over time as it relates to the entire signal fig 1 illustrates one iteration of the emd algorithm for more detailed information about emd and its options for example various interpolation methods to create the envelope of the extrema in step 2 we refer the reader to huanget al 1998 huang and wu 2008 emd works best with regularly spaced data as the envelopes are most easily interpolated to a regular spacing it is difficult to apply this process to a signal with gaps as most existing codes assume regularly spaced data and it is not clear how well the method would work on data with gaps therefore before applying the emd algorithm any gaps or missing data points should be filled we explored a number of different methods for filling these gaps prior to applying the emd algorithm and found that simple linear interpolation was sufficient e g a linear interpolation between the two end points of the gap it has the advantage of being quickly and easily implemented and does not add any unobserved or artificial maxima or minima that may influence the emd algorithm for our case more complex methods did not provide any benefit this was partially true because while data in the gap that were initially linearly interpolated these data were replaced with bcs model results while this was sufficient to support our data imputation models it may be insufficient if emd methods are used for other analysis the linear imputation method can be described as follows if the known points on either side of the gap are t 0 y 0 and t 1 y 1 then any missing value y t is calculated by 5 y t y 0 t 1 t 0 y 1 y 0 t 1 t 0 we used this formulation both to initialize the data prior to performing emd and as one of the data imputation methods described below 3 method comparisons 3 1 data series as noted we used the first 10 950 days approximately thirty years of data from the salt lake city international airport station for discuss purposes we assume a year of 365 days ignoring leap years since we only discuss gap lengths not actual dates this assumption does not impact the results the actual dates were from december 1 1887 to november 24 1917 the longest gap we investigated was approximately 6 years we used the salt lake city international airport station data to create nine signals to test and compare the imputation methods the first four signals had 1 110 days 5 548 days or 1 5 years 10 1096 days or 3 years or 20 2190 days or 6 years of missing data percentages were used for computational ease the gaps were created in the center of the signal at day 5475 november 28 1902 the gaps were october 4 1902 to january 22 1903 1 february 27 1902 to august 29 1903 5 may 29 1901 to may 29 10 1904 and november 28 1899 to november 27 1905 20 all plots are presented as days from december 1 1887 as actual dates are not important fig 2 shows the original data in the top panel with each of these four gaps in the subsequent panels the data we are attempting to reconstruct from these gaps have an interesting feature the winter of the year at the center of the gap near day 5 500 is clearly colder than any other years in the period and the year preceding the center of the gap at approximately day 5 100 was also quite cold and was cold over a longer period than most of the other years in the sequence this means the data we are attempting to impute in these four gaps are somewhat anomalous in addition to these four signals with gaps of varying length we created five additional signals all with 10 1096 or 3 year gaps the gap locations were randomly selected with gaps beginning august 6 1890 september 1 1897 july 13 1901 february 14 1909 and november 21 1914 for the five signals respectively the last gap from november 21 1914 to november 22 1917 caused some problems with the emd based method as the gap ends just 2 days before the data end on november 24 1917 the location of these gaps are shown as bars in the 10 gap panel in fig 2 we imputed data for these five gaps using all five methods to provide an indication of the general performance of the algorithms 3 2 statistical comparison measures we used three different performance measures to compare goodness of fit of the imputed data to the original we only calculated statistics for the portion of the signal where data was missing rather than the full 30 year signal so that the results would not be weighted by the gap size that is we calculated the goodness of fit only for the imputed data the three measures of fit were root mean squared error rmse spectral angle sa and lagged correlation lc we used three different measures because research has shown the rmse the most common measure reported in the literature may not always be appropriate for periodic or semi periodic signals wang and bovik 2009 these three metrics highlight different aspects of the fit between the imputed and measured data rmse measures how well both the shape lag and amplitude correlate it is sensitive to phase and amplitude shifts and less sensitive to smaller data values and more sensitive to larger data values sa measures how well the shape of the data correlate and is less sensitive to small shifts in phase or amplitude and more equally weights small and large values lc measures if there is a phase shift and also provides a metric into how well the signals are correlated at that optimal phase shift if you have an lc of 1 at some lag this means that both the shape and amplitude are perfectly matched but phase shifted the highest lc value shows both the apparent lag and also the how well the shape and amplitude correlate at that lag the other two metrics rmse and sa only measure the fit at lag 0 3 2 1 root mean squared error root mean squared error rmse measures the difference between the estimator and what is being estimated using 6 r m s e t 1 t y t y ˆ t 2 t where y ˆ t are the imputed data y t are the original data and t is the number of data points for the rmse a smaller number indicates a better fit to the original data the value of the rmse has units of degrees 3 2 2 spectral angle spectral angle sa is a measurement often used to determine if two time series are similar or not with a spectral angle of 0 meaning the two series are similar sa treats each time series as a multi dimensional vector and calculates the vector angle between them in multi dimensional space in general this method looks at the shape of a time series rather than the magnitude it is most commonly used to determine how well different spectral measurements match it is calculated by 7 s a a r c c o s t 1 t y t y ˆ t t 1 t y t 2 t 1 t y ˆ t 2 where y t ˆ are the imputed data y t are the original data and t is the number of data points for the sa a lower number indicates a better fit with values ranging from 0 to π 3 2 3 lagged correlation lagged correlation lc measures the correlation between the signals when they are shifted against one another the correlation at each of these shifts is calculated and the highest correlation at any lag is reported if the signals are perfectly correlated in time at some lag value then the lc value is 1 this measure can help determine if an imputed time series is correct in shape and amplitude but offset in time lagged correlation is calculated by 8 l c m a x r c t 1 t y t y y ˆ t c y t 1 t y t y 2 t 1 t y ˆ t c y 2 0 2 t c like above y t ˆ is the filled signal y t is the original signal y is the signal average and t is the number of data points we chose to evaluate shifts up to 20 of the data in either direction as indicated by the term 0 2 t c in equation 8 the range of the lc value is bounded 1 1 with 1 meaning the signals are perfectly correlated at some lag 4 imputation methods to demonstrate the two bcs methods we developed we compared them with three commonly used methods from the literature the comparison methods in order of complexity are linear interpolation a temporal model and a spatio temporal model we call the first bcs method bcs on signal this method applies bcs algorithm to the original signal to develop a model and then uses that model to impute data across the gap we call the second method bcs on emd this method first decomposes the original signal into a set of imfs develops a bcs model for each imf imputes the data for the gaps in each imf and then reconstructs the target signal from the imputed imfs 4 1 bcs on signal bcs on signal applies the bcs algorithm to the original signal to create a model of the signal it then uses this model to impute data across the gaps traditionally bcs is used to create sparse representations of signals candès and wakin 2008 and generally a bcs model uses a sparse set of data points along with the fitted model to reconstruct the target signal at a higher resolution for the bcs on signal method we relied on the ability of bcs to represent signals with limited data for our data imputation approach we fit the bcs model using all the measured data points in a given signal that is the entire signal but with no data in the gaps we then used these same data points as the sparse measurements and used the fitted bcs model to reconstruct the data in the gaps the reconstructed signal includes both the measured data points and the imputed data points at unmeasured times this is a somewhat unusual use of bcs as the reconstructed portion of the signal has no supporting data points through the gap region all the support for the model is outside the gap that is being filled 4 2 bcs on emd bcs on emd first decomposes the original signal into a set of imfs then follows the same process described above to impute data across the gap locations as noted the emd algorithm requires a signal with no gaps so the gaps were filled using linear interpolation prior to running emd after the imfs were computed the data in the gap regions were removed a bcs model was then fitted on the remaining data in the same manner as the bcs on signal model and used to impute the data in the gaps once all the imfs gaps were filled the full signal reconstructed by summing the individual imfs as discussed above emd breaks down a signal into a set of nearly orthogonal imfs that represent different frequency components of the signal the first imf generally represents the noise component in the signal with subsequent imfs representing processes or portions of the signal with longer periods or lower frequencies wu and huangsssp 2014 we sought to exploit this feature we hypothesized that as each imf was simpler than the original signal and we could model these individual imfs using bcs more accurately than we could model the complex full signal we thought we could then use the individual imf modes to impute data in the gaps and then reassemble the missing signal from the reconstituted imfs fig 3 shows the imfs that result from application of the emd algorithm to the salt lake international airport station data note that each panel has a different scale each imf other than the trend or final imf is centered on 0 the y axis for the top five plots ranges from 8 to 8 c the y axis on imf 6 and imf 7 ranges from 20 to 20 c while the y axis for imf 8 imf 9 imf 10 and imf 11 range from 8 to 8 c 2 to 2 c 1 to 1 c and 0 8 to 0 8 c respectively imf 1 through imf 11 are centered on zero imf 12 is not and represents the offset in the temperature data the bottom panel of fig 3 is the original signal and ranges from about 15 6 41 7 c with a mean of 17 7 c and a standard deviation of 11 8 c the mean median and mode of 17 7 17 2 and 16 7 respectively are relatively close and the skew and kurtosis values are relatively small 1 1 and 0 5 respectively indicating the data are approximately normally distributed as noted below the emd algorithm finds a small trend in the data that is reflected in the skew value emd is an empirical method only based on the data and while the resulting imfs are nearly orthogonal they may or may not represent actual periodic environmental processes emd can also split or deconvolve the signal from a single physical process such as annual variation into two or more imfs this can happen if for example there are a period of years that have a late spring and early fall as opposed to a series of years that have an early spring and late fall emd can at times separate these into different imfs the user needs to understand the data set and potential physical processes that may be represented in the data to assign meanings to individual imfs for this data set we interpret the first 5 imfs imf 1 through imf 5 as representing noise each of these imfs have a range about 16 c from about 8 to 8 c though there is some variation analysis shows some apparent periodic signal with a period of about 30 days mostly in imf 4 and imf 5 but this is not strong and most likely is an artifact of the data these five imfs could be summed to provide a good approximately of the noise in the signal all have a standard deviation of about 2 c with low kurtosis and skewness numbers indicating an approximate normal distribution we interpret imf 6 through imf 8 as representing the annual periodicity in the data these imfs have a larger range 39 6 36 2 and 12 1 c for imf 6 imf 7 and imf 8 respectively for our data the emd process appears to have separated the annual signal into three imfs visually it is clear the imf 6 and imf 7 represent the majority of the annual periodicity a close inspection not presented here shows that the algorithm separated the annual signal based on the length of the winter period with the colder years in imf 6 and warmer in imf 7 imf 8 has a significantly smaller range 12 1 c and when added the other two annual imfs affects mostly either the fall or spring seasons summing imf 6 through imf 8 provides a good approximation of the annual periodicity in the data imf 9 has a range of only 3 1 c and shows a process with a period that varies between 4 and 6 years we interpret imf 9 as the signal from the el niño climate pattern which has an approximately five year period the smaller range indicates that the effect is not large at the salt lake airport imf 10 also has a small range of 1 8 c and an approximate 11 year period this is most likely the effect of sunspot variability on the temperature sunspot activity has an approximate 11 year period and effects climate and weather patterns there have been several papers that use the emd algorithm to extract periodic behavior in environmental data related to sunspot data for example barnhart and eichinger 2011a 2011b zolotova and ponyavin 2007 chorley et al 2010 the last two imfs imf 11 and imf 12 in our interpretation represent the long term trends in the data both have smaller ranges 1 1 and 0 6 c for imf 11 and imf 12 respectively imf 12 is the trend in the data set and ranges from 17 8 to 18 4 c representing the offset from zero in the temperature series we interpret imf 11 as being a part of the trend but adding some detail it could also represent some longer period process that is not easily extracted from the short 30 year data period these two imfs indicate an approximate 1 c warming trend over the 30 year period but this is not constant imf 12 shows the overall trend more or less linearly increasing over the full period while imf 11 indicates an initial cooling period following by almost a full 1 c of warming peaking at about day 4 500 then cooling and re warming at the end of the period fig 3 shows how the individual imfs are simpler than the original signal and we expected that bcs would be able to better model these signals however as noted this was not the case and potential causes are discussed below 4 3 linear interpolation for data imputation using linear interpolation we used the same method as described for preliminary imputation for emd processing equation 5 we select the end points of the data gap and perform a linear interpolation across the gap while this approach may provide useful data for short gaps it is generally not applicable to longer gaps we present it here as the simplest baseline comparison over short gaps such as a few days linear interpolation may be sufficient but for any type of periodic or nonlinear signal this approach quickly breaks down once the gap is some significant portion of the signal period 4 4 temporal model environmental data often exhibit strong temporal dependence and trends for example the plots of the temperature data used in this paper clearly show the annual cycle e g fig 2 other periodic temporal patterns can also exist in these data such as el niño as discussed we exploit the strong annual trend for the temporal model the temporal model imputes the missing data by modeling the annual seasonal cycle using b splines wahba 1990 the model was fit to the data outside the gaps and represents the estimated average annual cycle we then use the fitted model of the annual cycle to impute data across the gap since this model is based on the average annual trend it results in an overly smooth signal to add the expected variance to the imputed data we added white noise with the same variance as the original signal to the average annual cycle the imputed data are calculated by fitting the model 9 y xβ ε where y is a vector of length t of temperature values though only the measured time points are used for fitting the model x is an t by p matrix of basis functions where the b spline basis function is evaluated for each day at its day of year in our case p was equal to 11 where the first column of x is a vector of ones and the next ten columns are the b spline bases evaluated at 10 equally spaced knots between 1 and 366 the number of possible days in a year we used the r package splines to compute the b spline basis team 2014 β is a p by 1 vector of coefficients where the first element is the y intercept and the remaining elements are the b spline coefficients therefore xβ represents the mean intercept and the annual trend ε is a vector of error terms such that ε n 0 σ i 2 where σ i 2 is equal to the residual variance of the original signal we fit the model by estimating β and σ i 2 using least squares estimation and the observed data then computed the imputed data using x and the estimated value of β and adding zero mean normally distributed noise with variance equal to the estimated value of σ i 2 the imputed data are a noise added version of the estimated annual period 4 5 spatio temporal the spatio temporal model is much like the temporal model except that data from 12 other stations are included in the analysis this was meant to improve on the temporal model since in addition to temporal dependence temperatures also exhibit spatial dependence i e temperatures from one location tend to be similar to temperatures in a nearby location this allows us to estimate any deviations from the 30 average that might occur during the gap this is done by determining how the other potentially spatially correlated stations differed from the average during the same period this is different from the other models presented in this paper in that data from other stations during the time period that includes the gap are required for imputation spatio temporal models are commonly used to model environmental data and there are many ways to specify these models all with varying degrees of complexity and computational burden see for example banerjee et al 2014 cressie and wikle 2015 in our analysis since we were using this model for comparison we chose to specify the model in a way that was easy and quick to implement our spatio temporal model estimates a smooth annual trend and a smooth spatial trend and does not capture the slight deviations from the trend that the actual data exhibit to determine the weight the other stations should receive we first removed the annual trend from each station so that the correlation with the annual trend across all the stations does not eclipse the correlation of the other trends present we then computed empirical orthogonal functions eof s and used them as additional covariates to the model presented in equation 9 brynjarsdóttir and berliner 2014 in this case the eof s are the first three eigenvectors of the empirical covariance matrix among the measured values minus the annual trend of the 13 stations as with the temporal spline we added white noise with the same variance as the original data to account for over smoothing effects the imputed data are calculated by fitting the model 10 y i x i β x i e i β e ε i where y i is a vector y i 1 y i t of temperatures at station i x i is a t by p matrix of b spline basis where just as in the temporal model the basis function is evaluated at the day of year for each time β x i is the p by 1 vector of coefficients e i is the portion of the eof corresponding to the ith station and β e the corresponding coefficient vector ε i is a vector of error terms such that ε i n 0 σ i 2 where σ i 2 is equal to the residual variance of the original signal we fit the model by estimating β x i β e and σ i 2 using least squares estimation and the data from all stations as with the temporal model the imputed data use the estimated coefficients β x i and β e to impute the annual and spatial trends then zero mean normally distributed noise using the estimated value of σ i 2 is added to account for over smoothing 5 discussion and results we present two bcs based data imputation models both of which only require data from the target signal i e signal with the gap the first model bcs on signal uses fourier transforms as the basis functions to create a sparse model of the full signal which then is used to impute the missing data the second model bcs on imf first uses emd to deconvolve the signal into independent imfs then applies the same bayesian compressive sensing algorithms with fourier basis functions to create models of the individual imfs uses these models to impute data in the gaps then reassembles the full signal by summing the reconstructed imfs our objective is to determine the effectiveness of sparse data models using bcs methods for environmental data imputation our hypothesis was that the bcs on imf method would be more accurate because it is built on simpler imfs which would be easier to model than the complex full signal while the results did not support this hypothesis the bcs on signal imputation method performed very well below we discuss this performance as well as potential features that affected this performance we included three other common data imputation methods for comparison linear interpolation a temporal model which only requires data from the target signal and a spatio temporal model which requires data from nearby spatially correlated stations we viewed the temporal model as a direct competitor to the bcs approaches as it only uses data from the target station while the spatio temporal model provides a standard for comparison of more accurate methods we expected it to outperform the other approaches as it exploits additional information at the time of the gap found in data from surrounding stations the imputation methods can be divided into two categories based on the data required the first category only use data from the target signal and includes the bcs on signal bcs on imf linear interpolation and the temporal model the second category requires data from surrounding stations or locations and includes the spatio temporal model these requirements have implications for both application and accuracy methods that only require data from the target signal are easier to use and can be applied more widely than methods that require data from both the target station and surrounding stations where the surrounding stations have data during the period where data are missing from the target signal methods that use data from surrounding stations should be able to better exploit temporal patterns that are not periodic in the data as spatially correlated stations should exhibit similar response to anomalous weather patterns for example if the gap were an anomalously cold year as was the year used for gaps of varying length in this paper we would expect surrounding stations to experience the same cold year and using data from these stations could result in more accurate imputation we would not expect a method that only uses data from the target station to be able to reconstruct this anomaly as there would be no way to infer the anomaly from the available data however in some cases apparent anomalies may be the result of several long term trends that combine either constructively or deconstructively creating an apparent anomalous pattern that can be predicted using only available data from the target station thus while a cold year might appear to be anomalous it may be a result of the superposition of several predictable processes as noted above the year at the center of the data set and the preceding year appear anomalous as they have colder temperatures than the other 28 years of data the year at the center of the gap about day 5 500 is clearly colder than any other years in the series while the preceding year is colder over a longer period than most other years in the series based on a visual examination the four gaps of different lengths are centered on this anomaly these types of anomalies are very difficult to reconstruct only using only data from the target signal as there are no patterns or information that seems to predict this behavior the spatio temporal model however can use information from other weather stations which should also experience similarly cold anomalous conditions this is the basic assumption used in the spatio temporal model that there are spatial correlations in the data for example if the weather at the salt lake airport was anomalously cold in the center year then other stations in utah would also experience cold weather that year they would have similar patterns that could be used to better impute missing data during this time table 1 summarizes the results from comparing the five different imputation models on each of the four different gap lengths 1 5 10 and 20 and includes each of the five models linear bcs on signal bcs on imf temporal and spatio temporal the table presents three different goodness of fit metrics for each of these combinations rmse sa and lc the best metric for each gap length is in bold text in all cases but one the rmse value shows that the linear interpolation model is the worst fit the exception is for the 5 gap where the bcs on imf has a value similar in magnitude but slightly larger 13 70 versus 19 08 for the linear interpolation and bcs on imf respectively the 5 gap occurs over the large cold spike around day 5 500 the bcs on imf model does a good job of matching the cold spike but is too low in the pre and post time periods the linear model almost provides an average for this period as the gap is centered over the cold spike data not shown which results in a surprisingly good rmse value but this is due to the unique circumstances and the gap centered over the temperature minimum as noted with this exception the linear model is not competitive and will not be discussed further depending on the metric used and the gap size either the bcs on signal the temporal or the spatio temporal model exhibit the best fit to the data in fact when one of these methods is better one of the others exhibits similar performance metrics there are no cases where one model if significantly superior to another table 2 using the average of only 5 runs shows that the spatio temporal model exhibits the best fit for any of the three metrics the spatio temporal has a significantly smaller range for the sa metric and relatively small range for the rmse metric compared to the other methods fig 4 shows the imputed values from each of the five models against the centered 10 3 year gap for a visual comparison the top panel is the bcs on signal results which follow the annual trend very well and also captures some variability that matches the variability of the seasons but is consistently smoother than the original data the second panel is the bcs on imf results which under predict the summer fall seasons from about 5100 days to 5400 days but matches the cold winter period and the remainder of the gap well the annual variability in these imputed data visually appears closer to that exhibited by the original signal with some regions having larger variations and other smaller the third panel is the temporal model results which mostly matches both the trends and the apparent variability throughout the imputed data however it does not match the very cold winter at about day 5500 or the cold period of the prior winter season days about 5100 to 5300 this is expected as these data are outside the range of the data available outside the gap that were used to develop the model panel four is the spatio temporal model results which visually match both the trends and the variability but appear to have an offset to the future of about 100 days similar to the temporal model it does not predict the very cold winter centered at day 5500 or the previous cold winter starting at about day 5100 we expected a better match as other utah weather stations recorded this cold period the bottom panel shows the linear model across the 10 3 year gap the linear model is obviously a poor choice for gaps longer than about 100 days 1 4 of a year a short 100 day segment can be approximated by a linear trend with added noise but over any longer periods a linear trend is a poor approximation the two bcs models which are the subject of this paper do a relatively good job as shown in top panel of fig 4 the bcs on signal model results follow the general periodicity of the data very well but a visual examination shows that the data do not exhibit the short term variability present in the original data this is highlighted in fig 5 which presents similar results for all 4 gap lengths the data visually match the original well but with less apparent variability conversely the variability of the bcs on imf model results is larger than the original data this can be attributed to the fact that the first 3 or 4 imfs capture the variability of the original signal when these imfs are modeled across the gap at certain points this variability is additive and at certain points subtractive either amplifying or damping the imputed data relative to the original signal producing spikes and smooth areas depending on the aliasing of the underlying imfs in general the bcs on imf follows the general processes well but over different periods can either under or over predict the data for example an imf representing approximate 7 year periods perhaps driven by the el nino cycle may not exactly match the amplitude of this period process over the gap causing a period of over or under prediction interestingly the bcs on imf model better matches the anomalously cold temperature at about day 5500 and the longer winter from day about 5100 to 5200 than the other models however it shows a much colder summer and significantly colder fall from about day 5300 to day 5500 performing worse than the other models in this range the better match could be the result of longer term processes such as el nino combining constructively to produce the cold winter which could be predicted by the model or could be random table 2 highlights one issue with the bcs on imf method the emd algorithm exhibits significant end point effects that is imf decomposition near the end of a signal is affected by the boundary or end of the data as noted one of the five random gap locations ended within two days of the end of the data this caused significant anomalies in the imfs produced for this signal the large maximum values for rmse and spectral angle measures and the very small minimum lag correlation values are from the results of imputing at this gap knowing this behavior it would be appropriate to restrict this method to signals where the gaps have a significant amount of data relative to the gap size on either side of the gap if measures from this gap near the end of the data were removed from the average then the results of the bcs on imf would be similar too though still not as good as the bcs on signal values another factor that contributed to the poorer performance of the bcs on imf method compared to the bcs on signal method was that the emd algorithm separated some of the periodic components into several separate imfs as discussed above and shown in fig 3 the emd algorithm separated the annual variation signal into three separate imfs imf 6 through imf 8 and the noise into the first 5 imfs these are difficult to model as they do not present a consistent pattern for example imf 6 see fig 3 has in initial period up to about day 4 000 of very low variation that almost appears as noise then a period that represents annual variation from about day 4000 to day 5 200 then a flat period again this type of behavior is difficult to model and predict we expect that if we had visually inspected the imfs and summed those that represented different processes for example sum imf 1 through imf 5 to represent noise sum imf 6 through imf 8 to represent the annual cycle left imf 9 and imf 10 separate to represent individual physical processes and sum imf 11 and imf 12 to represent the long term trend then the bcs on imf method may have performed better as each imf would have had a more consistent pattern that could be better modeled fig 5 shows the bcs on signal results on all four gaps these panels show that bcs on signal visually does a good job filling the four different gap lengths the top two panels 1 and 5 gaps show that the coldest winter days at approximately day 5500 do not exactly line up with the center of the gap with the imputed coldest period slightly offset to the left the bottom panel shows that bcs on signal visually does of good job of data imputation even for the 20 6 year gap using only data from the target station a meaningful result the two bcs based models both ensure continuity at the end points of the gap there are no large offsets this is not true of the temporal or spatio temporal models which can have discontinuities or offsets at the beginning or end of the gaps for example the spatio temporal model results in the fourth panel from the top of fig 4 clearly shows a significant temperature offset at the beginning of the imputed data the temporal model and the spatio temporal model both perform well contrary to our expectations the temporal model often outperforms the spatio temporal model table 1 including the average performance of the five runs on the 10 gap table 2 visually the spatio temporal model appears to perform as well as or better than the temporal model but with an offset in time which resulted in poor goodness of fit metrics this may be due to the fact that the stations the algorithm selected as most correlated with the salt lake airport station and used to impute the data in the gap have similar trends are correlated but are offset in time because of differences in altitude or other local weather patterns this offset seems to be consistent as the average results from the five different gap locations are similar to the results from the centered gap this indicates this offset is not just the result of a specific time period this result may not hold for other stations practically the top three methods bcs on signal temporal and spatio temporal perform very similarly to each other with different algorithms winning depending on the specific gap or performance metric used the bcs on signal and the temporal model metrics are very similar to each other based on our results we cannot distinguish one of these two models from the other in terms of performance both these models identify temporal patterns in the data and model those patterns across the gaps to impute the missing data it is reasonable that these methods have similar performance an interesting result from this study is that the performance measures of the four methods not including linear interpolation do not significantly change with the size of the gap for example the rmse for bcs on signal ranged from 6 28 to 6 82 over the four gaps which ranged from about 30 days to about 6 years in length with the worst fit on the shortest gap the other models were similar with rmse ranges of 6 26 7 02 and 7 33 to 9 43 on the different gap lengths for the temporal and spatio temporal models respectively the bcs on imf rmse metrics ranged from 9 85 to 11 06 with the 5 gap exhibiting an outlier of 19 08 the other two performance metrics spectral angle and lag correlation exhibited similar small ranges across the various gap lengths for each model 6 summary we presented two bcs based data imputation methods which only require data from the target signal there was no clear best method and the results varied depending on the goodness of fit measurement used and which gap size was compared though either the temporal or the bcs on signal models were usually best fit and had performance metrics very close to each other we expected the bcs on imf method to outperform the bcs on signal method our hypothesis was that the individual imfs would be simpler to model than the complex complete signal however the bcs on signal approach performed better in practice the emd algorithm appears to over decompose the signal this results in individual imfs that are difficult to model as the imf may look very different at different times as shown by the three imfs that represent the annual cycle imfs 6 7 and 8 shown in fig 3 there may be other issues contributing to the lower performance the bcs method relies on decomposing the original signal into a set of components using different basis functions then finding a sparse representation using these basis functions if we first apply emd before applying bcs the resulting imfs may not be well suited for decomposition since they should already be independent and not amenable to decomposition also the average of the bcs on imf results included an outlier caused by a random gap very near the end of the data an issue with emd if this run were removed from the average the average bcs on imf results would be better though still not as good as the bcs on signal values but showing the two methods as comparable to each other in performance the statistical performance measures were not sensitive to the gap size the methods did as well imputing data for the large 20 6 year gap as they did for the small 1 110 day gap the methods based only on data from the target signal had performance metrics in the same range and often better than the method that used data from surrounding area a result we did not expect as a final observation the bcs on signal method outperforms the bcs on imf method based on all the metrics used in this study however the imputed data from the bcs on signal model do not visually exhibit the longer term variability observed in the original data this is also true of both the temporal and spatio temporal models which are visually smoother than the original data even though each has a noise term in the model based on the noise in the original data visually the bcs on imf results better represent the larger random fluctuations seen in the actual data but do not more accurately recreate the missing data based on our performance metrics while the bcs on imf method has poorer performance metrics if the results of the data imputation exercise are to be used in a model than the more realistic variability of the bcs on imf method may be more applicable funding this work was supported by the national nuclear security administration department of nuclear nonproliferation research and development grant number de na0002491 appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 01 012 
26423,we present two bayesian compressive sensing bcs imputation methods bcs on signal and bcs on imf and compare to temporal and spatio temporal methods we build sparse bcs models using available data then use this sparse model for imputation most bcs applications have the sparse data distributed across the computational space in our adaptation the sparse data are outside the reconstruction space we used 30 years of temperature data and created gaps of 1 110 days 5 1 5 years 10 3 years and 20 6 years performance was not sensitive to gap size with rmse slightly above 6 c for the bcs on signal and temporal models the two best methods the methods which only required data from the target station performed as well as or better than the spatio temporal model which requires data from surrounding stations visually the bcs on imf results seem to better represent longer period random temporal fluctuations while having poorer performance metrics graphical abstract image 1 keywords environmental data imputation bayesian compressive sensing empirical mode decomposition 1 introduction various data compression or reduction methods can accurately represent full data sets using very sparse data representations to our knowledge the ability to regenerate a signal from a sparse data set has not been exploited to impute missing data in the environmental sciences although that is essentially what many of these sparsity based compression reduction methods are doing the main difference between using a sparse representation to reconstruct the original signal and using this approach for data imputation is that data used for reconstruction are distributed throughout the signal while data used for imputation are complete or dense outside gaps and no data within the gaps this paper explores whether these sparsity based approaches can be used for data imputation given this difference we propose two novel imputation methods using sparsity based compression sensing cs approach we find that one method provides predictions as accurate as commonly used imputation methods while the other provides promise in accurately capturing trends and variation in the data this work is of interest and beneficial because sparsity based compressive data reduction methods are fast rely only on the signal of interest do not need data from other locations or sources and can be implemented relatively automatically that is a user does not need to provide additional information about a signal most other commonly used methods for environmental data imputation such as the temporal and spatio temporal models compared in this paper require more application and data specific information for modeling for example we know our temperature data have a strong annual trend and use that feature in both the temporal and spatio temporal models in contrast the cs methods did not require domain specific knowledge to fit and model the data we did not need to explicitly use the annual periodic nature of the data this study provides evidence that these sparsity based methods both the cs methods presented and other compressive techniques can be of value for environmental data imputation environmental sciences use time series data such as streamflow temperature wind speed and solar radiation to describe the environment these data are used to better understand plan manage or control a wide variety of important hydrologic and environmental processes khalil et al 2001 unfortunately while there are numerous environmental data archives nearly all the available data series have gaps or periods of missing data gill et al 2007 di piazza et al 2011 gyau boakye and schultz 1994 sorjamaa et al 2010 mariethoz et al 2015 studies have shown that these missing data can affect data analysis and modeling to effectively use these data series the gaps must be filled using imputation methods i e data estimation gill et al 2007 sorjamaa et al 2010 gilroy 1970 henn et al 2012 raman et al 1995 oriani et al 2016 wang 2008 data imputation in the earth sciences has a significant amount of reported research with methods ranging from simple replacement to using spatially correlated data to various interpolation schemes to complex statistical models simple examples include battaglia and protopapas 2012 and auer et al aueret al 2007 who estimated missing temperature values using nearby stations with a simple offset craigmile and guttorp 2011 who used only data from the target site and for a single missing value averaged observed values before and after and for longer gaps used an average of the values from the previous and following year and benth et al benth et al 2007 and lemos et al lemos et al 2007 who reported similar approaches more complex approaches include spatio temporal models jeffrey et al 2001 pattern matching mariethoz et al 2015 and data modeling romanowicz et al 2006 a large part of the published imputation work addresses streamflow or precipitation data and includes approaches such as spatial correlation with nearby sites gilroy 1970 beard 1962 fiering 1962 moran 1974 giustarini et al 2016 serrano notivoli et al 2017 multivariate statistics kuczera 1987 vogel and stedinger 1985 grygier et al 1989 bayesian modeling wang 2008 and models such as neural networks khalil et al 2001 coulibaly and evora 2007 kim and ahn 2009 chaos theory elshorbagy et al 2002a and a markov chain monte carlo algorithm within a bayesian modeling framework lemos et al 2007 researchers have reported comparisons between different methods examples include beauchamp et al beauchamp et al 1989 who compared a regression approach to a time series approach and found that the time series approach performed better gyau boakye and schultz 1994 who evaluated 10 methods and reported the best method varied depending on location and data raman et al raman et al 1995 who compared regression methods and hirsch 1982 who compared two regression methods and two maintenance of variance extension move methods and found the move approaches better other researchers have extended move techniques to include other variables grygier et al 1989 researchers have reported that nearest neighbor methods were better than arma models for stream flows jayawardena and lai 1994 that ann approaches were better than arma models hsu et al 1995 and that anns performed better than linear regression elshorbagy et al 2000 and nonlinear regression elshorbagy et al 2002b besides stream flow using ann models for data imputation for other spatio temporal data types has been reported diamantopoulou 2010 this is not a comprehensive literature survey as the field is very large with a long tradition but is intended to show the range of work in this area and also that reported best methods are dependent on data specifics with no clear best method this paper presents two data imputation methods using bayesian compressive sensing bcs in the first we apply bcs to the original signal to impute data to fill gaps in the data series in the second we first decompose the signal into a series of imfs using emd then apply bcs to each imf in turn to impute data to fill the gaps then recreate the signal by summing the filled imfs we hypothesized that as imfs are less complex than the parent signal it would be easier to use bcs to model each imf individually then use the imfs to recreate the full signal we present some background on bcs and emd methods along with the formulation of our two models using three different performance metrics and a number of different gap sizes and locations we compare these bcs models to three other approaches simple linear interpolation a temporal model and a spatio temporal model these methods were selected to represent a range of complexity and data requirements for context we present information on the data series the performance metrics and the three imputation models used for comparison we discuss the results and some features of the models and present our conclusions 2 background 2 1 approach cs exploits sparsity and is capable of representing a signal using a sampling frequency significantly less than the nyquist frequency candès and wakin 2008 candès 2006 shannon 1949 bcs casts the resulting optimization problem in a bayesian framework for computational advantages cs and bcs are most often used for data reduction or compression by taking an original signal and discovering a sparse representation then using this sparse representation to recreate the original signal when required we use this process for data imputation by building a sparse cs model with existing data then recreate missing data using the sparse model gemmeke et al 2010 gemmeke and cranen 2008 commonly cs uses the sparse signal model to recreate the original signal between sparse measurement points that are distributed throughout the signal space this allows a minimal number of measurement points to either be taken reducing measurement time or stored reducing data size we adapted this approach for data imputation by retaining all the original data then recreating the data across the gap as noted above the main difference between using a sparse representation to reconstruct the original signal and using a sparse model for data imputation is that data used for reconstruction are distributed throughout the reconstruction space while data used for imputation are complete or dense outside gaps and no data within the reconstruction space or gaps empirical mode decomposition emd huang et al 1998 is a signal deconvolution method that works with non stationary non linear semi periodic data typical of environmental data series emd is data driven and does not assume or require a stationary or linear signal emd decomposes a signal into a series of intrinsic mode functions imfs each representing independent components of the signal and a residual summing the imfs and the residual exactly reproduces the original signal we evaluated these two bcs methods using historic temperature data from the salt lake city airport a long term record to characterize their behavior we created gaps of various lengths in the signal used the different methods to fill the gaps and then compared the imputed data with the original signal we compared the bcs methods with commonly used data imputation methods a temporal spline a spatial temporal spline and linear interpolation the two bcs methods and the temporal spline only require data from the target station the spatio temporal spline requires data from surrounding stations while we used temperature data for this example mostly because of the available long continuous data set for verification this approach could be applied to any semi periodic or cyclic earth science data set such as stream flow 2 2 data we acquired the data from the utah climate data set ucds the ucds contains daily measurements of environmental data at 815 utah locations with some stations starting from december 1 1887 and continuing to the present though many stations start much later than 1887 many are missing large amounts of data and some do not continue to the present for this paper we used daily maximum temperature measurements from the salt lake international airport station observed over a 30 year period between december 1 1887 and december 1 1917 while this is one of the longer and more complete stations with data until 2014 we only used the first 30 years for ease of analysis in this paper y y 1 y t represents the vector of daily temperatures at the salt lake city airport station where t is the total number of days under consideration t 10 950 the spatio temporal model used in the comparison requires data from other stations we selected 20 stations from the ucds that had continuous data during the same 30 year time period blanding deseret escalante fillmore fort duchesne heber laketown levan logan utah state university manti moab morgan power and light oak city parowan richfield radio scipio spanish fork power house saint george tooele and vernal municipal airport this selection covers the state of utah with some stations near the salt lake airport station and others ranging to several hundred miles from the target station 2 3 bayesian compressive sensing cs is widely used candès and wakin 2008 candès 2006 candes and tao 2005 here we briefly describe cs in matrix terms davenport et al 2011 take a target signal f with length t assume f is sparse with respect to some basis ψ 1 f ψ w where ψ is a matrix with dimensions t by k and w is a sparse signal with length k in addition for each individual measurements y of the signal f with assumed measurement noise n we can write 2 y φ f n where ф is the measurement matrix that indicates where measurements exist for f for simplicity if we let φ φ ψ we can write 3 y φ w n we do not know the signal f but only the observed samples y to reconstruct the signal f we solve for w in equation 3 then use equation 1 for the reconstruction this can be thought of as finding a sparse representation of the original signal davenport et al 2011 for our work f is continuous and the observed samples y are not sparse in most of the signal but have missing data or gaps of varying length with no measurements but we follow the same formulation we solve for w in equation 3 using the dense y samples then use this formulation to estimate the signal f across the gaps equation 3 is undetermined meaning there is an infinite number of potential solutions cs imposes the restriction that w is sparse and uses the ℓ 0 and ℓ 1 norms as sparsity measures davenport et al 2011 the solution is found by casting the formulation as an optimization problem that minimizes these sparsity measurements candes and tao 2005 a bayesian framework i e bcs provides a more efficient solution method raman et al 1995 with computational advantages along with error estimates and bounds for the estimated coefficients babacan et al 2010 in bayesian formulation rather than inverting the matrixes we take advantage of the fact that w is sparse with most entries being 0 we solve the problem in a reduced dimension by choosing the bayesian assumption that initially all the coefficients are 0 in one of the solution matrixes babacan et al 2010 these values are updated one at a time in the optimization algorithm eliminating the need to invert the matrixes babacan et al 2010 practically bcs decomposes the original signal using a set of basis functions and selects a subset of these basis functions to represent the signal for this work we used the fast laplace algorithm presented in babacan et al 2010 and modified code developed by matt goodwin goodwin 2017 goodwin and reese 2016 this code decomposes the measured signal into separate components with options for three basis function expansion methods fourier basis wavelets and a b spine basis see babacan et al 2010 goodwin 2017 nelson et al 2013 for details mathematical treatment and practical solution methods in this work we used the fourier basis for expansion 2 4 empirical mode decomposition emd to analyze the underlying processes of a time series or to capture an underlying trend or signal researchers often deconvolve or decompose a signal into separate periodic components using transformations based on methods such as fourier or wavelet analysis pelletier and turcotte 1997 markovic and koch 2005 kantelhardtet al 2003 kantelhardt et al 2002 these methods restrict the resulting periodic components to be regular and stationary making it more difficult to fit natural phenomenon that follow irregular cycles markovic and koch 2005 environmental data can also include trends such as temperature increases resulting from climate change many if not most environmental time series are nonlinear and nonstationary the mean of the time series changes over time i e has a trend and in a nonlinear fashion huang et al 1998 for example air pollution and temperature values rise and fall daily and seasonally but not on even intervals pelletier and turcotte 1997 as discussed emd is a signal analysis method like the fourier transform that decomposes signals into independent periodic components called imfs and a trend the imfs form a complete and nearly orthogonal basis for the signal each imf may or may not represent physical processes though in many cases physical processes can be attributed to the imfs in huang et al 2011 huang demonstrates associating specific imfs resulting from the decomposition of length of day data with el nino events and tidal variations when summed these imfs exactly represent the original data the emd process separates the riding wave of a high frequency component from the lower frequency waves this process is repeated level by level to extract n mono component waves from high to low frequencies with the residual representing the trend this can be summarized as wu and huangsssp 2014 4 x t r e j 1 n a j t e i ω u t d t r n where re represents the real part of a j t and ω j t are instantaneous amplitude and frequency of the jth imf respectively the decomposition process is data driven and does not rely on assumptions of linearity stationarity or any specific statistical distribution for a given series y we use the following process to deconvolve the signal to extract the imfs huang et al 1998 1 let m k t y and let k 1 2 identify the time series extrema m k t to create a vector of points representing the local minima and maxima of the series 3 interpolate between all the minima e 1 k t then all the maxima e 2 k t to create an envelope defined by the two interpolated boundaries as shown in fig 1 4 compute the instantaneous mean m k 1 t e 1 k t e 2 k t 2 of these two envelopes 5 extract the detail d k t m k t m k 1 by subtracting the instantaneous mean from remaining signal this is one iteration of the sifting process repeat steps 2 5 until the mean extracted in step 4 is approximately zero that is below some threshold at this point the remaining signal is the kth imf the kth imf is then subtracted from the original signal in step 1 and the result becomes the new original signal for the next imf extraction process 6 let k k 1 and repeat steps 2 6 until there is only 1 maxima and minima remaining in the time series this is the final imf or residual which represents the overall trend in the data series 7 other than the final imf the other imfs are all centered on zero the final imf or residual clearly defines the trend over time as it relates to the entire signal fig 1 illustrates one iteration of the emd algorithm for more detailed information about emd and its options for example various interpolation methods to create the envelope of the extrema in step 2 we refer the reader to huanget al 1998 huang and wu 2008 emd works best with regularly spaced data as the envelopes are most easily interpolated to a regular spacing it is difficult to apply this process to a signal with gaps as most existing codes assume regularly spaced data and it is not clear how well the method would work on data with gaps therefore before applying the emd algorithm any gaps or missing data points should be filled we explored a number of different methods for filling these gaps prior to applying the emd algorithm and found that simple linear interpolation was sufficient e g a linear interpolation between the two end points of the gap it has the advantage of being quickly and easily implemented and does not add any unobserved or artificial maxima or minima that may influence the emd algorithm for our case more complex methods did not provide any benefit this was partially true because while data in the gap that were initially linearly interpolated these data were replaced with bcs model results while this was sufficient to support our data imputation models it may be insufficient if emd methods are used for other analysis the linear imputation method can be described as follows if the known points on either side of the gap are t 0 y 0 and t 1 y 1 then any missing value y t is calculated by 5 y t y 0 t 1 t 0 y 1 y 0 t 1 t 0 we used this formulation both to initialize the data prior to performing emd and as one of the data imputation methods described below 3 method comparisons 3 1 data series as noted we used the first 10 950 days approximately thirty years of data from the salt lake city international airport station for discuss purposes we assume a year of 365 days ignoring leap years since we only discuss gap lengths not actual dates this assumption does not impact the results the actual dates were from december 1 1887 to november 24 1917 the longest gap we investigated was approximately 6 years we used the salt lake city international airport station data to create nine signals to test and compare the imputation methods the first four signals had 1 110 days 5 548 days or 1 5 years 10 1096 days or 3 years or 20 2190 days or 6 years of missing data percentages were used for computational ease the gaps were created in the center of the signal at day 5475 november 28 1902 the gaps were october 4 1902 to january 22 1903 1 february 27 1902 to august 29 1903 5 may 29 1901 to may 29 10 1904 and november 28 1899 to november 27 1905 20 all plots are presented as days from december 1 1887 as actual dates are not important fig 2 shows the original data in the top panel with each of these four gaps in the subsequent panels the data we are attempting to reconstruct from these gaps have an interesting feature the winter of the year at the center of the gap near day 5 500 is clearly colder than any other years in the period and the year preceding the center of the gap at approximately day 5 100 was also quite cold and was cold over a longer period than most of the other years in the sequence this means the data we are attempting to impute in these four gaps are somewhat anomalous in addition to these four signals with gaps of varying length we created five additional signals all with 10 1096 or 3 year gaps the gap locations were randomly selected with gaps beginning august 6 1890 september 1 1897 july 13 1901 february 14 1909 and november 21 1914 for the five signals respectively the last gap from november 21 1914 to november 22 1917 caused some problems with the emd based method as the gap ends just 2 days before the data end on november 24 1917 the location of these gaps are shown as bars in the 10 gap panel in fig 2 we imputed data for these five gaps using all five methods to provide an indication of the general performance of the algorithms 3 2 statistical comparison measures we used three different performance measures to compare goodness of fit of the imputed data to the original we only calculated statistics for the portion of the signal where data was missing rather than the full 30 year signal so that the results would not be weighted by the gap size that is we calculated the goodness of fit only for the imputed data the three measures of fit were root mean squared error rmse spectral angle sa and lagged correlation lc we used three different measures because research has shown the rmse the most common measure reported in the literature may not always be appropriate for periodic or semi periodic signals wang and bovik 2009 these three metrics highlight different aspects of the fit between the imputed and measured data rmse measures how well both the shape lag and amplitude correlate it is sensitive to phase and amplitude shifts and less sensitive to smaller data values and more sensitive to larger data values sa measures how well the shape of the data correlate and is less sensitive to small shifts in phase or amplitude and more equally weights small and large values lc measures if there is a phase shift and also provides a metric into how well the signals are correlated at that optimal phase shift if you have an lc of 1 at some lag this means that both the shape and amplitude are perfectly matched but phase shifted the highest lc value shows both the apparent lag and also the how well the shape and amplitude correlate at that lag the other two metrics rmse and sa only measure the fit at lag 0 3 2 1 root mean squared error root mean squared error rmse measures the difference between the estimator and what is being estimated using 6 r m s e t 1 t y t y ˆ t 2 t where y ˆ t are the imputed data y t are the original data and t is the number of data points for the rmse a smaller number indicates a better fit to the original data the value of the rmse has units of degrees 3 2 2 spectral angle spectral angle sa is a measurement often used to determine if two time series are similar or not with a spectral angle of 0 meaning the two series are similar sa treats each time series as a multi dimensional vector and calculates the vector angle between them in multi dimensional space in general this method looks at the shape of a time series rather than the magnitude it is most commonly used to determine how well different spectral measurements match it is calculated by 7 s a a r c c o s t 1 t y t y ˆ t t 1 t y t 2 t 1 t y ˆ t 2 where y t ˆ are the imputed data y t are the original data and t is the number of data points for the sa a lower number indicates a better fit with values ranging from 0 to π 3 2 3 lagged correlation lagged correlation lc measures the correlation between the signals when they are shifted against one another the correlation at each of these shifts is calculated and the highest correlation at any lag is reported if the signals are perfectly correlated in time at some lag value then the lc value is 1 this measure can help determine if an imputed time series is correct in shape and amplitude but offset in time lagged correlation is calculated by 8 l c m a x r c t 1 t y t y y ˆ t c y t 1 t y t y 2 t 1 t y ˆ t c y 2 0 2 t c like above y t ˆ is the filled signal y t is the original signal y is the signal average and t is the number of data points we chose to evaluate shifts up to 20 of the data in either direction as indicated by the term 0 2 t c in equation 8 the range of the lc value is bounded 1 1 with 1 meaning the signals are perfectly correlated at some lag 4 imputation methods to demonstrate the two bcs methods we developed we compared them with three commonly used methods from the literature the comparison methods in order of complexity are linear interpolation a temporal model and a spatio temporal model we call the first bcs method bcs on signal this method applies bcs algorithm to the original signal to develop a model and then uses that model to impute data across the gap we call the second method bcs on emd this method first decomposes the original signal into a set of imfs develops a bcs model for each imf imputes the data for the gaps in each imf and then reconstructs the target signal from the imputed imfs 4 1 bcs on signal bcs on signal applies the bcs algorithm to the original signal to create a model of the signal it then uses this model to impute data across the gaps traditionally bcs is used to create sparse representations of signals candès and wakin 2008 and generally a bcs model uses a sparse set of data points along with the fitted model to reconstruct the target signal at a higher resolution for the bcs on signal method we relied on the ability of bcs to represent signals with limited data for our data imputation approach we fit the bcs model using all the measured data points in a given signal that is the entire signal but with no data in the gaps we then used these same data points as the sparse measurements and used the fitted bcs model to reconstruct the data in the gaps the reconstructed signal includes both the measured data points and the imputed data points at unmeasured times this is a somewhat unusual use of bcs as the reconstructed portion of the signal has no supporting data points through the gap region all the support for the model is outside the gap that is being filled 4 2 bcs on emd bcs on emd first decomposes the original signal into a set of imfs then follows the same process described above to impute data across the gap locations as noted the emd algorithm requires a signal with no gaps so the gaps were filled using linear interpolation prior to running emd after the imfs were computed the data in the gap regions were removed a bcs model was then fitted on the remaining data in the same manner as the bcs on signal model and used to impute the data in the gaps once all the imfs gaps were filled the full signal reconstructed by summing the individual imfs as discussed above emd breaks down a signal into a set of nearly orthogonal imfs that represent different frequency components of the signal the first imf generally represents the noise component in the signal with subsequent imfs representing processes or portions of the signal with longer periods or lower frequencies wu and huangsssp 2014 we sought to exploit this feature we hypothesized that as each imf was simpler than the original signal and we could model these individual imfs using bcs more accurately than we could model the complex full signal we thought we could then use the individual imf modes to impute data in the gaps and then reassemble the missing signal from the reconstituted imfs fig 3 shows the imfs that result from application of the emd algorithm to the salt lake international airport station data note that each panel has a different scale each imf other than the trend or final imf is centered on 0 the y axis for the top five plots ranges from 8 to 8 c the y axis on imf 6 and imf 7 ranges from 20 to 20 c while the y axis for imf 8 imf 9 imf 10 and imf 11 range from 8 to 8 c 2 to 2 c 1 to 1 c and 0 8 to 0 8 c respectively imf 1 through imf 11 are centered on zero imf 12 is not and represents the offset in the temperature data the bottom panel of fig 3 is the original signal and ranges from about 15 6 41 7 c with a mean of 17 7 c and a standard deviation of 11 8 c the mean median and mode of 17 7 17 2 and 16 7 respectively are relatively close and the skew and kurtosis values are relatively small 1 1 and 0 5 respectively indicating the data are approximately normally distributed as noted below the emd algorithm finds a small trend in the data that is reflected in the skew value emd is an empirical method only based on the data and while the resulting imfs are nearly orthogonal they may or may not represent actual periodic environmental processes emd can also split or deconvolve the signal from a single physical process such as annual variation into two or more imfs this can happen if for example there are a period of years that have a late spring and early fall as opposed to a series of years that have an early spring and late fall emd can at times separate these into different imfs the user needs to understand the data set and potential physical processes that may be represented in the data to assign meanings to individual imfs for this data set we interpret the first 5 imfs imf 1 through imf 5 as representing noise each of these imfs have a range about 16 c from about 8 to 8 c though there is some variation analysis shows some apparent periodic signal with a period of about 30 days mostly in imf 4 and imf 5 but this is not strong and most likely is an artifact of the data these five imfs could be summed to provide a good approximately of the noise in the signal all have a standard deviation of about 2 c with low kurtosis and skewness numbers indicating an approximate normal distribution we interpret imf 6 through imf 8 as representing the annual periodicity in the data these imfs have a larger range 39 6 36 2 and 12 1 c for imf 6 imf 7 and imf 8 respectively for our data the emd process appears to have separated the annual signal into three imfs visually it is clear the imf 6 and imf 7 represent the majority of the annual periodicity a close inspection not presented here shows that the algorithm separated the annual signal based on the length of the winter period with the colder years in imf 6 and warmer in imf 7 imf 8 has a significantly smaller range 12 1 c and when added the other two annual imfs affects mostly either the fall or spring seasons summing imf 6 through imf 8 provides a good approximation of the annual periodicity in the data imf 9 has a range of only 3 1 c and shows a process with a period that varies between 4 and 6 years we interpret imf 9 as the signal from the el niño climate pattern which has an approximately five year period the smaller range indicates that the effect is not large at the salt lake airport imf 10 also has a small range of 1 8 c and an approximate 11 year period this is most likely the effect of sunspot variability on the temperature sunspot activity has an approximate 11 year period and effects climate and weather patterns there have been several papers that use the emd algorithm to extract periodic behavior in environmental data related to sunspot data for example barnhart and eichinger 2011a 2011b zolotova and ponyavin 2007 chorley et al 2010 the last two imfs imf 11 and imf 12 in our interpretation represent the long term trends in the data both have smaller ranges 1 1 and 0 6 c for imf 11 and imf 12 respectively imf 12 is the trend in the data set and ranges from 17 8 to 18 4 c representing the offset from zero in the temperature series we interpret imf 11 as being a part of the trend but adding some detail it could also represent some longer period process that is not easily extracted from the short 30 year data period these two imfs indicate an approximate 1 c warming trend over the 30 year period but this is not constant imf 12 shows the overall trend more or less linearly increasing over the full period while imf 11 indicates an initial cooling period following by almost a full 1 c of warming peaking at about day 4 500 then cooling and re warming at the end of the period fig 3 shows how the individual imfs are simpler than the original signal and we expected that bcs would be able to better model these signals however as noted this was not the case and potential causes are discussed below 4 3 linear interpolation for data imputation using linear interpolation we used the same method as described for preliminary imputation for emd processing equation 5 we select the end points of the data gap and perform a linear interpolation across the gap while this approach may provide useful data for short gaps it is generally not applicable to longer gaps we present it here as the simplest baseline comparison over short gaps such as a few days linear interpolation may be sufficient but for any type of periodic or nonlinear signal this approach quickly breaks down once the gap is some significant portion of the signal period 4 4 temporal model environmental data often exhibit strong temporal dependence and trends for example the plots of the temperature data used in this paper clearly show the annual cycle e g fig 2 other periodic temporal patterns can also exist in these data such as el niño as discussed we exploit the strong annual trend for the temporal model the temporal model imputes the missing data by modeling the annual seasonal cycle using b splines wahba 1990 the model was fit to the data outside the gaps and represents the estimated average annual cycle we then use the fitted model of the annual cycle to impute data across the gap since this model is based on the average annual trend it results in an overly smooth signal to add the expected variance to the imputed data we added white noise with the same variance as the original signal to the average annual cycle the imputed data are calculated by fitting the model 9 y xβ ε where y is a vector of length t of temperature values though only the measured time points are used for fitting the model x is an t by p matrix of basis functions where the b spline basis function is evaluated for each day at its day of year in our case p was equal to 11 where the first column of x is a vector of ones and the next ten columns are the b spline bases evaluated at 10 equally spaced knots between 1 and 366 the number of possible days in a year we used the r package splines to compute the b spline basis team 2014 β is a p by 1 vector of coefficients where the first element is the y intercept and the remaining elements are the b spline coefficients therefore xβ represents the mean intercept and the annual trend ε is a vector of error terms such that ε n 0 σ i 2 where σ i 2 is equal to the residual variance of the original signal we fit the model by estimating β and σ i 2 using least squares estimation and the observed data then computed the imputed data using x and the estimated value of β and adding zero mean normally distributed noise with variance equal to the estimated value of σ i 2 the imputed data are a noise added version of the estimated annual period 4 5 spatio temporal the spatio temporal model is much like the temporal model except that data from 12 other stations are included in the analysis this was meant to improve on the temporal model since in addition to temporal dependence temperatures also exhibit spatial dependence i e temperatures from one location tend to be similar to temperatures in a nearby location this allows us to estimate any deviations from the 30 average that might occur during the gap this is done by determining how the other potentially spatially correlated stations differed from the average during the same period this is different from the other models presented in this paper in that data from other stations during the time period that includes the gap are required for imputation spatio temporal models are commonly used to model environmental data and there are many ways to specify these models all with varying degrees of complexity and computational burden see for example banerjee et al 2014 cressie and wikle 2015 in our analysis since we were using this model for comparison we chose to specify the model in a way that was easy and quick to implement our spatio temporal model estimates a smooth annual trend and a smooth spatial trend and does not capture the slight deviations from the trend that the actual data exhibit to determine the weight the other stations should receive we first removed the annual trend from each station so that the correlation with the annual trend across all the stations does not eclipse the correlation of the other trends present we then computed empirical orthogonal functions eof s and used them as additional covariates to the model presented in equation 9 brynjarsdóttir and berliner 2014 in this case the eof s are the first three eigenvectors of the empirical covariance matrix among the measured values minus the annual trend of the 13 stations as with the temporal spline we added white noise with the same variance as the original data to account for over smoothing effects the imputed data are calculated by fitting the model 10 y i x i β x i e i β e ε i where y i is a vector y i 1 y i t of temperatures at station i x i is a t by p matrix of b spline basis where just as in the temporal model the basis function is evaluated at the day of year for each time β x i is the p by 1 vector of coefficients e i is the portion of the eof corresponding to the ith station and β e the corresponding coefficient vector ε i is a vector of error terms such that ε i n 0 σ i 2 where σ i 2 is equal to the residual variance of the original signal we fit the model by estimating β x i β e and σ i 2 using least squares estimation and the data from all stations as with the temporal model the imputed data use the estimated coefficients β x i and β e to impute the annual and spatial trends then zero mean normally distributed noise using the estimated value of σ i 2 is added to account for over smoothing 5 discussion and results we present two bcs based data imputation models both of which only require data from the target signal i e signal with the gap the first model bcs on signal uses fourier transforms as the basis functions to create a sparse model of the full signal which then is used to impute the missing data the second model bcs on imf first uses emd to deconvolve the signal into independent imfs then applies the same bayesian compressive sensing algorithms with fourier basis functions to create models of the individual imfs uses these models to impute data in the gaps then reassembles the full signal by summing the reconstructed imfs our objective is to determine the effectiveness of sparse data models using bcs methods for environmental data imputation our hypothesis was that the bcs on imf method would be more accurate because it is built on simpler imfs which would be easier to model than the complex full signal while the results did not support this hypothesis the bcs on signal imputation method performed very well below we discuss this performance as well as potential features that affected this performance we included three other common data imputation methods for comparison linear interpolation a temporal model which only requires data from the target signal and a spatio temporal model which requires data from nearby spatially correlated stations we viewed the temporal model as a direct competitor to the bcs approaches as it only uses data from the target station while the spatio temporal model provides a standard for comparison of more accurate methods we expected it to outperform the other approaches as it exploits additional information at the time of the gap found in data from surrounding stations the imputation methods can be divided into two categories based on the data required the first category only use data from the target signal and includes the bcs on signal bcs on imf linear interpolation and the temporal model the second category requires data from surrounding stations or locations and includes the spatio temporal model these requirements have implications for both application and accuracy methods that only require data from the target signal are easier to use and can be applied more widely than methods that require data from both the target station and surrounding stations where the surrounding stations have data during the period where data are missing from the target signal methods that use data from surrounding stations should be able to better exploit temporal patterns that are not periodic in the data as spatially correlated stations should exhibit similar response to anomalous weather patterns for example if the gap were an anomalously cold year as was the year used for gaps of varying length in this paper we would expect surrounding stations to experience the same cold year and using data from these stations could result in more accurate imputation we would not expect a method that only uses data from the target station to be able to reconstruct this anomaly as there would be no way to infer the anomaly from the available data however in some cases apparent anomalies may be the result of several long term trends that combine either constructively or deconstructively creating an apparent anomalous pattern that can be predicted using only available data from the target station thus while a cold year might appear to be anomalous it may be a result of the superposition of several predictable processes as noted above the year at the center of the data set and the preceding year appear anomalous as they have colder temperatures than the other 28 years of data the year at the center of the gap about day 5 500 is clearly colder than any other years in the series while the preceding year is colder over a longer period than most other years in the series based on a visual examination the four gaps of different lengths are centered on this anomaly these types of anomalies are very difficult to reconstruct only using only data from the target signal as there are no patterns or information that seems to predict this behavior the spatio temporal model however can use information from other weather stations which should also experience similarly cold anomalous conditions this is the basic assumption used in the spatio temporal model that there are spatial correlations in the data for example if the weather at the salt lake airport was anomalously cold in the center year then other stations in utah would also experience cold weather that year they would have similar patterns that could be used to better impute missing data during this time table 1 summarizes the results from comparing the five different imputation models on each of the four different gap lengths 1 5 10 and 20 and includes each of the five models linear bcs on signal bcs on imf temporal and spatio temporal the table presents three different goodness of fit metrics for each of these combinations rmse sa and lc the best metric for each gap length is in bold text in all cases but one the rmse value shows that the linear interpolation model is the worst fit the exception is for the 5 gap where the bcs on imf has a value similar in magnitude but slightly larger 13 70 versus 19 08 for the linear interpolation and bcs on imf respectively the 5 gap occurs over the large cold spike around day 5 500 the bcs on imf model does a good job of matching the cold spike but is too low in the pre and post time periods the linear model almost provides an average for this period as the gap is centered over the cold spike data not shown which results in a surprisingly good rmse value but this is due to the unique circumstances and the gap centered over the temperature minimum as noted with this exception the linear model is not competitive and will not be discussed further depending on the metric used and the gap size either the bcs on signal the temporal or the spatio temporal model exhibit the best fit to the data in fact when one of these methods is better one of the others exhibits similar performance metrics there are no cases where one model if significantly superior to another table 2 using the average of only 5 runs shows that the spatio temporal model exhibits the best fit for any of the three metrics the spatio temporal has a significantly smaller range for the sa metric and relatively small range for the rmse metric compared to the other methods fig 4 shows the imputed values from each of the five models against the centered 10 3 year gap for a visual comparison the top panel is the bcs on signal results which follow the annual trend very well and also captures some variability that matches the variability of the seasons but is consistently smoother than the original data the second panel is the bcs on imf results which under predict the summer fall seasons from about 5100 days to 5400 days but matches the cold winter period and the remainder of the gap well the annual variability in these imputed data visually appears closer to that exhibited by the original signal with some regions having larger variations and other smaller the third panel is the temporal model results which mostly matches both the trends and the apparent variability throughout the imputed data however it does not match the very cold winter at about day 5500 or the cold period of the prior winter season days about 5100 to 5300 this is expected as these data are outside the range of the data available outside the gap that were used to develop the model panel four is the spatio temporal model results which visually match both the trends and the variability but appear to have an offset to the future of about 100 days similar to the temporal model it does not predict the very cold winter centered at day 5500 or the previous cold winter starting at about day 5100 we expected a better match as other utah weather stations recorded this cold period the bottom panel shows the linear model across the 10 3 year gap the linear model is obviously a poor choice for gaps longer than about 100 days 1 4 of a year a short 100 day segment can be approximated by a linear trend with added noise but over any longer periods a linear trend is a poor approximation the two bcs models which are the subject of this paper do a relatively good job as shown in top panel of fig 4 the bcs on signal model results follow the general periodicity of the data very well but a visual examination shows that the data do not exhibit the short term variability present in the original data this is highlighted in fig 5 which presents similar results for all 4 gap lengths the data visually match the original well but with less apparent variability conversely the variability of the bcs on imf model results is larger than the original data this can be attributed to the fact that the first 3 or 4 imfs capture the variability of the original signal when these imfs are modeled across the gap at certain points this variability is additive and at certain points subtractive either amplifying or damping the imputed data relative to the original signal producing spikes and smooth areas depending on the aliasing of the underlying imfs in general the bcs on imf follows the general processes well but over different periods can either under or over predict the data for example an imf representing approximate 7 year periods perhaps driven by the el nino cycle may not exactly match the amplitude of this period process over the gap causing a period of over or under prediction interestingly the bcs on imf model better matches the anomalously cold temperature at about day 5500 and the longer winter from day about 5100 to 5200 than the other models however it shows a much colder summer and significantly colder fall from about day 5300 to day 5500 performing worse than the other models in this range the better match could be the result of longer term processes such as el nino combining constructively to produce the cold winter which could be predicted by the model or could be random table 2 highlights one issue with the bcs on imf method the emd algorithm exhibits significant end point effects that is imf decomposition near the end of a signal is affected by the boundary or end of the data as noted one of the five random gap locations ended within two days of the end of the data this caused significant anomalies in the imfs produced for this signal the large maximum values for rmse and spectral angle measures and the very small minimum lag correlation values are from the results of imputing at this gap knowing this behavior it would be appropriate to restrict this method to signals where the gaps have a significant amount of data relative to the gap size on either side of the gap if measures from this gap near the end of the data were removed from the average then the results of the bcs on imf would be similar too though still not as good as the bcs on signal values another factor that contributed to the poorer performance of the bcs on imf method compared to the bcs on signal method was that the emd algorithm separated some of the periodic components into several separate imfs as discussed above and shown in fig 3 the emd algorithm separated the annual variation signal into three separate imfs imf 6 through imf 8 and the noise into the first 5 imfs these are difficult to model as they do not present a consistent pattern for example imf 6 see fig 3 has in initial period up to about day 4 000 of very low variation that almost appears as noise then a period that represents annual variation from about day 4000 to day 5 200 then a flat period again this type of behavior is difficult to model and predict we expect that if we had visually inspected the imfs and summed those that represented different processes for example sum imf 1 through imf 5 to represent noise sum imf 6 through imf 8 to represent the annual cycle left imf 9 and imf 10 separate to represent individual physical processes and sum imf 11 and imf 12 to represent the long term trend then the bcs on imf method may have performed better as each imf would have had a more consistent pattern that could be better modeled fig 5 shows the bcs on signal results on all four gaps these panels show that bcs on signal visually does a good job filling the four different gap lengths the top two panels 1 and 5 gaps show that the coldest winter days at approximately day 5500 do not exactly line up with the center of the gap with the imputed coldest period slightly offset to the left the bottom panel shows that bcs on signal visually does of good job of data imputation even for the 20 6 year gap using only data from the target station a meaningful result the two bcs based models both ensure continuity at the end points of the gap there are no large offsets this is not true of the temporal or spatio temporal models which can have discontinuities or offsets at the beginning or end of the gaps for example the spatio temporal model results in the fourth panel from the top of fig 4 clearly shows a significant temperature offset at the beginning of the imputed data the temporal model and the spatio temporal model both perform well contrary to our expectations the temporal model often outperforms the spatio temporal model table 1 including the average performance of the five runs on the 10 gap table 2 visually the spatio temporal model appears to perform as well as or better than the temporal model but with an offset in time which resulted in poor goodness of fit metrics this may be due to the fact that the stations the algorithm selected as most correlated with the salt lake airport station and used to impute the data in the gap have similar trends are correlated but are offset in time because of differences in altitude or other local weather patterns this offset seems to be consistent as the average results from the five different gap locations are similar to the results from the centered gap this indicates this offset is not just the result of a specific time period this result may not hold for other stations practically the top three methods bcs on signal temporal and spatio temporal perform very similarly to each other with different algorithms winning depending on the specific gap or performance metric used the bcs on signal and the temporal model metrics are very similar to each other based on our results we cannot distinguish one of these two models from the other in terms of performance both these models identify temporal patterns in the data and model those patterns across the gaps to impute the missing data it is reasonable that these methods have similar performance an interesting result from this study is that the performance measures of the four methods not including linear interpolation do not significantly change with the size of the gap for example the rmse for bcs on signal ranged from 6 28 to 6 82 over the four gaps which ranged from about 30 days to about 6 years in length with the worst fit on the shortest gap the other models were similar with rmse ranges of 6 26 7 02 and 7 33 to 9 43 on the different gap lengths for the temporal and spatio temporal models respectively the bcs on imf rmse metrics ranged from 9 85 to 11 06 with the 5 gap exhibiting an outlier of 19 08 the other two performance metrics spectral angle and lag correlation exhibited similar small ranges across the various gap lengths for each model 6 summary we presented two bcs based data imputation methods which only require data from the target signal there was no clear best method and the results varied depending on the goodness of fit measurement used and which gap size was compared though either the temporal or the bcs on signal models were usually best fit and had performance metrics very close to each other we expected the bcs on imf method to outperform the bcs on signal method our hypothesis was that the individual imfs would be simpler to model than the complex complete signal however the bcs on signal approach performed better in practice the emd algorithm appears to over decompose the signal this results in individual imfs that are difficult to model as the imf may look very different at different times as shown by the three imfs that represent the annual cycle imfs 6 7 and 8 shown in fig 3 there may be other issues contributing to the lower performance the bcs method relies on decomposing the original signal into a set of components using different basis functions then finding a sparse representation using these basis functions if we first apply emd before applying bcs the resulting imfs may not be well suited for decomposition since they should already be independent and not amenable to decomposition also the average of the bcs on imf results included an outlier caused by a random gap very near the end of the data an issue with emd if this run were removed from the average the average bcs on imf results would be better though still not as good as the bcs on signal values but showing the two methods as comparable to each other in performance the statistical performance measures were not sensitive to the gap size the methods did as well imputing data for the large 20 6 year gap as they did for the small 1 110 day gap the methods based only on data from the target signal had performance metrics in the same range and often better than the method that used data from surrounding area a result we did not expect as a final observation the bcs on signal method outperforms the bcs on imf method based on all the metrics used in this study however the imputed data from the bcs on signal model do not visually exhibit the longer term variability observed in the original data this is also true of both the temporal and spatio temporal models which are visually smoother than the original data even though each has a noise term in the model based on the noise in the original data visually the bcs on imf results better represent the larger random fluctuations seen in the actual data but do not more accurately recreate the missing data based on our performance metrics while the bcs on imf method has poorer performance metrics if the results of the data imputation exercise are to be used in a model than the more realistic variability of the bcs on imf method may be more applicable funding this work was supported by the national nuclear security administration department of nuclear nonproliferation research and development grant number de na0002491 appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 01 012 
26424,this work introduces the locally filtered transport lft method for numerical transport models locally turning off the transport computation in areas of nearly uniform concentration is proposed as a new approach for reducing computational cost in ecosystem models that require transport of tens to hundreds of constituent concentrations the proposed method is locally mass conservative just as the discontinuous galerkin finite element scheme it is based on the performance of the method is illustrated using numerical examples including an advection reaction ecosystem simulation with a simple nitrogen phytoplankton and zooplankton npz model keywords discontinuous galerkin finite element method shallow water equations multi component advection reaction system npz model transport scheme 1 introduction this work introduces and evaluates a new computationally efficient scheme for transport equations in multi component advection diffusion reaction models the idea behind our method is quite simple many physical chemical and biological processes take place on highly localized spatial and temporal scales such that one or more transported constituents might be at quasi uniform or background concentrations over large areas for example outside of a localized algae bloom the chlorophyll concentration in a water quality model is typically at some background level such that the same small concentration is fluxed in and out of most computational cells these computational cycles of the transport equations are wasted and cannot affect the model results until the reaction equations initiate local growth of a bloom thus an ecosystem model coupled to a large scale circulation model e g regional or global ocean climate incurs substantial computational costs for transport in parts of the spatio temporal domain where some or all of the constituents are not present have only the background concentration or do not play a significant role in the reactions for simple ecosystem models implemented in only two dimensions 2d such as the nitrogen phytoplankton zooplankton npz model used herein for demonstration purposes the increased computational costs of transporting a few scalars is generally irrelevant however for ecosystem models that transport different species of plankton and include chemical speciation no3 nh4 dissolved inorganic carbon dissolved organic carbon etc the number of transported variables can easily be several dozen or more e g robson and hamilton 2004 schwalb et al 2015 in three dimensions 3d the extensive scalar transport requirements can dominate the overall computational time arguably such models are computationally inefficient as they are not generally designed to identify and transport constituents only when and where they are significant in this study we demonstrate how to add such capability to an existing hydrodynamic transport model with an approach we call locally filtered transport lft we propose the new lft algorithm that adaptively turns on off the computation of certain discrete terms the model performance is evaluated using a conventional hydrostatic inviscid 2d shallow water and transport model based on the discontinuous galerkin dg finite element method aizinger 2004 aizinger and dawson 2002 the present work builds on the background filtering approach hodges 2014 which required an unconventional mass transport algorithm herein we show that localization techniques can be efficiently extended to standard concentration transport schemes furthermore the computational costs associated with localization that were identified in hodges 2014 are elegantly handled within the dg framework using the vertex based slope limiter aizinger 2011 kuzmin 2010 the utility of the proposed approach is not limited to discontinuous galerkin methods or geophysical applications it can be easily transferred to any numerical pde partial differential equation solver containing transport equations and might achieve meaningful performance gains even in the absence of reaction terms or in situations when only a few species are transported this paper is structured as follows the system of governing equations is introduced in sec 2 followed by a description of the lft method in sec 3 an npz ecosystem model is presented in sec 4 which is used as a test case for the lft method combined with a dg hydrodynamic transport model in sec 5 for completeness and to allow others to build on the modeling approach the details of dg discretization are provided in appendix a a brief discussion and conclusions section completes the paper 2 governing equations the model problem for this study is the 2d shallow water equations in conservative form eqs 1 and 2 below combined with a varying number of equations for advection reaction represented by eq 3 below and augmented as needed by the corresponding initial and boundary conditions 1 t ξ u h 0 2 t u h u u h g h ξ f c k u h τ b f u h h f 3 t c m h u h c m h r m c 1 c m h f m m 1 m the primary unknowns in eqs 1 3 are the water surface elevation ξ measured from a uniform datum the depth integrated horizontal velocity vector u h u v t and the depth integrated concentrations of multiple transported species c m h m 1 m given a boundary condition of spatially varying bathymetry elevation b x y the auxiliary variable h denotes the total water depth ξ b all equations are required to hold on some lipschitz bounded 2d domain ω and on time interval 0 t end furthermore g denotes gravity f c is the coriolis coefficient k is the vertical unit vector pointing upwards τ b f is the coefficient of the quadratic friction law f f x f y lumps together the forcing terms in the momentum equation e g tidal potential and f m r m m 1 m are the source sink terms and reaction rates in advection reaction equations respectively with the exception of the reaction terms the model is very similar to aizinger and dawson 2002 where the dg method was proposed for the 2d shallow water equations combined with non reactive species transport equations 1 and 2 utilize three types of boundary conditions land river open sea denoted by γ l γ r γ s ω respectively while eq 3 for constituent transport may have boundaries that are inflow outflow or wall no flow in this work a river boundary is always an inflow boundary a land boundary is always a wall boundary and open sea boundaries are dynamically switched between the in outflow modes depending on the flow direction these conditions are presented formally in table 1 3 locally filtered transport the lft method relies on definition of an individualized active domain for each transported constituent that is a subset of the total domain ω each active domain evolves over time as advection diffusion and reactions change the constituent concentration outside of the active domain the advection terms are ignored we consider two cases first as discussed in the introduction there are regions of a computational domain over which some constituent might fall below a dynamically meaningful concentration and may safely be ignored second there is the possibility of regions with nearly uniform concentrations where transport is merely moving the same concentration about with no effect on the local distribution for example far away from the influence of estuaries a large scale ocean model that is not resolving salinity effects for meso scale features might be simply shuffling around minuscule changes in salinity over the majority of the domain a modeler might want to retain salinity transport for estuarine input to the coastal shelf but it could be safely excised from the majority of the domain both cases can be addressed by monitoring the local concentration difference herein we define a uniformity difference or δ u for each constituent such that concentration differences between two neighboring grid elements smaller than δ u allow the concentration to be considered locally uniform so that no transport computation is needed this general idea was introduced in hodges 2014 as part of an algorithm using mass transport rather than concentration transport to allow local subtime stepping in regions where high velocities strictly limit the local advective time step the prior methods had relatively high computational costs due to the approach taken to identify the active domain in the present work we adapt the background filtering from hodges 2014 to the dg algorithm using uniform time steps and a conventional concentration transport discretization although the lft idea can be extended to any model the slope limiting dg method aizinger 2011 kuzmin 2010 has a particular advantage in that the majority of the computational effort for localization is already required in the existing transport algorithm namely by the slope limiting function see appendix a 5 which identifies and sorts the local concentration differences to maintain monotonicity the discrete time advance of eq 3 for constituent transport is described in detail in appendix a 5 for purposes of the lft method the key point is that the advance from time level t n to t n 1 uses an explicit runge kutta scheme with a slope limiter at time t n the minimum and maximum values of a transported constituent for each computational element t k that is connected to a node x i are known from the slope limiter of the time advance from the n 1 to n step i e a i min and a i max are already defined at each vertex see eq a 9 we use this time lagged data to find the maximum nodal jump of scalar field a for element t k and its nodal neighbors as ξ k max a i max a i min x i is vertex of t k it then follows that the only elements t k included in the runge kutta time advance for a given constituent are those where ξ k δ u all elements where the δ u condition is not met will simply proceed with c k n 1 c k n note that because we are using an explicit time advance with a cfl 1 limitation it is guaranteed that the concentration error associated with neglecting transport in any element is δ u that is if we compute the transport between all elements any element whose neighbors are within δ u cannot change by more than that value in a conservative slope limited transport scheme some observations on our implementation of the lft scheme each species has its own active domain however the algorithm could be modified to treat combinations of species e g by considering sums of concentrations setting the δ u 0 yields the same results as the standard transport scheme which means our method is fully consistent implemented as shown in the appendix our scheme guarantees full local conservation of mass for each constituent this can be seen using a simple observation since our dg discretization utilizes the conservative or divergence form of the 2d shallow water and transport equations the computation of edge fluxes is the only potential source of mass error the uniqueness of the solution to the riemann problem see eq a 5 automatically provides the conservation for the exterior boundary edges and those interior edges that lie between two masked active elements no flux computation takes place on interior edges shared by two non mask elements this leaves fluxes on interior edges between masked and non masked elements as the only potential source of error if the flux contributions from such edges are accounted for on both adjoining elements as is the case in our implementation the scheme is fully mass conservative otherwise the missing flux terms result in a mass error this accounting for can be carried out in two different ways both of them fully mass conserving first setting the fluxes to zero which is equivalent to turning off the contribution from one edge on masked elements second treating these fluxes in the same way as fluxes between two active elements this would be equivalent to updating the affected non masked elements using a single edge contribution for performance reasons our implementation employs the first approach we use computed concentrations c m rather than the evolved depth integrated concentrations c m h of eq 3 if the latter were used the changes in the water depth h could produce large gradients in areas of constant local concentration and vice versa the ideas are readily generalized to diffusion and reaction operators but for simplicity we confine ourselves to advection terms in this study another important generalization of the presented methodology can be made in the context of 3d modeling since the 3d circulation and transport is often marked by a clear anisotropy between horizontal and vertical directions the lft framework would greatly benefit from introducing separate masks for vertical and horizontal transport both of them fully three dimensional i e working on the 3d mesh this can be accomplished similarly to introducing separate masks for different species as illustrated in sec 5 2 4 npz model to demonstrate the effectiveness of the lft method we solve the shallow water equations coupled to an npz ecosystem model of nitrogen phytoplankton and zooplankton dynamics franks 2002 wroblewski et al 1988 the system is governed by eqs 1 3 with m 3 and reaction equations r 1 k i n i p p h p z r 2 1 γ h p j z z r 3 γ h p j z z k i n i p p where n p z are concentrations of nitrogen phytoplankton and zooplankton respectively our npz model is similar to wroblewski et al 1988 with some minor modifications as discussed below a summary of all model coefficients is provided in table 2 note that in some sophisticated models these coefficients can be functions of the ecosystem state but herein are taken as constants for simplicity the following provides a brief overview of the model equations the reader is referred to franks 2002 wroblewski et al 1988 for more in depth discussions the phytoplankton response k i n is the product of the responses to light i and nutrients n in this case k i n f i g mm n where f i is the depth averaged phytoplankton response to irradiance and g mm is the michaelis menten uptake herein we use the saturating response model for light c f franks 2002 rinke et al 2010 4 f i i exp k ex ξ ζ i 0 i exp k ex ξ ζ where i 0 is the half saturation constant for photosynthetically active radiation par i is the incident par and k ex is the light extinction coefficient here ζ is the vertical coordinate with respect to datum as is common g mm n is specified as g mm n v m n k s n where k s is the nutrient uptake half saturation constant and v m is the maximum phytoplankton growth rate the grazing of zooplankton on phytoplankton h p is given by h p r m 1 exp λ p where r m is the maximum herbivore ingestion rate and λ is ivlev s constant for herbivore grazing finally the death rates for both plankton types are simple constants i p ε j z δ in general local phytoplankton growth depends on the local light intensity which depends on the water depth however our 2d hydrodynamic model uses depth averaged concentrations so for consistency we use a depth averaged response to irradiance f i for further simplification we neglect the relatively minor motions of the free surface and use the water depth at rest for the computation of f i similarly to wroblewski et al 1988 we also do not account for seasonal and diel variations in light intensity although our approach is clearly too simplistic for an accurate ecosystem model it will suffice for a demonstration of how the lft method applies to an advection diffusion reaction problem we use galveston bay texas usa as a test case in sec 5 2 and computed the value of i in table 2 using the noaa long term mean radiation value for this area during april may and june which has been multiplied by 0 4 to obtain the par fraction of shortwave radiation six and maier reimer 1996 to compute the depth averaged phytoplankton response to light we integrate eq 4 over the water column obtaining f i 1 b b 0 f i d ζ 1 k ex b log i 0 i i 0 i exp k ex b 5 numerical results 5 1 simple plume motion to demonstrate that localized transport only within the active domain does not affect the computed concentration fields we use a standard orbital transport of a concentration without reaction terms in this problem a concentration plume is transported in a circular orbit around the domain center by a fixed in time velocity field the domain ω is a unit square uniformly partitioned into 8192 triangular elements and the initial plumes are circular the forcing function f on right hand side of eq 2 is chosen to produce a stationary velocity field given by u x y 0 5 y x 0 5 t over the time interval 0 2 π corresponding to a full circle rotation of the initial scene two different configurations are considered with the initial concentration consisting of one fig 1 and four fig 2 individual plumes respectively in addition each configuration is further subdivided into four test cases such that the area of the plume is doubled in each succeeding case the initial concentration is equal to one inside the plume and zero outside of it we compare results to control cases without the lft algorithm and examine effects of using δ u c of 10 6 and 10 8 as the discriminator for the active domain table 3 details the radii of the plumes and lists the average percentages of elements in the active domain in relation to the total number of elements note that in figs 1 and 2 the upper panel initial conditions show the active domain computational mesh is only around the edges of the plume and not in the plume center or the empty domain which have uniform concentrations relative to δ u as the plume undergoes minor numerical diffusion during its orbit the computational mesh evolves lower panels to include the entire plume as well as a diffusion area surrounding it where concentration gradients develop again relative to δ u fig 3 shows how the fraction of the domain in active computation evolves over time as the plume diffuses during its orbit since the analytical solution at final time t 2 π is equal to the initial condition the discretization error can be computed as the difference between the final solution and the initial condition the l 1 errors for all tests are listed in table 4 they appear to be dominated by the numerical diffusion of the transport scheme and display virtually no sensitivity to the lft approximation since the actual speed up achieved by the lft scheme is strongly dependent on the programming language the discrete transport scheme implementation and data structures in the code the runtimes displayed in table 4 should be only used as an indicator supplementing a more rigorous metric given in table 3 and fig 3 by the fraction of the domain that is active these runtimes were obtained by averaging over three separate runs and can be considered reasonably reliable since runtime variations between runs were well below 1 columns titled transport in table 4 detail the time spent in the transport scheme exempt from the slope limiter that took ca 200 s in all runs whereas columns under heading total list wall times of the entire simulation for this test case we observe between 10 and 55 of speed up in the transport part depending on the size of the active domain to illustrate our claim that the lft scheme is fully mass conservative we plot the development of the mass error over time in fig 4 the lft mass errors bottom turn out to be in the round off range just as in the transport scheme without lft top the results for δ u 10 6 turned out to be nearly identical to those for δ u 10 8 and are not shown here 5 2 reactive transport in galveston bay galveston bay texas usa was chosen as the setting for demonstrating the performance of the lft method with the npz model of sec 4 this bay is challenging for hydrodynamic and transport modeling due to its complex geometry which includes 17 islands and the narrow houston ship channel this channel has steep sides and is three times deeper than typical depths throughout the bay all simulations were performed on an unstructured triangular mesh consisting of 3397 elements as illustrated in fig 5 to provide meaningful initial conditions for the velocity and water elevation a cold start simulation zero initial velocities and initially flat water surface was run for 10 days as a model spin up period test simulations were all started from this 10 day data and continued for 90 days with a constant time step of 5 s the only river inflow we included is the san jacinto river which is modeled for simplicity as a constant inflow with a flux corresponding to velocity of 0 5 m s 1 at the northernmost river element the southernmost boundary is considered open sea with tidal elevations imposed for 5 tidal components o1 k1 n2 m2 s2 the remaining boundary edges are land no flux boundaries wind forcing is neglected in all the model runs the initial conditions for the transport model are constant 10 6 k g m 3 phyto zoo plankton concentrations on a few elements in the bay far away from the open sea boundary and not intersecting each other see fig 6 with zero concentrations in the remainder of the domain the initial values were chosen with the intent to prevent the maximum concentrations of both planktons exceeding 5 10 6 kg m3 at any time as suggested in carlson 1977 the initial nitrogen concentrations are zero everywhere with a constant nitrogen concentration of 4 10 4 kg m 3 as suggested in gruber 2008 as a boundary condition for the san jacinto river inflow note that in this idealized case the use of zero values as background conditions means that the model is only representing the plankton growth death involving the intersection of the initial patches and the river nitrogen flux thus the model is not illustrative of expected real ecosystem processes across the bay simulations were conducted with the new lft transport algorithm and a conventional non lft transport algorithm as a control for the lft simulations the δ u to discriminate between active and inactive regions were selected as δ u n 10 10 kg m 3 δ u p 10 13 kg m 3 and δ u z 10 13 kg m 3 the δ u used herein were six and seven orders of magnitude below the boundary and initial concentrations for nitrogen and plankton respectively i e close to numerical zero for the concentration magnitudes involved the appropriate setting of δ u for each constituent depends on the expected accuracy and uncertainty allowable for the scalar transport and cannot be a priori defined from the lft method itself for simple passive scalars the allowable δ u will be associated with concentrations and concentration gradients that are deemed important by the modeler for ecosystem models the nonlinear interaction algorithms between constituents will need to be considered it seems likely that δ u might be substantially higher than applied herein given the typical uncertainty in the coefficients of the reaction equations in ecosystem models for example arguably the practical δ u p can be set simply to ensure the numerical death or growth of phytoplankton attributable to error in the lft field is one order of magnitude smaller than the uncertainty in the death growth rates associated with uncertainties in the zooplankton grazing rate phytoplankton death rate and nutrient uptake coefficients note that increasing δ u serves to decrease the number of elements in the computational domain increasing computational efficiency but also increases the error in the concentration field thus a particular application might obtain dramatic increases in computational efficiency where larger errors are acceptable complex ecosystem models can probably be exercised for simple test cases to determine effective settings for δ u values the upper panels of figs 7 and 8 as well as fig 9 illustrate concentrations kg m 3 of each species in the lft model and the active domain after 15 30 45 60 75 and 90 days exponential color scales are used where appropriate the lower panels of figs 7 and 8 show the magnitude of the difference between the lft and conventional transport simulations for nitrogen and phytoplankton the difference plot of zooplankton is qualitatively similar to that of phytoplankton and is omitted the differences between the lft method and the control are several orders of magnitude lower than the corresponding concentrations themselves thus with appropriate selection of δ u the results from the lft method are sufficiently similar to those of the conventional transport method for typical ecosystem simulations the performance of the lft algorithm is quantified in fig 10 where the percentage of elements in the active domain and the l error for each species are plotted as functions of time once again even in this particularly sensitive norm the results do not differ much from those of the control simulations using the conventional transport method as might be expected a constituent that is continuously and widely distributed such as nitrogen after day 20 provides little opportunity for improving computational efficiency however it can be seen that the phytoplankton and zooplankton behaviors can allow dramatic improvements indeed beyond day 50 for phytoplankton and day 60 for zooplankton the lft transport computations required for these constituents are near zero as indicated by the disappearing mesh in the upper frames of figs 8 and 9 it follows that effective implementation of the lft should consider the likely distribution of a given constituent and values for δ u that reflect the importance of gradients across the ecosystem the small oscillations in fig 10 do not reflect incipient instability but instead are the natural consequence of the small differences that occur as new cells are activated deactivated in the computational mask to illustrate the effect of the lft method on the computational efficiency in the setting of the reactive transport model we list runtimes of main algorithm parts in table 5 as well as the mean percentages of elements in the active domain for each species the achieved speed up of ca 36 in the transport part of the model or ca 20 of the total computation time is substantial for such a small number of species however the actual size of the speed up if any will vary strongly with the implementation programming framework etc 6 discussion conclusions a new locally filtered transport method is proposed and implemented in the transport routines of a discontinuous galerkin hydrodynamic model coupled to an npz ecosystem model the lft method limits transport to active regions which are defined based on concentration differences between local computational elements if for a given constituent the local concentration differences between element values and those of its neighbors are smaller than a pre defined tolerance δ u the advection on the element is not computed for that constituent i e advection in out is considered sufficiently balanced and cannot change the local concentration this approach requires time space varying of the active transport regions which are different for different constituents which does not introduce significant errors for sufficiently small δ u thus the lft method can be used to decrease the number of computational elements where the discrete transport terms are evaluated both the local error induced by the lft method and the resulting improvement in computational efficiency are directly related to the choice of δ u it follows that where larger errors are acceptable greater computational efficiencies can be achieved for comprehensive ecosystem models where constituent transport dominates the computational time and there are large uncertainties in the source sink components of reaction terms the lft method has the potential to significantly decrease the overall computational costs without degrading the model validity the lft method does not come without some overhead thus in situations when some or all constituents vary significantly in large parts of the computational domain and over long simulation times the overall efficiency may not get improved or can even degrade another important aspect related to the implementation of the lft method is the choice of the uniformity indicator the slope limiter as an integral part of the dg discretization offers a particularly efficient and elegant solution to this problem but a number of alternative techniques could be easily used in its stead providing a great degree of flexibility for large classes of numerical schemes an extension of the proposed framework to 3d would be rather straightforward for many types of passive tracers however a more sophisticated approach might be called for in the case of biogeochemical or sediment transport simulations the natural anisotropy between the horizontal and vertical transport mechanisms different boundary conditions stratification effects may require a clear separation between the horizontal and vertical lft schemes by introducing separate vertical and horizontal masks similar to those used for different constituents of the npz model our methodology can be readily extended to the 3d case the lft method and its predecessor background filtering for mass transport hodges 2014 are ideas that are still in early development and exploration herein we have provided the mathematical foundations for a mass conservative approach with conventional hydrodynamic and transport algorithms demonstrating the full potential of the method to improve computational efficiency of scalar transport remains a subject for future exploration with a more complex ecosystem application in particular there is a need for a consistent and effective method for selecting the uniformity difference δ u as a function of transport scales and reaction coefficients and predicting the speed up that can be achieved at the present selecting an effective δ u is more art than science acknowledgments hodges was partially supported by the us national science foundation under grant nos 1331610 and 1417433 and the research and development program of the texas general land office oil spill prevention and response division under grant no 16 098 000 9290 appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 01 003 appendix a discretization appendix a 1 notation prior to describing our dg scheme for eqs 1 3 we introduce some notation let t h t be a regular family of non overlapping partitions of ω into k closed triangles t of characteristic size h such that ω t for t t h let ν t denote the unit normal on t exterior to t let e ω denote the set of interior edges e ω the set of boundary edges and e e ω e ω e the set of all edges the subscript h is suppressed here for an interior edge e e ω shared by triangles t and t we define the one sided values of a scalar quantity w w x on e by w x lim ε 0 w x ε ν t and w x lim ε 0 w x ε ν t respectively for an edge e e ω only the first definition is meaningful appendix a 2 conservative equations for compact notation we introduce a new set of unknowns q s q t ξ u v s t c 1 h c m h and rewrite eqs 1 3 as a 1 t q b 1 q m 1 q a 2 t s b 2 q s m 2 q s with b 1 q u v u 2 h g ξ ξ 2 b u v h v u h v 2 h g ξ ξ 2 b b 2 q s u s 1 h v s 1 h u s m h v s m h m 1 q 0 h f x g ξ x b τ b f u f c v h f y g ξ y b τ b f v f c u m 2 q s h r 1 s h h f 1 h r m s h h f m appendix a 3 semi discrete formulation although our dg implementation of the shallow water and transport equations is very general and currently supports polynomial discretization spaces of orders zero to four we only use piecewise linear discretization in this work thus we denote by ℙ 1 t the space of polynomials of degree at most 1 on each element t t h let ℙ 1 t h w h ω ℝ t t h w h t ℙ 1 t denote the broken polynomial space on the triangulation t h for the semi discrete formulation we assume that the coefficient functions for any fixed t 0 t end or their components in case of vector valued coefficients are approximated in this space similarly to frank et al 2015b we use the standard l 2 projection into ℙ 1 t to compute these approximations because of the local nature of the dg method we can formulate the semi discrete system of equations on an element by element basis by multiplying eqs a 1 and a 2 with some test functions ψ h ℙ t h 3 ϕ h ℙ t h m and integrating the advective terms by parts over element t t h then our semi discrete formulation reads seek q h t s h t ℙ 1 t h 3 m such that the following holds for any t 0 t end and t t h ψ h ℙ 1 t h 3 ϕ h ℙ 1 t h m a 3 t ψ h t q h d x t ψ h b 1 q h d x t ψ h b 1 q h ν t ˆ d s t ψ h m 1 q h d x a 4 t ϕ h t s h d x t ϕ h b 2 q h s h d x t ϕ h b 2 q h s h ν t ˆ d s t ϕ h m 2 q h s h d x due to the discontinuous nature of a dg approximation particular care has to be taken when computing the boundary fluxes the specific way to perform this task has critical impact on the stability accuracy and conservation properties of the numerical scheme in eq a 3 these fluxes are obtained as solutions for local riemann problems using the lax friedrichs approximation aizinger 2004 combined with the roe pike averaging roe and pike 1985 a 5 b 1 q h ν t ˆ 1 2 b 1 q h b 1 q h ν t λ ˆ q h q h with adding h subscripts to indicate discrete unknowns λ ˆ u ˆ v ˆ ν t g h h 3 2 h h 3 2 h h h h u ˆ v ˆ h h u h h h u h h h h h h h h h h h v h h h v h h h h h h h h h on an exterior boundary edge q h is defined as follows ξ u v ξ u 2 u ν x v ν y ν x v 2 u ν x v ν y ν y on land boundaries ξ d u d v d t on river boundaries ξ d u v t on open boundaries in our modified transport algorithm concentrations c m i 1 m are used in addition to the depth integrated concentrations s m even though the latter are employed as the primary unknowns in eq a 2 values of c h are obtained by dividing each component of s h by h h and using the l 2 projection into space ℙ 1 t h to ensure the mass conservation of transported components the boundary fluxes in the discrete versions of eqs 1 and 3 have to agree denoting by u ν the first component of b 1 q h ν t ˆ we can now use it to define the upwind boundary fluxes in eq a 4 as b 2 q h x s h x ν t ˆ u ν t x c h x if u ν t x 0 u ν t x c h x if u ν t x 0 and x ω u ν t x c d x if u ν t x 0 and x ω note that on land boundaries u ν is equal to zero therefore b 2 q h s h ν t ˆ is zero there as well appendix a 4 local basis representation the semi discrete formulation in eqs a 3 a 4 is discretized using an orthonormal with respect to the standard l 2 scalar product piecewise linear dg basis resulting in a time dependent system of equations for the degrees of freedom as a first step all functions and unknowns are expressed in terms of a local basis φ k 1 φ k 2 φ k 3 of ℙ 1 t k as described in frank et al 2015b reuter et al 2016 jaust et al 2017 for a fixed t k t h the basis functions defined in the reference coordinate system x ˆ y ˆ on the unit reference triangle t ˆ are a 6 φ ˆ 1 x ˆ 2 φ ˆ 2 x ˆ 2 6 x ˆ 1 φ ˆ 3 x ˆ 2 3 1 x ˆ 1 2 x ˆ 2 thus a scalar function a h ℙ 1 t h can be locally written as a h x t k i 1 3 a k i φ k i x where a k i denote the degrees of freedom of the representation of a h in the local dg basis the latter is obtained as affine linear transformation of the reference basis given in eq a 6 appendix a 5 time discretization and slope limiting time discretization of eqs a 3 and a 4 uses an explicit runge kutta method gottlieb and shu 1998 of order two that belongs to the class of ssp strong stability preserving methods gottlieb et al 2001 this type of time stepping method preserves the monotonicity of the solution if it is postprocessed by a slope limiter details on how these methods are implemented can be found in reuter et al 2016 let 0 t 1 t 2 t end be a not necessarily equidistant decomposition of the time interval 0 t and let δ t n t n 1 t n denote the time step size system a 3 a 4 can be compactly written in operator form as a 7 m q t q h t l q t q h m s t s h t l s t q h s h where q h s h are the vectors of degrees of freedom associated with q h s h m q m s are the corresponding mass matrices and l q l s are the non linear operators comprising terms not involving time derivatives in eqs a 3 and a 4 the update scheme of the runge kutta method is then given by a 8 q h 0 s h 0 q h n s h n q h 1 s h 1 q h 0 s h 0 δ t n m q 1 l q t n q h 0 m s 1 l s t n q h 0 s h 0 q h 2 s h 2 1 2 q h n s h n 1 2 q h 1 s h 1 δ t n 2 m q 1 l q t n 1 q h 1 m s 1 l s t n 1 q h 1 s h 1 q h n 1 s h n 1 q h 2 s h 2 where we abbreviated q h n q h t n etc to preserve the monotonicity of the solution and to prevent the appearance of negative concentrations the numerical solution has to be postprocessed using a slope limiter the methods used here are vertex based limiters proposed by kuzmin in kuzmin 2010 and aizinger in aizinger 2011 and further generalized in kuzmin 2013 reuter et al 2016 their advantages include low numerical diffusion support of arbitrary element shapes and intuitive extensions to higher order dg discretizations and anisotropic problems aizinger et al 2017 since the limiters play an essential role in our modified transport algorithm a short description is in order the common idea underlying all limiters is to rely on the piecewise constant part of the discrete solution guaranteed to be monotonous to limit higher order degrees of freedom of the dg solution formulating the vertex based limiting scheme for a linear dg approximation is very simple for a hierarchical basis e g taylor kuzmin 2010 or orthogonal aizinger 2011 since our choice of basis eq a 6 certainly falls under this description we can write our linear solution locally on element t k as a h x t k a k 1 φ k 1 x a k 2 φ k 2 x a k 3 φ k 3 x the basis functions defined in eq a 6 or rather their mappings to element t k are supported only on element t k and are orthonormal with respect to the l 2 scalar product on t k in particular φ k 2 and φ k 3 are orthogonal to any constant function and by construction φ k 1 t k 1 2 holds where t k t k 1 d x is the area of t k thus a k 1 t k t k a h x d x a k 1 t k 1 2 where a k denotes the integral mean of a h on t k therefore any changes in the linear degrees of freedom a k 2 and a k 3 do not affect the local conservation properties of the solution since the latter only depends on a k 1 by adjusting coefficients a k 2 a k 3 in an appropriate way the slope limiting procedure produces a piecewise linear dg solution fulfilling the maximum principle we seek the maximum admissible value of coefficient θ k 0 θ k 1 in a h x t k a k 1 φ k 1 x θ k a k 2 φ k 2 x a k 3 φ k 3 x here θ k 1 corresponds to an unlimited solution and θ k 0 reduces the solution to its piecewise constant component the correction factor θ k is chosen as a maximum admissible value so that the above reconstruction is bounded in all vertices x i t k by the minimum and maximum mean solution values of a h on all elements containing x i see fig a 11 a 9 a i min a h x i t k a i max i x i is vertex of t k where a i min min t k a k a i max max t k a k x i is vertex of t k to enforce eq a 9 the correction factor θ k is defined as in kuzmin 2010 θ k min x i is vertex of t k a i max a k a h x i t k a k if a h x i t k a i max a i min a k a h x i t k a k if a h x i t k a i min 1 otherwise alternatively the same θ k can be obtained by solving a one dimensional optimization problem as shown in aizinger 2011 fig a 11 patch of elements red involved in the computation of limits a i min max at vertex x i of a single element t k fig a 11 the fact that our slope limiting procedure must be performed on the values on concentration unknowns c m m 1 m instead of the depth integrated concentrations s m that serve as the primary unknowns in transport eq a 2 suggests the following structure of each substep of the time stepping algorithm given in eq a 8 s h c h π 1 s h h h s h lim π 1 λ c h h h where π 1 denotes the l 2 projection into broken piecewise linear polynomial space ℙ 1 t h and λ the slope limiting operator this procedure must be performed after each time substep in eq a 8 and replaces values of s h i i 1 2 there with their limited counterparts the values of a i min and a i max for each species calculated in the course of our limiting procedure can be then employed in the modified transport scheme described in sec 3 the above scheme is computationally efficient compared to the region identification approach used in the subtime stepping work of hodges 2014 
26424,this work introduces the locally filtered transport lft method for numerical transport models locally turning off the transport computation in areas of nearly uniform concentration is proposed as a new approach for reducing computational cost in ecosystem models that require transport of tens to hundreds of constituent concentrations the proposed method is locally mass conservative just as the discontinuous galerkin finite element scheme it is based on the performance of the method is illustrated using numerical examples including an advection reaction ecosystem simulation with a simple nitrogen phytoplankton and zooplankton npz model keywords discontinuous galerkin finite element method shallow water equations multi component advection reaction system npz model transport scheme 1 introduction this work introduces and evaluates a new computationally efficient scheme for transport equations in multi component advection diffusion reaction models the idea behind our method is quite simple many physical chemical and biological processes take place on highly localized spatial and temporal scales such that one or more transported constituents might be at quasi uniform or background concentrations over large areas for example outside of a localized algae bloom the chlorophyll concentration in a water quality model is typically at some background level such that the same small concentration is fluxed in and out of most computational cells these computational cycles of the transport equations are wasted and cannot affect the model results until the reaction equations initiate local growth of a bloom thus an ecosystem model coupled to a large scale circulation model e g regional or global ocean climate incurs substantial computational costs for transport in parts of the spatio temporal domain where some or all of the constituents are not present have only the background concentration or do not play a significant role in the reactions for simple ecosystem models implemented in only two dimensions 2d such as the nitrogen phytoplankton zooplankton npz model used herein for demonstration purposes the increased computational costs of transporting a few scalars is generally irrelevant however for ecosystem models that transport different species of plankton and include chemical speciation no3 nh4 dissolved inorganic carbon dissolved organic carbon etc the number of transported variables can easily be several dozen or more e g robson and hamilton 2004 schwalb et al 2015 in three dimensions 3d the extensive scalar transport requirements can dominate the overall computational time arguably such models are computationally inefficient as they are not generally designed to identify and transport constituents only when and where they are significant in this study we demonstrate how to add such capability to an existing hydrodynamic transport model with an approach we call locally filtered transport lft we propose the new lft algorithm that adaptively turns on off the computation of certain discrete terms the model performance is evaluated using a conventional hydrostatic inviscid 2d shallow water and transport model based on the discontinuous galerkin dg finite element method aizinger 2004 aizinger and dawson 2002 the present work builds on the background filtering approach hodges 2014 which required an unconventional mass transport algorithm herein we show that localization techniques can be efficiently extended to standard concentration transport schemes furthermore the computational costs associated with localization that were identified in hodges 2014 are elegantly handled within the dg framework using the vertex based slope limiter aizinger 2011 kuzmin 2010 the utility of the proposed approach is not limited to discontinuous galerkin methods or geophysical applications it can be easily transferred to any numerical pde partial differential equation solver containing transport equations and might achieve meaningful performance gains even in the absence of reaction terms or in situations when only a few species are transported this paper is structured as follows the system of governing equations is introduced in sec 2 followed by a description of the lft method in sec 3 an npz ecosystem model is presented in sec 4 which is used as a test case for the lft method combined with a dg hydrodynamic transport model in sec 5 for completeness and to allow others to build on the modeling approach the details of dg discretization are provided in appendix a a brief discussion and conclusions section completes the paper 2 governing equations the model problem for this study is the 2d shallow water equations in conservative form eqs 1 and 2 below combined with a varying number of equations for advection reaction represented by eq 3 below and augmented as needed by the corresponding initial and boundary conditions 1 t ξ u h 0 2 t u h u u h g h ξ f c k u h τ b f u h h f 3 t c m h u h c m h r m c 1 c m h f m m 1 m the primary unknowns in eqs 1 3 are the water surface elevation ξ measured from a uniform datum the depth integrated horizontal velocity vector u h u v t and the depth integrated concentrations of multiple transported species c m h m 1 m given a boundary condition of spatially varying bathymetry elevation b x y the auxiliary variable h denotes the total water depth ξ b all equations are required to hold on some lipschitz bounded 2d domain ω and on time interval 0 t end furthermore g denotes gravity f c is the coriolis coefficient k is the vertical unit vector pointing upwards τ b f is the coefficient of the quadratic friction law f f x f y lumps together the forcing terms in the momentum equation e g tidal potential and f m r m m 1 m are the source sink terms and reaction rates in advection reaction equations respectively with the exception of the reaction terms the model is very similar to aizinger and dawson 2002 where the dg method was proposed for the 2d shallow water equations combined with non reactive species transport equations 1 and 2 utilize three types of boundary conditions land river open sea denoted by γ l γ r γ s ω respectively while eq 3 for constituent transport may have boundaries that are inflow outflow or wall no flow in this work a river boundary is always an inflow boundary a land boundary is always a wall boundary and open sea boundaries are dynamically switched between the in outflow modes depending on the flow direction these conditions are presented formally in table 1 3 locally filtered transport the lft method relies on definition of an individualized active domain for each transported constituent that is a subset of the total domain ω each active domain evolves over time as advection diffusion and reactions change the constituent concentration outside of the active domain the advection terms are ignored we consider two cases first as discussed in the introduction there are regions of a computational domain over which some constituent might fall below a dynamically meaningful concentration and may safely be ignored second there is the possibility of regions with nearly uniform concentrations where transport is merely moving the same concentration about with no effect on the local distribution for example far away from the influence of estuaries a large scale ocean model that is not resolving salinity effects for meso scale features might be simply shuffling around minuscule changes in salinity over the majority of the domain a modeler might want to retain salinity transport for estuarine input to the coastal shelf but it could be safely excised from the majority of the domain both cases can be addressed by monitoring the local concentration difference herein we define a uniformity difference or δ u for each constituent such that concentration differences between two neighboring grid elements smaller than δ u allow the concentration to be considered locally uniform so that no transport computation is needed this general idea was introduced in hodges 2014 as part of an algorithm using mass transport rather than concentration transport to allow local subtime stepping in regions where high velocities strictly limit the local advective time step the prior methods had relatively high computational costs due to the approach taken to identify the active domain in the present work we adapt the background filtering from hodges 2014 to the dg algorithm using uniform time steps and a conventional concentration transport discretization although the lft idea can be extended to any model the slope limiting dg method aizinger 2011 kuzmin 2010 has a particular advantage in that the majority of the computational effort for localization is already required in the existing transport algorithm namely by the slope limiting function see appendix a 5 which identifies and sorts the local concentration differences to maintain monotonicity the discrete time advance of eq 3 for constituent transport is described in detail in appendix a 5 for purposes of the lft method the key point is that the advance from time level t n to t n 1 uses an explicit runge kutta scheme with a slope limiter at time t n the minimum and maximum values of a transported constituent for each computational element t k that is connected to a node x i are known from the slope limiter of the time advance from the n 1 to n step i e a i min and a i max are already defined at each vertex see eq a 9 we use this time lagged data to find the maximum nodal jump of scalar field a for element t k and its nodal neighbors as ξ k max a i max a i min x i is vertex of t k it then follows that the only elements t k included in the runge kutta time advance for a given constituent are those where ξ k δ u all elements where the δ u condition is not met will simply proceed with c k n 1 c k n note that because we are using an explicit time advance with a cfl 1 limitation it is guaranteed that the concentration error associated with neglecting transport in any element is δ u that is if we compute the transport between all elements any element whose neighbors are within δ u cannot change by more than that value in a conservative slope limited transport scheme some observations on our implementation of the lft scheme each species has its own active domain however the algorithm could be modified to treat combinations of species e g by considering sums of concentrations setting the δ u 0 yields the same results as the standard transport scheme which means our method is fully consistent implemented as shown in the appendix our scheme guarantees full local conservation of mass for each constituent this can be seen using a simple observation since our dg discretization utilizes the conservative or divergence form of the 2d shallow water and transport equations the computation of edge fluxes is the only potential source of mass error the uniqueness of the solution to the riemann problem see eq a 5 automatically provides the conservation for the exterior boundary edges and those interior edges that lie between two masked active elements no flux computation takes place on interior edges shared by two non mask elements this leaves fluxes on interior edges between masked and non masked elements as the only potential source of error if the flux contributions from such edges are accounted for on both adjoining elements as is the case in our implementation the scheme is fully mass conservative otherwise the missing flux terms result in a mass error this accounting for can be carried out in two different ways both of them fully mass conserving first setting the fluxes to zero which is equivalent to turning off the contribution from one edge on masked elements second treating these fluxes in the same way as fluxes between two active elements this would be equivalent to updating the affected non masked elements using a single edge contribution for performance reasons our implementation employs the first approach we use computed concentrations c m rather than the evolved depth integrated concentrations c m h of eq 3 if the latter were used the changes in the water depth h could produce large gradients in areas of constant local concentration and vice versa the ideas are readily generalized to diffusion and reaction operators but for simplicity we confine ourselves to advection terms in this study another important generalization of the presented methodology can be made in the context of 3d modeling since the 3d circulation and transport is often marked by a clear anisotropy between horizontal and vertical directions the lft framework would greatly benefit from introducing separate masks for vertical and horizontal transport both of them fully three dimensional i e working on the 3d mesh this can be accomplished similarly to introducing separate masks for different species as illustrated in sec 5 2 4 npz model to demonstrate the effectiveness of the lft method we solve the shallow water equations coupled to an npz ecosystem model of nitrogen phytoplankton and zooplankton dynamics franks 2002 wroblewski et al 1988 the system is governed by eqs 1 3 with m 3 and reaction equations r 1 k i n i p p h p z r 2 1 γ h p j z z r 3 γ h p j z z k i n i p p where n p z are concentrations of nitrogen phytoplankton and zooplankton respectively our npz model is similar to wroblewski et al 1988 with some minor modifications as discussed below a summary of all model coefficients is provided in table 2 note that in some sophisticated models these coefficients can be functions of the ecosystem state but herein are taken as constants for simplicity the following provides a brief overview of the model equations the reader is referred to franks 2002 wroblewski et al 1988 for more in depth discussions the phytoplankton response k i n is the product of the responses to light i and nutrients n in this case k i n f i g mm n where f i is the depth averaged phytoplankton response to irradiance and g mm is the michaelis menten uptake herein we use the saturating response model for light c f franks 2002 rinke et al 2010 4 f i i exp k ex ξ ζ i 0 i exp k ex ξ ζ where i 0 is the half saturation constant for photosynthetically active radiation par i is the incident par and k ex is the light extinction coefficient here ζ is the vertical coordinate with respect to datum as is common g mm n is specified as g mm n v m n k s n where k s is the nutrient uptake half saturation constant and v m is the maximum phytoplankton growth rate the grazing of zooplankton on phytoplankton h p is given by h p r m 1 exp λ p where r m is the maximum herbivore ingestion rate and λ is ivlev s constant for herbivore grazing finally the death rates for both plankton types are simple constants i p ε j z δ in general local phytoplankton growth depends on the local light intensity which depends on the water depth however our 2d hydrodynamic model uses depth averaged concentrations so for consistency we use a depth averaged response to irradiance f i for further simplification we neglect the relatively minor motions of the free surface and use the water depth at rest for the computation of f i similarly to wroblewski et al 1988 we also do not account for seasonal and diel variations in light intensity although our approach is clearly too simplistic for an accurate ecosystem model it will suffice for a demonstration of how the lft method applies to an advection diffusion reaction problem we use galveston bay texas usa as a test case in sec 5 2 and computed the value of i in table 2 using the noaa long term mean radiation value for this area during april may and june which has been multiplied by 0 4 to obtain the par fraction of shortwave radiation six and maier reimer 1996 to compute the depth averaged phytoplankton response to light we integrate eq 4 over the water column obtaining f i 1 b b 0 f i d ζ 1 k ex b log i 0 i i 0 i exp k ex b 5 numerical results 5 1 simple plume motion to demonstrate that localized transport only within the active domain does not affect the computed concentration fields we use a standard orbital transport of a concentration without reaction terms in this problem a concentration plume is transported in a circular orbit around the domain center by a fixed in time velocity field the domain ω is a unit square uniformly partitioned into 8192 triangular elements and the initial plumes are circular the forcing function f on right hand side of eq 2 is chosen to produce a stationary velocity field given by u x y 0 5 y x 0 5 t over the time interval 0 2 π corresponding to a full circle rotation of the initial scene two different configurations are considered with the initial concentration consisting of one fig 1 and four fig 2 individual plumes respectively in addition each configuration is further subdivided into four test cases such that the area of the plume is doubled in each succeeding case the initial concentration is equal to one inside the plume and zero outside of it we compare results to control cases without the lft algorithm and examine effects of using δ u c of 10 6 and 10 8 as the discriminator for the active domain table 3 details the radii of the plumes and lists the average percentages of elements in the active domain in relation to the total number of elements note that in figs 1 and 2 the upper panel initial conditions show the active domain computational mesh is only around the edges of the plume and not in the plume center or the empty domain which have uniform concentrations relative to δ u as the plume undergoes minor numerical diffusion during its orbit the computational mesh evolves lower panels to include the entire plume as well as a diffusion area surrounding it where concentration gradients develop again relative to δ u fig 3 shows how the fraction of the domain in active computation evolves over time as the plume diffuses during its orbit since the analytical solution at final time t 2 π is equal to the initial condition the discretization error can be computed as the difference between the final solution and the initial condition the l 1 errors for all tests are listed in table 4 they appear to be dominated by the numerical diffusion of the transport scheme and display virtually no sensitivity to the lft approximation since the actual speed up achieved by the lft scheme is strongly dependent on the programming language the discrete transport scheme implementation and data structures in the code the runtimes displayed in table 4 should be only used as an indicator supplementing a more rigorous metric given in table 3 and fig 3 by the fraction of the domain that is active these runtimes were obtained by averaging over three separate runs and can be considered reasonably reliable since runtime variations between runs were well below 1 columns titled transport in table 4 detail the time spent in the transport scheme exempt from the slope limiter that took ca 200 s in all runs whereas columns under heading total list wall times of the entire simulation for this test case we observe between 10 and 55 of speed up in the transport part depending on the size of the active domain to illustrate our claim that the lft scheme is fully mass conservative we plot the development of the mass error over time in fig 4 the lft mass errors bottom turn out to be in the round off range just as in the transport scheme without lft top the results for δ u 10 6 turned out to be nearly identical to those for δ u 10 8 and are not shown here 5 2 reactive transport in galveston bay galveston bay texas usa was chosen as the setting for demonstrating the performance of the lft method with the npz model of sec 4 this bay is challenging for hydrodynamic and transport modeling due to its complex geometry which includes 17 islands and the narrow houston ship channel this channel has steep sides and is three times deeper than typical depths throughout the bay all simulations were performed on an unstructured triangular mesh consisting of 3397 elements as illustrated in fig 5 to provide meaningful initial conditions for the velocity and water elevation a cold start simulation zero initial velocities and initially flat water surface was run for 10 days as a model spin up period test simulations were all started from this 10 day data and continued for 90 days with a constant time step of 5 s the only river inflow we included is the san jacinto river which is modeled for simplicity as a constant inflow with a flux corresponding to velocity of 0 5 m s 1 at the northernmost river element the southernmost boundary is considered open sea with tidal elevations imposed for 5 tidal components o1 k1 n2 m2 s2 the remaining boundary edges are land no flux boundaries wind forcing is neglected in all the model runs the initial conditions for the transport model are constant 10 6 k g m 3 phyto zoo plankton concentrations on a few elements in the bay far away from the open sea boundary and not intersecting each other see fig 6 with zero concentrations in the remainder of the domain the initial values were chosen with the intent to prevent the maximum concentrations of both planktons exceeding 5 10 6 kg m3 at any time as suggested in carlson 1977 the initial nitrogen concentrations are zero everywhere with a constant nitrogen concentration of 4 10 4 kg m 3 as suggested in gruber 2008 as a boundary condition for the san jacinto river inflow note that in this idealized case the use of zero values as background conditions means that the model is only representing the plankton growth death involving the intersection of the initial patches and the river nitrogen flux thus the model is not illustrative of expected real ecosystem processes across the bay simulations were conducted with the new lft transport algorithm and a conventional non lft transport algorithm as a control for the lft simulations the δ u to discriminate between active and inactive regions were selected as δ u n 10 10 kg m 3 δ u p 10 13 kg m 3 and δ u z 10 13 kg m 3 the δ u used herein were six and seven orders of magnitude below the boundary and initial concentrations for nitrogen and plankton respectively i e close to numerical zero for the concentration magnitudes involved the appropriate setting of δ u for each constituent depends on the expected accuracy and uncertainty allowable for the scalar transport and cannot be a priori defined from the lft method itself for simple passive scalars the allowable δ u will be associated with concentrations and concentration gradients that are deemed important by the modeler for ecosystem models the nonlinear interaction algorithms between constituents will need to be considered it seems likely that δ u might be substantially higher than applied herein given the typical uncertainty in the coefficients of the reaction equations in ecosystem models for example arguably the practical δ u p can be set simply to ensure the numerical death or growth of phytoplankton attributable to error in the lft field is one order of magnitude smaller than the uncertainty in the death growth rates associated with uncertainties in the zooplankton grazing rate phytoplankton death rate and nutrient uptake coefficients note that increasing δ u serves to decrease the number of elements in the computational domain increasing computational efficiency but also increases the error in the concentration field thus a particular application might obtain dramatic increases in computational efficiency where larger errors are acceptable complex ecosystem models can probably be exercised for simple test cases to determine effective settings for δ u values the upper panels of figs 7 and 8 as well as fig 9 illustrate concentrations kg m 3 of each species in the lft model and the active domain after 15 30 45 60 75 and 90 days exponential color scales are used where appropriate the lower panels of figs 7 and 8 show the magnitude of the difference between the lft and conventional transport simulations for nitrogen and phytoplankton the difference plot of zooplankton is qualitatively similar to that of phytoplankton and is omitted the differences between the lft method and the control are several orders of magnitude lower than the corresponding concentrations themselves thus with appropriate selection of δ u the results from the lft method are sufficiently similar to those of the conventional transport method for typical ecosystem simulations the performance of the lft algorithm is quantified in fig 10 where the percentage of elements in the active domain and the l error for each species are plotted as functions of time once again even in this particularly sensitive norm the results do not differ much from those of the control simulations using the conventional transport method as might be expected a constituent that is continuously and widely distributed such as nitrogen after day 20 provides little opportunity for improving computational efficiency however it can be seen that the phytoplankton and zooplankton behaviors can allow dramatic improvements indeed beyond day 50 for phytoplankton and day 60 for zooplankton the lft transport computations required for these constituents are near zero as indicated by the disappearing mesh in the upper frames of figs 8 and 9 it follows that effective implementation of the lft should consider the likely distribution of a given constituent and values for δ u that reflect the importance of gradients across the ecosystem the small oscillations in fig 10 do not reflect incipient instability but instead are the natural consequence of the small differences that occur as new cells are activated deactivated in the computational mask to illustrate the effect of the lft method on the computational efficiency in the setting of the reactive transport model we list runtimes of main algorithm parts in table 5 as well as the mean percentages of elements in the active domain for each species the achieved speed up of ca 36 in the transport part of the model or ca 20 of the total computation time is substantial for such a small number of species however the actual size of the speed up if any will vary strongly with the implementation programming framework etc 6 discussion conclusions a new locally filtered transport method is proposed and implemented in the transport routines of a discontinuous galerkin hydrodynamic model coupled to an npz ecosystem model the lft method limits transport to active regions which are defined based on concentration differences between local computational elements if for a given constituent the local concentration differences between element values and those of its neighbors are smaller than a pre defined tolerance δ u the advection on the element is not computed for that constituent i e advection in out is considered sufficiently balanced and cannot change the local concentration this approach requires time space varying of the active transport regions which are different for different constituents which does not introduce significant errors for sufficiently small δ u thus the lft method can be used to decrease the number of computational elements where the discrete transport terms are evaluated both the local error induced by the lft method and the resulting improvement in computational efficiency are directly related to the choice of δ u it follows that where larger errors are acceptable greater computational efficiencies can be achieved for comprehensive ecosystem models where constituent transport dominates the computational time and there are large uncertainties in the source sink components of reaction terms the lft method has the potential to significantly decrease the overall computational costs without degrading the model validity the lft method does not come without some overhead thus in situations when some or all constituents vary significantly in large parts of the computational domain and over long simulation times the overall efficiency may not get improved or can even degrade another important aspect related to the implementation of the lft method is the choice of the uniformity indicator the slope limiter as an integral part of the dg discretization offers a particularly efficient and elegant solution to this problem but a number of alternative techniques could be easily used in its stead providing a great degree of flexibility for large classes of numerical schemes an extension of the proposed framework to 3d would be rather straightforward for many types of passive tracers however a more sophisticated approach might be called for in the case of biogeochemical or sediment transport simulations the natural anisotropy between the horizontal and vertical transport mechanisms different boundary conditions stratification effects may require a clear separation between the horizontal and vertical lft schemes by introducing separate vertical and horizontal masks similar to those used for different constituents of the npz model our methodology can be readily extended to the 3d case the lft method and its predecessor background filtering for mass transport hodges 2014 are ideas that are still in early development and exploration herein we have provided the mathematical foundations for a mass conservative approach with conventional hydrodynamic and transport algorithms demonstrating the full potential of the method to improve computational efficiency of scalar transport remains a subject for future exploration with a more complex ecosystem application in particular there is a need for a consistent and effective method for selecting the uniformity difference δ u as a function of transport scales and reaction coefficients and predicting the speed up that can be achieved at the present selecting an effective δ u is more art than science acknowledgments hodges was partially supported by the us national science foundation under grant nos 1331610 and 1417433 and the research and development program of the texas general land office oil spill prevention and response division under grant no 16 098 000 9290 appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 01 003 appendix a discretization appendix a 1 notation prior to describing our dg scheme for eqs 1 3 we introduce some notation let t h t be a regular family of non overlapping partitions of ω into k closed triangles t of characteristic size h such that ω t for t t h let ν t denote the unit normal on t exterior to t let e ω denote the set of interior edges e ω the set of boundary edges and e e ω e ω e the set of all edges the subscript h is suppressed here for an interior edge e e ω shared by triangles t and t we define the one sided values of a scalar quantity w w x on e by w x lim ε 0 w x ε ν t and w x lim ε 0 w x ε ν t respectively for an edge e e ω only the first definition is meaningful appendix a 2 conservative equations for compact notation we introduce a new set of unknowns q s q t ξ u v s t c 1 h c m h and rewrite eqs 1 3 as a 1 t q b 1 q m 1 q a 2 t s b 2 q s m 2 q s with b 1 q u v u 2 h g ξ ξ 2 b u v h v u h v 2 h g ξ ξ 2 b b 2 q s u s 1 h v s 1 h u s m h v s m h m 1 q 0 h f x g ξ x b τ b f u f c v h f y g ξ y b τ b f v f c u m 2 q s h r 1 s h h f 1 h r m s h h f m appendix a 3 semi discrete formulation although our dg implementation of the shallow water and transport equations is very general and currently supports polynomial discretization spaces of orders zero to four we only use piecewise linear discretization in this work thus we denote by ℙ 1 t the space of polynomials of degree at most 1 on each element t t h let ℙ 1 t h w h ω ℝ t t h w h t ℙ 1 t denote the broken polynomial space on the triangulation t h for the semi discrete formulation we assume that the coefficient functions for any fixed t 0 t end or their components in case of vector valued coefficients are approximated in this space similarly to frank et al 2015b we use the standard l 2 projection into ℙ 1 t to compute these approximations because of the local nature of the dg method we can formulate the semi discrete system of equations on an element by element basis by multiplying eqs a 1 and a 2 with some test functions ψ h ℙ t h 3 ϕ h ℙ t h m and integrating the advective terms by parts over element t t h then our semi discrete formulation reads seek q h t s h t ℙ 1 t h 3 m such that the following holds for any t 0 t end and t t h ψ h ℙ 1 t h 3 ϕ h ℙ 1 t h m a 3 t ψ h t q h d x t ψ h b 1 q h d x t ψ h b 1 q h ν t ˆ d s t ψ h m 1 q h d x a 4 t ϕ h t s h d x t ϕ h b 2 q h s h d x t ϕ h b 2 q h s h ν t ˆ d s t ϕ h m 2 q h s h d x due to the discontinuous nature of a dg approximation particular care has to be taken when computing the boundary fluxes the specific way to perform this task has critical impact on the stability accuracy and conservation properties of the numerical scheme in eq a 3 these fluxes are obtained as solutions for local riemann problems using the lax friedrichs approximation aizinger 2004 combined with the roe pike averaging roe and pike 1985 a 5 b 1 q h ν t ˆ 1 2 b 1 q h b 1 q h ν t λ ˆ q h q h with adding h subscripts to indicate discrete unknowns λ ˆ u ˆ v ˆ ν t g h h 3 2 h h 3 2 h h h h u ˆ v ˆ h h u h h h u h h h h h h h h h h h v h h h v h h h h h h h h h on an exterior boundary edge q h is defined as follows ξ u v ξ u 2 u ν x v ν y ν x v 2 u ν x v ν y ν y on land boundaries ξ d u d v d t on river boundaries ξ d u v t on open boundaries in our modified transport algorithm concentrations c m i 1 m are used in addition to the depth integrated concentrations s m even though the latter are employed as the primary unknowns in eq a 2 values of c h are obtained by dividing each component of s h by h h and using the l 2 projection into space ℙ 1 t h to ensure the mass conservation of transported components the boundary fluxes in the discrete versions of eqs 1 and 3 have to agree denoting by u ν the first component of b 1 q h ν t ˆ we can now use it to define the upwind boundary fluxes in eq a 4 as b 2 q h x s h x ν t ˆ u ν t x c h x if u ν t x 0 u ν t x c h x if u ν t x 0 and x ω u ν t x c d x if u ν t x 0 and x ω note that on land boundaries u ν is equal to zero therefore b 2 q h s h ν t ˆ is zero there as well appendix a 4 local basis representation the semi discrete formulation in eqs a 3 a 4 is discretized using an orthonormal with respect to the standard l 2 scalar product piecewise linear dg basis resulting in a time dependent system of equations for the degrees of freedom as a first step all functions and unknowns are expressed in terms of a local basis φ k 1 φ k 2 φ k 3 of ℙ 1 t k as described in frank et al 2015b reuter et al 2016 jaust et al 2017 for a fixed t k t h the basis functions defined in the reference coordinate system x ˆ y ˆ on the unit reference triangle t ˆ are a 6 φ ˆ 1 x ˆ 2 φ ˆ 2 x ˆ 2 6 x ˆ 1 φ ˆ 3 x ˆ 2 3 1 x ˆ 1 2 x ˆ 2 thus a scalar function a h ℙ 1 t h can be locally written as a h x t k i 1 3 a k i φ k i x where a k i denote the degrees of freedom of the representation of a h in the local dg basis the latter is obtained as affine linear transformation of the reference basis given in eq a 6 appendix a 5 time discretization and slope limiting time discretization of eqs a 3 and a 4 uses an explicit runge kutta method gottlieb and shu 1998 of order two that belongs to the class of ssp strong stability preserving methods gottlieb et al 2001 this type of time stepping method preserves the monotonicity of the solution if it is postprocessed by a slope limiter details on how these methods are implemented can be found in reuter et al 2016 let 0 t 1 t 2 t end be a not necessarily equidistant decomposition of the time interval 0 t and let δ t n t n 1 t n denote the time step size system a 3 a 4 can be compactly written in operator form as a 7 m q t q h t l q t q h m s t s h t l s t q h s h where q h s h are the vectors of degrees of freedom associated with q h s h m q m s are the corresponding mass matrices and l q l s are the non linear operators comprising terms not involving time derivatives in eqs a 3 and a 4 the update scheme of the runge kutta method is then given by a 8 q h 0 s h 0 q h n s h n q h 1 s h 1 q h 0 s h 0 δ t n m q 1 l q t n q h 0 m s 1 l s t n q h 0 s h 0 q h 2 s h 2 1 2 q h n s h n 1 2 q h 1 s h 1 δ t n 2 m q 1 l q t n 1 q h 1 m s 1 l s t n 1 q h 1 s h 1 q h n 1 s h n 1 q h 2 s h 2 where we abbreviated q h n q h t n etc to preserve the monotonicity of the solution and to prevent the appearance of negative concentrations the numerical solution has to be postprocessed using a slope limiter the methods used here are vertex based limiters proposed by kuzmin in kuzmin 2010 and aizinger in aizinger 2011 and further generalized in kuzmin 2013 reuter et al 2016 their advantages include low numerical diffusion support of arbitrary element shapes and intuitive extensions to higher order dg discretizations and anisotropic problems aizinger et al 2017 since the limiters play an essential role in our modified transport algorithm a short description is in order the common idea underlying all limiters is to rely on the piecewise constant part of the discrete solution guaranteed to be monotonous to limit higher order degrees of freedom of the dg solution formulating the vertex based limiting scheme for a linear dg approximation is very simple for a hierarchical basis e g taylor kuzmin 2010 or orthogonal aizinger 2011 since our choice of basis eq a 6 certainly falls under this description we can write our linear solution locally on element t k as a h x t k a k 1 φ k 1 x a k 2 φ k 2 x a k 3 φ k 3 x the basis functions defined in eq a 6 or rather their mappings to element t k are supported only on element t k and are orthonormal with respect to the l 2 scalar product on t k in particular φ k 2 and φ k 3 are orthogonal to any constant function and by construction φ k 1 t k 1 2 holds where t k t k 1 d x is the area of t k thus a k 1 t k t k a h x d x a k 1 t k 1 2 where a k denotes the integral mean of a h on t k therefore any changes in the linear degrees of freedom a k 2 and a k 3 do not affect the local conservation properties of the solution since the latter only depends on a k 1 by adjusting coefficients a k 2 a k 3 in an appropriate way the slope limiting procedure produces a piecewise linear dg solution fulfilling the maximum principle we seek the maximum admissible value of coefficient θ k 0 θ k 1 in a h x t k a k 1 φ k 1 x θ k a k 2 φ k 2 x a k 3 φ k 3 x here θ k 1 corresponds to an unlimited solution and θ k 0 reduces the solution to its piecewise constant component the correction factor θ k is chosen as a maximum admissible value so that the above reconstruction is bounded in all vertices x i t k by the minimum and maximum mean solution values of a h on all elements containing x i see fig a 11 a 9 a i min a h x i t k a i max i x i is vertex of t k where a i min min t k a k a i max max t k a k x i is vertex of t k to enforce eq a 9 the correction factor θ k is defined as in kuzmin 2010 θ k min x i is vertex of t k a i max a k a h x i t k a k if a h x i t k a i max a i min a k a h x i t k a k if a h x i t k a i min 1 otherwise alternatively the same θ k can be obtained by solving a one dimensional optimization problem as shown in aizinger 2011 fig a 11 patch of elements red involved in the computation of limits a i min max at vertex x i of a single element t k fig a 11 the fact that our slope limiting procedure must be performed on the values on concentration unknowns c m m 1 m instead of the depth integrated concentrations s m that serve as the primary unknowns in transport eq a 2 suggests the following structure of each substep of the time stepping algorithm given in eq a 8 s h c h π 1 s h h h s h lim π 1 λ c h h h where π 1 denotes the l 2 projection into broken piecewise linear polynomial space ℙ 1 t h and λ the slope limiting operator this procedure must be performed after each time substep in eq a 8 and replaces values of s h i i 1 2 there with their limited counterparts the values of a i min and a i max for each species calculated in the course of our limiting procedure can be then employed in the modified transport scheme described in sec 3 the above scheme is computationally efficient compared to the region identification approach used in the subtime stepping work of hodges 2014 
