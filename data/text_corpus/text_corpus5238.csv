index,text
26190,maximizing crop production with minimal resources such as water and energy is the primary focus of sustainable agriculture subsurface water retention technology swrt is a stable approach that preserves water in sandy soils using water saving membranes an optimal use of swrt depends on its shape location and other factors in order to predict crop yield for different irrigation schedule we require at least two computational processes i a crop growth modeling process and ii a water and nutrient permeation process through soil to the root system validation of software parameters to suit properties of specific field becomes increasingly hard since they involve a coordination with field data and coordination between two software in this paper we propose a computationally fast approach that utilizes hydrus 2d software for water and nutrient flow simulation and dssat crop simulation software with an evolutionary multi objective optimization emo procedure in a coordinated manner to minimize water utilization and maximize crop yield prediction our proposed method consists of training one dimensional crop model dssat on data generated by two dimensional model calibrates and validates hydrus 2d that accounts for water accumulation in the swrt membranes then we used dssat model to find the best irrigation schedules for maximizing crop yield with the highest plant water use efficiency tambussi et al 2007 blum 2009 using for the emo methodology the optimization procedure minimizes water usage with the help of rainfall water and increases corn yield prediction as much as six times compare to a non optimized and random irrigation schedule without any swrt membrane our framework also demonstrates an integration of latest computing software and hardware technologies synergistically to facilitate better crop production with minimal water requirement keywords crop yield hydrus 2d dssat multi objective optimization irrigation rate 1 introduction in today s world optimal coordination of food energy and water few nexus has become an international priority and necessity verma 2015 to achieve such a task sustainable technologies for crop production by using minimal water and energy has become essential wallace 2000 national and international projections identify 65 more food and biomass production will be required to support global human populations approaching 9 6 billion by requiring 50 more irrigation water by 2050 how to feed the world in 2050 water is vital for irrigation of crops but water is also scarce and today s food and biomass producers are obligated to use judicial consumption of water for irrigation greenland et al 2018 in order to have a sustainable crop production system we need to use minimal water yet holding most of them in the soil for plant growth sandy soil has much less holding capacity and high hydraulic conductivity at least for high water contents rawls et al 1991 there are some other agronomic solution to the problem of water sparing such as choice of genotype time of sowing soil and soil nutrient management soil organic carbon management mulching etc neupane and guo 2019 researchers have also used machine learning methods on soil moisture data collected from remote sensors to facilitate precision irrigation mccarthy et al 2014 goldstein et al 2018 traditional approach using asphalt barrier has proved to be efficient and is widely accepted for sandy soil guber et al 2015 tutum et al 2015 other technologies are also introduced in literature to hold water in sandy soil yang et al 2014 bruun et al 2014 being costly and labor intensive new polyethylene membranes are also used recently subsurface water retention technology swrt smucker 2011 has been developed and commercialized to improve soil water holding capacity in plant root zone the proper membrane design and installation depth in specific soil and weather condition has been studied in guber et al 2015 using two dimensional modeling of water flow using hydrus 2d software simunek et al 2012 in sandy soils they have investigated a profile distribution of water in a lysimeter filled with pure sand with installed swrt membranes at different depths based on their experiments it is evident that hydrus 2d model with membrane geometry highly reliable estimating water content as in practice with a considerable accuracy it is also established that swrt technology can reduce deep drainage and increase water availability in time by helping in retaining water close to the root zone thereby helping to increase crop production it has been shown that the membrane based water retention technology smucker 2011 in which bowl shaped troughs of impermeable membranes fig 1 are placed at a certain depth below the soil surface in a systematic staggered manner increases crop yields between 1 4 and 3 4 fold smucker et al 2014 compare to not using a membrane coupling the shape and placement of swrt membranes with prescriptive irrigation and fertilization schedules are vital parameters for achieving an optimal crop yield in a previous study tutum et al 2015 water flow and nutrient transport simulation model using hydrus 2d was combined with an evolutionary multi objective optimization emo algorithm deb 2001 to obtain optimal membrane geometry and placement in soil profile along with prescriptive irrigation scheduling under two conflicting objectives the study revealed a number of insights about the worthwhile sizes of the membranes and useful supply of irrigation water for achieving certain level of water at the soil root zone although hydrus 2d can predict the water and nutrient accumulation at the root zone of a plant it cannot simulate the crop growth which is a direct measurable outcome of the irrigation process that we are interested in maximizing besides a continuous supply of water at the root level either through an optimal irrigation pattern or through rainfall the growth of crop and eventual crop yield depend on many other factors such as incoming solar energy plant transpiration rate temperature type of crop etc thus to have a better estimate of crop growth it is necessary to take help of another computational simulator that can explicitly provide an estimate of crop yield for given soil water mix nutrient content and other parameters in a time series manner for this purpose we use dssat decision support system for agrotechnology transfer software which is a widely accepted tool for agronomists hoogenboom et al 2015 however dssat uses a one dimensional approach to water flow modeling and thus cannot simulate swrt membranes due to their two cylindrical or three dimensional spherical shapes we need at least two stand alone software namely hydrus 2d and dssat in order to simulate swrt membrane in soil and analyze the predict for crop production specifically hydrus 2d should be used for two dimensional water flow simulation while dssat for modeling plant growth and assessment of crop yield hydrus 2d is computationally expensive to run whereas dssat is many times faster due to their one dimensional modeling since an optimization process usually requires many iterations of different solutions to be evaluated during a run we need to devise an efficient optimization strategy which will use a few calls to the hydrus 2d software all these issues pose a great challenge to computing plant science and agricultural engineering researchers to devise an efficient methodology which uses each of the two software effectively and produce reliable and accurate results revealing the underlying optimal irrigation pattern and swrt shapes for known variation of weather soil and other environmental conditions the aim of this paper is to introduce a mathematical modeling that performs crop yield simulation under different swrt membranes by using two different software the paper provides a methodology to optimize parameters of swrt namely shape and location to achieve better crop yield prediction it can also be used to find an irrigation pattern that minimizes predicted water usage with the help of computational resources the proposed method can be easily extended to different soil crops weather conditions that will provide useful information before any cultivation is carried out the rest of the paper is organized as follows in section 2 we make a brief introduction about the background of this problem and discuss two software systems mentioned above in section 3 we discuss our calibration and validation process of both software in details in section 4 optimization methodology for maximizing crop yield and water usage is presented section 5 discusses our experimental results section 6 concludes the findings of this paper with a direction for future studies 2 background in this section we introduce two software systems used in this study hydrus and dssat hydrus 2d 3d software was developed to simulate two and three dimensional movement of water heat and multiple solutes by solving richards equation for saturated unsaturated water flow and the fickian based convection dispersion equation for heat and solute transport simunek et al 2012 fig 1 shows a 2d mesh design of a swrt membrane with water content values after irrigation on the right we observe that much of the water is contained inside the membrane just after irrigation while we can use hydrus 2d software to simulate water flow with a swrt membrane geometry and find water use efficiency accurately currently it cannot model crop growth involving plant genetics root growth transpiration and other complicated processes thus in addition we require another software which is capable of predicting crop growth given the water and nutrient content at the root zone predicted by hydrus 2d under the condition that swrt membrane is inserted in the soil dssat is a software that simulates growth and development of plants over time in an one dimensional arrangement with its own soil water carbon and nutrient processes it is comprised of several modules namely land unit weather management soil soil plant atmosphere cropgro plant type etc hoogenboom et al 2015 the software is capable of simulating different crop models namely maize wheat barley etc and calculate yield at the end of crop growing period a tipping bucket model hoogenboom et al 2015 is used to produce soil water content while dssat is capable of modeling water and nutrient flow as an one dimensional model it cannot simulate the water and nutrient flow with swrt inserted in the soil thus a standard simulation of dssat will produce wrong results about water and nutrient values at the root zone and subsequently the crop yield in the present of a swrt membrane currently one software is not adequate for estimating the crop yield and water nutrient content at the root zone with swrt membranes implanted in the soil we need to use both software but coordinate their parameter values in a way so as to influence each other s simulation in a synergistic and meaningful manner two different models are used to simulate water content in the soil and predict crop yield by simulating water content from swrt in a more correct way using hydrus 2d we will use predicted soil water content dynamics to train dssat and estimate crop yield the water flow component of dssat namely tipping bucket model is computationally more efficient compare to richards model used in hydrus 2d three key hydraulic parameters in dssat are the wilting point s l l l lower limit saturation s s a t field capacity drained upper limit s d u l and saturated hydraulic conductivity k s hoogenboom et al 2015 hydrus uses parameters from multiple numerical models one of such models is van genuchten mualem model van genuchten 1980 often used if it is selected to introduce swrt into dssat model we will generate soil water content swc dynamics for different soil and irrigation scenarios using hydrus 2d model then by varying the dssat hydraulic parameters for different scenarios we will train dssat model to predict the same swc for the same scenarios once the dssat training is completed the computationally faster software dssat can then be used to complete the optimization process for maximizing predictable crop yield and minimize water usage maximize usage efficiency 3 material and method for calibration in this section we describe the dssat training on hydrus generated swc dynamics hydrus 2d is computationally expensive to run and its use to evaluate every solution processed within an optimization run would be an expensive proposition although we first attempt to negotiate the computational time by using a parallel computing platform the crux of this paper lies in ways of simulating the results of hydrus 2d using a less expensive dssat simulation process with derived dssat parameter values we describe the dssat training below we assume that swrt can be introduced into dssat by modifying its soil hydraulic properties in a way that swc predicted by 1d tipping bucket model dssat would match averaged swc predicted by hydrus 2d in soil layers therefore we subdivide the soil domain overall 120 cm depth into ten layers layers are l 0 8 8 13 13 18 18 23 23 28 28 33 33 38 38 43 43 48 48 120 cm depth without loss of generality we refer them as layer 1 to layer 10 in hydrus 2d we create a 2d mesh with 30 cm width and 120 cm depth our dssat model also has the same computational layers our objective function is then to minimize the difference of swc predicted by dssat and hydrus 2d in each layer for different soil and irrigation scenarios by varying values of dssat parameters there are three dssat parameters per layer sdul ssat and slll as mentioned in the previous section therefore we need to find 30 parameters in total that could produce the water content similar as much as possible for different irrigation schedules thus for a given irrigation schedule and rainfall pattern over the entire t days of simulation we run hydrus 2d and averaged swc calculated in nodes separately for each soil layer on daily basis then we attempt to find one set of optimized parameter values of dssat so as to match the hydrus generated swc layer in a day wise manner to minimize the search space of this parameter optimization we have optimized the parameters of each layer independently it is true that parameters of one layer can affect the water content of the next layer since we are finding an artificial set of soil parameters the effect of neighboring layers can be captured by slightly modifying the parameters of next layer performed by optimization hence the effect among layers can be reduced if we conduct the optimization starting from layer 1 0 8 cm to layer 10 48 120 cm incrementally the simulations are conducted for corn root system assuming that most roots are located within the swrt zone we denote our objective function as f l for layer l irrigation schedule i s starts from the day of planting and lasts for t 110 days in order to find robust parameters we optimize this objective over a number of irrigation schedules so that the behavior is acceptable for an unknown irrigation schedule we can define our objective function in the next subsection 3 1 dssat training objective the objective is to minimize the mean squared error of between water content θ of hydrus 2d and dssat over the entire crop growing season for different irrigation schedules s 1 minimize l l f l i s t 1 t θ h y d r u s i t l θ d s s a t i t l 2 2 subject to fixed soil evaporation rate for both system here θ d s s a t i t l and θ h y d r u s i t l are the water contents of dssat and hydrus 2d simulators respectively at day t for the corresponding layer l and irrigation schedule i s note that the criterion is mean squared error or mse another objective function namely nash sutcliffe efficiency index nse nash and sutcliffe 1970 can also be used in this regard nse can be defined as follows 2 n s e 1 t 1 t θ d s s a t t θ h y d r u s t 2 t 1 t θ h y d r u s t θ ˆ h y d r u s 2 here θ ˆ h y d r u s is the average of hydrus 2d output 3 2 training steps here we discuss the detailed steps of our proposed training procedure fig 2 presents a sketch of it we divide this task into six different steps the description of these steps are as follows this training validation process provides us the optimum parameters of dssat that can now be used to simulate plant growth and estimate crop yield with swrt fairly closely and without the use of computationally expensive hydrus 2d using these parameters our task now is to find optimum irrigation schedules that would minimize water usage since dssat can parallely provide us with crop growth estimates day wise we are now ready to execute a bi objective optimization study by minimizing water usage and maximizing crop yield 4 material and method for optimization our first objective is to predict crop yield given by dssat measured in kilogram per hectare the second objective is the water use efficiency wue tambussi et al 2007 blum 2009 equation 3 the ratio of utilized water by plants and total amount of supplied irrigation the upper limit of wue is 1 0 when the water is solely used by the plants which is an idealistic situation the upper limit of corn production is hard to predict thus to measure the quality of optimized solutions we have compared our optimization solutions with two different scenarios 1 the potential crop yields were estimated in the dssat simulations for each weather and soil scenario at irrigation scenarios which did not cause water stress on plant growth the best possible irrigation and 2 yield estimated by dssat for random irrigation schedules with and without swrt membrane soil organic content and nutrient levels were set to optimal conditions and did not affect plant growth 4 1 data for this study we have collected five years 2011 2015 of precipitation data fig 3 and temperature data fig 4 from nearby weather station in east lansing msuhort michigan in this study we have selected a particular type of corn decalb xl71 due to known parameters a popular crop produced in north america inside dssat we turn off the effect of nutrients and study the effect of water on crop growth alone irrigation schedule starts just after plantation and crop is assumed to be harvested at maturity the optimization of wue and crop yield was conducted for soil scenarios shown in table 1 4 2 a prior study a previous study tutum et al 2015 considered two objectives related to water retention and utilization at the root system 1 water use efficiency and 2 root uptake efficiency by using hydrus 2d software for different aspect ratios of swrt membranes an evolutionary multi objective optimization methodology nsga ii deb et al 2002 was integrated only to hydrus 2d software to achieve optimal aspect ratio of swrt and an irrigation rate which is kept fixed for every day of the cultivation process nsga ii and other evolutionary algorithms have been used in other irrigation studies cisty et al 2017 de paly and zell 2009 schütze et al 2006 ikudayisi and adeyemo 2015 maier et al 2015 zheng et al 2015 fowler et al 2015 hadka and reed 2015 and a review fanuel et al 2018 is available the objective functions used in the study were as follows 3 max f 1 a r t d u r w u e a r t d u r 1 s d r a i n s i r r max f 2 a r t d u r r u e a r t d u r c u m q r c u m q r p subject to a r 2 1 3 1 4 1 5 1 t m i n t d u r t m a x hydrus 2d simulates triggered sprinkler irrigation system irrigation is given by centimeter per day the aspect ratio ar of the membranes are used as discrete variables and a finite number of options are specified 2 1 3 1 4 1 5 1 following conclusions were made from the study i a larger root uptake requires larger supply of water and b shallower membrane enhances root water uptake in the soil system while deeper membrane can hold more water and facilitates more efficient water usage for this purpose we use the same soil properties and membrane sizes nsga ii optimization procedure deb et al 2002 is used to handle two water retention related objectives wue and rue as discussed before for our initial simulations here we have used a population size of 32 maximum number of high fidelity evaluations of 4 000 a four time increase from the prior study sbx recombination operator deb and agrawal 1995 with probability of 0 9 and crossover index of 15 and polynomial mutation deb and deb 2014 with a probability of 0 5 and mutation index of 20 these are standard parameters used in real parameter nsga ii studies deb 2001 the obtained results are given in results section 4 3 improvement over prior study in the prior study discussed above each evaluation of a solution by hydrus 2d took about 1 5 2 min of computational time on a high performance desktop computer for a run involving 1000 evaluations it requires 1 1 5 days to complete one optimization run the study used a single irrigation rate for each day of the entire 110 day schedule moreover each solution was evaluated without considering any weather variation no rainfall was considered thus every irrigation pattern was evaluated only once in a deterministic manner however to extend the proposed optimization procedure for any realistic situation would involve a number of different extensions each day may require a different irrigation rate depending on the root water demands for an optimal crop growth thereby increasing the number of irrigation scenarios in the resulting optimization problem from one to 110 each irrigation scenario rate and frequency must be tested on a number of rainfall patterns in the region and in the season of the crop growth requiring a 110 day irrigation pattern to be evaluated multiple times to speed up the computational process in this study we procure a high performance server machine and run the training and optimization using parallel threads 4 4 proposed optimization methodology in this section we will discuss our optimization procedure that improves predicted yield by less amount of irrigated water 4 4 1 optimization objectives our primary goal is to maximize predicted crop yield y and water use efficiency wue this can be described as follows 4 max i 1 w w y i w a r max i 1 w w w u e i w a r subject to i ω domain of all irrigation schedules w w set of different weather conditions ar 2 1 3 1 4 1 swrt designs as clear from the above formulation for every swrt membrane design ar we optimize both average crop yield and average water use over different weather conditions for five years to obtain an optimized irrigation schedule i for the entire crop growing season these years represent dry and wet weather scenarios the water use efficiency wue was computed from the mass conservation equation irrigation precipitation run off drainage soil evaporation transpiration accumulation in soil since precipitation is available without any cost or energy to the farmer irrigation is the only cost that we want to minimize therefore our optimization is likely to utilize rain water to produce optimized irrigation schedule loss of water is the sum of all terms on the right except plant transpiration therefore water use efficiency is estimated as the ratio between plant transpiration and the supplied irrigation water wue transpiration irrigation if amount of rain water is more than loss of water then no irrigation water is needed and we set to wue to 100 our second objective crop production is readily obtained from the dssat software after running a simulation on a particular irrigation schedule the unit of this objective is kg per hectare as mentioned earlier we take an average over five years of production to compute this objective function now we consider the variables of this problem one irrigation schedule may contain starting date of irrigation day and amount of irrigation in the crop growing season we declare the daily irrigation amount at most 50 mm per day as a variable and this makes the number of variables same as the number of days 110 used in this study we consider the following three case studies in our initial study we formulate the problem with two variables only for the entire 110 days irrigation interval in days and amount of irrigation in mm since daily irrigation rate is fixed and cannot be adjusted as per daily rainfall an effect of changing weather conditions cannot be properly simulated in this formulation secondly we consider the full representation of the problem comprised of 110 variables for each of the 110 days of our simulation each variable represents the amount of irrigation needed on that particular day of the season although this case study is more flexible the search space is huge and it is difficult to find good solutions in a reasonable computational time thus in our final formulation we fix a 10 day interval and use a fixed irrigation amount for this period thus there will be a total of 11 variables for the entire 110 day simulation 4 5 choice of an optimization algorithm we use a derivative free stochastic and global optimization method nsga ii deb 2001 nsga ii optimization procedure works by sampling and evaluating different schedules and constructing new schedules from the better ones using specific operators population of solutions gradually evolves and produces a diverse set of solutions optimizing both wue and the crop yield population size is kept 128 and the number of generations is fixed at 2000 we have used similar parameters as before for nsga ii s operators we ran our algorithm in our server machine with 32 threads in parallel 5 simulation results in this section we summarize our results first we present computationally fast procedure of prior study then we show training validation results after that the results from the overall optimization procedure are demonstrated 5 1 improved speed up fig 5 a shows the computational speed up obtained by increasing the number of threads in our multi core computer the computational time t 1 needed with a single thread is noted and the speed up for a p threaded application requiring t p time is calculated as the ratio t 1 t p i y axis on the figure we observe that beyond 30 threads the overhead of inter thread communications increase and the marginal rate of time saving diminishes fig 5 b shows the trade off solutions obtained for four aspect ratios by executing the entire optimization run to a single thread from the prior study tutum et al 2015 while the overall behaviors of the two optimization runs are more of less identical we reduce the running time of nsga ii run with 4000 solution evaluations to 2 h compare to 4 6 days of simulation 5 2 dssat training and validation results in this section we investigate the results provided by dssat training and validating process 5 2 1 training and test data we divide the irrigation schedule data into training and test sets in each case dssat and hydrus is run for 110 days and a sum of squared error is measured our training data is irrigation rate of 5 5 20 1 50 5 and 10 mm for every 5 5 20 1 50 5 and 10 days respectively we have then tested our model with 33 different instances between 5 and 50 mm daily irrigation and 2 10 days between two consecutive irrigation in fig 6 we show the comparison of volumetric water content dynamics predicted by hydrus and dssat software under the same irrigation and soil evaporation rates in some test cases for the year 2011 results for other years are similarly validated the results show that the water content dynamics of trained dssat closely match to those obtained with hydrus we observed there is a maximum of 5 average per day error between hydrus and dssat water content values this percentage is computed by the absolute difference of hydrus and dssat values over hydrus value it means that the modeling of dssat is good enough for our purpose here is table for dssat parameters for each soil with averaged for soil layers nse and rmse for training and validation datasets 5 2 2 training error estimation in fig 7 we show nash sutcliffe efficiency nse and root mean square error rmse values for different layers in 33 different validation instances we show these results for different years having different weather conditions and different ar designs calibrated separately we observe that most of the nse values are positive and many of them are higher than 0 5 the layers close to ground level have larger nse value than the ones deep underground we could not achieve the perfect nse value which is 1 0 in any of the cases negative nse values in some cases suggest that the proposed model of dssat is not able to integrate well with hydrus 2d in those cases for shallower swrt membrane we get higher accuracy for example ar 3 1 in the figure has better nse and rmse values on average than that of ar 2 1 total rmse error over entire irrigation period in most of the cases are below 0 02 in fig 8 we show cumulative water fluxes from hydrus and dssat for the year 2011 we show fluxes for infiltration total input run off drainage and soil evaporation in each test case we observe that water fluxes of dssat effectively match with that obtained by hydrus 2d wherever swrt membrane is present the above validation procedure between hydrus 2d and dssat has also provided us with optimized soil parameter values of dssat for each aspect ratio of the swrt membrane these optimized dssat parameters have matched volumetric and cumulative water flux values obtained by the hydrus 2d software with swrt membrane embedded in the soil now we are ready to use dssat with the obtained optimized parameters to simulate the crop growth which hydrus 2d can not do currently and perform our overall bi objective optimization study 5 3 bi objective optimization results after we estimated the parameters of dssat model that provide the same swc dynamics as hudrus 2d for swrt we run nsga ii algorithm using the optimized parameters to find the best irrigation schedule for two objectives i maximize water use efficiency wue and ii maximize predicted crop yield now all five years having different weather conditions are considered and objectives are averaged before using them in the optimization process we summarize the results of different case studies below 5 3 1 results of case study i in this study with only two variables interval and amount of irrigation we observe that from almost 1 to 4 mm of water is needed everyday to optimize both wue and crop yield but the amount of crop yield is not more than 9000 kg per hectare which is less than the potential amount of yield of 10 000 kg per hectare in reality different amount of water is necessary for different period of time for the best crop production 5 3 2 results of case study ii we perform an optimization with 110 independent variables irrigation amount in each day the crop yield obtained by our approach is less than 7000 kg per hectare which is not close to the maximum achievable 5 3 3 results of case study iii as mentioned before here the irrigation rate is kept fixed for 10 consecutive days of simulation and then varied to a different value for the next 10 days and so on since the number of variables are reasonably low 11 variables the optimization algorithm is able to produce a well distributed set of pareto optimal solutions trading off wue and crop yield objectives well we investigate the nature of trade off solutions in the following paragraphs optimization without swrt we create a similar project in hydrus 2d with the same domain size 30 cm 120 cm and with the same coarse sand soil but without any impermeable swrt membrane we then searched for variables related to soil of dssat so that it matches the water content of hydrus 2d simulation to imitate properties of that soil after that we optimize wue and crop production using nsga ii with the optimized parameters in fig 9 a we present the obtained non dominated solutions without swrt it can be observed that at most 2000 kg per hectare corn is possible to be grown on average while gaining as much as 25 in wue since water conductivity is very high in sandy soil it is expected that we obtain much less corn production in dry years optimization with swrt in fig 9 b d the non dominated solutions are shown for different ar values there is a slight variation of non dominated solutions obtained from these three different swrt configurations it is observed that 2 1 aspect ratio ar design is able to retain more water in the soil compare to other designs from fig 9 e it is clear that the non dominated solutions with swrt are better than those obtained without swrt this is because swrt retains more water thereby helping plant growth at their crucial stages the figure depicts that one of the obtained non dominated solutions produces a crop yield of as high as 12 630 kg ha fig 9 f shows that irrespective of ar in the membrane design a small amount of daily water is needed initially amount of water varies significantly when crop becomes more mature we divide the entire non dominated set of solutions into three parts the first part limits the amount of daily irrigation to 7 8 mm in the second part middle we obtain better crop yield by increasing the irrigation rate to 15 25 mm over the vegetation period in the final part after around 40 50 days we need to increase the water supply to around 40 50 mm per day when the crop reaches maturity the harvest time begins and we need less amount of water all making sense to standard irrigation practices and resulting from our optimization process plant biomass would be better indicator of plant growth it is available in dssat in fig 10 a e we show the leaf weight difference between swrt and swrt less irrigation results for different weather conditions it is evident that plants without swrt do not grow much due to lack of water containment at the root system in fig 11 a b c and d we also observe that the root density information of plants with or without swrt for different layers and different weather conditions plants develop a better root system and transpiration also becomes high when they get much water the irrigation pattern for best crop yield from the previous figure also matches that with the leaf weight root density and transpiration pattern of healthy plants these patterns confirm the validity of our simulation set up and the overall procedure adopted in linking both hydrus 2d and dssat software 5 3 4 analysis of bi objective optimization results we have produced an increased amount of corn by simulating the irrigation schedule obtained from our optimization process but let us now investigate the quality of our obtained solution table 2 shows the potential and optimized yields produced by dssat software in both cases we run on the optimized parameters for 2 1 ar design the potential yield is the maximum possible yield under standardized parameter settings provided by dssat when the effect of irrigation is not taken into account dssat internally put their best possible irrigation needed for the crop growth no optimization is involved in the selection of these irrigation schedules on the other hand the optimized yield is obtained by finding the best irrigation schedules for the same 2 1 ar design using our proposed optimization procedure keeping water use efficiency criteria fixed we observe that in all of the years 2011 2015 having different weather conditions the crop yield produced by our optimization procedure is about 99 to that of the potential yield which indicates the efficacy of our proposed integration approach 5 3 5 overall time savings it is necessary to compute the overall running time since our automated process of introducing swrt into crop yield simulation can be extended to a number of crops soil weather conditions etc first we present the total time savings by our proposed methodology for our initial study we need to run our optimization in hydrus 2d for 4000 solution evaluations each evaluation takes 1 5 min on an average thus making 6000 min or 100 h to complete all evaluations we reduce this time to around 2 h by using a parallel computer thereafter we have proposed an overall efficient methodology by linking two software along with a multi objective optimization algorithm that uses much less computational time compare to a method that needs to use hydrus 2d for evaluating every new solution in our approach hydrus 2d is only used for a few cases during the training and testing phases for dssat we evaluate 2000 high fidelity evaluations where each evaluation takes 2 2 5 s time on an average by our parallel implementation we reduce the total execution time to 6 h for this optimization process here are the details of time requirements calibration and training process step 1 evaluation of s schedules in dssat each takes t 1 seconds but using a parallel evaluation process it takes only t 1 in total step 2 evaluation of s schedules in hydrus each taking t 2 seconds it takes t 2 seconds in total by s processors step 3 dssat parameter optimization run for q 1 function evaluations takes time q 1 t 1 32 seconds by using 32 parallel threads step 4 evaluation of all validation cases on s v a l processors takes t 2 time where s v a l 33 optimization using dssat full dssat optimization runs for q 2 function evaluations takes q 2 t 1 32 seconds of time with 32 processors typical values for case study iii are t 1 2 2 5 s t 2 1 5 2 min where q 1 10 000 and q 2 2 000 therefore the overall optimization run takes about 6 h with 32 processors as mentioned in 3 table 3 6 summary and future work in this paper we have proposed a computational approach to find an optimum irrigation schedule that not only minimizes water usage but also maximizes crop production in order to simulate soil water movement under embedded swrt membranes we have used hydrus 2d simulation software in a two dimensional setting since hydrus 2d cannot simulate crop growth directly another software dssat is used to predict the crop yield between the two software hydrus 2d simulation is exceedingly more time consuming thus we have developed a computationally fast and integrated procedure to interlink the two software with a multi objective optimization algorithm so that both conflicting criteria can be optimized in a reasonable computational time we have proposed a new training validation methodology to find suitable dssat parameters that match level wise water transport and evaporation properties with the same obtained using computationally expensive hydrus 2d software the optimization has then been carried out with dssat over different weather conditions to predict the amount of crop production our method not only reduces computational effort by a multi threaded implementation of solution evaluations but also greatly cuts down the human effort of optimizing parameters by introducing evolutionary multi objective optimization emo in the precision irrigation field results presented here have clearly indicated the promise of our proposed approach the study also spurs a number of future research directions in addition to crop yield and water usage fertilizer usage can considered to be minimized by simulating nutrient flow and accumulation under swrt under the two software using a similar integration procedure to make the overall procedure more pragmatic a three dimensional water and nutrient flows using hydrus 2d for example may be considered subsurface water supply can be considered in our simulation process easily for minimal evaporation loss the use of surrogate modeling methods tsoukalas and makropoulos 2015 deb et al 2017 roy and deb 2016 can be used for improving computational speed other key farm specific criteria can also be considered in our optimization procedure to make the computational approach more practical acknowledgments this material is based in part upon work supported by the national science foundation under cooperative agreement no dbi 0939454 any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the national science foundation 
26190,maximizing crop production with minimal resources such as water and energy is the primary focus of sustainable agriculture subsurface water retention technology swrt is a stable approach that preserves water in sandy soils using water saving membranes an optimal use of swrt depends on its shape location and other factors in order to predict crop yield for different irrigation schedule we require at least two computational processes i a crop growth modeling process and ii a water and nutrient permeation process through soil to the root system validation of software parameters to suit properties of specific field becomes increasingly hard since they involve a coordination with field data and coordination between two software in this paper we propose a computationally fast approach that utilizes hydrus 2d software for water and nutrient flow simulation and dssat crop simulation software with an evolutionary multi objective optimization emo procedure in a coordinated manner to minimize water utilization and maximize crop yield prediction our proposed method consists of training one dimensional crop model dssat on data generated by two dimensional model calibrates and validates hydrus 2d that accounts for water accumulation in the swrt membranes then we used dssat model to find the best irrigation schedules for maximizing crop yield with the highest plant water use efficiency tambussi et al 2007 blum 2009 using for the emo methodology the optimization procedure minimizes water usage with the help of rainfall water and increases corn yield prediction as much as six times compare to a non optimized and random irrigation schedule without any swrt membrane our framework also demonstrates an integration of latest computing software and hardware technologies synergistically to facilitate better crop production with minimal water requirement keywords crop yield hydrus 2d dssat multi objective optimization irrigation rate 1 introduction in today s world optimal coordination of food energy and water few nexus has become an international priority and necessity verma 2015 to achieve such a task sustainable technologies for crop production by using minimal water and energy has become essential wallace 2000 national and international projections identify 65 more food and biomass production will be required to support global human populations approaching 9 6 billion by requiring 50 more irrigation water by 2050 how to feed the world in 2050 water is vital for irrigation of crops but water is also scarce and today s food and biomass producers are obligated to use judicial consumption of water for irrigation greenland et al 2018 in order to have a sustainable crop production system we need to use minimal water yet holding most of them in the soil for plant growth sandy soil has much less holding capacity and high hydraulic conductivity at least for high water contents rawls et al 1991 there are some other agronomic solution to the problem of water sparing such as choice of genotype time of sowing soil and soil nutrient management soil organic carbon management mulching etc neupane and guo 2019 researchers have also used machine learning methods on soil moisture data collected from remote sensors to facilitate precision irrigation mccarthy et al 2014 goldstein et al 2018 traditional approach using asphalt barrier has proved to be efficient and is widely accepted for sandy soil guber et al 2015 tutum et al 2015 other technologies are also introduced in literature to hold water in sandy soil yang et al 2014 bruun et al 2014 being costly and labor intensive new polyethylene membranes are also used recently subsurface water retention technology swrt smucker 2011 has been developed and commercialized to improve soil water holding capacity in plant root zone the proper membrane design and installation depth in specific soil and weather condition has been studied in guber et al 2015 using two dimensional modeling of water flow using hydrus 2d software simunek et al 2012 in sandy soils they have investigated a profile distribution of water in a lysimeter filled with pure sand with installed swrt membranes at different depths based on their experiments it is evident that hydrus 2d model with membrane geometry highly reliable estimating water content as in practice with a considerable accuracy it is also established that swrt technology can reduce deep drainage and increase water availability in time by helping in retaining water close to the root zone thereby helping to increase crop production it has been shown that the membrane based water retention technology smucker 2011 in which bowl shaped troughs of impermeable membranes fig 1 are placed at a certain depth below the soil surface in a systematic staggered manner increases crop yields between 1 4 and 3 4 fold smucker et al 2014 compare to not using a membrane coupling the shape and placement of swrt membranes with prescriptive irrigation and fertilization schedules are vital parameters for achieving an optimal crop yield in a previous study tutum et al 2015 water flow and nutrient transport simulation model using hydrus 2d was combined with an evolutionary multi objective optimization emo algorithm deb 2001 to obtain optimal membrane geometry and placement in soil profile along with prescriptive irrigation scheduling under two conflicting objectives the study revealed a number of insights about the worthwhile sizes of the membranes and useful supply of irrigation water for achieving certain level of water at the soil root zone although hydrus 2d can predict the water and nutrient accumulation at the root zone of a plant it cannot simulate the crop growth which is a direct measurable outcome of the irrigation process that we are interested in maximizing besides a continuous supply of water at the root level either through an optimal irrigation pattern or through rainfall the growth of crop and eventual crop yield depend on many other factors such as incoming solar energy plant transpiration rate temperature type of crop etc thus to have a better estimate of crop growth it is necessary to take help of another computational simulator that can explicitly provide an estimate of crop yield for given soil water mix nutrient content and other parameters in a time series manner for this purpose we use dssat decision support system for agrotechnology transfer software which is a widely accepted tool for agronomists hoogenboom et al 2015 however dssat uses a one dimensional approach to water flow modeling and thus cannot simulate swrt membranes due to their two cylindrical or three dimensional spherical shapes we need at least two stand alone software namely hydrus 2d and dssat in order to simulate swrt membrane in soil and analyze the predict for crop production specifically hydrus 2d should be used for two dimensional water flow simulation while dssat for modeling plant growth and assessment of crop yield hydrus 2d is computationally expensive to run whereas dssat is many times faster due to their one dimensional modeling since an optimization process usually requires many iterations of different solutions to be evaluated during a run we need to devise an efficient optimization strategy which will use a few calls to the hydrus 2d software all these issues pose a great challenge to computing plant science and agricultural engineering researchers to devise an efficient methodology which uses each of the two software effectively and produce reliable and accurate results revealing the underlying optimal irrigation pattern and swrt shapes for known variation of weather soil and other environmental conditions the aim of this paper is to introduce a mathematical modeling that performs crop yield simulation under different swrt membranes by using two different software the paper provides a methodology to optimize parameters of swrt namely shape and location to achieve better crop yield prediction it can also be used to find an irrigation pattern that minimizes predicted water usage with the help of computational resources the proposed method can be easily extended to different soil crops weather conditions that will provide useful information before any cultivation is carried out the rest of the paper is organized as follows in section 2 we make a brief introduction about the background of this problem and discuss two software systems mentioned above in section 3 we discuss our calibration and validation process of both software in details in section 4 optimization methodology for maximizing crop yield and water usage is presented section 5 discusses our experimental results section 6 concludes the findings of this paper with a direction for future studies 2 background in this section we introduce two software systems used in this study hydrus and dssat hydrus 2d 3d software was developed to simulate two and three dimensional movement of water heat and multiple solutes by solving richards equation for saturated unsaturated water flow and the fickian based convection dispersion equation for heat and solute transport simunek et al 2012 fig 1 shows a 2d mesh design of a swrt membrane with water content values after irrigation on the right we observe that much of the water is contained inside the membrane just after irrigation while we can use hydrus 2d software to simulate water flow with a swrt membrane geometry and find water use efficiency accurately currently it cannot model crop growth involving plant genetics root growth transpiration and other complicated processes thus in addition we require another software which is capable of predicting crop growth given the water and nutrient content at the root zone predicted by hydrus 2d under the condition that swrt membrane is inserted in the soil dssat is a software that simulates growth and development of plants over time in an one dimensional arrangement with its own soil water carbon and nutrient processes it is comprised of several modules namely land unit weather management soil soil plant atmosphere cropgro plant type etc hoogenboom et al 2015 the software is capable of simulating different crop models namely maize wheat barley etc and calculate yield at the end of crop growing period a tipping bucket model hoogenboom et al 2015 is used to produce soil water content while dssat is capable of modeling water and nutrient flow as an one dimensional model it cannot simulate the water and nutrient flow with swrt inserted in the soil thus a standard simulation of dssat will produce wrong results about water and nutrient values at the root zone and subsequently the crop yield in the present of a swrt membrane currently one software is not adequate for estimating the crop yield and water nutrient content at the root zone with swrt membranes implanted in the soil we need to use both software but coordinate their parameter values in a way so as to influence each other s simulation in a synergistic and meaningful manner two different models are used to simulate water content in the soil and predict crop yield by simulating water content from swrt in a more correct way using hydrus 2d we will use predicted soil water content dynamics to train dssat and estimate crop yield the water flow component of dssat namely tipping bucket model is computationally more efficient compare to richards model used in hydrus 2d three key hydraulic parameters in dssat are the wilting point s l l l lower limit saturation s s a t field capacity drained upper limit s d u l and saturated hydraulic conductivity k s hoogenboom et al 2015 hydrus uses parameters from multiple numerical models one of such models is van genuchten mualem model van genuchten 1980 often used if it is selected to introduce swrt into dssat model we will generate soil water content swc dynamics for different soil and irrigation scenarios using hydrus 2d model then by varying the dssat hydraulic parameters for different scenarios we will train dssat model to predict the same swc for the same scenarios once the dssat training is completed the computationally faster software dssat can then be used to complete the optimization process for maximizing predictable crop yield and minimize water usage maximize usage efficiency 3 material and method for calibration in this section we describe the dssat training on hydrus generated swc dynamics hydrus 2d is computationally expensive to run and its use to evaluate every solution processed within an optimization run would be an expensive proposition although we first attempt to negotiate the computational time by using a parallel computing platform the crux of this paper lies in ways of simulating the results of hydrus 2d using a less expensive dssat simulation process with derived dssat parameter values we describe the dssat training below we assume that swrt can be introduced into dssat by modifying its soil hydraulic properties in a way that swc predicted by 1d tipping bucket model dssat would match averaged swc predicted by hydrus 2d in soil layers therefore we subdivide the soil domain overall 120 cm depth into ten layers layers are l 0 8 8 13 13 18 18 23 23 28 28 33 33 38 38 43 43 48 48 120 cm depth without loss of generality we refer them as layer 1 to layer 10 in hydrus 2d we create a 2d mesh with 30 cm width and 120 cm depth our dssat model also has the same computational layers our objective function is then to minimize the difference of swc predicted by dssat and hydrus 2d in each layer for different soil and irrigation scenarios by varying values of dssat parameters there are three dssat parameters per layer sdul ssat and slll as mentioned in the previous section therefore we need to find 30 parameters in total that could produce the water content similar as much as possible for different irrigation schedules thus for a given irrigation schedule and rainfall pattern over the entire t days of simulation we run hydrus 2d and averaged swc calculated in nodes separately for each soil layer on daily basis then we attempt to find one set of optimized parameter values of dssat so as to match the hydrus generated swc layer in a day wise manner to minimize the search space of this parameter optimization we have optimized the parameters of each layer independently it is true that parameters of one layer can affect the water content of the next layer since we are finding an artificial set of soil parameters the effect of neighboring layers can be captured by slightly modifying the parameters of next layer performed by optimization hence the effect among layers can be reduced if we conduct the optimization starting from layer 1 0 8 cm to layer 10 48 120 cm incrementally the simulations are conducted for corn root system assuming that most roots are located within the swrt zone we denote our objective function as f l for layer l irrigation schedule i s starts from the day of planting and lasts for t 110 days in order to find robust parameters we optimize this objective over a number of irrigation schedules so that the behavior is acceptable for an unknown irrigation schedule we can define our objective function in the next subsection 3 1 dssat training objective the objective is to minimize the mean squared error of between water content θ of hydrus 2d and dssat over the entire crop growing season for different irrigation schedules s 1 minimize l l f l i s t 1 t θ h y d r u s i t l θ d s s a t i t l 2 2 subject to fixed soil evaporation rate for both system here θ d s s a t i t l and θ h y d r u s i t l are the water contents of dssat and hydrus 2d simulators respectively at day t for the corresponding layer l and irrigation schedule i s note that the criterion is mean squared error or mse another objective function namely nash sutcliffe efficiency index nse nash and sutcliffe 1970 can also be used in this regard nse can be defined as follows 2 n s e 1 t 1 t θ d s s a t t θ h y d r u s t 2 t 1 t θ h y d r u s t θ ˆ h y d r u s 2 here θ ˆ h y d r u s is the average of hydrus 2d output 3 2 training steps here we discuss the detailed steps of our proposed training procedure fig 2 presents a sketch of it we divide this task into six different steps the description of these steps are as follows this training validation process provides us the optimum parameters of dssat that can now be used to simulate plant growth and estimate crop yield with swrt fairly closely and without the use of computationally expensive hydrus 2d using these parameters our task now is to find optimum irrigation schedules that would minimize water usage since dssat can parallely provide us with crop growth estimates day wise we are now ready to execute a bi objective optimization study by minimizing water usage and maximizing crop yield 4 material and method for optimization our first objective is to predict crop yield given by dssat measured in kilogram per hectare the second objective is the water use efficiency wue tambussi et al 2007 blum 2009 equation 3 the ratio of utilized water by plants and total amount of supplied irrigation the upper limit of wue is 1 0 when the water is solely used by the plants which is an idealistic situation the upper limit of corn production is hard to predict thus to measure the quality of optimized solutions we have compared our optimization solutions with two different scenarios 1 the potential crop yields were estimated in the dssat simulations for each weather and soil scenario at irrigation scenarios which did not cause water stress on plant growth the best possible irrigation and 2 yield estimated by dssat for random irrigation schedules with and without swrt membrane soil organic content and nutrient levels were set to optimal conditions and did not affect plant growth 4 1 data for this study we have collected five years 2011 2015 of precipitation data fig 3 and temperature data fig 4 from nearby weather station in east lansing msuhort michigan in this study we have selected a particular type of corn decalb xl71 due to known parameters a popular crop produced in north america inside dssat we turn off the effect of nutrients and study the effect of water on crop growth alone irrigation schedule starts just after plantation and crop is assumed to be harvested at maturity the optimization of wue and crop yield was conducted for soil scenarios shown in table 1 4 2 a prior study a previous study tutum et al 2015 considered two objectives related to water retention and utilization at the root system 1 water use efficiency and 2 root uptake efficiency by using hydrus 2d software for different aspect ratios of swrt membranes an evolutionary multi objective optimization methodology nsga ii deb et al 2002 was integrated only to hydrus 2d software to achieve optimal aspect ratio of swrt and an irrigation rate which is kept fixed for every day of the cultivation process nsga ii and other evolutionary algorithms have been used in other irrigation studies cisty et al 2017 de paly and zell 2009 schütze et al 2006 ikudayisi and adeyemo 2015 maier et al 2015 zheng et al 2015 fowler et al 2015 hadka and reed 2015 and a review fanuel et al 2018 is available the objective functions used in the study were as follows 3 max f 1 a r t d u r w u e a r t d u r 1 s d r a i n s i r r max f 2 a r t d u r r u e a r t d u r c u m q r c u m q r p subject to a r 2 1 3 1 4 1 5 1 t m i n t d u r t m a x hydrus 2d simulates triggered sprinkler irrigation system irrigation is given by centimeter per day the aspect ratio ar of the membranes are used as discrete variables and a finite number of options are specified 2 1 3 1 4 1 5 1 following conclusions were made from the study i a larger root uptake requires larger supply of water and b shallower membrane enhances root water uptake in the soil system while deeper membrane can hold more water and facilitates more efficient water usage for this purpose we use the same soil properties and membrane sizes nsga ii optimization procedure deb et al 2002 is used to handle two water retention related objectives wue and rue as discussed before for our initial simulations here we have used a population size of 32 maximum number of high fidelity evaluations of 4 000 a four time increase from the prior study sbx recombination operator deb and agrawal 1995 with probability of 0 9 and crossover index of 15 and polynomial mutation deb and deb 2014 with a probability of 0 5 and mutation index of 20 these are standard parameters used in real parameter nsga ii studies deb 2001 the obtained results are given in results section 4 3 improvement over prior study in the prior study discussed above each evaluation of a solution by hydrus 2d took about 1 5 2 min of computational time on a high performance desktop computer for a run involving 1000 evaluations it requires 1 1 5 days to complete one optimization run the study used a single irrigation rate for each day of the entire 110 day schedule moreover each solution was evaluated without considering any weather variation no rainfall was considered thus every irrigation pattern was evaluated only once in a deterministic manner however to extend the proposed optimization procedure for any realistic situation would involve a number of different extensions each day may require a different irrigation rate depending on the root water demands for an optimal crop growth thereby increasing the number of irrigation scenarios in the resulting optimization problem from one to 110 each irrigation scenario rate and frequency must be tested on a number of rainfall patterns in the region and in the season of the crop growth requiring a 110 day irrigation pattern to be evaluated multiple times to speed up the computational process in this study we procure a high performance server machine and run the training and optimization using parallel threads 4 4 proposed optimization methodology in this section we will discuss our optimization procedure that improves predicted yield by less amount of irrigated water 4 4 1 optimization objectives our primary goal is to maximize predicted crop yield y and water use efficiency wue this can be described as follows 4 max i 1 w w y i w a r max i 1 w w w u e i w a r subject to i ω domain of all irrigation schedules w w set of different weather conditions ar 2 1 3 1 4 1 swrt designs as clear from the above formulation for every swrt membrane design ar we optimize both average crop yield and average water use over different weather conditions for five years to obtain an optimized irrigation schedule i for the entire crop growing season these years represent dry and wet weather scenarios the water use efficiency wue was computed from the mass conservation equation irrigation precipitation run off drainage soil evaporation transpiration accumulation in soil since precipitation is available without any cost or energy to the farmer irrigation is the only cost that we want to minimize therefore our optimization is likely to utilize rain water to produce optimized irrigation schedule loss of water is the sum of all terms on the right except plant transpiration therefore water use efficiency is estimated as the ratio between plant transpiration and the supplied irrigation water wue transpiration irrigation if amount of rain water is more than loss of water then no irrigation water is needed and we set to wue to 100 our second objective crop production is readily obtained from the dssat software after running a simulation on a particular irrigation schedule the unit of this objective is kg per hectare as mentioned earlier we take an average over five years of production to compute this objective function now we consider the variables of this problem one irrigation schedule may contain starting date of irrigation day and amount of irrigation in the crop growing season we declare the daily irrigation amount at most 50 mm per day as a variable and this makes the number of variables same as the number of days 110 used in this study we consider the following three case studies in our initial study we formulate the problem with two variables only for the entire 110 days irrigation interval in days and amount of irrigation in mm since daily irrigation rate is fixed and cannot be adjusted as per daily rainfall an effect of changing weather conditions cannot be properly simulated in this formulation secondly we consider the full representation of the problem comprised of 110 variables for each of the 110 days of our simulation each variable represents the amount of irrigation needed on that particular day of the season although this case study is more flexible the search space is huge and it is difficult to find good solutions in a reasonable computational time thus in our final formulation we fix a 10 day interval and use a fixed irrigation amount for this period thus there will be a total of 11 variables for the entire 110 day simulation 4 5 choice of an optimization algorithm we use a derivative free stochastic and global optimization method nsga ii deb 2001 nsga ii optimization procedure works by sampling and evaluating different schedules and constructing new schedules from the better ones using specific operators population of solutions gradually evolves and produces a diverse set of solutions optimizing both wue and the crop yield population size is kept 128 and the number of generations is fixed at 2000 we have used similar parameters as before for nsga ii s operators we ran our algorithm in our server machine with 32 threads in parallel 5 simulation results in this section we summarize our results first we present computationally fast procedure of prior study then we show training validation results after that the results from the overall optimization procedure are demonstrated 5 1 improved speed up fig 5 a shows the computational speed up obtained by increasing the number of threads in our multi core computer the computational time t 1 needed with a single thread is noted and the speed up for a p threaded application requiring t p time is calculated as the ratio t 1 t p i y axis on the figure we observe that beyond 30 threads the overhead of inter thread communications increase and the marginal rate of time saving diminishes fig 5 b shows the trade off solutions obtained for four aspect ratios by executing the entire optimization run to a single thread from the prior study tutum et al 2015 while the overall behaviors of the two optimization runs are more of less identical we reduce the running time of nsga ii run with 4000 solution evaluations to 2 h compare to 4 6 days of simulation 5 2 dssat training and validation results in this section we investigate the results provided by dssat training and validating process 5 2 1 training and test data we divide the irrigation schedule data into training and test sets in each case dssat and hydrus is run for 110 days and a sum of squared error is measured our training data is irrigation rate of 5 5 20 1 50 5 and 10 mm for every 5 5 20 1 50 5 and 10 days respectively we have then tested our model with 33 different instances between 5 and 50 mm daily irrigation and 2 10 days between two consecutive irrigation in fig 6 we show the comparison of volumetric water content dynamics predicted by hydrus and dssat software under the same irrigation and soil evaporation rates in some test cases for the year 2011 results for other years are similarly validated the results show that the water content dynamics of trained dssat closely match to those obtained with hydrus we observed there is a maximum of 5 average per day error between hydrus and dssat water content values this percentage is computed by the absolute difference of hydrus and dssat values over hydrus value it means that the modeling of dssat is good enough for our purpose here is table for dssat parameters for each soil with averaged for soil layers nse and rmse for training and validation datasets 5 2 2 training error estimation in fig 7 we show nash sutcliffe efficiency nse and root mean square error rmse values for different layers in 33 different validation instances we show these results for different years having different weather conditions and different ar designs calibrated separately we observe that most of the nse values are positive and many of them are higher than 0 5 the layers close to ground level have larger nse value than the ones deep underground we could not achieve the perfect nse value which is 1 0 in any of the cases negative nse values in some cases suggest that the proposed model of dssat is not able to integrate well with hydrus 2d in those cases for shallower swrt membrane we get higher accuracy for example ar 3 1 in the figure has better nse and rmse values on average than that of ar 2 1 total rmse error over entire irrigation period in most of the cases are below 0 02 in fig 8 we show cumulative water fluxes from hydrus and dssat for the year 2011 we show fluxes for infiltration total input run off drainage and soil evaporation in each test case we observe that water fluxes of dssat effectively match with that obtained by hydrus 2d wherever swrt membrane is present the above validation procedure between hydrus 2d and dssat has also provided us with optimized soil parameter values of dssat for each aspect ratio of the swrt membrane these optimized dssat parameters have matched volumetric and cumulative water flux values obtained by the hydrus 2d software with swrt membrane embedded in the soil now we are ready to use dssat with the obtained optimized parameters to simulate the crop growth which hydrus 2d can not do currently and perform our overall bi objective optimization study 5 3 bi objective optimization results after we estimated the parameters of dssat model that provide the same swc dynamics as hudrus 2d for swrt we run nsga ii algorithm using the optimized parameters to find the best irrigation schedule for two objectives i maximize water use efficiency wue and ii maximize predicted crop yield now all five years having different weather conditions are considered and objectives are averaged before using them in the optimization process we summarize the results of different case studies below 5 3 1 results of case study i in this study with only two variables interval and amount of irrigation we observe that from almost 1 to 4 mm of water is needed everyday to optimize both wue and crop yield but the amount of crop yield is not more than 9000 kg per hectare which is less than the potential amount of yield of 10 000 kg per hectare in reality different amount of water is necessary for different period of time for the best crop production 5 3 2 results of case study ii we perform an optimization with 110 independent variables irrigation amount in each day the crop yield obtained by our approach is less than 7000 kg per hectare which is not close to the maximum achievable 5 3 3 results of case study iii as mentioned before here the irrigation rate is kept fixed for 10 consecutive days of simulation and then varied to a different value for the next 10 days and so on since the number of variables are reasonably low 11 variables the optimization algorithm is able to produce a well distributed set of pareto optimal solutions trading off wue and crop yield objectives well we investigate the nature of trade off solutions in the following paragraphs optimization without swrt we create a similar project in hydrus 2d with the same domain size 30 cm 120 cm and with the same coarse sand soil but without any impermeable swrt membrane we then searched for variables related to soil of dssat so that it matches the water content of hydrus 2d simulation to imitate properties of that soil after that we optimize wue and crop production using nsga ii with the optimized parameters in fig 9 a we present the obtained non dominated solutions without swrt it can be observed that at most 2000 kg per hectare corn is possible to be grown on average while gaining as much as 25 in wue since water conductivity is very high in sandy soil it is expected that we obtain much less corn production in dry years optimization with swrt in fig 9 b d the non dominated solutions are shown for different ar values there is a slight variation of non dominated solutions obtained from these three different swrt configurations it is observed that 2 1 aspect ratio ar design is able to retain more water in the soil compare to other designs from fig 9 e it is clear that the non dominated solutions with swrt are better than those obtained without swrt this is because swrt retains more water thereby helping plant growth at their crucial stages the figure depicts that one of the obtained non dominated solutions produces a crop yield of as high as 12 630 kg ha fig 9 f shows that irrespective of ar in the membrane design a small amount of daily water is needed initially amount of water varies significantly when crop becomes more mature we divide the entire non dominated set of solutions into three parts the first part limits the amount of daily irrigation to 7 8 mm in the second part middle we obtain better crop yield by increasing the irrigation rate to 15 25 mm over the vegetation period in the final part after around 40 50 days we need to increase the water supply to around 40 50 mm per day when the crop reaches maturity the harvest time begins and we need less amount of water all making sense to standard irrigation practices and resulting from our optimization process plant biomass would be better indicator of plant growth it is available in dssat in fig 10 a e we show the leaf weight difference between swrt and swrt less irrigation results for different weather conditions it is evident that plants without swrt do not grow much due to lack of water containment at the root system in fig 11 a b c and d we also observe that the root density information of plants with or without swrt for different layers and different weather conditions plants develop a better root system and transpiration also becomes high when they get much water the irrigation pattern for best crop yield from the previous figure also matches that with the leaf weight root density and transpiration pattern of healthy plants these patterns confirm the validity of our simulation set up and the overall procedure adopted in linking both hydrus 2d and dssat software 5 3 4 analysis of bi objective optimization results we have produced an increased amount of corn by simulating the irrigation schedule obtained from our optimization process but let us now investigate the quality of our obtained solution table 2 shows the potential and optimized yields produced by dssat software in both cases we run on the optimized parameters for 2 1 ar design the potential yield is the maximum possible yield under standardized parameter settings provided by dssat when the effect of irrigation is not taken into account dssat internally put their best possible irrigation needed for the crop growth no optimization is involved in the selection of these irrigation schedules on the other hand the optimized yield is obtained by finding the best irrigation schedules for the same 2 1 ar design using our proposed optimization procedure keeping water use efficiency criteria fixed we observe that in all of the years 2011 2015 having different weather conditions the crop yield produced by our optimization procedure is about 99 to that of the potential yield which indicates the efficacy of our proposed integration approach 5 3 5 overall time savings it is necessary to compute the overall running time since our automated process of introducing swrt into crop yield simulation can be extended to a number of crops soil weather conditions etc first we present the total time savings by our proposed methodology for our initial study we need to run our optimization in hydrus 2d for 4000 solution evaluations each evaluation takes 1 5 min on an average thus making 6000 min or 100 h to complete all evaluations we reduce this time to around 2 h by using a parallel computer thereafter we have proposed an overall efficient methodology by linking two software along with a multi objective optimization algorithm that uses much less computational time compare to a method that needs to use hydrus 2d for evaluating every new solution in our approach hydrus 2d is only used for a few cases during the training and testing phases for dssat we evaluate 2000 high fidelity evaluations where each evaluation takes 2 2 5 s time on an average by our parallel implementation we reduce the total execution time to 6 h for this optimization process here are the details of time requirements calibration and training process step 1 evaluation of s schedules in dssat each takes t 1 seconds but using a parallel evaluation process it takes only t 1 in total step 2 evaluation of s schedules in hydrus each taking t 2 seconds it takes t 2 seconds in total by s processors step 3 dssat parameter optimization run for q 1 function evaluations takes time q 1 t 1 32 seconds by using 32 parallel threads step 4 evaluation of all validation cases on s v a l processors takes t 2 time where s v a l 33 optimization using dssat full dssat optimization runs for q 2 function evaluations takes q 2 t 1 32 seconds of time with 32 processors typical values for case study iii are t 1 2 2 5 s t 2 1 5 2 min where q 1 10 000 and q 2 2 000 therefore the overall optimization run takes about 6 h with 32 processors as mentioned in 3 table 3 6 summary and future work in this paper we have proposed a computational approach to find an optimum irrigation schedule that not only minimizes water usage but also maximizes crop production in order to simulate soil water movement under embedded swrt membranes we have used hydrus 2d simulation software in a two dimensional setting since hydrus 2d cannot simulate crop growth directly another software dssat is used to predict the crop yield between the two software hydrus 2d simulation is exceedingly more time consuming thus we have developed a computationally fast and integrated procedure to interlink the two software with a multi objective optimization algorithm so that both conflicting criteria can be optimized in a reasonable computational time we have proposed a new training validation methodology to find suitable dssat parameters that match level wise water transport and evaporation properties with the same obtained using computationally expensive hydrus 2d software the optimization has then been carried out with dssat over different weather conditions to predict the amount of crop production our method not only reduces computational effort by a multi threaded implementation of solution evaluations but also greatly cuts down the human effort of optimizing parameters by introducing evolutionary multi objective optimization emo in the precision irrigation field results presented here have clearly indicated the promise of our proposed approach the study also spurs a number of future research directions in addition to crop yield and water usage fertilizer usage can considered to be minimized by simulating nutrient flow and accumulation under swrt under the two software using a similar integration procedure to make the overall procedure more pragmatic a three dimensional water and nutrient flows using hydrus 2d for example may be considered subsurface water supply can be considered in our simulation process easily for minimal evaporation loss the use of surrogate modeling methods tsoukalas and makropoulos 2015 deb et al 2017 roy and deb 2016 can be used for improving computational speed other key farm specific criteria can also be considered in our optimization procedure to make the computational approach more practical acknowledgments this material is based in part upon work supported by the national science foundation under cooperative agreement no dbi 0939454 any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the national science foundation 
26191,built up land impervious surface expansion links urbanization and environmental change to enable large scale long term spatially explicit studies we took a data driven approach exploiting newly available time series of fine spatial resolution remote sensing observations and developed the spatially explicit long term empirical city development select model closely calibrated to observational data select functions at several spatial scales with multiple design traits capturing local variations of urbanization and ensuring performance for long term extrapolations in scenario analyses e g the shared socioeconomic pathways it showed low estimation residuals explained high fractions of the response s variations and scored well in all robustness and generalizability tests we ran when compared with a typical spatial interaction based model for projecting global built up land in 2030 select allocated more new development to areas with similar characteristics to locations that exhibited expansive urban growth historically while the example spatial interaction based model allocated more new development to areas with high amounts of existing built up land keywords select built up impervious surface urban ssps land cover land use change 1 introduction built up land impervious surface expansion is a prevalent characteristic of contemporary urbanization and one of the most profound anthropogenic changes to the earth s surface aaas 2016 built up consists of primarily manmade materials e g cement asphalt steel glass etc it is a physical expression of the socioeconomic processes related to urbanization can greatly alter environmental and climatic dynamics and strongly affects the vulnerability of human societies to environmental stress at multiple scales e g s anchez rodŕıguez et al 2005 spatial patterns of built up land modify the intensity and the duration of extreme heat events e g oleson et al 2015 and play crucial roles in determining these events societal impacts on air conditioning related energy use and human health e g jones et al 2018 meanwhile new built up land often replaces prime farmland the competition between urban and agricultural land uses directly influences people s livelihood at local scale and potentially affects food production and security at larger scales e g d amour et al 2017 in this analysis we focus on built up land a k a developed land impervious surface despite the many possible definitions of urban land spatially explicit modeling of built up land that is consistent with commonly used long term societal scenarios can effectively contribute to integrated modeling and investigations of large scale socio environmental interactions e g those between global urbanization and climate change however existing global efforts projecting potential spatiotemporal patterns of future built up land are rudimentary they are usually based on either of two types of models 1 stylized fact models e g jackson et al 2010 klein goldewijk et al 2010 rely on simplifying assumptions about the relation between built up land and population such as thresholding population density to map land development intensity classes these models show a wide range of uncertainties due to alternative simplifying assumptions and often overlook important spatial and temporal variations as the simplifying assumptions are often strong and homogeneous e g schwanitz 2013 klein goldewijk et al 2010 2 spatial interaction based models e g li et al 2017 seto et al 2012 such as applications of cellular automata e g clarke and gaydos 1998 batty 2007 are widely used in the land cover land use change literature where common temporal horizons up to 3 decades and spatial scales local to regional are different from those of large scale impact assessments of climate and environmental change the latter often require global coverage longer temporal horizons e g throughout the 21st century and consistent integration with scenario frameworks addressing a wide range of societal and environmental uncertainties the existing dominance of these two types of modeling practices is related to the longstanding lack of time series of global spatial built up land change data while in recent years a few spatially explicit time series global datasets of built up land became available exploiting new data availability we took a data driven approach of statistical learning and data mining techniques and developed the spatially explicit long term empirical city development select model for mapping built up land change it functions at national sub national regional and local scales reflecting multiscale spatiotemporal dynamics in land development and its driving forces the model utilizes newly available historical time series of fine spatial resolution remote sensing observations of built up land spanning the past 40 years as well as best available historical data on spatial population and environmental variables e g topographic contexts access to waterbodies and change in population size to meet the needs of large scale impact assessment of global environmental change especially climate change select was designed to 1 use globally consistent spatial data while recognizing vibrant local variations in the urbanization process across the world 2 extrapolate well temporally under long term societal scenarios allowing places to switch between different development styles present in observational data e g currently undeveloped places may become rapidly expanding and currently fast developing places may stabilize and stop expanding and 3 offer easy integration with aggregate land development urbanization scenarios we are particularly interested in eventually using select under the ssp rcp scenario framework the ssp rcp framework is widely used for investigating interactions among future societal climate and environmental conditions ssps stand for the shared socioeconomic pathways o neill et al 2015 describing potential societal trends in demographics economics technological development and governance throughout the 21st century and rcps stand for the representative concentration pathways van vuuren et al 2014 driving commonly used climate scenarios the ssp rcp framework accounts for many important uncertainties in the long term future but built up urban land expansion has yet to be incorporated aside from the work presented here we are currently developing national level ssp consistent urbanization scenarios and plan to use them in combination with select to eventually produce a first set of ssp consistent spatial built up land map series for the 21st century select is unique and improves existing long term large scale spatial built up land change models in at least four major ways a it is closely calibrated to historical time series data in contrast to the common approach relying heavily on stylized assumptions about spatiotemporal interactions b it embodies multiple design features to capture sub national local variations in urbanization process and land use history in contrast to existing models regional or larger scale e g one model setup per continent c it incorporates multiple design features to ensure reasonable temporal extrapolation under scenarios accounting for long term uncertainties d it has been intensively validated through evaluations of statistical robustness spatial and temporal generalizability and comparison with an existing spatial interaction based model seto et al 2012 for mapping global built up land in 2030 2 materials and methods 2 1 spatial and temporal resolutions select can be trained and applied at many possible spatial resolutions but here we used 1 8 in the initial model development and testing and the global land is covered by 997 022 grid cells it is a balance between common spatial resolutions of climate modeling 0 5 2 and land cover change studies 30 500 m 1 8 degree roughly 14 km at the equator is also the spatial resolution of the ssp consistent spatial population projections s2 pop jones o neill 2016 which is an important driver used by select for built up land development patterns although 1 8 degree is coarser than typical spatial increments of built up land expansion spatial precision is achieved by modeling land development as a continuous variable change in the fraction of built up land within each grid cell rather than the more commonly used binary variable i e developed vs non developed or transition probability to be developed although intuitive this choice makes select numerically different from many common urban land change models because continuous and binary variables call for different analytical methods to model using this continuous response variable also makes the model applicable at a range of spatial resolutions without needing a structural change or losing spatial precision therefore select is more scalable than binary response models especially at coarse spatial resolutions temporally select was trained to make estimates at 10 year intervals this is the update pace of the historical time series data used in model training finer temporal resolution was not pursued because built up land expansions are often incremental and would require precisions beyond what current global observations could offer to be mapped accurately for shorter time intervals 2 2 data the model is based on a wide range of rich datasets table 1 the most noteworthy is a new time series of fine spatial resolution built up land map series spanning the past 40 years ghsl v 1 pesaresi et al 2015 prior to ghsl s release modis yearly land cover type had been considered the most accurate and spatially resolved global data for built up land potere et al 2009 however ghsl better fits the needs of built up land modeling than modis being landsat based ghsl has a finer spatial resolution 38 m than modis 500 m and offers a longer time series since 1975 than modis since 2001 moreover ghsl has been shown to outperform modis for mapping built up land especially in areas of low development densities leyk et al 2018 from ghsl fig 1 we derived the fraction of built up land within each grid cell at 1 8 degree resolution for three decades 1980 2010 at 10 year interval as well as spatial and temporal descriptive indices of the built up fraction maps spatial population is an important driver for modeling built up land change especially for projecting into long term futures we used gpw v 4 ciesin 2017 in model training and s2 pop 2016 for future projections gpw is a direct observation of historical spatial population it uniformly distributes population within the finest census spatial units available across the world s2 pop were quantitatively downscaled from ssp national population and urbanization projections kc lutz 2014 jiang and o neill 2015 using a parameterized gravity model reflecting the ssp narratives of spatial development patterns both are state of the art data for spatial population using s2 pop also helps select establish consistency with patterns already described by existing ssp components from these spatial population datasets we derived total population counts at 1 8 resolution decadal change and change ratio of the population counts since s2 pop 2016 is the basis for the spatial population variables in our model we used the same habitable land mask as s2 pop 2016 which accounts for waterbodies protected areas extreme topologies etc additionally considering s2 pop 2016 used grump v 1 ciesin 2011 as a base dataset we used grump s land water boundary for calculating land areas and distance to waterbodies at 1 8 resolution wup united nations 2015 includes a list of existing cities whose population sizes are greater than or equal to 300 000 these cities were used to generate the distance to sizeable cities variable in our model furthermore the locations and the densities of these cities were used to delineate boundaries of sub national regions that select models separately to account for spatial variations of the urbanization process at regional and local scales the 1 km gmted 2010 danielson and gesch 2011 provides a new level of detail in global topographic data it was used to calculate the mean elevation and the range of slope gradients an index of surface roughness within 1 8 grid cells 2 3 model design training select has two components that function at multiple spatial scales fig 2 the spatial built up land change model estimates for each grid cell decadal built up land development potentials using spatially explicit driving factors and built up land change history of the antecedent 20 years this is a data driven model trained using three decades 1980 2010 of observational data where the last decade 2000 2010 of built up land change is the response variable and information on the earlier two decades 1980 2000 provides a suite of explanatory variables the sub national spatial allocation algorithm takes in exogenously estimated national decadal total amount of new built up development first distributes it among sub national regions according to their anticipated population change and land use history and then allocates the sub national regional totals to 1 8 degree grids proportionally according to their development potential estimated by the spatial built up land change model using exogenous national total amounts allows select the flexibility to cooperate with future scenarios other than the ssps making it a general purpose urban land change model for long term spatial scenario analyses when making future projections the two components run in turn to generate a built up land map for one decade the model then updates the explanatory variables characterizing built up land change history for the antecedent 20 years of the next decade and subsequently makes projections for that decade the process repeats decade by decade to reach long term future 2 3 1 spatial built up land change model select s spatial built up land change model estimates the change in built up fraction for each grid cell by summing three parts part 1 general trend is a quadratic polynomial model qpm that captures the average grid cell level trajectory of land development applicable to all global grid cells generally when an undeveloped grid cell starts to become developed the speed of development is slow at first then gradually accelerates entering a fast developing phase and finally tapers as the place becomes and stays highly developed fig 3 shows this relationship between x built up land fraction within a 1 8 degree grid cell at the beginning of a decade and y change in that fraction during the decade with a modified scatterplot because currently the majority of global land grid cells are of low development density or not developed at all using all data points in model training will bias the resulting model s fit to emphasize low density development to generate a global trend line that well reflects patterns for all levels of development we divided the range of built up land fraction at the beginning of a decade i e 0 1 into bins of 0 05 width casted global grid cells into these bins based on their built up fraction at the beginning of a decade x and averaged for each bin its member grid cells change in built up fraction during the decade y these decadal binned means are visualized in fig 3 they show quadratic relationships respectively for all three decades with observational data and the qpm was parameterized using the stack of binned means from two decades 1980 1990 and 1990 2000 leaving out 2000 2010 the decade of the response variable in model training the final model is the gold line in fig 3 part 2 local dynamic is a unique model part enabling select to capture local variations in the process of built up land conversion across the world it achieves this through three design features a we divided the world into 375 sub national regions considering locations and densities of existing cities with a population size no less than 300 000 fig 4 and model each region separately we detail the procedure that delineated the sub national regions in appendix a essentially it starts with thiessen polygons of the above mentioned existing cities and aggregates small polygons in areas where cities cluster and incorporates national boundaries to generate sub national regions that mostly consist of 600 1200 grid cells each this procedure embodied two primary considerations i each region consists of at least one existing sizeable city and covers complete geographic gradients of urban rural land use transitions having the model see the geographic transition during training allows it to use that information as an analogy to temporal urban rural land use transitions so that when historically undeveloped areas enter high development scenarios in the future the model will not extrapolate erratically ii the regions are small enough to distinguish local scale variations big enough to encompass a fair number of grid cells so that every local model can be parameterized with confidence and are as similarly sized as possible to be of consistent spatial scale using these modeling units allows distinctively different treatments for different land development mechanisms of sub national local scales which improves existing large scale long term urban land change models that usually treat the world as a few or a few tens of regions aggregating multiple countries b select uses heuristic indices listed in the bottom right cell of table 2 as explanatory variables along with demographic and environmental drivers listed in top four rows in table 2 in the model these indices describe geometric and temporal patterns of built up land change calculated over local scale neighborhoods surrounding each grid cell they help capture local dynamics and address the issue that for many factors affecting built up land development e g land tenure regulations public policies cultural preferences etc there is no globally consistent dataset or representation that can be used by a large scale model assuming patterns of historical built up land change reflect the combined effects of all these factors the descriptive indices we generate here are proxies of the missing explanatory variables and the varying neighborhood sizes can help capture mechanisms functioning at different levels of local scale this redundancy although was built in for a good reason does introduce some correlation among certain indices e g means over different neighborhood sizes to remove excessive collinearity we conducted a principal component analysis on the 108 descriptive indices and used the first 15 principal components pcs as explanatory variables in select the 15 pcs collectively explain more than 95 of cumulative variance and each has an eigenvalue greater than 0 5 we used 0 5 instead of 1 a commonly used rule of thumb as the cut off because it raised the explained percentage of cumulative variance to a more satisfactory level also the slightly lower ranked pcs are likely more useful for this model considering that the demographic and environmental drivers already capture the broad strokes of land development processes the most prominent pcs may offer similar information to what is already accounted for while the slightly lower ranked pcs may be more likely to bring new insights altogether 23 explanatory variables i e the first 15 pcs and the 8 drivers are used to train the statistical model described below c the local dynamic part of select is a non parametric statistical model generalized additive model gam gam estimates the response as the addition of smooth transformations of individual explanatory variables y ˆ α s 1 x 1 s 2 x 2 s p x p hastie 2017 the relationships between response and explanatory variables i e the smooth transformations are fully determined by data based training this feature is highly desirable for at least three reasons i analysts often do not have enough prior knowledge about the nature of relationships between new land development and its drivers to construct parametric models especially for those heuristic indices based explanatory variables table 2 ii these relationships are expected to vary among the 375 sub national regions and gam s flexibility allows select to capture such spatial variation automatically iii analysts can investigate the impacts of each explanatory variable on new land development in different parts of the world by examining the variables smooth transformations in different sub national regions after model training e g for the showcase gam in fig 5 the response y ˆ has a quadratic relation with x 1 a linear relation with x 2 and a logarithmic relation with x p besides the conceptual advantages we compared gam s numerical performance for continental u s with a few other modeling methods i least square linear model lm using the same set of explanatory variables ii lasso least absolute shrinkage and selection operator using the same explanatory variables and iii lasso using the same explanatory variables as well as their squares lasso sq to accommodate potential high power relationships in comparison gam showed lower magnitude residuals and explained higher fractions of variance in the response variable more detail in appendix b the gam based local dynamic model was implemented in r using the mgcv package which bases the smooth transformations on penalized splines each sub national region has its own gam parameterized using data from the region s constituent grid cells assuming a gaussian error distribution with an identity link function the default reml fitting algorithm was used for its ability to avoid overfitting for varying parameters wood 2011 additional experiments testing the models robustness against overfitting were conducted as part of model validation described in section 2 4 2 part 3 residuals of general trend and local dynamic are modeled as stochastic errors this part of the spatial built up land change model assumes residual errors follow independent and identical gaussian distribution at each grid cell within the same sub national region the residual model part was parameterized for each sub national region at the same time while the region s local dynamic gam was trained when using select for making statistically best guess estimates general trend and local dynamic are strung together as shown in fig 6 when stochastic estimates are desired e g for uncertainty analyses the residuals part of the model can be used to generate random noise of reasonable magnitudes and amounts to be added to best guess estimates 2 3 2 sub national spatial allocation algorithm the sub national spatial allocation algorithm links select s data driven spatiotemporal model and long term scenarios existing literature on land cover land use change modeling has found that accurately estimating the amount and the spatial pattern of change are two distinctively different tasks pontius and millones 2011 modeling the two separately therefore allows long term national scenarios based on macro scale relationships between built up land development and socioeconomic drivers e g population change economic growth to be spatially mapped in ways consistent with patterns shown by observational data for a given decade the sub national spatial allocation algorithm first allocates the exogenously estimated national total amount of new built up land development to the sub national regions in proportion to the products of the regions total population sizes in the end of the decade and their per capita built up land areas in base year i e 2000 using these sub national regional weights i e end of decade population size base year per capita built up land area the model can react to potential population migrations in long term societal scenarios if a sub national region gains a large amount of population during a decade new land development will follow when the model allocates that decade s national total new built up land and sub national regions losing population will see a halt on their land development also considering base year per capita built up land allows different regions to maintain their conventional preference for land development densities after distributing the national totals to sub national regions the regional total amount of new built up development is allocated to each region s constituent grid cells proportionally to the grid cells development potentials which is defined as the product of the grid cell s land area and its decadal δ built up fraction estimated by the spatial built up land change model sometimes especially under fast urban expansion scenarios the exogenously estimated national total amount of new built up land is so high that the proportional allocation for some grid cells may exceed the cell s total available land area to fulfill the national total the overflows are collected and re distributed using the same proportional process to grid cells that still have available land under extremely fast development scenarios the national and the regional total amounts of development may exceed all available land in grid cells that the spatial built up land model estimates where development may take place and the spill over amount of development will be allocated to all other grids in proportion to their built up land fractions at the beginning of the decade 2 4 model evaluation although no dataset is available to directly assess select s performance for long term spatially explicit modeling we evaluated its short and mid term applications and gained understanding of and confidence in its potential for longer term studies all performance descriptive indices were calculated for the entire model treating the combination of the spatial built up land change model and the sub national spatial allocation algorithm as a whole unless otherwise noted 2 4 1 short and mid term validations for short term evaluation we assessed select as a whole for estimating built up land change during the training period of its statistical components i e qpm pca and gam 2000 2010 we examined the magnitudes and the spatial patterns of the full model s residuals and inspected its ability to explain the response variable residual is defined at each grid cell as the difference between estimated and observed built up land fractions for mid term evaluation we compared select with an existing spatial interaction based model urbanmod used for mapping 2030 global built up land seto et al 2012 with 2000 as base year seto et al used urbanmod to generate a global probability map of 5 km resolution for built up land development by 2030 and created spatial built up land scenarios allocating different amounts of national total new land development by flipping 5 km grids from undeveloped to developed in a binary fashion according to ranking of the grids development probability within each nation grids with the highest probability are developed first only one development probability map was created and it was the basis for all spatial scenarios and all time points in contrast to select that allows the spatial patterns under each scenario to iteratively evolve decade by decade and responds to diverging drivers e g different spatial population patterns under different scenarios at grid cell and regional scales in addition to national scale besides shedding light on select s performance this model comparison will also provide general insights about how spatial statistical machine learning models and spatial interaction based models differ in long term built up land mapping because urbanmod s drivers including slope distance to roads snapshot population density in 2000 snapshot land cover type in 2000 and habitable land mask cover similar factors as select differences in model performance can be attributed primarily to the difference between the two modeling methods especially whether and how they use time series information the comparison used a scenario of very fast development as a stress test for the spatial models urbanmod and select mid term stress test helps understand long term model behaviors because the models get to allocate large amounts of new development despite the actual time span not being particularly long to construct the national scenario we first identified the maximum and the minimum observed decadal amounts of built up land change for each country over the three decades we have data and used the larger of the maximum value and twice the minimum as the decadal amount of new development the same national scenarios were applied to the two models while their base year maps came from different sources fig 7 to make it easier to compare the results of the two models we aggregated urbanmod s 5 km development probability map to 1 8 degree resolution treated it as a development potential map and allocated national total amount of new development proportionally to the grid cells development potential according to urbanmod this proportional allocation mechanism was described as ideal by seto et al 2012 although their application used a different process 2 4 2 robustness and generalizability tests as will be shown in the results section select showed a very high level of performance in evaluations outlined above considering the model s data driven nature it is important to test its robustness and generalizability eliminating potential harmful overfitting for this part of model evaluation we focused on the u s as an example of developed countries and china as an example of developing countries and conducted three sets of experiments 1 statistical robustness we used the residual part of select s spatial built up land change model to add stochastic noise to the observed response variable retrained the local dynamic part of the model using the noise added response and compared the resulting gams with the original gams the rationale is that if the select modeling scheme leads to overfitting the gams based on the noise added response would be very different from the gams based on the original response and if the modeling scheme is robust and captures generalizable effects the two sets of gams should be very similar e g goodfellow et al 2016 bishop 1995 to determine similarity we compare corresponding smooth transformations used by the two sets of gams for the same sub national region and the same explanatory variable 2 spatial generalizability select models sub national regions separately to capture local spatial variations in the urbanization process since the regional boundaries were properly delineated the local dynamic part trained for one region should not be expected to perform highly i e spatially generalizable for another region however it is reasonable to expect the model trained for a given region to perform better when applied to a nearby region than a region far away due to the general trend of spatial autocorrelation to examine whether this holds for select for each sub national region in the u s and china we i applied models trained for its nearest median distanced and farthest neighboring regions within the country ii calculated the residuals of these neighboring models when applied to the region and iii compared the estimation residuals of the three experiments with the residuals of the region s indigenous model if our modeling framework has reasonable spatial generalizability the magnitudes of residuals shown by these experiments should rank in descending order from farthest neighbor median distanced neighbor nearest neighbor to indigenous models this set of experiments was run only on the spatial built up land change model component of select because the national total amount of built up land change used by the sub national spatial allocation algorithm is a boundary condition that would moderate the spatial patterns we plan to examine in this experiment and may make the general trend difficult to identify with this setting we expect the magnitudes of residuals seen in this experiment to be noticeably higher than in other experiments 3 temporal generalizability since select is designed for long term modeling it should capture the general trend over time rather than being overly tuned to a single time epoch i e a single decade in this study the challenge for evaluating temporal generalizability is that only three decades of observational data are available to utilize as much temporal information as possible in the model the first two decades were used to generate explanatory variables for estimating change over the third decade and then no data is left for independent temporal validation as a substitute we trained the select modeling scheme with only the first decade of data for estimating change over the second decade the resulting model was then validated for estimating change during the third decade the single decade input model is of course different from the two decade input standard version of the model this exercise therefore is not a direct evaluation of how temporally generalizable select is but rather how temporally robust select s modeling scheme workflow is meanwhile this test sheds light on the temporal stationarity of the urbanization process applying an empirical model trained on past data to making future projections implicitly assumes the underlying process is static over time if this assumption does not hold model performance will be hampered even if the model is robustly developed although did not occur in this work if the single decade model trained on the first time period had performed poorly for the second period investigations into whether the real cause is model overfitting or misplaced stationarity assumption would be necessary 3 results discussion 3 1 select performance the model performed very well for short term estimation 2000 2010 out of the 375 sub national regions globally select explained more than 75 of the variation of decadal built up land change for 276 regions and more than 50 of the variation for 346 regions fig 8 b the magnitudes and variations of select s residuals are low table 3 the residuals are mostly unbiased numerically fig 8a and spatially fig 8c that is for most parts of the world overestimates and underestimates mix spatially rather than cluster except for one area along the southeast coast of china this area experienced drastic diverse urbanization that varied greatly over time during the three decades of observational data so it is a more challenging modeling task than most world regions that said although some clustering of the model residuals occurred in that area the magnitudes of the residuals are not alarming also as the spatial scale of the residual analysis decreases from global to regional table 3 the average magnitudes of residuals did not show substantial increases as one would generally expect indicating the model s potential multi scale applicability introduced by modeling the world as many sub national regions of course good performance in short term evaluations cannot guarantee a model s usability for long term projections as small biases if persistent can accumulate over time and have substantial impacts on projections far in the future hence it is crucial to examine the model s statistical robustness and generalizability select is statistically robust to determine the model s robustness we examined the similarity between two versions of gams trained respectively using the original vs the noise added response variables we pair wise compared the two versions of smooth transformations that make up the two versions of gams trained for the same explanatory variables in the same sub national regions and found corresponding smooth transformations are identical or close to identical across the two versions of gams fig 9 shows a few of the least similar pairs we saw in the analysis yet the smooth transformations i e the solid lines in fig 9a and 9b are still of very similar shapes pair wise this means select is capturing the generalizable trend of the urbanization process and is not overfitting select showed expected level of spatial generalizability for both example countries estimations made by models from neighboring sub national regions showed higher residuals than each region s indigenous model figs 10 and 8 for china models from more distant regions made worse estimations than models from nearby regions for the u s models from median distance neighbors made worse estimations than the farthest neighbor models fig 10 due to the geography of urban areas in the country namely most of the large urban areas in the u s locate along the coasts for example a californian region s farthest neighbor is in new england while its median distance neighbor is around kansas although land development processes in the northeast and california are different the process in rural midwest differs more from both of the highly urbanized coastal regions and its model is therefore naturally less generalizable to those regions our experiment results for both example countries show that much spatial heterogeneity exists in the urbanization process and it is necessary to capture sub national fine spatial scale variations especially for large developing countries moreover the results endorse the use and our delineation of the sub national regions in select s model design select showed expected level of temporal generalizability as expected the test model trained with data of a shorter time series than the standard select showed higher mean magnitude of residual for the independent validation time period 0 00068 than the training period 0 00032 while the magnitude of validation residuals remained low relative to the global average built up land fraction across all grid cells 0 00553 numerically the validation residuals are mostly unbiased but spatially they are clustered fig 11 the model generally overestimated new development potentials around existing established cities which is consistent with the known global trend that in recent years more urban development occurs in and around medium to small settlement sites united nations 2015 like all empirical models select may encounter performance challenges when extrapolating outside its training range for long term applications it is therefore necessary to pair select with scenarios e g those based on the ssps that both account for a wide range of uncertainties and provide boundary conditions that can ground the spatial model s behavior when extrapolating over time results from this experiment also highlight the advantage of using longer historical time series in model training for that reason the standard select is likely more robust than the test model used in this experiment 3 2 built up land in 2030 model comparison under our hypothesized high development scenario by 2030 new built up land of 84 173 km2 will be developed in the u s and 92 855 km2 in china the spatial distributions of these national totals generated by select and urbanmod show distinctively different patterns fig 12 below we discuss the two most prominent ones 1 the two models identified different regions as hotspots for new development fig 12 urbanmod and spatial interaction based models in general considers implicitly or explicitly that the present amount of built up land at a location is an indicator of the place s attractiveness for new development as a result these models assign high future development potentials to regions with high amounts of built up in the base year figs 7a and 12a in contrast select learned from time series data and assigns high future development potentials to regions that show similar characteristics to where fast development occurred during the training period the model therefore is capable of reproducing spatial patterns that are consistent with observed data at regional and local scales as historical data showed in some sub national regions new land development primarily increased the density of built up land in existing cities while in some other sub national regions new land development primarily expanded the horizontal extent of existing settlement sites while carrying these patterns forward into the future select overall tends to allocate more new built up land to regions that historically developed in more expansive fashions and to areas around smaller settlement sites in addition to already mega cities figs 7b and 12b 2 urbanmod and spatial interaction based models in general incrementally expands existing settlement sites as new development occurs while select gives similar estimates usually of low magnitude but non zero to all grid cells within large homogeneous undeveloped areas select s spatial built up land change model estimates statistical expectations of development potentials at various grid cells which can be viewed as the increase of a grid cell s built up land fraction the frequentist interpretation or the probability of a grid cell becoming further developed the probabilistic interpretation in reality usually what happens is a small percentage of the undeveloped grid cells sometimes transform into new settlement sites while the vast majority of the grids remain undeveloped the probabilistic interpretation and it is unlikely that all undeveloped grid cells become developed evenly by a small amount the frequentist interpretation nonetheless select s sub national allocation algorithm reflects the frequentist view by distributing national total amounts of new built up development in direct proportion to each grid cell s development potential estimated by select s spatial built up change model although this may seem unrealistic especially for short term projections we found it more suitable for long term large scale studies comparing to alternative methods for two reasons i there usually is not enough information available for built up land change models to identify the exact location of the next new settlement site a commonly used method to break ties in estimated built up development potentials is to randomly choose and develop a subset of all the grid cells with the same development potential though this may generate a visually more realistic look it introduces a tremendous amount of uncertainty into the results for long term modeling because an initially randomly selected location can attract more future development and grow into sizeable settlement sites over time a k a the path dependency effect this method would end up randomly placing new settlement sites across space over long term ii for long term large scale studies more emphasis lies in general trends rather than exact locations select excels at capturing regional trends by using the sub national regions in coordination with spatially varying explanatory factors capturing local dynamics additionally the model s empirical components are defensible and well performing according to the validation results if a region historically had more new towns booming out of undeveloped areas select s projections will show that pattern and over time new settlement sites will form at locations that show relative advantage for new development however small that may be according to statistical best guesses of probability 4 conclusions this research took a data driven approach to developing a long term spatially explicit urban land change model oriented towards the needs of global environmental change impact assessments the resulting model select showed a high level of performance and great promise for contributing to long term large scale investigations of human environment interactions select functions at multiple spatial scales and captures local variations of urbanization by relying on non parametric statistical modeling techniques using heuristic indices describing the spatial and temporal dynamics of built up change as explanatory variables and treating 375 sub national regions across the world separately the model grounds its long term behaviors by using exogenously estimated national total amounts of built up land change providing an easy interface with scenario analyses having new land development spatially follow potential future population change at the subnational level and allowing the model to use spatial rural urban transitions as analogies to temporal rural urban transitions we found that the newly available time series data of fine spatial resolution built up land and population greatly improved our ability to model long term built up change and that the data driven method is advantageous compared to existing large scale urban land change models in particular select can automatically capture the multiple scale spatiotemporal variations of long term urbanization and reproduce generalizable spatial temporal trends shown by data while spatial interaction based models allocate more new development to areas with high densities of existing built up land select allocates more new development to places where rapid urbanization is likely to occur according to observed past patterns and trends e g areas around smaller settlement sites in fast developing regions we also found using statistical learning models in large scale long term spatial modeling of built up land both challenging and rewarding since it is still a novelty designing and validating model features appropriate for long term projections was accomplished with little precedent for guidance nonetheless the final product was satisfactory select scored well in all performance and robustness tests that we ran for future work we are currently developing national level urban land development scenarios that are consistent with the ssps and will combine those scenarios with select to generate ssp consistent spatial built up land map series for the 21st century we plan to implement select with a finer spatial resolution along with other improvements for its next version and will downscale the 1 8 degree modeling results of current select as an interim product declarations of interest none involvement of funding source in conduct of the research and preparation of the article none acknowledgements this work was supported by the national science foundation through programs ibss grant number 1416860 and easm2 grant number 1243095 and the department of energy through the facets project grant number de sc0016438 we thank doug nychka and claudia tebaldi for their constructive input and suggestions appendix a delineating sub national regions for local dynamic modeling the sub national regions were delineated based on the location and the density of existing cities with a population size 300 000 according to wup we started with generating thiessen polygons of these existing cities thiessen polygons boundaries define the area that is closest to each city relative to all others and can be viewed as areas of influence around each of the cities for parts of the world e g the area shown in fig a1 a existing settlement sites cluster while for some other parts e g northern canada few settlement sites exist as a result the size of thiessen polygons vary greatly we made the following adjustments to the original thiessen polygons to i add the influence of national boundaries ii guarantee each sub national region consists of enough grid cells as training data points to parameterize its own local dynamic model part with confidence and iii unify sizes of sub national regions across the world so that they capture roughly the same spatial scale fig a1 b 1 countries that consist of more than 300 grid cells and only one city are considered individual regions 2 countries that consist of 300 1200 grid cells and more than one city are considered individual regions 3 countries that consist of more than 1200 grid cells and more than one city enter an automated process which iteratively aggregates thiessen polygons consisting of less than 600 grid cells to the polygon that has the longest shared border with them the process stops for a country when all of its sub national regions consist of more than 600 grid cells 4 countries that consist of less than 300 grid cells and at least one city are added to a nearby region as a whole 5 countries that consist of no cities are divided by the thiessen polygons of closest international cities 6 due to land continuity alaska is merged with sub national regions of canada for the local dynamic model part while being part of the u s for the sub national spatial allocation algorithm 7 small patches of isolated grid cells e g islands are merged into bordering nearby regions for example most pacific islands are merged with hawaii as one region for the local dynamic model part so that their regional model could use the spatial transition of urbanization in hawaii as an analogy for temporal urban land transitions despite that these islands have been historically underdeveloped fig a1 delineating sub national regions according to location and density of existing cities a thiessen polygons of existing cities b sub national regions after adjustments fig a1 appendix b comparing various statistical models for local dynamic modeling conceptually gam s intrinsic characteristics well match the needs of this modeling task nonetheless we compared its numerical performance with a few alternative statistical models for continental u s below table b1 fig b1 we present a summary of this comparison note that all performance measures presented here are for the local dynamic modeling task only different from the performance measures presented in the main text that evaluated the entire select workflow table b1 model comparison estimation residuals for short term 2000 2010 table b1 mean magnitude of residual maximum magnitude of residual gam 0 000816 0 072151 lm 0 001273 0 128758 lasso 0 001267 0 166366 lasso sq 0 001122 0 108371 fig b1 model comparison fraction of response variable deviance explained by model for short term 2000 2010 fig b1 
26191,built up land impervious surface expansion links urbanization and environmental change to enable large scale long term spatially explicit studies we took a data driven approach exploiting newly available time series of fine spatial resolution remote sensing observations and developed the spatially explicit long term empirical city development select model closely calibrated to observational data select functions at several spatial scales with multiple design traits capturing local variations of urbanization and ensuring performance for long term extrapolations in scenario analyses e g the shared socioeconomic pathways it showed low estimation residuals explained high fractions of the response s variations and scored well in all robustness and generalizability tests we ran when compared with a typical spatial interaction based model for projecting global built up land in 2030 select allocated more new development to areas with similar characteristics to locations that exhibited expansive urban growth historically while the example spatial interaction based model allocated more new development to areas with high amounts of existing built up land keywords select built up impervious surface urban ssps land cover land use change 1 introduction built up land impervious surface expansion is a prevalent characteristic of contemporary urbanization and one of the most profound anthropogenic changes to the earth s surface aaas 2016 built up consists of primarily manmade materials e g cement asphalt steel glass etc it is a physical expression of the socioeconomic processes related to urbanization can greatly alter environmental and climatic dynamics and strongly affects the vulnerability of human societies to environmental stress at multiple scales e g s anchez rodŕıguez et al 2005 spatial patterns of built up land modify the intensity and the duration of extreme heat events e g oleson et al 2015 and play crucial roles in determining these events societal impacts on air conditioning related energy use and human health e g jones et al 2018 meanwhile new built up land often replaces prime farmland the competition between urban and agricultural land uses directly influences people s livelihood at local scale and potentially affects food production and security at larger scales e g d amour et al 2017 in this analysis we focus on built up land a k a developed land impervious surface despite the many possible definitions of urban land spatially explicit modeling of built up land that is consistent with commonly used long term societal scenarios can effectively contribute to integrated modeling and investigations of large scale socio environmental interactions e g those between global urbanization and climate change however existing global efforts projecting potential spatiotemporal patterns of future built up land are rudimentary they are usually based on either of two types of models 1 stylized fact models e g jackson et al 2010 klein goldewijk et al 2010 rely on simplifying assumptions about the relation between built up land and population such as thresholding population density to map land development intensity classes these models show a wide range of uncertainties due to alternative simplifying assumptions and often overlook important spatial and temporal variations as the simplifying assumptions are often strong and homogeneous e g schwanitz 2013 klein goldewijk et al 2010 2 spatial interaction based models e g li et al 2017 seto et al 2012 such as applications of cellular automata e g clarke and gaydos 1998 batty 2007 are widely used in the land cover land use change literature where common temporal horizons up to 3 decades and spatial scales local to regional are different from those of large scale impact assessments of climate and environmental change the latter often require global coverage longer temporal horizons e g throughout the 21st century and consistent integration with scenario frameworks addressing a wide range of societal and environmental uncertainties the existing dominance of these two types of modeling practices is related to the longstanding lack of time series of global spatial built up land change data while in recent years a few spatially explicit time series global datasets of built up land became available exploiting new data availability we took a data driven approach of statistical learning and data mining techniques and developed the spatially explicit long term empirical city development select model for mapping built up land change it functions at national sub national regional and local scales reflecting multiscale spatiotemporal dynamics in land development and its driving forces the model utilizes newly available historical time series of fine spatial resolution remote sensing observations of built up land spanning the past 40 years as well as best available historical data on spatial population and environmental variables e g topographic contexts access to waterbodies and change in population size to meet the needs of large scale impact assessment of global environmental change especially climate change select was designed to 1 use globally consistent spatial data while recognizing vibrant local variations in the urbanization process across the world 2 extrapolate well temporally under long term societal scenarios allowing places to switch between different development styles present in observational data e g currently undeveloped places may become rapidly expanding and currently fast developing places may stabilize and stop expanding and 3 offer easy integration with aggregate land development urbanization scenarios we are particularly interested in eventually using select under the ssp rcp scenario framework the ssp rcp framework is widely used for investigating interactions among future societal climate and environmental conditions ssps stand for the shared socioeconomic pathways o neill et al 2015 describing potential societal trends in demographics economics technological development and governance throughout the 21st century and rcps stand for the representative concentration pathways van vuuren et al 2014 driving commonly used climate scenarios the ssp rcp framework accounts for many important uncertainties in the long term future but built up urban land expansion has yet to be incorporated aside from the work presented here we are currently developing national level ssp consistent urbanization scenarios and plan to use them in combination with select to eventually produce a first set of ssp consistent spatial built up land map series for the 21st century select is unique and improves existing long term large scale spatial built up land change models in at least four major ways a it is closely calibrated to historical time series data in contrast to the common approach relying heavily on stylized assumptions about spatiotemporal interactions b it embodies multiple design features to capture sub national local variations in urbanization process and land use history in contrast to existing models regional or larger scale e g one model setup per continent c it incorporates multiple design features to ensure reasonable temporal extrapolation under scenarios accounting for long term uncertainties d it has been intensively validated through evaluations of statistical robustness spatial and temporal generalizability and comparison with an existing spatial interaction based model seto et al 2012 for mapping global built up land in 2030 2 materials and methods 2 1 spatial and temporal resolutions select can be trained and applied at many possible spatial resolutions but here we used 1 8 in the initial model development and testing and the global land is covered by 997 022 grid cells it is a balance between common spatial resolutions of climate modeling 0 5 2 and land cover change studies 30 500 m 1 8 degree roughly 14 km at the equator is also the spatial resolution of the ssp consistent spatial population projections s2 pop jones o neill 2016 which is an important driver used by select for built up land development patterns although 1 8 degree is coarser than typical spatial increments of built up land expansion spatial precision is achieved by modeling land development as a continuous variable change in the fraction of built up land within each grid cell rather than the more commonly used binary variable i e developed vs non developed or transition probability to be developed although intuitive this choice makes select numerically different from many common urban land change models because continuous and binary variables call for different analytical methods to model using this continuous response variable also makes the model applicable at a range of spatial resolutions without needing a structural change or losing spatial precision therefore select is more scalable than binary response models especially at coarse spatial resolutions temporally select was trained to make estimates at 10 year intervals this is the update pace of the historical time series data used in model training finer temporal resolution was not pursued because built up land expansions are often incremental and would require precisions beyond what current global observations could offer to be mapped accurately for shorter time intervals 2 2 data the model is based on a wide range of rich datasets table 1 the most noteworthy is a new time series of fine spatial resolution built up land map series spanning the past 40 years ghsl v 1 pesaresi et al 2015 prior to ghsl s release modis yearly land cover type had been considered the most accurate and spatially resolved global data for built up land potere et al 2009 however ghsl better fits the needs of built up land modeling than modis being landsat based ghsl has a finer spatial resolution 38 m than modis 500 m and offers a longer time series since 1975 than modis since 2001 moreover ghsl has been shown to outperform modis for mapping built up land especially in areas of low development densities leyk et al 2018 from ghsl fig 1 we derived the fraction of built up land within each grid cell at 1 8 degree resolution for three decades 1980 2010 at 10 year interval as well as spatial and temporal descriptive indices of the built up fraction maps spatial population is an important driver for modeling built up land change especially for projecting into long term futures we used gpw v 4 ciesin 2017 in model training and s2 pop 2016 for future projections gpw is a direct observation of historical spatial population it uniformly distributes population within the finest census spatial units available across the world s2 pop were quantitatively downscaled from ssp national population and urbanization projections kc lutz 2014 jiang and o neill 2015 using a parameterized gravity model reflecting the ssp narratives of spatial development patterns both are state of the art data for spatial population using s2 pop also helps select establish consistency with patterns already described by existing ssp components from these spatial population datasets we derived total population counts at 1 8 resolution decadal change and change ratio of the population counts since s2 pop 2016 is the basis for the spatial population variables in our model we used the same habitable land mask as s2 pop 2016 which accounts for waterbodies protected areas extreme topologies etc additionally considering s2 pop 2016 used grump v 1 ciesin 2011 as a base dataset we used grump s land water boundary for calculating land areas and distance to waterbodies at 1 8 resolution wup united nations 2015 includes a list of existing cities whose population sizes are greater than or equal to 300 000 these cities were used to generate the distance to sizeable cities variable in our model furthermore the locations and the densities of these cities were used to delineate boundaries of sub national regions that select models separately to account for spatial variations of the urbanization process at regional and local scales the 1 km gmted 2010 danielson and gesch 2011 provides a new level of detail in global topographic data it was used to calculate the mean elevation and the range of slope gradients an index of surface roughness within 1 8 grid cells 2 3 model design training select has two components that function at multiple spatial scales fig 2 the spatial built up land change model estimates for each grid cell decadal built up land development potentials using spatially explicit driving factors and built up land change history of the antecedent 20 years this is a data driven model trained using three decades 1980 2010 of observational data where the last decade 2000 2010 of built up land change is the response variable and information on the earlier two decades 1980 2000 provides a suite of explanatory variables the sub national spatial allocation algorithm takes in exogenously estimated national decadal total amount of new built up development first distributes it among sub national regions according to their anticipated population change and land use history and then allocates the sub national regional totals to 1 8 degree grids proportionally according to their development potential estimated by the spatial built up land change model using exogenous national total amounts allows select the flexibility to cooperate with future scenarios other than the ssps making it a general purpose urban land change model for long term spatial scenario analyses when making future projections the two components run in turn to generate a built up land map for one decade the model then updates the explanatory variables characterizing built up land change history for the antecedent 20 years of the next decade and subsequently makes projections for that decade the process repeats decade by decade to reach long term future 2 3 1 spatial built up land change model select s spatial built up land change model estimates the change in built up fraction for each grid cell by summing three parts part 1 general trend is a quadratic polynomial model qpm that captures the average grid cell level trajectory of land development applicable to all global grid cells generally when an undeveloped grid cell starts to become developed the speed of development is slow at first then gradually accelerates entering a fast developing phase and finally tapers as the place becomes and stays highly developed fig 3 shows this relationship between x built up land fraction within a 1 8 degree grid cell at the beginning of a decade and y change in that fraction during the decade with a modified scatterplot because currently the majority of global land grid cells are of low development density or not developed at all using all data points in model training will bias the resulting model s fit to emphasize low density development to generate a global trend line that well reflects patterns for all levels of development we divided the range of built up land fraction at the beginning of a decade i e 0 1 into bins of 0 05 width casted global grid cells into these bins based on their built up fraction at the beginning of a decade x and averaged for each bin its member grid cells change in built up fraction during the decade y these decadal binned means are visualized in fig 3 they show quadratic relationships respectively for all three decades with observational data and the qpm was parameterized using the stack of binned means from two decades 1980 1990 and 1990 2000 leaving out 2000 2010 the decade of the response variable in model training the final model is the gold line in fig 3 part 2 local dynamic is a unique model part enabling select to capture local variations in the process of built up land conversion across the world it achieves this through three design features a we divided the world into 375 sub national regions considering locations and densities of existing cities with a population size no less than 300 000 fig 4 and model each region separately we detail the procedure that delineated the sub national regions in appendix a essentially it starts with thiessen polygons of the above mentioned existing cities and aggregates small polygons in areas where cities cluster and incorporates national boundaries to generate sub national regions that mostly consist of 600 1200 grid cells each this procedure embodied two primary considerations i each region consists of at least one existing sizeable city and covers complete geographic gradients of urban rural land use transitions having the model see the geographic transition during training allows it to use that information as an analogy to temporal urban rural land use transitions so that when historically undeveloped areas enter high development scenarios in the future the model will not extrapolate erratically ii the regions are small enough to distinguish local scale variations big enough to encompass a fair number of grid cells so that every local model can be parameterized with confidence and are as similarly sized as possible to be of consistent spatial scale using these modeling units allows distinctively different treatments for different land development mechanisms of sub national local scales which improves existing large scale long term urban land change models that usually treat the world as a few or a few tens of regions aggregating multiple countries b select uses heuristic indices listed in the bottom right cell of table 2 as explanatory variables along with demographic and environmental drivers listed in top four rows in table 2 in the model these indices describe geometric and temporal patterns of built up land change calculated over local scale neighborhoods surrounding each grid cell they help capture local dynamics and address the issue that for many factors affecting built up land development e g land tenure regulations public policies cultural preferences etc there is no globally consistent dataset or representation that can be used by a large scale model assuming patterns of historical built up land change reflect the combined effects of all these factors the descriptive indices we generate here are proxies of the missing explanatory variables and the varying neighborhood sizes can help capture mechanisms functioning at different levels of local scale this redundancy although was built in for a good reason does introduce some correlation among certain indices e g means over different neighborhood sizes to remove excessive collinearity we conducted a principal component analysis on the 108 descriptive indices and used the first 15 principal components pcs as explanatory variables in select the 15 pcs collectively explain more than 95 of cumulative variance and each has an eigenvalue greater than 0 5 we used 0 5 instead of 1 a commonly used rule of thumb as the cut off because it raised the explained percentage of cumulative variance to a more satisfactory level also the slightly lower ranked pcs are likely more useful for this model considering that the demographic and environmental drivers already capture the broad strokes of land development processes the most prominent pcs may offer similar information to what is already accounted for while the slightly lower ranked pcs may be more likely to bring new insights altogether 23 explanatory variables i e the first 15 pcs and the 8 drivers are used to train the statistical model described below c the local dynamic part of select is a non parametric statistical model generalized additive model gam gam estimates the response as the addition of smooth transformations of individual explanatory variables y ˆ α s 1 x 1 s 2 x 2 s p x p hastie 2017 the relationships between response and explanatory variables i e the smooth transformations are fully determined by data based training this feature is highly desirable for at least three reasons i analysts often do not have enough prior knowledge about the nature of relationships between new land development and its drivers to construct parametric models especially for those heuristic indices based explanatory variables table 2 ii these relationships are expected to vary among the 375 sub national regions and gam s flexibility allows select to capture such spatial variation automatically iii analysts can investigate the impacts of each explanatory variable on new land development in different parts of the world by examining the variables smooth transformations in different sub national regions after model training e g for the showcase gam in fig 5 the response y ˆ has a quadratic relation with x 1 a linear relation with x 2 and a logarithmic relation with x p besides the conceptual advantages we compared gam s numerical performance for continental u s with a few other modeling methods i least square linear model lm using the same set of explanatory variables ii lasso least absolute shrinkage and selection operator using the same explanatory variables and iii lasso using the same explanatory variables as well as their squares lasso sq to accommodate potential high power relationships in comparison gam showed lower magnitude residuals and explained higher fractions of variance in the response variable more detail in appendix b the gam based local dynamic model was implemented in r using the mgcv package which bases the smooth transformations on penalized splines each sub national region has its own gam parameterized using data from the region s constituent grid cells assuming a gaussian error distribution with an identity link function the default reml fitting algorithm was used for its ability to avoid overfitting for varying parameters wood 2011 additional experiments testing the models robustness against overfitting were conducted as part of model validation described in section 2 4 2 part 3 residuals of general trend and local dynamic are modeled as stochastic errors this part of the spatial built up land change model assumes residual errors follow independent and identical gaussian distribution at each grid cell within the same sub national region the residual model part was parameterized for each sub national region at the same time while the region s local dynamic gam was trained when using select for making statistically best guess estimates general trend and local dynamic are strung together as shown in fig 6 when stochastic estimates are desired e g for uncertainty analyses the residuals part of the model can be used to generate random noise of reasonable magnitudes and amounts to be added to best guess estimates 2 3 2 sub national spatial allocation algorithm the sub national spatial allocation algorithm links select s data driven spatiotemporal model and long term scenarios existing literature on land cover land use change modeling has found that accurately estimating the amount and the spatial pattern of change are two distinctively different tasks pontius and millones 2011 modeling the two separately therefore allows long term national scenarios based on macro scale relationships between built up land development and socioeconomic drivers e g population change economic growth to be spatially mapped in ways consistent with patterns shown by observational data for a given decade the sub national spatial allocation algorithm first allocates the exogenously estimated national total amount of new built up land development to the sub national regions in proportion to the products of the regions total population sizes in the end of the decade and their per capita built up land areas in base year i e 2000 using these sub national regional weights i e end of decade population size base year per capita built up land area the model can react to potential population migrations in long term societal scenarios if a sub national region gains a large amount of population during a decade new land development will follow when the model allocates that decade s national total new built up land and sub national regions losing population will see a halt on their land development also considering base year per capita built up land allows different regions to maintain their conventional preference for land development densities after distributing the national totals to sub national regions the regional total amount of new built up development is allocated to each region s constituent grid cells proportionally to the grid cells development potentials which is defined as the product of the grid cell s land area and its decadal δ built up fraction estimated by the spatial built up land change model sometimes especially under fast urban expansion scenarios the exogenously estimated national total amount of new built up land is so high that the proportional allocation for some grid cells may exceed the cell s total available land area to fulfill the national total the overflows are collected and re distributed using the same proportional process to grid cells that still have available land under extremely fast development scenarios the national and the regional total amounts of development may exceed all available land in grid cells that the spatial built up land model estimates where development may take place and the spill over amount of development will be allocated to all other grids in proportion to their built up land fractions at the beginning of the decade 2 4 model evaluation although no dataset is available to directly assess select s performance for long term spatially explicit modeling we evaluated its short and mid term applications and gained understanding of and confidence in its potential for longer term studies all performance descriptive indices were calculated for the entire model treating the combination of the spatial built up land change model and the sub national spatial allocation algorithm as a whole unless otherwise noted 2 4 1 short and mid term validations for short term evaluation we assessed select as a whole for estimating built up land change during the training period of its statistical components i e qpm pca and gam 2000 2010 we examined the magnitudes and the spatial patterns of the full model s residuals and inspected its ability to explain the response variable residual is defined at each grid cell as the difference between estimated and observed built up land fractions for mid term evaluation we compared select with an existing spatial interaction based model urbanmod used for mapping 2030 global built up land seto et al 2012 with 2000 as base year seto et al used urbanmod to generate a global probability map of 5 km resolution for built up land development by 2030 and created spatial built up land scenarios allocating different amounts of national total new land development by flipping 5 km grids from undeveloped to developed in a binary fashion according to ranking of the grids development probability within each nation grids with the highest probability are developed first only one development probability map was created and it was the basis for all spatial scenarios and all time points in contrast to select that allows the spatial patterns under each scenario to iteratively evolve decade by decade and responds to diverging drivers e g different spatial population patterns under different scenarios at grid cell and regional scales in addition to national scale besides shedding light on select s performance this model comparison will also provide general insights about how spatial statistical machine learning models and spatial interaction based models differ in long term built up land mapping because urbanmod s drivers including slope distance to roads snapshot population density in 2000 snapshot land cover type in 2000 and habitable land mask cover similar factors as select differences in model performance can be attributed primarily to the difference between the two modeling methods especially whether and how they use time series information the comparison used a scenario of very fast development as a stress test for the spatial models urbanmod and select mid term stress test helps understand long term model behaviors because the models get to allocate large amounts of new development despite the actual time span not being particularly long to construct the national scenario we first identified the maximum and the minimum observed decadal amounts of built up land change for each country over the three decades we have data and used the larger of the maximum value and twice the minimum as the decadal amount of new development the same national scenarios were applied to the two models while their base year maps came from different sources fig 7 to make it easier to compare the results of the two models we aggregated urbanmod s 5 km development probability map to 1 8 degree resolution treated it as a development potential map and allocated national total amount of new development proportionally to the grid cells development potential according to urbanmod this proportional allocation mechanism was described as ideal by seto et al 2012 although their application used a different process 2 4 2 robustness and generalizability tests as will be shown in the results section select showed a very high level of performance in evaluations outlined above considering the model s data driven nature it is important to test its robustness and generalizability eliminating potential harmful overfitting for this part of model evaluation we focused on the u s as an example of developed countries and china as an example of developing countries and conducted three sets of experiments 1 statistical robustness we used the residual part of select s spatial built up land change model to add stochastic noise to the observed response variable retrained the local dynamic part of the model using the noise added response and compared the resulting gams with the original gams the rationale is that if the select modeling scheme leads to overfitting the gams based on the noise added response would be very different from the gams based on the original response and if the modeling scheme is robust and captures generalizable effects the two sets of gams should be very similar e g goodfellow et al 2016 bishop 1995 to determine similarity we compare corresponding smooth transformations used by the two sets of gams for the same sub national region and the same explanatory variable 2 spatial generalizability select models sub national regions separately to capture local spatial variations in the urbanization process since the regional boundaries were properly delineated the local dynamic part trained for one region should not be expected to perform highly i e spatially generalizable for another region however it is reasonable to expect the model trained for a given region to perform better when applied to a nearby region than a region far away due to the general trend of spatial autocorrelation to examine whether this holds for select for each sub national region in the u s and china we i applied models trained for its nearest median distanced and farthest neighboring regions within the country ii calculated the residuals of these neighboring models when applied to the region and iii compared the estimation residuals of the three experiments with the residuals of the region s indigenous model if our modeling framework has reasonable spatial generalizability the magnitudes of residuals shown by these experiments should rank in descending order from farthest neighbor median distanced neighbor nearest neighbor to indigenous models this set of experiments was run only on the spatial built up land change model component of select because the national total amount of built up land change used by the sub national spatial allocation algorithm is a boundary condition that would moderate the spatial patterns we plan to examine in this experiment and may make the general trend difficult to identify with this setting we expect the magnitudes of residuals seen in this experiment to be noticeably higher than in other experiments 3 temporal generalizability since select is designed for long term modeling it should capture the general trend over time rather than being overly tuned to a single time epoch i e a single decade in this study the challenge for evaluating temporal generalizability is that only three decades of observational data are available to utilize as much temporal information as possible in the model the first two decades were used to generate explanatory variables for estimating change over the third decade and then no data is left for independent temporal validation as a substitute we trained the select modeling scheme with only the first decade of data for estimating change over the second decade the resulting model was then validated for estimating change during the third decade the single decade input model is of course different from the two decade input standard version of the model this exercise therefore is not a direct evaluation of how temporally generalizable select is but rather how temporally robust select s modeling scheme workflow is meanwhile this test sheds light on the temporal stationarity of the urbanization process applying an empirical model trained on past data to making future projections implicitly assumes the underlying process is static over time if this assumption does not hold model performance will be hampered even if the model is robustly developed although did not occur in this work if the single decade model trained on the first time period had performed poorly for the second period investigations into whether the real cause is model overfitting or misplaced stationarity assumption would be necessary 3 results discussion 3 1 select performance the model performed very well for short term estimation 2000 2010 out of the 375 sub national regions globally select explained more than 75 of the variation of decadal built up land change for 276 regions and more than 50 of the variation for 346 regions fig 8 b the magnitudes and variations of select s residuals are low table 3 the residuals are mostly unbiased numerically fig 8a and spatially fig 8c that is for most parts of the world overestimates and underestimates mix spatially rather than cluster except for one area along the southeast coast of china this area experienced drastic diverse urbanization that varied greatly over time during the three decades of observational data so it is a more challenging modeling task than most world regions that said although some clustering of the model residuals occurred in that area the magnitudes of the residuals are not alarming also as the spatial scale of the residual analysis decreases from global to regional table 3 the average magnitudes of residuals did not show substantial increases as one would generally expect indicating the model s potential multi scale applicability introduced by modeling the world as many sub national regions of course good performance in short term evaluations cannot guarantee a model s usability for long term projections as small biases if persistent can accumulate over time and have substantial impacts on projections far in the future hence it is crucial to examine the model s statistical robustness and generalizability select is statistically robust to determine the model s robustness we examined the similarity between two versions of gams trained respectively using the original vs the noise added response variables we pair wise compared the two versions of smooth transformations that make up the two versions of gams trained for the same explanatory variables in the same sub national regions and found corresponding smooth transformations are identical or close to identical across the two versions of gams fig 9 shows a few of the least similar pairs we saw in the analysis yet the smooth transformations i e the solid lines in fig 9a and 9b are still of very similar shapes pair wise this means select is capturing the generalizable trend of the urbanization process and is not overfitting select showed expected level of spatial generalizability for both example countries estimations made by models from neighboring sub national regions showed higher residuals than each region s indigenous model figs 10 and 8 for china models from more distant regions made worse estimations than models from nearby regions for the u s models from median distance neighbors made worse estimations than the farthest neighbor models fig 10 due to the geography of urban areas in the country namely most of the large urban areas in the u s locate along the coasts for example a californian region s farthest neighbor is in new england while its median distance neighbor is around kansas although land development processes in the northeast and california are different the process in rural midwest differs more from both of the highly urbanized coastal regions and its model is therefore naturally less generalizable to those regions our experiment results for both example countries show that much spatial heterogeneity exists in the urbanization process and it is necessary to capture sub national fine spatial scale variations especially for large developing countries moreover the results endorse the use and our delineation of the sub national regions in select s model design select showed expected level of temporal generalizability as expected the test model trained with data of a shorter time series than the standard select showed higher mean magnitude of residual for the independent validation time period 0 00068 than the training period 0 00032 while the magnitude of validation residuals remained low relative to the global average built up land fraction across all grid cells 0 00553 numerically the validation residuals are mostly unbiased but spatially they are clustered fig 11 the model generally overestimated new development potentials around existing established cities which is consistent with the known global trend that in recent years more urban development occurs in and around medium to small settlement sites united nations 2015 like all empirical models select may encounter performance challenges when extrapolating outside its training range for long term applications it is therefore necessary to pair select with scenarios e g those based on the ssps that both account for a wide range of uncertainties and provide boundary conditions that can ground the spatial model s behavior when extrapolating over time results from this experiment also highlight the advantage of using longer historical time series in model training for that reason the standard select is likely more robust than the test model used in this experiment 3 2 built up land in 2030 model comparison under our hypothesized high development scenario by 2030 new built up land of 84 173 km2 will be developed in the u s and 92 855 km2 in china the spatial distributions of these national totals generated by select and urbanmod show distinctively different patterns fig 12 below we discuss the two most prominent ones 1 the two models identified different regions as hotspots for new development fig 12 urbanmod and spatial interaction based models in general considers implicitly or explicitly that the present amount of built up land at a location is an indicator of the place s attractiveness for new development as a result these models assign high future development potentials to regions with high amounts of built up in the base year figs 7a and 12a in contrast select learned from time series data and assigns high future development potentials to regions that show similar characteristics to where fast development occurred during the training period the model therefore is capable of reproducing spatial patterns that are consistent with observed data at regional and local scales as historical data showed in some sub national regions new land development primarily increased the density of built up land in existing cities while in some other sub national regions new land development primarily expanded the horizontal extent of existing settlement sites while carrying these patterns forward into the future select overall tends to allocate more new built up land to regions that historically developed in more expansive fashions and to areas around smaller settlement sites in addition to already mega cities figs 7b and 12b 2 urbanmod and spatial interaction based models in general incrementally expands existing settlement sites as new development occurs while select gives similar estimates usually of low magnitude but non zero to all grid cells within large homogeneous undeveloped areas select s spatial built up land change model estimates statistical expectations of development potentials at various grid cells which can be viewed as the increase of a grid cell s built up land fraction the frequentist interpretation or the probability of a grid cell becoming further developed the probabilistic interpretation in reality usually what happens is a small percentage of the undeveloped grid cells sometimes transform into new settlement sites while the vast majority of the grids remain undeveloped the probabilistic interpretation and it is unlikely that all undeveloped grid cells become developed evenly by a small amount the frequentist interpretation nonetheless select s sub national allocation algorithm reflects the frequentist view by distributing national total amounts of new built up development in direct proportion to each grid cell s development potential estimated by select s spatial built up change model although this may seem unrealistic especially for short term projections we found it more suitable for long term large scale studies comparing to alternative methods for two reasons i there usually is not enough information available for built up land change models to identify the exact location of the next new settlement site a commonly used method to break ties in estimated built up development potentials is to randomly choose and develop a subset of all the grid cells with the same development potential though this may generate a visually more realistic look it introduces a tremendous amount of uncertainty into the results for long term modeling because an initially randomly selected location can attract more future development and grow into sizeable settlement sites over time a k a the path dependency effect this method would end up randomly placing new settlement sites across space over long term ii for long term large scale studies more emphasis lies in general trends rather than exact locations select excels at capturing regional trends by using the sub national regions in coordination with spatially varying explanatory factors capturing local dynamics additionally the model s empirical components are defensible and well performing according to the validation results if a region historically had more new towns booming out of undeveloped areas select s projections will show that pattern and over time new settlement sites will form at locations that show relative advantage for new development however small that may be according to statistical best guesses of probability 4 conclusions this research took a data driven approach to developing a long term spatially explicit urban land change model oriented towards the needs of global environmental change impact assessments the resulting model select showed a high level of performance and great promise for contributing to long term large scale investigations of human environment interactions select functions at multiple spatial scales and captures local variations of urbanization by relying on non parametric statistical modeling techniques using heuristic indices describing the spatial and temporal dynamics of built up change as explanatory variables and treating 375 sub national regions across the world separately the model grounds its long term behaviors by using exogenously estimated national total amounts of built up land change providing an easy interface with scenario analyses having new land development spatially follow potential future population change at the subnational level and allowing the model to use spatial rural urban transitions as analogies to temporal rural urban transitions we found that the newly available time series data of fine spatial resolution built up land and population greatly improved our ability to model long term built up change and that the data driven method is advantageous compared to existing large scale urban land change models in particular select can automatically capture the multiple scale spatiotemporal variations of long term urbanization and reproduce generalizable spatial temporal trends shown by data while spatial interaction based models allocate more new development to areas with high densities of existing built up land select allocates more new development to places where rapid urbanization is likely to occur according to observed past patterns and trends e g areas around smaller settlement sites in fast developing regions we also found using statistical learning models in large scale long term spatial modeling of built up land both challenging and rewarding since it is still a novelty designing and validating model features appropriate for long term projections was accomplished with little precedent for guidance nonetheless the final product was satisfactory select scored well in all performance and robustness tests that we ran for future work we are currently developing national level urban land development scenarios that are consistent with the ssps and will combine those scenarios with select to generate ssp consistent spatial built up land map series for the 21st century we plan to implement select with a finer spatial resolution along with other improvements for its next version and will downscale the 1 8 degree modeling results of current select as an interim product declarations of interest none involvement of funding source in conduct of the research and preparation of the article none acknowledgements this work was supported by the national science foundation through programs ibss grant number 1416860 and easm2 grant number 1243095 and the department of energy through the facets project grant number de sc0016438 we thank doug nychka and claudia tebaldi for their constructive input and suggestions appendix a delineating sub national regions for local dynamic modeling the sub national regions were delineated based on the location and the density of existing cities with a population size 300 000 according to wup we started with generating thiessen polygons of these existing cities thiessen polygons boundaries define the area that is closest to each city relative to all others and can be viewed as areas of influence around each of the cities for parts of the world e g the area shown in fig a1 a existing settlement sites cluster while for some other parts e g northern canada few settlement sites exist as a result the size of thiessen polygons vary greatly we made the following adjustments to the original thiessen polygons to i add the influence of national boundaries ii guarantee each sub national region consists of enough grid cells as training data points to parameterize its own local dynamic model part with confidence and iii unify sizes of sub national regions across the world so that they capture roughly the same spatial scale fig a1 b 1 countries that consist of more than 300 grid cells and only one city are considered individual regions 2 countries that consist of 300 1200 grid cells and more than one city are considered individual regions 3 countries that consist of more than 1200 grid cells and more than one city enter an automated process which iteratively aggregates thiessen polygons consisting of less than 600 grid cells to the polygon that has the longest shared border with them the process stops for a country when all of its sub national regions consist of more than 600 grid cells 4 countries that consist of less than 300 grid cells and at least one city are added to a nearby region as a whole 5 countries that consist of no cities are divided by the thiessen polygons of closest international cities 6 due to land continuity alaska is merged with sub national regions of canada for the local dynamic model part while being part of the u s for the sub national spatial allocation algorithm 7 small patches of isolated grid cells e g islands are merged into bordering nearby regions for example most pacific islands are merged with hawaii as one region for the local dynamic model part so that their regional model could use the spatial transition of urbanization in hawaii as an analogy for temporal urban land transitions despite that these islands have been historically underdeveloped fig a1 delineating sub national regions according to location and density of existing cities a thiessen polygons of existing cities b sub national regions after adjustments fig a1 appendix b comparing various statistical models for local dynamic modeling conceptually gam s intrinsic characteristics well match the needs of this modeling task nonetheless we compared its numerical performance with a few alternative statistical models for continental u s below table b1 fig b1 we present a summary of this comparison note that all performance measures presented here are for the local dynamic modeling task only different from the performance measures presented in the main text that evaluated the entire select workflow table b1 model comparison estimation residuals for short term 2000 2010 table b1 mean magnitude of residual maximum magnitude of residual gam 0 000816 0 072151 lm 0 001273 0 128758 lasso 0 001267 0 166366 lasso sq 0 001122 0 108371 fig b1 model comparison fraction of response variable deviance explained by model for short term 2000 2010 fig b1 
26192,integrated environmental modelling iem couples environmental models geoprocessing algorithms and data together to solve complex environmental problems there are two prominent modelling frameworks for iem component based and service oriented frameworks the former one has been widely employed in local computer environments and has its advantage in having concrete and community wide tools the latter one can take advantage of web technologies for integrating distributed environmental models however the complex interaction between environmental models such as time marching models is still a challenge for integrating models on the web this paper suggests to take the best of both frameworks models are shared on the web as websocket services a hybrid approach to leverage openmi and websocket for coupling components and services is proposed to provide a flexible iem environment the result illustrates the applicability of the approach and demonstrates the advantage of websocket over the traditional http approach in model execution keywords environmental modelling model web openmi websocket workflow 1 introduction integrating and accessing multiple models are often needed in environment modelling laniak et al 2013a parker et al 2002 as the spread use of inter disciplinary knowledge and network standards and tools for integrated environmental modelling iem are needed to facilitate the discovery access and integration of various modelling component like data and models from different vendors laniak et al 2013b the component based modelling frameworks for iem tools have been investigated extensively in the iem domain hill et al 2004 bernholdt et al 2006 gregersen et al 2007 peckham et al 2013 granell et al 2013 the models are wrapped as software components and interact with each other through component interfaces nativi et al 2013 for example the recent community consensus on openmi open modelling interface specifies a set of interfaces for component access as well as how the data is being exchanged between components vanecek 2014 the component based iem often requires the same programing language the same computer hardware architecture and works in local computer environments castronova et al 2013a recent work such as the vision of the model web tries to leverage web technologies for distributed environmental modelling nativi et al 2013 yue et al 2016 the model as a service maas geller and melton 2008 geller and turner 2007 approach is being used in the model web initiative to provide an engineering approach towards the implementation of integrated modelling systems layered on environment information infrastructures nativi et al 2013 geller and turner 2007 yue et al 2015a distributed geoprocessing algorithms models and data are accessed as services and coupled together into workflows for integrated environmental modelling iem thus increasing the flexibility extensibility and interoperability of iem systems nativi and fox 2010 granell et al 2010 foster and kesselman 2006 yue et al 2010 although the web has shown great potential to provide models on remote computers it is still in the early stages there are still some challenges when moving model interactions from traditional personal computer environments to distributed service environments on the web one is that in some cases iem needs dynamic and complex interaction between modelling components laniak et al 2013b for example the numerical time marching models often require a time step dependent loop for data exchanges such functionality is hard to achieve sufficient reliability and efficiency in the web environment laniak et al 2013b in this paper we propose to couple components and services for integrated environmental modelling we suggest providing models on the web through the websocket protocol which is particularly beneficial for time step dependent communications between distributed models the component based framework has been widely employed in local computer environments and has its advantage in having concrete and community wide tools on the one hand the legacy openmi model component can be provided on the web using the websocket protocol on the other hand the legacy iem tools can be extended to include the websocket services in this way the legacy modelling systems are improved to integrate distributed models the approach is implemented by extending an existing iem tool named geojmodelbuilder the result illustrates the applicability of the approach and demonstrates the advantage of websocket over the traditional http approach in model execution the remainder of the paper is structured as follows section 2 introduces some background concepts and related work section 3 presents two iem scenarios the proposed method is described in section 4 section 5 presents the implementation evaluation and discussion are provided in section 6 finally section 7 presents conclusions and future work 2 backgrounds and related work traditional iem tools often adopt component based modelling frameworks granell et al 2013 such as open modelling interface openmi gregersen et al 2007 common component architecture cca bernholdt et al 2006 earth system modelling framework esmf hill et al 2004 and community surface dynamics modelling system csdms peckham et al 2013 among them the openmi has been proposed as a global standard and many existing modelling frameworks are moving forward to accommodate the standard laniak et al 2013b zhang et al 2017 for example some work presented their experience in using openmi and suggested advancing openmi when performing integrated water resources modelling buahin and horsburgh 2018 it also becomes an ogc open geospatial consortium standard in the geospatial community vanecek 2014 openmi defines three fundamental concepts a linkable component an exchange item and a link castronova et al 2013b a linkable component wraps the internal business logic of a model exchange items are data communicated between components links define the source and destination for exchange items and are used to couple components together there are two significant methods i e initialize and getvalues in openmi the former one initializes models with predefined values the latter one enables components to get values of variables from other components goodall et al 2011 values exchanged between openmi components include what they represent iquantity where they apply ielementset and when they happen itime the service oriented framework advertises models as services geller and turner 2007 geller and melton 2008 nativi et al 2013 the use of web service technologies for sharing models on the web allows that iem is not limited to the same programing language and cross platform compared to component based modelling frameworks there has been extensive work on providing geospatial data and data processing algorithms on the web as services especially in the geospatial community ogc 2017 yue et al 2010 the term geoprocessing algorithms means the traditional data processing and analysis functions in geographic information systems gis for example the open geospatial consortium ogc has defined the web processing service wps specification to provide geoprocessing algorithms on the web schut 2007 it specifies standard operations over the http protocol and is widely used in the geospatial community yue et al 2015b there are still some challenges to provide models as web services some work in the environmental community investigated the possibility of applying wps to provide models on the web and found that the current wps has some limitations in dealing with different phases of simulation in models in the environmental community castronova et al 2013a the feedback loop in model interactions often requires a full duplex communication this is nontrivial in implementations based on the http some service oriented systems have been developed for sharing heterogeneous geo simulation models zhang et al 2019 it is worthwhile to further explore how different protocols could fit into the complex time step dependent model interactions in service systems the communication established with http is often a short lived stateless and unidirectional connection pimentel and nickerson 2012 the time step dependent feedback interactions between models on the web requires server side push function and long lived connection in the past decades some methods have been proposed to support the server side push function such as http polling and long polling wang et al 2013 http polling is a conventional method in which the client sends requests at intervals regardless of whether the server has an update the method seems to achieve a real time connection but the deficiency of which is that it would lead a bad performance due to too many unnecessary requests the long polling method is then proposed where the server will block the request until the server has an update for the client or the time expires pimentel and nickerson 2012 however the client needs to reconnect to the server to fetch new information which leads to the bad performance when the information on the server updates frequently another issue with long polling is the lack of standard implementations wang et al 2013 to address these short comings websocket was developed which has been included in the html5 specification recommended by the world wide web consortium w3c websocket is a web protocol based on tcp transmission control protocol and updated from http protocol w3c 2016 it does not need to close the connection maintains a long lived state and can allow the server side model to send messages to the client side model actively besides websocket protocol reduces much overhead in communication compared to http therefore we propose to use the websocket to expose models as services to address the low efficiency and time consuming deficiency on models as services using the http protocol 3 iem scenarios this section focuses two environmental cases to address the feasibility of coupling websocket services and openmi components for iem the two cases could help test the applicability of the approach on different models 3 1 runoff simulation the runoff simulation case fig 1 involves several hydrological processes precipitation evapotranspiration and runoff the precipitation and evapotranspiration are time dependent processes that will generate runoff on an area the primary model used to generate runoff is the topography based hydrological model topmodel which is a continuous simulation model and has been applied in various regions throughout the world castronova et al 2013a beven 2012 the model takes the daily precipitation daily evapotranspiration and topographic index as inputs and outputs daily runoff beven 1997 the runoff consists of two parts the subsurface and overland flow castronova et al 2013a hornberger 1998 the equation is 1 q t o t a l t m a x exp λ exp s m a s a t a p q r e t u r n where t m a x is the saturated soil transmissivity λ is the average topographic indices of watershed s is the average saturation deficit and m is a scaling parameter a s a t is the saturated soil area a is the watershed area p is the rate of throughfall and q r e t u r n is the return flow another model is the hargreaves model which takes daily temperature as an input and produces daily evapotranspiration hargreaves and samani 1985 hargreaves and allen 2003 the equation used in the model is shown as follows 2 e t 0 c r a t m a x t m i n e t a v e r t where e t 0 is the potential evapotranspiration value r a is the extraterrestrial radiation depends on the julian day number and latitude t m a x t m i n and t a v e r are the daily maximum minimum and average temperature c e and t are three empirical coefficients 3 2 waterlogging disaster simulation the waterlogging disaster case adopts a different model on runoff generation and connects it with a waterlogging model to derive water elevation the soil conservation service curve number scscn model is used to simulate the runoff in the case mishra and singh 2013 the equation is 3 q p i a 2 p s i a p i a 0 p i a where q is the runoff volume p is the precipitation i a is an initial abstraction of precipitation volume and s is the potential maximum retention since i a is affected by various factors like soil infiltration plant interception and depression detention the case uses an empirical method to calculate i a the equation is 4 i a 0 2 s 5 s 25400 c n 254 where cn can be obtained from a cn table of the local region which is determined by the underlying surface another model is used to calculate the volume of water accumulation in a certain area the equation is adopted from zhang and pan 2014 6 w q 1000 a d 7 d d t n where w is the total volume of water accumulation q is the runoff volume a is the area of the rainfall region d is the overall drainage volume calculated from 7 where d is the average drainage volume per second t is the cumulative rainfall time and n is the number of drainage outlets the last model is to calculate the elevation of water accumulation in low altitude area based on a gis identical bulk model yin et al 2011 shown in equation 8 8 w i 1 n e w e g i δ σ 0 where e g i is the elevation of each grid cell δ σ is the area of grid n is the cell number of the dem grid and e w is the elevation of water to be derived 4 design and method in this section we describe how to couple components and services for integrated environmental modelling our investigation finds a match between websocket and openmi the three websocket methods onopen onmessage and onclose w3c 2016 match the three simulation phases of openmi initialize run and finish castronova et al 2013a elag et al 2011 such a match allows that we could either wrap openmi components as websocket services or use openmi components as clients to consume websocket services section 4 1 then we suggest that existing openmi based iem tools could be extended to accommodate models on the web thus increasing capabilities of legacy systems section 4 2 a model as a websocket service could be plugged into existing iem tools by creating an openmi component as a client furthermore we provide a uniform workflow interface to integrate different modelling components including remote websocket services or wps and local components openmi or geoprocessing algorithm libraries section 4 3 the workflow approach helps glue geoprocessing algorithms either local libraries or wps and models either openmi or websocket services together the separation of abstract and concrete layers of workflows provides a uniform view at the abstract layer and instantiates modelling components at the concrete layer 4 1 providing openmi compliant models as websocket services on the web this section shows how to wrap openmi compliant models as websocket services on the web fig 2 shows an uml sequence diagram illustrating the interaction flow between websocket api and openmi there are four event handlers that must be supported by the server to implement the websocket api coward 2013 onopen onmessage onclose and onerror in openmi there are three consecutive phases in model execution initialize run and finish castronova et al 2013a elag et al 2011 the investigation matches them as follows 1 the onopen method in the websocket server is used to confirm the connecting request from the client in this method the initialize phase of the openmi model on the server can be invoked where some pre processing with input files can be performed in particular time independent values can be set up for example the function to derive topographic index can be invoked in this method other time independent values like the scaling parameter the saturated soil transmissivity the interception parameter recession coefficient and grid cell size can be initialized in this method too 2 the onmessage method can be used for the time step based computation the data exchange during each time step will invoke the onmessage method which in turn drive the run phase of the openmi component i e the model execution at each time step time dependent values like daily precipitation and evapotranspiration will be transferred in this method 3 the onclose method is used to close the connection between the client and the server it will trigger the finish phase of the openmi component in which some post processing routines can be performed despite the similarity in capability to support model simulation the websocket includes an onerror method in which some code to monitor and process errors happened in communication can be added furthermore there is another issue that needs to be addressed in the web environment the traditional time step based computations in local computers is enabled by the procedural programming paradigm van roy and haridi 2004 it means that the computations will be performed step by step and the next time step model computation will not be performed until finishing the computation at the current time step the websocket programming follows the event driven programing paradigm where the program control flow is determined by events van roy and haridi 2004 the methods onopen onmessage onclose and onerror respond to different events respectively both the server and client will implement these methods to support bidirectional communication once the client side performs a time step it just sends a message data exchange to the remote server and will continue next steps no matter the computation on the remote server finishes or not this is inconsistent with the control flow of model computations once the computation finishes on the server side it will send a message data exchange back to the client side and the onmessage method in the client side will respond the event the model computation flow requires that the next step computation should wait until the client receives the results in the onmessage method to solve the inconsistency the thread mechanism is used the main thread in the client is used to perform time steps and the other thread is used to send and receive messages the main thread will wait until the communication thread receives the result message and then continue the next time step 4 2 using openmi based iem framework to consume modelling service once models are accessible on the web through websocket it is possible to reuse existing component based iem frameworks to consume models on remote servers thus improving capabilities of legacy iem systems in the study we select the openmi based iem framework as an example since openmi is standard as shown in fig 3 the integration of the websocket service and iem framework takes place between the backend of the iem framework and front end of websocket services the key is to make a websocket service accessible through an openmi interface following the three phases initialize run and finish of openmi components fig 3 shows that each phase the openmi component will communicate with websocket services using a middleware approach the middleware is a delegate for transform operations of openmi to websocket methods and communicate data exchanges between openmi components and websocket servers e g transforming the data with specific itime ielementset and iquantity into websocket messages the middleware also serves the role of websocket client to the websocket server in the initialize phase the middleware instantiates a websocket object to connect to the server side model and then some initialization parameters to the server side model the run phase consists of multiple time step computations which will involve not only the traditional data exchanges between openmi components but also the data exchanges in the web environment between openmi components and websocket services specifically the control center controls one openmi component using the getvalues method to pull required time dependent parameter values from another openmi component which in turn will send request messages to the websocket service then the model on the server side is triggered to finish the current time step computation and send results back the finish phase is invoked when the run phase is completed in this phase the whole simulation process is finished and the websocket connection is closed 4 3 coupling components and services in workflows it has shown that there are various components could be used in iem such as openmi components websocket services or wps each has its own advantages for example the wps has its wide application in providing geoprocessing algorithms and websocket is better for providing time step based complex models when designing the use interface of iem tools it is important to provide a user friendly and uniform interface to end users while at the same time hiding the underlying technical heterogeneity of modelling components in this study we propose to take a two layers of workflow approach where the abstract and concrete layers of workflows are advocated to separate the business logics from resource usage the abstract workflow can be instantiated to a concrete workflow where activity nodes in the abstract workflow are bound to specific services and components based on the type and parameter mapping fig 4 an activity is a functional unit in workflows which is represented in an input process output ipo form activities are connected by data flows data flows not only reflect data exchanges but also imply execution sequences of activities in addition it also implies the execution sequence is from activity a to activity b although the use of workflows for integrated environmental modelling is not new granell 2014 the work on the separation of the abstract and concrete layers for workflows in the iem domain is still missing such an approach could allow end users to focus on domain business logics instead of taking care of underlying technical details it also brings dynamics in discovering and binding different models in the broad web context compared to traditional pre specified models in local environments 5 implementation the approach is implemented using an existing geoprocessing tool named geojmodelbuilder which can manage and coordinate openmi models data analysis functions and sensors in a workflow environment yue et al 2015a we implement the middleware to support the interaction between openmi and websocket the models in the runoff simulation case have open source c implementations as openmi components in hydromodeler castronova et al 2013b and are further rewritten in java as openmi components in geojmodelbuilder yue et al 2015a for simplicity the precipitation is provided as an openmi component providing time series data castronova et al 2013b yue et al 2015a and the hargreaves model is also an openmi component the topmodel component is partitioned into the topmodel client and the topmodel server the topmodel client is an openmi component which delegates the model execution to the topmodel server implemented as a websocket service during the model simulation precipitation and hargreaves components provide precipitation and evapotranspiration to the topmdoel client at each time step and then the tomodel client derives the runoff for each time step by invoking the topmdoel server similarly the three models in the waterlogging case are implemented as openmi components javaee 7 0 version provides the support to websocket and is used for the implementation of both the topmodel client and topmodel server we use the apache tomcat 8 0 as the web container and the annotated endpoint to employ topmodel on the server table 1 shows the configuration file for the model with key value pairs using the topmodel as an example the values in the configuration file are adjustable to reflect different simulations fig 5 shows the workflow interface to integrate various modelling components together the workflow consists of three kinds of nodes activities represented graphically as rounded boxes data flowing between activities represented graphically as circles and triggers that start up the model simulation represented graphically as diamonds the tool allows users to drag and drop environmental models and geoprocessing functions from the left panel and compose them into workflows for integrated modelling in the figure the rounded boxes in the purple color represents models and the rounded boxes in the orange color represents geoprocessing functions there are three models in the workflow hargreaves topmodel and sosreader the topmodel needs time dependent precipitation data the precipitation could be retrieved from a model or observations from a sos at each time step the sos could be treated as a model to provide an output exchange item the location of the sensor in sos can be put in the element set and the observation observable property of the sensor can be assigned to the quantity in the output exchange item two geoprocessing functions rasterclip and topographicindex are chained together to provide the topographic index 6 evaluation and discussion 6 1 evaluation in the runoff simulation case we simulate the runoff of the coweeta 18 watershed located in the northwest of carolina u s a and the observation data in the experiment were collected from the hydro desktop ames et al 2012 hydro desktop is a freely available and open source software tool used for discovering downloading visualizing and analyzing hydrologic data a snippet of input and output of the simulation are presented in fig 6 the daily maximum temperature minimum temperature and average temperature as input data to the hargreaves model are presented in fig 6 a b illustrates the evapotranspiration results calculated by the hargreaves model the results show that the peak of potential evapotranspiration every year often happens in june july and august and the valley of evapotranspiration appears in january february and march the results are reasonable and which are consistent with the rainfall and illumination conditions each year fig 6 c shows the precipitation data as input to the topmodel fig 6 d illustrates the runoff results to evaluate the effectiveness of deploying models as websocket services we did a comparative analysis of performance over model simulation between websocket and http protocols in the experiments the topmodel is provided on the server and can be accessed through both the websocket and http protocols the http approach follows the same idea by castronova et al 2013a the model state is maintained artificially which can be done by storing data objects on the server after the model is initialized the data objects are referred through a randomly generated key which is shared by the client and server in the execute phase castronova et al 2013a in the experiments we use a universally unique identifier uuid in the communication between the client and server the performance of the topmodel services using the http and websocket is compared given different time steps and sizes of data exchanges respectively the time steps are adjusted by modifying the simulating time span of topmodel the sizes of exchanged data are controlled by adding specific sizes of bytes to the body of http tcp message the total time cost of the model simulation consists of the model computation against the data and protocol overhead for pulling the data all experiments are performed in a local area network lan environment and the network bandwidth is controlled at about 455 kb s since the communication is based on the http or websocket protocols the evaluation results still apply in the web environment both topmodel client and topmodel server use the personal computers pcs each pc is equipped with a 3 60 ghz inter r core tm i7 4790 processor and 8 0 gb of memory the client side runs the microsoft windows 10 and the server side runs the centos 6 5 linux it is noted that http 1 1 can support both long lived and short lived connections the experiments are performed against both types of http connections to provide a comprehensive understanding of the http and websocket performance in supporting environmental models fig 7 shows the total simulation time and protocol overhead of the long lived connection http short lived connection http and websocket approaches in the runoff simulation case using different sizes of exchange items and time steps fig 8 a and b show the protocol overhead difference between the http approaches and websocket approach in the runoff simulation case table 2 lists the network traffic of the long lived connection http and websocket when the size of exchanged data is 600 bytes the following results can be observed in the waterlogging disaster case we select an area in hubei china the area covers about 7000 square meters the range of the elevation is 74 33 m 84 47 m as shown in fig 9 the area of waterlogging has an obvious increase over time in the region to test the results observed in the runoff simulation case when comparing the http and websocket approaches a similar comparative analysis experiment evaluating performance of the three approaches is implemented in the experiments the model used to calculate the elevation of retained water is provided on the server and can be accessed through both the websocket and http protocols from fig 8 c and d we can conclude same results as the runoff simulation case showing the advantage of the websocket approach over the http approach 6 2 discussion the paper provides an engineering approach to couple components and services for integrated environmental modelling there are already many component based modelling frameworks and tools available and they work in practice however when moving models on the web the legacy software faces challenges in model interaction data communication and sufficient reliability laniak et al 2013b castronova et al 2013a the work shows how component based modelling and service oriented frameworks could be interleaved together on the one hand the work reuses existing iem technologies and software it wraps legacy openmi components as services and reuses the openmi based iem framework to consume model services so existing iem tools could be enhanced with new capabilities to use remote models on the other hand it matches methods between the websocket and openmi and shows that websocket technology is more suitable and efficient for time dependent model interaction on the web today s iem needs various resources such as data gis analysis functions and models these resources are often provided using different software technologies such as local api libraries or web services the heterogeneity problem is more prominent in the web environment where different computer platforms and programming languages could be used by different vendors to provide services on these resources we are facing challenges on model heterogeneity gis analysis heterogeneity and data heterogeneity yue et al 2016 models could take different forms such as empirical analytical statistical and numerical models some models are complex and include time step dependent feedback loops the geospatial domain has specified a set of interoperable standards for web services to provide gis analysis functions and data such as ogc wps web feature services wfs and web coverage services wcs ogc there is still no standard on model services on the web models as services on the web are still in their early stages nativi et al 2013 and we will see competing solutions like http and websocket are emerging the iem software systems are better to accommodate different implementations to this degree the two layers of workflow approach could separate the business concerns of modelers from the technology concerns of software developers the workflow approach also addresses the traditional concern of coupling environmental models and gis goodchild et al 1992 bennett 1997 huang and jiang 2002 the traditional gis analysis functions are time independent and follow unidirectional computation while complex models often need time dependent and bidirectional computation the heterogeneity is handled by the workflow engine which executes the concrete workflows pull data according to the time step loop when performing the model computation and push data when executing analysis functions to demonstrate the efficiency and feasibility of the websocket approach we performed two cases study and test experiments to share models on the web and compared the performance of the websocket and http approaches the results both show that the websocket approach has better performance than the http approach however currently the web service development adopts the http protocol and has the wide deployment for example the geospatial domain has specified the web service standard i e wps for sharing geoprocessing algorithms on the web using the wps for exposing models as services on the web can help standardize the simulation data communicated between the client and server castronova et al 2013a but there are still some limitations when using the wps standard for exposing models as web services first wps is designed for time independent computations and does not support the reference to previous computations the time dependent environmental models however often involve the initialize run and finish phases in the model simulation thus it requires extensions to handle time dependent modelling for maintaining the state of the server castronova et al 2013a second the wps only provides a skeleton for providing algorithms as processes in a service it does not standardize descriptions of specific processes the process descriptions for various geoprocessing functions are different such as the selection of vector raster text input data or multiple output data files for a given process although the wps describeprocess operation can retrieve the inputs and outputs metadata the regulation of inputs outputs for particular processes depends on the service providers schut 2007 michaelis and ames 2009 thus it still needs additional work to specify the encodings of simulation data transferred between client and server sides third the http requests responses have too much overhead than the websocket for example the http messages need to have a verb uri headers and a body in recent years some features including multiplexed request prioritized requests header compression and server pushed streams have been added in http 2 ietf 2015 compared to websocket http 2 still lags in performance in terms of protocol overhead although it provides compressed bidirectional communication shekhada et al 2018 it should be still noted that the standardization is still an open problem also for the websocket case the websocket provides communications of bytes over a tcp connection and does not dictate what the bytes can be in the future it is worthwhile to check the possibility of standardizing the simulation data in the websocket communication for geospatial standards 7 conclusions and future work this paper presents an approach on how to couple components and services together for integrated environmental modelling it leverages the existing openmi standard and websocket protocol and finds a match between them then it suggests coupling openmi and websocket for an open standard based integrated environmental modelling by wrapping openmi models as services and consuming services using openmi the workflow method helps to provide a logically unified user interface for front end modelling and glue resources with physically different technological implementations together the websocket method maintains the long lived connection in the time step loop computation and shows much better efficiency in communication than the http method when more time steps are involved wps2 0 has been proposed which provides a new conceptual model and more operations as a continuation of wps1 0 ogc 2015 we are thinking to create a websocket binding for wps2 0 the wps has its advantage in having domain consensus on operations and data schemes but it only includes the http binding for the wps operations which specifies the encoding of wps operations using http get post it is possible to specify a second binding for wps using websocket thus facilitating more competitive implementation of wps for model services in addition it is still necessary to extend the implementation and approach to more iem cases in these cases a comprehensive investigation of existing models is necessary identifying and classifying their data and simulation requirements this can help better overcome the data and model heterogeneity and improve the standardization effort software data availability the software and test data are available at https github com fgao1994 coupling components and services for iem acknowledgements we appreciate the reviewers and editors for their constructive comments that helped improve the quality of the paper the work was supported by major state research development program of china no 2017yfb0504103 national natural science foundation of china no 41722109 61825103 91738302 hubei provincial natural science foundation of china no 2018cfa053 nature science foundation innovation group project of hubei province china no 2016cfa003 and wuhan yellow crane talents science program 2016 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 003 
26192,integrated environmental modelling iem couples environmental models geoprocessing algorithms and data together to solve complex environmental problems there are two prominent modelling frameworks for iem component based and service oriented frameworks the former one has been widely employed in local computer environments and has its advantage in having concrete and community wide tools the latter one can take advantage of web technologies for integrating distributed environmental models however the complex interaction between environmental models such as time marching models is still a challenge for integrating models on the web this paper suggests to take the best of both frameworks models are shared on the web as websocket services a hybrid approach to leverage openmi and websocket for coupling components and services is proposed to provide a flexible iem environment the result illustrates the applicability of the approach and demonstrates the advantage of websocket over the traditional http approach in model execution keywords environmental modelling model web openmi websocket workflow 1 introduction integrating and accessing multiple models are often needed in environment modelling laniak et al 2013a parker et al 2002 as the spread use of inter disciplinary knowledge and network standards and tools for integrated environmental modelling iem are needed to facilitate the discovery access and integration of various modelling component like data and models from different vendors laniak et al 2013b the component based modelling frameworks for iem tools have been investigated extensively in the iem domain hill et al 2004 bernholdt et al 2006 gregersen et al 2007 peckham et al 2013 granell et al 2013 the models are wrapped as software components and interact with each other through component interfaces nativi et al 2013 for example the recent community consensus on openmi open modelling interface specifies a set of interfaces for component access as well as how the data is being exchanged between components vanecek 2014 the component based iem often requires the same programing language the same computer hardware architecture and works in local computer environments castronova et al 2013a recent work such as the vision of the model web tries to leverage web technologies for distributed environmental modelling nativi et al 2013 yue et al 2016 the model as a service maas geller and melton 2008 geller and turner 2007 approach is being used in the model web initiative to provide an engineering approach towards the implementation of integrated modelling systems layered on environment information infrastructures nativi et al 2013 geller and turner 2007 yue et al 2015a distributed geoprocessing algorithms models and data are accessed as services and coupled together into workflows for integrated environmental modelling iem thus increasing the flexibility extensibility and interoperability of iem systems nativi and fox 2010 granell et al 2010 foster and kesselman 2006 yue et al 2010 although the web has shown great potential to provide models on remote computers it is still in the early stages there are still some challenges when moving model interactions from traditional personal computer environments to distributed service environments on the web one is that in some cases iem needs dynamic and complex interaction between modelling components laniak et al 2013b for example the numerical time marching models often require a time step dependent loop for data exchanges such functionality is hard to achieve sufficient reliability and efficiency in the web environment laniak et al 2013b in this paper we propose to couple components and services for integrated environmental modelling we suggest providing models on the web through the websocket protocol which is particularly beneficial for time step dependent communications between distributed models the component based framework has been widely employed in local computer environments and has its advantage in having concrete and community wide tools on the one hand the legacy openmi model component can be provided on the web using the websocket protocol on the other hand the legacy iem tools can be extended to include the websocket services in this way the legacy modelling systems are improved to integrate distributed models the approach is implemented by extending an existing iem tool named geojmodelbuilder the result illustrates the applicability of the approach and demonstrates the advantage of websocket over the traditional http approach in model execution the remainder of the paper is structured as follows section 2 introduces some background concepts and related work section 3 presents two iem scenarios the proposed method is described in section 4 section 5 presents the implementation evaluation and discussion are provided in section 6 finally section 7 presents conclusions and future work 2 backgrounds and related work traditional iem tools often adopt component based modelling frameworks granell et al 2013 such as open modelling interface openmi gregersen et al 2007 common component architecture cca bernholdt et al 2006 earth system modelling framework esmf hill et al 2004 and community surface dynamics modelling system csdms peckham et al 2013 among them the openmi has been proposed as a global standard and many existing modelling frameworks are moving forward to accommodate the standard laniak et al 2013b zhang et al 2017 for example some work presented their experience in using openmi and suggested advancing openmi when performing integrated water resources modelling buahin and horsburgh 2018 it also becomes an ogc open geospatial consortium standard in the geospatial community vanecek 2014 openmi defines three fundamental concepts a linkable component an exchange item and a link castronova et al 2013b a linkable component wraps the internal business logic of a model exchange items are data communicated between components links define the source and destination for exchange items and are used to couple components together there are two significant methods i e initialize and getvalues in openmi the former one initializes models with predefined values the latter one enables components to get values of variables from other components goodall et al 2011 values exchanged between openmi components include what they represent iquantity where they apply ielementset and when they happen itime the service oriented framework advertises models as services geller and turner 2007 geller and melton 2008 nativi et al 2013 the use of web service technologies for sharing models on the web allows that iem is not limited to the same programing language and cross platform compared to component based modelling frameworks there has been extensive work on providing geospatial data and data processing algorithms on the web as services especially in the geospatial community ogc 2017 yue et al 2010 the term geoprocessing algorithms means the traditional data processing and analysis functions in geographic information systems gis for example the open geospatial consortium ogc has defined the web processing service wps specification to provide geoprocessing algorithms on the web schut 2007 it specifies standard operations over the http protocol and is widely used in the geospatial community yue et al 2015b there are still some challenges to provide models as web services some work in the environmental community investigated the possibility of applying wps to provide models on the web and found that the current wps has some limitations in dealing with different phases of simulation in models in the environmental community castronova et al 2013a the feedback loop in model interactions often requires a full duplex communication this is nontrivial in implementations based on the http some service oriented systems have been developed for sharing heterogeneous geo simulation models zhang et al 2019 it is worthwhile to further explore how different protocols could fit into the complex time step dependent model interactions in service systems the communication established with http is often a short lived stateless and unidirectional connection pimentel and nickerson 2012 the time step dependent feedback interactions between models on the web requires server side push function and long lived connection in the past decades some methods have been proposed to support the server side push function such as http polling and long polling wang et al 2013 http polling is a conventional method in which the client sends requests at intervals regardless of whether the server has an update the method seems to achieve a real time connection but the deficiency of which is that it would lead a bad performance due to too many unnecessary requests the long polling method is then proposed where the server will block the request until the server has an update for the client or the time expires pimentel and nickerson 2012 however the client needs to reconnect to the server to fetch new information which leads to the bad performance when the information on the server updates frequently another issue with long polling is the lack of standard implementations wang et al 2013 to address these short comings websocket was developed which has been included in the html5 specification recommended by the world wide web consortium w3c websocket is a web protocol based on tcp transmission control protocol and updated from http protocol w3c 2016 it does not need to close the connection maintains a long lived state and can allow the server side model to send messages to the client side model actively besides websocket protocol reduces much overhead in communication compared to http therefore we propose to use the websocket to expose models as services to address the low efficiency and time consuming deficiency on models as services using the http protocol 3 iem scenarios this section focuses two environmental cases to address the feasibility of coupling websocket services and openmi components for iem the two cases could help test the applicability of the approach on different models 3 1 runoff simulation the runoff simulation case fig 1 involves several hydrological processes precipitation evapotranspiration and runoff the precipitation and evapotranspiration are time dependent processes that will generate runoff on an area the primary model used to generate runoff is the topography based hydrological model topmodel which is a continuous simulation model and has been applied in various regions throughout the world castronova et al 2013a beven 2012 the model takes the daily precipitation daily evapotranspiration and topographic index as inputs and outputs daily runoff beven 1997 the runoff consists of two parts the subsurface and overland flow castronova et al 2013a hornberger 1998 the equation is 1 q t o t a l t m a x exp λ exp s m a s a t a p q r e t u r n where t m a x is the saturated soil transmissivity λ is the average topographic indices of watershed s is the average saturation deficit and m is a scaling parameter a s a t is the saturated soil area a is the watershed area p is the rate of throughfall and q r e t u r n is the return flow another model is the hargreaves model which takes daily temperature as an input and produces daily evapotranspiration hargreaves and samani 1985 hargreaves and allen 2003 the equation used in the model is shown as follows 2 e t 0 c r a t m a x t m i n e t a v e r t where e t 0 is the potential evapotranspiration value r a is the extraterrestrial radiation depends on the julian day number and latitude t m a x t m i n and t a v e r are the daily maximum minimum and average temperature c e and t are three empirical coefficients 3 2 waterlogging disaster simulation the waterlogging disaster case adopts a different model on runoff generation and connects it with a waterlogging model to derive water elevation the soil conservation service curve number scscn model is used to simulate the runoff in the case mishra and singh 2013 the equation is 3 q p i a 2 p s i a p i a 0 p i a where q is the runoff volume p is the precipitation i a is an initial abstraction of precipitation volume and s is the potential maximum retention since i a is affected by various factors like soil infiltration plant interception and depression detention the case uses an empirical method to calculate i a the equation is 4 i a 0 2 s 5 s 25400 c n 254 where cn can be obtained from a cn table of the local region which is determined by the underlying surface another model is used to calculate the volume of water accumulation in a certain area the equation is adopted from zhang and pan 2014 6 w q 1000 a d 7 d d t n where w is the total volume of water accumulation q is the runoff volume a is the area of the rainfall region d is the overall drainage volume calculated from 7 where d is the average drainage volume per second t is the cumulative rainfall time and n is the number of drainage outlets the last model is to calculate the elevation of water accumulation in low altitude area based on a gis identical bulk model yin et al 2011 shown in equation 8 8 w i 1 n e w e g i δ σ 0 where e g i is the elevation of each grid cell δ σ is the area of grid n is the cell number of the dem grid and e w is the elevation of water to be derived 4 design and method in this section we describe how to couple components and services for integrated environmental modelling our investigation finds a match between websocket and openmi the three websocket methods onopen onmessage and onclose w3c 2016 match the three simulation phases of openmi initialize run and finish castronova et al 2013a elag et al 2011 such a match allows that we could either wrap openmi components as websocket services or use openmi components as clients to consume websocket services section 4 1 then we suggest that existing openmi based iem tools could be extended to accommodate models on the web thus increasing capabilities of legacy systems section 4 2 a model as a websocket service could be plugged into existing iem tools by creating an openmi component as a client furthermore we provide a uniform workflow interface to integrate different modelling components including remote websocket services or wps and local components openmi or geoprocessing algorithm libraries section 4 3 the workflow approach helps glue geoprocessing algorithms either local libraries or wps and models either openmi or websocket services together the separation of abstract and concrete layers of workflows provides a uniform view at the abstract layer and instantiates modelling components at the concrete layer 4 1 providing openmi compliant models as websocket services on the web this section shows how to wrap openmi compliant models as websocket services on the web fig 2 shows an uml sequence diagram illustrating the interaction flow between websocket api and openmi there are four event handlers that must be supported by the server to implement the websocket api coward 2013 onopen onmessage onclose and onerror in openmi there are three consecutive phases in model execution initialize run and finish castronova et al 2013a elag et al 2011 the investigation matches them as follows 1 the onopen method in the websocket server is used to confirm the connecting request from the client in this method the initialize phase of the openmi model on the server can be invoked where some pre processing with input files can be performed in particular time independent values can be set up for example the function to derive topographic index can be invoked in this method other time independent values like the scaling parameter the saturated soil transmissivity the interception parameter recession coefficient and grid cell size can be initialized in this method too 2 the onmessage method can be used for the time step based computation the data exchange during each time step will invoke the onmessage method which in turn drive the run phase of the openmi component i e the model execution at each time step time dependent values like daily precipitation and evapotranspiration will be transferred in this method 3 the onclose method is used to close the connection between the client and the server it will trigger the finish phase of the openmi component in which some post processing routines can be performed despite the similarity in capability to support model simulation the websocket includes an onerror method in which some code to monitor and process errors happened in communication can be added furthermore there is another issue that needs to be addressed in the web environment the traditional time step based computations in local computers is enabled by the procedural programming paradigm van roy and haridi 2004 it means that the computations will be performed step by step and the next time step model computation will not be performed until finishing the computation at the current time step the websocket programming follows the event driven programing paradigm where the program control flow is determined by events van roy and haridi 2004 the methods onopen onmessage onclose and onerror respond to different events respectively both the server and client will implement these methods to support bidirectional communication once the client side performs a time step it just sends a message data exchange to the remote server and will continue next steps no matter the computation on the remote server finishes or not this is inconsistent with the control flow of model computations once the computation finishes on the server side it will send a message data exchange back to the client side and the onmessage method in the client side will respond the event the model computation flow requires that the next step computation should wait until the client receives the results in the onmessage method to solve the inconsistency the thread mechanism is used the main thread in the client is used to perform time steps and the other thread is used to send and receive messages the main thread will wait until the communication thread receives the result message and then continue the next time step 4 2 using openmi based iem framework to consume modelling service once models are accessible on the web through websocket it is possible to reuse existing component based iem frameworks to consume models on remote servers thus improving capabilities of legacy iem systems in the study we select the openmi based iem framework as an example since openmi is standard as shown in fig 3 the integration of the websocket service and iem framework takes place between the backend of the iem framework and front end of websocket services the key is to make a websocket service accessible through an openmi interface following the three phases initialize run and finish of openmi components fig 3 shows that each phase the openmi component will communicate with websocket services using a middleware approach the middleware is a delegate for transform operations of openmi to websocket methods and communicate data exchanges between openmi components and websocket servers e g transforming the data with specific itime ielementset and iquantity into websocket messages the middleware also serves the role of websocket client to the websocket server in the initialize phase the middleware instantiates a websocket object to connect to the server side model and then some initialization parameters to the server side model the run phase consists of multiple time step computations which will involve not only the traditional data exchanges between openmi components but also the data exchanges in the web environment between openmi components and websocket services specifically the control center controls one openmi component using the getvalues method to pull required time dependent parameter values from another openmi component which in turn will send request messages to the websocket service then the model on the server side is triggered to finish the current time step computation and send results back the finish phase is invoked when the run phase is completed in this phase the whole simulation process is finished and the websocket connection is closed 4 3 coupling components and services in workflows it has shown that there are various components could be used in iem such as openmi components websocket services or wps each has its own advantages for example the wps has its wide application in providing geoprocessing algorithms and websocket is better for providing time step based complex models when designing the use interface of iem tools it is important to provide a user friendly and uniform interface to end users while at the same time hiding the underlying technical heterogeneity of modelling components in this study we propose to take a two layers of workflow approach where the abstract and concrete layers of workflows are advocated to separate the business logics from resource usage the abstract workflow can be instantiated to a concrete workflow where activity nodes in the abstract workflow are bound to specific services and components based on the type and parameter mapping fig 4 an activity is a functional unit in workflows which is represented in an input process output ipo form activities are connected by data flows data flows not only reflect data exchanges but also imply execution sequences of activities in addition it also implies the execution sequence is from activity a to activity b although the use of workflows for integrated environmental modelling is not new granell 2014 the work on the separation of the abstract and concrete layers for workflows in the iem domain is still missing such an approach could allow end users to focus on domain business logics instead of taking care of underlying technical details it also brings dynamics in discovering and binding different models in the broad web context compared to traditional pre specified models in local environments 5 implementation the approach is implemented using an existing geoprocessing tool named geojmodelbuilder which can manage and coordinate openmi models data analysis functions and sensors in a workflow environment yue et al 2015a we implement the middleware to support the interaction between openmi and websocket the models in the runoff simulation case have open source c implementations as openmi components in hydromodeler castronova et al 2013b and are further rewritten in java as openmi components in geojmodelbuilder yue et al 2015a for simplicity the precipitation is provided as an openmi component providing time series data castronova et al 2013b yue et al 2015a and the hargreaves model is also an openmi component the topmodel component is partitioned into the topmodel client and the topmodel server the topmodel client is an openmi component which delegates the model execution to the topmodel server implemented as a websocket service during the model simulation precipitation and hargreaves components provide precipitation and evapotranspiration to the topmdoel client at each time step and then the tomodel client derives the runoff for each time step by invoking the topmdoel server similarly the three models in the waterlogging case are implemented as openmi components javaee 7 0 version provides the support to websocket and is used for the implementation of both the topmodel client and topmodel server we use the apache tomcat 8 0 as the web container and the annotated endpoint to employ topmodel on the server table 1 shows the configuration file for the model with key value pairs using the topmodel as an example the values in the configuration file are adjustable to reflect different simulations fig 5 shows the workflow interface to integrate various modelling components together the workflow consists of three kinds of nodes activities represented graphically as rounded boxes data flowing between activities represented graphically as circles and triggers that start up the model simulation represented graphically as diamonds the tool allows users to drag and drop environmental models and geoprocessing functions from the left panel and compose them into workflows for integrated modelling in the figure the rounded boxes in the purple color represents models and the rounded boxes in the orange color represents geoprocessing functions there are three models in the workflow hargreaves topmodel and sosreader the topmodel needs time dependent precipitation data the precipitation could be retrieved from a model or observations from a sos at each time step the sos could be treated as a model to provide an output exchange item the location of the sensor in sos can be put in the element set and the observation observable property of the sensor can be assigned to the quantity in the output exchange item two geoprocessing functions rasterclip and topographicindex are chained together to provide the topographic index 6 evaluation and discussion 6 1 evaluation in the runoff simulation case we simulate the runoff of the coweeta 18 watershed located in the northwest of carolina u s a and the observation data in the experiment were collected from the hydro desktop ames et al 2012 hydro desktop is a freely available and open source software tool used for discovering downloading visualizing and analyzing hydrologic data a snippet of input and output of the simulation are presented in fig 6 the daily maximum temperature minimum temperature and average temperature as input data to the hargreaves model are presented in fig 6 a b illustrates the evapotranspiration results calculated by the hargreaves model the results show that the peak of potential evapotranspiration every year often happens in june july and august and the valley of evapotranspiration appears in january february and march the results are reasonable and which are consistent with the rainfall and illumination conditions each year fig 6 c shows the precipitation data as input to the topmodel fig 6 d illustrates the runoff results to evaluate the effectiveness of deploying models as websocket services we did a comparative analysis of performance over model simulation between websocket and http protocols in the experiments the topmodel is provided on the server and can be accessed through both the websocket and http protocols the http approach follows the same idea by castronova et al 2013a the model state is maintained artificially which can be done by storing data objects on the server after the model is initialized the data objects are referred through a randomly generated key which is shared by the client and server in the execute phase castronova et al 2013a in the experiments we use a universally unique identifier uuid in the communication between the client and server the performance of the topmodel services using the http and websocket is compared given different time steps and sizes of data exchanges respectively the time steps are adjusted by modifying the simulating time span of topmodel the sizes of exchanged data are controlled by adding specific sizes of bytes to the body of http tcp message the total time cost of the model simulation consists of the model computation against the data and protocol overhead for pulling the data all experiments are performed in a local area network lan environment and the network bandwidth is controlled at about 455 kb s since the communication is based on the http or websocket protocols the evaluation results still apply in the web environment both topmodel client and topmodel server use the personal computers pcs each pc is equipped with a 3 60 ghz inter r core tm i7 4790 processor and 8 0 gb of memory the client side runs the microsoft windows 10 and the server side runs the centos 6 5 linux it is noted that http 1 1 can support both long lived and short lived connections the experiments are performed against both types of http connections to provide a comprehensive understanding of the http and websocket performance in supporting environmental models fig 7 shows the total simulation time and protocol overhead of the long lived connection http short lived connection http and websocket approaches in the runoff simulation case using different sizes of exchange items and time steps fig 8 a and b show the protocol overhead difference between the http approaches and websocket approach in the runoff simulation case table 2 lists the network traffic of the long lived connection http and websocket when the size of exchanged data is 600 bytes the following results can be observed in the waterlogging disaster case we select an area in hubei china the area covers about 7000 square meters the range of the elevation is 74 33 m 84 47 m as shown in fig 9 the area of waterlogging has an obvious increase over time in the region to test the results observed in the runoff simulation case when comparing the http and websocket approaches a similar comparative analysis experiment evaluating performance of the three approaches is implemented in the experiments the model used to calculate the elevation of retained water is provided on the server and can be accessed through both the websocket and http protocols from fig 8 c and d we can conclude same results as the runoff simulation case showing the advantage of the websocket approach over the http approach 6 2 discussion the paper provides an engineering approach to couple components and services for integrated environmental modelling there are already many component based modelling frameworks and tools available and they work in practice however when moving models on the web the legacy software faces challenges in model interaction data communication and sufficient reliability laniak et al 2013b castronova et al 2013a the work shows how component based modelling and service oriented frameworks could be interleaved together on the one hand the work reuses existing iem technologies and software it wraps legacy openmi components as services and reuses the openmi based iem framework to consume model services so existing iem tools could be enhanced with new capabilities to use remote models on the other hand it matches methods between the websocket and openmi and shows that websocket technology is more suitable and efficient for time dependent model interaction on the web today s iem needs various resources such as data gis analysis functions and models these resources are often provided using different software technologies such as local api libraries or web services the heterogeneity problem is more prominent in the web environment where different computer platforms and programming languages could be used by different vendors to provide services on these resources we are facing challenges on model heterogeneity gis analysis heterogeneity and data heterogeneity yue et al 2016 models could take different forms such as empirical analytical statistical and numerical models some models are complex and include time step dependent feedback loops the geospatial domain has specified a set of interoperable standards for web services to provide gis analysis functions and data such as ogc wps web feature services wfs and web coverage services wcs ogc there is still no standard on model services on the web models as services on the web are still in their early stages nativi et al 2013 and we will see competing solutions like http and websocket are emerging the iem software systems are better to accommodate different implementations to this degree the two layers of workflow approach could separate the business concerns of modelers from the technology concerns of software developers the workflow approach also addresses the traditional concern of coupling environmental models and gis goodchild et al 1992 bennett 1997 huang and jiang 2002 the traditional gis analysis functions are time independent and follow unidirectional computation while complex models often need time dependent and bidirectional computation the heterogeneity is handled by the workflow engine which executes the concrete workflows pull data according to the time step loop when performing the model computation and push data when executing analysis functions to demonstrate the efficiency and feasibility of the websocket approach we performed two cases study and test experiments to share models on the web and compared the performance of the websocket and http approaches the results both show that the websocket approach has better performance than the http approach however currently the web service development adopts the http protocol and has the wide deployment for example the geospatial domain has specified the web service standard i e wps for sharing geoprocessing algorithms on the web using the wps for exposing models as services on the web can help standardize the simulation data communicated between the client and server castronova et al 2013a but there are still some limitations when using the wps standard for exposing models as web services first wps is designed for time independent computations and does not support the reference to previous computations the time dependent environmental models however often involve the initialize run and finish phases in the model simulation thus it requires extensions to handle time dependent modelling for maintaining the state of the server castronova et al 2013a second the wps only provides a skeleton for providing algorithms as processes in a service it does not standardize descriptions of specific processes the process descriptions for various geoprocessing functions are different such as the selection of vector raster text input data or multiple output data files for a given process although the wps describeprocess operation can retrieve the inputs and outputs metadata the regulation of inputs outputs for particular processes depends on the service providers schut 2007 michaelis and ames 2009 thus it still needs additional work to specify the encodings of simulation data transferred between client and server sides third the http requests responses have too much overhead than the websocket for example the http messages need to have a verb uri headers and a body in recent years some features including multiplexed request prioritized requests header compression and server pushed streams have been added in http 2 ietf 2015 compared to websocket http 2 still lags in performance in terms of protocol overhead although it provides compressed bidirectional communication shekhada et al 2018 it should be still noted that the standardization is still an open problem also for the websocket case the websocket provides communications of bytes over a tcp connection and does not dictate what the bytes can be in the future it is worthwhile to check the possibility of standardizing the simulation data in the websocket communication for geospatial standards 7 conclusions and future work this paper presents an approach on how to couple components and services together for integrated environmental modelling it leverages the existing openmi standard and websocket protocol and finds a match between them then it suggests coupling openmi and websocket for an open standard based integrated environmental modelling by wrapping openmi models as services and consuming services using openmi the workflow method helps to provide a logically unified user interface for front end modelling and glue resources with physically different technological implementations together the websocket method maintains the long lived connection in the time step loop computation and shows much better efficiency in communication than the http method when more time steps are involved wps2 0 has been proposed which provides a new conceptual model and more operations as a continuation of wps1 0 ogc 2015 we are thinking to create a websocket binding for wps2 0 the wps has its advantage in having domain consensus on operations and data schemes but it only includes the http binding for the wps operations which specifies the encoding of wps operations using http get post it is possible to specify a second binding for wps using websocket thus facilitating more competitive implementation of wps for model services in addition it is still necessary to extend the implementation and approach to more iem cases in these cases a comprehensive investigation of existing models is necessary identifying and classifying their data and simulation requirements this can help better overcome the data and model heterogeneity and improve the standardization effort software data availability the software and test data are available at https github com fgao1994 coupling components and services for iem acknowledgements we appreciate the reviewers and editors for their constructive comments that helped improve the quality of the paper the work was supported by major state research development program of china no 2017yfb0504103 national natural science foundation of china no 41722109 61825103 91738302 hubei provincial natural science foundation of china no 2018cfa053 nature science foundation innovation group project of hubei province china no 2016cfa003 and wuhan yellow crane talents science program 2016 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 003 
26193,modeling alternative irrigation strategies can be a cost effective and time saving approach to field based experiments however the efficacy of irrigation scheduling algorithms should be verified using field data from multiple locations in this study an auto irrigation algorithm recently developed for soil and water assessment tool swat was further evaluated using irrigation data for corn zea mays l grown at six research sites across the southern great plains simulated monthly irrigation based on the management allowed depletion mad of plant available soil water was compared to measured data for irrigation applied in accordance with crop water requirement guidelines outlined by the food and agriculture organization irrigation and drainage paper 56 overall results indicated the mad algorithm simulated monthly field based irrigation amounts well nash sutcliffe efficiency nse 0 56 comparisons revealed the mad algorithm outperformed the plant water demand and soil water content approaches in swat which tended to underestimate and overestimate irrigations respectively keywords fao 56 irrigation algorithm management allowed depletion corn semi arid region groundwater software availability algorithm name mad irrigation algorithm description the mad algorithm simulates irrigation scheduling by incorporating an allowable depletion percentage of plant available soil water determined by crop specific maximum rooting depth and site specific soil properties into an irrigation trigger the algorithm also suspends irrigation events following crop harvest developed by y chen yongchen neo tamu edu and g w marek gary marek ars usda gov ecosystem science and management texas a m university college station texas and usda ars conservation and production research laboratory cprl bushland texas respectively year available 2018 availability contact developers cost free language fortran 1 introduction drought and water scarcity are the predominant elements constraining agricultural production in arid and semi arid regions of the world irrigation is a principal agricultural management practice for sustaining food security food and agriculture organization fao 2002 in many regions of the united states u s irrigation water supplies are declining due to frequent periods of drought claeys and inzé 2013 ongoing depletion of groundwater levels colaizzi et al 2009 mcguire 2004 regulatory restrictions on irrigation pumping high plains underground water conservation district hpuwcd 2016 north plains groundwater conservation district npgcd 2014 panhandle groundwater conservation district pgwcd 2016 and management policy shifting irrigation water use from agriculture to other uses payero et al 2006 the u s southern great plains consisting of texas colorado oklahoma new mexico kansas nebraska wyoming and south dakota is one of the most productive agricultural regions in the u s however this centrally located region also faces frequent droughts and decreased groundwater availability the study of alternative irrigation management strategies is therefore of utmost importance as it explores ways to improve current agricultural irrigation management practices in the southern great plains to promote more sustainable production and water conservation representative modeling can serve as a time and cost efficient tool for evaluating multiple irrigation scheduling algorithms chen et al 2018a li et al 2014 however the existing irrigation algorithms in many simulation models have demonstrated potential deficiencies chen et al 2017 marek et al 2017a panagopoulos et al 2014a and 2014b for example chen et al 2017 reported that the soil and water assessment tool swat model using the auto irrigation function soil water content option simulated actual crop evapotranspiration et reasonably well as compared to measured lysimeter values in the texas high plains nevertheless simulated irrigation amount and frequency varied greatly from actual irrigation events scheduled according to crop water requirement guidelines outlined by the fao irrigation and drainage paper 56 fao 56 marek et al 2017a found that simulated growing season irrigation of conventional crops in the texas high plains could approximate that of actual irrigation by adjusting the trigger threshold of the plant water demand approach in swat another existing auto irrigation function however the authors also suggested that use of the plant water demand option with no consideration of well capacity could impede reasonable simulation of irrigation particularly during drought years therefore a more representative auto irrigation algorithm is required to more accurately simulate field irrigation management the widely implemented management allowed depletion mad irrigation scheduling approach callison 2012 evett et al 2011 hao et al 2015 klocke et al 2011 lamm et al 1996 marek et al 2005 2009 and 2011 merriam 1966 payero et al 2008 suarez rey et al 2000 thorp et al 2017 trout and bausch 2017 vories et al 2017 allows a user producer defined percentage of plant available water paw depletion before irrigation is applied the mad irrigation concept is a common framework used for irrigation scheduling as outlined by merriam 1966 however the current auto irrigation functions in simulation models are unable to simulate mad irrigation scheduling recently chen et al 2018a developed a mad auto irrigation algorithm and incorporated it into the swat model ten years of agronomic data from lysimeter fields at the u s department of agriculture agricultural research service usda ars conservation and production research laboratory cprl at bushland texas were used to develop and test the mad algorithm the mad auto irrigation algorithm performed well nash sutcliffe efficiency nse 0 57 and percent bias pbias 21 7 using irrigated field data from bushland nevertheless a comprehensive evaluation of the mad auto irrigation algorithm and existing auto irrigation algorithms using field fao 56 irrigation data from multiple sites having different soil and climatic conditions is needed to warrant the acceptance use and distribution of the new auto irrigation algorithm corn zea mays l is one of the major field crops grown under irrigated conditions in the southern great plains as such corn irrigation management data are readily available from literature and field measurements in general precipitation during the growing season supplies only a portion of the corn s water demand or actual crop et and a large amount of irrigation is required to supplement the remaining portion irrigated winter wheat triticum aestivum l and cotton gossypium hirsutum l have reduced irrigation water use requirements by 27 and 37 respectively as compared to irrigated corn in the texas high plains texas water development board 2011 using the swat mad model chen et al 2018b found that approximately 19 21 and 32 reductions in annual irrigation amounts were associated with irrigated soybean glycine max l sunflower helianthus annuus l and sorghum sorghum bicolor l respectively as compared to irrigated corn in the northern high plains of texas several field corn studies have been conducted using the fao 56 irrigation spreadsheet methods in the southern great plains araya et al 2017 hao et al 2015 klocke et al 2011 marek et al 2017b payero et al 2006 2008 trout 2016 trout and bausch 2017 vories et al 2017 however limited studies report comparisons of different irrigation scheduling methods algorithms in simulation models with well tested fao 56 irrigation spreadsheet methods k state research extension mobile irrigation lab 2018 marek et al 2005 2009 pereira et al 2015 snyder 2014 thorp et al 2017 across multiple sites among simulation models with auto irrigation functions swat is a widely used license free and open source model the source code of swat 2012 revision 664 was used in this study in addition swat has two popular auto irrigation options based on plant water demand and soil water content in chen et al 2018a an evaluation of a sensitive and intuitive auto irrigation algorithm mad auto irrigation algorithm was shown to be representative of long term actual field based irrigation management practices at the usda ars cprl the objective of this study was to further evaluate the efficacy of the newly developed mad algorithm along with the two existing swat auto irrigation options using actual fao 56 based monthly field irrigation data from six study sites located in five states across the u s southern great plains 2 materials and methods 2 1 study sites grain corn was grown under full irrigation using a linear move sprinkler in a large lysimeter field located at the usda ars cprl bushland tx in 2013 and 2016 fig 1 and table 1 corn was also grown at the texas a m agrilife research north plains research field nprf etter tx from 2011 to 2014 under center pivot sprinkler irrigation and at usda ars limited irrigation research farm lirf greeley co from 2008 to 2011 under surface drip irrigation trout 2016 trout and bausch 2017 table 1 corn experiments were carried out at the kansas state university southwest research extension center swrec finnup farm near garden city ks from 2005 to 2012 under a linear move sprinkler system araya et al 2017 klocke et al 2011 corn experiments were conducted during the 2003 to 2006 growing seasons at the university of nebraska lincoln west central research and extension center wcrec north platte ne under subsurface drip irrigation payero et al 2006 2008 table 1 corn was grown at new mexico state university agricultural science center clovis nm from 2009 to 2012 under center pivot sprinkler irrigation marsalis et al 2009 2010 2011 2012 table 1 the regional climate of all six study sites is classified as semi arid peel et al 2007 average annual precipitation during the study years ranged between 250 and 506 mm table 2 average annual maximum temperatures varied between 17 3 c and 22 8 c and minimum temperatures were between 0 4 and 5 3 c table 2 soils for the study sites obtained from soil survey staff 2010 are shown in table 2 2 2 irrigation scheduling in this study two existing swat auto irrigation algorithms and a newly developed mad auto irrigation algorithm chen et al 2018a were compared to the fao 56 based irrigations in the field studies the auto irrigation function in swat is based on a water stress identifier of either 1 plant water demand or 2 soil water content 2 2 1 swat plant water demand method swat s plant water demand auto irrigation method initiates an irrigation whenever a user defined reduction in plant growth occurs due to water stress neitsch et al 2011 1 strsw auto wstr where strsw is the fraction of potential plant growth achieved on the day where the reduction is caused by water stress and auto wstr is water stress threshold that triggers irrigation 0 1 the recommended range is 0 90 0 95 2 2 2 swat soil water content method swat s soil water content auto irrigation method triggers irrigation when the total soil water content falls below field capacity by more than the user defined soil water depletion threshold irrespective of the presence of a growing crop 2 sol sumfc sol sw auto wstr where sol sumfc is the amount of water held in the soil profile at field capacity mm sol sw is the amount of water stored in soil profile on a given day mm and auto wstr is the water stress threshold that triggers an irrigation event mm 2 2 3 swat management allowed depletion mad method the newly developed mad auto irrigation algorithm triggers irrigation according to a user defined allowable depletion percentage of plant available soil water determined by crop specific maximum rooting depth and soil properties 3 sol sumfc sol sw paw mad where paw is plant available water determined by both soil specific properties and plant specific maximum rooting depth and mad is the management allowed depletion percentage expressed as a decimal value ranging from 0 to 1 mad values approaching 0 denote irrigation management that allows relatively less depletion of plant available water before triggering irrigation resulting in low plant water stress conversely values approaching 1 indicate irrigation management that allows relatively more depletion of plant available water before initiating irrigation resulting in high plant water stress more detailed information and explanation regarding the algorithm can be found in chen et al 2018a 2 2 4 fao 56 based irrigation scheduling the fao 56 based irrigation scheduling was performed according to daily standardized grass reference evapotranspiration etos a crop coefficient and plant available soil water in the root zone fao 56 method allen et al 1998 daily etos was computed using the standardized reference evapotranspiration equation of american society of civil engineers asce 2005 full irrigation was targeted to satisfy 100 of the corn et requirement and prevent crop water stress fao 56 based irrigation scheduling was used for all six study sites the reported or estimated mad values were 60 55 30 50 and 50 at bushland etter greeley garden city and north platte respectively table 1 the mad value was not reported at clovis groundwater was the irrigation source for all study sites although most irrigation scheduling spreadsheets are based primarily on fao 56 guidelines individual approaches may differ to some degree for instance some may use different reference evapotranspiration eto estimation methods and incorporate equations for either fao 56 single or dual crop coefficients based on days after corn planting to calculate a simple daily water balance of the root zone allen et al 1998 jensen et al 2016 irrigation at bushland and etter was scheduled using a single crop coefficient kc method while a dual kc approach was used at greeley and north platte the kc approach was not reported for garden city or clovis daily climate data including precipitation maximum and minimum air temperature wind speed relative humidity and solar radiation were required and used to calculate eto walter et al 2005 to schedule weekly irrigation events irrigation was triggered according to the user defined depletion of plant available water based on the predicted soil water depletion one week in advance the monthly irrigation amount based on the fao 56 methodology actual applied irrigation was used as the benchmark to evaluate the other auto irrigation algorithms 2 3 measured climate and agronomic data daily climate data for the penman monteith method in swat were obtained from research grade weather stations maintained in accordance with the environmental water resources institute ewri of asce 2005 specifications positioned over irrigated mowed grass reference plots at bushland etter and greeley daily meteorological data for garden city north platte and clovis were obtained from the high plains regional climate center hprcc weather network http www hprcc unl edu home html daily meteorological data included precipitation maximum and minimum air temperatures solar radiation wind speed and relative humidity climate data for the six sites for the corn study years were then formatted for use in swat measured hydrologic and agronomic data for the corn grown in 2013 and 2016 at bushland included daily et leaf area index lai and aboveground biomass corn growth data including lai and aboveground biomass were collected periodically during the growing seasons of 2011 2014 at etter and only lai data were collected periodically during the growing seasons of 2008 2011 at greeley no corn growth data collected were available from north platte garden city or clovis 2 4 swat model setup and evaluation the swat model is a continuous time semi distributed and process based model arnold et al 1998 the primary model input information of topography land use soil climate and management practices are required arcswat version 2012 10 2 19 revision 664 for arcgis 10 2 2 was used in this study for simulation purposes each study site was set up as one hydrologic response unit hru using the arcswat interface the three auto irrigation algorithms namely the swat plant water demand auto irrigation algorithm the swat soil water content auto irrigation algorithm and the swat mad auto irrigation algorithm were compared to actual fao 56 based irrigation at all six study sites according to the swat input output file documentation a plant water stress threshold value of 0 95 was selected under the full irrigation conditions when using the swat plant water demand auto irrigation method the reported maximum corn rooting depths were 1 4 1 4 1 05 1 2 1 7 m at bushland etter greeley garden city and north platte respectively an assumed value of 1 4 m for maximum rooting depth was used for corn grown at clovis table 3 the swat models were calibrated against field measured data such as lai aboveground biomass yield or daily et at bushland etter and greeley the swat calibrated parameters for bushland during the simulation period of 2000 2010 were obtained from chen et al 2018a and adapted for corn grown in 2013 and 2016 table s1 the calibrated swat models for etter and greeley for the simulation periods of 2011 2014 and 2008 2011 were used in this study chen et al 2019 as for study sites at garden city north platte and clovis the parameter regionalization calibration approach was used in this approach the calibrated parameters in gauged watersheds or study fields with sufficient measured data were transferred to hydrologically and climatically similar ungauged watersheds or study fields with limited observed data buytaert and beven 2009 cibin et al 2014 vogel 2005 wagener and wheater 2006 this method is a widely used approach to simulate the water balance of ungauged watersheds and study fields therefore the calibrated swat hydrologic parameters from etter were used for garden city fig 1 and table s1 the calibrated parameters from greeley were applied to north platte and the calibrated parameters from bushland were used for clovis the calibrated selected hydrologic and crop growth parameters for the six study sites are listed in table s1 in the supplementary materials the algorithm performance for simulating irrigation amount was evaluated using percent bias pbias gupta et al 1999 square of pearson s product moment correlation coefficient r 2 legates and mccabe 1999 and nash sutcliffe efficiency nse nash and sutcliffe 1970 statistics 3 results and discussion 3 1 swat model calibration and evaluation at six study sites the nse r 2 and pbias values for daily et simulations were 0 75 0 81 and 11 7 respectively during the simulation periods of 2013 and 2016 at bushland fig 2 the model performance demonstrated good agreement between the simulated and observed et according to criteria by moriasi et al 2007 the average measured seasonal corn et was about 850 mm in 2013 and 2016 the average amount of growing season irrigation and precipitation were about 890 mm therefore about 96 of growing season water input was lost through et at bushland in semi arid regions of the southern great plains et is the dominant component of the hydrologic cycle hao et al 2014 reported that more than 90 of the summer growing season rainfall was attributed to growing season et of biomass sorghum in the texas high plains in addition more than 95 of the total precipitation and irrigation was lost through corn et in the texas high plains according to the lysimeter corn data wagle et al 2018 on average rainfall accounts for approximately 25 of crop et requirement during the summer growing season in this region with the remaining coming from irrigation and stored soil water moorhead et al 2015 a graphical comparison showed that swat simulated lai matched the observed data well at bushland fig 3 the nse r 2 and pbias values were 0 86 0 88 and 5 9 respectively a good fit between the simulated and measured aboveground biomass was also observed with the nse r 2 and pbias of 0 84 0 86 and 5 3 respectively fig 4 good agreements of lai and aboveground biomass after calibration nse 0 70 and pbias within 6 table 4 were reported at etter and greeley during the corn growing periods of 2011 2014 and 2008 2011 chen et al 2019 3 2 comparison of monthly irrigation the mad auto irrigation algorithm was first evaluated in chen et al 2018a using a ten year 2001 2010 lysimeter measured dataset for various crops to further evaluate the effectiveness of the newly developed mad algorithm an additional two years 2013 and 2016 of lysimeter measured corn production data were used in this study to evaluate the mad algorithm for simulating irrigation scheduling at the same site but during a different time period results showed the nse r 2 and pbias values achieved for the simulation of monthly irrigation amount during the simulation years of 2013 and 2016 were 0 81 0 83 and 0 4 respectively when compared to the fao 56 based actual irrigation fig 5 the performance of the mad algorithm was improved over the swat plant water demand auto irrigation algorithm nse r 2 and pbias 0 69 0 78 and 20 6 and swat soil water content auto irrigation algorithm nse r 2 and pbias 0 51 0 55 and 30 8 simulated irrigation using the swat soil water content auto irrigation option was clearly larger pbias 31 than that of the actual irrigation amount based on the fao 56 method this was in part due to a deficiency in the swat soil water content method that applied irrigation during the corn non growing season for example irrigation events were triggered during february to april of 2016 in swat fig 5a the swat existing soil water content method applies irrigation water strictly according to the trigger threshold of user defined soil water depletion and does not consider the growing season therefore whenever the trigger threshold of soil water depletion is reached an irrigation event will occur irrespective of the presence of a crop it is worth noting that the mad algorithm obtained the best model performance statistics at bushland where the algorithm was developed as compared to other locations table s2 in the supplementary materials irrigation comparisons at another texas study site near etter indicated the mad algorithm nse 0 77 outperformed the two existing swat auto irrigation algorithms nse 0 69 for both fig 6 the average annual precipitation of the study period 2011 2014 at etter was 250 mm which was unusually low due to a historic and prolonged drought period in the region table s2 consequently a large amount of irrigation water average annual 580 mm was used to meet corn growth requirements under full irrigation table 5 actual field based irrigation events in 2011 2013 at etter were generally conducted with a greater amount per event and less frequency due to the extreme drought conditions encountered and thus more total irrigation quantity may have been applied than required for actual crop et this may explain the reasonable model performance statistics using the swat soil water content method which tends to overestimate field irrigation the robust performance of the mad irrigation algorithm in swat under both typical and drought conditions demonstrate its potential usefulness for irrigation scheduling simulation suleiman et al 2007 used the csm cropgro cotton model to schedule irrigations for cotton in georgia and evaluated the simulated irrigation results using the fao 56 irrigation scheduling method recently thorp et al 2017 compared field irrigation treatment determined by the calibrated dssat csm cropgro cotton simulation model to the well tested fao 56 based irrigation scheduling spreadsheet at the maricopa agricultural center arizona and found irrigation scheduling with csm cropgro cotton resulted in similar irrigation amounts but with slightly different in season irrigation distributions and equal or higher cotton yield at greeley co the simulation performance of the mad algorithm and the swat soil water content auto irrigation function were similar fig 7 the nse r 2 and pbias values were 0 64 0 67 and 4 8 for the swat soil water content auto irrigation algorithm corresponding values were 0 67 0 68 and 4 8 for the mad auto irrigation algorithm however simulations using the swat plant water demand auto irrigation function resulted in an unsatisfactory performance with nse r 2 and pbias values of 0 36 0 41 and 36 respectively fig 7c at greeley the soils are sandy and a considerable amount of water inputs precipitation irrigation contribute to recharging groundwater therefore the irrigation source and well capacity are sufficient for maintaining a low mad level of about 30 in the field study and corn was managed for the maximum yield potential in this case field corn production will experience little to no water stress and this can explain the unsatisfactory model performance using the swat plant water demand auto irrigation function which needs to have crop water stress to trigger irrigation table s2 at garden city ks the mad algorithm nse 0 70 outperformed the soil water content nse 0 60 and plant water demand nse 0 56 auto irrigation functions over an eight year 2005 2012 simulation fig 8 the management information of garden city site was accessed from the literature araya et al 2017 klocke et al 2011 although a mad level of 50 in the field study was reported it may have not been consistent across the long term simulation in this case a mad value of 30 was used in the simulation to achieve a good match between the simulated and actual irrigation based on the fao 56 method it is worth noting that the pbias was 35 5 when using the plant water demand auto irrigation function which was an unsatisfactory model performance value fig 8c at garden city precipitation is relatively high 439 mm yr 1 therefore the full irrigation management can eliminate the corn water stress which resulted in an unsatisfactory model performance using the plant water demand method in swat table s2 at north platte ne the three auto irrigation algorithms all resulted in satisfactory performance nse 0 50 and pbias within 10 for simulating irrigation amount compared to the fao 56 based irrigation fig 9 the swat plant water demand auto irrigation function performed the best nse r 2 and pbias 0 58 0 58 and 3 9 followed by the mad algorithm having performance statistics very close to those of the swat plant water demand auto irrigation function nse r 2 and pbias 0 56 0 56 and 3 9 in the region of north platte ne precipitation 506 mm yr 1 is relatively higher than all other selected study sites thus there are typically fewer irrigation events in addition the air temperature is relatively cooler as compared to others resulting in lower crop et requirements also the initial irrigation was scheduled in july even though corn was planted in may due to the sufficient rainfall early in the growing season and initial soil moisture therefore a smaller total irrigation amount and frequency was applied at this site as compared to the others generally a better agreement between the simulated and actual irrigations can be achieved if fewer irrigation events are triggered or applied this partially explains the satisfactory performances of the swat plant water demand and soil water content auto irrigation functions table s2 similar to the garden city site the management information of north platte site was accessed from the literature payero et al 2006 2008 the reported 50 mad used in the field study might not have been maintained throughout the season in this case as a result a 70 mad value was used to approximate the actual irrigation based on the fao 56 method for this relatively cool and humid climate at clovis nm the simulated irrigation with the swat soil water content auto irrigation algorithm failed to match the fao 56 based actual irrigation with nse r 2 and pbias values of 0 43 0 69 and 32 9 respectively fig 10 a the mad algorithm resulted in the nse r 2 and pbias of 0 62 0 72 and 5 7 respectively fig 10b the swat plant water demand auto irrigation algorithm achieved the optimal nse of 0 72 fig 10c limited management information was available for clovis according to the corn performance reports and no mad level was reported in areas around the clovis site the producers commonly face the dilemma of high irrigation requirements and low well capacity table s2 the long term historical average annual precipitation is about 427 mm while the average annual precipitation of the study period 2009 2012 was 370 mm in this case corn growth likely experienced water stress these environmental growing conditions were similar to the deficit limited irrigation conditions even though they were reported as full irrigation conditions as a reference for comparing different deficit limited irrigation treatments in this situation the swat plant water demand auto irrigation algorithm achieved the optimal nse value and the soil water content method indicated the highest pbias value among six sites table s2 the recommended mad levels ranged from 20 to 80 for full irrigation conditions however under the deficit limited irrigation conditions like clovis a high mad value 85 can be assigned 3 3 evaluation of irrigation scheduling algorithms based on comparisons in this study the swat incorporated with the mad algorithm was found to be an adequate and robust irrigation simulation and scheduling method this algorithm demonstrated satisfactory simulation of irrigation compared to the fao 56 based spreadsheet irrigation method used at all six study sites across five states in the u s southern great plains the mad algorithm also demonstrated good performance with management and agronomic information derived from literature and extension summary reports the swat mad algorithm not only follows fao 56 irrigation protocols but also considers crop growth characteristics multiple soil layer properties water use efficiency and complex agricultural management practices in calculating the water balance however the actual field based fao 56 irrigation practices may not always consider these factors in reality the producers and researchers may also postpone the irrigation according to the weather forecast or their work schedule therefore the swat mad algorithm may provide benefit for scheduling of future smart irrigation in field practice however comparisons between the fao 56 based spreadsheet irrigation and the swat mad algorithm through field experiments are needed regarding the mad levels the range of 20 80 was recommended when using the mad algorithm under full irrigation conditions however model users are allowed to adjust the mad values from 0 to 100 according to their needs under special conditions for example producers may initially target a 60 mad level for corn production nevertheless a large amount of growing season rainfall might result in the need for minimal irrigation in this situation the actual mad value might be closer to 85 or greater on the other hand drought years may require irrigation amounts that are not achievable due to limited well capacity this may also result in the setting of a higher mad level as compared to the nominal ideal or suggested level this emphasizes that the reported mad trigger levels in the field studies are targeted goals which are influenced by several important factors such as unanticipated precipitation soil water holding capacity well capacity allowable pumpage etc the plant water demand auto irrigation algorithm is the default option in the swat model which is widely used around the world akhavan et al 2010 chen et al 2016 guzman et al 2015 jager et al 2015 marek et al 2017a under full irrigation conditions the water plant stress threshold of 0 95 for this approach is recommended arnold et al 2012 in this case there was a less subjective influence for selecting the water stress threshold to trigger irrigation relative to the soil water content and mad auto irrigation methods when no reported irrigation trigger threshold was available in addition the plant water demand algorithm performed well at the north platte and clovis sites where the high mad values were maintained figs 9 and 10 on the other hand the small mad level used at greeley indicated an unsatisfactory simulation of irrigation using the plant water demand method fig 7 in general the plant water demand auto irrigation algorithm underestimated actual irrigation according to the pbias results this is because the plant water demand method triggers an irrigation once a given reduction in plant growth occurs due to water stress in this case the plant will experience water stress before the irrigation application therefore the total irrigation amount during the crop growing season tends to lower than the actual full irrigation no water stress management however the soil water content auto irrigation method exhibited the overestimation of irrigation as compared to the fao 56 based irrigation method this was largely attributed to the triggered irrigation events outside of the crop growing season in reality irrigation is scheduled according to measured estimated soil water depletion or management allowed depletion of plant available water rather than the arbitrary plant water demand method for these reasons the mad auto irrigation algorithm was developed using the framework of the existing soil water content auto irrigation method in swat including parameters such as plant available water crop specific maximum rooting depth and site specific soil properties additionally mad auto irrigation was programmed to occur only during the growing season these improvements have resulted in the swat mad algorithm consistently outperforming the existing auto irrigation algorithms in multiple locations and climates 4 conclusions in this study a swat management allowed depletion mad auto irrigation algorithm was evaluated using field irrigation data based on the fao 56 scheduling method from six study sites across five states including texas colorado kansas nebraska and new mexico the mad algorithm was compared with the two existing swat auto irrigation algorithms based on plant water demand and soil water content methods results indicated that the mad algorithm satisfactory represented actual irrigation at all study sites nse 0 56 and pbias within 17 and outperformed the existing swat auto irrigation algorithms in general the soil water content auto irrigation algorithm tended to overestimate irrigation pbias of 32 9 at clovis site however the plant water demand auto irrigation algorithm tended to underestimate irrigation amount pbias of 36 at greeley and garden city sites these results demonstrate the robust nature of the mad algorithm and reinforce its potential as an effective evaluation tool in assessing irrigation management strategies which may be useful in irrigation scheduling recommendations and subsequent water planning efforts the mad algorithm was developed at usda ars cprl bushland according to ten year lysimeter measured data and evaluated across multiple locations with different climatic conditions and soil types in this study these enhanced the confidence of the users to explore this algorithm for potential alternative irrigation management strategies in addition the mad code was developed using the fortran language this will provide opportunities for incorporating the representative algorithm into other simulation models additionally further testing of the mad algorithm in several complex irrigation watersheds will further evaluate the applicability of the algorithm for regional water management and planning acknowledgments this research was supported in part by the ogallala aquifer program a consortium between usda agricultural research service kansas state university texas a m agrilife research texas a m agrilife extension service texas tech university and west texas a m university the authors would like to recognize the considerable efforts of the usda s national agricultural library and water management and systems research unit of the center for agricultural resources research in creating and making the maize data from greeley colorado available on the ag data commons website appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 001 
26193,modeling alternative irrigation strategies can be a cost effective and time saving approach to field based experiments however the efficacy of irrigation scheduling algorithms should be verified using field data from multiple locations in this study an auto irrigation algorithm recently developed for soil and water assessment tool swat was further evaluated using irrigation data for corn zea mays l grown at six research sites across the southern great plains simulated monthly irrigation based on the management allowed depletion mad of plant available soil water was compared to measured data for irrigation applied in accordance with crop water requirement guidelines outlined by the food and agriculture organization irrigation and drainage paper 56 overall results indicated the mad algorithm simulated monthly field based irrigation amounts well nash sutcliffe efficiency nse 0 56 comparisons revealed the mad algorithm outperformed the plant water demand and soil water content approaches in swat which tended to underestimate and overestimate irrigations respectively keywords fao 56 irrigation algorithm management allowed depletion corn semi arid region groundwater software availability algorithm name mad irrigation algorithm description the mad algorithm simulates irrigation scheduling by incorporating an allowable depletion percentage of plant available soil water determined by crop specific maximum rooting depth and site specific soil properties into an irrigation trigger the algorithm also suspends irrigation events following crop harvest developed by y chen yongchen neo tamu edu and g w marek gary marek ars usda gov ecosystem science and management texas a m university college station texas and usda ars conservation and production research laboratory cprl bushland texas respectively year available 2018 availability contact developers cost free language fortran 1 introduction drought and water scarcity are the predominant elements constraining agricultural production in arid and semi arid regions of the world irrigation is a principal agricultural management practice for sustaining food security food and agriculture organization fao 2002 in many regions of the united states u s irrigation water supplies are declining due to frequent periods of drought claeys and inzé 2013 ongoing depletion of groundwater levels colaizzi et al 2009 mcguire 2004 regulatory restrictions on irrigation pumping high plains underground water conservation district hpuwcd 2016 north plains groundwater conservation district npgcd 2014 panhandle groundwater conservation district pgwcd 2016 and management policy shifting irrigation water use from agriculture to other uses payero et al 2006 the u s southern great plains consisting of texas colorado oklahoma new mexico kansas nebraska wyoming and south dakota is one of the most productive agricultural regions in the u s however this centrally located region also faces frequent droughts and decreased groundwater availability the study of alternative irrigation management strategies is therefore of utmost importance as it explores ways to improve current agricultural irrigation management practices in the southern great plains to promote more sustainable production and water conservation representative modeling can serve as a time and cost efficient tool for evaluating multiple irrigation scheduling algorithms chen et al 2018a li et al 2014 however the existing irrigation algorithms in many simulation models have demonstrated potential deficiencies chen et al 2017 marek et al 2017a panagopoulos et al 2014a and 2014b for example chen et al 2017 reported that the soil and water assessment tool swat model using the auto irrigation function soil water content option simulated actual crop evapotranspiration et reasonably well as compared to measured lysimeter values in the texas high plains nevertheless simulated irrigation amount and frequency varied greatly from actual irrigation events scheduled according to crop water requirement guidelines outlined by the fao irrigation and drainage paper 56 fao 56 marek et al 2017a found that simulated growing season irrigation of conventional crops in the texas high plains could approximate that of actual irrigation by adjusting the trigger threshold of the plant water demand approach in swat another existing auto irrigation function however the authors also suggested that use of the plant water demand option with no consideration of well capacity could impede reasonable simulation of irrigation particularly during drought years therefore a more representative auto irrigation algorithm is required to more accurately simulate field irrigation management the widely implemented management allowed depletion mad irrigation scheduling approach callison 2012 evett et al 2011 hao et al 2015 klocke et al 2011 lamm et al 1996 marek et al 2005 2009 and 2011 merriam 1966 payero et al 2008 suarez rey et al 2000 thorp et al 2017 trout and bausch 2017 vories et al 2017 allows a user producer defined percentage of plant available water paw depletion before irrigation is applied the mad irrigation concept is a common framework used for irrigation scheduling as outlined by merriam 1966 however the current auto irrigation functions in simulation models are unable to simulate mad irrigation scheduling recently chen et al 2018a developed a mad auto irrigation algorithm and incorporated it into the swat model ten years of agronomic data from lysimeter fields at the u s department of agriculture agricultural research service usda ars conservation and production research laboratory cprl at bushland texas were used to develop and test the mad algorithm the mad auto irrigation algorithm performed well nash sutcliffe efficiency nse 0 57 and percent bias pbias 21 7 using irrigated field data from bushland nevertheless a comprehensive evaluation of the mad auto irrigation algorithm and existing auto irrigation algorithms using field fao 56 irrigation data from multiple sites having different soil and climatic conditions is needed to warrant the acceptance use and distribution of the new auto irrigation algorithm corn zea mays l is one of the major field crops grown under irrigated conditions in the southern great plains as such corn irrigation management data are readily available from literature and field measurements in general precipitation during the growing season supplies only a portion of the corn s water demand or actual crop et and a large amount of irrigation is required to supplement the remaining portion irrigated winter wheat triticum aestivum l and cotton gossypium hirsutum l have reduced irrigation water use requirements by 27 and 37 respectively as compared to irrigated corn in the texas high plains texas water development board 2011 using the swat mad model chen et al 2018b found that approximately 19 21 and 32 reductions in annual irrigation amounts were associated with irrigated soybean glycine max l sunflower helianthus annuus l and sorghum sorghum bicolor l respectively as compared to irrigated corn in the northern high plains of texas several field corn studies have been conducted using the fao 56 irrigation spreadsheet methods in the southern great plains araya et al 2017 hao et al 2015 klocke et al 2011 marek et al 2017b payero et al 2006 2008 trout 2016 trout and bausch 2017 vories et al 2017 however limited studies report comparisons of different irrigation scheduling methods algorithms in simulation models with well tested fao 56 irrigation spreadsheet methods k state research extension mobile irrigation lab 2018 marek et al 2005 2009 pereira et al 2015 snyder 2014 thorp et al 2017 across multiple sites among simulation models with auto irrigation functions swat is a widely used license free and open source model the source code of swat 2012 revision 664 was used in this study in addition swat has two popular auto irrigation options based on plant water demand and soil water content in chen et al 2018a an evaluation of a sensitive and intuitive auto irrigation algorithm mad auto irrigation algorithm was shown to be representative of long term actual field based irrigation management practices at the usda ars cprl the objective of this study was to further evaluate the efficacy of the newly developed mad algorithm along with the two existing swat auto irrigation options using actual fao 56 based monthly field irrigation data from six study sites located in five states across the u s southern great plains 2 materials and methods 2 1 study sites grain corn was grown under full irrigation using a linear move sprinkler in a large lysimeter field located at the usda ars cprl bushland tx in 2013 and 2016 fig 1 and table 1 corn was also grown at the texas a m agrilife research north plains research field nprf etter tx from 2011 to 2014 under center pivot sprinkler irrigation and at usda ars limited irrigation research farm lirf greeley co from 2008 to 2011 under surface drip irrigation trout 2016 trout and bausch 2017 table 1 corn experiments were carried out at the kansas state university southwest research extension center swrec finnup farm near garden city ks from 2005 to 2012 under a linear move sprinkler system araya et al 2017 klocke et al 2011 corn experiments were conducted during the 2003 to 2006 growing seasons at the university of nebraska lincoln west central research and extension center wcrec north platte ne under subsurface drip irrigation payero et al 2006 2008 table 1 corn was grown at new mexico state university agricultural science center clovis nm from 2009 to 2012 under center pivot sprinkler irrigation marsalis et al 2009 2010 2011 2012 table 1 the regional climate of all six study sites is classified as semi arid peel et al 2007 average annual precipitation during the study years ranged between 250 and 506 mm table 2 average annual maximum temperatures varied between 17 3 c and 22 8 c and minimum temperatures were between 0 4 and 5 3 c table 2 soils for the study sites obtained from soil survey staff 2010 are shown in table 2 2 2 irrigation scheduling in this study two existing swat auto irrigation algorithms and a newly developed mad auto irrigation algorithm chen et al 2018a were compared to the fao 56 based irrigations in the field studies the auto irrigation function in swat is based on a water stress identifier of either 1 plant water demand or 2 soil water content 2 2 1 swat plant water demand method swat s plant water demand auto irrigation method initiates an irrigation whenever a user defined reduction in plant growth occurs due to water stress neitsch et al 2011 1 strsw auto wstr where strsw is the fraction of potential plant growth achieved on the day where the reduction is caused by water stress and auto wstr is water stress threshold that triggers irrigation 0 1 the recommended range is 0 90 0 95 2 2 2 swat soil water content method swat s soil water content auto irrigation method triggers irrigation when the total soil water content falls below field capacity by more than the user defined soil water depletion threshold irrespective of the presence of a growing crop 2 sol sumfc sol sw auto wstr where sol sumfc is the amount of water held in the soil profile at field capacity mm sol sw is the amount of water stored in soil profile on a given day mm and auto wstr is the water stress threshold that triggers an irrigation event mm 2 2 3 swat management allowed depletion mad method the newly developed mad auto irrigation algorithm triggers irrigation according to a user defined allowable depletion percentage of plant available soil water determined by crop specific maximum rooting depth and soil properties 3 sol sumfc sol sw paw mad where paw is plant available water determined by both soil specific properties and plant specific maximum rooting depth and mad is the management allowed depletion percentage expressed as a decimal value ranging from 0 to 1 mad values approaching 0 denote irrigation management that allows relatively less depletion of plant available water before triggering irrigation resulting in low plant water stress conversely values approaching 1 indicate irrigation management that allows relatively more depletion of plant available water before initiating irrigation resulting in high plant water stress more detailed information and explanation regarding the algorithm can be found in chen et al 2018a 2 2 4 fao 56 based irrigation scheduling the fao 56 based irrigation scheduling was performed according to daily standardized grass reference evapotranspiration etos a crop coefficient and plant available soil water in the root zone fao 56 method allen et al 1998 daily etos was computed using the standardized reference evapotranspiration equation of american society of civil engineers asce 2005 full irrigation was targeted to satisfy 100 of the corn et requirement and prevent crop water stress fao 56 based irrigation scheduling was used for all six study sites the reported or estimated mad values were 60 55 30 50 and 50 at bushland etter greeley garden city and north platte respectively table 1 the mad value was not reported at clovis groundwater was the irrigation source for all study sites although most irrigation scheduling spreadsheets are based primarily on fao 56 guidelines individual approaches may differ to some degree for instance some may use different reference evapotranspiration eto estimation methods and incorporate equations for either fao 56 single or dual crop coefficients based on days after corn planting to calculate a simple daily water balance of the root zone allen et al 1998 jensen et al 2016 irrigation at bushland and etter was scheduled using a single crop coefficient kc method while a dual kc approach was used at greeley and north platte the kc approach was not reported for garden city or clovis daily climate data including precipitation maximum and minimum air temperature wind speed relative humidity and solar radiation were required and used to calculate eto walter et al 2005 to schedule weekly irrigation events irrigation was triggered according to the user defined depletion of plant available water based on the predicted soil water depletion one week in advance the monthly irrigation amount based on the fao 56 methodology actual applied irrigation was used as the benchmark to evaluate the other auto irrigation algorithms 2 3 measured climate and agronomic data daily climate data for the penman monteith method in swat were obtained from research grade weather stations maintained in accordance with the environmental water resources institute ewri of asce 2005 specifications positioned over irrigated mowed grass reference plots at bushland etter and greeley daily meteorological data for garden city north platte and clovis were obtained from the high plains regional climate center hprcc weather network http www hprcc unl edu home html daily meteorological data included precipitation maximum and minimum air temperatures solar radiation wind speed and relative humidity climate data for the six sites for the corn study years were then formatted for use in swat measured hydrologic and agronomic data for the corn grown in 2013 and 2016 at bushland included daily et leaf area index lai and aboveground biomass corn growth data including lai and aboveground biomass were collected periodically during the growing seasons of 2011 2014 at etter and only lai data were collected periodically during the growing seasons of 2008 2011 at greeley no corn growth data collected were available from north platte garden city or clovis 2 4 swat model setup and evaluation the swat model is a continuous time semi distributed and process based model arnold et al 1998 the primary model input information of topography land use soil climate and management practices are required arcswat version 2012 10 2 19 revision 664 for arcgis 10 2 2 was used in this study for simulation purposes each study site was set up as one hydrologic response unit hru using the arcswat interface the three auto irrigation algorithms namely the swat plant water demand auto irrigation algorithm the swat soil water content auto irrigation algorithm and the swat mad auto irrigation algorithm were compared to actual fao 56 based irrigation at all six study sites according to the swat input output file documentation a plant water stress threshold value of 0 95 was selected under the full irrigation conditions when using the swat plant water demand auto irrigation method the reported maximum corn rooting depths were 1 4 1 4 1 05 1 2 1 7 m at bushland etter greeley garden city and north platte respectively an assumed value of 1 4 m for maximum rooting depth was used for corn grown at clovis table 3 the swat models were calibrated against field measured data such as lai aboveground biomass yield or daily et at bushland etter and greeley the swat calibrated parameters for bushland during the simulation period of 2000 2010 were obtained from chen et al 2018a and adapted for corn grown in 2013 and 2016 table s1 the calibrated swat models for etter and greeley for the simulation periods of 2011 2014 and 2008 2011 were used in this study chen et al 2019 as for study sites at garden city north platte and clovis the parameter regionalization calibration approach was used in this approach the calibrated parameters in gauged watersheds or study fields with sufficient measured data were transferred to hydrologically and climatically similar ungauged watersheds or study fields with limited observed data buytaert and beven 2009 cibin et al 2014 vogel 2005 wagener and wheater 2006 this method is a widely used approach to simulate the water balance of ungauged watersheds and study fields therefore the calibrated swat hydrologic parameters from etter were used for garden city fig 1 and table s1 the calibrated parameters from greeley were applied to north platte and the calibrated parameters from bushland were used for clovis the calibrated selected hydrologic and crop growth parameters for the six study sites are listed in table s1 in the supplementary materials the algorithm performance for simulating irrigation amount was evaluated using percent bias pbias gupta et al 1999 square of pearson s product moment correlation coefficient r 2 legates and mccabe 1999 and nash sutcliffe efficiency nse nash and sutcliffe 1970 statistics 3 results and discussion 3 1 swat model calibration and evaluation at six study sites the nse r 2 and pbias values for daily et simulations were 0 75 0 81 and 11 7 respectively during the simulation periods of 2013 and 2016 at bushland fig 2 the model performance demonstrated good agreement between the simulated and observed et according to criteria by moriasi et al 2007 the average measured seasonal corn et was about 850 mm in 2013 and 2016 the average amount of growing season irrigation and precipitation were about 890 mm therefore about 96 of growing season water input was lost through et at bushland in semi arid regions of the southern great plains et is the dominant component of the hydrologic cycle hao et al 2014 reported that more than 90 of the summer growing season rainfall was attributed to growing season et of biomass sorghum in the texas high plains in addition more than 95 of the total precipitation and irrigation was lost through corn et in the texas high plains according to the lysimeter corn data wagle et al 2018 on average rainfall accounts for approximately 25 of crop et requirement during the summer growing season in this region with the remaining coming from irrigation and stored soil water moorhead et al 2015 a graphical comparison showed that swat simulated lai matched the observed data well at bushland fig 3 the nse r 2 and pbias values were 0 86 0 88 and 5 9 respectively a good fit between the simulated and measured aboveground biomass was also observed with the nse r 2 and pbias of 0 84 0 86 and 5 3 respectively fig 4 good agreements of lai and aboveground biomass after calibration nse 0 70 and pbias within 6 table 4 were reported at etter and greeley during the corn growing periods of 2011 2014 and 2008 2011 chen et al 2019 3 2 comparison of monthly irrigation the mad auto irrigation algorithm was first evaluated in chen et al 2018a using a ten year 2001 2010 lysimeter measured dataset for various crops to further evaluate the effectiveness of the newly developed mad algorithm an additional two years 2013 and 2016 of lysimeter measured corn production data were used in this study to evaluate the mad algorithm for simulating irrigation scheduling at the same site but during a different time period results showed the nse r 2 and pbias values achieved for the simulation of monthly irrigation amount during the simulation years of 2013 and 2016 were 0 81 0 83 and 0 4 respectively when compared to the fao 56 based actual irrigation fig 5 the performance of the mad algorithm was improved over the swat plant water demand auto irrigation algorithm nse r 2 and pbias 0 69 0 78 and 20 6 and swat soil water content auto irrigation algorithm nse r 2 and pbias 0 51 0 55 and 30 8 simulated irrigation using the swat soil water content auto irrigation option was clearly larger pbias 31 than that of the actual irrigation amount based on the fao 56 method this was in part due to a deficiency in the swat soil water content method that applied irrigation during the corn non growing season for example irrigation events were triggered during february to april of 2016 in swat fig 5a the swat existing soil water content method applies irrigation water strictly according to the trigger threshold of user defined soil water depletion and does not consider the growing season therefore whenever the trigger threshold of soil water depletion is reached an irrigation event will occur irrespective of the presence of a crop it is worth noting that the mad algorithm obtained the best model performance statistics at bushland where the algorithm was developed as compared to other locations table s2 in the supplementary materials irrigation comparisons at another texas study site near etter indicated the mad algorithm nse 0 77 outperformed the two existing swat auto irrigation algorithms nse 0 69 for both fig 6 the average annual precipitation of the study period 2011 2014 at etter was 250 mm which was unusually low due to a historic and prolonged drought period in the region table s2 consequently a large amount of irrigation water average annual 580 mm was used to meet corn growth requirements under full irrigation table 5 actual field based irrigation events in 2011 2013 at etter were generally conducted with a greater amount per event and less frequency due to the extreme drought conditions encountered and thus more total irrigation quantity may have been applied than required for actual crop et this may explain the reasonable model performance statistics using the swat soil water content method which tends to overestimate field irrigation the robust performance of the mad irrigation algorithm in swat under both typical and drought conditions demonstrate its potential usefulness for irrigation scheduling simulation suleiman et al 2007 used the csm cropgro cotton model to schedule irrigations for cotton in georgia and evaluated the simulated irrigation results using the fao 56 irrigation scheduling method recently thorp et al 2017 compared field irrigation treatment determined by the calibrated dssat csm cropgro cotton simulation model to the well tested fao 56 based irrigation scheduling spreadsheet at the maricopa agricultural center arizona and found irrigation scheduling with csm cropgro cotton resulted in similar irrigation amounts but with slightly different in season irrigation distributions and equal or higher cotton yield at greeley co the simulation performance of the mad algorithm and the swat soil water content auto irrigation function were similar fig 7 the nse r 2 and pbias values were 0 64 0 67 and 4 8 for the swat soil water content auto irrigation algorithm corresponding values were 0 67 0 68 and 4 8 for the mad auto irrigation algorithm however simulations using the swat plant water demand auto irrigation function resulted in an unsatisfactory performance with nse r 2 and pbias values of 0 36 0 41 and 36 respectively fig 7c at greeley the soils are sandy and a considerable amount of water inputs precipitation irrigation contribute to recharging groundwater therefore the irrigation source and well capacity are sufficient for maintaining a low mad level of about 30 in the field study and corn was managed for the maximum yield potential in this case field corn production will experience little to no water stress and this can explain the unsatisfactory model performance using the swat plant water demand auto irrigation function which needs to have crop water stress to trigger irrigation table s2 at garden city ks the mad algorithm nse 0 70 outperformed the soil water content nse 0 60 and plant water demand nse 0 56 auto irrigation functions over an eight year 2005 2012 simulation fig 8 the management information of garden city site was accessed from the literature araya et al 2017 klocke et al 2011 although a mad level of 50 in the field study was reported it may have not been consistent across the long term simulation in this case a mad value of 30 was used in the simulation to achieve a good match between the simulated and actual irrigation based on the fao 56 method it is worth noting that the pbias was 35 5 when using the plant water demand auto irrigation function which was an unsatisfactory model performance value fig 8c at garden city precipitation is relatively high 439 mm yr 1 therefore the full irrigation management can eliminate the corn water stress which resulted in an unsatisfactory model performance using the plant water demand method in swat table s2 at north platte ne the three auto irrigation algorithms all resulted in satisfactory performance nse 0 50 and pbias within 10 for simulating irrigation amount compared to the fao 56 based irrigation fig 9 the swat plant water demand auto irrigation function performed the best nse r 2 and pbias 0 58 0 58 and 3 9 followed by the mad algorithm having performance statistics very close to those of the swat plant water demand auto irrigation function nse r 2 and pbias 0 56 0 56 and 3 9 in the region of north platte ne precipitation 506 mm yr 1 is relatively higher than all other selected study sites thus there are typically fewer irrigation events in addition the air temperature is relatively cooler as compared to others resulting in lower crop et requirements also the initial irrigation was scheduled in july even though corn was planted in may due to the sufficient rainfall early in the growing season and initial soil moisture therefore a smaller total irrigation amount and frequency was applied at this site as compared to the others generally a better agreement between the simulated and actual irrigations can be achieved if fewer irrigation events are triggered or applied this partially explains the satisfactory performances of the swat plant water demand and soil water content auto irrigation functions table s2 similar to the garden city site the management information of north platte site was accessed from the literature payero et al 2006 2008 the reported 50 mad used in the field study might not have been maintained throughout the season in this case as a result a 70 mad value was used to approximate the actual irrigation based on the fao 56 method for this relatively cool and humid climate at clovis nm the simulated irrigation with the swat soil water content auto irrigation algorithm failed to match the fao 56 based actual irrigation with nse r 2 and pbias values of 0 43 0 69 and 32 9 respectively fig 10 a the mad algorithm resulted in the nse r 2 and pbias of 0 62 0 72 and 5 7 respectively fig 10b the swat plant water demand auto irrigation algorithm achieved the optimal nse of 0 72 fig 10c limited management information was available for clovis according to the corn performance reports and no mad level was reported in areas around the clovis site the producers commonly face the dilemma of high irrigation requirements and low well capacity table s2 the long term historical average annual precipitation is about 427 mm while the average annual precipitation of the study period 2009 2012 was 370 mm in this case corn growth likely experienced water stress these environmental growing conditions were similar to the deficit limited irrigation conditions even though they were reported as full irrigation conditions as a reference for comparing different deficit limited irrigation treatments in this situation the swat plant water demand auto irrigation algorithm achieved the optimal nse value and the soil water content method indicated the highest pbias value among six sites table s2 the recommended mad levels ranged from 20 to 80 for full irrigation conditions however under the deficit limited irrigation conditions like clovis a high mad value 85 can be assigned 3 3 evaluation of irrigation scheduling algorithms based on comparisons in this study the swat incorporated with the mad algorithm was found to be an adequate and robust irrigation simulation and scheduling method this algorithm demonstrated satisfactory simulation of irrigation compared to the fao 56 based spreadsheet irrigation method used at all six study sites across five states in the u s southern great plains the mad algorithm also demonstrated good performance with management and agronomic information derived from literature and extension summary reports the swat mad algorithm not only follows fao 56 irrigation protocols but also considers crop growth characteristics multiple soil layer properties water use efficiency and complex agricultural management practices in calculating the water balance however the actual field based fao 56 irrigation practices may not always consider these factors in reality the producers and researchers may also postpone the irrigation according to the weather forecast or their work schedule therefore the swat mad algorithm may provide benefit for scheduling of future smart irrigation in field practice however comparisons between the fao 56 based spreadsheet irrigation and the swat mad algorithm through field experiments are needed regarding the mad levels the range of 20 80 was recommended when using the mad algorithm under full irrigation conditions however model users are allowed to adjust the mad values from 0 to 100 according to their needs under special conditions for example producers may initially target a 60 mad level for corn production nevertheless a large amount of growing season rainfall might result in the need for minimal irrigation in this situation the actual mad value might be closer to 85 or greater on the other hand drought years may require irrigation amounts that are not achievable due to limited well capacity this may also result in the setting of a higher mad level as compared to the nominal ideal or suggested level this emphasizes that the reported mad trigger levels in the field studies are targeted goals which are influenced by several important factors such as unanticipated precipitation soil water holding capacity well capacity allowable pumpage etc the plant water demand auto irrigation algorithm is the default option in the swat model which is widely used around the world akhavan et al 2010 chen et al 2016 guzman et al 2015 jager et al 2015 marek et al 2017a under full irrigation conditions the water plant stress threshold of 0 95 for this approach is recommended arnold et al 2012 in this case there was a less subjective influence for selecting the water stress threshold to trigger irrigation relative to the soil water content and mad auto irrigation methods when no reported irrigation trigger threshold was available in addition the plant water demand algorithm performed well at the north platte and clovis sites where the high mad values were maintained figs 9 and 10 on the other hand the small mad level used at greeley indicated an unsatisfactory simulation of irrigation using the plant water demand method fig 7 in general the plant water demand auto irrigation algorithm underestimated actual irrigation according to the pbias results this is because the plant water demand method triggers an irrigation once a given reduction in plant growth occurs due to water stress in this case the plant will experience water stress before the irrigation application therefore the total irrigation amount during the crop growing season tends to lower than the actual full irrigation no water stress management however the soil water content auto irrigation method exhibited the overestimation of irrigation as compared to the fao 56 based irrigation method this was largely attributed to the triggered irrigation events outside of the crop growing season in reality irrigation is scheduled according to measured estimated soil water depletion or management allowed depletion of plant available water rather than the arbitrary plant water demand method for these reasons the mad auto irrigation algorithm was developed using the framework of the existing soil water content auto irrigation method in swat including parameters such as plant available water crop specific maximum rooting depth and site specific soil properties additionally mad auto irrigation was programmed to occur only during the growing season these improvements have resulted in the swat mad algorithm consistently outperforming the existing auto irrigation algorithms in multiple locations and climates 4 conclusions in this study a swat management allowed depletion mad auto irrigation algorithm was evaluated using field irrigation data based on the fao 56 scheduling method from six study sites across five states including texas colorado kansas nebraska and new mexico the mad algorithm was compared with the two existing swat auto irrigation algorithms based on plant water demand and soil water content methods results indicated that the mad algorithm satisfactory represented actual irrigation at all study sites nse 0 56 and pbias within 17 and outperformed the existing swat auto irrigation algorithms in general the soil water content auto irrigation algorithm tended to overestimate irrigation pbias of 32 9 at clovis site however the plant water demand auto irrigation algorithm tended to underestimate irrigation amount pbias of 36 at greeley and garden city sites these results demonstrate the robust nature of the mad algorithm and reinforce its potential as an effective evaluation tool in assessing irrigation management strategies which may be useful in irrigation scheduling recommendations and subsequent water planning efforts the mad algorithm was developed at usda ars cprl bushland according to ten year lysimeter measured data and evaluated across multiple locations with different climatic conditions and soil types in this study these enhanced the confidence of the users to explore this algorithm for potential alternative irrigation management strategies in addition the mad code was developed using the fortran language this will provide opportunities for incorporating the representative algorithm into other simulation models additionally further testing of the mad algorithm in several complex irrigation watersheds will further evaluate the applicability of the algorithm for regional water management and planning acknowledgments this research was supported in part by the ogallala aquifer program a consortium between usda agricultural research service kansas state university texas a m agrilife research texas a m agrilife extension service texas tech university and west texas a m university the authors would like to recognize the considerable efforts of the usda s national agricultural library and water management and systems research unit of the center for agricultural resources research in creating and making the maize data from greeley colorado available on the ag data commons website appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 001 
26194,parameter estimation using historical observed data is an important part of the environmental modeling the uncertainty in the parameter estimation limits the applications of environmental models in this paper the influence of limited and uncertain calibrated data on the performance of the parameter estimation are systematically investigated for this purpose synthetic observations with a given uncertainty and frequency are used to estimate the model parameters of a conceptual water quality wq model of the river zenne belgium bayesian inference using markov chain monte carlo sampling is adopted to simultaneously perform the automatic calibration and the uncertainty analysis the results highlight the critical roles of measurement frequency and uncertainty in the model calibration we found that the effect of the measurement uncertainty on the parameter estimation is significant when the calibrated data points are limited e g monthly data the research findings can be used to support measurement prioritization and resource allocation keywords measurement uncertainty measurement frequency parameter estimation parameter uncertainty simulation uncertainty dream zs 1 introduction in order to use environmental models for different tasks such as predictions scenario analysis and setting up regulations they should represent the reality adequately moreover they should be scientifically sound robust and defensible u s epa 2002 in general model results are affected by the model structure i e the model assumptions the model inputs the boundary conditions and the model parameters van griensven and meixner 2006 rode et al 2010 the underlying assumptions of the model are often fixed and therefore the model structure is not changed during the modeling processes moreover the input data and the boundary conditions obtained through measuring campaigns or provided by responsible authorities are not altered by the modeler nossent 2012 on the other hand most of the model parameters representing some properties of the system cannot be measured directly vrugt et al 2003 as a consequence the model parameters should be set to appropriate values in order to increase the agreement between the model results and the real system the parameters adjustment is based on the reduction of the difference between the model results and historical measurements of the system response laloy et al 2010 vrugt et al 2013 leta et al 2015 this procedure is referred to as parameter estimation parameter optimization model calibration or inverse modeling raat et al 2004 parameter estimation using historical observed data is an important part of the environmental modeling practice which has been the focus of many researches and studies e g duan et al 2006 however because the models are only an approximation of the real system and the observed data used for the calibration contain error i e measurement uncertainty parameter estimation is error prone vrugt et al 2002 as a result it is difficult to well identify the parameters and the parameter uncertainty is caused in addition in some fields such as water quality modeling the data collection is resource intensive mannina and viviani 2010 consequently the available data has a limited frequency e g biweekly or even monthly intervals zheng and keller 2007a in these cases a serious complication for the model calibration is the lack of reliable calibration data which results in parameter uncertainty raat et al 2004 franceschini and tsai 2010a b the ambiguity in the parameter estimation has considerable impact on the model simulation uncertainty and therefore limits the applications of environmental models such as water quality models wagener et al 2003 despite the critical role of amount and reliability of calibration data on the performance of the parameter estimation to the best of the authors knowledge the literature contains very few studies on quantifying the impact of these two aspects of the measurements i e amount and reliability on the parameter uncertainty of water quality models for example for a catchment nitrogen modeling raat et al 2004 explored the relationship between the quality of the calibration data i e measurement uncertainty and the uncertainty associated with the final parameter estimates i e parameter uncertainty using virtual data wang et al 2017 used synthetic data to explore how the number of tracer i e isotope data samples i e measurement amount affect model calibration however the effect of measurement errors of the tracer data on the parameter estimation was not studies therefore it is needed to investigate the effect of both amount and reliability of the calibration data on the performance of the calibration neglecting the other source of uncertainties e g model structural uncertainty input data uncertainty a modeler should first investigate that it is feasible to reach a pre defined model performance with a given amount of uncertain measured data as the impact of these critical characteristics of calibration data amount and reliability on the model calibration has not fully addressed in literature in this study we focus on assessing the influence of limited uncertain calibrated data on the performance of the parameter estimation and on the parameter uncertainty intervals of water quality models for this purpose the following questions are formulated 1 what is the effect of increasing the measurement frequency on the water quality parameter estimation 2 what is the effect of reducing the measurement error of the observed data on the water quality parameter estimation to address the research questions synthetic observations with a given uncertainty and frequency are used to estimate the model parameters of a conceptual water quality wq model of the river zenne in belgium for simulation dissolved oxygen o2 and biological oxygen demand bod as an optimization tool bayesian inference using markov chain monte carlo mcmc sampling bates and campbell 2001 kuczera and parent 1998 is adopted to simultaneously perform the automatic calibration and the uncertainty analysis the synthetic data series are generated by running a wq model with a given set of model parameter values as true values the model outputs are then perturbed with a pre specified random error as measurement error and sampled with a given frequency to mimic discrete measurements the sampled data are then considered as if they were observed data and used to calibrate the model parameters using the mcmc algorithm finally it is verified if the model parameters can be identified using the limited and uncertain data to evaluate the relationship between the amount and reliability of the calibration data and the parameter uncertainty nine different synthetic data sets are generated by increasing the measurement error and decreasing the measurement frequency then the generated synthetic data are used as calibration data in subsequent optimization runs 2 methods and materials 2 1 conceptual integrated tool for water quality assessment citowa in order to enhance the applicability of conceptual river water quality simulators woldegiorgis woldegiorgis 2017 woldegiorgis et al 2017 developed a conceptual integrated tool for water quality assessment citowa as an alternative to detailed wq simulators in citowa the river system is represented by reaches which are conceptual elements that divide the channel longitudinally into different parts citowa obtains estimates of discharges and velocities of the reaches from external simulation tools the reaches are modeled as continuously stirred tanks reactors cstr whereby complete mixing is assumed within each reach the citowa integrates several components the first component is the simulator of dissolved oxygen biochemical oxygen demand nitrogen compounds phosphorus compounds and algae the simulator applies a formulation based on ordinary differential equations of the physico chemical processes based on a qual 2e brown and barnwell 1985 type formulation it integrates the mass balance equation with the transformation decay processes and solves the resulting differential equation using a quasi analytical solution scheme woldegiorgis et al 2017 the variables which can be simulated in the model are nitrogen compounds phosphorus compounds algae biological oxygen demand and dissolved oxygen moreover citowa can be used to simulate the sediment sediment bound pollutants and any arbitrary pollutant along a river the qual 2e differential equations used for simulating o2 and bod in citowa are 1 d o 2 d t r k 2 o 2 o 2 α 3 μ α 4 ρ a r k 1 b o d r k 4 d α 5 b c 1 n h 4 α 6 b c 2 n o 2 where o2 is the dissolved oxygen concentration mg l t is the time day rk2 is the oxygen reaeration rate 1 day o2 is the dissolved oxygen concentration at saturation mg l α3 is the rate of oxygen production per unit of algal photosynthesis mg o2 mg algae μ is algal growth rate 1 day α4 is the rate of oxygen uptake per unit of algae respired mg o2 mg algae ρ is algal respiration rate 1 day a is algal concentration mg l rk1 is carbonaceous bod deoxygenation rate 1 day bod is the carbonaceous biochemical oxygen demand mg l rk4 is sediment oxygen demand rate g m2 day 1 d is the water depth m α5 is the rate of oxygen uptake per unit of ammonia nitrogen oxydation mg o mg n bc1 is rate constant for biological oxidation of nh4 to no2 1 day nh4 is ammonia nitrogen concentration mg l α6 is the rate of oxygen uptake per unit of nitrite nitrogen oxydation mg o mg n bc2 is rate constant for biological oxidation of no2 to no3 1 day no2 and no3 are nitrite and nitrate nitrogen concentrations mg l respectively 2 d b o d d t r k 1 b o d r k 3 b o d where rk3 is the rate of loss of bod due to settling 1 day 2 2 the case study a conceptual water quality model of the river zenne built up using citowa woldegiorgis 2017 is selected as a case study the river zenne is a lowland tributary of the river scheldt in belgium the study area extends from the border of the walloon region to eppegem the upstream end of the tidal part of the river the river is connected to the nearby navigation canal brussels charleroi at multiple overflow points to protect the urban region of brussels from flooding woldegiorgis 2017 the model takes into account the interactions between the river and the canal and comprises a total of 52 reaches the reader may refer to woldegiorgis 2017 for the complex stream network configuration the major pollution sources of the system are the effluent from waste water treatment plants wwtps the diffuse pollution from agricultural areas and the emissions from combined overflows csos all the pollution sources including the inlet from wallonia at the upstream border of the model the inlet from the tributary rivers and the effluent of the wwtps and the csos are treated as point source pollutant boundaries woldegiorgis 2017 the zenne citowa model used the reach discharges the flow depths and the velocity data from a reservoir type model of meert et al 2016 the sources of the input data and the boundary data are explained in woldegiorgis 2017 the time step of the citowa simulation is hourly and the simulation period is 2007 2010 in this study we focus on the o2 and the bod simulations 2 3 the optimization algorithm automatic calibration methods search for an optimal set of parameters in the parameter space although it is possible to obtain an optimum parameter set and an optimum model output when calibrating against a specific set of data this calibrated model may perform poorly for a different data set during the validation in that case other parameter sets may be ranked as optimal brazier et al 2000 moreover even if a researcher puts his faith in a single best parameter set this only minimizes the overall residuals but does not eliminate them completely nossent 2012 consequently the model results are made with a certain amount of uncertainty beven 2000 therefore the existence of a single best parameter set is rejected by some optimization algorithms instead of searching for a single optimal parameter set and for the best model simulation results they propose alternative methods for finding a range a distribution or a probability function of model parameters i e posterior distributions of the parameters and model output beven and binley 1992 the parameter uncertainty range can be inferred from the posterior distribution of the parameter values kuczera and parent 1998 vrugt et al 2003 indeed rejecting the idea of an optimal parameter set allows for obtaining estimates of the parameter uncertainty and the model simulation uncertainty bayesian inference using markov chain monte carlo mcmc sampling bates and campbell 2001 kuczera and parent 1998 is a well known solution to infer the posterior distribution of the parameters measurements of the system responses i e calibration data play an important role in bayesian inferences using mcmc algorithm one of the advantage of the bayesian inference is that the measurement error of the calibration data commonly express as standard deviation σ is explicitly incorporated into the estimation of the model parameters the value of the measurement error directly determines the quality of the calibration data and influences on the inferred uncertainty intervals of the parameters raat et al 2004 schoups and vrugt 2010 showed that the mcmc algorithm is able to simultaneously identify the hydrologic model parameters and the appropriate statistical distribution of the residual errors using corrupted synthetic streamflow data dream vrugt et al 2008 2009 is a state of the art mcmc algorithm which applies a formal statistical bayesian approach to perform automatic calibration and uncertainty analysis the convergence of the posterior distributions is evaluated using the r statistic gelman and rubin 1992 in practice a value of the r statistic smaller than 1 2 for all parameters points to convergence vrugt et al 2009 dream zs vrugt et al 2008 2009 is an extension of dream to simplify inference and to enhance the convergence rate due to the robustness and efficiency dream and its extensions have found widespread applications and use in numerous fields in just a few years e g surface hydrology leta et al 2015 soil hydrology wöhling and vrugt 2011 groundwater hydrology mustafa et al 2018 and geophysics laloy et al 2012 considering its advantages the dream zs algorithm is chosen for the automatic calibration and the uncertainty analysis in this research 2 4 the sensitivity analysis it is not feasible to include all the model parameters in the calibration process bekele and nicklow 2007 nossent et al 2011 actually the performance of automatic calibration algorithms is reduced when the number of parameters is large duan et al 1992 in order to support the choice of which model parameters should be focused on during calibration and which ones could be instead excluded from calibration and set to default values global sensitivity analysis gsa is becoming popular in environmental modeling practices e g muleta and nicklow 2005 van werkhoven et al 2009 norton 2015 pianosi et al 2016 gsa indeed allows for the identification of those parameters that have negligible influences on the model output and therefore can be fixed to any value within their feasible range so called factor fixing ff moreover gsa is applied to identify the parameters that have the largest influence on the model output so called factor prioritization fp saltelli et al 2008 among gsa methods the advanced and quantitative gsa method called pawn pianosi and wagener 2015 khorashadi zadeh et al 2017 is adopted to perform the sa for the conceptual wq model prior to calibration and uncertainty analysis pawn is a moment independent and density based gsa method where the entire model output distribution is used to quantify the relative influence of the parameters on the model output therefore the pawn method is suitable for models with highly skewed or mulita modal output distributions the pawn sensitivity indices measure the sensitivity to parameter x i by the distance between the unconditional cumulative distribution function cdf of y i e y is a representative scalar variable such as a performance measure which is obtained by varying all parameters simultaneously and the conditional cdfs of y which are obtained by varying all parameters but x i i e x i is fixed at a nominal value x i liu et al 2006 borgonovo 2007 pianosi and wagener 2015 proposed to measure the distance between the conditional and unconditional cdfs by the kolmogorov smirnov statistic ks kolmogorov 1933 smirnov 1939 the pawn index varies between 0 and 1 the higher the value the more influential x i the approximate numerical procedure to calculate the pawn index is explained in pianosi and wagener 2015 since numerical approximations rather than analytical solutions are utilized in the pawn method to calculate the sensitivity indices small but non zero indices may be obtained for the indices of non influential parameters to define a threshold for screening the parameters khorashadi zadeh et al 2017 proposed to calculate the sensitivity index of a dummy parameter this dummy parameter has no influence on the model output but will have a non zero sensitivity index representing the error due to the numerical approximation hence the parameters whose indices are above the sensitivity index of the dummy parameter can be classified as influential whereas the parameters whose indices are below this index are within the range of the numerical error and should be considered as non influential the readers are referred to khorashadi zadeh et al 2017 for the procedure to calculate the pawn index for the dummy parameter model calibration is performed for dissolved oxygen o2 and biological oxygen demand bod in order to identify the influential parameters for simulating o2 and bod the parameters of the conceptual model reported in table 1 are considered to be analyzed and ranked the ranges of the parameters are determined according to literature and the qual2e manual brown and barnwell 1987 since there is no prior information on the parameter distributions a uniform distribution is considered for each model parameter to select the random data points as mentioned a dummy parameter which has no influence on the model output is added to the list of parameters to set a threshold for the parameter screening so the total number of parameters is 11 a sample size of 300 is used to approximate the conditional and unconditional cumulative distribution functions of the model output the approximation of the conditional distribution is repeated for 10 different condition values for each parameter the total number of model evaluations is 33 300 as a scalar representation for the model output the nash sutcliffe efficiency nse nash and sutcliffe 1970 see eq 3 is calculated by comparing the results of the conceptual model with the observed data points the model parameters are set to default values and the obtained model results are used as observed data points for the sa in eq 3 3 n s e 1 i 1 m c i o b s c i s i m 2 i 1 m c i o b s c i o b s 2 where c i o b s is the ith observed concentration c i s i m is the simulated concentration corresponding to the ith observation c i o b s is the average of the observations and m is the total number of observed data points 2 5 setting up the numerical experiments parameters of the conceptual water quality model of the river zenne are calibrated using synthetic observations for o2 and bod considering the previous studies e g raat et al 2004 talamba et al 2010 van griensven and bauwens 2003 using multiple responses in the calibration results in reduce parameter uncertainty and improves in parameter identification the dream zs algorithm is linked with the conceptual water quality model to simultaneously perform the automatic multi response calibration and the uncertainty analysis the dream zs algorithm starts the search from the prior range of the parameters and tries to reach the true values of the parameters using the synthetic observations in literature it is shown that the dream algorithm is able to find the true parameter values schoups and vrugt 2010 han and zheng 2016 however the impact of the uncertain observations with limited frequency is not systematically investigated we used the synthetic observation as calibrated data due to the following reasons firstly using this way we know the concentration of o2 and bod in the river at every potential time step e g hourly biweekly and monthly whereas it is difficult to collect such high resolution measured wq data in reality secondly for evaluating the performance of the model calibration the true values of the parameters are known while in reality those are always unknown thirdly the model results are not affected by any errors in input data boundary conditions and model structure otherwise those may affect our interpretation for the role of measurement frequency and measurement error on the performance of the automatic calibration and uncertainty analysis the procedure to generate the synthetic observations for o2 and bod is as follows 1 among 10 model parameters three parameters rk2 oxygen reaeration rate rk1 bod deoxygenation rate and rk3 rate of loss of bod due to settling are identified as influential for simulating o2 and bod see section 3 1 therefore the calibration and the uncertainty analysis are performed for these three parameters rk1 rk2 and rk3 are set to assumed true values 0 5 0 and 6 respectively and the other model parameters are set to default values this synthetic parameter set is selected from a number of randomly generated realizations ensuring that the simulated results are reasonable and representative 2 the model is ran to get the model output for o2 and bod at eppegem 3 random errors from a normal distribution with zero mean μ and constant standard deviation σ are selected and added to the simulated o2 and bod to perturb the simulation results see fig 1 to investigate the effect of measurement uncertainty on the parameter estimation process three different sets of perturbed simulation results are generated using three different normal distributions with increasing σ i e 10 20 and 30 of the average simulated values are considered for σ 4 limited data points from the perturbed time series are selected as synthetic observations see fig 2 to investigate the effect of measurement frequency on the parameter estimation three different frequencies are considered all data i e continues time series with an hourly time step biweekly data and monthly data the monthly data is the most common measurement frequency in belgium for wq variables also it should be noted that there are no measurements during the night which is of influence on o2 in the next step rk1 rk2 and rk3 are considered as unknown model parameters and the dream zs algorithm is applied to infer the posterior distributions for these parameters using the synthetic observations the other model parameters are fixed at their default values if it is assumed that the model residuals i e difference between observed and simulated values are uncorrelated and normally distributed with constant σ then the formal log likelihood function can be written as follows vrugt et al 2013 4 l θ σ z n 2 log 2 π n log σ 1 2 σ t 1 n z t y t θ 2 where θ is the parameter set z t is the observation y t θ is the simulated variable σ is the standrad deviation of the model error i e residuals and n is the number of observed data points as shown in eq 4 the standard deviation of the residuals σ is explicitly considered in the likelihood function therefore together with the model parameters the optimum range for σ can be inferred using the dream zs algorithm the size of the measurement error has important effect on dream application results considering eq 4 an increase in the size of σ will result in a wider range of parameter sets that will be considered acceptable in the fitting of the measurements actually large uncertainties in the measurements will result in parameter value selection and consequently a large uncertainty in the model simulations raat et al 2004 for the multi response calibration it is assumed that the residuals of o2 and bod are uncorrelated therefore the multiple log likelihood l m u l t i p l e is equal to the sum of the log likelihood for o2 l o 2 and bod l b o d 5 l o 2 θ σ ˆ o 2 o 2 n 2 log 2 π n log σ ˆ o 2 1 2 σ ˆ o 2 2 t 1 n o 2 t o 2 t θ 2 6 l b o d θ σ ˆ b o d b o d n 2 log 2 π n log σ ˆ b o d 1 2 σ ˆ b o d 2 t 1 n b o d t b o d t θ 2 7 l m u l t i p l e l o 2 l b o d where o 2 t and b o d t are synthetic obsevations for o2 and bod respectively and o 2 t θ and b o d t θ are simulated o2 and bod using parameter set θ respectively σ ˆ o 2 and σ ˆ b o d are standard deviations of the model residuals for simulating o2 and bod respectively as the synthetic observations of o2 and bod are generated using the equations of the wq simulator it is assumed that the model structure i e the structure of its equations is true therefore all different sources of the uncertainty including the input uncertainty the boundary condition uncertainty and the model structural uncertainty are excluded from the calibration by using the synthetic data the uncertainty is only related to the unknown model parameters rk1 rk2 and rk3 and the measurement error used to generate the synthetic observations the algorithm explicitly takes into account the parameter uncertainty and searches for the optimal ranges of the parameters therefore the true values of the standard deviations of the residuals in eqs 5 and 6 σ ˆ o 2 and σ ˆ b o d should be equal to the standard deviations of the random errors used to generate the synthetic observations σ o 2 and σ b o d see fig 1 therefore in addition to the unknown model parameters rk1 rk2 and rk3 σ ˆ o 2 and σ ˆ b o d are considered unknown and subject to calibration actually σ ˆ o 2 and σ ˆ b o d quantify the model uncertainty and determine the total uncertainty bounds for o2 and bod respectively in this way the possibility to quantify the model error using limited and uncertain observed data is evaluated the prior ranges of the parameters are determined according to literature and the qual2e manual brown and barnwell 1987 the prior ranges and the true values of the parameters and the standard deviations are reported in table 2 to change the model parameters in the wq simulator multiplier factors are applied on the default values that are pre programmed in the simulator the multiplier are provided in a specific input file of the model the multiplier factors of the non influential parameters are set to 1 the ranges reported in table 2 are the prior ranges for the multiplier factors of rk1 rk2 and rk3 in order to investigate the impact of the measurement error and the measurement frequency on the parameter estimation process and the model uncertainty quantification 9 scenarios with different measurement frequencies and measurement errors are performed see table 3 the dream zs algorithm with 9000 iterations is performed for each scenario the preliminary results show that 9000 iterations are sufficient to reach convergence for the posterior distributions of the model parameters and the standard deviations 3 results and discussion 3 1 the results of the sensitivity analysis the results of the pawn sensitivity indices of the model parameters together with the dummy parameter are presented in fig 3 the dash line represents the sensitivity index of the dummy parameter as a threshold for the parameter screening for simulating o2 fig 3 a rk2 is the most influential parameters followed by rk1 and rk3 considering the sensitivity index of the dummy parameter dash line in fig 3 a the other parameters considered less influential for simulating o2 for simulating bod rk3 and rk1 are the influential parameters therefore for calibrating o2 and bod three parameters rk1 rk2 and rk3 are selected 3 2 the synthetic observations the generated synthetic observations for scenarios 30 error all 30 error biweekly and 30 error monthly are depicted in fig 4 the bold black graphs are the simulated model output as true data the normal error with standard deviation equal to 30 of the average simulated values is added to the model output to generate synthetic observations for scenario 30 error all then biweekly and monthly samplings are performed to generate the synthetic data for scenarios 30 error biweekly and 30 error monthly the synthetic observations for scenarios with 20 and 10 errors are the same as fig 4 but with lower errors as compared to those in fig 4 3 3 the results of the convergence analysis in practice a value of the r statistic smaller than 1 2 for all parameters indicates that the algorithm finds stationary posterior distributions for the parameters and converged results are obtained figs 5 and 6 show the r statistic values of all the parameters for scenarios 10 error all 10 error biweekly and 10 error monthly respectively considering the r statistic values of the parameters converged results are obtained with the maximum number of iterations 9000 iterations the r statistic is lower than 1 2 for all the parameters similar graphs are obtained for the r statistic values in other scenarios as converged results are needed for sampling the posterior distribution the last 25 generated samples are selected to infer the posterior distributions the iteration number 6750 to 9000 3 4 the residuals post processing one of the advantages of the formal bayesian approach for the parameter inference is that the assumptions regarding the residuals i e model error can be verified to formulate the likelihood function it is assumed that the residuals are normally distributed with constant standard deviation fig 7 illustrates that the model residuals for o2 and bod simulations are randomly distributed around zero with constant standard deviation for scenarios 30 error monthly and 30 error biweekly the residual analysis for the other scenarios not shown also confirm that the model errors have a constant standard deviations the empirical cdf of o2 and bod residuals for scenarios 30 error monthly and 30 error biweekly are compare with a cdf of the normal distribution as shown in fig 8 the empirical cdf of the residuals are almost match with the cdf of the normal distribution moreover the statistical normality test one sample kolmogorov smirnov test kolmogorov 1933 smirnov 1939 confirms that the o2 and bod residuals have a normal distribution for all scenarios 3 5 the parameters uncertainty figs 9 11 show the inferred posterior probability distributions of the parameters when synthetic observations with 10 error 20 error and 30 error are used respectively the vertical blue lines represent the true values for the model parameters and the standard deviations used to generate the synthetic observations the first rows of the figures show the results when all data points i e continues time series with an hourly time step are used the second and the third rows correspond to the results when biweekly and monthly data are used respectively although the true values are included in the posterior ranges wider ranges are obtained for the parameters when biweekly and monthly data are used as compared to those obtained by using all data points the posterior distribution of the model parameters and the standard deviations well embrace the respective synthetic true values when all data points are used irrespective of the measurement uncertainty see first rows in figs 9 11 when the measurement uncertainty is 10 there are no significant differences between the inferred posterior ranges of the model parameters using biweekly and monthly data see second and third rows in fig 9 however inferred posterior distributions of σ ˆ o 2 and σ ˆ b o d are got wider when the measurement frequency is reduced from biweekly to monthly for the measurement uncertainty of 20 reducing the measurement frequency from biweekly to monthly has a significant effect on the posterior ranges of rk1 rk3 and σ ˆ b o d see second and third rows in fig 10 the uncertainty intervals for these three parameters increase by decreasing the calibration data points when the measurement error is 30 decreasing the measurement frequency from biweekly to monthly has a significant effect on the inferred posterior distributions when the measurement uncertainty is 30 see second and third rows in fig 11 the posterior distributions are got wider and the true values of the model parameters rk1 rk2 and rk3 are not well identified in order to further evaluate the performance of the parameter estimation and the uncertainty quantification in different scenarios the average absolute distance aad between the posterior parameter sets last 25 of the parameter sets and the true values is calculated for this purpose the absolute distance between the posterior samples and the true value are calculated for all five parameters i e 3 model parameters and 2 standard deviations then the mean of the absolute distances of all the posterior samples is calculated as aad for each parameter the perfect value for the aad is zero a lower aad indicates a better performance for the parameter estimation and the uncertainty quantification fig 12 shows the adds for the model parameters and the standard deviations in different scenarios the aads are close to zero and the parameter estimation is almost perfect when all data points are used as synthetic observations irrespective to measurement error the aads of the parameters increase by reducing the data points of the synthetic observations from all data to biweekly and from biweekly to monthly data except for rk1 and rk2 when the measurement uncertainty is 10 this increase is larger when the measurement uncertainty is 30 green graphs in the figure interesting when the frequency of the calibration data is biweekly the values of aads of model parameters are not changed considerably by increasing the measurement uncertainty while increasing the measurement uncertainty results in considerable decrease in the calibration performance higher add value when the measurement frequency is monthly this highlight the importance of the measurement frequency on the performance of the calibration 3 6 the model uncertainty the 95 confidence intervals of the model simulation due to the parameter uncertainty is obtained by running the model using the posterior samples then the simulation results are ranked for each time step and the 0 025th and 0 975th quantiles are obtained for every time step the black ranges in figs 13 and 14 show the 95 confidence intervals of the o2 and bod simulations respectively due to the parameter uncertainty for the scenario 30 error biweekly top panels in the figures and for the scenario 30 error monthly bottom panels in the figures interestingly parameter uncertainty ranges for o2 and bod follow the trend of the true values in both scenarios however the model uncertainty ranges due to the parameter uncertainty is wider when the frequency of the calibration data is monthly as compared to those when the calibration data is biweekly the reason is related to the fewer number of calibration data in the scenario with the monthly calibration data similarly in section 3 5 the wider posterior distributions for the parameters i e the wider parameter uncertainty are observed when the monthly data is used as represented in figs 13 and 14 the 95 confidence intervals due to the parameter uncertainty cannot cover the synthetic observations it indicates that the parameter uncertainty explains the small part of the total uncertainty and the measurement uncertainty should be considered to explain the total uncertainty the inferred standard deviations for o2 and bod residuals see fig 11 are used to add the measurement error to the simulation results then the 0 025th and 0 975th quantiles of the simulation results are obtained for every time step the gray ranges in figs 13 and 14 show the 95 confidence intervals due to the total uncertainty the total uncertainty ranges cover the most synthetic measurements for o2 and bod as σ ˆ o 2 has a wider posterior distribution in the scenario 30 error monthly as compare to that in the scenario 30 error biweekly see fig 11 the total uncertainty range of the o2 simulation is wider in the scenario 30 error monthly see fig 13 on the other hand the posterior distributions of the σ ˆ b o d in the scenarios 30 error biweekly and 30 error monthly are almost the same see fig 11 therefore the total uncertainty ranges of bod simulations have almost the same wide in these two scenarios see fig 14 the figures for the model uncertainty of the other scenarios have the same patterns as figs 13 and 14 the uncertainty range due to the parameter uncertainty is very narrow for scenarios where all data points are used the uncertainty ranges for scenarios 20 error and 10 error are narrower as compared to those in scenarios 30 error because the measurement error in scenarios 20 error and 10 error are lower than that of 30 error 3 7 discussion the results of this study are used to investigate the impact of the uncertain calibration data with limited frequency on the performance of the mcmc algorithm dream zs to identify the optimum ranges of the parameters and to characterize the simulation uncertainty the evaluation is performed by comparing the synthetic true parameters with the posterior distributions of the parameters inferred using synthetic observations as calibration data the comparisons are performed for 9 experiments with different configurations for the synthetic observations i e different measurement errors and different measurement frequencies however it should be noted that the comparisons are performed for a specific case study river zenne in belgium many choices made in this study including the conceptual river water quality simulator the climate conditions the flow regimes the pollutant concentrations at boundaries and the selection of the simulation period may affect the results moreover the effect of the other sources of uncertainties e g input data uncertainty model structural uncertainty on the model calibration are avoided by using synthetic measurements in this analysis the sampling plans were nonstrategic for the monthly data the samples were selected on 15th of each month and for the biweekly data the samples were selected on 7th and 21st of each month however a strategic sampling plan which incorporates our prior knowledge about water quality problems of concern can generate more desirable results than a nonstrategic sampling plan zheng and keller 2007b therefore the importance of designing cost effective sampling plan is evident in a cost effective sampling plan a limited number of samples is collected while all relevant characteristic of the watershed response are represented the main point to design a strategic sampling plan is to identify the connection between the management variable s of concern and the driving forces e g rainfall irrigation pollutant source loading pattern etc to reduce the cost of sampling sampling times with a high information content should be selected wang et al 2017 for different water quality issues e g nutrient dissolve oxygen and metals different sampling strategies may work coupling the model with the optimization algorithm and using the synthetic observations it is possible to systematically evaluate the effectiveness of different sampling plans for parameter estimation and uncertainty quantification it supports the monitoring program to decide what and when to measure for an already gauged catchment with a given frequency reliability and type of observed data for the calibration the methodology presented in this paper informs the modeler about the minimum uncertainty associated with a model application because in this analysis the simulation is free of model error and input and boundary data errors the advantage of knowing this beforehand is that it may prevent the modeler from an endless search for a best parameter set it should be noted that the results of such studies are very conservative estimate of the parameter and model uncertainties and actual uncertainties are higher raat et al 2004 the results of this study showed that increasing the number of data points by increasing the measurement frequency substantially improves the performance of the calibration however increasing the number of observations in the calibration dataset can also be extend by extending the period of the data collection i e extending the simulation period further study is needed to evaluate the effectiveness of extending the period of the dataset on the performance of the calibration and uncertainty quantification furthermore this study can be extended by including the nitrogen components i e organic nitrogen nitrate and ammonia in the model calibration this results in increasing the number of calibration parameters in this way it is possible to investigate the effect of combining different datasets and increasing the number of parameters on the performance of the multi response calibration using mcmc algorithm last but not least computational cost is a great concern for the mcmc based bayesian calibration han and zheng 2016 although the conceptual model in this study only takes about 17 s on a machine with the processor intel core i3 350m 2 26 ghz and 4 gb ram to complete a four years simulation running dream zs with 9000 iterations still requires five days to finish however although costly it is feasible on the other hand it is not comparable with the computational cost of the detailed river water quality model developed by shrestha et al 2017 for the river zenne which requires about one week to run a four years simulation woldegiorgis 2017 therefore the conceptual wq model can be applied to support decisions regarding the measurement prioritization 4 conclusion parameter estimation requires measurements of the system response however in some fields such as water quality modeling the measurement frequency is limited moreover the measurements are uncertain these limitations cause uncertainty in the parameter estimation and in the simulation results the objective of this study was to investigate the effect of the measurement frequency and uncertainty on the parameter estimation process and the uncertainty quantification to this aim the dream zs algorithm a state of the art mcmc algorithm is applied to estimate the parameter values and to infer the parameter uncertainty and the measurement uncertainty of the citowa wq model of the river zenne belgium using synthetic observations as calibration data finally the inferred posterior distributions of the parameters are compared with their true values to evaluate the performance of the parameter estimation and uncertainty quantification major conclusions include the following the results highlight the critical roles of measurement frequency and measurement uncertainty in identifying the posterior parameter distribution the effect of the measurement uncertainty on the performance of the parameter estimation is significant when the observed data points for the calibration are limited e g monthly data in conclusion the methodology proposed in this paper can be applied for the following purposes to support the measurement prioritization and resource allocation to estimate the minimum parameter uncertainty and model uncertainty due to the limited uncertain measurement to evaluate the effectiveness of extending the period of dataset on the calibration performance to evaluate the effect of combining different datasets on the calibration performance multi response calibration although our application focused on the conceptual wq model the presented methodology is entirely general and may be useful for dealing with complex models with a large number of parameters and multiple outputs software data availability the pawn method is implemented in the safe matlab octave toolbox for gsa pianosi et al 2015 safe is freely available for non commercial purposes at www bristol ac uk cabot resources safe toolbox the matlab toolbox of dream is available upon request from the author jasper uci edu the citowa tool is available upon request from the author befekadu woldegiorgis usask ca acknowledgment the authors would like to thank the flanders hydraulics research for supporting and coordinating the project of development of conceptual models for an integrated river basin management 
26194,parameter estimation using historical observed data is an important part of the environmental modeling the uncertainty in the parameter estimation limits the applications of environmental models in this paper the influence of limited and uncertain calibrated data on the performance of the parameter estimation are systematically investigated for this purpose synthetic observations with a given uncertainty and frequency are used to estimate the model parameters of a conceptual water quality wq model of the river zenne belgium bayesian inference using markov chain monte carlo sampling is adopted to simultaneously perform the automatic calibration and the uncertainty analysis the results highlight the critical roles of measurement frequency and uncertainty in the model calibration we found that the effect of the measurement uncertainty on the parameter estimation is significant when the calibrated data points are limited e g monthly data the research findings can be used to support measurement prioritization and resource allocation keywords measurement uncertainty measurement frequency parameter estimation parameter uncertainty simulation uncertainty dream zs 1 introduction in order to use environmental models for different tasks such as predictions scenario analysis and setting up regulations they should represent the reality adequately moreover they should be scientifically sound robust and defensible u s epa 2002 in general model results are affected by the model structure i e the model assumptions the model inputs the boundary conditions and the model parameters van griensven and meixner 2006 rode et al 2010 the underlying assumptions of the model are often fixed and therefore the model structure is not changed during the modeling processes moreover the input data and the boundary conditions obtained through measuring campaigns or provided by responsible authorities are not altered by the modeler nossent 2012 on the other hand most of the model parameters representing some properties of the system cannot be measured directly vrugt et al 2003 as a consequence the model parameters should be set to appropriate values in order to increase the agreement between the model results and the real system the parameters adjustment is based on the reduction of the difference between the model results and historical measurements of the system response laloy et al 2010 vrugt et al 2013 leta et al 2015 this procedure is referred to as parameter estimation parameter optimization model calibration or inverse modeling raat et al 2004 parameter estimation using historical observed data is an important part of the environmental modeling practice which has been the focus of many researches and studies e g duan et al 2006 however because the models are only an approximation of the real system and the observed data used for the calibration contain error i e measurement uncertainty parameter estimation is error prone vrugt et al 2002 as a result it is difficult to well identify the parameters and the parameter uncertainty is caused in addition in some fields such as water quality modeling the data collection is resource intensive mannina and viviani 2010 consequently the available data has a limited frequency e g biweekly or even monthly intervals zheng and keller 2007a in these cases a serious complication for the model calibration is the lack of reliable calibration data which results in parameter uncertainty raat et al 2004 franceschini and tsai 2010a b the ambiguity in the parameter estimation has considerable impact on the model simulation uncertainty and therefore limits the applications of environmental models such as water quality models wagener et al 2003 despite the critical role of amount and reliability of calibration data on the performance of the parameter estimation to the best of the authors knowledge the literature contains very few studies on quantifying the impact of these two aspects of the measurements i e amount and reliability on the parameter uncertainty of water quality models for example for a catchment nitrogen modeling raat et al 2004 explored the relationship between the quality of the calibration data i e measurement uncertainty and the uncertainty associated with the final parameter estimates i e parameter uncertainty using virtual data wang et al 2017 used synthetic data to explore how the number of tracer i e isotope data samples i e measurement amount affect model calibration however the effect of measurement errors of the tracer data on the parameter estimation was not studies therefore it is needed to investigate the effect of both amount and reliability of the calibration data on the performance of the calibration neglecting the other source of uncertainties e g model structural uncertainty input data uncertainty a modeler should first investigate that it is feasible to reach a pre defined model performance with a given amount of uncertain measured data as the impact of these critical characteristics of calibration data amount and reliability on the model calibration has not fully addressed in literature in this study we focus on assessing the influence of limited uncertain calibrated data on the performance of the parameter estimation and on the parameter uncertainty intervals of water quality models for this purpose the following questions are formulated 1 what is the effect of increasing the measurement frequency on the water quality parameter estimation 2 what is the effect of reducing the measurement error of the observed data on the water quality parameter estimation to address the research questions synthetic observations with a given uncertainty and frequency are used to estimate the model parameters of a conceptual water quality wq model of the river zenne in belgium for simulation dissolved oxygen o2 and biological oxygen demand bod as an optimization tool bayesian inference using markov chain monte carlo mcmc sampling bates and campbell 2001 kuczera and parent 1998 is adopted to simultaneously perform the automatic calibration and the uncertainty analysis the synthetic data series are generated by running a wq model with a given set of model parameter values as true values the model outputs are then perturbed with a pre specified random error as measurement error and sampled with a given frequency to mimic discrete measurements the sampled data are then considered as if they were observed data and used to calibrate the model parameters using the mcmc algorithm finally it is verified if the model parameters can be identified using the limited and uncertain data to evaluate the relationship between the amount and reliability of the calibration data and the parameter uncertainty nine different synthetic data sets are generated by increasing the measurement error and decreasing the measurement frequency then the generated synthetic data are used as calibration data in subsequent optimization runs 2 methods and materials 2 1 conceptual integrated tool for water quality assessment citowa in order to enhance the applicability of conceptual river water quality simulators woldegiorgis woldegiorgis 2017 woldegiorgis et al 2017 developed a conceptual integrated tool for water quality assessment citowa as an alternative to detailed wq simulators in citowa the river system is represented by reaches which are conceptual elements that divide the channel longitudinally into different parts citowa obtains estimates of discharges and velocities of the reaches from external simulation tools the reaches are modeled as continuously stirred tanks reactors cstr whereby complete mixing is assumed within each reach the citowa integrates several components the first component is the simulator of dissolved oxygen biochemical oxygen demand nitrogen compounds phosphorus compounds and algae the simulator applies a formulation based on ordinary differential equations of the physico chemical processes based on a qual 2e brown and barnwell 1985 type formulation it integrates the mass balance equation with the transformation decay processes and solves the resulting differential equation using a quasi analytical solution scheme woldegiorgis et al 2017 the variables which can be simulated in the model are nitrogen compounds phosphorus compounds algae biological oxygen demand and dissolved oxygen moreover citowa can be used to simulate the sediment sediment bound pollutants and any arbitrary pollutant along a river the qual 2e differential equations used for simulating o2 and bod in citowa are 1 d o 2 d t r k 2 o 2 o 2 α 3 μ α 4 ρ a r k 1 b o d r k 4 d α 5 b c 1 n h 4 α 6 b c 2 n o 2 where o2 is the dissolved oxygen concentration mg l t is the time day rk2 is the oxygen reaeration rate 1 day o2 is the dissolved oxygen concentration at saturation mg l α3 is the rate of oxygen production per unit of algal photosynthesis mg o2 mg algae μ is algal growth rate 1 day α4 is the rate of oxygen uptake per unit of algae respired mg o2 mg algae ρ is algal respiration rate 1 day a is algal concentration mg l rk1 is carbonaceous bod deoxygenation rate 1 day bod is the carbonaceous biochemical oxygen demand mg l rk4 is sediment oxygen demand rate g m2 day 1 d is the water depth m α5 is the rate of oxygen uptake per unit of ammonia nitrogen oxydation mg o mg n bc1 is rate constant for biological oxidation of nh4 to no2 1 day nh4 is ammonia nitrogen concentration mg l α6 is the rate of oxygen uptake per unit of nitrite nitrogen oxydation mg o mg n bc2 is rate constant for biological oxidation of no2 to no3 1 day no2 and no3 are nitrite and nitrate nitrogen concentrations mg l respectively 2 d b o d d t r k 1 b o d r k 3 b o d where rk3 is the rate of loss of bod due to settling 1 day 2 2 the case study a conceptual water quality model of the river zenne built up using citowa woldegiorgis 2017 is selected as a case study the river zenne is a lowland tributary of the river scheldt in belgium the study area extends from the border of the walloon region to eppegem the upstream end of the tidal part of the river the river is connected to the nearby navigation canal brussels charleroi at multiple overflow points to protect the urban region of brussels from flooding woldegiorgis 2017 the model takes into account the interactions between the river and the canal and comprises a total of 52 reaches the reader may refer to woldegiorgis 2017 for the complex stream network configuration the major pollution sources of the system are the effluent from waste water treatment plants wwtps the diffuse pollution from agricultural areas and the emissions from combined overflows csos all the pollution sources including the inlet from wallonia at the upstream border of the model the inlet from the tributary rivers and the effluent of the wwtps and the csos are treated as point source pollutant boundaries woldegiorgis 2017 the zenne citowa model used the reach discharges the flow depths and the velocity data from a reservoir type model of meert et al 2016 the sources of the input data and the boundary data are explained in woldegiorgis 2017 the time step of the citowa simulation is hourly and the simulation period is 2007 2010 in this study we focus on the o2 and the bod simulations 2 3 the optimization algorithm automatic calibration methods search for an optimal set of parameters in the parameter space although it is possible to obtain an optimum parameter set and an optimum model output when calibrating against a specific set of data this calibrated model may perform poorly for a different data set during the validation in that case other parameter sets may be ranked as optimal brazier et al 2000 moreover even if a researcher puts his faith in a single best parameter set this only minimizes the overall residuals but does not eliminate them completely nossent 2012 consequently the model results are made with a certain amount of uncertainty beven 2000 therefore the existence of a single best parameter set is rejected by some optimization algorithms instead of searching for a single optimal parameter set and for the best model simulation results they propose alternative methods for finding a range a distribution or a probability function of model parameters i e posterior distributions of the parameters and model output beven and binley 1992 the parameter uncertainty range can be inferred from the posterior distribution of the parameter values kuczera and parent 1998 vrugt et al 2003 indeed rejecting the idea of an optimal parameter set allows for obtaining estimates of the parameter uncertainty and the model simulation uncertainty bayesian inference using markov chain monte carlo mcmc sampling bates and campbell 2001 kuczera and parent 1998 is a well known solution to infer the posterior distribution of the parameters measurements of the system responses i e calibration data play an important role in bayesian inferences using mcmc algorithm one of the advantage of the bayesian inference is that the measurement error of the calibration data commonly express as standard deviation σ is explicitly incorporated into the estimation of the model parameters the value of the measurement error directly determines the quality of the calibration data and influences on the inferred uncertainty intervals of the parameters raat et al 2004 schoups and vrugt 2010 showed that the mcmc algorithm is able to simultaneously identify the hydrologic model parameters and the appropriate statistical distribution of the residual errors using corrupted synthetic streamflow data dream vrugt et al 2008 2009 is a state of the art mcmc algorithm which applies a formal statistical bayesian approach to perform automatic calibration and uncertainty analysis the convergence of the posterior distributions is evaluated using the r statistic gelman and rubin 1992 in practice a value of the r statistic smaller than 1 2 for all parameters points to convergence vrugt et al 2009 dream zs vrugt et al 2008 2009 is an extension of dream to simplify inference and to enhance the convergence rate due to the robustness and efficiency dream and its extensions have found widespread applications and use in numerous fields in just a few years e g surface hydrology leta et al 2015 soil hydrology wöhling and vrugt 2011 groundwater hydrology mustafa et al 2018 and geophysics laloy et al 2012 considering its advantages the dream zs algorithm is chosen for the automatic calibration and the uncertainty analysis in this research 2 4 the sensitivity analysis it is not feasible to include all the model parameters in the calibration process bekele and nicklow 2007 nossent et al 2011 actually the performance of automatic calibration algorithms is reduced when the number of parameters is large duan et al 1992 in order to support the choice of which model parameters should be focused on during calibration and which ones could be instead excluded from calibration and set to default values global sensitivity analysis gsa is becoming popular in environmental modeling practices e g muleta and nicklow 2005 van werkhoven et al 2009 norton 2015 pianosi et al 2016 gsa indeed allows for the identification of those parameters that have negligible influences on the model output and therefore can be fixed to any value within their feasible range so called factor fixing ff moreover gsa is applied to identify the parameters that have the largest influence on the model output so called factor prioritization fp saltelli et al 2008 among gsa methods the advanced and quantitative gsa method called pawn pianosi and wagener 2015 khorashadi zadeh et al 2017 is adopted to perform the sa for the conceptual wq model prior to calibration and uncertainty analysis pawn is a moment independent and density based gsa method where the entire model output distribution is used to quantify the relative influence of the parameters on the model output therefore the pawn method is suitable for models with highly skewed or mulita modal output distributions the pawn sensitivity indices measure the sensitivity to parameter x i by the distance between the unconditional cumulative distribution function cdf of y i e y is a representative scalar variable such as a performance measure which is obtained by varying all parameters simultaneously and the conditional cdfs of y which are obtained by varying all parameters but x i i e x i is fixed at a nominal value x i liu et al 2006 borgonovo 2007 pianosi and wagener 2015 proposed to measure the distance between the conditional and unconditional cdfs by the kolmogorov smirnov statistic ks kolmogorov 1933 smirnov 1939 the pawn index varies between 0 and 1 the higher the value the more influential x i the approximate numerical procedure to calculate the pawn index is explained in pianosi and wagener 2015 since numerical approximations rather than analytical solutions are utilized in the pawn method to calculate the sensitivity indices small but non zero indices may be obtained for the indices of non influential parameters to define a threshold for screening the parameters khorashadi zadeh et al 2017 proposed to calculate the sensitivity index of a dummy parameter this dummy parameter has no influence on the model output but will have a non zero sensitivity index representing the error due to the numerical approximation hence the parameters whose indices are above the sensitivity index of the dummy parameter can be classified as influential whereas the parameters whose indices are below this index are within the range of the numerical error and should be considered as non influential the readers are referred to khorashadi zadeh et al 2017 for the procedure to calculate the pawn index for the dummy parameter model calibration is performed for dissolved oxygen o2 and biological oxygen demand bod in order to identify the influential parameters for simulating o2 and bod the parameters of the conceptual model reported in table 1 are considered to be analyzed and ranked the ranges of the parameters are determined according to literature and the qual2e manual brown and barnwell 1987 since there is no prior information on the parameter distributions a uniform distribution is considered for each model parameter to select the random data points as mentioned a dummy parameter which has no influence on the model output is added to the list of parameters to set a threshold for the parameter screening so the total number of parameters is 11 a sample size of 300 is used to approximate the conditional and unconditional cumulative distribution functions of the model output the approximation of the conditional distribution is repeated for 10 different condition values for each parameter the total number of model evaluations is 33 300 as a scalar representation for the model output the nash sutcliffe efficiency nse nash and sutcliffe 1970 see eq 3 is calculated by comparing the results of the conceptual model with the observed data points the model parameters are set to default values and the obtained model results are used as observed data points for the sa in eq 3 3 n s e 1 i 1 m c i o b s c i s i m 2 i 1 m c i o b s c i o b s 2 where c i o b s is the ith observed concentration c i s i m is the simulated concentration corresponding to the ith observation c i o b s is the average of the observations and m is the total number of observed data points 2 5 setting up the numerical experiments parameters of the conceptual water quality model of the river zenne are calibrated using synthetic observations for o2 and bod considering the previous studies e g raat et al 2004 talamba et al 2010 van griensven and bauwens 2003 using multiple responses in the calibration results in reduce parameter uncertainty and improves in parameter identification the dream zs algorithm is linked with the conceptual water quality model to simultaneously perform the automatic multi response calibration and the uncertainty analysis the dream zs algorithm starts the search from the prior range of the parameters and tries to reach the true values of the parameters using the synthetic observations in literature it is shown that the dream algorithm is able to find the true parameter values schoups and vrugt 2010 han and zheng 2016 however the impact of the uncertain observations with limited frequency is not systematically investigated we used the synthetic observation as calibrated data due to the following reasons firstly using this way we know the concentration of o2 and bod in the river at every potential time step e g hourly biweekly and monthly whereas it is difficult to collect such high resolution measured wq data in reality secondly for evaluating the performance of the model calibration the true values of the parameters are known while in reality those are always unknown thirdly the model results are not affected by any errors in input data boundary conditions and model structure otherwise those may affect our interpretation for the role of measurement frequency and measurement error on the performance of the automatic calibration and uncertainty analysis the procedure to generate the synthetic observations for o2 and bod is as follows 1 among 10 model parameters three parameters rk2 oxygen reaeration rate rk1 bod deoxygenation rate and rk3 rate of loss of bod due to settling are identified as influential for simulating o2 and bod see section 3 1 therefore the calibration and the uncertainty analysis are performed for these three parameters rk1 rk2 and rk3 are set to assumed true values 0 5 0 and 6 respectively and the other model parameters are set to default values this synthetic parameter set is selected from a number of randomly generated realizations ensuring that the simulated results are reasonable and representative 2 the model is ran to get the model output for o2 and bod at eppegem 3 random errors from a normal distribution with zero mean μ and constant standard deviation σ are selected and added to the simulated o2 and bod to perturb the simulation results see fig 1 to investigate the effect of measurement uncertainty on the parameter estimation process three different sets of perturbed simulation results are generated using three different normal distributions with increasing σ i e 10 20 and 30 of the average simulated values are considered for σ 4 limited data points from the perturbed time series are selected as synthetic observations see fig 2 to investigate the effect of measurement frequency on the parameter estimation three different frequencies are considered all data i e continues time series with an hourly time step biweekly data and monthly data the monthly data is the most common measurement frequency in belgium for wq variables also it should be noted that there are no measurements during the night which is of influence on o2 in the next step rk1 rk2 and rk3 are considered as unknown model parameters and the dream zs algorithm is applied to infer the posterior distributions for these parameters using the synthetic observations the other model parameters are fixed at their default values if it is assumed that the model residuals i e difference between observed and simulated values are uncorrelated and normally distributed with constant σ then the formal log likelihood function can be written as follows vrugt et al 2013 4 l θ σ z n 2 log 2 π n log σ 1 2 σ t 1 n z t y t θ 2 where θ is the parameter set z t is the observation y t θ is the simulated variable σ is the standrad deviation of the model error i e residuals and n is the number of observed data points as shown in eq 4 the standard deviation of the residuals σ is explicitly considered in the likelihood function therefore together with the model parameters the optimum range for σ can be inferred using the dream zs algorithm the size of the measurement error has important effect on dream application results considering eq 4 an increase in the size of σ will result in a wider range of parameter sets that will be considered acceptable in the fitting of the measurements actually large uncertainties in the measurements will result in parameter value selection and consequently a large uncertainty in the model simulations raat et al 2004 for the multi response calibration it is assumed that the residuals of o2 and bod are uncorrelated therefore the multiple log likelihood l m u l t i p l e is equal to the sum of the log likelihood for o2 l o 2 and bod l b o d 5 l o 2 θ σ ˆ o 2 o 2 n 2 log 2 π n log σ ˆ o 2 1 2 σ ˆ o 2 2 t 1 n o 2 t o 2 t θ 2 6 l b o d θ σ ˆ b o d b o d n 2 log 2 π n log σ ˆ b o d 1 2 σ ˆ b o d 2 t 1 n b o d t b o d t θ 2 7 l m u l t i p l e l o 2 l b o d where o 2 t and b o d t are synthetic obsevations for o2 and bod respectively and o 2 t θ and b o d t θ are simulated o2 and bod using parameter set θ respectively σ ˆ o 2 and σ ˆ b o d are standard deviations of the model residuals for simulating o2 and bod respectively as the synthetic observations of o2 and bod are generated using the equations of the wq simulator it is assumed that the model structure i e the structure of its equations is true therefore all different sources of the uncertainty including the input uncertainty the boundary condition uncertainty and the model structural uncertainty are excluded from the calibration by using the synthetic data the uncertainty is only related to the unknown model parameters rk1 rk2 and rk3 and the measurement error used to generate the synthetic observations the algorithm explicitly takes into account the parameter uncertainty and searches for the optimal ranges of the parameters therefore the true values of the standard deviations of the residuals in eqs 5 and 6 σ ˆ o 2 and σ ˆ b o d should be equal to the standard deviations of the random errors used to generate the synthetic observations σ o 2 and σ b o d see fig 1 therefore in addition to the unknown model parameters rk1 rk2 and rk3 σ ˆ o 2 and σ ˆ b o d are considered unknown and subject to calibration actually σ ˆ o 2 and σ ˆ b o d quantify the model uncertainty and determine the total uncertainty bounds for o2 and bod respectively in this way the possibility to quantify the model error using limited and uncertain observed data is evaluated the prior ranges of the parameters are determined according to literature and the qual2e manual brown and barnwell 1987 the prior ranges and the true values of the parameters and the standard deviations are reported in table 2 to change the model parameters in the wq simulator multiplier factors are applied on the default values that are pre programmed in the simulator the multiplier are provided in a specific input file of the model the multiplier factors of the non influential parameters are set to 1 the ranges reported in table 2 are the prior ranges for the multiplier factors of rk1 rk2 and rk3 in order to investigate the impact of the measurement error and the measurement frequency on the parameter estimation process and the model uncertainty quantification 9 scenarios with different measurement frequencies and measurement errors are performed see table 3 the dream zs algorithm with 9000 iterations is performed for each scenario the preliminary results show that 9000 iterations are sufficient to reach convergence for the posterior distributions of the model parameters and the standard deviations 3 results and discussion 3 1 the results of the sensitivity analysis the results of the pawn sensitivity indices of the model parameters together with the dummy parameter are presented in fig 3 the dash line represents the sensitivity index of the dummy parameter as a threshold for the parameter screening for simulating o2 fig 3 a rk2 is the most influential parameters followed by rk1 and rk3 considering the sensitivity index of the dummy parameter dash line in fig 3 a the other parameters considered less influential for simulating o2 for simulating bod rk3 and rk1 are the influential parameters therefore for calibrating o2 and bod three parameters rk1 rk2 and rk3 are selected 3 2 the synthetic observations the generated synthetic observations for scenarios 30 error all 30 error biweekly and 30 error monthly are depicted in fig 4 the bold black graphs are the simulated model output as true data the normal error with standard deviation equal to 30 of the average simulated values is added to the model output to generate synthetic observations for scenario 30 error all then biweekly and monthly samplings are performed to generate the synthetic data for scenarios 30 error biweekly and 30 error monthly the synthetic observations for scenarios with 20 and 10 errors are the same as fig 4 but with lower errors as compared to those in fig 4 3 3 the results of the convergence analysis in practice a value of the r statistic smaller than 1 2 for all parameters indicates that the algorithm finds stationary posterior distributions for the parameters and converged results are obtained figs 5 and 6 show the r statistic values of all the parameters for scenarios 10 error all 10 error biweekly and 10 error monthly respectively considering the r statistic values of the parameters converged results are obtained with the maximum number of iterations 9000 iterations the r statistic is lower than 1 2 for all the parameters similar graphs are obtained for the r statistic values in other scenarios as converged results are needed for sampling the posterior distribution the last 25 generated samples are selected to infer the posterior distributions the iteration number 6750 to 9000 3 4 the residuals post processing one of the advantages of the formal bayesian approach for the parameter inference is that the assumptions regarding the residuals i e model error can be verified to formulate the likelihood function it is assumed that the residuals are normally distributed with constant standard deviation fig 7 illustrates that the model residuals for o2 and bod simulations are randomly distributed around zero with constant standard deviation for scenarios 30 error monthly and 30 error biweekly the residual analysis for the other scenarios not shown also confirm that the model errors have a constant standard deviations the empirical cdf of o2 and bod residuals for scenarios 30 error monthly and 30 error biweekly are compare with a cdf of the normal distribution as shown in fig 8 the empirical cdf of the residuals are almost match with the cdf of the normal distribution moreover the statistical normality test one sample kolmogorov smirnov test kolmogorov 1933 smirnov 1939 confirms that the o2 and bod residuals have a normal distribution for all scenarios 3 5 the parameters uncertainty figs 9 11 show the inferred posterior probability distributions of the parameters when synthetic observations with 10 error 20 error and 30 error are used respectively the vertical blue lines represent the true values for the model parameters and the standard deviations used to generate the synthetic observations the first rows of the figures show the results when all data points i e continues time series with an hourly time step are used the second and the third rows correspond to the results when biweekly and monthly data are used respectively although the true values are included in the posterior ranges wider ranges are obtained for the parameters when biweekly and monthly data are used as compared to those obtained by using all data points the posterior distribution of the model parameters and the standard deviations well embrace the respective synthetic true values when all data points are used irrespective of the measurement uncertainty see first rows in figs 9 11 when the measurement uncertainty is 10 there are no significant differences between the inferred posterior ranges of the model parameters using biweekly and monthly data see second and third rows in fig 9 however inferred posterior distributions of σ ˆ o 2 and σ ˆ b o d are got wider when the measurement frequency is reduced from biweekly to monthly for the measurement uncertainty of 20 reducing the measurement frequency from biweekly to monthly has a significant effect on the posterior ranges of rk1 rk3 and σ ˆ b o d see second and third rows in fig 10 the uncertainty intervals for these three parameters increase by decreasing the calibration data points when the measurement error is 30 decreasing the measurement frequency from biweekly to monthly has a significant effect on the inferred posterior distributions when the measurement uncertainty is 30 see second and third rows in fig 11 the posterior distributions are got wider and the true values of the model parameters rk1 rk2 and rk3 are not well identified in order to further evaluate the performance of the parameter estimation and the uncertainty quantification in different scenarios the average absolute distance aad between the posterior parameter sets last 25 of the parameter sets and the true values is calculated for this purpose the absolute distance between the posterior samples and the true value are calculated for all five parameters i e 3 model parameters and 2 standard deviations then the mean of the absolute distances of all the posterior samples is calculated as aad for each parameter the perfect value for the aad is zero a lower aad indicates a better performance for the parameter estimation and the uncertainty quantification fig 12 shows the adds for the model parameters and the standard deviations in different scenarios the aads are close to zero and the parameter estimation is almost perfect when all data points are used as synthetic observations irrespective to measurement error the aads of the parameters increase by reducing the data points of the synthetic observations from all data to biweekly and from biweekly to monthly data except for rk1 and rk2 when the measurement uncertainty is 10 this increase is larger when the measurement uncertainty is 30 green graphs in the figure interesting when the frequency of the calibration data is biweekly the values of aads of model parameters are not changed considerably by increasing the measurement uncertainty while increasing the measurement uncertainty results in considerable decrease in the calibration performance higher add value when the measurement frequency is monthly this highlight the importance of the measurement frequency on the performance of the calibration 3 6 the model uncertainty the 95 confidence intervals of the model simulation due to the parameter uncertainty is obtained by running the model using the posterior samples then the simulation results are ranked for each time step and the 0 025th and 0 975th quantiles are obtained for every time step the black ranges in figs 13 and 14 show the 95 confidence intervals of the o2 and bod simulations respectively due to the parameter uncertainty for the scenario 30 error biweekly top panels in the figures and for the scenario 30 error monthly bottom panels in the figures interestingly parameter uncertainty ranges for o2 and bod follow the trend of the true values in both scenarios however the model uncertainty ranges due to the parameter uncertainty is wider when the frequency of the calibration data is monthly as compared to those when the calibration data is biweekly the reason is related to the fewer number of calibration data in the scenario with the monthly calibration data similarly in section 3 5 the wider posterior distributions for the parameters i e the wider parameter uncertainty are observed when the monthly data is used as represented in figs 13 and 14 the 95 confidence intervals due to the parameter uncertainty cannot cover the synthetic observations it indicates that the parameter uncertainty explains the small part of the total uncertainty and the measurement uncertainty should be considered to explain the total uncertainty the inferred standard deviations for o2 and bod residuals see fig 11 are used to add the measurement error to the simulation results then the 0 025th and 0 975th quantiles of the simulation results are obtained for every time step the gray ranges in figs 13 and 14 show the 95 confidence intervals due to the total uncertainty the total uncertainty ranges cover the most synthetic measurements for o2 and bod as σ ˆ o 2 has a wider posterior distribution in the scenario 30 error monthly as compare to that in the scenario 30 error biweekly see fig 11 the total uncertainty range of the o2 simulation is wider in the scenario 30 error monthly see fig 13 on the other hand the posterior distributions of the σ ˆ b o d in the scenarios 30 error biweekly and 30 error monthly are almost the same see fig 11 therefore the total uncertainty ranges of bod simulations have almost the same wide in these two scenarios see fig 14 the figures for the model uncertainty of the other scenarios have the same patterns as figs 13 and 14 the uncertainty range due to the parameter uncertainty is very narrow for scenarios where all data points are used the uncertainty ranges for scenarios 20 error and 10 error are narrower as compared to those in scenarios 30 error because the measurement error in scenarios 20 error and 10 error are lower than that of 30 error 3 7 discussion the results of this study are used to investigate the impact of the uncertain calibration data with limited frequency on the performance of the mcmc algorithm dream zs to identify the optimum ranges of the parameters and to characterize the simulation uncertainty the evaluation is performed by comparing the synthetic true parameters with the posterior distributions of the parameters inferred using synthetic observations as calibration data the comparisons are performed for 9 experiments with different configurations for the synthetic observations i e different measurement errors and different measurement frequencies however it should be noted that the comparisons are performed for a specific case study river zenne in belgium many choices made in this study including the conceptual river water quality simulator the climate conditions the flow regimes the pollutant concentrations at boundaries and the selection of the simulation period may affect the results moreover the effect of the other sources of uncertainties e g input data uncertainty model structural uncertainty on the model calibration are avoided by using synthetic measurements in this analysis the sampling plans were nonstrategic for the monthly data the samples were selected on 15th of each month and for the biweekly data the samples were selected on 7th and 21st of each month however a strategic sampling plan which incorporates our prior knowledge about water quality problems of concern can generate more desirable results than a nonstrategic sampling plan zheng and keller 2007b therefore the importance of designing cost effective sampling plan is evident in a cost effective sampling plan a limited number of samples is collected while all relevant characteristic of the watershed response are represented the main point to design a strategic sampling plan is to identify the connection between the management variable s of concern and the driving forces e g rainfall irrigation pollutant source loading pattern etc to reduce the cost of sampling sampling times with a high information content should be selected wang et al 2017 for different water quality issues e g nutrient dissolve oxygen and metals different sampling strategies may work coupling the model with the optimization algorithm and using the synthetic observations it is possible to systematically evaluate the effectiveness of different sampling plans for parameter estimation and uncertainty quantification it supports the monitoring program to decide what and when to measure for an already gauged catchment with a given frequency reliability and type of observed data for the calibration the methodology presented in this paper informs the modeler about the minimum uncertainty associated with a model application because in this analysis the simulation is free of model error and input and boundary data errors the advantage of knowing this beforehand is that it may prevent the modeler from an endless search for a best parameter set it should be noted that the results of such studies are very conservative estimate of the parameter and model uncertainties and actual uncertainties are higher raat et al 2004 the results of this study showed that increasing the number of data points by increasing the measurement frequency substantially improves the performance of the calibration however increasing the number of observations in the calibration dataset can also be extend by extending the period of the data collection i e extending the simulation period further study is needed to evaluate the effectiveness of extending the period of the dataset on the performance of the calibration and uncertainty quantification furthermore this study can be extended by including the nitrogen components i e organic nitrogen nitrate and ammonia in the model calibration this results in increasing the number of calibration parameters in this way it is possible to investigate the effect of combining different datasets and increasing the number of parameters on the performance of the multi response calibration using mcmc algorithm last but not least computational cost is a great concern for the mcmc based bayesian calibration han and zheng 2016 although the conceptual model in this study only takes about 17 s on a machine with the processor intel core i3 350m 2 26 ghz and 4 gb ram to complete a four years simulation running dream zs with 9000 iterations still requires five days to finish however although costly it is feasible on the other hand it is not comparable with the computational cost of the detailed river water quality model developed by shrestha et al 2017 for the river zenne which requires about one week to run a four years simulation woldegiorgis 2017 therefore the conceptual wq model can be applied to support decisions regarding the measurement prioritization 4 conclusion parameter estimation requires measurements of the system response however in some fields such as water quality modeling the measurement frequency is limited moreover the measurements are uncertain these limitations cause uncertainty in the parameter estimation and in the simulation results the objective of this study was to investigate the effect of the measurement frequency and uncertainty on the parameter estimation process and the uncertainty quantification to this aim the dream zs algorithm a state of the art mcmc algorithm is applied to estimate the parameter values and to infer the parameter uncertainty and the measurement uncertainty of the citowa wq model of the river zenne belgium using synthetic observations as calibration data finally the inferred posterior distributions of the parameters are compared with their true values to evaluate the performance of the parameter estimation and uncertainty quantification major conclusions include the following the results highlight the critical roles of measurement frequency and measurement uncertainty in identifying the posterior parameter distribution the effect of the measurement uncertainty on the performance of the parameter estimation is significant when the observed data points for the calibration are limited e g monthly data in conclusion the methodology proposed in this paper can be applied for the following purposes to support the measurement prioritization and resource allocation to estimate the minimum parameter uncertainty and model uncertainty due to the limited uncertain measurement to evaluate the effectiveness of extending the period of dataset on the calibration performance to evaluate the effect of combining different datasets on the calibration performance multi response calibration although our application focused on the conceptual wq model the presented methodology is entirely general and may be useful for dealing with complex models with a large number of parameters and multiple outputs software data availability the pawn method is implemented in the safe matlab octave toolbox for gsa pianosi et al 2015 safe is freely available for non commercial purposes at www bristol ac uk cabot resources safe toolbox the matlab toolbox of dream is available upon request from the author jasper uci edu the citowa tool is available upon request from the author befekadu woldegiorgis usask ca acknowledgment the authors would like to thank the flanders hydraulics research for supporting and coordinating the project of development of conceptual models for an integrated river basin management 
