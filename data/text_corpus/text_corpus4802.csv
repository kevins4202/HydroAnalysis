index,text
24010,in this study we examined the possible impacts of atmospheric model resolution and physics on simulating two types of el niño events four experiments were conducted using two different versions of the community atmosphere model i e cam4 and cam5 at two different resolutions compared with the cam4 experiments the cam5 experiments yielded longer periods for both types of el niño events while the period yielded by the cam5 experiments with a higher resolution was closest to the observations the cam5 experiments also had a larger anomaly amplitude of the thermocline depth during eastern pacific ep el niño events all four coupled experiments could accurately simulate the phase locking of the central pacific cp el niño events but not the peak phase of the ep el niño events specifically in the two cam4 experiments the peak phase appeared too early compared with the observations interestingly the coupled experiment with a lower resolution or cam5 could yield much stronger el niño and larger el niño southern oscillation enso skewness than the experiment with a higher resolution or cam4 the amplitude in the cam4 experiment with a higher resolution was the lowest but was closest to that observed heat budget analysis of each of the four coupled experiments demonstrated that the thermocline feedback in the coupled model was much larger than that observed for the ep el niño events there existed a large deviation in the simulated zonal advection terms specifically the zonal advection term of the anomalous temperature gradient found by averaging the mean current over the niño4 region was unrealistically negative during the developing phase of the cp el niño events and directly affected the simulation of the cp el niño events in the coupled experiments with a lower resolution 1 introduction the el niño southern oscillation enso is a phenomenon where the sea surface temperature anomaly ssta is anomalously warm or cold in the tropical eastern pacific every 2 7 years philander 1985 the enso has a major impact on the global climate being the culprit behind destructive droughts floods and storms around the world e g mcphaden et al 2006 indeje et al 2015 recent studies have revealed a new type of el niño event the difference between the two types of enso is usually defined by the location of the ssta which persists during the entire period of enso kug et al 2009 the maximum ssta of a conventional el niño event is located in the eastern tropical pacific which is generally called an eastern pacific ep el niño event when the maximum ssta is located in the central tropical pacific it is called a central pacific cp el niño event note that the nomenclature for this new type of el niño event is still inconsistent and it can also be called el niño modoki ashok et al 2007 warm pool el niño kug et al 2009 and date line el niño larkin and harrison 2005 although the terms are inconsistent these studies basically describe the same phenomenon the two types of el niño events have quite different effects on the climate system e g mcphaden 2002 weng et al 2007 graf and zanchettin 2012 kim et al 2009 lee et al 2010 ding et al 2011 wang et al 2013 capotondi et al 2015 in view of the important impact of the enso on the global climate system its simulation is a core issue in the modeling community therefore it is necessary to understand the current status of enso simulation duan et al 2014 found that the zebiak cane model can reproduce the ep el niño events but fails to reproduce the cp el niño events moreover many state of the art coupled atmosphere ocean general circulation models cgcms still have problems in simulating enso yu and kim 2010 assessed the simulation of the two types of el niño events by the coupled model intercomparison project phase 3 cmip3 models meehl et al 2007 they found that most of the models could accurately simulate the strong cp el niño events but that only a few of them could accurately simulate the strong ep el niño event in addition the period of the ep el niño event was linearly related to the zonal width of the tropical sst variability and the period of the cp el niño event was related to the location of the tropical ssta center ham and kug 2012 found that most of the ssta patterns in cmip3 did not distinguish between ep and cp el niño events that is there was still some correlation between them this may be due to the systematic errors of the models in simulating the climatological sst patterns in the tropical pacific ocean kim and yu 2012 also used cmip5 models to analyze the strengths of the two types of el niño events and compared them with the results of cmip3 they showed that nearly half of the cmip5 models could not accurately simulate the strong ep el niño and cp el niño events but that cmip5 could better simulate the spatial patterns of the two types of el niño and significantly reduce the difference in el niño intensity between the two patterns fang et al 2015 found that although the cmip5 models could well capture the two types of enso modes they have severe biases in simulating realistic cp el niño structures this is because few models can simulate significantly weaker warming anomalies in the eastern pacific to the central pacific many scholars have used observational data and numerical models to analyze the surface and subsurface characteristics of the two types of el niño events and their dynamic processes ashok et al 2007 kao and yu 2009 kug et al 2009 2010 capotondi 2013 hu et al 2016 kug et al 2009 described the characteristics and differences of the two types of el niño events in terms of ssta precipitation anomalies and zonal wind anomalies by analyzing the heat budget of the mixed layer they concluded that the thermocline feedback played a leading role in the ep el niño event while the zonal advection feedback played a leading role in the cp el niño event kug et al 2010 used the gfdl cm2 1 model to reveal that the ssta maximum areas of the two types of el niño events in the model were approximately 20 to the west of the observations the authors believed that the process of heat loss in the upper ocean discharge was more important in the ep el niño event but less effective in the cp el niño event capotondi 2013 used the community climate system model version 4 ccsm4 a coupled model to analyze the ep and cp el niño events and compared them with the results from the gfdl cm2 1 model in kug et al 2010 their analysis showed that the location of the development center of an el niño event was controlled by the nonlinear latitudinal advection term which was not the same as the result based on the gfdl cm2 1 model using a simple coupled model fang and zheng 2018 confirmed the dominant role played by zonal advective feedback in the development of cp el niño events additionally fang and mu 2018 extended the recharge oscillator paradigm to a three region conceptual model including the zonal advective feedback to better illustrate the cp el niño event in this study we investigated the simulation of the two types of el niño events using the community earth system model cesm among the models that are widely used in the climate modeling community the national center for atmospheric research ncar cesm and various versions of its predecessor the community climate system model ccsm have long played an important role the cesm is one of the most popular climate models used for integrating physical chemical and biological processes and is also essential for making predictions used by society and policy makers e g hurrell et al 2013 several studies have noted the impacts of model resolution and atmospheric physics on the simulation of various phenomena using the ccsm and cesm for example deser et al 2012 assessed the simulations of the enso by two experiments using the ccsm with different atmospheric physics one using the community atmosphere model version 4 cam4 and the other using cam3 they found that different atmospheric physics and resolutions had significant impacts on simulating enso phases i e el niño and la niña events capotondi 2013 analyzed the enso diversity in the ccsm4 climate model however these studies did not address different impacts of model configuration on the two types of el niño events in particular the authors used the old version of the cesm li et al 2016 and yao et al 2016 found that the improvement in resolution and the change to the parameterization scheme in cam5 were beneficial for simulating the madden julian oscillation mjo and indian ocean dipole iod respectively how will different atmospheric physics and resolutions affect the simulation of the two types of el niño events when using the cesm thus a careful comparison of results obtained from models with different atmospheric physics and resolutions is carried out in this study the remainder of this paper is structured as follows in section 2 we briefly introduce the model configuration experiments design and observational data used in section 3 we briefly describe the distinction between the two types of el niño events the results of sensitivity experiments using the cesm are presented in section 4 a diagnosis of the mixed layer heat budget is described in section 5 a summary and discussion follow in section 6 2 model experiments design and data we employ the cesm version 1 2 which is one of the state of the art climate models the model consists of atmosphere ocean land sea ice and land ice components interacting via a coupler vertenstein et al 2013 the atmospheric component is cam5 conley et al 2012 in the cesm while the old version has cam4 neale et al 2010 in the ccsm4 a previous version of the cesm cam5 can represent the atmospheric physical processes better than cam4 thanks to a range of improvements it includes a three mode modal aerosol scheme mam3 liu et al 2012 an updated radiation scheme iacono et al 2008 a new stratiform cloud microphysical scheme morrison and gettelman 2008 and a shallow convection scheme park and bretherton 2009 the number of vertical levels is 26 and 30 in cam4 and cam5 respectively more detailed information on the cesm can be found in hurrell et al 2013 to investigate the simulation of the two types of el niño using the fully coupled cesm two different horizontal resolutions lower 1 9 1 5 and higher 0 9 1 25 and two different cam versions cam4 and cam5 were combined to form four experiments table 1 hereafter exps cpl4 2 and cpl5 2 indicate the coupled cpl experiments using the lower resolution of cam4 and cam5 respectively and exps cpl4 1 and cpl5 1 indicate the coupled experiments using a higher horizontal resolution of cam4 and cam5 respectively the ocean component in these four experiments is the same namely 1 1 at the greenland pole grid all experiments ran for approximately 200 years using the built in initial conditions which could ensure that the coupled runs were adequately spun up the last 50 years of each run were used for analysis observational and reanalysis data sets were used to validate the simulation results the sst data were from the hadley centre sea ice and sea surface temperature hadisst data set on a 1 1 grid rayner et al 2003 the wind data were from the national centers for environmental prediction ncep reanalysis on a 2 5 1 5 grid kalnay et al 1996 the precipitation data were from the climate prediction center merged analysis of precipitation cmap from a 2 5 1 5 grid xie and arkin 1997 the ocean temperature and ocean circulation data were from the ncep global ocean data assimilation system godas on a 1 longitude and 1 3 latitude grid behringer and xue 2004 3 distinction between ep and cp el niño events the difference between the two types of el niño events the ep el niño and the cp el niño lies in the difference in the main distribution of the ssta the ep el niño is concentrated in the eastern equatorial pacific while the cp el niño is concentrated in the central equatorial pacific many scholars have different criteria for judging these two types of events kao and yu 2009 kug et al 2009 2010 yeh et al 2009 takahashi et al 2011 lian et al 2014 capotondi 2013 su et al 2014b for example all ep cp el niño events were selected when the mean value of the niño3 niño4 index from september to the following february was greater than the standard deviation of the index itself in kug et al 2009 and yeh et al 2009 however wittenberg 2009 found that the simulated pattern of tropical pacific ssta in the gfdl model was displaced by 20 to the west of the observed pattern thus kug et al 2010 defined two new indices niño3 m and niño4 m that were shifted longitudinally by 20 to the west of the classic niño3 and niño4 indices to distinguish the two types of el niño events in their model results in this study the ep cp el niño events are identified by the condition that the niño3 niño4 index during the peak phase djf is larger than 0 5 c and larger than the niño4 niño3 index this is similar to that used in capotondi 2013 to remove high frequency noise we first use a running mean of three months for each of the niño3 and niño4 indices based on the above definition we find that the numbers of the ep el niño events and the cp el niño events in the observations 1960 2009 are 8 and 10 respectively which are the same as in the observational study of kug et al 2009 4 simulation of ep el niño and cp el niño in cesm 4 1 air sea fields during the peak phase in this section we evaluate the impacts of atmospheric model resolution and atmospheric physics on the two types of el niño events as the first step we provide an overview of the air sea fields during the peak phase of these sensitivity experiments including both oceanic and atmospheric fields and compare model results with the observations for the oceanic fields the ssta and thermocline depth anomalies are two important indicators representing different types of el niño events the ssta patterns during d 0 j 1 f 1 for observations and coupled experiments are shown in fig 1 for the ep events the maximum ssta is located between 100 w and 150 w in the observations fig 1a the ssta distribution in the coupled experiments is basically the same as that observed but its location is somewhat to the west especially in exp cpl5 1 for the cp events the maximum ssta is located between 160 e and 150 w in the observations fig 1b the ssta distribution in the experiments is similar to that observed among them the location in exp cpl4 1 is closest to that observed while that in the other three experiments is approximately 20 to the west the maximum ssta intensity of ep and cp events in the experiments is also different from that observed for the ep events it is apparent in fig 1 that the ssta is strongest in exp cpl5 2 followed by cpl5 1 cpl4 2 obs and cpl4 1 their corresponding maximum ssta values are 4 8 3 0 2 6 2 5 and 2 2 c respectively the ssta intensity in exp cpl4 2 is closest to that observed it can be seen that the coupled experiments using cam5 exp cpl5 yield much stronger ssta than those experiments using cam4 exp cpl4 and the experiments with 2 resolution yield much stronger sstas than those experiments with 1 resolution however similar results cannot be found in the cp events all these results of exp cpl4 1 are similar to the findings of the old version of the cesm kug et al 2009 capotondi 2013 during typical enso processes the thermocline depth is used to represent the upper ocean warm water volume the tilting of the thermocline is associated with the enso cycle and can be explained by the recharge paradigm jin 1997 fig 2 shows the thermocline depth anomalies of the two types of el niño events during d 0 j 1 f 1 here 0 1 and 2 refer to the year during the first year after and the second year after the developing phase of the el niño event respectively the depth of the 15 c isotherm is defined as the thermocline depth as in deser et al 2012 and capotondi 2013 the thermocline depth anomalies associated with the ep el niño exhibit a dipole pattern positive in the eastern and negative in the western pacific in the observations and coupled experiments fig 2 left the thermocline depth anomalies of ep and cp events in the experiments are also different from those observed for the ep events the positive thermocline depth anomalies are the strongest in exp cpl5 2 followed by cpl5 1 obs cpl4 1 and cpl4 2 with maximum depth anomalies of 94 82 76 61 and 58 m respectively the negative thermocline anomalies are strongest in exp cpl5 2 followed by cpl5 1 cpl4 2 obs and cpl4 1 with corresponding anomalies of 62 56 48 37 and 32 m respectively the dipole pattern also appears in the cp el niño event in the observations however the amplitude is much weaker than that in the ep events in the coupled experiments both exps cpl4 1 and cpl5 1 can capture the dipole pattern however exps cpl4 2 and cpl5 2 fail to the impacts of different resolutions on the distribution and intensity of thermocline depth anomalies are pronounced for the cp el niño events it seems that the recharge discharge thermocline processes do not play the main role for the cp el niño events in the observation and coupled experiments which was also reported in capotondi 2013 next we compare model atmospheric fields to the observations the precipitation composites of the two types of el niño events during d 0 j 1 f 1 are shown in fig 3 similar to the ssta pattern the precipitation anomaly patterns are distinctively different for different types of el niño in the observations precipitation anomalies associated with the ep el niño almost occupy the entire equatorial pacific which has an anomaly center near 170 w whereas the precipitation anomalies associated with the cp el niño are located in the western pacific with a center at 165 e the location of large precipitation anomalies is consistent with the location of the strong zonal gradient of the ssta kug et al 2010 these results are similar to the findings of kug et al 2009 2010 in each experiment the precipitation anomaly distribution is similar to the observations for the ep events the center of anomalies is approximately 10 and 20 farther to the east in cpl5 2 and cpl4 1 respectively the anomaly is strongest in the obs followed by exp cpl5 2 cpl5 1 cpl4 1 and cpl4 2 their corresponding maximum precipitation anomalies are 10 3 9 1 9 0 8 6 and 8 1 mm day 1 respectively for the cp events the center of anomalies is approximately 10 farther to the west in exp cpl4 2 and cpl5 2 and approximately 20 farther to the east in exp cpl4 1 the anomaly is strongest in cpl4 1 followed by exp cpl4 2 cpl5 1 cpl5 2 and obs their corresponding maximum precipitation anomalies are 8 2 6 5 6 1 5 3 and 5 1 mm day 1 respectively it can be seen that the precipitation anomalies are stronger in exp cpl5 than in exp cpl4 for the ep el niño events whereas the opposite is true for the cp el niño events the impacts of different resolutions on precipitation strength are not pronounced for both types of el niño events the precipitation anomaly intensity in exp cpl5 2 is closest to that of the observed anomalies for both types of el niño events fig 4 shows the zonal wind anomaly composites of the two types of el niño events during d 0 j 1 f 1 in the observations fig 4a the zonal wind anomalies associated with the ep el niño display a pattern of positive values which extends from the western pacific to the eastern pacific and is mainly located in the southern hemisphere the ep events in the coupled experiments show a similar zonal pattern whereas the strength is different from that observed the anomalies show a southeastward extension east of 130 e and are stronger than those observed the wind anomalies are the strongest in exp cpl5 2 followed by cpl5 1 obs cpl4 2 and cpl4 1 their corresponding maximum wind anomalies are 8 35 4 84 4 56 4 42 and 4 37 m s 1 respectively it can be seen that the experiments with cam5 or lower resolution cause larger zonal wind anomalies than the experiments with cam4 or higher resolution the zonal wind anomaly intensity in exp cpl4 2 is closest to that of the observed anomalies in the experiments with cam5 the results from exp cpl5 1 with higher resolution are closer to those observed than those from exp cpl5 2 with lower resolution the opposite occurs in the experiments with cam4 in the cp cases the positive anomalies are located farther to the east in the experiments the relationships between the experiments and the observations are not obvious 4 2 evolutions of the two types of el niño events to identify different evolutions of the two types of el niño events composite evolutions of the equatorial ssta as a function of longitude from observations and experiments are shown in fig 5 in the observations the ssta of the ep el niño fig 5a begins to develop in spring reaches its maximum during winter and then changes to negative anomalies the ssta center is at 120 w in the observations all experimental results reflect the ssta evolution however it is apparent that the intensity of the ssta is the strongest in exp cpl5 2 followed by cpl5 1 cpl4 2 obs and cpl4 1 which is the same as in fig 1 their corresponding maximum ssta values are 4 8 3 0 2 6 2 5 and 2 2 c respectively the ssta intensity in exp cpl4 2 is closest to those observed compared with the ep events the cp events show a similar but weaker evolution fig 5b and the maximum center is farther to the west being centered at 170 w the ssta in the eastern pacific becomes negative at the beginning of year 1 which is earlier than that in the ep events the experiments can capture the maximum values during winter however positive anomalies still exist throughout the following year which is not realistic these unrealistic features in the experiments can also be found in the results of kug et al 2010 and capotondi 2013 the enso cycle is associated with the recharge discharge of warm water to from the equatorial upper mixed layer according to the recharge oscillation dynamic paradigm jin 1997 at the peak of an el niño event weakening easterly wind leads to deeper thermocline in the eastern equatorial pacific and shallower thermocline in the western equatorial pacific then a discharge of warm water from the equator to higher latitudes causes a shallower thermocline depth to examine the heat exchange between the equatorial and off equatorial regions the time latitude sections of the zonal mean 140 e 80 w thermocline depth anomalies from observations and experiments are shown in fig 6 in the observations the equatorial thermocline depth anomalies for the ep el niño are positive beginning from january of year 0 reach a maximum around september and then become negative after march of year 1 at the same time the anomalies become positive north of the equator while the anomalies south of the equator are negative these results are similar to the findings of kug et al 2009 2010 and capotondi 2013 the process is associated with the recharge discharge theory jin 1997 the evolution of the thermocline depth anomalies in all experiments is similar to those observed except for the amplitude compared with the ep el niño events the thermocline depth anomalies in the cp el niño events are different in the observations the anomalies along the equator are positive from january of year 0 and then become negative from march of year 1 whereas the anomalies north of 10 n are almost negative from year 0 to the following year however these features cannot be well captured by the models the thermocline depth anomalies in the cp el niño of exps cpl4 1 and cpl5 1 are similar to those in the ep el niño notably the thermocline depth anomalies associated with the ep el niño events fig 6g develop in jan 0 and maximize in oct 0 along the equator in exp cpl4 1 the anomalies in cp el niño events show a similar but weaker evolution fig 6h the evolution and intensity of anomalies of exp cpl4 1 for both types of el niño events are similar to the findings of capotondi 2013 4 3 enso period the enso has a broadband period of 2 7 years philander 1985 the power spectra of the niño3 and niño4 indices can be used to represent the period and amplitude of ep and cp el niño events respectively as shown in fig 7 for the ep el niño the observational data show a period of approximately 3 8 years fig 7a all experimental results capture the period well lying within a range of 3 5 years among the four experiments the best simulation of the period is seen in exp cpl5 1 the periods in both exps cpl5 2 and cpl4 2 are approximately 4 2 years being longer than the observed ones and the period in exp cpl4 1 is approximately 3 3 years which is shorter than the observed counterpart an interesting feature in fig 7a is that the period is strongly dependent on the atmospheric model resolution i e the period in the 2 resolution model is longer than that in the 1 resolution regardless of the physics of cam4 or cam5 however the impacts of different atmosphere physics on the period are inconsistent when the resolution is 2 the periods for cam4 and cam5 are similar i e the periods of exps cpl4 2 and cpl4 2 are similar the period of cam5 is longer than that of cam4 and it is closer to the observations when the resolution is 1 fig 7b shows the power spectra of niño4 from observations and model experiments similar to the ep el niño the periods of observations and experiments are all between three and five years the period of the cp el niño is sensitive to the horizontal resolution and atmospheric physics either low resolution or cam5 atmospheric physics can make the period larger for both types of el niño events among the experiments exp cpl5 1 simulates the period best comparison of the power spectra of both types of el niño events shows that the atmospheric physics and resolution have significant influences on the strength of el niño the maximum power is approximately twice as large in the exp cpl5 compared with exp cpl4 for both 1 and 2 resolutions by contrast the 2 resolution models yield maximum power 1 5 times as high as the 1 resolution models for both exps cpl4 and cpl5 for both types of events the exp cpl4 1 simulates the strength best 4 4 phase locking an important feature of the el niño events is their phase locking behavior i e their peak always occurs in winter fig 8 shows the standard deviations of the niño3 and niño4 indices for observations and experiments for the ep el niño events fig 8a the standard deviations of the exps cpl5 2 and cpl5 1 peak in winter concentrated in november and december respectively the exps cpl4 2 and cpl4 1 peak in october and september respectively these results indicate that the exp cpl5 can reproduce the phase locking feature however the exp cpl4 fail to do so in contrast all experiments can capture the phase locking feature for the cp el niño events fig 8b note that most of the amplitudes of the two types of el niño events are stronger in the models than in the observations except for exp cpl4 1 which is slightly weaker but closest to the observations the amplitude yielded by the exp cpl5 is stronger than that yielded by the exp cpl4 and the lower resolution experiments yield stronger amplitudes than do the higher resolution experiments note that many other models including these in cmip3 and cmip5 also have problems in simulating realistic enso phase locking features ham and kug 2014 it was found that the simulation of the phase locking has a strong relation to the simulation of the climatological zonal sst gradient in the tropical pacific ham et al 2013 ham and kug 2014 we have investigated the iod in a similar framework as that used here which was published previously yao et al 2016 some interesting and similar results are found when comparing the impact of atmospheric physics and model resolution on the simulation of el niño and the iod for example the strengths of the two types of el niño and iod events in the model simulations are almost larger than those observed and the strength yielded by the experiments with cam5 or 2 is larger than that by the experiments with cam4 or 1 the periods of the two types of el niño and iod events in the 2 resolution model are always longer than those in the 1 resolution regardless of the physics of cam4 or cam5 the phase locking feature of the ep el niño and iod events in the experiments with cam5 is reproduced better than that in the experiments with cam4 the consistent impact of these model configurations on enso and iod suggests that some common mechanisms characterized in these simulations might dominate both variability modes for example equatorial wave dynamics and bjerknes positive mechanisms 4 5 enso skewness el niño events are stronger than la niña events this intrinsic nonlinear characteristic is called the skewness of the enso i e asymmetry of the enso an and jin 2004 to measure the nonlinearity of the enso cai et al 2018 introduced a parameter α characterizing the nonlinearity of the relationship between ep and cp events and their skewness the nonlinearity is determined by fitting the monthly data with quadratic function pc 2 t α p c 1 t 2 β p c 1 t γ where pc1 and pc2 are the first two principal components of an empirical orthogonal function eof analysis of the monthly ssta greater α produces larger positive skewness in ep enso events and larger negative skewness in cp enso events positive negative skewness indicates that the ep enso cp enso tends to appear more often as strong el niño la niña events than as strong la niña el niño events these nonlinear relationships for the observation and coupled experiments are shown in fig 9 the value of α is 0 37 in the observation close to the value 0 31 in cai et al 2018 the value of α is the largest in exp cpl5 2 followed by cpl4 2 cpl5 1 and cpl4 1 with 0 57 0 46 0 38 and 0 12 respectively the value in exp cpl5 1 is closest to that observed it can be seen that the experiments with cam5 or lower resolution cause larger skewness than the experiments with cam4 or higher resolution the above analysis suggests that the dominant dynamical processes may be different in the two types of el niño events which is examined in the next section 5 diagnosis of mixed layer heat budget 5 1 heat budget analysis different types of el niño events involve different dynamic processes to identify the leading dynamics the mixed layer heat budgets for the two types of el niño were analyzed e g kug et al 2009 2010 capotondi 2013 ren and jin 2013 fang and zheng 2018 in these published studies the authors considered only the advection terms and did not analyze the role of the thermal forcing term in addition to the advection terms how important will the thermal forcing be the term thermal forcing is included here the heat budget equation is as follows 1 t t u t x u t x u t x v t y v t y v t y w t z w t z w t z q n e t ρ c p h r where the overbar and prime indicate monthly climatology and anomaly respectively the variables u v and t indicate zonal current meridional current and oceanic temperature averages in the mixed layer respectively variable w indicates vertical velocity at the bottom of the mixed layer q n e t ρ and c p indicate net surface heat flux density of water and specific heat of sea water respectively h indicates the climatological mixed layer depth and h 50 m variable r indicates the residual term and is not considered in this study q n e t can be determined using the equation below 2 q n e t q s h o r t q l o n g q l a t e n t q s e n s i b l e where q s h o r t q l o n g q l a t e n t and q s e n s i b l e indicate net shortwave heat flux net longwave heat flux latent heat flux and sensible heat flux respectively fig 10a shows the composite of each mixed layer heat budget term for the ep el niño in the niño3 region for the observations the total advection seems to capture the observed tendency of the niño3 sst the u t x v t y and w t z terms are the most important terms for the development and decay of ep events the results are similar to the findings in kug et al 2009 the heat flux terms hinder the development and decay of ep events however adding the contribution of the heat flux terms helps the sum of the total advection and heat flux terms better explain the observed ssta tendency fig 10b shows the composite of each mixed layer heat budget term for the cp el niño in the niño4 region for the observations the total advection term seems to capture the variation pattern of the niño4 sst in the observations the u t x and u t x terms are the most important terms for the development and decay of cp events the results are similar to the findings in kug et al 2009 however the heat flux terms play significantly negative roles during the development and decay of cp events and the sum of the total advection and heat flux terms is almost negative during the whole process of a cp event the composites of the mixed layer heat budget terms for the ep el niño in the niño3 region for the coupled experiments are shown in fig 11 the tendency of the ssta in each experiment is consistent with that of the observations fig 10a the total advection term in the experiments reaches the maximum positive value in july but the time to reach the minimum value also lags behind the change in the ssta the sum of the advection and heat flux terms is consistent with the tendency of the ssta especially the periods of warming and cooling the correlations of the sum and tendency of the ssta are 0 99 0 97 0 98 and 0 94 in exp cpl4 2 cpl5 2 cpl4 1 and cpl5 1 respectively the linear terms u t x v t y w t z and w t z play leading roles and promote the development of ep events the effect of the nonlinear terms should be small however the terms u t x and w t z are too large in exp cpl5 2 the term denoting the mean upwelling and anomalous temperature gradient w t z also called the thermocline feedback an and jin 2001 is too large in all four experiments which is not consistent with the observations fig 12 shows the composites of the mixed layer heat budget terms for the cp el niño in the niño4 region for the coupled experiments in all four experiments the periods of warming and cooling of the ssta are similar to those of the observations however the simulation of the sum of the advection and heat flux terms is not good especially in exps cpl4 2 and cpl5 2 in which the tendencies are even opposite in exps cpl4 1 and cpl5 1 the sum is good in the developing phase but not during the decaying phase the nonlinear term u t x is too large in all four experiments the correlations of the sum term and tendency of the ssta are 0 20 0 39 0 75 and 0 88 in exp cpl4 2 cpl5 2 cpl4 1 and cpl5 1 respectively it seems that a higher resolution benefits the simulation of the advection and heat flux terms for the cp events therefore the effect of the model resolution on the physical process is significant while the effect of atmospheric physics is not serious ideally the correlation between the temperature tendency and the sum of all other terms should be one however in reality the correlations are always less than one for the ep and cp el niño events in the model simulations figs 11 and 12 here we define the imbalance as the difference between the ssta tendency and the sum of all other terms that is the difference between the thick solid and dashed black lines in figs 11 and 12 many factors can contribute to the imbalance including the estimates of the mixed layer depth temporal resolution of the model output disregard of the eddy heat flux diffusion and nonlinear higher order terms for example the mixed layer depth is defined as a fixed depth 50 m for simplicity here which is not very realistic e g dong et al 2007 in addition the monthly average of the model output is used in the study probably resulting in oversmoothing of the distribution of the mixed layer depth neglecting the eddy heat flux especially fluxes associated with mesoscale eddies is probably another reason for the imbalance schiller and ridgway 2013 other factors such as nonlinear higher order terms are also neglected during calculation of the formula the heat budget analysis in the cp el niño events is not as good as in the ep el niño events suggesting that these concerns are more serious in the cp el niño than in the ep el niño one possible reason is that the inaccurate estimate of the items related to horizontal advections which are actually dominated by advection can cause more imbalance to the cp el niño 5 2 dynamic analysis of cp events during the developing phase a significant bias evident in these experiments lies in their simulation of the mixed layer heat budget for different types of el niño events as indicated by the heat budget analysis the simulations of advection and heat flux terms for the cp events are not good particularly in exps cpl4 2 and cpl5 2 during the developing phase therefore we focused on the developing phase of the cp el niño events and analyzed the possible causes for the poor simulation results of the lower resolution experiments we defined the period from june to november as the developing phase su et al 2014b and analyzed the variation in the ssta in the mixed layer for the cp events during the developing phase fig 13 shows the temperature tendency 3d temperature advection surface flux heating and the sum of the advection and surface heat fluxes during the developing phase of the cp events the advection term promotes the development of the cp events and its amplitude is larger than the temperature tendency term this result is consistent with the finding in kug et al 2009 however kug et al 2009 only considered the effect of 3d temperature advection and neglected the important role of surface heat fluxes in reality the total heat flux terms play a negative role in the observations hindering the development of cp events the simulation results are not good especially the simulation of advection in exps cpl4 2 and cpl5 2 the simulations of advection in exps cpl4 1 and cpl5 1 are similar to the observations therefore increasing the resolution is helpful to improve the simulated effect of the advection term in the developing phase of cp events we further analyzed the relative sizes of all the 3d temperature advection terms in the developing phase of the cp events fig 14 it can be seen that exps cpl4 2 and cpl5 2 are not well simulated for the cp events mainly due to a large deviation in the simulation of the zonal advection term in addition the negative anomaly in the zonal advection term is contrary to that of the observations which causes the sum of all advection terms to be negative fig 15 shows a synthetic diagram of the mean zonal flow eastward positive and temperature anomalies in the niño4 region of the cp el niño events in the observations the mean zonal current u is reversed near the equator it is positive east of 165 w and negative in the west fig 15a however the center of the temperature anomalies is near 165 w fig 15b therefore the zonal gradient of the temperature anomalies t x is approximately positive negative west east of 165 w making the zonal advection term of the anomalous temperature gradient found by averaging the mean current u t x over the niño4 region positive compared with the observations the coupled experiments show almost negative i e westward mean zonal currents in the niño4 region the temperature anomalies are centered at 175 e in both exps cpl4 2 and cpl5 2 being more to the west than those observed thus the term t x is positive negative which makes the term u t x positive negative west east of 175 e the negative areas are larger than the positive areas leading to the average value of the entire niño4 region being negative in both exps cpl4 2 and cpl5 2 however the temperature anomalies are centered at 175 w in exps cpl4 1 and cpl5 1 which makes the area where the term t x is negative in the experiments with higher resolution much smaller than that in the experiments with lower resolution therefore the average value of u t x in the niño4 region is positive finally it can be seen that different resolutions result in different ssta centers in the niño4 region the anomaly centers of the lower resolution experiments are more to the west while the anomaly centers of the higher resolution experiments are closer to those observed then the zonal advection term of the anomalous temperature gradient found on the basis of the mean current u t x simulated in the higher resolution experiments is better than that in the lower resolution experiments which causes poor simulation of the advection and heat flux terms for the cp events in both exps cpl4 2 and cpl5 2 during the developing phase 6 summary and discussion in this study we presented a detailed assessment of the simulation of two types of el niño using the latest version of the cesm emphasis was placed on investigating the impacts of different atmospheric physics and resolutions on simulating the two types of el niño events for this purpose two coupled experiments using low and high horizontal resolutions of cam4 exps cpl4 2 and cpl4 1 were conducted in addition because the new version of the cesm includes the updated cam5 atmospheric physic model two additional experiments using different resolutions of cam5 namely exps cpl5 2 and cpl5 1 were performed the main results are summarized below 1 each experiment realistically reproduces the ssta and thermocline depth anomalies of the ep events however the simulation of the cp events is not satisfactory cam5 leads to much larger thermocline depth anomalies of the ep events 2 the intensity of a cp event is much smaller than that of an ep event either low resolution or cam5 atmospheric physics can make the intensity larger and the period longer for both types of el niño events cam5 gives unrealistically stronger intensity compared with that of cam4 3 the observed phase locking of the ep events is not reproduced in exp cpl4 the horizontal resolution has little effect on the simulation of the phase locking of the two types of el niño events 4 either low resolution or cam5 atmospheric physics can cause larger enso skewness 5 the simulation of thermocline feedback in the coupled experiments is much larger than that in the observations for the ep events 6 better simulated zonal advection of the anomalous temperature gradient by the mean current benefited the simulation of cp events the two types of el niño events correspond to different dynamic processes to investigate this difference we analyzed the dynamic advection and heat flux during the development and decay of ep and cp events we found that the dynamic processes of the ep events are basically the same as those observed that is the anomalies of zonal advection and vertical advection lead to ep events however for the cp events the advection term in the low resolution experiments prevents sst warming and the heat equation is dominated by the remainder of the heat balance namely diffusion and dissipation these results are opposite to those observed and those found in the high resolution experiments therefore although the two types of el niño events are found in the experiments with low resolution the dynamics of the cp events are wrong studies of the two types of el niño events have been described in a large number of studies over the past few years this work aims to investigate several possible factors that may impact the simulation of the two types of el niño events through sensitivity experiments using the cesm which serves as a necessary step for improving el niño prediction additional studies with more sensitivity experiments as well as diagnostic analyses are needed to further elucidate the dominant physical and dynamic processes of the two types of el niño events acknowledgments the observational data sets used in this study are from http apdrc soest hawaii edu data data php this research was supported by the national key r d program of china 2017yfa0604202 the china ocean mineral resources r d association dy135 e2 3 02 the national science foundation of china 41690124 41705050 41806032 and 41621064 and the national programme on global change and air sea interaction gasi ipovai 06 yt is also supported by the discovery grant of the natural sciences and engineering research council of canada nserc 
24010,in this study we examined the possible impacts of atmospheric model resolution and physics on simulating two types of el niño events four experiments were conducted using two different versions of the community atmosphere model i e cam4 and cam5 at two different resolutions compared with the cam4 experiments the cam5 experiments yielded longer periods for both types of el niño events while the period yielded by the cam5 experiments with a higher resolution was closest to the observations the cam5 experiments also had a larger anomaly amplitude of the thermocline depth during eastern pacific ep el niño events all four coupled experiments could accurately simulate the phase locking of the central pacific cp el niño events but not the peak phase of the ep el niño events specifically in the two cam4 experiments the peak phase appeared too early compared with the observations interestingly the coupled experiment with a lower resolution or cam5 could yield much stronger el niño and larger el niño southern oscillation enso skewness than the experiment with a higher resolution or cam4 the amplitude in the cam4 experiment with a higher resolution was the lowest but was closest to that observed heat budget analysis of each of the four coupled experiments demonstrated that the thermocline feedback in the coupled model was much larger than that observed for the ep el niño events there existed a large deviation in the simulated zonal advection terms specifically the zonal advection term of the anomalous temperature gradient found by averaging the mean current over the niño4 region was unrealistically negative during the developing phase of the cp el niño events and directly affected the simulation of the cp el niño events in the coupled experiments with a lower resolution 1 introduction the el niño southern oscillation enso is a phenomenon where the sea surface temperature anomaly ssta is anomalously warm or cold in the tropical eastern pacific every 2 7 years philander 1985 the enso has a major impact on the global climate being the culprit behind destructive droughts floods and storms around the world e g mcphaden et al 2006 indeje et al 2015 recent studies have revealed a new type of el niño event the difference between the two types of enso is usually defined by the location of the ssta which persists during the entire period of enso kug et al 2009 the maximum ssta of a conventional el niño event is located in the eastern tropical pacific which is generally called an eastern pacific ep el niño event when the maximum ssta is located in the central tropical pacific it is called a central pacific cp el niño event note that the nomenclature for this new type of el niño event is still inconsistent and it can also be called el niño modoki ashok et al 2007 warm pool el niño kug et al 2009 and date line el niño larkin and harrison 2005 although the terms are inconsistent these studies basically describe the same phenomenon the two types of el niño events have quite different effects on the climate system e g mcphaden 2002 weng et al 2007 graf and zanchettin 2012 kim et al 2009 lee et al 2010 ding et al 2011 wang et al 2013 capotondi et al 2015 in view of the important impact of the enso on the global climate system its simulation is a core issue in the modeling community therefore it is necessary to understand the current status of enso simulation duan et al 2014 found that the zebiak cane model can reproduce the ep el niño events but fails to reproduce the cp el niño events moreover many state of the art coupled atmosphere ocean general circulation models cgcms still have problems in simulating enso yu and kim 2010 assessed the simulation of the two types of el niño events by the coupled model intercomparison project phase 3 cmip3 models meehl et al 2007 they found that most of the models could accurately simulate the strong cp el niño events but that only a few of them could accurately simulate the strong ep el niño event in addition the period of the ep el niño event was linearly related to the zonal width of the tropical sst variability and the period of the cp el niño event was related to the location of the tropical ssta center ham and kug 2012 found that most of the ssta patterns in cmip3 did not distinguish between ep and cp el niño events that is there was still some correlation between them this may be due to the systematic errors of the models in simulating the climatological sst patterns in the tropical pacific ocean kim and yu 2012 also used cmip5 models to analyze the strengths of the two types of el niño events and compared them with the results of cmip3 they showed that nearly half of the cmip5 models could not accurately simulate the strong ep el niño and cp el niño events but that cmip5 could better simulate the spatial patterns of the two types of el niño and significantly reduce the difference in el niño intensity between the two patterns fang et al 2015 found that although the cmip5 models could well capture the two types of enso modes they have severe biases in simulating realistic cp el niño structures this is because few models can simulate significantly weaker warming anomalies in the eastern pacific to the central pacific many scholars have used observational data and numerical models to analyze the surface and subsurface characteristics of the two types of el niño events and their dynamic processes ashok et al 2007 kao and yu 2009 kug et al 2009 2010 capotondi 2013 hu et al 2016 kug et al 2009 described the characteristics and differences of the two types of el niño events in terms of ssta precipitation anomalies and zonal wind anomalies by analyzing the heat budget of the mixed layer they concluded that the thermocline feedback played a leading role in the ep el niño event while the zonal advection feedback played a leading role in the cp el niño event kug et al 2010 used the gfdl cm2 1 model to reveal that the ssta maximum areas of the two types of el niño events in the model were approximately 20 to the west of the observations the authors believed that the process of heat loss in the upper ocean discharge was more important in the ep el niño event but less effective in the cp el niño event capotondi 2013 used the community climate system model version 4 ccsm4 a coupled model to analyze the ep and cp el niño events and compared them with the results from the gfdl cm2 1 model in kug et al 2010 their analysis showed that the location of the development center of an el niño event was controlled by the nonlinear latitudinal advection term which was not the same as the result based on the gfdl cm2 1 model using a simple coupled model fang and zheng 2018 confirmed the dominant role played by zonal advective feedback in the development of cp el niño events additionally fang and mu 2018 extended the recharge oscillator paradigm to a three region conceptual model including the zonal advective feedback to better illustrate the cp el niño event in this study we investigated the simulation of the two types of el niño events using the community earth system model cesm among the models that are widely used in the climate modeling community the national center for atmospheric research ncar cesm and various versions of its predecessor the community climate system model ccsm have long played an important role the cesm is one of the most popular climate models used for integrating physical chemical and biological processes and is also essential for making predictions used by society and policy makers e g hurrell et al 2013 several studies have noted the impacts of model resolution and atmospheric physics on the simulation of various phenomena using the ccsm and cesm for example deser et al 2012 assessed the simulations of the enso by two experiments using the ccsm with different atmospheric physics one using the community atmosphere model version 4 cam4 and the other using cam3 they found that different atmospheric physics and resolutions had significant impacts on simulating enso phases i e el niño and la niña events capotondi 2013 analyzed the enso diversity in the ccsm4 climate model however these studies did not address different impacts of model configuration on the two types of el niño events in particular the authors used the old version of the cesm li et al 2016 and yao et al 2016 found that the improvement in resolution and the change to the parameterization scheme in cam5 were beneficial for simulating the madden julian oscillation mjo and indian ocean dipole iod respectively how will different atmospheric physics and resolutions affect the simulation of the two types of el niño events when using the cesm thus a careful comparison of results obtained from models with different atmospheric physics and resolutions is carried out in this study the remainder of this paper is structured as follows in section 2 we briefly introduce the model configuration experiments design and observational data used in section 3 we briefly describe the distinction between the two types of el niño events the results of sensitivity experiments using the cesm are presented in section 4 a diagnosis of the mixed layer heat budget is described in section 5 a summary and discussion follow in section 6 2 model experiments design and data we employ the cesm version 1 2 which is one of the state of the art climate models the model consists of atmosphere ocean land sea ice and land ice components interacting via a coupler vertenstein et al 2013 the atmospheric component is cam5 conley et al 2012 in the cesm while the old version has cam4 neale et al 2010 in the ccsm4 a previous version of the cesm cam5 can represent the atmospheric physical processes better than cam4 thanks to a range of improvements it includes a three mode modal aerosol scheme mam3 liu et al 2012 an updated radiation scheme iacono et al 2008 a new stratiform cloud microphysical scheme morrison and gettelman 2008 and a shallow convection scheme park and bretherton 2009 the number of vertical levels is 26 and 30 in cam4 and cam5 respectively more detailed information on the cesm can be found in hurrell et al 2013 to investigate the simulation of the two types of el niño using the fully coupled cesm two different horizontal resolutions lower 1 9 1 5 and higher 0 9 1 25 and two different cam versions cam4 and cam5 were combined to form four experiments table 1 hereafter exps cpl4 2 and cpl5 2 indicate the coupled cpl experiments using the lower resolution of cam4 and cam5 respectively and exps cpl4 1 and cpl5 1 indicate the coupled experiments using a higher horizontal resolution of cam4 and cam5 respectively the ocean component in these four experiments is the same namely 1 1 at the greenland pole grid all experiments ran for approximately 200 years using the built in initial conditions which could ensure that the coupled runs were adequately spun up the last 50 years of each run were used for analysis observational and reanalysis data sets were used to validate the simulation results the sst data were from the hadley centre sea ice and sea surface temperature hadisst data set on a 1 1 grid rayner et al 2003 the wind data were from the national centers for environmental prediction ncep reanalysis on a 2 5 1 5 grid kalnay et al 1996 the precipitation data were from the climate prediction center merged analysis of precipitation cmap from a 2 5 1 5 grid xie and arkin 1997 the ocean temperature and ocean circulation data were from the ncep global ocean data assimilation system godas on a 1 longitude and 1 3 latitude grid behringer and xue 2004 3 distinction between ep and cp el niño events the difference between the two types of el niño events the ep el niño and the cp el niño lies in the difference in the main distribution of the ssta the ep el niño is concentrated in the eastern equatorial pacific while the cp el niño is concentrated in the central equatorial pacific many scholars have different criteria for judging these two types of events kao and yu 2009 kug et al 2009 2010 yeh et al 2009 takahashi et al 2011 lian et al 2014 capotondi 2013 su et al 2014b for example all ep cp el niño events were selected when the mean value of the niño3 niño4 index from september to the following february was greater than the standard deviation of the index itself in kug et al 2009 and yeh et al 2009 however wittenberg 2009 found that the simulated pattern of tropical pacific ssta in the gfdl model was displaced by 20 to the west of the observed pattern thus kug et al 2010 defined two new indices niño3 m and niño4 m that were shifted longitudinally by 20 to the west of the classic niño3 and niño4 indices to distinguish the two types of el niño events in their model results in this study the ep cp el niño events are identified by the condition that the niño3 niño4 index during the peak phase djf is larger than 0 5 c and larger than the niño4 niño3 index this is similar to that used in capotondi 2013 to remove high frequency noise we first use a running mean of three months for each of the niño3 and niño4 indices based on the above definition we find that the numbers of the ep el niño events and the cp el niño events in the observations 1960 2009 are 8 and 10 respectively which are the same as in the observational study of kug et al 2009 4 simulation of ep el niño and cp el niño in cesm 4 1 air sea fields during the peak phase in this section we evaluate the impacts of atmospheric model resolution and atmospheric physics on the two types of el niño events as the first step we provide an overview of the air sea fields during the peak phase of these sensitivity experiments including both oceanic and atmospheric fields and compare model results with the observations for the oceanic fields the ssta and thermocline depth anomalies are two important indicators representing different types of el niño events the ssta patterns during d 0 j 1 f 1 for observations and coupled experiments are shown in fig 1 for the ep events the maximum ssta is located between 100 w and 150 w in the observations fig 1a the ssta distribution in the coupled experiments is basically the same as that observed but its location is somewhat to the west especially in exp cpl5 1 for the cp events the maximum ssta is located between 160 e and 150 w in the observations fig 1b the ssta distribution in the experiments is similar to that observed among them the location in exp cpl4 1 is closest to that observed while that in the other three experiments is approximately 20 to the west the maximum ssta intensity of ep and cp events in the experiments is also different from that observed for the ep events it is apparent in fig 1 that the ssta is strongest in exp cpl5 2 followed by cpl5 1 cpl4 2 obs and cpl4 1 their corresponding maximum ssta values are 4 8 3 0 2 6 2 5 and 2 2 c respectively the ssta intensity in exp cpl4 2 is closest to that observed it can be seen that the coupled experiments using cam5 exp cpl5 yield much stronger ssta than those experiments using cam4 exp cpl4 and the experiments with 2 resolution yield much stronger sstas than those experiments with 1 resolution however similar results cannot be found in the cp events all these results of exp cpl4 1 are similar to the findings of the old version of the cesm kug et al 2009 capotondi 2013 during typical enso processes the thermocline depth is used to represent the upper ocean warm water volume the tilting of the thermocline is associated with the enso cycle and can be explained by the recharge paradigm jin 1997 fig 2 shows the thermocline depth anomalies of the two types of el niño events during d 0 j 1 f 1 here 0 1 and 2 refer to the year during the first year after and the second year after the developing phase of the el niño event respectively the depth of the 15 c isotherm is defined as the thermocline depth as in deser et al 2012 and capotondi 2013 the thermocline depth anomalies associated with the ep el niño exhibit a dipole pattern positive in the eastern and negative in the western pacific in the observations and coupled experiments fig 2 left the thermocline depth anomalies of ep and cp events in the experiments are also different from those observed for the ep events the positive thermocline depth anomalies are the strongest in exp cpl5 2 followed by cpl5 1 obs cpl4 1 and cpl4 2 with maximum depth anomalies of 94 82 76 61 and 58 m respectively the negative thermocline anomalies are strongest in exp cpl5 2 followed by cpl5 1 cpl4 2 obs and cpl4 1 with corresponding anomalies of 62 56 48 37 and 32 m respectively the dipole pattern also appears in the cp el niño event in the observations however the amplitude is much weaker than that in the ep events in the coupled experiments both exps cpl4 1 and cpl5 1 can capture the dipole pattern however exps cpl4 2 and cpl5 2 fail to the impacts of different resolutions on the distribution and intensity of thermocline depth anomalies are pronounced for the cp el niño events it seems that the recharge discharge thermocline processes do not play the main role for the cp el niño events in the observation and coupled experiments which was also reported in capotondi 2013 next we compare model atmospheric fields to the observations the precipitation composites of the two types of el niño events during d 0 j 1 f 1 are shown in fig 3 similar to the ssta pattern the precipitation anomaly patterns are distinctively different for different types of el niño in the observations precipitation anomalies associated with the ep el niño almost occupy the entire equatorial pacific which has an anomaly center near 170 w whereas the precipitation anomalies associated with the cp el niño are located in the western pacific with a center at 165 e the location of large precipitation anomalies is consistent with the location of the strong zonal gradient of the ssta kug et al 2010 these results are similar to the findings of kug et al 2009 2010 in each experiment the precipitation anomaly distribution is similar to the observations for the ep events the center of anomalies is approximately 10 and 20 farther to the east in cpl5 2 and cpl4 1 respectively the anomaly is strongest in the obs followed by exp cpl5 2 cpl5 1 cpl4 1 and cpl4 2 their corresponding maximum precipitation anomalies are 10 3 9 1 9 0 8 6 and 8 1 mm day 1 respectively for the cp events the center of anomalies is approximately 10 farther to the west in exp cpl4 2 and cpl5 2 and approximately 20 farther to the east in exp cpl4 1 the anomaly is strongest in cpl4 1 followed by exp cpl4 2 cpl5 1 cpl5 2 and obs their corresponding maximum precipitation anomalies are 8 2 6 5 6 1 5 3 and 5 1 mm day 1 respectively it can be seen that the precipitation anomalies are stronger in exp cpl5 than in exp cpl4 for the ep el niño events whereas the opposite is true for the cp el niño events the impacts of different resolutions on precipitation strength are not pronounced for both types of el niño events the precipitation anomaly intensity in exp cpl5 2 is closest to that of the observed anomalies for both types of el niño events fig 4 shows the zonal wind anomaly composites of the two types of el niño events during d 0 j 1 f 1 in the observations fig 4a the zonal wind anomalies associated with the ep el niño display a pattern of positive values which extends from the western pacific to the eastern pacific and is mainly located in the southern hemisphere the ep events in the coupled experiments show a similar zonal pattern whereas the strength is different from that observed the anomalies show a southeastward extension east of 130 e and are stronger than those observed the wind anomalies are the strongest in exp cpl5 2 followed by cpl5 1 obs cpl4 2 and cpl4 1 their corresponding maximum wind anomalies are 8 35 4 84 4 56 4 42 and 4 37 m s 1 respectively it can be seen that the experiments with cam5 or lower resolution cause larger zonal wind anomalies than the experiments with cam4 or higher resolution the zonal wind anomaly intensity in exp cpl4 2 is closest to that of the observed anomalies in the experiments with cam5 the results from exp cpl5 1 with higher resolution are closer to those observed than those from exp cpl5 2 with lower resolution the opposite occurs in the experiments with cam4 in the cp cases the positive anomalies are located farther to the east in the experiments the relationships between the experiments and the observations are not obvious 4 2 evolutions of the two types of el niño events to identify different evolutions of the two types of el niño events composite evolutions of the equatorial ssta as a function of longitude from observations and experiments are shown in fig 5 in the observations the ssta of the ep el niño fig 5a begins to develop in spring reaches its maximum during winter and then changes to negative anomalies the ssta center is at 120 w in the observations all experimental results reflect the ssta evolution however it is apparent that the intensity of the ssta is the strongest in exp cpl5 2 followed by cpl5 1 cpl4 2 obs and cpl4 1 which is the same as in fig 1 their corresponding maximum ssta values are 4 8 3 0 2 6 2 5 and 2 2 c respectively the ssta intensity in exp cpl4 2 is closest to those observed compared with the ep events the cp events show a similar but weaker evolution fig 5b and the maximum center is farther to the west being centered at 170 w the ssta in the eastern pacific becomes negative at the beginning of year 1 which is earlier than that in the ep events the experiments can capture the maximum values during winter however positive anomalies still exist throughout the following year which is not realistic these unrealistic features in the experiments can also be found in the results of kug et al 2010 and capotondi 2013 the enso cycle is associated with the recharge discharge of warm water to from the equatorial upper mixed layer according to the recharge oscillation dynamic paradigm jin 1997 at the peak of an el niño event weakening easterly wind leads to deeper thermocline in the eastern equatorial pacific and shallower thermocline in the western equatorial pacific then a discharge of warm water from the equator to higher latitudes causes a shallower thermocline depth to examine the heat exchange between the equatorial and off equatorial regions the time latitude sections of the zonal mean 140 e 80 w thermocline depth anomalies from observations and experiments are shown in fig 6 in the observations the equatorial thermocline depth anomalies for the ep el niño are positive beginning from january of year 0 reach a maximum around september and then become negative after march of year 1 at the same time the anomalies become positive north of the equator while the anomalies south of the equator are negative these results are similar to the findings of kug et al 2009 2010 and capotondi 2013 the process is associated with the recharge discharge theory jin 1997 the evolution of the thermocline depth anomalies in all experiments is similar to those observed except for the amplitude compared with the ep el niño events the thermocline depth anomalies in the cp el niño events are different in the observations the anomalies along the equator are positive from january of year 0 and then become negative from march of year 1 whereas the anomalies north of 10 n are almost negative from year 0 to the following year however these features cannot be well captured by the models the thermocline depth anomalies in the cp el niño of exps cpl4 1 and cpl5 1 are similar to those in the ep el niño notably the thermocline depth anomalies associated with the ep el niño events fig 6g develop in jan 0 and maximize in oct 0 along the equator in exp cpl4 1 the anomalies in cp el niño events show a similar but weaker evolution fig 6h the evolution and intensity of anomalies of exp cpl4 1 for both types of el niño events are similar to the findings of capotondi 2013 4 3 enso period the enso has a broadband period of 2 7 years philander 1985 the power spectra of the niño3 and niño4 indices can be used to represent the period and amplitude of ep and cp el niño events respectively as shown in fig 7 for the ep el niño the observational data show a period of approximately 3 8 years fig 7a all experimental results capture the period well lying within a range of 3 5 years among the four experiments the best simulation of the period is seen in exp cpl5 1 the periods in both exps cpl5 2 and cpl4 2 are approximately 4 2 years being longer than the observed ones and the period in exp cpl4 1 is approximately 3 3 years which is shorter than the observed counterpart an interesting feature in fig 7a is that the period is strongly dependent on the atmospheric model resolution i e the period in the 2 resolution model is longer than that in the 1 resolution regardless of the physics of cam4 or cam5 however the impacts of different atmosphere physics on the period are inconsistent when the resolution is 2 the periods for cam4 and cam5 are similar i e the periods of exps cpl4 2 and cpl4 2 are similar the period of cam5 is longer than that of cam4 and it is closer to the observations when the resolution is 1 fig 7b shows the power spectra of niño4 from observations and model experiments similar to the ep el niño the periods of observations and experiments are all between three and five years the period of the cp el niño is sensitive to the horizontal resolution and atmospheric physics either low resolution or cam5 atmospheric physics can make the period larger for both types of el niño events among the experiments exp cpl5 1 simulates the period best comparison of the power spectra of both types of el niño events shows that the atmospheric physics and resolution have significant influences on the strength of el niño the maximum power is approximately twice as large in the exp cpl5 compared with exp cpl4 for both 1 and 2 resolutions by contrast the 2 resolution models yield maximum power 1 5 times as high as the 1 resolution models for both exps cpl4 and cpl5 for both types of events the exp cpl4 1 simulates the strength best 4 4 phase locking an important feature of the el niño events is their phase locking behavior i e their peak always occurs in winter fig 8 shows the standard deviations of the niño3 and niño4 indices for observations and experiments for the ep el niño events fig 8a the standard deviations of the exps cpl5 2 and cpl5 1 peak in winter concentrated in november and december respectively the exps cpl4 2 and cpl4 1 peak in october and september respectively these results indicate that the exp cpl5 can reproduce the phase locking feature however the exp cpl4 fail to do so in contrast all experiments can capture the phase locking feature for the cp el niño events fig 8b note that most of the amplitudes of the two types of el niño events are stronger in the models than in the observations except for exp cpl4 1 which is slightly weaker but closest to the observations the amplitude yielded by the exp cpl5 is stronger than that yielded by the exp cpl4 and the lower resolution experiments yield stronger amplitudes than do the higher resolution experiments note that many other models including these in cmip3 and cmip5 also have problems in simulating realistic enso phase locking features ham and kug 2014 it was found that the simulation of the phase locking has a strong relation to the simulation of the climatological zonal sst gradient in the tropical pacific ham et al 2013 ham and kug 2014 we have investigated the iod in a similar framework as that used here which was published previously yao et al 2016 some interesting and similar results are found when comparing the impact of atmospheric physics and model resolution on the simulation of el niño and the iod for example the strengths of the two types of el niño and iod events in the model simulations are almost larger than those observed and the strength yielded by the experiments with cam5 or 2 is larger than that by the experiments with cam4 or 1 the periods of the two types of el niño and iod events in the 2 resolution model are always longer than those in the 1 resolution regardless of the physics of cam4 or cam5 the phase locking feature of the ep el niño and iod events in the experiments with cam5 is reproduced better than that in the experiments with cam4 the consistent impact of these model configurations on enso and iod suggests that some common mechanisms characterized in these simulations might dominate both variability modes for example equatorial wave dynamics and bjerknes positive mechanisms 4 5 enso skewness el niño events are stronger than la niña events this intrinsic nonlinear characteristic is called the skewness of the enso i e asymmetry of the enso an and jin 2004 to measure the nonlinearity of the enso cai et al 2018 introduced a parameter α characterizing the nonlinearity of the relationship between ep and cp events and their skewness the nonlinearity is determined by fitting the monthly data with quadratic function pc 2 t α p c 1 t 2 β p c 1 t γ where pc1 and pc2 are the first two principal components of an empirical orthogonal function eof analysis of the monthly ssta greater α produces larger positive skewness in ep enso events and larger negative skewness in cp enso events positive negative skewness indicates that the ep enso cp enso tends to appear more often as strong el niño la niña events than as strong la niña el niño events these nonlinear relationships for the observation and coupled experiments are shown in fig 9 the value of α is 0 37 in the observation close to the value 0 31 in cai et al 2018 the value of α is the largest in exp cpl5 2 followed by cpl4 2 cpl5 1 and cpl4 1 with 0 57 0 46 0 38 and 0 12 respectively the value in exp cpl5 1 is closest to that observed it can be seen that the experiments with cam5 or lower resolution cause larger skewness than the experiments with cam4 or higher resolution the above analysis suggests that the dominant dynamical processes may be different in the two types of el niño events which is examined in the next section 5 diagnosis of mixed layer heat budget 5 1 heat budget analysis different types of el niño events involve different dynamic processes to identify the leading dynamics the mixed layer heat budgets for the two types of el niño were analyzed e g kug et al 2009 2010 capotondi 2013 ren and jin 2013 fang and zheng 2018 in these published studies the authors considered only the advection terms and did not analyze the role of the thermal forcing term in addition to the advection terms how important will the thermal forcing be the term thermal forcing is included here the heat budget equation is as follows 1 t t u t x u t x u t x v t y v t y v t y w t z w t z w t z q n e t ρ c p h r where the overbar and prime indicate monthly climatology and anomaly respectively the variables u v and t indicate zonal current meridional current and oceanic temperature averages in the mixed layer respectively variable w indicates vertical velocity at the bottom of the mixed layer q n e t ρ and c p indicate net surface heat flux density of water and specific heat of sea water respectively h indicates the climatological mixed layer depth and h 50 m variable r indicates the residual term and is not considered in this study q n e t can be determined using the equation below 2 q n e t q s h o r t q l o n g q l a t e n t q s e n s i b l e where q s h o r t q l o n g q l a t e n t and q s e n s i b l e indicate net shortwave heat flux net longwave heat flux latent heat flux and sensible heat flux respectively fig 10a shows the composite of each mixed layer heat budget term for the ep el niño in the niño3 region for the observations the total advection seems to capture the observed tendency of the niño3 sst the u t x v t y and w t z terms are the most important terms for the development and decay of ep events the results are similar to the findings in kug et al 2009 the heat flux terms hinder the development and decay of ep events however adding the contribution of the heat flux terms helps the sum of the total advection and heat flux terms better explain the observed ssta tendency fig 10b shows the composite of each mixed layer heat budget term for the cp el niño in the niño4 region for the observations the total advection term seems to capture the variation pattern of the niño4 sst in the observations the u t x and u t x terms are the most important terms for the development and decay of cp events the results are similar to the findings in kug et al 2009 however the heat flux terms play significantly negative roles during the development and decay of cp events and the sum of the total advection and heat flux terms is almost negative during the whole process of a cp event the composites of the mixed layer heat budget terms for the ep el niño in the niño3 region for the coupled experiments are shown in fig 11 the tendency of the ssta in each experiment is consistent with that of the observations fig 10a the total advection term in the experiments reaches the maximum positive value in july but the time to reach the minimum value also lags behind the change in the ssta the sum of the advection and heat flux terms is consistent with the tendency of the ssta especially the periods of warming and cooling the correlations of the sum and tendency of the ssta are 0 99 0 97 0 98 and 0 94 in exp cpl4 2 cpl5 2 cpl4 1 and cpl5 1 respectively the linear terms u t x v t y w t z and w t z play leading roles and promote the development of ep events the effect of the nonlinear terms should be small however the terms u t x and w t z are too large in exp cpl5 2 the term denoting the mean upwelling and anomalous temperature gradient w t z also called the thermocline feedback an and jin 2001 is too large in all four experiments which is not consistent with the observations fig 12 shows the composites of the mixed layer heat budget terms for the cp el niño in the niño4 region for the coupled experiments in all four experiments the periods of warming and cooling of the ssta are similar to those of the observations however the simulation of the sum of the advection and heat flux terms is not good especially in exps cpl4 2 and cpl5 2 in which the tendencies are even opposite in exps cpl4 1 and cpl5 1 the sum is good in the developing phase but not during the decaying phase the nonlinear term u t x is too large in all four experiments the correlations of the sum term and tendency of the ssta are 0 20 0 39 0 75 and 0 88 in exp cpl4 2 cpl5 2 cpl4 1 and cpl5 1 respectively it seems that a higher resolution benefits the simulation of the advection and heat flux terms for the cp events therefore the effect of the model resolution on the physical process is significant while the effect of atmospheric physics is not serious ideally the correlation between the temperature tendency and the sum of all other terms should be one however in reality the correlations are always less than one for the ep and cp el niño events in the model simulations figs 11 and 12 here we define the imbalance as the difference between the ssta tendency and the sum of all other terms that is the difference between the thick solid and dashed black lines in figs 11 and 12 many factors can contribute to the imbalance including the estimates of the mixed layer depth temporal resolution of the model output disregard of the eddy heat flux diffusion and nonlinear higher order terms for example the mixed layer depth is defined as a fixed depth 50 m for simplicity here which is not very realistic e g dong et al 2007 in addition the monthly average of the model output is used in the study probably resulting in oversmoothing of the distribution of the mixed layer depth neglecting the eddy heat flux especially fluxes associated with mesoscale eddies is probably another reason for the imbalance schiller and ridgway 2013 other factors such as nonlinear higher order terms are also neglected during calculation of the formula the heat budget analysis in the cp el niño events is not as good as in the ep el niño events suggesting that these concerns are more serious in the cp el niño than in the ep el niño one possible reason is that the inaccurate estimate of the items related to horizontal advections which are actually dominated by advection can cause more imbalance to the cp el niño 5 2 dynamic analysis of cp events during the developing phase a significant bias evident in these experiments lies in their simulation of the mixed layer heat budget for different types of el niño events as indicated by the heat budget analysis the simulations of advection and heat flux terms for the cp events are not good particularly in exps cpl4 2 and cpl5 2 during the developing phase therefore we focused on the developing phase of the cp el niño events and analyzed the possible causes for the poor simulation results of the lower resolution experiments we defined the period from june to november as the developing phase su et al 2014b and analyzed the variation in the ssta in the mixed layer for the cp events during the developing phase fig 13 shows the temperature tendency 3d temperature advection surface flux heating and the sum of the advection and surface heat fluxes during the developing phase of the cp events the advection term promotes the development of the cp events and its amplitude is larger than the temperature tendency term this result is consistent with the finding in kug et al 2009 however kug et al 2009 only considered the effect of 3d temperature advection and neglected the important role of surface heat fluxes in reality the total heat flux terms play a negative role in the observations hindering the development of cp events the simulation results are not good especially the simulation of advection in exps cpl4 2 and cpl5 2 the simulations of advection in exps cpl4 1 and cpl5 1 are similar to the observations therefore increasing the resolution is helpful to improve the simulated effect of the advection term in the developing phase of cp events we further analyzed the relative sizes of all the 3d temperature advection terms in the developing phase of the cp events fig 14 it can be seen that exps cpl4 2 and cpl5 2 are not well simulated for the cp events mainly due to a large deviation in the simulation of the zonal advection term in addition the negative anomaly in the zonal advection term is contrary to that of the observations which causes the sum of all advection terms to be negative fig 15 shows a synthetic diagram of the mean zonal flow eastward positive and temperature anomalies in the niño4 region of the cp el niño events in the observations the mean zonal current u is reversed near the equator it is positive east of 165 w and negative in the west fig 15a however the center of the temperature anomalies is near 165 w fig 15b therefore the zonal gradient of the temperature anomalies t x is approximately positive negative west east of 165 w making the zonal advection term of the anomalous temperature gradient found by averaging the mean current u t x over the niño4 region positive compared with the observations the coupled experiments show almost negative i e westward mean zonal currents in the niño4 region the temperature anomalies are centered at 175 e in both exps cpl4 2 and cpl5 2 being more to the west than those observed thus the term t x is positive negative which makes the term u t x positive negative west east of 175 e the negative areas are larger than the positive areas leading to the average value of the entire niño4 region being negative in both exps cpl4 2 and cpl5 2 however the temperature anomalies are centered at 175 w in exps cpl4 1 and cpl5 1 which makes the area where the term t x is negative in the experiments with higher resolution much smaller than that in the experiments with lower resolution therefore the average value of u t x in the niño4 region is positive finally it can be seen that different resolutions result in different ssta centers in the niño4 region the anomaly centers of the lower resolution experiments are more to the west while the anomaly centers of the higher resolution experiments are closer to those observed then the zonal advection term of the anomalous temperature gradient found on the basis of the mean current u t x simulated in the higher resolution experiments is better than that in the lower resolution experiments which causes poor simulation of the advection and heat flux terms for the cp events in both exps cpl4 2 and cpl5 2 during the developing phase 6 summary and discussion in this study we presented a detailed assessment of the simulation of two types of el niño using the latest version of the cesm emphasis was placed on investigating the impacts of different atmospheric physics and resolutions on simulating the two types of el niño events for this purpose two coupled experiments using low and high horizontal resolutions of cam4 exps cpl4 2 and cpl4 1 were conducted in addition because the new version of the cesm includes the updated cam5 atmospheric physic model two additional experiments using different resolutions of cam5 namely exps cpl5 2 and cpl5 1 were performed the main results are summarized below 1 each experiment realistically reproduces the ssta and thermocline depth anomalies of the ep events however the simulation of the cp events is not satisfactory cam5 leads to much larger thermocline depth anomalies of the ep events 2 the intensity of a cp event is much smaller than that of an ep event either low resolution or cam5 atmospheric physics can make the intensity larger and the period longer for both types of el niño events cam5 gives unrealistically stronger intensity compared with that of cam4 3 the observed phase locking of the ep events is not reproduced in exp cpl4 the horizontal resolution has little effect on the simulation of the phase locking of the two types of el niño events 4 either low resolution or cam5 atmospheric physics can cause larger enso skewness 5 the simulation of thermocline feedback in the coupled experiments is much larger than that in the observations for the ep events 6 better simulated zonal advection of the anomalous temperature gradient by the mean current benefited the simulation of cp events the two types of el niño events correspond to different dynamic processes to investigate this difference we analyzed the dynamic advection and heat flux during the development and decay of ep and cp events we found that the dynamic processes of the ep events are basically the same as those observed that is the anomalies of zonal advection and vertical advection lead to ep events however for the cp events the advection term in the low resolution experiments prevents sst warming and the heat equation is dominated by the remainder of the heat balance namely diffusion and dissipation these results are opposite to those observed and those found in the high resolution experiments therefore although the two types of el niño events are found in the experiments with low resolution the dynamics of the cp events are wrong studies of the two types of el niño events have been described in a large number of studies over the past few years this work aims to investigate several possible factors that may impact the simulation of the two types of el niño events through sensitivity experiments using the cesm which serves as a necessary step for improving el niño prediction additional studies with more sensitivity experiments as well as diagnostic analyses are needed to further elucidate the dominant physical and dynamic processes of the two types of el niño events acknowledgments the observational data sets used in this study are from http apdrc soest hawaii edu data data php this research was supported by the national key r d program of china 2017yfa0604202 the china ocean mineral resources r d association dy135 e2 3 02 the national science foundation of china 41690124 41705050 41806032 and 41621064 and the national programme on global change and air sea interaction gasi ipovai 06 yt is also supported by the discovery grant of the natural sciences and engineering research council of canada nserc 
24011,to resolve nonlinear internal wave motions in regional domains we propose a one way ocean model nesting framework in which a nonhydrostatic model is nested within a regional model the nesting scheme produces internal tides that propagate freely into and out of the nonhydrostatic model domain with no reflections while nudging the low frequency motions we demonstrate the method with idealized test cases and a field scale simulation of the south china sea the method relies on four parameters the time scale of low pass filtering the time scale of low frequency forcing the time scale of sponge damping and the width of the sponge damping layer we present guidelines for selecting these four parameters based on the background stratification the internal tides from the large scale model steepen through nonlinear and nonhydrostatic effects and reasonably match observations at a fraction of the computational cost needed with a single uncoupled model the model critically relies on the background stratification inherited from the large scale model which in this case leads to an underpredicted phase speed and amplitude in the nested model result this highlights the need for large scale ocean models to incorporate observational stratification data especially in the ocean interior 1 introduction internal gravity waves are typically on the scale of hundreds of kilometers in space hours in time and are ubiquitous in the world s oceans they are thought to contribute an estimated one terawatt of deep sea turbulent dissipation which is understood to play a crucial role in the ocean s global redistribution of heat and momentum alford et al 2015 and serve important ecological functions to mixing in the nearshore environment walter et al 2012 internal waves are generated by tidal motions over topography and can steepen into nonlinear solitary like wave trains alford et al 2010 when these internal waves encounter shallow depths such as the shelf they steepen and break often forming dissipative bores arthur and fringer 2016 walter et al 2012 recently global ocean models such as the hycom model martin 2000 have been improved to explicitly resolve the internal tidal motions arbic et al 2018 and thus contain long wavelength internal tides in their solution at a regional scale hydrostatic ocean models can reasonably estimate net dissipation from internal tides carter et al 2008 suanda et al 2018 however the coarse spatial resolution o km and hydrostatic pressure formulation of these models limit the development of nonlinear solitary wave trains and computation of breaking and dissipation kinematics vitousek and fringer 2011 to model high resolution internal wave processes a grid nesting approach is often employed where boundary conditions from the large scale model are used to force a higher resolution nested model blayo and rousseau 2016 chen et al 2013 kumar et al 2015 pickering et al 2015 warn varnas et al 2015 for grid nesting problems with waves the fundamental problem is that wave information propagates differently in the parent grid versus the child grid and thus the two models contain inconsistent information amplitude wavelength timing etc at the outgoing model boundary blayo and rousseau 2016 vitousek and fringer 2011 this difference typically leads to reflection of spurious wave information back into the domain by design the child grid and parent grid compute different physical processes appropriate for their scale i e non hydrostatic vs hydrostatic pressure resolved vs parameterized turbulence and thus this is a fundamental challenge for modeling waves in nested domains a related problem relates to the need to match the outgoing direction of propagation at open boundaries imposing an inconsistent wave form on a model boundary can send incorrect wave information into the domain for example if we wish to model a wave exiting the nested domain and naively apply these wave velocities on the outgoing model boundary a spurious wave will be forced into the domain when there is a mismatch between the large scale and nested domain internal tide solution previous work has attempted to solve this basic nesting problem with several approaches the first is two way nesting which attempts to solve for a uniform solution between model types velocity pressure scalars and has been successfully demonstrated for the nonhydrostatic navier stokes equations blayo and rousseau 2016 while two way nesting is often employed for hydrostatic ocean models with different models including ncom ko et al 2008 and roms warner et al 2010 solving a two way nested large scale nonhydrostatic ocean model is under development with the croco model auclair et al 2018 and would require orders of magnitude more computational power the second approach is to use adaptive mesh refinement amr methods which adaptively refine the grid in specific areas and have been demonstrated in stratified flows koltakov and fringer 2013 and large scale oceanic flows using the roms agrif debreu et al 2012 and icom piggott et al 2008 models the disadvantage of a single grid with grid stretching is that grid quality can compromise accuracy when there is significant stretching and the time step is dictated by the finest grid spacing which can become limiting in some cases the third approach is to employ one way nesting and radiate wave energy out of the domain using a radiative function which requires solving for the wave speed and properties which are then propagated out of the model domain this has been applied to both surface gravity waves chapman 1985 atmospheric gravity waves klemp and durran 1983 and hydrostatic oceanic boundary conditions including internal waves carter and merrifield 2007 marsaleix et al 2006 mason et al 2010 suanda et al 2018 a variant of this approach uses an adaptive algorithm where inward and outward information fluxes are treated separately marchesiello et al 2001 the limitation of these approaches is that the approximation of wave speeds and properties vertical and horizontal wavenumbers inevitably leads to errors in the radiative boundary conditions which lead to spurious waves reflecting into the domain it can also be computationally expensive to solve for spectral wave number properties frequency wavenumber amplitude and estimate the propagation speed we present a novel approach which splits the parent grid solution into low frequency mean and high frequency wave components at some dividing frequency the low frequency components are applied as dirichlet boundary conditions on the nested grid and nudged over the model domain the wave component is forced for wave energy propagating into the domain as dirichlet boundary conditions on the nested grid boundary and with a sponge damping layer near these boundaries which damps outgoing wave energy this combination provides an efficient and practical method to conduct one way multiscale model nesting our aim here is to model specific wave events although the approach is similar to laboratory scale modeling studies of stratified fluids winters 2016 winters and de la fuente 2012 it has not been applied to a regional scale ocean model we show that the accuracy of the nesting strategy is dependent on four time scales that are related to physical properties of the system most importantly we show that for realistic applications the results depend critically on accuracy of the large scale stratification in this manuscript we present the theoretical framework section 2 and illustrate the methods in a series of idealized test cases section 3 we then present a realistic model application in the south china sea nesting the suntans model fringer et al 2006 into the us navy ncom model martin 2000 focusing on modeling internal waves section 4 a discussion section 5 and summary section 6 follow 2 theoretical framework the general setting we wish to model is a three dimensional nested model domain ω n within a large scale model ω l s and overlapping boundary γ fig 1a with oceanic flows at a range of temporal and spatial scales including internal wave propagation and steepening the nested model domain has a higher resolution grid and bathymetry than the large scale model and may have enhanced physics appropriate for the higher resolution i e non hydrostatic vs hydrostatic pressure to accomplish the nesting we implement a method to nudge the suntans model velocity temperature and salinity u t s to a large scale model velocity temperature and salinity u l s t l s s l s at appropriate time scales using a horizontal source term in the momentum equations the suntans model solves the three dimensional nonhydrostatic reynolds averaged navier stokes equations under the boussinesq approximation along with the scalar transport equations and the continuity equation fringer et al 2006 1 u t u u 2 ω u 1 ρ 0 p g ρ 0 ρ k ˆ h ν h h u z ν v u z f m 2 t t u t h κ h h t z κ v t z f t 3 s t u s h γ h h s z κ v s z f s 4 u 0 where u u v w is the velocity vector ω is the earth s angular velocity vector g is gravitational acceleration ν κ γ are the eddy viscosity eddy thermal diffusivity and eddy mass diffusivity respectively k ˆ is the vertical unit vector and h and v are the horizontal and vertical components of a variable or operator notation is summarized in table 1 to compute density ρ an equation of state is employed as a function of temperature t and or salinity s a horizontal source term in the momentum equations f m x y z t is introduced such that 5 f m f l f d where f l is the low frequency forcing and f d is the sponge layer high frequency damping term fig 1b likewise the temperature and salinity equations are nudged at low frequency with f t and f s respectively to perform our nesting approach it is necessary to separate velocity density and pressure fields into reference background and spatially varying components following kang and fringer 2012 the total density is decomposed with 6 ρ x y z t ρ 0 ρ b x y z t ρ x y z t where ρ 0 is the constant reference density ρ b ρ ρ 0 is the time averaged background density with time low pass ρ filtering and ρ is the perturbation density due to wave motions the pressure is split into its hydrostatic p h and nonhydrostatic q parts with p p h q and p h is further decomposed into constant background and perturbation components with 7 p h p 0 p b p ρ 0 g η z g z η ρ b d z g z η ρ d z where η is the free surface we separate the flow into barotropic and baroclinic velocity components u u u where denotes the deviation from the barotropic velocity and the barotropic velocity is given by the depth average 8 u 1 h d η u h d z 1 h u h where d η d z the depth integration of a quantity u h is the horizontal component of the velocity vector the bottom is defined at z d x y and the total water depth is given by h d η adopting a decomposition of variables into time high ρ and time low pass ρ components with filtering time scale τ f i l t the velocity can be further decomposed into both temporally u u and vertically u u varying components with 9 u u u u u u u which represent from left to right the low frequency barotropic and baroclinic and the high frequency barotropic and baroclinic terms respectively the free surface is also be decomposed with 10 η η 0 η 0 η which represent from left to right low frequency barotropic high frequency barotropic and high frequency baroclinic motions here η 0 is the low pass time filtered free surface η 0 is the spatially low pass filtered free surface obtained by spatially filtering η at a scale much larger than the internal wave wavelength λ and thus η is the high frequency high wavenumber component of the free surface for oceanic conditions the time scale of separation is taken at a frequency less than the inertial frequency f and the primary tidal frequencies thus the high frequency components primarily consist of propagating internal gravity waves with frequency ω within the range f 2 ω 2 n b 2 where the buoyancy frequency n b is given by n b 2 g ρ 0 ρ b z the separation of time scales is treated separately in the large scale grid ls and the suntans computations for the large scale grid ω l s the simulations are run in a one way nesting routine and therefore the large scale results are known a priori to running the coupled model therefore u l s is calculated with a simple low pass filter using a fourier transform and u l s is the deviation from this mean for the suntans domain ω n u is computed using a running average of previous results following wolfram and ringler 2017 the ordinary differential equation for a two level impulse function is 11 d u d t 1 τ f i l t u u where τ f i l t is the low pass time filtering scale a running mean at time step n 1 is thus approximated with the backward in time discretization 12 u n 1 u n δ t 1 τ f i l t u n 1 u n this filter is equal to a standard single pole filter smith 1997 and as long as δ t τ f i l t 12 typically has significantly lower error than a moving box filter to obtain the energy flux which is needed in the sponge layers the kinetic energy density is decomposed into the barotropic horizontal baroclinic and cross terms kang and fringer 2012 13 e k 0 1 2 ρ 0 u 2 v 2 14 e k 1 2 ρ 0 u 2 v 2 w 2 15 e k 0 ρ 0 u u v v the depth integrated barotropic and baroclinic energy flux terms ignoring small unclosed and diffusion terms are given by 16 f 0 u h e k 0 u h h ρ 0 g η u h p u h q 17 f u h e k u h e k 0 u h p u h q where in oceanic conditions the dominant terms in 16 are u h h ρ 0 g η and u h p due to hydrostatic pressure work and the dominant term in 17 is u h p or the energy flux due to the wave induced hydrostatic pressure work kang and fringer 2012 2 1 boundary conditions we apply dirichlet boundary conditions along γ using a decomposed horizontal velocity u b c from the large scale grid figs 1b 2 the boundary velocities were chosen to be consistent with the lateral sponge layer such that boundary is forced by the barotropic and low frequency baroclinic components of the large scale forcing and the baroclinic wave velocity associated with waves propagating into the domain 18 u b c u l s u l s u l s f ˆ u l s t ˆ r where the operator 19 f ˆ x y t 1 if f l s l p n 0 0 if f l s l p n 0 selects only the wave velocities associated with energy flux propagation into the domain and n is the unit vector directed out of the nested model domain normal to the model boundary thus the baroclinic high frequency motions are modified depending on the wave direction but all other components of the flow are specified by the large scale model for the computation of 19 the low pass filtered flux in time f l s l p is spatially smoothed lp using a gaussian filter to a scale much larger than the internal wave wavelength λ to eliminate variations in f ˆ smaller than the sponge layer width we also initialize the boundary velocities smoothly in time with a ramping function t ˆ r 1 exp t θ r where t is the model run time and θ r is a ramping time scale the boundary conditions for tracers t b c s b c are upwind set to the large scale value for flow entering the domain t l s s l s and the free surface is not constrained as we apply dirichlet boundary conditions on the horizontal velocity fig 2 to ensure long term volume consistency very few boundary grid cells in a corner of the domain are specified as neumann velocity boundary conditions instead of the dirichlet condition in 18 with a specified free surface η b c η 0 η 0 which is the instantaneous spatially low pass filtered free surface from the large scale model i e removing iw motions 2 2 lateral sponge layer to minimize wave reflections from the nested model boundaries a sponge damping term is implemented in the horizontal momentum equation eq 1 of the form 20 f d u f ˆ t ˆ r u l s τ d d ˆ 21 d ˆ x exp 4 r x δ d where τ d is the damping time scale and d ˆ modulates the strength of damping spatially which decreases exponentially at a length scale of δ d from the nearest boundary and r is the distance from the nearest boundary fig 1c the damping timescale is thus a local value given by d ˆ x τ d which means that the strength of the damping of outgoing high frequency baroclinic components decreases exponentially when moving from the boundary into the domain by nudging the suntans baroclinic wave velocity u to the large scale baroclinic wave velocities associated with incoming waves only f ˆ u l s incoming waves and flows are allowed to propagate into the domain while all outgoing baroclinic wave motions are damped fig 2 this approach is very flexible and allows waves to propagate at different directions and frequencies within the domain while damping energy near the boundaries thereby avoiding spurious reflections from the boundaries 2 3 low frequency forcing the low frequency motions in the suntans model u are forced by the large scale low frequency motions u ls with the forcing term f l in eq 5 over the entire model domain using 22 f l u u ls τ l t ˆ r where τ l is the low frequency forcing time scale similarly the low frequency suntans model temperature and salinity t s are nudged towards the large scale low frequency variables t l s s l s using 23 f t t t ls τ t t ˆ r 24 f s s s ls τ s t ˆ r where τ t τ s are the low frequency forcing time scales for temperature and salinity respectively this framework assumes the model is initialized with t l s and s l s but zero velocity and free surface forcing terms for the low frequency components 22 24 are initially zero because of t ˆ r the sponge damping term 20 has t ˆ r applied to u l s and initially damps all baroclinic high frequency motions u arising from spurious motions from the initial conditions and slowly transitions to forcing to the large scale boundary motions u l s to force wave velocities and not temperature or salinity the method proposed here relies on four parameters the time scale of low pass filtering τ filt the time scale of low frequency forcing τ l the time scale of sponge damping τ d and the width of the sponge damping layer δ d 3 idealized test cases model performance we apply a series of test cases to demonstrate the model capabilities and the effects of the four boundary parameters under different conditions including a two dimensional x z idealized internal wave generation case and a three dimensional idealized south china sea 3 1 base case internal wave propagation in two dimensions we compute an idealized model simulation of internal wave propagation in one direction x z plane based on an idealization of observations from the south china sea ramp et al 2010 zhang et al 2011 specifically we include a typical south china sea density field given by ρ b 2 1 x 1 0 4 ρ 0 t and temperature t 23 36 exp z 44 12 293 12 3 13 fig 3a with buoyancy frequency n coriolis frequency f 5 7 10 5 rad s typical for this latitude and an imposed forcing frequency at the m 2 tidal period ω m 2 2 π t m 2 t m 2 12 42 h for most of the depth f 2 ω m 2 2 n 2 thus this environment is capable of sustaining propagating internal waves fig 3b we apply this test case on a 520 0 75 3 km length l width w depth h domain with grid resolution δ x δ y 250 m which meets the requirements of vitousek and fringer 2011 for minimal numerical dispersion and 100 vertical z levels table 2 with enhanced resolution near the surface and exponentially increasing vertical spacing with a growth rate of 1 08 the time step δ t is dictated by stability of internal gravity wave propagation at speed c i w and given by the courant number constraint c i w δ t δ x 1 boundary conditions are dirichlet at the x east west boundaries and periodic in y north south an idealized large scale velocity field u l s is applied to the model as described above using boundary conditions a lateral sponge layer and low frequency forcing and contains the four components in 17 u l s u l s u l s u l s u l s with u l s u u l s u u l s u t i d e k t i d e k t i d e cos k t i d e x ω m 2 t ϕ t i d e 25 u l s u i w f z k i w k i w cos k i w x ω m 2 t ϕ i w where f z is the mode 1 velocity eigenfunction u 0 1 ms 1 u 0 u t i d e 0 06 ms 1 u i w 0 50 ms 1 and ϕ t i d e ϕ i w 0 waves in this system consist of both barotropic internal tides with wavenumber k tide and baroclinic internal waves with wavenumber k iw waves propagate in the positive x direction θ 0 and have a horizontal wavelength λ x 2 π k i w x 130 km consistent with the stratification and forcing frequency we apply forcing on the baroclinic flow consistent with mode 1 horizontal oscillations for the imposed stratification solved using the eigenmode problem fringer and street 2003 kundu and cohen 2008 fig 3c the mode 1 eigenfunction is approximated as f z α 1 exp z h 2 δ α 2 where α 1 1 4248 α 2 0 1831 δ 394 4 m h 2 9 995 m in 25 which is a result specific to the density field and sponge layer width δ d 100 km damping time scale τ d 800 s in 19 and low frequency forcing time scale and filtering time τ l τ f i l t 2 t m 2 in 21 in the y direction v l s 0 05 ms 1 and to obtain geostrophic balance in the periodic domain a forcing f l y f u t ˆ r is applied in 1 and 5 the results for the free surface and isopycnal displacement show internal waves emanating from the left wavemaker region and propagating in the positive x direction with steepening leading to the formation of a train of rank ordered solitary like internal gravity waves fig 4a b these waves are subsequently damped within the right sponge layer the barotropic velocity u u is nearly uniform within the domain and is not influenced by the wavemaker or damping layers fig 4c the long2dnodamp model is twice as long as the base2d model but with no lateral sponge layers δ d 0 table 2 which serves as a reference solution for this domain i e freely propagating waves with no damping layers or boundary effects the results are similar to the base2d model but the waves are more nonlinear exhibiting faster steepening this is because the sponge layer in the base2d model is forced with a linear wave 25 which delays wave steepening for x 100 km in this case the long2d model is twice as long at the base2d model but does contain lateral sponge layers table 2 which serves as a reference solution for the case of no sponge layer damping or boundary at the right side of the domain x 420 km the results are nearly identical to the base2d model except in the sponge damping region 420 x 520 km this indicates reflections from the sponge damping region are negligible an important feature of this framework the density anomaly and wave velocity fields ρ u for the base2d model show characteristic internal wave generation consistent with u i w 0 50 ms 1 in 20 and subsequent propagation steepening and damping at the model edges fig 4d e thus the expected behavior of the lateral sponge layer 20 21 in wave generation and dissipation is confirmed the baroclinic mean flow u within the thermocline is oriented in the direction of wave propagation in the model interior but is aligned with the wave propagation in the areas of wave dissipation fig 4e this trend is reversed in the upper and lower parts of the water column rotational dispersion due to coriolis reduces the number of waves in the train the case without rotation f 0 shows development of seven to eight solitons supporting material figure sm1 rather than the four to five as shown in fig 4 the time averaged barotropic energy flux f x 0 increases with x over the model domain due to the applied geostrophic balance in this periodic domain fig 5a while its variation in time shows oscillations emanating from the boundaries due to the applied barotropic tides fig 5b the time averaged baroclinic energy flux f x is relatively constant near the left boundary decreases slightly over the model domain with small oscillations developing due to poorly resolved high wavenumber nonlinearities and subsequently is damped in the sponge damping layers near the right model boundary fig 5a the time varying baroclinic energy flux shows wave energy emanating from the left boundary wave energy propagation across the model domain and damping along in the left model sponge layer fig 5c we estimate the phase speed c based on the background density profile ρ b using the first mode solution to the eigenvalue problem to estimate the base phase speed c 0 fringer and street 2003 kundu and cohen 2008 and adjusting for coriolis effects with c c 0 1 f ω 2 1 2 helfrich 2007 for the m 2 tidal period in this problem c 3 13 m s 1 which closely follows the propagation speed of individual waves fig 5c 3 2 sponge layer dynamics we apply a test case similar to the south china sea base case but with a model domain length of 293 km and horizontal grid size 1000 m and vary the damping time scale τ d and sponge layer width δ d in 20 to evaluate the effects of wave damping and reflection from the sponge layer sponge2d table 2 the shorter domain length is used to evaluate the sponge layer effects on linear waves where the generated waves on the boundary do not have sufficient distance to steepen we also evaluate the sponge layer effects on nonlinear waves by comparing the results of the base2d model with the long2d model we define the reflection coefficient as a measure of the amount of wave energy reflected by the sponge layer and solid boundary and is given by r f 1 f x l δ d f where f is the wave energy flux in the absence of a sponge layer or reflections at x l δ d because f is a bulk parameter integrating wave energy flux propagating in both the positive and negative directions f decreases when wave energy is reflected from the sponge layer we normalize τ d by the travel time for a wave to traverse the sponge layer δ d c for τ d 0 02 c δ d reflections are significant due to reflected wave energy from the sponge layer itself fig 6 for τ d 0 08 c δ d reflections can occur from the model boundary if δ d is not large enough to dissipate all wave energy while the parameter space is covered for linear waves the base2d model with nonlinear waves also follows a similar trend with near zero r f fig 6 thus while the sponge layer performed well over a range of values to minimize reflections from both mechanisms the optimal values appear to be τ d 0 04 c δ d and δ d λ 2 within this framework waves with wavelengths smaller than λ higher modes will be filtered while waves with wavelengths larger than λ lower modes will not a larger value of δ d λ has a broader region of low r f and would be more applicable to a broad spectrum of wave frequencies but requires more computational space used for the damping region these results of competing reflection mechanisms are qualitatively similar to wave damping theory for surface gravity waves zhang et al 2014 a second test case is developed using the same setup as the base case but with wave energy flux coming from both boundaries with the same period but different magnitudes wave velocities on the right boundary are half as large as those on the left the incoming wave energy flux is effectively generated in the model at both boundaries and the outgoing wave energy flux is efficiently damped supp mat figure sm2 additional test cases varying the wave period t iw and magnitude u iw produced similar results this demonstrates that the sponge layer formulation is robust to efficiently damp outgoing waves while allowing incoming waves to propagate into the domain over a wide range of wave conditions 3 3 low frequency forcing variables the primary parameter governing the low frequency forcing is the time scale τ l to test how τ l affects the high frequency waves we modify the base case to have a 260 km domain with 1000 m horizontal grid spacing table 2 setting all velocities to zero except the wave velocity u i w 0 5 ms 1 propagating in the positive x direction we then vary τ l in 22 and record the wave energy flux one wavelength into the domain the wave attenuation coefficient is α 1 f x l δ d f and f is the wave energy flux without low frequency forcing τ l the expression for α is similar for the reflection coefficient r f but in this case is a measure of wave energy damping since all wave energy flux is in the positive x direction and there are no reflections from the damping layer or boundary wave attenuation α decreases as τ l increases and for τ l τ f i l t 1 α 0 01 fig 7 the total propagation distance must be considered when setting τ l since α here is the damping over one wavelength in practice τ l should be as large as possible so as not to influence the wave energy but small enough to control the low frequency flow 3 4 internal waves mean flow in three dimensions we extend the base case model to three dimensions in a 520 520 3 km x y z domain with a refined stretched grid in the model exterior sponge damping region δ d 100 km τ d 800 s and a constant interior horizontal resolution of 500 m table 2 dirichlet boundary conditions are applied on all four boundaries forcing parameters are the same as the base case except the internal waves and barotropic tide are oriented at θ 45 the barotropic mean flow u is spatially constant and the mean free surface η 0 is tilted consistent with geostrophic flow fig 8a the barotropic tide u is directed at an angle θ 45 with the tidal surface η 0 consistent with a poincaré wave with rotation fig 8b both u and u and associated components of the free surface show little sign of interaction with the sponge layer in the outer damping region the wave velocity u at the surface and wave component of free surface η show internal waves emanating from the south and western boundaries propagating and steepening towards the upper right and dissipating along the top and right boundaries fig 8c the wave velocity u and the interface displacement ζ defined as deviation of the isopycnal originating at 366 m depth of o 100 m show similar trends fig 8d these results show that the model framework can effectively simulate a range of flow conditions in an idealized three dimensional environment with several complex flow elements an important consideration in the three dimensional framework is a lateral damping effect which occurs when waves propagate parallel to a sponge layer boundary fig 9 we illustrate this effect with waves propagating parallel to a sponge layer and a solid wall on the opposite boundary 3d lateral model table 2 the waves are distorted near the edge of the sponge layer fig 9b which leads to a slowed propagation speed and loss of amplitude when compared to a two dimensional simulation with the same resolution fig 9a the two dimensional simulation serves here as a reference solution for waves propagating without lateral effects thus there is a buffer region of influence δ b which equals approximately one wavelength for waves propagating parallel to a sponge layer when waves are propagating normal to a boundary this effect does not occur assuming τ d and δ d are selected in a way that eliminates reflections 4 realistic model application test case 4 1 model setup the previous section showed how the model framework is implemented in two and three dimensions under a range of idealized flow conditions we now apply the framework to a realistic application in the south china sea and nest suntans within the us navy ncom model martin 2000 fig 10a the large scale ncom model is implemented here at 1 50 horizontal resolution approximately 2 3 km with 41 vertical sigma z levels with 12 sigma levels on the shelf and hourly output the model domain location and 475 x 390 km extent is chosen because of the strong internal tides over the luzon straight which propagate westward and steepen and are well documented in the literature alford et al 2015 farmer et al 2009 lien et al 2014 ramp et al 2010 zhang et al 2011 the suntans model is run with 100 stretched vertical z levels with a stretched grid in the model exterior sponge damping region and a constant horizontal resolution in the interior fig 10b c to investigate the effects of grid resolution in the suntans model interior horizontal resolutions of 250 525 1100 and 2300 m are employed table 2 to apply the boundary conditions and sponge layer the f ˆ operator 29 is computed from f l s l p which uses a low pass filter in time to obtain the energy flux over multiple waves and spatially filtered at a scale of the domain size w to smooth out variability at scales smaller than λ fig 10d the model is run during may 26 to june 5 2011 a time period when observational data is available from the tc1 mooring in a depth of 450 m lien et al 2014 fig 10 due to bathymetric differences and approximations in the spatial and temporal interpolations from ncom to the suntans open boundaries volume is not exactly conserved in the model domain thus to ensure long term volume consistency several boundary grid cells in a corner of the domain are specified as neumann velocity boundary conditions with a specified free surface η l p η 0 η 0 which is the instantaneous spatially low pass filtered free surface from the ncom model this eliminates wave effects but retains large scale dynamics including the barotropic tide for this example these points were chosen as the three grid cells in the southwest corner of the domain as they are within the wave damping region fig 10d in this way waves generated by small mismatches in η l p and η are damped the suntans model is initialized with the ncom temperature salinity and spatially smoothed free surface output and initial velocities are set to zero kinematic viscosity is set to ν 1 0 4 m2 s in the vertical direction and ν h 1 m2 s in the horizontal direction similar to values used in other studies at this scale kang and fringer 2012 zhang et al 2011 boundary conditions are interpolated at hourly time intervals model spinup is approximately seven days for the barotropic tides to equilibrate to the large scale model from initial velocities set to zero supp mat figure sm3 the results from initializing the model with interpolated velocities show initial high frequency waves from inconsistencies in the model solution and a slightly faster equilibration to the barotropic tides of approximately five days supp mat figure sm3 additional testing with 15 min boundary conditions gave nearly identical results as the 1 h boundary conditions based on these results the hourly boundary condition forcing and zero velocity initial conditions were chosen and we focus on results of the last three days of the ten day simulations for the remainder of the runs 4 2 validation at station tc1 the different models show small differences in background density but large differences in buoyancy frequency fig 11a b in general the observations have a higher buoyancy frequency than the ncom and suntans models the suntans model is initiated with ncom stratification fig 11a we approximate the linear mode 1 phase speed c for these average profiles using the eigenvalue solution adjusted for frequency as in the base case the results show the phase speed for both k 1 and m 2 periods is lower for the ncom and suntans models compared to the observations fig 11c the results for η and t at 140 m depth show propagation and steepening of the internal tide from east to west and steepening of the internal waves with propagation fig 12 surface deflections from the internal waves are o 0 1 m and internal temperature deflections are o 2 c consistent with previous simulations of this region zhang et al 2011 outgoing internal wave motions were absorbed in the sponge layers and there is no observable internal wave reflection supp mat figure sm4 the results of the suntans simulations at a coarse 2300 m and fine 250 m grid resolution are compared to both aerial imagery from modis nasa worldview application https worldview earthdata nasa gov operated by the nasa goddard space flight center esdis project and the ncom model on june 4 2011 fig 13a b c d the wave crest from the ncom result lags behind the modis data due to lack of resolution of nonlinear effects which lead to slower wave propagation the suntans result matches modis better due to improved resolution of nonlinear effects as discussed in vitousek and fringer 2011 coarse resolution gives rise to numerical dispersion which produces the unphysical wave train in ncom fig 13e the dispersion is weaker in the suntans model with coarse resolution fig 13f and is eliminated with higher resolution fig 13g the results are also compared to observational mooring data at location tc1 fig 10a lien et al 2014 the barotropic velocities from the observations compare well to the model results for ncom and suntans especially in the ew direction fig 14a while in the ns direction capture the long term average flow but miss some of the m 2 tidal flow fig 14b this mismatch in the ns direction is potentially a result of inaccuracy in the ncom model whose results are very similar to the suntans model temperature observations show internal waves passing by the tc1 mooring with both broad waves and sharply nonlinear wave trains fig 14c the ncom model captures some of the variability but the steep waves lag the observational result by several hours fig 14d the suntans model results show increasing variability and wave steepness with increasing grid resolution and more closely approximate the wave timing when compared to the observations although still with some lag fig 14e f observed and modeled power spectra of wave velocities 100 m below the surface at station m6 in 3000 m depth are relatively similar between the ncom sun2300 and sun250 models fig 15 this indicates that the nested suntans model is correctly reproducing the spectrum of wave velocities on the eastern boundary which are propagating primarily towards the west into the domain the spectra show peaks at the inertial period f the k l and m 2 tidal periods as expected for this system at the tc1 mooring in 450 m of water the modeled and observed wave velocity spectra 100 m below the surface are similar for periods near the k 1 and m 2 tidal periods fig 15 however for frequencies higher than one hr 1 the observations show significantly more energy a trend which is also apparent in the temperature profiles which show higher richness than the model results fig 14 the high resolution suntans model with 250 m resolution produces roughly 100 times the energy at high frequencies than the 2300 m results but it is still 100 times lower than the observations the energy flux is compared at sites m6 and tc1 fig 16 the energy flux here is computed as f h 0 u h p d z where p is calculated from the density deviation from the time averaged background state ρ b the results at m6 in deeper water show some differences in the time variable energy flux however the time integrated energy flux is similar for the ncom model and the sun2300 model indicating the boundary forcing adequately injects the total energy into the nested domain fig 16a c at the tc1 site the wave energy flux is quite variable in time between the models due to differences in timing fig 16b however in the time integrated sense the energy flux is under predicted by the ncom model over predicted by the low resolution suntans model and well predicted by the high resolution suntans model fig 16d the amplitude and phase of the waves on june 3 2011 based on the displacement of the 20 c isotherm at site tc1 are compared between the observations ncom model and suntans model fig 17 the ncom model underpredicts the amplitude compared to the observations while the suntans model more closely approximates the observation with increasing grid resolution fig 17a the phase lag between the ncom model and the observations is approximately five hours while the suntans model shows similar phase lag for different resolutions fig 17b to evaluate the effect of different boundary and stratification parameters on the model results a series of runs were conducted including the sunlow sunhigh suntcor and sunnof models table 2 the sunlow model has only low frequency forcing with no internal tide forcing u 0 for all boundary conditions the sunhigh model has only high frequency forcing such that low frequency nudging is off f l 0 with no low frequency velocities in the model boundary conditions u 0 the temperature profile of the observations shows denser isotherm spacing and a larger overall vertical temperature difference compared to the ncom and suntans models fig 14 which could potentially affect the internal wave propagation to evaluate the effects of the difference in average stratification between the ncom model and the observational data at tc1 the suntcor model adjusts the temperature in the initial condition and boundary conditions by a constant offset δ t z over the model domain computed as the difference between the average temperature over the observation period and the ncom model at site tc1 i e δ t t o b s t n c o m as shown in fig 18a the results for the barotropic velocities for the sun250 model are in good agreement with the observations while the sunlow model contains the mean flows but no tidal oscillations and the sunhigh model contains the tidal oscillations but no mean flow fig 18a the temperature profile of the sun250 model compares well with the observations fig 18b c although as expected the sunlow model temperature profile contains very few wave oscillations fig 18d in the absence of low frequency forcing the sunhigh model results are similar to the sun250 model results but with some lag in the wave arrival times fig 18e the suntcor model shows higher amplitude waves compared to the sun250 model and these waves are closer in amplitude to the observations shown in fig 18b indicating that the initial stratification is important at this location 5 discussion 5 1 nonhydrostatic effects an important consideration in systems with high frequency internal waves is the importance of nonhydrostatic pressure under certain circumstances internal waves can represent a dynamical balance between nonlinearity and nonhydrostasy dispersion and thus may require computationally expensive nonhydrostatic simulations to be well resolved model results at various grid resolutions show little effect of including the nonhydrostatic pressure at low grid resolution but differences are most pronounced at high resolution in both wave timing and amplitude fig 19 following vitousek and fringer 2011 to lowest order the ratio of numerical to physical dispersion is k λ g 2 where k is a constant dependent on the discretization of the governing equations and λ g δ x h e is grid lepticity where δ x is the horizontal grid spacing and h e is the effective depth of the internal interface and depends on the normal modes solution for a continuous stratification although it is given roughly by the upper mixed layer depth fig 20c in simulations of a freshwater lake a grid lepticity of less than roughly two appears to be required to correctly resolve nonlinear internal waves dorostkar et al 2017 a result which is similar to our findings over the south china sea model domain the grid lepticity is smaller within the interior of the domain where the water is deeper and the grid resolution is finer fig 20a b over the entire model domain the grid lepticity is less than two for 90 of the grid cells at the highest resolution sun250 while the lepticity is larger than two over most of the domain for the coarsest resolution sun2300 this explains why the nonhydrostatic and hydrostatic solutions are nearly identical for the low resolution model sun2300 fig 19b and significantly different for the high resolution model sun250 fig 19e in addition the modeled wave energy flux is better approximated at lower grid lepticities fig 16d thus if nonhydrostatic effects are important in the model creating a grid lepticity less than two is important for correctly resolving the nonlinear wave effects and wave energy flux 5 2 frequency effects the model framework contains several important time scales the most important being the filtering frequency used to separate high and low frequency motions this frequency should be selected such that it represents an area of relatively low energy as in this south china sea example the value of τ filt was selected to be less than the inertial period fig 15 thus in this example inertial and tidal motions are deemed high frequency motions and longer period processes such as mesoscale eddies are deemed low frequency motions to analyze the effect of the forcing terms on the high and low frequency components we time average the momentum equation 1 and decompose variables into their high ρ and low passed ρ components ignoring viscous effects the equation for low frequency motions given by 26 u t u u u u 2 ω u 1 ρ 0 p g ρ 0 ρ k ˆ f l where f m f l f d f l f l from 5 since the damping term contains the high frequency forcing so that f d 0 taking 1 and substituting u u u and subtracting 26 gives the equation for the high frequency motions 27 u t u u u u u u 2 ω u 1 ρ 0 p g ρ 0 ρ k ˆ f d eqs 26 and 27 show that f l is forcing the mean flow u while f d acts to force and damp the high frequency flow u however the nonlinear terms in 26 and 27 are capable of forcing low frequency flows from the wave motions and similarly the mean flows can interact with wave flows to force or damp wave flows these effects can be seen in the comparison of runs with full forcing sun250 low frequency only sunlow and high frequency only sunhigh fig 18c d e the sunlow model contains primarily low frequency motions but some higher frequencies are still present from waves generated within the model domain the sunhigh model contains much of the variability related to the full forcing but the timing of the internal waves is delayed compared to the observations internal wave properties such as phase speed damping and nonlinear steepening are very sensitive to the background stratification which we define as the time averaged density ρ b ρ ρ 0 and the buoyancy frequency n b in the south china sea example at station tc1 the different models show smaller differences in background density but large differences in buoyancy frequency fig 11a b in general the observations have a higher buoyancy frequency than the ncom and suntans models fig 11a the suntans models show decreased phase speeds for the three models with internal waves present sun250 sunhigh suntcor a result of the changes to the density and buoyancy frequency profiles due to mixing near the surface fig 17 in addition nonlinear solitons have an amplitude dependent phase speed which likely contributes to the observed phase lag for models with lower amplitude waves rayson et al 2018 these results indicate that correctly approximating the density profiles appears to be critical to modeling internal waves in this case the large scale ncom model temperature profile was under predicting the density and buoyancy frequency which leads to an underprediction of the phase speed the nested suntans model inherits this under prediction of density and buoyancy frequency through the initial and boundary conditions added to this are mixing effects which are most pronounced at the surface in order to correct for this difference a heat model could be added to the suntans model to account for heat fluxes at the surface additionally the low frequency temperature profile could be nudged using f t in 2 the approach proposed here is based on integrated energy flux and allows for only one mean direction of energy flux propagation θ at each grid cell which is applied at the boundaries and damping layer i e f ˆ x y t and u l s x y z t this approach is expected to work well under general oceanic conditions with a well defined mean direction however for cases of strong opposing wave directions of the same frequency or complex energy flux directed in multiple directions this approach may not perform well in this case an alternate approach would be to decompose the energy flux into frequency ω and direction θ i e f ˆ x y t ω θ and u l s x y z t ω θ but this approach would likely require a very large computational effort by forcing the barotropic velocities to match the large scale flow at the suntans model boundary 18 there is the potential for barotropic wave reflections off the model boundaries the velocities perpendicular to the boundary are nearly identical e w boundary u and n s boundary v between the suntans and ncom models supp mat figure sm5 in the direction parallel to the boundary there are some small mismatches between suntans and ncom most likely arising from free surface waves the presence of these small spurious barotropic waves on the exterior does not affect the results much in the model interior figs 12 13 14 especially since the focus of this present paper is on internal waves a flather type boundary condition flather 1976 could be imposed on the high frequency barotropic velocity to modify 18 and further reduce these small oscillations however we found it unnecessary particularly given the need to introduce another tunable parameter representing the clamping time scale relative to the flather condition additionally the application of the barotropic tides could also be applied by computing u l s in 18 using tidal constituents janeković and powell 2012 for some applications this could improve the tidal signal by computing u l s at each time step instead of the coarser time step of the boundary condition file however for this model testing with 15 min vs 1 h boundary file time step yielded nearly identical results therefore we included u l s as a time series in 18 5 3 recipes for modeling internal waves we have presented a generalized framework for computation of internal waves in a one way nested grid the model features four parameters that must be selected based on the model conditions which include the sponge layer damping time scale τ d the sponge layer width δ d the low frequency filtering time scale τ f i l t and the low frequency forcing time scale τ l in addition the nested model grid domain location model extent l w grid spacing δ x δ y δ z forcing time step and spin up time scale θ r are important parameters for the sponge region τ d is selected based on the wave speed of the internal waves optimal results for dissipation in the sponge layer were obtained for δ d λ 2 and τ d 0 04 c δ d fig 6 where c is the wave speed an important consideration is the location of the region where most of the internal wave energy is dissipated in the model domain which can be used to select these parameters in the south china sea example most of the dissipation is located in the shallow western edge of the domain where because of the shorter internal tides in shallow water a much smaller sponge layer width δ d can be used for the low frequency components τ f i l t should be selected so that it separates the low and high frequency components of the flow at a location in the frequency spectrum of little energy for the south china sea example τ f i l t was selected to be less than the coriolis frequency to render inertial and tidal motions as high frequency motions that are computed by suntans rather than nudged by ncom the low frequency forcing time scale τ l should be selected to ensure that it is large enough to have minimal damping effect on the high frequency motions fig 7 while still being small enough to nudge the flow towards ncom fig 18a for the south china sea example τ l 4 τ f i l t was selected to balance these goals the model grid has a significant effect on the model results horizontal grid resolution affects the grid lepticity δ x h e which should be less than about 2 to adequately resolve nonhydrostatic processes for the south china sea example at the tc1 mooring this was important for correctly modeling the wave amplitude timing and energy flux figs 16 17 although not evaluated in this study the nonlinear steepening length scale may be important in selecting the horizontal grid resolution as nonlinear internal waves require sufficient distance to steepen rayson et al 2018 vertical resolution is less important as long as the mode shapes are adequately resolved and 50 to 100 vertical z levels were used with good results in addition the model grid extents should be large enough to minimize boundary effects which influence the waves within some buffer region δ b fig 9 when waves are propagating perpendicular to model boundaries and the damping region is smoothed so as to not create reflections there is very little effect and δ b 0 however when waves are propagating parallel to model boundaries along a sponge damping region the waves are slowed and modified near the boundary based on model simulations the distance of this effect is approximately one wavelength δ b λ the boundary conditions are forced by a time series with discrete time step δ t for an example barotropic tidal signal in deep water fig 21a the maximum velocity imposed on the boundaries creates the largest internal waves which steepen in the domain interior to obtain at least 90 of this velocity magnitude a record length of at least 12 days and a time step of less than three hours is required for this site fig 21b in addition the model must be run for sufficient length to spin up the barotropic tides and dissipate inconsistencies related to the initial conditions for this south china sea example the spin up time was approximately seven days supp mat figure sm3 finally the background stratification has a significant effect on the model results small differences in the temperature profiles o 1 c and hence background density ρ b can create large differences in the buoyancy frequency n b 2 and in the propagation speed fig 11 when wave features are propagated over large distances this results in large errors in wave arrival times in addition the effect of internal waves is to induce mixing which was most pronounced near the surface to improve the model results either a surface heat flux model or a low frequency nudging of the background temperature field could be employed thus while we employ state of the art computational nesting methods specific wave events are not modeled as accurately as would be desired this discrepancy is primarily due to the background stratification inherited from the large scale model which critically affects the solution of modeled internal waves a significant advantage of our method is the reduced computational cost from running a regional model vs a much larger grid required to simulate the internal tide as an example if we wish to model the internal waves at some location in the south china sea the first approach would be to model the entire south china sea basin and luzon ridge zhang et al 2011 modeled this region with 11 m grid cells on a variable unstructured grid with two km resolution in the south china sea basin and an 11 s time step due to small grid cells in other parts of the domain the second approach as proposed in this study using nesting at similar two km resolution sun2300 has only 2 4 m grid cells and uses a time step size of 100 s assuming a scalable computation this represents a factor of 40 savings in computational resources for a similar level of resolution 6 summary we present a method for seamless integration of one way nested ocean model for internal waves the method relies on a large scale model which provides initial and boundary conditions to the nested model the forcing is applied through a decomposition of time scales low and high and depth averaging barotropic and baroclinic and requires four constants the barotropic component of flow is applied at the boundaries a low frequency nudging is applied to the model interior to nudge the solution towards the low frequency flow of the large scale model finally the wave component of flow high frequency baroclinic is applied at the model boundaries where the wave energy enters the domain a sponge layer on the model exterior allows incoming wave energy to pass through but damps wave energy exiting the domain to prevent reflections we apply the model framework to idealized two and three dimensional model domains in 3000 m depth with stratification and forcing typical of the south china sea the results are as expected showing waves generated along one boundary propagating through the interior and damped on the opposing end we further apply the model to a realistic model application in the south china sea the results show reasonable agreement with field observations of both in situ temperature and energy flux measurements and also satellite observations of wave timing and shape the model results are sensitive to horizontal grid spacing and it is found that a grid lepticity less than two is required to accurately model nonhydrostatic effects which are important in some areas of the domain the model domain size should be selected such that at a minimum space is allowed for the sponge damping layers and a buffer region where waves propagating parallel to the sponge layer are distorted differences in wave timing and amplitude are observed between the observations and model results the most likely reason for this is differences in the background stratification due to either incorrect large scale initial boundary conditions or enhanced mixing the model could likely be improved through inclusion of a surface heat flux model and nudging of the background temperature profile to observational data thus even with state of the art computational methods the wave results are less accurate than would be desired and critically depend on the stratification inherited from the large scale model this highlights the need for large scale ocean models to incorporate observational data especially related to stratification in the ocean interior overall the results are in good agreement with observations when the large scale model provides accurate forcing and present a flexible framework for application of regional nested models for internal wave simulations the method relies on four parameters first the time scale of low pass filtering τ filt should be selected so that it separates the low and high frequency components of the flow at a location in the frequency spectrum with little energy here selected as four times the m 2 tidal period second the time scale of low frequency forcing τ l should be selected to ensure that it is large enough to have minimal damping effect on the high frequency motions here selected as τ l 4 τ f i l t third the width of the sponge damping layer δ d is based on the internal tide wavelength δ d λ 2 fourth the time scale of sponge damping τ d is selected based on the wave speed of the wave τ d 0 04 c δ d a significant advantage of this method is the reduced computational cost from running a regional nested model vs a much larger grid required to simulate the internal tide over a basin in the south china sea example presented here this represents a factor of 40 savings in computational resources for a similar level of resolution acknowledgments this research is supported by onr grants n00014 15 1 2287 of n00014 16 1 2256 of and n00014 17 wx0 1702 dsk this manuscript was improved by helpful discussions with stephen monismith leif thomas and kristen davis validation data was generously provided by ren chieh lien and chris jackson eric mayer yun zhang bing wang and kurt nelson all contributed code for the modeling we gratefully acknowledge the us army research laboratory dod supercomputing resource center for computer time on excalibur and especially thank the diligent staff at the hpc help desk for their support appendix a supplementary data supplementary material related to this article can be found online at http dx doi org 10 1016 j ocemod 2019 101462 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
24011,to resolve nonlinear internal wave motions in regional domains we propose a one way ocean model nesting framework in which a nonhydrostatic model is nested within a regional model the nesting scheme produces internal tides that propagate freely into and out of the nonhydrostatic model domain with no reflections while nudging the low frequency motions we demonstrate the method with idealized test cases and a field scale simulation of the south china sea the method relies on four parameters the time scale of low pass filtering the time scale of low frequency forcing the time scale of sponge damping and the width of the sponge damping layer we present guidelines for selecting these four parameters based on the background stratification the internal tides from the large scale model steepen through nonlinear and nonhydrostatic effects and reasonably match observations at a fraction of the computational cost needed with a single uncoupled model the model critically relies on the background stratification inherited from the large scale model which in this case leads to an underpredicted phase speed and amplitude in the nested model result this highlights the need for large scale ocean models to incorporate observational stratification data especially in the ocean interior 1 introduction internal gravity waves are typically on the scale of hundreds of kilometers in space hours in time and are ubiquitous in the world s oceans they are thought to contribute an estimated one terawatt of deep sea turbulent dissipation which is understood to play a crucial role in the ocean s global redistribution of heat and momentum alford et al 2015 and serve important ecological functions to mixing in the nearshore environment walter et al 2012 internal waves are generated by tidal motions over topography and can steepen into nonlinear solitary like wave trains alford et al 2010 when these internal waves encounter shallow depths such as the shelf they steepen and break often forming dissipative bores arthur and fringer 2016 walter et al 2012 recently global ocean models such as the hycom model martin 2000 have been improved to explicitly resolve the internal tidal motions arbic et al 2018 and thus contain long wavelength internal tides in their solution at a regional scale hydrostatic ocean models can reasonably estimate net dissipation from internal tides carter et al 2008 suanda et al 2018 however the coarse spatial resolution o km and hydrostatic pressure formulation of these models limit the development of nonlinear solitary wave trains and computation of breaking and dissipation kinematics vitousek and fringer 2011 to model high resolution internal wave processes a grid nesting approach is often employed where boundary conditions from the large scale model are used to force a higher resolution nested model blayo and rousseau 2016 chen et al 2013 kumar et al 2015 pickering et al 2015 warn varnas et al 2015 for grid nesting problems with waves the fundamental problem is that wave information propagates differently in the parent grid versus the child grid and thus the two models contain inconsistent information amplitude wavelength timing etc at the outgoing model boundary blayo and rousseau 2016 vitousek and fringer 2011 this difference typically leads to reflection of spurious wave information back into the domain by design the child grid and parent grid compute different physical processes appropriate for their scale i e non hydrostatic vs hydrostatic pressure resolved vs parameterized turbulence and thus this is a fundamental challenge for modeling waves in nested domains a related problem relates to the need to match the outgoing direction of propagation at open boundaries imposing an inconsistent wave form on a model boundary can send incorrect wave information into the domain for example if we wish to model a wave exiting the nested domain and naively apply these wave velocities on the outgoing model boundary a spurious wave will be forced into the domain when there is a mismatch between the large scale and nested domain internal tide solution previous work has attempted to solve this basic nesting problem with several approaches the first is two way nesting which attempts to solve for a uniform solution between model types velocity pressure scalars and has been successfully demonstrated for the nonhydrostatic navier stokes equations blayo and rousseau 2016 while two way nesting is often employed for hydrostatic ocean models with different models including ncom ko et al 2008 and roms warner et al 2010 solving a two way nested large scale nonhydrostatic ocean model is under development with the croco model auclair et al 2018 and would require orders of magnitude more computational power the second approach is to use adaptive mesh refinement amr methods which adaptively refine the grid in specific areas and have been demonstrated in stratified flows koltakov and fringer 2013 and large scale oceanic flows using the roms agrif debreu et al 2012 and icom piggott et al 2008 models the disadvantage of a single grid with grid stretching is that grid quality can compromise accuracy when there is significant stretching and the time step is dictated by the finest grid spacing which can become limiting in some cases the third approach is to employ one way nesting and radiate wave energy out of the domain using a radiative function which requires solving for the wave speed and properties which are then propagated out of the model domain this has been applied to both surface gravity waves chapman 1985 atmospheric gravity waves klemp and durran 1983 and hydrostatic oceanic boundary conditions including internal waves carter and merrifield 2007 marsaleix et al 2006 mason et al 2010 suanda et al 2018 a variant of this approach uses an adaptive algorithm where inward and outward information fluxes are treated separately marchesiello et al 2001 the limitation of these approaches is that the approximation of wave speeds and properties vertical and horizontal wavenumbers inevitably leads to errors in the radiative boundary conditions which lead to spurious waves reflecting into the domain it can also be computationally expensive to solve for spectral wave number properties frequency wavenumber amplitude and estimate the propagation speed we present a novel approach which splits the parent grid solution into low frequency mean and high frequency wave components at some dividing frequency the low frequency components are applied as dirichlet boundary conditions on the nested grid and nudged over the model domain the wave component is forced for wave energy propagating into the domain as dirichlet boundary conditions on the nested grid boundary and with a sponge damping layer near these boundaries which damps outgoing wave energy this combination provides an efficient and practical method to conduct one way multiscale model nesting our aim here is to model specific wave events although the approach is similar to laboratory scale modeling studies of stratified fluids winters 2016 winters and de la fuente 2012 it has not been applied to a regional scale ocean model we show that the accuracy of the nesting strategy is dependent on four time scales that are related to physical properties of the system most importantly we show that for realistic applications the results depend critically on accuracy of the large scale stratification in this manuscript we present the theoretical framework section 2 and illustrate the methods in a series of idealized test cases section 3 we then present a realistic model application in the south china sea nesting the suntans model fringer et al 2006 into the us navy ncom model martin 2000 focusing on modeling internal waves section 4 a discussion section 5 and summary section 6 follow 2 theoretical framework the general setting we wish to model is a three dimensional nested model domain ω n within a large scale model ω l s and overlapping boundary γ fig 1a with oceanic flows at a range of temporal and spatial scales including internal wave propagation and steepening the nested model domain has a higher resolution grid and bathymetry than the large scale model and may have enhanced physics appropriate for the higher resolution i e non hydrostatic vs hydrostatic pressure to accomplish the nesting we implement a method to nudge the suntans model velocity temperature and salinity u t s to a large scale model velocity temperature and salinity u l s t l s s l s at appropriate time scales using a horizontal source term in the momentum equations the suntans model solves the three dimensional nonhydrostatic reynolds averaged navier stokes equations under the boussinesq approximation along with the scalar transport equations and the continuity equation fringer et al 2006 1 u t u u 2 ω u 1 ρ 0 p g ρ 0 ρ k ˆ h ν h h u z ν v u z f m 2 t t u t h κ h h t z κ v t z f t 3 s t u s h γ h h s z κ v s z f s 4 u 0 where u u v w is the velocity vector ω is the earth s angular velocity vector g is gravitational acceleration ν κ γ are the eddy viscosity eddy thermal diffusivity and eddy mass diffusivity respectively k ˆ is the vertical unit vector and h and v are the horizontal and vertical components of a variable or operator notation is summarized in table 1 to compute density ρ an equation of state is employed as a function of temperature t and or salinity s a horizontal source term in the momentum equations f m x y z t is introduced such that 5 f m f l f d where f l is the low frequency forcing and f d is the sponge layer high frequency damping term fig 1b likewise the temperature and salinity equations are nudged at low frequency with f t and f s respectively to perform our nesting approach it is necessary to separate velocity density and pressure fields into reference background and spatially varying components following kang and fringer 2012 the total density is decomposed with 6 ρ x y z t ρ 0 ρ b x y z t ρ x y z t where ρ 0 is the constant reference density ρ b ρ ρ 0 is the time averaged background density with time low pass ρ filtering and ρ is the perturbation density due to wave motions the pressure is split into its hydrostatic p h and nonhydrostatic q parts with p p h q and p h is further decomposed into constant background and perturbation components with 7 p h p 0 p b p ρ 0 g η z g z η ρ b d z g z η ρ d z where η is the free surface we separate the flow into barotropic and baroclinic velocity components u u u where denotes the deviation from the barotropic velocity and the barotropic velocity is given by the depth average 8 u 1 h d η u h d z 1 h u h where d η d z the depth integration of a quantity u h is the horizontal component of the velocity vector the bottom is defined at z d x y and the total water depth is given by h d η adopting a decomposition of variables into time high ρ and time low pass ρ components with filtering time scale τ f i l t the velocity can be further decomposed into both temporally u u and vertically u u varying components with 9 u u u u u u u which represent from left to right the low frequency barotropic and baroclinic and the high frequency barotropic and baroclinic terms respectively the free surface is also be decomposed with 10 η η 0 η 0 η which represent from left to right low frequency barotropic high frequency barotropic and high frequency baroclinic motions here η 0 is the low pass time filtered free surface η 0 is the spatially low pass filtered free surface obtained by spatially filtering η at a scale much larger than the internal wave wavelength λ and thus η is the high frequency high wavenumber component of the free surface for oceanic conditions the time scale of separation is taken at a frequency less than the inertial frequency f and the primary tidal frequencies thus the high frequency components primarily consist of propagating internal gravity waves with frequency ω within the range f 2 ω 2 n b 2 where the buoyancy frequency n b is given by n b 2 g ρ 0 ρ b z the separation of time scales is treated separately in the large scale grid ls and the suntans computations for the large scale grid ω l s the simulations are run in a one way nesting routine and therefore the large scale results are known a priori to running the coupled model therefore u l s is calculated with a simple low pass filter using a fourier transform and u l s is the deviation from this mean for the suntans domain ω n u is computed using a running average of previous results following wolfram and ringler 2017 the ordinary differential equation for a two level impulse function is 11 d u d t 1 τ f i l t u u where τ f i l t is the low pass time filtering scale a running mean at time step n 1 is thus approximated with the backward in time discretization 12 u n 1 u n δ t 1 τ f i l t u n 1 u n this filter is equal to a standard single pole filter smith 1997 and as long as δ t τ f i l t 12 typically has significantly lower error than a moving box filter to obtain the energy flux which is needed in the sponge layers the kinetic energy density is decomposed into the barotropic horizontal baroclinic and cross terms kang and fringer 2012 13 e k 0 1 2 ρ 0 u 2 v 2 14 e k 1 2 ρ 0 u 2 v 2 w 2 15 e k 0 ρ 0 u u v v the depth integrated barotropic and baroclinic energy flux terms ignoring small unclosed and diffusion terms are given by 16 f 0 u h e k 0 u h h ρ 0 g η u h p u h q 17 f u h e k u h e k 0 u h p u h q where in oceanic conditions the dominant terms in 16 are u h h ρ 0 g η and u h p due to hydrostatic pressure work and the dominant term in 17 is u h p or the energy flux due to the wave induced hydrostatic pressure work kang and fringer 2012 2 1 boundary conditions we apply dirichlet boundary conditions along γ using a decomposed horizontal velocity u b c from the large scale grid figs 1b 2 the boundary velocities were chosen to be consistent with the lateral sponge layer such that boundary is forced by the barotropic and low frequency baroclinic components of the large scale forcing and the baroclinic wave velocity associated with waves propagating into the domain 18 u b c u l s u l s u l s f ˆ u l s t ˆ r where the operator 19 f ˆ x y t 1 if f l s l p n 0 0 if f l s l p n 0 selects only the wave velocities associated with energy flux propagation into the domain and n is the unit vector directed out of the nested model domain normal to the model boundary thus the baroclinic high frequency motions are modified depending on the wave direction but all other components of the flow are specified by the large scale model for the computation of 19 the low pass filtered flux in time f l s l p is spatially smoothed lp using a gaussian filter to a scale much larger than the internal wave wavelength λ to eliminate variations in f ˆ smaller than the sponge layer width we also initialize the boundary velocities smoothly in time with a ramping function t ˆ r 1 exp t θ r where t is the model run time and θ r is a ramping time scale the boundary conditions for tracers t b c s b c are upwind set to the large scale value for flow entering the domain t l s s l s and the free surface is not constrained as we apply dirichlet boundary conditions on the horizontal velocity fig 2 to ensure long term volume consistency very few boundary grid cells in a corner of the domain are specified as neumann velocity boundary conditions instead of the dirichlet condition in 18 with a specified free surface η b c η 0 η 0 which is the instantaneous spatially low pass filtered free surface from the large scale model i e removing iw motions 2 2 lateral sponge layer to minimize wave reflections from the nested model boundaries a sponge damping term is implemented in the horizontal momentum equation eq 1 of the form 20 f d u f ˆ t ˆ r u l s τ d d ˆ 21 d ˆ x exp 4 r x δ d where τ d is the damping time scale and d ˆ modulates the strength of damping spatially which decreases exponentially at a length scale of δ d from the nearest boundary and r is the distance from the nearest boundary fig 1c the damping timescale is thus a local value given by d ˆ x τ d which means that the strength of the damping of outgoing high frequency baroclinic components decreases exponentially when moving from the boundary into the domain by nudging the suntans baroclinic wave velocity u to the large scale baroclinic wave velocities associated with incoming waves only f ˆ u l s incoming waves and flows are allowed to propagate into the domain while all outgoing baroclinic wave motions are damped fig 2 this approach is very flexible and allows waves to propagate at different directions and frequencies within the domain while damping energy near the boundaries thereby avoiding spurious reflections from the boundaries 2 3 low frequency forcing the low frequency motions in the suntans model u are forced by the large scale low frequency motions u ls with the forcing term f l in eq 5 over the entire model domain using 22 f l u u ls τ l t ˆ r where τ l is the low frequency forcing time scale similarly the low frequency suntans model temperature and salinity t s are nudged towards the large scale low frequency variables t l s s l s using 23 f t t t ls τ t t ˆ r 24 f s s s ls τ s t ˆ r where τ t τ s are the low frequency forcing time scales for temperature and salinity respectively this framework assumes the model is initialized with t l s and s l s but zero velocity and free surface forcing terms for the low frequency components 22 24 are initially zero because of t ˆ r the sponge damping term 20 has t ˆ r applied to u l s and initially damps all baroclinic high frequency motions u arising from spurious motions from the initial conditions and slowly transitions to forcing to the large scale boundary motions u l s to force wave velocities and not temperature or salinity the method proposed here relies on four parameters the time scale of low pass filtering τ filt the time scale of low frequency forcing τ l the time scale of sponge damping τ d and the width of the sponge damping layer δ d 3 idealized test cases model performance we apply a series of test cases to demonstrate the model capabilities and the effects of the four boundary parameters under different conditions including a two dimensional x z idealized internal wave generation case and a three dimensional idealized south china sea 3 1 base case internal wave propagation in two dimensions we compute an idealized model simulation of internal wave propagation in one direction x z plane based on an idealization of observations from the south china sea ramp et al 2010 zhang et al 2011 specifically we include a typical south china sea density field given by ρ b 2 1 x 1 0 4 ρ 0 t and temperature t 23 36 exp z 44 12 293 12 3 13 fig 3a with buoyancy frequency n coriolis frequency f 5 7 10 5 rad s typical for this latitude and an imposed forcing frequency at the m 2 tidal period ω m 2 2 π t m 2 t m 2 12 42 h for most of the depth f 2 ω m 2 2 n 2 thus this environment is capable of sustaining propagating internal waves fig 3b we apply this test case on a 520 0 75 3 km length l width w depth h domain with grid resolution δ x δ y 250 m which meets the requirements of vitousek and fringer 2011 for minimal numerical dispersion and 100 vertical z levels table 2 with enhanced resolution near the surface and exponentially increasing vertical spacing with a growth rate of 1 08 the time step δ t is dictated by stability of internal gravity wave propagation at speed c i w and given by the courant number constraint c i w δ t δ x 1 boundary conditions are dirichlet at the x east west boundaries and periodic in y north south an idealized large scale velocity field u l s is applied to the model as described above using boundary conditions a lateral sponge layer and low frequency forcing and contains the four components in 17 u l s u l s u l s u l s u l s with u l s u u l s u u l s u t i d e k t i d e k t i d e cos k t i d e x ω m 2 t ϕ t i d e 25 u l s u i w f z k i w k i w cos k i w x ω m 2 t ϕ i w where f z is the mode 1 velocity eigenfunction u 0 1 ms 1 u 0 u t i d e 0 06 ms 1 u i w 0 50 ms 1 and ϕ t i d e ϕ i w 0 waves in this system consist of both barotropic internal tides with wavenumber k tide and baroclinic internal waves with wavenumber k iw waves propagate in the positive x direction θ 0 and have a horizontal wavelength λ x 2 π k i w x 130 km consistent with the stratification and forcing frequency we apply forcing on the baroclinic flow consistent with mode 1 horizontal oscillations for the imposed stratification solved using the eigenmode problem fringer and street 2003 kundu and cohen 2008 fig 3c the mode 1 eigenfunction is approximated as f z α 1 exp z h 2 δ α 2 where α 1 1 4248 α 2 0 1831 δ 394 4 m h 2 9 995 m in 25 which is a result specific to the density field and sponge layer width δ d 100 km damping time scale τ d 800 s in 19 and low frequency forcing time scale and filtering time τ l τ f i l t 2 t m 2 in 21 in the y direction v l s 0 05 ms 1 and to obtain geostrophic balance in the periodic domain a forcing f l y f u t ˆ r is applied in 1 and 5 the results for the free surface and isopycnal displacement show internal waves emanating from the left wavemaker region and propagating in the positive x direction with steepening leading to the formation of a train of rank ordered solitary like internal gravity waves fig 4a b these waves are subsequently damped within the right sponge layer the barotropic velocity u u is nearly uniform within the domain and is not influenced by the wavemaker or damping layers fig 4c the long2dnodamp model is twice as long as the base2d model but with no lateral sponge layers δ d 0 table 2 which serves as a reference solution for this domain i e freely propagating waves with no damping layers or boundary effects the results are similar to the base2d model but the waves are more nonlinear exhibiting faster steepening this is because the sponge layer in the base2d model is forced with a linear wave 25 which delays wave steepening for x 100 km in this case the long2d model is twice as long at the base2d model but does contain lateral sponge layers table 2 which serves as a reference solution for the case of no sponge layer damping or boundary at the right side of the domain x 420 km the results are nearly identical to the base2d model except in the sponge damping region 420 x 520 km this indicates reflections from the sponge damping region are negligible an important feature of this framework the density anomaly and wave velocity fields ρ u for the base2d model show characteristic internal wave generation consistent with u i w 0 50 ms 1 in 20 and subsequent propagation steepening and damping at the model edges fig 4d e thus the expected behavior of the lateral sponge layer 20 21 in wave generation and dissipation is confirmed the baroclinic mean flow u within the thermocline is oriented in the direction of wave propagation in the model interior but is aligned with the wave propagation in the areas of wave dissipation fig 4e this trend is reversed in the upper and lower parts of the water column rotational dispersion due to coriolis reduces the number of waves in the train the case without rotation f 0 shows development of seven to eight solitons supporting material figure sm1 rather than the four to five as shown in fig 4 the time averaged barotropic energy flux f x 0 increases with x over the model domain due to the applied geostrophic balance in this periodic domain fig 5a while its variation in time shows oscillations emanating from the boundaries due to the applied barotropic tides fig 5b the time averaged baroclinic energy flux f x is relatively constant near the left boundary decreases slightly over the model domain with small oscillations developing due to poorly resolved high wavenumber nonlinearities and subsequently is damped in the sponge damping layers near the right model boundary fig 5a the time varying baroclinic energy flux shows wave energy emanating from the left boundary wave energy propagation across the model domain and damping along in the left model sponge layer fig 5c we estimate the phase speed c based on the background density profile ρ b using the first mode solution to the eigenvalue problem to estimate the base phase speed c 0 fringer and street 2003 kundu and cohen 2008 and adjusting for coriolis effects with c c 0 1 f ω 2 1 2 helfrich 2007 for the m 2 tidal period in this problem c 3 13 m s 1 which closely follows the propagation speed of individual waves fig 5c 3 2 sponge layer dynamics we apply a test case similar to the south china sea base case but with a model domain length of 293 km and horizontal grid size 1000 m and vary the damping time scale τ d and sponge layer width δ d in 20 to evaluate the effects of wave damping and reflection from the sponge layer sponge2d table 2 the shorter domain length is used to evaluate the sponge layer effects on linear waves where the generated waves on the boundary do not have sufficient distance to steepen we also evaluate the sponge layer effects on nonlinear waves by comparing the results of the base2d model with the long2d model we define the reflection coefficient as a measure of the amount of wave energy reflected by the sponge layer and solid boundary and is given by r f 1 f x l δ d f where f is the wave energy flux in the absence of a sponge layer or reflections at x l δ d because f is a bulk parameter integrating wave energy flux propagating in both the positive and negative directions f decreases when wave energy is reflected from the sponge layer we normalize τ d by the travel time for a wave to traverse the sponge layer δ d c for τ d 0 02 c δ d reflections are significant due to reflected wave energy from the sponge layer itself fig 6 for τ d 0 08 c δ d reflections can occur from the model boundary if δ d is not large enough to dissipate all wave energy while the parameter space is covered for linear waves the base2d model with nonlinear waves also follows a similar trend with near zero r f fig 6 thus while the sponge layer performed well over a range of values to minimize reflections from both mechanisms the optimal values appear to be τ d 0 04 c δ d and δ d λ 2 within this framework waves with wavelengths smaller than λ higher modes will be filtered while waves with wavelengths larger than λ lower modes will not a larger value of δ d λ has a broader region of low r f and would be more applicable to a broad spectrum of wave frequencies but requires more computational space used for the damping region these results of competing reflection mechanisms are qualitatively similar to wave damping theory for surface gravity waves zhang et al 2014 a second test case is developed using the same setup as the base case but with wave energy flux coming from both boundaries with the same period but different magnitudes wave velocities on the right boundary are half as large as those on the left the incoming wave energy flux is effectively generated in the model at both boundaries and the outgoing wave energy flux is efficiently damped supp mat figure sm2 additional test cases varying the wave period t iw and magnitude u iw produced similar results this demonstrates that the sponge layer formulation is robust to efficiently damp outgoing waves while allowing incoming waves to propagate into the domain over a wide range of wave conditions 3 3 low frequency forcing variables the primary parameter governing the low frequency forcing is the time scale τ l to test how τ l affects the high frequency waves we modify the base case to have a 260 km domain with 1000 m horizontal grid spacing table 2 setting all velocities to zero except the wave velocity u i w 0 5 ms 1 propagating in the positive x direction we then vary τ l in 22 and record the wave energy flux one wavelength into the domain the wave attenuation coefficient is α 1 f x l δ d f and f is the wave energy flux without low frequency forcing τ l the expression for α is similar for the reflection coefficient r f but in this case is a measure of wave energy damping since all wave energy flux is in the positive x direction and there are no reflections from the damping layer or boundary wave attenuation α decreases as τ l increases and for τ l τ f i l t 1 α 0 01 fig 7 the total propagation distance must be considered when setting τ l since α here is the damping over one wavelength in practice τ l should be as large as possible so as not to influence the wave energy but small enough to control the low frequency flow 3 4 internal waves mean flow in three dimensions we extend the base case model to three dimensions in a 520 520 3 km x y z domain with a refined stretched grid in the model exterior sponge damping region δ d 100 km τ d 800 s and a constant interior horizontal resolution of 500 m table 2 dirichlet boundary conditions are applied on all four boundaries forcing parameters are the same as the base case except the internal waves and barotropic tide are oriented at θ 45 the barotropic mean flow u is spatially constant and the mean free surface η 0 is tilted consistent with geostrophic flow fig 8a the barotropic tide u is directed at an angle θ 45 with the tidal surface η 0 consistent with a poincaré wave with rotation fig 8b both u and u and associated components of the free surface show little sign of interaction with the sponge layer in the outer damping region the wave velocity u at the surface and wave component of free surface η show internal waves emanating from the south and western boundaries propagating and steepening towards the upper right and dissipating along the top and right boundaries fig 8c the wave velocity u and the interface displacement ζ defined as deviation of the isopycnal originating at 366 m depth of o 100 m show similar trends fig 8d these results show that the model framework can effectively simulate a range of flow conditions in an idealized three dimensional environment with several complex flow elements an important consideration in the three dimensional framework is a lateral damping effect which occurs when waves propagate parallel to a sponge layer boundary fig 9 we illustrate this effect with waves propagating parallel to a sponge layer and a solid wall on the opposite boundary 3d lateral model table 2 the waves are distorted near the edge of the sponge layer fig 9b which leads to a slowed propagation speed and loss of amplitude when compared to a two dimensional simulation with the same resolution fig 9a the two dimensional simulation serves here as a reference solution for waves propagating without lateral effects thus there is a buffer region of influence δ b which equals approximately one wavelength for waves propagating parallel to a sponge layer when waves are propagating normal to a boundary this effect does not occur assuming τ d and δ d are selected in a way that eliminates reflections 4 realistic model application test case 4 1 model setup the previous section showed how the model framework is implemented in two and three dimensions under a range of idealized flow conditions we now apply the framework to a realistic application in the south china sea and nest suntans within the us navy ncom model martin 2000 fig 10a the large scale ncom model is implemented here at 1 50 horizontal resolution approximately 2 3 km with 41 vertical sigma z levels with 12 sigma levels on the shelf and hourly output the model domain location and 475 x 390 km extent is chosen because of the strong internal tides over the luzon straight which propagate westward and steepen and are well documented in the literature alford et al 2015 farmer et al 2009 lien et al 2014 ramp et al 2010 zhang et al 2011 the suntans model is run with 100 stretched vertical z levels with a stretched grid in the model exterior sponge damping region and a constant horizontal resolution in the interior fig 10b c to investigate the effects of grid resolution in the suntans model interior horizontal resolutions of 250 525 1100 and 2300 m are employed table 2 to apply the boundary conditions and sponge layer the f ˆ operator 29 is computed from f l s l p which uses a low pass filter in time to obtain the energy flux over multiple waves and spatially filtered at a scale of the domain size w to smooth out variability at scales smaller than λ fig 10d the model is run during may 26 to june 5 2011 a time period when observational data is available from the tc1 mooring in a depth of 450 m lien et al 2014 fig 10 due to bathymetric differences and approximations in the spatial and temporal interpolations from ncom to the suntans open boundaries volume is not exactly conserved in the model domain thus to ensure long term volume consistency several boundary grid cells in a corner of the domain are specified as neumann velocity boundary conditions with a specified free surface η l p η 0 η 0 which is the instantaneous spatially low pass filtered free surface from the ncom model this eliminates wave effects but retains large scale dynamics including the barotropic tide for this example these points were chosen as the three grid cells in the southwest corner of the domain as they are within the wave damping region fig 10d in this way waves generated by small mismatches in η l p and η are damped the suntans model is initialized with the ncom temperature salinity and spatially smoothed free surface output and initial velocities are set to zero kinematic viscosity is set to ν 1 0 4 m2 s in the vertical direction and ν h 1 m2 s in the horizontal direction similar to values used in other studies at this scale kang and fringer 2012 zhang et al 2011 boundary conditions are interpolated at hourly time intervals model spinup is approximately seven days for the barotropic tides to equilibrate to the large scale model from initial velocities set to zero supp mat figure sm3 the results from initializing the model with interpolated velocities show initial high frequency waves from inconsistencies in the model solution and a slightly faster equilibration to the barotropic tides of approximately five days supp mat figure sm3 additional testing with 15 min boundary conditions gave nearly identical results as the 1 h boundary conditions based on these results the hourly boundary condition forcing and zero velocity initial conditions were chosen and we focus on results of the last three days of the ten day simulations for the remainder of the runs 4 2 validation at station tc1 the different models show small differences in background density but large differences in buoyancy frequency fig 11a b in general the observations have a higher buoyancy frequency than the ncom and suntans models the suntans model is initiated with ncom stratification fig 11a we approximate the linear mode 1 phase speed c for these average profiles using the eigenvalue solution adjusted for frequency as in the base case the results show the phase speed for both k 1 and m 2 periods is lower for the ncom and suntans models compared to the observations fig 11c the results for η and t at 140 m depth show propagation and steepening of the internal tide from east to west and steepening of the internal waves with propagation fig 12 surface deflections from the internal waves are o 0 1 m and internal temperature deflections are o 2 c consistent with previous simulations of this region zhang et al 2011 outgoing internal wave motions were absorbed in the sponge layers and there is no observable internal wave reflection supp mat figure sm4 the results of the suntans simulations at a coarse 2300 m and fine 250 m grid resolution are compared to both aerial imagery from modis nasa worldview application https worldview earthdata nasa gov operated by the nasa goddard space flight center esdis project and the ncom model on june 4 2011 fig 13a b c d the wave crest from the ncom result lags behind the modis data due to lack of resolution of nonlinear effects which lead to slower wave propagation the suntans result matches modis better due to improved resolution of nonlinear effects as discussed in vitousek and fringer 2011 coarse resolution gives rise to numerical dispersion which produces the unphysical wave train in ncom fig 13e the dispersion is weaker in the suntans model with coarse resolution fig 13f and is eliminated with higher resolution fig 13g the results are also compared to observational mooring data at location tc1 fig 10a lien et al 2014 the barotropic velocities from the observations compare well to the model results for ncom and suntans especially in the ew direction fig 14a while in the ns direction capture the long term average flow but miss some of the m 2 tidal flow fig 14b this mismatch in the ns direction is potentially a result of inaccuracy in the ncom model whose results are very similar to the suntans model temperature observations show internal waves passing by the tc1 mooring with both broad waves and sharply nonlinear wave trains fig 14c the ncom model captures some of the variability but the steep waves lag the observational result by several hours fig 14d the suntans model results show increasing variability and wave steepness with increasing grid resolution and more closely approximate the wave timing when compared to the observations although still with some lag fig 14e f observed and modeled power spectra of wave velocities 100 m below the surface at station m6 in 3000 m depth are relatively similar between the ncom sun2300 and sun250 models fig 15 this indicates that the nested suntans model is correctly reproducing the spectrum of wave velocities on the eastern boundary which are propagating primarily towards the west into the domain the spectra show peaks at the inertial period f the k l and m 2 tidal periods as expected for this system at the tc1 mooring in 450 m of water the modeled and observed wave velocity spectra 100 m below the surface are similar for periods near the k 1 and m 2 tidal periods fig 15 however for frequencies higher than one hr 1 the observations show significantly more energy a trend which is also apparent in the temperature profiles which show higher richness than the model results fig 14 the high resolution suntans model with 250 m resolution produces roughly 100 times the energy at high frequencies than the 2300 m results but it is still 100 times lower than the observations the energy flux is compared at sites m6 and tc1 fig 16 the energy flux here is computed as f h 0 u h p d z where p is calculated from the density deviation from the time averaged background state ρ b the results at m6 in deeper water show some differences in the time variable energy flux however the time integrated energy flux is similar for the ncom model and the sun2300 model indicating the boundary forcing adequately injects the total energy into the nested domain fig 16a c at the tc1 site the wave energy flux is quite variable in time between the models due to differences in timing fig 16b however in the time integrated sense the energy flux is under predicted by the ncom model over predicted by the low resolution suntans model and well predicted by the high resolution suntans model fig 16d the amplitude and phase of the waves on june 3 2011 based on the displacement of the 20 c isotherm at site tc1 are compared between the observations ncom model and suntans model fig 17 the ncom model underpredicts the amplitude compared to the observations while the suntans model more closely approximates the observation with increasing grid resolution fig 17a the phase lag between the ncom model and the observations is approximately five hours while the suntans model shows similar phase lag for different resolutions fig 17b to evaluate the effect of different boundary and stratification parameters on the model results a series of runs were conducted including the sunlow sunhigh suntcor and sunnof models table 2 the sunlow model has only low frequency forcing with no internal tide forcing u 0 for all boundary conditions the sunhigh model has only high frequency forcing such that low frequency nudging is off f l 0 with no low frequency velocities in the model boundary conditions u 0 the temperature profile of the observations shows denser isotherm spacing and a larger overall vertical temperature difference compared to the ncom and suntans models fig 14 which could potentially affect the internal wave propagation to evaluate the effects of the difference in average stratification between the ncom model and the observational data at tc1 the suntcor model adjusts the temperature in the initial condition and boundary conditions by a constant offset δ t z over the model domain computed as the difference between the average temperature over the observation period and the ncom model at site tc1 i e δ t t o b s t n c o m as shown in fig 18a the results for the barotropic velocities for the sun250 model are in good agreement with the observations while the sunlow model contains the mean flows but no tidal oscillations and the sunhigh model contains the tidal oscillations but no mean flow fig 18a the temperature profile of the sun250 model compares well with the observations fig 18b c although as expected the sunlow model temperature profile contains very few wave oscillations fig 18d in the absence of low frequency forcing the sunhigh model results are similar to the sun250 model results but with some lag in the wave arrival times fig 18e the suntcor model shows higher amplitude waves compared to the sun250 model and these waves are closer in amplitude to the observations shown in fig 18b indicating that the initial stratification is important at this location 5 discussion 5 1 nonhydrostatic effects an important consideration in systems with high frequency internal waves is the importance of nonhydrostatic pressure under certain circumstances internal waves can represent a dynamical balance between nonlinearity and nonhydrostasy dispersion and thus may require computationally expensive nonhydrostatic simulations to be well resolved model results at various grid resolutions show little effect of including the nonhydrostatic pressure at low grid resolution but differences are most pronounced at high resolution in both wave timing and amplitude fig 19 following vitousek and fringer 2011 to lowest order the ratio of numerical to physical dispersion is k λ g 2 where k is a constant dependent on the discretization of the governing equations and λ g δ x h e is grid lepticity where δ x is the horizontal grid spacing and h e is the effective depth of the internal interface and depends on the normal modes solution for a continuous stratification although it is given roughly by the upper mixed layer depth fig 20c in simulations of a freshwater lake a grid lepticity of less than roughly two appears to be required to correctly resolve nonlinear internal waves dorostkar et al 2017 a result which is similar to our findings over the south china sea model domain the grid lepticity is smaller within the interior of the domain where the water is deeper and the grid resolution is finer fig 20a b over the entire model domain the grid lepticity is less than two for 90 of the grid cells at the highest resolution sun250 while the lepticity is larger than two over most of the domain for the coarsest resolution sun2300 this explains why the nonhydrostatic and hydrostatic solutions are nearly identical for the low resolution model sun2300 fig 19b and significantly different for the high resolution model sun250 fig 19e in addition the modeled wave energy flux is better approximated at lower grid lepticities fig 16d thus if nonhydrostatic effects are important in the model creating a grid lepticity less than two is important for correctly resolving the nonlinear wave effects and wave energy flux 5 2 frequency effects the model framework contains several important time scales the most important being the filtering frequency used to separate high and low frequency motions this frequency should be selected such that it represents an area of relatively low energy as in this south china sea example the value of τ filt was selected to be less than the inertial period fig 15 thus in this example inertial and tidal motions are deemed high frequency motions and longer period processes such as mesoscale eddies are deemed low frequency motions to analyze the effect of the forcing terms on the high and low frequency components we time average the momentum equation 1 and decompose variables into their high ρ and low passed ρ components ignoring viscous effects the equation for low frequency motions given by 26 u t u u u u 2 ω u 1 ρ 0 p g ρ 0 ρ k ˆ f l where f m f l f d f l f l from 5 since the damping term contains the high frequency forcing so that f d 0 taking 1 and substituting u u u and subtracting 26 gives the equation for the high frequency motions 27 u t u u u u u u 2 ω u 1 ρ 0 p g ρ 0 ρ k ˆ f d eqs 26 and 27 show that f l is forcing the mean flow u while f d acts to force and damp the high frequency flow u however the nonlinear terms in 26 and 27 are capable of forcing low frequency flows from the wave motions and similarly the mean flows can interact with wave flows to force or damp wave flows these effects can be seen in the comparison of runs with full forcing sun250 low frequency only sunlow and high frequency only sunhigh fig 18c d e the sunlow model contains primarily low frequency motions but some higher frequencies are still present from waves generated within the model domain the sunhigh model contains much of the variability related to the full forcing but the timing of the internal waves is delayed compared to the observations internal wave properties such as phase speed damping and nonlinear steepening are very sensitive to the background stratification which we define as the time averaged density ρ b ρ ρ 0 and the buoyancy frequency n b in the south china sea example at station tc1 the different models show smaller differences in background density but large differences in buoyancy frequency fig 11a b in general the observations have a higher buoyancy frequency than the ncom and suntans models fig 11a the suntans models show decreased phase speeds for the three models with internal waves present sun250 sunhigh suntcor a result of the changes to the density and buoyancy frequency profiles due to mixing near the surface fig 17 in addition nonlinear solitons have an amplitude dependent phase speed which likely contributes to the observed phase lag for models with lower amplitude waves rayson et al 2018 these results indicate that correctly approximating the density profiles appears to be critical to modeling internal waves in this case the large scale ncom model temperature profile was under predicting the density and buoyancy frequency which leads to an underprediction of the phase speed the nested suntans model inherits this under prediction of density and buoyancy frequency through the initial and boundary conditions added to this are mixing effects which are most pronounced at the surface in order to correct for this difference a heat model could be added to the suntans model to account for heat fluxes at the surface additionally the low frequency temperature profile could be nudged using f t in 2 the approach proposed here is based on integrated energy flux and allows for only one mean direction of energy flux propagation θ at each grid cell which is applied at the boundaries and damping layer i e f ˆ x y t and u l s x y z t this approach is expected to work well under general oceanic conditions with a well defined mean direction however for cases of strong opposing wave directions of the same frequency or complex energy flux directed in multiple directions this approach may not perform well in this case an alternate approach would be to decompose the energy flux into frequency ω and direction θ i e f ˆ x y t ω θ and u l s x y z t ω θ but this approach would likely require a very large computational effort by forcing the barotropic velocities to match the large scale flow at the suntans model boundary 18 there is the potential for barotropic wave reflections off the model boundaries the velocities perpendicular to the boundary are nearly identical e w boundary u and n s boundary v between the suntans and ncom models supp mat figure sm5 in the direction parallel to the boundary there are some small mismatches between suntans and ncom most likely arising from free surface waves the presence of these small spurious barotropic waves on the exterior does not affect the results much in the model interior figs 12 13 14 especially since the focus of this present paper is on internal waves a flather type boundary condition flather 1976 could be imposed on the high frequency barotropic velocity to modify 18 and further reduce these small oscillations however we found it unnecessary particularly given the need to introduce another tunable parameter representing the clamping time scale relative to the flather condition additionally the application of the barotropic tides could also be applied by computing u l s in 18 using tidal constituents janeković and powell 2012 for some applications this could improve the tidal signal by computing u l s at each time step instead of the coarser time step of the boundary condition file however for this model testing with 15 min vs 1 h boundary file time step yielded nearly identical results therefore we included u l s as a time series in 18 5 3 recipes for modeling internal waves we have presented a generalized framework for computation of internal waves in a one way nested grid the model features four parameters that must be selected based on the model conditions which include the sponge layer damping time scale τ d the sponge layer width δ d the low frequency filtering time scale τ f i l t and the low frequency forcing time scale τ l in addition the nested model grid domain location model extent l w grid spacing δ x δ y δ z forcing time step and spin up time scale θ r are important parameters for the sponge region τ d is selected based on the wave speed of the internal waves optimal results for dissipation in the sponge layer were obtained for δ d λ 2 and τ d 0 04 c δ d fig 6 where c is the wave speed an important consideration is the location of the region where most of the internal wave energy is dissipated in the model domain which can be used to select these parameters in the south china sea example most of the dissipation is located in the shallow western edge of the domain where because of the shorter internal tides in shallow water a much smaller sponge layer width δ d can be used for the low frequency components τ f i l t should be selected so that it separates the low and high frequency components of the flow at a location in the frequency spectrum of little energy for the south china sea example τ f i l t was selected to be less than the coriolis frequency to render inertial and tidal motions as high frequency motions that are computed by suntans rather than nudged by ncom the low frequency forcing time scale τ l should be selected to ensure that it is large enough to have minimal damping effect on the high frequency motions fig 7 while still being small enough to nudge the flow towards ncom fig 18a for the south china sea example τ l 4 τ f i l t was selected to balance these goals the model grid has a significant effect on the model results horizontal grid resolution affects the grid lepticity δ x h e which should be less than about 2 to adequately resolve nonhydrostatic processes for the south china sea example at the tc1 mooring this was important for correctly modeling the wave amplitude timing and energy flux figs 16 17 although not evaluated in this study the nonlinear steepening length scale may be important in selecting the horizontal grid resolution as nonlinear internal waves require sufficient distance to steepen rayson et al 2018 vertical resolution is less important as long as the mode shapes are adequately resolved and 50 to 100 vertical z levels were used with good results in addition the model grid extents should be large enough to minimize boundary effects which influence the waves within some buffer region δ b fig 9 when waves are propagating perpendicular to model boundaries and the damping region is smoothed so as to not create reflections there is very little effect and δ b 0 however when waves are propagating parallel to model boundaries along a sponge damping region the waves are slowed and modified near the boundary based on model simulations the distance of this effect is approximately one wavelength δ b λ the boundary conditions are forced by a time series with discrete time step δ t for an example barotropic tidal signal in deep water fig 21a the maximum velocity imposed on the boundaries creates the largest internal waves which steepen in the domain interior to obtain at least 90 of this velocity magnitude a record length of at least 12 days and a time step of less than three hours is required for this site fig 21b in addition the model must be run for sufficient length to spin up the barotropic tides and dissipate inconsistencies related to the initial conditions for this south china sea example the spin up time was approximately seven days supp mat figure sm3 finally the background stratification has a significant effect on the model results small differences in the temperature profiles o 1 c and hence background density ρ b can create large differences in the buoyancy frequency n b 2 and in the propagation speed fig 11 when wave features are propagated over large distances this results in large errors in wave arrival times in addition the effect of internal waves is to induce mixing which was most pronounced near the surface to improve the model results either a surface heat flux model or a low frequency nudging of the background temperature field could be employed thus while we employ state of the art computational nesting methods specific wave events are not modeled as accurately as would be desired this discrepancy is primarily due to the background stratification inherited from the large scale model which critically affects the solution of modeled internal waves a significant advantage of our method is the reduced computational cost from running a regional model vs a much larger grid required to simulate the internal tide as an example if we wish to model the internal waves at some location in the south china sea the first approach would be to model the entire south china sea basin and luzon ridge zhang et al 2011 modeled this region with 11 m grid cells on a variable unstructured grid with two km resolution in the south china sea basin and an 11 s time step due to small grid cells in other parts of the domain the second approach as proposed in this study using nesting at similar two km resolution sun2300 has only 2 4 m grid cells and uses a time step size of 100 s assuming a scalable computation this represents a factor of 40 savings in computational resources for a similar level of resolution 6 summary we present a method for seamless integration of one way nested ocean model for internal waves the method relies on a large scale model which provides initial and boundary conditions to the nested model the forcing is applied through a decomposition of time scales low and high and depth averaging barotropic and baroclinic and requires four constants the barotropic component of flow is applied at the boundaries a low frequency nudging is applied to the model interior to nudge the solution towards the low frequency flow of the large scale model finally the wave component of flow high frequency baroclinic is applied at the model boundaries where the wave energy enters the domain a sponge layer on the model exterior allows incoming wave energy to pass through but damps wave energy exiting the domain to prevent reflections we apply the model framework to idealized two and three dimensional model domains in 3000 m depth with stratification and forcing typical of the south china sea the results are as expected showing waves generated along one boundary propagating through the interior and damped on the opposing end we further apply the model to a realistic model application in the south china sea the results show reasonable agreement with field observations of both in situ temperature and energy flux measurements and also satellite observations of wave timing and shape the model results are sensitive to horizontal grid spacing and it is found that a grid lepticity less than two is required to accurately model nonhydrostatic effects which are important in some areas of the domain the model domain size should be selected such that at a minimum space is allowed for the sponge damping layers and a buffer region where waves propagating parallel to the sponge layer are distorted differences in wave timing and amplitude are observed between the observations and model results the most likely reason for this is differences in the background stratification due to either incorrect large scale initial boundary conditions or enhanced mixing the model could likely be improved through inclusion of a surface heat flux model and nudging of the background temperature profile to observational data thus even with state of the art computational methods the wave results are less accurate than would be desired and critically depend on the stratification inherited from the large scale model this highlights the need for large scale ocean models to incorporate observational data especially related to stratification in the ocean interior overall the results are in good agreement with observations when the large scale model provides accurate forcing and present a flexible framework for application of regional nested models for internal wave simulations the method relies on four parameters first the time scale of low pass filtering τ filt should be selected so that it separates the low and high frequency components of the flow at a location in the frequency spectrum with little energy here selected as four times the m 2 tidal period second the time scale of low frequency forcing τ l should be selected to ensure that it is large enough to have minimal damping effect on the high frequency motions here selected as τ l 4 τ f i l t third the width of the sponge damping layer δ d is based on the internal tide wavelength δ d λ 2 fourth the time scale of sponge damping τ d is selected based on the wave speed of the wave τ d 0 04 c δ d a significant advantage of this method is the reduced computational cost from running a regional nested model vs a much larger grid required to simulate the internal tide over a basin in the south china sea example presented here this represents a factor of 40 savings in computational resources for a similar level of resolution acknowledgments this research is supported by onr grants n00014 15 1 2287 of n00014 16 1 2256 of and n00014 17 wx0 1702 dsk this manuscript was improved by helpful discussions with stephen monismith leif thomas and kristen davis validation data was generously provided by ren chieh lien and chris jackson eric mayer yun zhang bing wang and kurt nelson all contributed code for the modeling we gratefully acknowledge the us army research laboratory dod supercomputing resource center for computer time on excalibur and especially thank the diligent staff at the hpc help desk for their support appendix a supplementary data supplementary material related to this article can be found online at http dx doi org 10 1016 j ocemod 2019 101462 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
24012,despite the recent success achieved by the unstructured grid schism semi implicit cross scale hydroscience integrated system model in multi resolution studies its skill in simulating the baroclinic eddying ocean needs to be further improved in particular the classical 2nd order transport schemes for estuaries and coastal zones are too dissipative to resolve the baroclinic dynamics associated with strong boundary currents such as meso scale meanders and eddies to close this gap this paper presents a newly designed 3rd order finite volume transport scheme based on the weighted essentially non oscillatory weno formalism this new scheme strikes a delicate balance among accuracy efficiency and monotonicity for the transport in the eddying regime idealized numerical benchmark experiments demonstrate that the weno scheme is very effective in limiting numerical diffusion the scheme is then applied in a realistic simulation of the gulf stream and the surrounding circulation further confirming its capability of resolving baroclinic meso scale eddies and meanders this new high order transport scheme is therefore ideal for extending the ability of schism to study cross scale baroclinic applications that range from the river dynamics to the eddying ocean processes keywords schism eddying regime weno baroclinic gulf stream 1 introduction the past two decades have seen wider adoption of unstructured grid ug models in coastal ocean modeling mainly due to ug s flexibility in 1 resolving bathymetric and geometric features and 2 local refinement de refinement for the area of interest disinterest a key remaining challenge for ug model development is to fully realize its potentials in cross scale baroclinic problems a review by danilov 2013 indicates that until recently large scale ocean applications are still lacking in the ug model community baroclinic applications of ug models had been mostly confined to coastal regions chen et al 2003 fringer et al 2006 zhang and baptista 2008 and rarely extended to the eddying ocean a recent effort in allowing eddy resolving capability in a coastal ug model thetis was presented by kärnä et al 2017 but realistic applications are still lacking recently a good progress has been made by a triad of global ug models fesom wang et al 2014 mpas ringler et al 2013 and icon korn 2017 in simulating global scale processes with eddying permitting resolving capability different from the coastal models the designs of these ug models aim at simulating the global ocean circulation rather than finer scale coastal processes one example of such designs is the spherical centroidal voronoi tessellations used in mpas ocean ringler et al 2013 which handle multi resolutions well for the more regularly shaped deep ocean however this design is still not cost effective in resolving highly irregular geometries in estuaries and coastal regions so a coupling with a ug coastal model is often required to relate global basin scale ocean processes to local estuarine coastal processes the gap between global ug models and coastal ug models motivates us to extend our coastal model domain further into the deep ocean there are two benefits from this endeavor as far as model coupling is concerned 1 for the global ocean model its scale contrast can be reduced which not only reduces the size of the computational grid but more importantly also increases the maximum allowable time step because the most stringent cfl condition is generally associated with the finest grid resolution 2 for the coastal model the uncertainties from the ocean boundary conditions b c can also be reduced for example in our recent study on the chesapeake bay ye et al 2018 we observed errors that most likely came from the ocean boundary conditions derived from the global hycom reanalysis product 1 1 url https www hycom org dataserver gofs 3pt0 reanalysis last accessed jan 1 2019 even though the model boundary was set 100 km beyond the shelf break and 200 km from the chesapeake bay mouth the effect from the ocean b c can still be felt the goal of this study is to enhance ug coastal models capability in resolving the baroclinic circulation in the open ocean eddying regime with the numerical implementation done in schism semi implicit cross scale hydroscience integrated system model schism wiki previously high order momentum advection schemes that use viscosity filters as stability function have been developed in schism by zhang et al 2016 to address the issue of spurious modes exhibiting as noises in the velocity fields for recent applications in the eddying regime e g yu et al 2017 the high order momentum schemes were used along with a 2nd order tracer transport scheme although the latter s numerical dissipation is considered sufficiently small for estuarine applications we find its utility in simulating meso scale eddies and meanders is still less than satisfactory this is likely because the intrinsic physical dissipation is large in estuaries due to bottom friction and irregular geometry bathymetry which may effectively mask numerical dissipations in an eddying ocean however the forces are largely balanced ringler et al 2013 and physical dissipation is at a much lower level therefore the numerical dissipation needs to be carefully controlled and higher order transport schemes are generally deemed necessary in the eddying regime to improve schims s capability in the eddying ocean we seek a transport scheme that is 1 easily extendable to 3rd or higher order accuracy 2 monotonic or nearly so 3 robust and efficient on generic unstructured grid without geometric constraints such as orthogonality or mesh quality for the first requirement on accuracy our recent applications on kuroshio currents yu et al 2017 showed that the existing 2nd order transport scheme only needs modest improvement so a third order scheme is preferred over a fourth order scheme because the former s leading truncation errors are in the form of diffusion instead of dispersion greatly reducing the need for applying additional explicit diffusion to avoid unphysical oscillation hirsch 2007 the second requirement on monotonicity is necessitated by the large spatial gradients in dissolved substances concentrations in nearshore regions or across the base of the mixed layer in the deep ocean which may trigger unwanted numerical oscillations although various filtering techniques have been proposed before shchepetkin and mcwilliams 2005 they are not ideal because they reduce the effective order of accuracy the third requirement on robustness and efficiency is mainly to accommodate cross scale applications which are characterized by transitions from coarser and relatively uniform meshes in the ocean to finer and relatively irregular meshes in coastal regions previously the performance of various advective schemes for ug ocean modeling have been summarized by hanert et al 2004 and budgell et al 2007 the latter showed that the computational cost can increase significantly with the requirement of higher order accuracy and the techniques for limiting numerical oscillations for our purpose of improving schims s eddy resolving capability only a modest improvement on horizontal advection accuracy would be required but we do need a scheme design that is flexible enough to balance between accuracy and other above mentioned requirements especially the computational cost for which purpose the weno scheme serves as a good candidate or compromise long et al 2008 introduced by liu et al 1994 the weno scheme has been applied in various fields including computational fluid dynamics astrophysics and biology the review by shu 2009 provides a comprehensive list of the related applications in the field of ocean modeling it has been adopted by the structured grid sg model roms on sediment transport in the vertical dimension warner et al 2008 after the extension to ugs friedrich 1998 hu and shu 1999 weno has also attracted attention in the field of morphodynamics canestrelli et al 2010 guérin et al 2016 to our knowledge weno has not been applied to the horizontal tracer transport in ug models in this paper we present a numerical scheme based on weno formalism for the horizontal transport in the ug modeling system schism aiming at better resolving meso scale eddies and meanders in the eddying regime the following text is structured as follows section 2 briefly introduces the discretization of transport equation in schism then explains the challenges of adapting weno in schism and how we design the finite volume solver accordingly section 3 presents the results from a numerical benchmark confirming the superior behavior of the new scheme compared to lower order schemes 2nd order schemes are commonly used for the estuarine tracer transport in coastal models section 4 applies the new scheme to simulate the gulf stream and its surrounding circulation then compares the performance between the new scheme and a classical 2nd order tvd scheme see details in section 3 1 of zhang et al 2016 followed by the conclusions in section 5 2 methods since the focus of this study is the transport solver we will not repeat all governing equations in schism for which readers are referred to zhang et al 2016 the transport equation and its discretization in schism are described below to provide a background 2 1 transport equation and operator splitting the transport equation solved by schism reads 1 t t u t z κ t z q where t is the concentration of a generic tracer κ is vertical eddy diffusivity m2 s 1 q includes all source and sink terms u is the three dimensional velocity vector u v w m s 1 we have neglected horizontal diffusion in eq 1 for brevity as its implementation is straightforward zhang and baptista 2008 in a finite volume framework fig 1 the discretized form of eq 1 that centers on prism i is expressed as 2 t i n 1 t i n δ t v i j s i q j t j t i a i δ t v i κ j t j z j i i κ j t j z j i i δ t v i v i q d v where i is the prism index j is the face index n is the time level t i is the average concentration of prism i defined at prism centers v i is the volume of prism i s i denotes all faces of prism i q j is the total volumetric flux through face j positive going outward prism i which is already known after solving the momentum equation at the current step t j is the average concentration over face j such that the mass flux through face j can be expressed as q j t j for the diffusion term i e the 2nd term on the right hand side rhs of eq 2 additional symbols are introduced for convenience fig 1b i is the prism on top of prism i i is the prism below prism i i i is the top face of prism i i i is the bottom face of prism i κ is vertical diffusivity defined at top and bottom faces of a prism a i is the the projected area of prism i onto the horizontal plane the time levels of t on the rhs will be specified next schism applies an operator splitting procedure in solving the transport equation zhang et al 2016 based on the discretized transport equation 2 t i n is advanced to t i n 1 in three steps horizontal advection vertical advection and vertical diffusion plus other terms each step deals with different terms on the rhs of eq 2 step 1 horizontal advection 3 t i 1 t i n δ t v i j s i h q j t j t i step 2 vertical advection 4 t i 2 t i 1 δ t v i j s i v q j t j t i step 3 vertical diffusion and source 5 t i n 1 t i 2 a i δ t v i κ j t j z j i i n 1 κ j t j z j i i n 1 δ t v i v i q d v where s i h denotes the faces of prism i that are parallel to the z coordinate s i v denotes the top face i i and the bottom face i i of prism i which may or may not be parallel to the horizontal plane fig 1b for the horizontal advection a 2nd order tvd scheme is sufficient for estuarine applications zhang et al 2016 ye et al 2018 but not for the eddying ocean section 4 3 so the main task here is to find a better approximation for the average interface value t j in eq 3 this step is treated explicitly because horizontal advection is generally not the bottleneck for efficiency since eq 3 is an explicit scheme its time step is limited by 6 δ t v i j s i h q j c where s i h is a subset of s i h denoting the faces with outflowing fluxes the cfl condition for weno schemes is typically c 0 6 0 8 zhou et al 2001 and a smaller value c 0 4 0 6 is often used for solutions with discontinuity titarev and toro 2005 here a default value of 0 5 is chosen which is adjustable as a parameter in schism for the vertical advection an implicit 2nd order tvd scheme was previously designed for schism zhang et al 2016 to avoid small time steps and maintain accuracy we have not applied higher order schemes for step 2 and step 3 because the grid spacing in the vertical dimension is typically much smaller than that in the horizontal dimension our previous applications showed that 2nd order accuracy is sufficient to resolve sharp pycnocline in non eddying regimes ye et al 2016 2018 early applications of schism in the eddying regime section 4 show that the skill in capturing vertical temperature gradient is similar to the data assimilated hycom but more challenging applications e g on submarine canyon processes are needed to further assess the vertical transport scheme 2 2 transport solver design the existing weno schemes on ug mainly comprise of two types liu and zhang 2013 with type i focusing on non linear stability and type ii focusing on achieving higher order accuracy the scheme designed in this section belongs to type i compared to existing schemes we put more emphasize on simplifying the stencil to balance between the desired accuracy and efficiency robustness so that the scheme is suitable for cross scale estuarine ocean baroclinic applications on generic ugs carried out by parallel machines for this purpose the following specific considerations were taken into account in the design 1 third order accuracy is necessary and potential extension to fourth order is possible 2 violations in monotonicity should be kept minimal preferably without adding artificial diffusion 3 simple stencils are preferred to maintain the parallel efficiency 4 the algorithm should be robust for all grid configurations as found in oceans and estuaries these considerations are reflected in the weno procedure described below 2 2 1 numerical flux and stencils a key step in the weno procedure is to reconstruct the interfacial mass flux namely the term q j t j in the discretized horizontal advection equation 3 following hu and shu 1999 a q point gaussian integration is used 7 q j t j k 1 q q j ξ k t x g k y g k where q j t j is the mass flux through face j x g k y g k are the global coordinates of the k th quadrature point on face j with ξ k as its weight in schism tracer concentration is treated as vertically homogeneous within each prism zhang et al 2016 so we only need to decide the locations of quadrature points on each side in the horizontal dimension since gaussian integration is precise for polynomials of degree 2 q 1 or less q 1 one point at the side center and ξ 1 1 are chosen when linear polynomials are needed whereas q 2 two points on each side with distances of l 2 3 from the side center where l is the side length and ξ 1 ξ 2 0 5 are chosen when quadratic polynomials are needed hu and shu 1999 the use of gauss legendre quadrature rule with all positive weights does not introduce stability problem huybrechs 2009 the subscript k is dropped in the subsequent description because the reconstruction procedure is identical for each quadrature point note that the interfacial volume flux q j can be taken out of the summation on the r h s of eq 7 because it is independent of tracer concentration and treated as a constant on face j for simplicity and efficiency the upwind flux is used i e the face value is only reconstructed with stencils centered on the upwind prism fig 2 this seems sufficient for our applications in the eddying ocean e g the test case in section 4 other numerical fluxes have been applied for weno schemes titarev and toro 2005 assessing the cost effectiveness of different numerical fluxes for ocean transport are left for future studies the next step is to reconstruct the face value t x g y g in eq 7 by prismatic values this is illustrated by a sample horizontal mesh in fig 2 in this horizontal view we will use elements instead of prisms to avoid confusion based on an appropriate choice of stencil a polynomial of degree n denoted as p n can be constructed since we are using the upwind flux these stencils are centered on the upwind element of x g y g specifically we look for stencils associated with the tier 1 neighborhood which is defined by the elements that share 1 or 2 nodes with the center element fig 2 starting from the center element the stencil candidates are constructed by sequentially listing the elements in this neighborhood in counter clockwise order both 2nd order and 3rd order reconstructions are discussed next a 2nd order reconstruction requires a linear polynomial which can be expressed as 8 p s 1 1 x y a s b s c s lc where s is the stencil index among all possible stencils centered on the upwind element of x g y g c a s b s c s t is a group of coefficients to be determined by the information from the elements within stencil s in this case three elements are needed for example the potential choices in fig 2 are 0 1 2 0 2 3 0 12 1 inserting the concentration values and coordinates from the three elements of stencil s into eq 8 we have 9 t t k 1 x k y k 3 3 a s b s c s 3 1 l ˆ c where k 1 2 3 is the local element index within stencil s t t k is the concentration values at element centers the polynomial s coefficients c a s b s c s t are found by c l ˆ 1 t similarly a 3rd order reconstruction requires a quadratic polynomial 10 p s 2 1 x y x 2 x y y 2 a s b s c s d s e s f s lc in this case 6 elements are need which are also chosen from tier 1 for example the potential choices in fig 2 are 0 1 2 3 4 5 0 2 3 4 5 6 0 12 1 2 3 4 the procedure of finding the coefficients c a s b s c s d s e s f s t is similar to that in the linear case which will not be repeated tier 1 neighborhood is chosen because 1 it generally allows quadratic polynomials that lead to 3rd order accuracy on a triangular mesh 2 it simplifies the mpi communication among the neighboring sub domains 3 our experience suggests that tier 1 stencils are generally less prone to oscillations than tier 2 stencils center element tier 1 neighbors tier 1 neighbors of neighbors to reduce truncation error normalized local coordinates are used within each big stencil element 0 and other gray elements in fig 2 such that 1 the coordinate origin coincides with the centroid of the center element 2 the x and y axes are scaled without rotation by the distance between the origin and the furthest neighbor s centroid then the value at each quadrature point is reconstructed by a linear combination of the polynomials from all stencils 11 t x g y g s 1 n s w s p s n x g y g where s is the stencil index n s is the total number of stencils w s is the linear weight of each stencil which is to be determined later p s n is a polynomial of degree n based on the s th stencil the polynomials p s n in eq 11 are exact at element centers and n 1 th order accurate elsewhere moreover it is possible to achieve n 2 th order accuracy through an optimal choice of the weighting w s hu and shu 1999 2 2 2 weights although higher order accuracy can be achieved by weight optimization hu and shu 1999 we choose not to do so the rationale is that 3rd order accuracy is sufficient for our present applications more importantly optimal weighting is not always guaranteed on a generic ug with abrupt element size transitions and skew elements which reduces the robustness of the algorithm in cross scale applications we may adopt optimal weighting if there is a need for 4th order accuracy in future applications currently w s is solely determined by the concentration gradient within each stencil following hu and shu 1999 but without optimal weighting 12 w s w s s 1 n s w s with w s 1 ϵ i s s 2 where ϵ is a small number to prevent division by zero we find ϵ 10 5 10 7 generally works well for the 3rd order scheme with only negligible violations in monotonicity whereas ϵ 10 4 tends to generate noticeable oscillations near large tracer gradients i s is a smoothness indicator following hu and shu 1999 s formulation for linear stencils leading to a 2nd order reconstruction the smoothness indicator is 13 i s s 1 a p s 1 x 2 p s 1 y 2 d s where p s 1 is a linear polynomial based on the s th stencil a is the area of the centering element d s is an integration over the area of the centering element for quadratic stencils leading to a 3rd order reconstruction the smoothness indicator is 14 i s s 1 a p s 2 x 2 p s 2 y 2 d s 2 p s 2 x 2 2 2 p s 2 x y 2 2 p s 2 y 2 2 d s where p s 2 is a quadratic polynomial based on the s th stencil in realistic applications of schism the 3rd order reconstruction based on quadratic polynomials is applied by default which occasionally downgrades to the 2nd order reconstruction in abnormal cases 2 2 3 abnormal cases the use of a generic unstructured grid and a small neighborhood for reconstruction can lead to abnormal cases which must be carefully handled to ensure robustness one abnormal case is related to the availability of linear quadratic polynomials fig 3 illustrates an invalid stencil for constructing a linear polynomial 2nd order reconstruction where the three element centroids are co linear causing a singularity in the matrix inversion associated with the linear reconstruction a nearly singular stencil can be identified through the condition number of its associated matrix l ˆ in eq 9 while solving for the polynomial coefficients in our case since the stencils are already in normalized local coordinates we simply compute the determinant of the matrix then remove the stencils with the normalized determinants smaller than a prescribed threshold a threshold value of 10 3 is chosen and sensitivity tests suggest that the results are not very sensitive to this parameter another abnormal case occurs when the so called straddling stencils are the only choices for reconstruction fig 4 shows an example of straddling stencils vs non straddling stencils where element 2 is downwind of element 0 and we seek the reconstruction at face 0 2 interface between element 0 and element 2 some stencil candidates have elements lying on both sides of that face e g stencil 0 2 3 in fig 4a which may induce numerical oscillations if there is a shock at face 0 2 the weno procedure would assign a small weight to the shock containing stencil according to eq 12 while other stencils not containing the shock e g stencil 0 9 10 in fig 4b would receive larger weights therefore having stencils from different sectors that covers all directions from the centering element is ideal to prevent oscillations hu and shu 1999 liu and zhang 2013 however this may not always be possible for generic ugs especially when a small neighborhood is used for example the number of available choices for 3rd order stencils is limited on a uniform quadrangle mesh fig 5 in this case for any given side of element 0 the candidate stencils are either straddling it fig 5a shows two examples or singular fig 5b as a result excessive numerical dispersion is un avoidable when large concentration gradient is present at the side of interest because all available non singular stencils would be given similar weights by eq 12 even if their i s s are all large a straightforward remedy taken by us is to fall back to 2nd order stencils which only require 3 elements and so more stencil choices are available for example the 2nd order stencil 0 1 2 in fig 5b is of better quality than the 3rd order stencil 0 1 2 6 7 8 in suppressing numerical dispersion the degradation of accuracy from 3rd order to 2nd order is not against our objective of improving the model skill in the eddying regime because triangles are typically used in the open ocean whereas quadrangles are mostly used to resolve channels inside estuaries non eddying regime where 2nd order accuracy is sufficient alternatively tier 2 neighborhood can be used for quadrangular elements to obtain 3rd order accuracy but we have not seen its need for now the availability of 3rd order reconstruction in realistic applications is illustrated by two meshes a northeast pacific ocean mesh with refined columbia river fig 6a and a northwest atlantic mesh with refined chesapeake bay fig 6b in both meshes triangles are used in the oceanic region where stencils for 3rd order reconstruction are almost always available the main difference between the two meshes lies inside the non eddying regime namely columbia river in fig 6a and chesapeake bay in fig 6b the former applies pure triangles throughout the river whereas the latter applies quadrangles similar to those in a structured grid in the deep channels of the bay as a result the transport calculation in the chesapeake bay channels are mostly 2nd order accurate green color in fig 6b previously a schism model using a 2nd order tvd transport scheme was shown to achieve very good skills on salt intrusion and stratification inside the chesapeake bay ye et al 2018 so the degradation from 3rd order to 2nd order accuracy in the non eddying regime is acceptable there 2 2 4 boundary treatments as illustrated in fig 7 special treatments are applied at both the boundary sides with two nodes on the boundary and the near boundary sides with only one node on the boundary the treatment of land boundary sides is straightforward as no flux condition is imposed and thus no reconstruction is required refer to the terms enclosed by in eq 3 this also applies to the land boundary in deeper layers take for example the lsc2 vertical coordinate zhang et al 2015 used by schism cf section 4 2 and fig 10d the deeper layers touch the land at degeneration points where no horizontal flux can go through since the face area degenerates to 0 for open boundary sides the inflow case and outflow case are treated separately the inflow side values are relaxed to observation or other large domain model results the time scale of the relaxation is taken to be around 1 day in realistic applications the outflow sides take the values from the upwind prisms for near boundary sides 2nd order accuracy or less is applied since a higher order accuracy stencil is generally not available due to geometric constraints 3 numerical benchmark 3 1 rotating cone a rotating cone test case hubbard 1999 was conducted to compare the new 3rd order weno scheme with other lower order schemes in schism the test domain is a circular 2 d disk centering at x 0 y 0 with a radius of 3600 m the flow field on the disk was specified as 15 u w y v w x where w is the angular velocity set to 2 π 3000 rad s 1 i e the flow field is a solid body rotation with a period of 3000 s the initial concentration is in the form of a radially symmetric gaussian hill fig 8a with the maximum concentration at the center x 0 y 0 16 t 0 x y e x x 0 2 y y 0 2 σ 2 where x 0 0 m y 0 1800 m and σ 850 m the domain was discretized by essentially uniform triangles using distmesh persson and strang 2004 the 3rd order weno scheme was compared to the 1st order upwind scheme and the 2nd order tvd scheme the maximum courant number for all tests was chosen as 0 05 representative of the operational value in realistic applications typically with a horizontal grid resolution of a few kilometers and a time step of a few minutes after one rotation the shape of the gaussian hill was much better preserved by the 3rd order weno scheme fig 8ef than the other two schemes even with a low resolution of 400 m fig 8d the new scheme is comparable to the 1st order upwind results with a much higher resolution of 100 m fig 8b the superior performance of the weno scheme is partly due to its truly multi dimensional reconstruction whereas both the upwind and tvd schemes are based on quasi 1d reconstructions i e neglecting the cross wind direction hence susceptible to grid orientation the higher order accuracy of weno was achieved at the cost of a 30 increase in computation time compared to the 2nd order tvd scheme table 1 since weno is only essentially non oscillatory minor oscillations are present in the results in real applications these are small enough and require no diffusive filters to stabilize 3 2 stommel gyre the new scheme s performance was further assessed in a test case proposed by hecht et al 1995 in which a smooth concentration field was advected by the stommel gyre stommel 1948 in a rectangular domain fig 9a the analytical flow field exhibits westward intensification and large shear resembling realistic basin scale wind driven circulations the setup here mostly followed that described by hecht et al 1995 and budgell et al 2007 the rectangular domain is 107 m from the western boundary to the east boundary and 6 3 106 m from the south boundary to the north boundary fig 9a the steady flow field has the following stream function see streamlines in fig 9a 17 ψ 1 ρ γ b π 2 sin π y b p e a x q e b x 1 where x and y are cartesian coordinates pointing eastward and northward respectively with the origin at the south west corner of the domain b 6 3 106 m is the meridional dimension ρ 103 kg m 3 is the fluid density the quantity γ is evaluated as 18 γ f π r b where f 0 1 n m 2 is the surface wind stress r 10 6 s 1 is the frictional coefficient other coefficients are expressed as 19 a α 2 α 2 4 π b 2 1 2 20 b α 2 α 2 4 π b 2 1 2 21 p 1 e b λ e a λ e b λ 22 q 1 p with 23 α 1 r f y where f y 10 11 m 1 s 1 specifies the f plane location λ 107 m is the zonal width of the domain the velocity is found from the volume transport stream function as 24 u d 1 ψ y 25 v d 1 ψ x where d 200 m is the water depth the initial concentration t 0 is a gaussian hill specified by eq 16 with x 0 λ 3 y 0 b 3 and σ 800 2 km the gaussian hill is advected by the steady flow field for a total duration of 1 5 108 s about 5 years with a model time step of 2 10 4 s we used a nearly uniform triangular grid fig 9b with a resolution of 105 km close to the uniform grids in hecht et al 1995 and budgell et al 2007 but these grids are not identical because of different gridding techniques we chose sms as the grid generator so that the final grid resembles that in a realistic eddying ocean application cf section 4 2 and the deep region in fig 10c the results were compared with a reference solution fig 9d obtained from a sixth order runge kutta backtracking based on eqs 24 and 25 the following statistics were used for model assessment 26 min min t min t r 27 max max t max t r 28 l 2 norm t t r 2 1 2 t r 2 1 2 29 variance t t 2 t r t r 2 1 where t is the simulated concentration t r is the reference solution and the overbar denotes area weighted average the final concentration field on the 105 km resolution grid at the end of the 5 year simulation is shown in fig 9e which is similar to that of the fct flux corrected transport schemes in hecht et al 2000 and budgell et al 2007 with this coarse resolution the shape and peak were only modestly preserved with min 0 046 max 0 633 l 2 norm 0 728 and variance 0 681 which are slightly worse than the fct results in budgell et al 2007 improved results were obtained with a variable resolution ug with grid size ranging from 7 4 km to 219 km since this ug is not available to us we used another finer quasi uniform grid with 40 km resolution compared to the coarse grid the simulated concentration on the finer grid fig 9f is much closer to the reference solution and also to other models with the variable resolution ug budgell et al 2007 4 application to the gulf stream 4 1 background the new transport solver was next put into test in a realistic application with a regional domain covering the north west atlantic off the us east coast the gulf of mexico and the caribbean sea fig 10a local mesh refinements were applied in the chesapeake bay and the adjacent mid atlantic bight mab mainly to test the new solver s efficiency under large grid size transitions as well as robustness under both eddying and non eddying regimes large scale baroclinic ocean processes are the focus here on the mab the mean equator ward flowing shelf current and shelf break jet are strong signals on the shelf while the poleward flowing gulf stream dominates the upper ocean flow field seaward of the shelf break e g chen and he 2015 the gulf stream path veers towards the open ocean near cape hatteras where the current begins to shift from a topographically steering western boundary current to a vigorously meandering free jet recent observations suggest that the increasing variability of this separation point may be a plausible cause for the warming of the mab and local relative sea level rise andres 2016 ezer et al 2013 this baroclinic instability creates complex eddies and counter currents chen et al 2014a the variability in gulf stream location and its associated shelf intrusion meandering and eddy shedding are active research topics in the ocean modeling community e g chen et al 2014b so far most of the modeling efforts are based on sg models e g chen and he 2015 zeng and he 2016 where bathymetry smoothing at large slopes and grid nesting in coastal regions are often required successfully applying a multi resolution ug model to this system has two major benefits first it allows steep bathymetry e g submarine canyons to be faithfully represented with high resolution recently developed techniques in schism zhang et al 2015 2016 ensure that the accuracy and efficiency of a baroclinic simulation are not compromised for flows over steep bathymetry second it facilitates the simulation of estuary ocean exchange when high resolution in coastal areas is needed for an sg model two or more layers of grid nesting are typically required to transition km scale grid size in the ocean to meter scale grid size in coastal regions mason et al 2010 dauhajre and mcwilliams 2019 whereas a recent schism baroclinic application resolves small scale structures at 1 m resolution within an estuary to shelf domain with a time step of 120 s liu et al 2018 4 2 model setup the spatial domain fig 10a was discretized by an unstructured grid with 430 k nodes and 840 k elements in the horizontal dimension see illustrations in fig 10bc a resolution of 6 7 km was applied in the open ocean which smoothly transitioned to about 2 km near the coastline and down to 100 m inside the chesapeake bay the vertical grid adopted the terrain following lsc2 coordinate zhang et al 2015 with depth dependent layer numbers 27 0 layers on average and 44 layers in the deepest ocean fig 10d atmospheric forcing from north american regional reanalysis narr 2 2 url https www esrl noaa gov psd data gridded data narr html last accessed in april 2019 was applied at the surface including air temperature surface pressure humidity wind speed and direction short wave and long wave radiation to filter out the spurious inertial modes that often arise in ug models ringler et al 2010 danilov 2012 a bi harmonic viscosity was used in the horizontal momentum advection scheme zhang et al 2016 in addition a shapiro filter shapiro 1970 is locally applied at steep slopes where the spurious modes are most severe for example in the region shown in fig 10c the shapiro filter strength γ non dimensional is specified based on the bathymetric slopes α non dimensional as γ 0 5 tanh α α 0 where α 0 is a reference slope taken to be 0 5 in this case this leads to a maximum filter strength of 0 5 for bathymetric slopes exceeding 0 5 horizontal diffusivity was not explicitly applied and the vertical viscosity diffusivity is calculated by the generic length scale model k k l umlauf and burchard 2003 initialized from the hycom reanalysis product on april 1 2012 the model ran for 110 days with a time step of 150 s the simulation speed was 190 times faster than real time using 540 intel xeon cores broadwell e5 2695 v4 2 1 ghz since the initial condition was derived from hycom the model was considered properly spun up after 2 weeks similar to zeng and he 2016 it is worth noting that the model setup differs from standard sg models mainly in four aspects 1 large element size transition from estuaries 100 m to the open ocean 7 km in a single grid 2 original non smoothed bathymetry captured with local refinements at steep slopes fig 10c 3 depth varying but terrain following vertical layers fig 10d 4 no data assimilation 4 3 model assessment the regional schism model with the setup described in section 4 2 using the new transport solver is referred to as the base model in this study the results from the base model were validated against the observed sea surface height ssh from aviso global sea surface height product 3 3 url https www aviso altimetry fr en data products sea surface height products global html last accessed in june 2019 and sea surface temperature sst from nasa s ghrsst level 4 g1sst global foundation sea surface temperature analysis 4 4 url https podaac jpl nasa gov dataset jpl ourocean l4uhfnd glob g1sst last accessed in june 2019 since a comprehensive model assessment for the gulf stream is beyond the scope of this paper we only present the comparisons on the most important variables including 1 the time series of the error statistics for ssh and sst throughout the domain 2 vertical temperature profiles at argo 5 5 url http www argo ucsd edu last accessed in january 2019 casts near the gulf stream and 3 sst snapshots near the end of the simulation period when gulf stream develops strong meanders the time series of model error root mean square error and bias calculated over the entire model domain indicate the base model is slightly worse in ssh and sst skills than hycom fig 11 this is expected since hycom applies data assimilation which may also be the reason that hycom s error statistics on ssh show smaller temporal variations than schism s in general both models over predict sst during this spring summer period which is likely due to the uncertainties in the atmospheric forcing overall the base model is deemed capable of realistically capturing the general trend in ssh and sst its similar skill to the data assimilated hycom lends further confidence as an important variable delineating the gulf stream signature 3 d temperature was further validated against argo data this dataset consists of randomly located vertical profiles from surface to 2000 m depth that spread out the model domain fig 12 shows the comparisons on selected floats near the gulf stream course in july 2012 a period with strong meanders that are challenging for the model the simulated 3 d temperature matches most of the observed profiles well with an overall rmse root mean square error of 1 47 c and an overall cc correlation coefficient of 0 99 the skill is very close to the data assimilated hycom results which have an overall rmse of 1 48 c and an overall cc of 0 98 larger errors are noticeable near the east ocean boundary the 13th and 14th profiles in fig 12 this may be due to the base model trying to reconcile its interior solution with the boundary condition forced by hycom which itself has uncertainty sensitivity runs of the base model were then conducted to assess the relative performance between the new 3rd order weno scheme and the old 2nd order tvd scheme in particular the performances in the following two aspects are critical for a successful baroclinic simulation in the eddying regime the first aspect is the intensity of the simulated relative vorticity which is a good indicator of a model s capability in resolving baroclinic instability the simulated vorticity in the atlantic ocean at the end of the 110 day simulation are presented in fig 13 the patterns are generally consistent between the two schemes where the gulf stream core is identified by the large vorticity magnitudes compared to the relatively smooth features simulated by the lower order scheme the new scheme generates more energetic and sharply defined flow and eddying patterns corresponding to a 23 increase in eddy kinetic energy at the end of the simulation the second aspect is on the meanders and eddies associated with the gulf stream which have major implications for the exchange of water masses and tracers between ocean shelf and coastal regions ryan et al 2001 chen and he 2015 as the temperature rises in summer about 90 days into the simulation period the gulf stream starts to develop strong meanders after separating from the shelf near cape hatteras at least one eddy formation event is visible during this period as part of a strong meander folds on itself highlighted by the black arrow in fig 14a these features were not clearly captured by the old scheme as shown in fig 14c g in which the signature of the gulf stream is too diffuse in contrast the new scheme qualitatively captured this event fig 14d h and resolved the eddies and filaments in greater detail in fact the results from the new scheme seem not diffusive enough and tend to exaggerate the meanders and eddy detachments as compared with the observation fig 14h and the data assimilated hycom fig 14f the main cause is that the schism runs have a positive bias in sst in the region shown in fig 14g h as well as the whole domain fig 14c d as a result the simulated sst from the schism runs exhibits larger errors than hycom within this region see statistics for fig 14f h the surface over heating issue is expected to be corrected by further calibrations on the parameters of air sea heat exchange which is left for future studies focusing on gulf stream dynamics in this application the numerical dispersion i e spurious concentration patterns is small enough so no additional filtering or artificial diffusion is applied for the tracer transport in the eddying regime in the non eddying regime such as shallow waters a practical solution for minor dispersion issues is to transition to the 1st order upwind scheme when the local water depth is below a given threshold e g 5 m for the commonly used advection schemes in ocean models there are other approaches to eliminate oscillations budgell et al 2007 and we have experimented with some filters such as elad shchepetkin and mcwilliams 1998 we choose not to use them in the new transport scheme for now because they would degrade the order of accuracy in the eddying regime 5 conclusions in this paper a 3rd order transport scheme based on weno has been designed for improving the utility and skill of schism in simulating baroclinic circulation in open ocean eddying regime the specific designs focus on striking a delicate balance among accuracy monotonicity efficiency and robustness we show that baroclinic eddies and filaments are better resolved with this new scheme in the idealized benchmark tests and the realistic gulf stream simulation we have demonstrated that the new 3rd order scheme is capable of better controlling numerical dissipation maintaining sharp frontal instabilities and resolving major baroclinic circulation and eddying features associated with the gulf stream in all experiments the 3rd order scheme outperformed the 2nd order tvd scheme with a moderate increase of 20 30 in computational time explicit diffusion or diffusive filters which control numerical dispersion but essentially degrade accuracy were not applied in these experiments nevertheless minor over under shoots can happen locally in a large gradient zone e g in shallow waters since the scheme is essentially non oscillatory but these are minor enough to be tolerated by the model and the issue can also be mitigated by transitioning to lower order schemes in shallow waters acknowledgments simulations presented in this paper were conducted using the following computational facilities 1 sciclone at the college of william mary 2 the extreme science and engineering discovery environment xsede grant tg oce130032 supported by national science foundation grant number oci 1053575 3 nasa s pleiades 
24012,despite the recent success achieved by the unstructured grid schism semi implicit cross scale hydroscience integrated system model in multi resolution studies its skill in simulating the baroclinic eddying ocean needs to be further improved in particular the classical 2nd order transport schemes for estuaries and coastal zones are too dissipative to resolve the baroclinic dynamics associated with strong boundary currents such as meso scale meanders and eddies to close this gap this paper presents a newly designed 3rd order finite volume transport scheme based on the weighted essentially non oscillatory weno formalism this new scheme strikes a delicate balance among accuracy efficiency and monotonicity for the transport in the eddying regime idealized numerical benchmark experiments demonstrate that the weno scheme is very effective in limiting numerical diffusion the scheme is then applied in a realistic simulation of the gulf stream and the surrounding circulation further confirming its capability of resolving baroclinic meso scale eddies and meanders this new high order transport scheme is therefore ideal for extending the ability of schism to study cross scale baroclinic applications that range from the river dynamics to the eddying ocean processes keywords schism eddying regime weno baroclinic gulf stream 1 introduction the past two decades have seen wider adoption of unstructured grid ug models in coastal ocean modeling mainly due to ug s flexibility in 1 resolving bathymetric and geometric features and 2 local refinement de refinement for the area of interest disinterest a key remaining challenge for ug model development is to fully realize its potentials in cross scale baroclinic problems a review by danilov 2013 indicates that until recently large scale ocean applications are still lacking in the ug model community baroclinic applications of ug models had been mostly confined to coastal regions chen et al 2003 fringer et al 2006 zhang and baptista 2008 and rarely extended to the eddying ocean a recent effort in allowing eddy resolving capability in a coastal ug model thetis was presented by kärnä et al 2017 but realistic applications are still lacking recently a good progress has been made by a triad of global ug models fesom wang et al 2014 mpas ringler et al 2013 and icon korn 2017 in simulating global scale processes with eddying permitting resolving capability different from the coastal models the designs of these ug models aim at simulating the global ocean circulation rather than finer scale coastal processes one example of such designs is the spherical centroidal voronoi tessellations used in mpas ocean ringler et al 2013 which handle multi resolutions well for the more regularly shaped deep ocean however this design is still not cost effective in resolving highly irregular geometries in estuaries and coastal regions so a coupling with a ug coastal model is often required to relate global basin scale ocean processes to local estuarine coastal processes the gap between global ug models and coastal ug models motivates us to extend our coastal model domain further into the deep ocean there are two benefits from this endeavor as far as model coupling is concerned 1 for the global ocean model its scale contrast can be reduced which not only reduces the size of the computational grid but more importantly also increases the maximum allowable time step because the most stringent cfl condition is generally associated with the finest grid resolution 2 for the coastal model the uncertainties from the ocean boundary conditions b c can also be reduced for example in our recent study on the chesapeake bay ye et al 2018 we observed errors that most likely came from the ocean boundary conditions derived from the global hycom reanalysis product 1 1 url https www hycom org dataserver gofs 3pt0 reanalysis last accessed jan 1 2019 even though the model boundary was set 100 km beyond the shelf break and 200 km from the chesapeake bay mouth the effect from the ocean b c can still be felt the goal of this study is to enhance ug coastal models capability in resolving the baroclinic circulation in the open ocean eddying regime with the numerical implementation done in schism semi implicit cross scale hydroscience integrated system model schism wiki previously high order momentum advection schemes that use viscosity filters as stability function have been developed in schism by zhang et al 2016 to address the issue of spurious modes exhibiting as noises in the velocity fields for recent applications in the eddying regime e g yu et al 2017 the high order momentum schemes were used along with a 2nd order tracer transport scheme although the latter s numerical dissipation is considered sufficiently small for estuarine applications we find its utility in simulating meso scale eddies and meanders is still less than satisfactory this is likely because the intrinsic physical dissipation is large in estuaries due to bottom friction and irregular geometry bathymetry which may effectively mask numerical dissipations in an eddying ocean however the forces are largely balanced ringler et al 2013 and physical dissipation is at a much lower level therefore the numerical dissipation needs to be carefully controlled and higher order transport schemes are generally deemed necessary in the eddying regime to improve schims s capability in the eddying ocean we seek a transport scheme that is 1 easily extendable to 3rd or higher order accuracy 2 monotonic or nearly so 3 robust and efficient on generic unstructured grid without geometric constraints such as orthogonality or mesh quality for the first requirement on accuracy our recent applications on kuroshio currents yu et al 2017 showed that the existing 2nd order transport scheme only needs modest improvement so a third order scheme is preferred over a fourth order scheme because the former s leading truncation errors are in the form of diffusion instead of dispersion greatly reducing the need for applying additional explicit diffusion to avoid unphysical oscillation hirsch 2007 the second requirement on monotonicity is necessitated by the large spatial gradients in dissolved substances concentrations in nearshore regions or across the base of the mixed layer in the deep ocean which may trigger unwanted numerical oscillations although various filtering techniques have been proposed before shchepetkin and mcwilliams 2005 they are not ideal because they reduce the effective order of accuracy the third requirement on robustness and efficiency is mainly to accommodate cross scale applications which are characterized by transitions from coarser and relatively uniform meshes in the ocean to finer and relatively irregular meshes in coastal regions previously the performance of various advective schemes for ug ocean modeling have been summarized by hanert et al 2004 and budgell et al 2007 the latter showed that the computational cost can increase significantly with the requirement of higher order accuracy and the techniques for limiting numerical oscillations for our purpose of improving schims s eddy resolving capability only a modest improvement on horizontal advection accuracy would be required but we do need a scheme design that is flexible enough to balance between accuracy and other above mentioned requirements especially the computational cost for which purpose the weno scheme serves as a good candidate or compromise long et al 2008 introduced by liu et al 1994 the weno scheme has been applied in various fields including computational fluid dynamics astrophysics and biology the review by shu 2009 provides a comprehensive list of the related applications in the field of ocean modeling it has been adopted by the structured grid sg model roms on sediment transport in the vertical dimension warner et al 2008 after the extension to ugs friedrich 1998 hu and shu 1999 weno has also attracted attention in the field of morphodynamics canestrelli et al 2010 guérin et al 2016 to our knowledge weno has not been applied to the horizontal tracer transport in ug models in this paper we present a numerical scheme based on weno formalism for the horizontal transport in the ug modeling system schism aiming at better resolving meso scale eddies and meanders in the eddying regime the following text is structured as follows section 2 briefly introduces the discretization of transport equation in schism then explains the challenges of adapting weno in schism and how we design the finite volume solver accordingly section 3 presents the results from a numerical benchmark confirming the superior behavior of the new scheme compared to lower order schemes 2nd order schemes are commonly used for the estuarine tracer transport in coastal models section 4 applies the new scheme to simulate the gulf stream and its surrounding circulation then compares the performance between the new scheme and a classical 2nd order tvd scheme see details in section 3 1 of zhang et al 2016 followed by the conclusions in section 5 2 methods since the focus of this study is the transport solver we will not repeat all governing equations in schism for which readers are referred to zhang et al 2016 the transport equation and its discretization in schism are described below to provide a background 2 1 transport equation and operator splitting the transport equation solved by schism reads 1 t t u t z κ t z q where t is the concentration of a generic tracer κ is vertical eddy diffusivity m2 s 1 q includes all source and sink terms u is the three dimensional velocity vector u v w m s 1 we have neglected horizontal diffusion in eq 1 for brevity as its implementation is straightforward zhang and baptista 2008 in a finite volume framework fig 1 the discretized form of eq 1 that centers on prism i is expressed as 2 t i n 1 t i n δ t v i j s i q j t j t i a i δ t v i κ j t j z j i i κ j t j z j i i δ t v i v i q d v where i is the prism index j is the face index n is the time level t i is the average concentration of prism i defined at prism centers v i is the volume of prism i s i denotes all faces of prism i q j is the total volumetric flux through face j positive going outward prism i which is already known after solving the momentum equation at the current step t j is the average concentration over face j such that the mass flux through face j can be expressed as q j t j for the diffusion term i e the 2nd term on the right hand side rhs of eq 2 additional symbols are introduced for convenience fig 1b i is the prism on top of prism i i is the prism below prism i i i is the top face of prism i i i is the bottom face of prism i κ is vertical diffusivity defined at top and bottom faces of a prism a i is the the projected area of prism i onto the horizontal plane the time levels of t on the rhs will be specified next schism applies an operator splitting procedure in solving the transport equation zhang et al 2016 based on the discretized transport equation 2 t i n is advanced to t i n 1 in three steps horizontal advection vertical advection and vertical diffusion plus other terms each step deals with different terms on the rhs of eq 2 step 1 horizontal advection 3 t i 1 t i n δ t v i j s i h q j t j t i step 2 vertical advection 4 t i 2 t i 1 δ t v i j s i v q j t j t i step 3 vertical diffusion and source 5 t i n 1 t i 2 a i δ t v i κ j t j z j i i n 1 κ j t j z j i i n 1 δ t v i v i q d v where s i h denotes the faces of prism i that are parallel to the z coordinate s i v denotes the top face i i and the bottom face i i of prism i which may or may not be parallel to the horizontal plane fig 1b for the horizontal advection a 2nd order tvd scheme is sufficient for estuarine applications zhang et al 2016 ye et al 2018 but not for the eddying ocean section 4 3 so the main task here is to find a better approximation for the average interface value t j in eq 3 this step is treated explicitly because horizontal advection is generally not the bottleneck for efficiency since eq 3 is an explicit scheme its time step is limited by 6 δ t v i j s i h q j c where s i h is a subset of s i h denoting the faces with outflowing fluxes the cfl condition for weno schemes is typically c 0 6 0 8 zhou et al 2001 and a smaller value c 0 4 0 6 is often used for solutions with discontinuity titarev and toro 2005 here a default value of 0 5 is chosen which is adjustable as a parameter in schism for the vertical advection an implicit 2nd order tvd scheme was previously designed for schism zhang et al 2016 to avoid small time steps and maintain accuracy we have not applied higher order schemes for step 2 and step 3 because the grid spacing in the vertical dimension is typically much smaller than that in the horizontal dimension our previous applications showed that 2nd order accuracy is sufficient to resolve sharp pycnocline in non eddying regimes ye et al 2016 2018 early applications of schism in the eddying regime section 4 show that the skill in capturing vertical temperature gradient is similar to the data assimilated hycom but more challenging applications e g on submarine canyon processes are needed to further assess the vertical transport scheme 2 2 transport solver design the existing weno schemes on ug mainly comprise of two types liu and zhang 2013 with type i focusing on non linear stability and type ii focusing on achieving higher order accuracy the scheme designed in this section belongs to type i compared to existing schemes we put more emphasize on simplifying the stencil to balance between the desired accuracy and efficiency robustness so that the scheme is suitable for cross scale estuarine ocean baroclinic applications on generic ugs carried out by parallel machines for this purpose the following specific considerations were taken into account in the design 1 third order accuracy is necessary and potential extension to fourth order is possible 2 violations in monotonicity should be kept minimal preferably without adding artificial diffusion 3 simple stencils are preferred to maintain the parallel efficiency 4 the algorithm should be robust for all grid configurations as found in oceans and estuaries these considerations are reflected in the weno procedure described below 2 2 1 numerical flux and stencils a key step in the weno procedure is to reconstruct the interfacial mass flux namely the term q j t j in the discretized horizontal advection equation 3 following hu and shu 1999 a q point gaussian integration is used 7 q j t j k 1 q q j ξ k t x g k y g k where q j t j is the mass flux through face j x g k y g k are the global coordinates of the k th quadrature point on face j with ξ k as its weight in schism tracer concentration is treated as vertically homogeneous within each prism zhang et al 2016 so we only need to decide the locations of quadrature points on each side in the horizontal dimension since gaussian integration is precise for polynomials of degree 2 q 1 or less q 1 one point at the side center and ξ 1 1 are chosen when linear polynomials are needed whereas q 2 two points on each side with distances of l 2 3 from the side center where l is the side length and ξ 1 ξ 2 0 5 are chosen when quadratic polynomials are needed hu and shu 1999 the use of gauss legendre quadrature rule with all positive weights does not introduce stability problem huybrechs 2009 the subscript k is dropped in the subsequent description because the reconstruction procedure is identical for each quadrature point note that the interfacial volume flux q j can be taken out of the summation on the r h s of eq 7 because it is independent of tracer concentration and treated as a constant on face j for simplicity and efficiency the upwind flux is used i e the face value is only reconstructed with stencils centered on the upwind prism fig 2 this seems sufficient for our applications in the eddying ocean e g the test case in section 4 other numerical fluxes have been applied for weno schemes titarev and toro 2005 assessing the cost effectiveness of different numerical fluxes for ocean transport are left for future studies the next step is to reconstruct the face value t x g y g in eq 7 by prismatic values this is illustrated by a sample horizontal mesh in fig 2 in this horizontal view we will use elements instead of prisms to avoid confusion based on an appropriate choice of stencil a polynomial of degree n denoted as p n can be constructed since we are using the upwind flux these stencils are centered on the upwind element of x g y g specifically we look for stencils associated with the tier 1 neighborhood which is defined by the elements that share 1 or 2 nodes with the center element fig 2 starting from the center element the stencil candidates are constructed by sequentially listing the elements in this neighborhood in counter clockwise order both 2nd order and 3rd order reconstructions are discussed next a 2nd order reconstruction requires a linear polynomial which can be expressed as 8 p s 1 1 x y a s b s c s lc where s is the stencil index among all possible stencils centered on the upwind element of x g y g c a s b s c s t is a group of coefficients to be determined by the information from the elements within stencil s in this case three elements are needed for example the potential choices in fig 2 are 0 1 2 0 2 3 0 12 1 inserting the concentration values and coordinates from the three elements of stencil s into eq 8 we have 9 t t k 1 x k y k 3 3 a s b s c s 3 1 l ˆ c where k 1 2 3 is the local element index within stencil s t t k is the concentration values at element centers the polynomial s coefficients c a s b s c s t are found by c l ˆ 1 t similarly a 3rd order reconstruction requires a quadratic polynomial 10 p s 2 1 x y x 2 x y y 2 a s b s c s d s e s f s lc in this case 6 elements are need which are also chosen from tier 1 for example the potential choices in fig 2 are 0 1 2 3 4 5 0 2 3 4 5 6 0 12 1 2 3 4 the procedure of finding the coefficients c a s b s c s d s e s f s t is similar to that in the linear case which will not be repeated tier 1 neighborhood is chosen because 1 it generally allows quadratic polynomials that lead to 3rd order accuracy on a triangular mesh 2 it simplifies the mpi communication among the neighboring sub domains 3 our experience suggests that tier 1 stencils are generally less prone to oscillations than tier 2 stencils center element tier 1 neighbors tier 1 neighbors of neighbors to reduce truncation error normalized local coordinates are used within each big stencil element 0 and other gray elements in fig 2 such that 1 the coordinate origin coincides with the centroid of the center element 2 the x and y axes are scaled without rotation by the distance between the origin and the furthest neighbor s centroid then the value at each quadrature point is reconstructed by a linear combination of the polynomials from all stencils 11 t x g y g s 1 n s w s p s n x g y g where s is the stencil index n s is the total number of stencils w s is the linear weight of each stencil which is to be determined later p s n is a polynomial of degree n based on the s th stencil the polynomials p s n in eq 11 are exact at element centers and n 1 th order accurate elsewhere moreover it is possible to achieve n 2 th order accuracy through an optimal choice of the weighting w s hu and shu 1999 2 2 2 weights although higher order accuracy can be achieved by weight optimization hu and shu 1999 we choose not to do so the rationale is that 3rd order accuracy is sufficient for our present applications more importantly optimal weighting is not always guaranteed on a generic ug with abrupt element size transitions and skew elements which reduces the robustness of the algorithm in cross scale applications we may adopt optimal weighting if there is a need for 4th order accuracy in future applications currently w s is solely determined by the concentration gradient within each stencil following hu and shu 1999 but without optimal weighting 12 w s w s s 1 n s w s with w s 1 ϵ i s s 2 where ϵ is a small number to prevent division by zero we find ϵ 10 5 10 7 generally works well for the 3rd order scheme with only negligible violations in monotonicity whereas ϵ 10 4 tends to generate noticeable oscillations near large tracer gradients i s is a smoothness indicator following hu and shu 1999 s formulation for linear stencils leading to a 2nd order reconstruction the smoothness indicator is 13 i s s 1 a p s 1 x 2 p s 1 y 2 d s where p s 1 is a linear polynomial based on the s th stencil a is the area of the centering element d s is an integration over the area of the centering element for quadratic stencils leading to a 3rd order reconstruction the smoothness indicator is 14 i s s 1 a p s 2 x 2 p s 2 y 2 d s 2 p s 2 x 2 2 2 p s 2 x y 2 2 p s 2 y 2 2 d s where p s 2 is a quadratic polynomial based on the s th stencil in realistic applications of schism the 3rd order reconstruction based on quadratic polynomials is applied by default which occasionally downgrades to the 2nd order reconstruction in abnormal cases 2 2 3 abnormal cases the use of a generic unstructured grid and a small neighborhood for reconstruction can lead to abnormal cases which must be carefully handled to ensure robustness one abnormal case is related to the availability of linear quadratic polynomials fig 3 illustrates an invalid stencil for constructing a linear polynomial 2nd order reconstruction where the three element centroids are co linear causing a singularity in the matrix inversion associated with the linear reconstruction a nearly singular stencil can be identified through the condition number of its associated matrix l ˆ in eq 9 while solving for the polynomial coefficients in our case since the stencils are already in normalized local coordinates we simply compute the determinant of the matrix then remove the stencils with the normalized determinants smaller than a prescribed threshold a threshold value of 10 3 is chosen and sensitivity tests suggest that the results are not very sensitive to this parameter another abnormal case occurs when the so called straddling stencils are the only choices for reconstruction fig 4 shows an example of straddling stencils vs non straddling stencils where element 2 is downwind of element 0 and we seek the reconstruction at face 0 2 interface between element 0 and element 2 some stencil candidates have elements lying on both sides of that face e g stencil 0 2 3 in fig 4a which may induce numerical oscillations if there is a shock at face 0 2 the weno procedure would assign a small weight to the shock containing stencil according to eq 12 while other stencils not containing the shock e g stencil 0 9 10 in fig 4b would receive larger weights therefore having stencils from different sectors that covers all directions from the centering element is ideal to prevent oscillations hu and shu 1999 liu and zhang 2013 however this may not always be possible for generic ugs especially when a small neighborhood is used for example the number of available choices for 3rd order stencils is limited on a uniform quadrangle mesh fig 5 in this case for any given side of element 0 the candidate stencils are either straddling it fig 5a shows two examples or singular fig 5b as a result excessive numerical dispersion is un avoidable when large concentration gradient is present at the side of interest because all available non singular stencils would be given similar weights by eq 12 even if their i s s are all large a straightforward remedy taken by us is to fall back to 2nd order stencils which only require 3 elements and so more stencil choices are available for example the 2nd order stencil 0 1 2 in fig 5b is of better quality than the 3rd order stencil 0 1 2 6 7 8 in suppressing numerical dispersion the degradation of accuracy from 3rd order to 2nd order is not against our objective of improving the model skill in the eddying regime because triangles are typically used in the open ocean whereas quadrangles are mostly used to resolve channels inside estuaries non eddying regime where 2nd order accuracy is sufficient alternatively tier 2 neighborhood can be used for quadrangular elements to obtain 3rd order accuracy but we have not seen its need for now the availability of 3rd order reconstruction in realistic applications is illustrated by two meshes a northeast pacific ocean mesh with refined columbia river fig 6a and a northwest atlantic mesh with refined chesapeake bay fig 6b in both meshes triangles are used in the oceanic region where stencils for 3rd order reconstruction are almost always available the main difference between the two meshes lies inside the non eddying regime namely columbia river in fig 6a and chesapeake bay in fig 6b the former applies pure triangles throughout the river whereas the latter applies quadrangles similar to those in a structured grid in the deep channels of the bay as a result the transport calculation in the chesapeake bay channels are mostly 2nd order accurate green color in fig 6b previously a schism model using a 2nd order tvd transport scheme was shown to achieve very good skills on salt intrusion and stratification inside the chesapeake bay ye et al 2018 so the degradation from 3rd order to 2nd order accuracy in the non eddying regime is acceptable there 2 2 4 boundary treatments as illustrated in fig 7 special treatments are applied at both the boundary sides with two nodes on the boundary and the near boundary sides with only one node on the boundary the treatment of land boundary sides is straightforward as no flux condition is imposed and thus no reconstruction is required refer to the terms enclosed by in eq 3 this also applies to the land boundary in deeper layers take for example the lsc2 vertical coordinate zhang et al 2015 used by schism cf section 4 2 and fig 10d the deeper layers touch the land at degeneration points where no horizontal flux can go through since the face area degenerates to 0 for open boundary sides the inflow case and outflow case are treated separately the inflow side values are relaxed to observation or other large domain model results the time scale of the relaxation is taken to be around 1 day in realistic applications the outflow sides take the values from the upwind prisms for near boundary sides 2nd order accuracy or less is applied since a higher order accuracy stencil is generally not available due to geometric constraints 3 numerical benchmark 3 1 rotating cone a rotating cone test case hubbard 1999 was conducted to compare the new 3rd order weno scheme with other lower order schemes in schism the test domain is a circular 2 d disk centering at x 0 y 0 with a radius of 3600 m the flow field on the disk was specified as 15 u w y v w x where w is the angular velocity set to 2 π 3000 rad s 1 i e the flow field is a solid body rotation with a period of 3000 s the initial concentration is in the form of a radially symmetric gaussian hill fig 8a with the maximum concentration at the center x 0 y 0 16 t 0 x y e x x 0 2 y y 0 2 σ 2 where x 0 0 m y 0 1800 m and σ 850 m the domain was discretized by essentially uniform triangles using distmesh persson and strang 2004 the 3rd order weno scheme was compared to the 1st order upwind scheme and the 2nd order tvd scheme the maximum courant number for all tests was chosen as 0 05 representative of the operational value in realistic applications typically with a horizontal grid resolution of a few kilometers and a time step of a few minutes after one rotation the shape of the gaussian hill was much better preserved by the 3rd order weno scheme fig 8ef than the other two schemes even with a low resolution of 400 m fig 8d the new scheme is comparable to the 1st order upwind results with a much higher resolution of 100 m fig 8b the superior performance of the weno scheme is partly due to its truly multi dimensional reconstruction whereas both the upwind and tvd schemes are based on quasi 1d reconstructions i e neglecting the cross wind direction hence susceptible to grid orientation the higher order accuracy of weno was achieved at the cost of a 30 increase in computation time compared to the 2nd order tvd scheme table 1 since weno is only essentially non oscillatory minor oscillations are present in the results in real applications these are small enough and require no diffusive filters to stabilize 3 2 stommel gyre the new scheme s performance was further assessed in a test case proposed by hecht et al 1995 in which a smooth concentration field was advected by the stommel gyre stommel 1948 in a rectangular domain fig 9a the analytical flow field exhibits westward intensification and large shear resembling realistic basin scale wind driven circulations the setup here mostly followed that described by hecht et al 1995 and budgell et al 2007 the rectangular domain is 107 m from the western boundary to the east boundary and 6 3 106 m from the south boundary to the north boundary fig 9a the steady flow field has the following stream function see streamlines in fig 9a 17 ψ 1 ρ γ b π 2 sin π y b p e a x q e b x 1 where x and y are cartesian coordinates pointing eastward and northward respectively with the origin at the south west corner of the domain b 6 3 106 m is the meridional dimension ρ 103 kg m 3 is the fluid density the quantity γ is evaluated as 18 γ f π r b where f 0 1 n m 2 is the surface wind stress r 10 6 s 1 is the frictional coefficient other coefficients are expressed as 19 a α 2 α 2 4 π b 2 1 2 20 b α 2 α 2 4 π b 2 1 2 21 p 1 e b λ e a λ e b λ 22 q 1 p with 23 α 1 r f y where f y 10 11 m 1 s 1 specifies the f plane location λ 107 m is the zonal width of the domain the velocity is found from the volume transport stream function as 24 u d 1 ψ y 25 v d 1 ψ x where d 200 m is the water depth the initial concentration t 0 is a gaussian hill specified by eq 16 with x 0 λ 3 y 0 b 3 and σ 800 2 km the gaussian hill is advected by the steady flow field for a total duration of 1 5 108 s about 5 years with a model time step of 2 10 4 s we used a nearly uniform triangular grid fig 9b with a resolution of 105 km close to the uniform grids in hecht et al 1995 and budgell et al 2007 but these grids are not identical because of different gridding techniques we chose sms as the grid generator so that the final grid resembles that in a realistic eddying ocean application cf section 4 2 and the deep region in fig 10c the results were compared with a reference solution fig 9d obtained from a sixth order runge kutta backtracking based on eqs 24 and 25 the following statistics were used for model assessment 26 min min t min t r 27 max max t max t r 28 l 2 norm t t r 2 1 2 t r 2 1 2 29 variance t t 2 t r t r 2 1 where t is the simulated concentration t r is the reference solution and the overbar denotes area weighted average the final concentration field on the 105 km resolution grid at the end of the 5 year simulation is shown in fig 9e which is similar to that of the fct flux corrected transport schemes in hecht et al 2000 and budgell et al 2007 with this coarse resolution the shape and peak were only modestly preserved with min 0 046 max 0 633 l 2 norm 0 728 and variance 0 681 which are slightly worse than the fct results in budgell et al 2007 improved results were obtained with a variable resolution ug with grid size ranging from 7 4 km to 219 km since this ug is not available to us we used another finer quasi uniform grid with 40 km resolution compared to the coarse grid the simulated concentration on the finer grid fig 9f is much closer to the reference solution and also to other models with the variable resolution ug budgell et al 2007 4 application to the gulf stream 4 1 background the new transport solver was next put into test in a realistic application with a regional domain covering the north west atlantic off the us east coast the gulf of mexico and the caribbean sea fig 10a local mesh refinements were applied in the chesapeake bay and the adjacent mid atlantic bight mab mainly to test the new solver s efficiency under large grid size transitions as well as robustness under both eddying and non eddying regimes large scale baroclinic ocean processes are the focus here on the mab the mean equator ward flowing shelf current and shelf break jet are strong signals on the shelf while the poleward flowing gulf stream dominates the upper ocean flow field seaward of the shelf break e g chen and he 2015 the gulf stream path veers towards the open ocean near cape hatteras where the current begins to shift from a topographically steering western boundary current to a vigorously meandering free jet recent observations suggest that the increasing variability of this separation point may be a plausible cause for the warming of the mab and local relative sea level rise andres 2016 ezer et al 2013 this baroclinic instability creates complex eddies and counter currents chen et al 2014a the variability in gulf stream location and its associated shelf intrusion meandering and eddy shedding are active research topics in the ocean modeling community e g chen et al 2014b so far most of the modeling efforts are based on sg models e g chen and he 2015 zeng and he 2016 where bathymetry smoothing at large slopes and grid nesting in coastal regions are often required successfully applying a multi resolution ug model to this system has two major benefits first it allows steep bathymetry e g submarine canyons to be faithfully represented with high resolution recently developed techniques in schism zhang et al 2015 2016 ensure that the accuracy and efficiency of a baroclinic simulation are not compromised for flows over steep bathymetry second it facilitates the simulation of estuary ocean exchange when high resolution in coastal areas is needed for an sg model two or more layers of grid nesting are typically required to transition km scale grid size in the ocean to meter scale grid size in coastal regions mason et al 2010 dauhajre and mcwilliams 2019 whereas a recent schism baroclinic application resolves small scale structures at 1 m resolution within an estuary to shelf domain with a time step of 120 s liu et al 2018 4 2 model setup the spatial domain fig 10a was discretized by an unstructured grid with 430 k nodes and 840 k elements in the horizontal dimension see illustrations in fig 10bc a resolution of 6 7 km was applied in the open ocean which smoothly transitioned to about 2 km near the coastline and down to 100 m inside the chesapeake bay the vertical grid adopted the terrain following lsc2 coordinate zhang et al 2015 with depth dependent layer numbers 27 0 layers on average and 44 layers in the deepest ocean fig 10d atmospheric forcing from north american regional reanalysis narr 2 2 url https www esrl noaa gov psd data gridded data narr html last accessed in april 2019 was applied at the surface including air temperature surface pressure humidity wind speed and direction short wave and long wave radiation to filter out the spurious inertial modes that often arise in ug models ringler et al 2010 danilov 2012 a bi harmonic viscosity was used in the horizontal momentum advection scheme zhang et al 2016 in addition a shapiro filter shapiro 1970 is locally applied at steep slopes where the spurious modes are most severe for example in the region shown in fig 10c the shapiro filter strength γ non dimensional is specified based on the bathymetric slopes α non dimensional as γ 0 5 tanh α α 0 where α 0 is a reference slope taken to be 0 5 in this case this leads to a maximum filter strength of 0 5 for bathymetric slopes exceeding 0 5 horizontal diffusivity was not explicitly applied and the vertical viscosity diffusivity is calculated by the generic length scale model k k l umlauf and burchard 2003 initialized from the hycom reanalysis product on april 1 2012 the model ran for 110 days with a time step of 150 s the simulation speed was 190 times faster than real time using 540 intel xeon cores broadwell e5 2695 v4 2 1 ghz since the initial condition was derived from hycom the model was considered properly spun up after 2 weeks similar to zeng and he 2016 it is worth noting that the model setup differs from standard sg models mainly in four aspects 1 large element size transition from estuaries 100 m to the open ocean 7 km in a single grid 2 original non smoothed bathymetry captured with local refinements at steep slopes fig 10c 3 depth varying but terrain following vertical layers fig 10d 4 no data assimilation 4 3 model assessment the regional schism model with the setup described in section 4 2 using the new transport solver is referred to as the base model in this study the results from the base model were validated against the observed sea surface height ssh from aviso global sea surface height product 3 3 url https www aviso altimetry fr en data products sea surface height products global html last accessed in june 2019 and sea surface temperature sst from nasa s ghrsst level 4 g1sst global foundation sea surface temperature analysis 4 4 url https podaac jpl nasa gov dataset jpl ourocean l4uhfnd glob g1sst last accessed in june 2019 since a comprehensive model assessment for the gulf stream is beyond the scope of this paper we only present the comparisons on the most important variables including 1 the time series of the error statistics for ssh and sst throughout the domain 2 vertical temperature profiles at argo 5 5 url http www argo ucsd edu last accessed in january 2019 casts near the gulf stream and 3 sst snapshots near the end of the simulation period when gulf stream develops strong meanders the time series of model error root mean square error and bias calculated over the entire model domain indicate the base model is slightly worse in ssh and sst skills than hycom fig 11 this is expected since hycom applies data assimilation which may also be the reason that hycom s error statistics on ssh show smaller temporal variations than schism s in general both models over predict sst during this spring summer period which is likely due to the uncertainties in the atmospheric forcing overall the base model is deemed capable of realistically capturing the general trend in ssh and sst its similar skill to the data assimilated hycom lends further confidence as an important variable delineating the gulf stream signature 3 d temperature was further validated against argo data this dataset consists of randomly located vertical profiles from surface to 2000 m depth that spread out the model domain fig 12 shows the comparisons on selected floats near the gulf stream course in july 2012 a period with strong meanders that are challenging for the model the simulated 3 d temperature matches most of the observed profiles well with an overall rmse root mean square error of 1 47 c and an overall cc correlation coefficient of 0 99 the skill is very close to the data assimilated hycom results which have an overall rmse of 1 48 c and an overall cc of 0 98 larger errors are noticeable near the east ocean boundary the 13th and 14th profiles in fig 12 this may be due to the base model trying to reconcile its interior solution with the boundary condition forced by hycom which itself has uncertainty sensitivity runs of the base model were then conducted to assess the relative performance between the new 3rd order weno scheme and the old 2nd order tvd scheme in particular the performances in the following two aspects are critical for a successful baroclinic simulation in the eddying regime the first aspect is the intensity of the simulated relative vorticity which is a good indicator of a model s capability in resolving baroclinic instability the simulated vorticity in the atlantic ocean at the end of the 110 day simulation are presented in fig 13 the patterns are generally consistent between the two schemes where the gulf stream core is identified by the large vorticity magnitudes compared to the relatively smooth features simulated by the lower order scheme the new scheme generates more energetic and sharply defined flow and eddying patterns corresponding to a 23 increase in eddy kinetic energy at the end of the simulation the second aspect is on the meanders and eddies associated with the gulf stream which have major implications for the exchange of water masses and tracers between ocean shelf and coastal regions ryan et al 2001 chen and he 2015 as the temperature rises in summer about 90 days into the simulation period the gulf stream starts to develop strong meanders after separating from the shelf near cape hatteras at least one eddy formation event is visible during this period as part of a strong meander folds on itself highlighted by the black arrow in fig 14a these features were not clearly captured by the old scheme as shown in fig 14c g in which the signature of the gulf stream is too diffuse in contrast the new scheme qualitatively captured this event fig 14d h and resolved the eddies and filaments in greater detail in fact the results from the new scheme seem not diffusive enough and tend to exaggerate the meanders and eddy detachments as compared with the observation fig 14h and the data assimilated hycom fig 14f the main cause is that the schism runs have a positive bias in sst in the region shown in fig 14g h as well as the whole domain fig 14c d as a result the simulated sst from the schism runs exhibits larger errors than hycom within this region see statistics for fig 14f h the surface over heating issue is expected to be corrected by further calibrations on the parameters of air sea heat exchange which is left for future studies focusing on gulf stream dynamics in this application the numerical dispersion i e spurious concentration patterns is small enough so no additional filtering or artificial diffusion is applied for the tracer transport in the eddying regime in the non eddying regime such as shallow waters a practical solution for minor dispersion issues is to transition to the 1st order upwind scheme when the local water depth is below a given threshold e g 5 m for the commonly used advection schemes in ocean models there are other approaches to eliminate oscillations budgell et al 2007 and we have experimented with some filters such as elad shchepetkin and mcwilliams 1998 we choose not to use them in the new transport scheme for now because they would degrade the order of accuracy in the eddying regime 5 conclusions in this paper a 3rd order transport scheme based on weno has been designed for improving the utility and skill of schism in simulating baroclinic circulation in open ocean eddying regime the specific designs focus on striking a delicate balance among accuracy monotonicity efficiency and robustness we show that baroclinic eddies and filaments are better resolved with this new scheme in the idealized benchmark tests and the realistic gulf stream simulation we have demonstrated that the new 3rd order scheme is capable of better controlling numerical dissipation maintaining sharp frontal instabilities and resolving major baroclinic circulation and eddying features associated with the gulf stream in all experiments the 3rd order scheme outperformed the 2nd order tvd scheme with a moderate increase of 20 30 in computational time explicit diffusion or diffusive filters which control numerical dispersion but essentially degrade accuracy were not applied in these experiments nevertheless minor over under shoots can happen locally in a large gradient zone e g in shallow waters since the scheme is essentially non oscillatory but these are minor enough to be tolerated by the model and the issue can also be mitigated by transitioning to lower order schemes in shallow waters acknowledgments simulations presented in this paper were conducted using the following computational facilities 1 sciclone at the college of william mary 2 the extreme science and engineering discovery environment xsede grant tg oce130032 supported by national science foundation grant number oci 1053575 3 nasa s pleiades 
24013,a high resolution ocean circulation model for the indian ocean io using regional ocean modeling system roms is operational at indian national centre for ocean information services incois which provides ocean state forecasts for the bay of bengal bob and the arabian sea as to the indian ocean rim countries to provide an improved estimate of ocean state a variant of ensemble kalman filter enkf viz the local ensemble transform kalman filter letkf has been developed and interfaced with the present basin wide operational roms this system assimilates in situ temperature and salinity profiles and satellite track data of sea surface temperature sst the ensemble members of the assimilation system are initialized with different parameters like diffusion and viscosity coefficients and are subjected to an ensemble of atmospheric fluxes in addition one half of the ensemble members respond to k profile parameterization mixing scheme while the other half is subjected to mellor yamada mixing scheme this strategy aids in arresting the filter divergence which has always been a challenging task the assimilated system simulates the ocean state better than the present operational roms improvements permeate to deeper ocean depths with better correlation and reduced root mean squared deviation rmsd with respect to observations particularly in the northern indian ocean which is data rich in density analysis shows domain averaged rmsd reduction of about 0 2 0 4 c in sea surface temperature and 2 4 cm in sea level anomaly the assimilated system also manages to significantly improve the thickness of the temperature inversion layers and the duration of its occurrence in northern bay of bengal the most profound improvements are seen in currents with an error reduction of 15 cm s in zonal currents of central bay of bengal keywords roms ocean data assimilation letkf indian ocean 1 introduction improving ocean state forecast is necessary because a large proportion of the world population roughly about 40 resides within 100 km from the coastline or in the low elevation coastal zone center for international earth science information network ciesin of columbia university 2006 csd coastal population indicator data and methodology page http sedac ciesin columbia edu es csdcoastal html quite a few of them venture out into the sea for fishing and other marine activities and they require periodical information about the state of the ocean it is therefore imperative to provide ocean state forecasts bereft of errors lest it might adversely affect the coastal population the ocean state is estimated through general circulation models that numerically solve the dynamical equations for the momentum sea level anomaly and tracer variables like temperature and salinity the errors in the estimation however tend to grow in time due to various reasons ranging from inaccurate initial condition approximated parameterizations inaccurate atmospheric fluxes and boundary conditions if it is a regional model leading to a different realization of intrinsic ocean variability at a multitude of spatio temporal scales sérazin et al 2015 the inherent inability of the model to resolve length scales finer than its prescribed grid size also tends to contribute to these errors all these errors tend to diverge the model state from truth thereby gradually driving the forecasts far from the observed reality data assimilation da can however arrest the divergence of the model trajectory by periodically correcting the simulated ocean state using observations thereby providing improved estimates of the state of the ocean da is a method that imbibes information from both the model state and observation statistically churns them to reduce the cost function errors and spits out an initial condition for the model commonly known as analysis that is closer to the truth than either the input prior model state or the input observation in terms of uncertainty and root mean squared error asch et al 2016 bennett 2002 daley 1991 evensen 2009 kalnay 2003 lewis et al 2009 malanotte rizzoli 1996 park and xu 2009 the model is then restarted with this initial condition that ensures that the trajectory remains closer to reality for at least a short span of time this cycle is repeated typically at a fixed frequency an advanced data assimilation system has been developed using local ensemble transform kalman filter letkf hunt et al 2007 letkf is an intuitive amalgamation of local ensemble kalman filter lekf ott et al 2004 and ensemble transform kalman filter etkf bishop et al 2002 wang and bishop 2003 wherein letkf exploits the qualitative superiority of lekf and computational efficiency of etkf letkf has been used in multiple systems ranging from meteorology hoffman et al 2010 miyoshi et al 2010 2007 miyoshi and kunii 2012a b miyoshi and yamane 2007 seko et al 2011 szunyogh et al 2005 to oceanography penny 2014 2011 penny et al 2015 2013 with abundant success it has been used in regional ocean models like roms hoffman et al 2012 ecom hoffman et al 2008 pom miyazawa et al 2012 xu et al 2013 xu and oey 2014 and in global models like mom penny 2011 penny et al 2015 presently a high resolution operational model hencefortho roms using regional ocean modeling system roms provides ocean state forecasts to the indian ocean rim countries francis et al 2013 the o roms suffers from large biases in temperature and salinity particularly in the north indian ocean further there are large scope of improvements for surface and sub surface currents in the equatorial indian ocean the present system i e o roms does not assimilate any observations and hence is prone to accumulate and propagate errors in time in order to improve the performance of o roms letkf developed as a separate module is interfaced with this operational model for indian ocean region the in situ temperature and salinity profiles from multiple observation networks like argo floats xbts mooring ship tracks etc and satellite derived sea surface temperature sst from amsr e are assimilated results from this newly developed assimilation system for the basin wide indian ocean is presented in this work we henceforth call this assimilated system as letkf roms this assimilated system introduces a combination of mixing schemes and model parameters across the ensemble members as an additional prescription for arresting filter divergence the objective of this work is two fold to highlight that such a strategy appears to be beneficial to the system and indeed stabilizes the spread in the ensemble at least during short integration times which is susceptible to collapses in ensemble based schemes and to highlight that letkf roms is better suited to estimate ocean states in the indian ocean compared to o roms the improvements in the state estimation of prognostic variables from letkf roms in comparison to o roms are presented in this paper however the primary emphasis has been put on ocean currents which is a non assimilated variable section 2 provides a brief description of the roms model setup section 3 describes the letkf assimilation system section 4 presents results and improvements in state estimation of temperature salinity sea level anomaly and currents over o roms in section 5 we show that letkf roms in comparison to o roms is better equipped to estimate thermal inversions in the northern bay of bengal a region where temperature inversions are found in abundance girishkumar et al 2013 thadathil et al 2016 a summary and discussion of the results of this study is outlined in section 6 2 brief description of o roms the model used for estimating the ocean state of the indian ocean is the regional ocean modeling system roms version 3 7 roms is a free surface terrain following general circulation model which solves a set of primitive equations in an orthogonal curvilinear coordinate system haidvogel et al 2000 haidvogel and beckmann 1999 shchepetkin and mcwilliams 2005 song and haidvogel 1994 the domain of the indian ocean model based on roms i e o roms extends from 30 e to 120 e in the east west direction and from 30 n to 30 s in the north south direction the horizontal grid spacing of o roms is 1 12 approximately 9 km and it has 40 sigma levels in the vertical the vertical stretching parameters are chosen such that the vertical scales are finely resolved in the upper part of the ocean this is important because processes like diurnal radiation surface mixed layer restratification cycles bernie et al 2005 brainerd and gregg 1993 need to be judiciously resolved in the upper layers of the ocean the lateral boundary in the east is connected to the pacific ocean through the indonesian throughflow itf while the southern boundary is interfaced with the southern ocean on the other hand the northern and western boundaries are guarded by coastlines and no slip boundary conditions are employed the model exchanges information with the boundary every 5 days it is to be noted that due to these low frequency exchanges across the boundary fast moving barotropic waves that originates outside the stated indian ocean domain will not be efficiently resolved at the boundaries and remote effects due to these modes will be ignored resulting in likely escalation of errors in ocean state estimate fu 2003 rohith et al 2019 weijer 2010 this may be significant at times rohith et al 2019 particularly during the boreal winters when madden julian oscillation is known to generate strong wind stress anomalies and hence large barotropic fluctuations close to the north west coast of australia marshall and hendon 2014 wheeler et al 2009 wheeler and mcbride 2005 zhang 2013 10 day delayed ocean reanalysis derived from the incois godas analysis sivareddy 2015 provides the boundary condition to the tracer and momentum fields the eastern and southern boundaries in o roms therefore respond to an assimilated product and hence strictly speaking o roms is not absolutely free from assimilation however the effects of assimilation penetrating through boundary effects are likely to be negligible compared to the assimilation of observations in the interior of the domain and hence we pretend that o roms is free from assimilation in addition the ensemble models in letkf roms also respond to the same boundary conditions derived from incois godas therefore any improvements observed in letkf roms can be attributed to the assimilation of observations in the interior of the domain the model uses unless otherwise mentioned the k profile parameterization kpp mixing scheme large et al 1994 to parameterize the vertical mixing smagorinsky type viscosity along with harmonic and biharmonic horizontal mixing and diffusion schemes muschinski 1996 are chosen for horizontal mixing and a bulk parameterization scheme fairall et al 1996 is chosen for the computation of air sea fluxes of heat six hourly atmospheric fields obtained from national centre for medium range weather forecast ncmrwf george et al 2016 derived from ncmrwf unified model ncum at a horizontal resolution of 25 km are used to force the ocean state in o roms in addition the tidal forcing from tpx07 2 model is incorporated at the southern and eastern open boundaries the model however does not account for river runoff instead the model states are relaxed to monthly climatology of sea surface salinity antonov et al 2010 locarini et al 2010 with a relaxation time scale of 30 days o roms is able to capture the spatio temporal variabilities in the ocean with a fair amount of success including the salinity field in spite of the absence of river runoff fluxes there is however still room for improvement in the state variables francis et al 2013 and hence it calls for corrective measures particularly because this model is used for short term ocean state forecasts 3 letkf roms we implement local ensemble transform kalman filter hunt et al 2007 as the data assimilation scheme to regional ocean modeling system as the corrective measure to improve the quality of the ocean states it is a variant of ensemble kalman filter wherein the model error covariance is approximated as a sample covariance derived from the ensemble members rank is k 1 where k is number of ensemble members the sample covariance asymptotically converges to model error covariance when the ensemble number approaches infinity we have used 80 ensemble members in our study the inaccuracy introduced due to this approximation is traded off with the sharp decrease in computational resources and runtime the 80 member ensemble atmospheric fields obtained from gfs model implemented at ncmrwf prasad et al 2016 was used to force the ocean model component of letkf roms we prepare the initial ensemble by random sampling xu and oey 2014 from an ocean state derived from o roms the ocean state is perturbed with a gaussian random number whose mean is zero and standard deviation is σ x y z daily σ x y z is estimated from a set of 56 ensemble member ocean states obtained through an ensemble run of roms forced with 20th century atmospheric reanalysis ensemble fluxes compo et al 2011 for five years from 2003 2008 the spread of the ensemble during those five years of run was robust σ x y z of aug 15 2008 was chosen to generate the spread of the initial ensemble of the present letkf roms set up that takes off from aug 15 2016 this ensured that the ensemble spread of the initial ensemble so generated remains consistent with the previous experiment and hence it is reasonable to expect that the ensemble members do not collapse in time we also divide the ensemble members into two equal halves one half uses kpp mixing scheme large et al 1994 while the other uses mellor yamada mixing scheme mellor and yamada 1982 this is done to ensure that the advantages of both the mixing schemes are exploited this will also facilitate in arresting filter divergence we also arbitrarily spread the value of coefficients like tracer diffusion constant and horizontal viscosity coefficients about the value used in o roms across the ensembles table 1 lists the values used in the o roms experiment this division in mixing schemes and using a range of coefficients in tracer and momentum dynamics aids further in keeping the ensemble spread intact and improves background error covariance structure du et al 2014 kwon et al 2016 meng and zhang 2007 stensrud et al 2000 vandenbulcke and barth 2015 however there is a likely caveat the sample error covariance in this system no longer necessarily asymptotically approaches the model error covariance when the number of ensemble members approach infinity we however pretend that it does to further safeguard ensemble collapse we inflate the ensembles after each analysis by 10 inflation an exercise routinely performed across data assimilation community to prevent ensemble collapse anderson and anderson 1999 bocquet and sakov 2012 miyoshi 2011 however there are reports that show that use of constant multiplicative inflation can lead to filter divergence particularly in sparsely observed areas miyoshi et al 2010 penny 2011 over long integration time periods even though there are areas which are sparsely observed in our system particularly in the southern hemisphere we do not observe such filter divergence likely because of the use ensemble fluxes and mixture of schemes or because the integration time duration is not large we use a gaussian profile for localization with an e folding scale of 200 km anderson 2007 nurujjaman et al 2013 ying et al 2018 i e the model state variables at any particular grid are minimally influenced by observations that lies beyond 700 km from that grid i e around 3 5 times the localization radius the multivariate covariance error matrix allows the observation of any particular sort to influence all the prognostic variables unless specifically designed to restrict the influence in letkf roms each observation influences all the prognostic variables that fall within the localization radius of its location in many assimilation systems a constant observational error oe is attached to the observations i e the error covariance of the observation is uniform in space and time which is an oversimplification in some systems the observation error has a vertical structure behringer et al 1998 penny 2011 balmaseda et al 2013 sivareddy 2015 but lacks horizontal and temporal variability we however estimate oe following the method suggested by etherton and bishop 2004 wherein a model is used to estimate the errors due to unresolved scales and processes thereby introducing spatio temporal variations into oe through representation error re etherton and bishop 2004 hodyss and nichols 2015 janjić et al 2017 janjić and cohn 2006 oke and sakov 2008 van leeuwen 2015 which partly occurs due to the deficiency of the model to accurately project its state on to the observation space this estimated re is included as a component of observation error along with the instrument error to represent the uncertainty of observations it is to be noted that observations are not error free and hence do not represent the truth the performance of our assimilation system improves once we include re into the observation error statistics sanikommu et al 2019 we have used 1 12 degree model as the high resolution model and 1 4 degree model as a low resolution model to estimate the spatio temporal characteristics of re ideally this should have been used as the representation error in a lower resolution system but instead we have used this re for the 1 12 resolution experiment being presented here we however would like to highlight that the origin of representation error of a 1 4 degree model is characteristically different from that of a 1 12 degree model calculating re from a higher resolution 1 12 model to be used for the present 1 12 resolution model is computationally taxing and has been avoided the off diagonal elements in the observational error covariance matrix are assumed to be zero i e the observational errors do not influence each other this is a reasonable approximation particularly for in situ observations however this approximation is questionable for satellite observations bormann et al 2010 stewart 2009 stewart et al 2013 2008 in spite of this we assume the off diagonal elements of observation error covariance of sst to be zero this assumption is intended to simplify inversion of error covariance matrices we assimilate in situ temperature and salinity profiles from observation networks that include argo floats and moored buoys within the indian ocean domain these are the same quality controlled observations prepared originally for assimilation in incois godas sivareddy 2015 sivareddy et al 2017 we however discard assimilating observations that are too close to the domain boundaries or too close to any landmass we also assimilate satellite track data of sea surface temperature sst obtained from amsr e however since the data along the track is very dense we coarse grain the data over a length scale of 50 km before feeding it into the assimilation system this superobbing of observations is a common practice while dealing with dense data cummings 2005 fu 2016 oke and sakov 2008 we assimilate observations every 5 days except the observations on the 5th day all observations of the previous four days are discarded the advantage is that assimilation does not affect the ocean state daily and allows the ocean to respond to the fluxes unhindered the daily output from letkf roms therefore comprises of five days of free run followed by assimilation on fifth day we are correcting the initial condition of the model once in 5 days before proffering the results from the validation we demonstrate the integrity of the letkf roms by examining the ensemble spread in the model variables fig 1 a f shows the ensemble spread in the surface temperature 5 m in the top panel fig 1 a c and subsurface temperature 100 m in bottom panel fig 1 d e at three stages of the period of our study the spread is larger in the sub surface than in the near surface owing to usual large uncertainties in the thermocline layers further the domain averaged spread averaged only over x and y in temperature appears to be fluctuating about some mean value both at the surface blue curve fig 1 g and at the subsurface red curve fig 1 g the average spread is robust in time and does not collapse the spread is maintained at reasonable magnitudes because of the strategies ensemble atmospheric forcing change of diffusion schemes etc outlined in the previous section each of these strategies i e ensemble fluxes and the combination of mixing schemes play a role in maintaining the spread across the ensemble in order to understand the contribution of each of these two strategies viz atmospheric ensemble fluxes and combination of mixing schemes three controlled experiments were performed using 80 ensemble members starting from the same initial condition as that of letkf roms the initial spread in any variable is same for all the experiments no data were assimilated in any of these three controlled experiments the configuration of the first experiment exp1 is similar to letkf roms except that no data were assimilated this experiment is meant to quantify the combined role of the two strategies in the estimation of ensemble spread in absence of any assimilation in the second controlled experiment exp2 the mixing combination was kept intact but all the ocean members of the ensemble were driven by the mean of the 80 member ensemble atmospheric fluxes the purpose of exp2 is to quantify the role of the combination of mixing schemes in the estimation of ensemble spread in the third experiment exp3 all ocean members of the 80 member ensemble respond to kpp mixing scheme but is driven by the 80 ensemble atmospheric fluxes the kpp mixing scheme is chosen out of the two mixing scheme to make it commensurate with o roms evidently the purpose of exp3 is to quantify the role of the ensemble atmospheric fluxes in the estimation of ensemble spread in fig 2 we plot the initial distribution of sst black curve and the distribution of sst across the ensemble members after 600 days of run at an arbitrary location at 93 e 0 n from exp1 blue curve exp2 green curve and exp3 red curve along with the distribution of sst from the letkf roms dashed black curve the spread increases in all the three controlled experiments even though the distribution was initially gaussian the use of combination of mixing schemes in exp1 and exp2 rendered it into a bimodal distribution the departure from normal distribution in exp3 red curve is minimal compared to exp1 and exp2 it appears that the right peak of the blue green curve closer to the peak of exp3 is the manifestation of kpp mixing scheme in exp1 exp2 while the left peak is a result of mellor yamada mixing scheme the mean of the distribution in exp1 exp2 and exp3 is 30 46 0 21 30 59 0 17 and 30 67 0 08 respectively whereas the corresponding standard deviations are 1 85 1 57 and 0 73 the combination of mixing schemes facilitate a larger spread in the ensemble members compared to the ensemble fluxes 4 results both o roms and letkf roms were run from august 2016 to september 2018 the period of study is limited because of the unavailability of ensemble surface fluxes the first day forecast of o roms is compared here with the ocean states derived from letkf roms it is to be noted that in every assimilation cycle of 5 days the first five days of the letkf roms is a free run akin to forecast followed by correction of initial condition on the fifth day sea surface temperature sst is validated against multiple sources of observations i e daily quarter degree gridded observation of avhrr satellite reynolds et al 2007 in situ daily observations of rama moorings mcphaden et al 2009 and niot buoys we have compared results obtained from letkf roms and o roms both the model results are projected on to the observation space and compared this is true for all the subsequent validation exercises carried out in this study fig 3 a and b shows correlation of sst from letkf roms and o roms respectively with avhrr sst while fig 3 c which is simply a difference of fig 3 a and b is plotted to highlight the improvements degradations in correlation due to assimilation positive negative values correspond to improvements degradations the correlation is more than 0 8 in most of the domain for both letkf roms fig 3 a and o roms fig 3 b however the correlation of sst from o roms in the eastern bob eastern equatorial indian ocean and parts of western equatorial indian ocean is less than 0 6 assimilation manages to improve the temporal variability of sst in most of the aforementioned region except eastern equatorial io the reason is that the observation density ratio of the number of observations per day and the area of the boxed region excluding land region over the eastern indian ocean region r2 in fig 3 a and b is significantly low compared to the regions where significant improvements are seen for example region r1 in fig 3 a and b this is illustrated in fig 3 g such sparse observations coupled with a limited localization radius of 200 km limits the scope of improvement in the eastern equatorial indian ocean we have refrained from using a larger localization radius because that may introduce spurious correlation length scales kirchgessner et al 2014 root mean squared deviation rmsd of sst which quantifies the deviations of the model states from the observations are shown for letkf roms fig 3 d and o roms fig 3 e in fig 3 f the difference between fig 3 d and e is plotted to highlight the improvements and degradations due to assimilation negative positive values correspond to improvements degradations due to assimilation there appear to be significant improvements in most of the region except over some patches in the central and eastern part of the domain fig 3 f both the systems show that the model manages to estimate the amplitude of sst in the north indian ocean within an error of 1 c the rmsd in sst in o roms however reaches as large as 2 c in the southern indian ocean fig 3 e such a large error in sst is likely to be translated into an error in sea level anomaly sla through steric effects followed by an erroneous estimation of geostrophic currents the assimilation has significantly reduced the error to less than 1 c over most of the south indian ocean region fig 3 d the sst over the eastern coast of sumatra in letkf roms looks inferior to o roms possibly due to the contamination of satellite estimates by territorial boundaries and undetected clouds ricciardulli and wentz 2004 the period of our study is a little more than two years the correlation with observed sst anomaly and model sst anomaly from o roms at these locations is about 0 91 assimilation enhances this correlation to 0 95 the improvement in correlation due to assimilation is not very apparent however if we focus on the high frequency variability of sst by high pass filtering the observation and model sst with a cutoff of 200 days the improvements are striking the model sst anomaly derived from o roms and letkf roms at time scales smaller than 200 days is plotted against in situ observations spread across the indian ocean in fig 4 a and b respectively it is not surprising to see that the spread is reduced in letkf roms which has anomaly correlation coefficient acc of 0 75 compared to an acc of 0 66 in o roms the improvement is particularly evident if we focus on the moored buoy located in the eastern indian ocean yellow dot in the inset of fig 4 b and bay of bengal pink dot in the inset of fig 4 b the large spread in fig 4 a is reduced extensively by assimilation in fig 4 b this is corroborating the results obtained in fig 3 a and b but from a different observation network letkf roms is therefore more skillful in simulating the high frequency sst response of the ocean compared to o roms the ability of the model to capture the subsurface temperature structure is also analyzed we focus mainly in the upper 180 m this is due to the fact that the assimilation free o roms satisfactorily simulates the deeper layers figure not shown whereas it has large scope for improvement in the upper 180 m this analysis is done using temperature data obtained from the rama buoy at equator 80 5 e fig 5 both the systems o roms and letkf roms are able to reproduce the time averaged temperature variations similar to that of the observations fig 5 a the assimilation on the other hand has reduced the error throughout the depth fig 5 b whereas the reduction is about 0 2 c at the surface it progressively improves to 0 5 c at the thermocline the assimilated model however still falls short in predicting the thermocline by 1 c the standard deviation std of the observations and the two systems reveal that assimilation experiments are able to better reproduce the variability throughout the depth fig 5 b in fig 5 c we plot the correlation of temperature estimated from o roms and letkf roms with rama observation across depths both the systems show positive correlation throughout the depth of our analysis the correlation at the surface is 0 98 for letkf roms compared to 0 8 for o roms which we believe is a large improvement this significant difference is maintained throughout the depth of our analysis we have also looked into the skill of letkf in simulating the temperature profile along an argo trajectory fig 6 in the bay of bengal o roms simulates a warmer sst during may 2017 fig 6 c whereas letkf roms manages to reduce this unusual warmness fig 6 d which is not observed in the argo profile fig 6 a letkf roms not only simulates the surface temperature variability better than o roms fig 6 d it also manages to simulate an improved magnitude fig 6 e it appears that o roms better simulates the sub surface temperature variability between the depths of 50 m and 70 m but the performance deteriorates beyond this depth the temperature profile from o roms even gets decorrelated beyond 80 m in contrast the vertical variability in temperature reproduced by letkf roms is more robust whereas the rmsd in sst in o roms is larger than 1 c letkf reduces that difference down to less than 0 5 c which is a significant reduction the rmsd from letkf roms is consistently smaller than o roms all along the vertical section however both the systems fail to reproduce the observed strong rise in the 20 c during march 2017 in fig 7 a fig 7 b we have plotted the time evolution of root mean squared differences between the model temperature salinity and observed temperature salinity which were assimilated in the observation space averaged over all the three spatial dimensions of the entire indian ocean domain for both letkf roms blue curve and o roms red curve we see that there is a sharp reduction in global rmsd in temperature salinity by 0 5 c 0 1 psu we have also plotted the vertical profile of rmsd between the model temperature salinity and observed temperature salinity in the observation space averaged over all the spatial dimensions in the horizontal direction and time in fig 8 a fig 8 b for both letkf roms blue curve and o roms red curve we observe that there is a similar reduction of 0 5 c 0 08 psu in the thermocline region beyond the thermocline the difference in both temperature and salinity gradually decreases with depth but does not disappear altogether because of the absence of vertical localization i e even the deeper layers are affected by assimilation in the absence of vertical localization sea level anomaly sla estimates from o roms and letkf roms are validated using archiving validation and interpretation of satellite oceanographic aviso quarterly gridded daily product aviso 2009 no observations of sla were assimilated correlation of sla from letkf roms and o roms against aviso are shown in fig 9 a and b respectively the differences in the correlation pattern is highlighted in fig 9 c assimilation has improved the correlation significantly over western bob and parts of south indian ocean however there are multiple patches in the southern extreme of the domain that suffer from negative correlation fig 9 a b the southern indian ocean is connected to the southern ocean through the southern boundary of our domain at 30 s barotropic dynamics is prevalent in the southern ocean fu 2003 rohith et al 2019 weijer 2010 since these dynamics are fast an exchange of information through the southern boundary every 5 days does not suffice to simulate the southern part of our domain assimilation has reduced the error in o roms over western bob western equatorial indian ocean and south indian ocean by 2 4 cm fig 9 d and e to highlight the improvements degradations in rmsd a difference of the two rmsds is plotted in fig 9 f it is seen that neither letkf roms nor o roms is particularly skilled in simulating the sla amplitudes this is not a surprise because these models are not known to accurately simulate sla over finer length scales which would require accurate initial conditions and accurate representation of processes in the model dynamics a minor shift in eddy location can potentially give rise to several centimeters of rmsd in sla however a reasonably good model should be able to simulate the variability in sla in fig 9 g i the standard deviation of sla is plotted from letkf roms o roms and aviso respectively it is seen that letkf roms is more adept in simulating the standard deviation in sla compared to o roms it however fails to simulate the variability in the southern section of the domain possibly because of the inadequate low frequency boundary exchanges discussed earlier hovmoller diagram of sla along 8 s from aviso fig 10 a letkf roms fig 10 b and o roms fig 10 c show that rossby waves are better captured in letkf roms than o roms o roms tends to simulate a more intense rossby wave resulting in an anomalous deepening of the thermocline fig 10 d e and f corresponds to rama observation letkf roms and o roms respectively and consequently the sst anomaly is larger at the surface assimilation prevents such contaminations to a large degree resulting in an improved estimation of surface and subsurface temperature properties this result is supported by the correlation and rmsd of letkf roms blue curve fig 10 g and h and o roms red curve fig 10 g and h with respect to the observation ocean currents both the systems o roms and letkf roms are able to capture the robust seasonal current systems in the indian ocean such as the eastward flowing equatorial counter current during winter westward flowing north equatorial current during winter the seasonally reversing current systems such as east india coastal current west india coastal current somali current and also the permanent westward flowing south equatorial current the eastward flowing equatorially trapped wyrtki jets that develop during inter monsoon periods october november and may june are well captured in both the systems fig 11 the o roms overestimates jets fig 11 b and shows larger bias than letkf roms when compared to ocean surface current analysis real time oscar that combines geostrophic ekman and stommel shear dynamics using the prescription of bonjean and lagerloef 2002 involving quasi linear approximation and steady state momentum dynamics i e it neglects local acceleration in the formulation it is however generally accepted that the quality of oscar current is debatable in the equatorial belt the inferences from comparisons using oscar currents in the equatorial belt are therefore less trustworthy in general o roms shows strong westward current anomaly during winter season along northern part of the equator fig 11 b in contrast letkf roms shows fewer anomalies during the summer and winter monsoon periods the o roms overestimates the strength of the equatorial currents as compared to the oscar currents whereas the bias is less in the letkf roms fig 11 however oscar currents are known to be underestimated during may and october sikhakolli et al 2013 fig 12 a and b shows the correlation of zonal current from letkf roms and o roms with respect to oscar zonal current respectively fig 12 c highlights the differences in these two correlation patterns there are major improvements in the correlation across most regions in the basin i e letkf roms better simulates the variability in the zonal current assimilation has improved the correlation in the bay of bengal and arabian sea this result is also corroborated by the plot of the standard deviation of zonal current from letkf roms o roms and oscar current in fig 12 g h and i respectively whereas o roms shows a much stronger variability of zonal current in the equatorial belt and the eastern coast of africa than the observed variability assimilation weakens it to a good extent and pushes it towards observed variability in fig 12 d and e we plot the rmsd of zonal current from letkf roms and o roms with respect to oscar zonal current fig 12 f highlights the difference in rmsd of the patterns of fig 12 d and e a negative positive difference indicates improvements degradations due to assimilation there has been significant improvement not only in the equatorial belt but also along both the coast of india and along major fraction of the african coast except the mozambique channel the rmsd is less than 20 cm s in the arabian sea and the south indian ocean in both the systems the assimilation reduces the error considerably in the central bay of bengal the reduction is as large as 15 cm s in the equatorial indian ocean and east coast of africa the rmsd is as large as 45 cm s for o roms and 30 cm s for letkf roms there has been a minimal improvement in rmsd in the southern indian ocean as well the reason is evident if we inspect the observations that have been assimilated during the period of our study fig 12 j the southern indian ocean has the least observation density of in situ t and s profiles and that undermines the steric height correction in sla reflected in the analysis of sla as well in fig 9 and consequently no significant improvement in geostrophic currents which constitutes a major fraction of the current we choose to validate the performance of the currents with an adcp located at equator 80 5 e this adcp is chosen particularly because of uninterrupted data availability during the first year of our study we plot cos θ across depths where θ is the angle made by the current vector obtained from this adcp with the equator fig 13 a values close to 1 indicate that the direction of currents is primarily zonal while values close to zero indicate the direction as mostly meridional it is evident from fig 13 a that the current is mostly zonal during the period of our study across depths hence in the subsequent analysis we focus only on the zonal component of the current unless otherwise mentioned letkf roms manages to reproduce the observed time averaged vertical variability of zonal current from adcp located at equator 80 5 e up to 120 m beyond 120 m it follows the depth profile of zonal current estimated by o roms whereas the error in the estimation of mean zonal current at the surface by o roms is about 18 cm s fig 13 b the rmsd of the zonal current estimation by o roms during the period of our study is large at the surface 37 cm s and monotonically decreases to about 20 cm s at a depth of 140 m letkf roms on the other hand exhibits a lesser rmsd at the surface 28 cm s the rmsd decreases with depth before merging with the o roms profile at about 150 m fig 13 c solid lines the standard deviation of zonal current in letkf roms follows closely the standard deviation of the adcp observation while the standard deviation in o roms is larger till about top 100 m following which it mimics the observation fig 13 c dotted lines the zonal current in o roms exhibits a slightly higher correlation at the surface than letkf roms fig 13 d red curve however the vertical profile of the correlation exhibits a large variability and almost gets decorrelated at deeper depths less than 0 15 in contrast the correlation of zonal current from letkf roms shows a more robust vertical profile even at deeper depths it shows a correlation of about 0 4 fig 13 d blue curve time depth section of zonal current profiles obtained from the same adcp observation letkf roms and o roms shows that letkf roms manages to reproduce the observed spatio temporal variability to a much better degree than o roms fig 14 o roms tend to overestimate the currents the large currents seen during october december wyrtki jet is reflected in both letkf roms and o roms however this intense current persists for a longer time interval in o roms letkf roms manages to better simulate the current reversals both in surface and subsurface brought forth by the changing winds the high frequency hf radar records hourly surface currents every 6 km over a radial distance of 200 km from its stationed location in the present validation we have compared the daily averaged surface currents from letkf roms and o roms with respect to an hf radar observations situated on the east coast of india at kalpakkam 12 5 n 80 2 e and cuddalore 11 7 n 79 8 e the data is available only for the first year of our study the surface current is along the coast throughout the year schott et al 2009 shankar et al 2002 either poleward during summer or equatorward during winter and hence the meridional current dominates on a daily time scale we plot the correlation rmsd of the meridional surface current obtained from hf radar with respect to the meridional surface current from letkf roms in fig 15 a fig 15 d and o roms in fig 15 b fig 15 e while the differences between fig 15 a fig 15 d and fig 15 b fig 15 e in these patterns are plotted in fig 15 c fig 15 f the large positive difference in correlation fig 15 c and large negative difference seen in rmsd pattern fig 15 f indicates that the letkf roms manages to better estimate the amplitude of the surface meridional current in fig 15 g we plot the time evolution of spatially averaged rmsd of the meridional surface current from letkf roms blue curve and o roms red curve there is an appreciable reduction in rmsd particularly during the latter half of the period of our study letkf roms likely improves the coastal circulation along the eastern coast of india this inference however is precariously based on a single observation covering a small fraction of the entire eastern coast of india we have also verified the performance of letkf roms along the west coast of india against an adcp observation 15 2 n 72 7 e situated near goa india we could only manage to analyze the performance for the period aug 2016 to sep 2017 due to non availability of data beyond that period there appears to be a strong off shore sub surface zonal current directed away from the coast in the observation during early october to end of february fig 16 a o roms on the other hand estimates a strong off shore zonal current that is directed towards the coast during mid november to january fig 16 e letkf roms manages to suppress this to a good extent and even estimates a weak off shore subsurface zonal current directed away from the coast during this period fig 16 c however both letkf roms and o roms fail to estimate the strong sub surface zonal current beyond 150 m of depth during the month of december the meridional sub surface current from letkf roms fig 16 d better mimics the observations fig 16 b than o roms fig 16 f letkf roms manages to reproduce the strong poleward sub surface meridional current during september february to a reasonable extent whereas o roms occasionally reverses the direction to equatorward during this period however none of the system managed to reproduce the observed strong equatorward subsurface current beyond 150 m during november a quick look at the statistics confirm that letkf roms perform a much better job in estimating the subsurface currents the subsurface zonal current in o roms is decorrelated or even shows a negative correlation with the adcp observation solid red curve in fig 16 g whereas subsurface zonal current from letkf roms solid blue curve in fig 16 g shows a positive correlation ranging from 0 4 to 0 2 the subsurface meridional current from o roms shows an improved correlation compared to its zonal counterpart that progressively decreases with increasing depth dashed red curve in fig 16 g the performance of letkf roms is even better dashed blue curve in fig 16 g the correlation at 50 m is about 0 8 and it monotonically decreases with increasing depth there appears to be no difference between letkf roms and o roms beyond 200 m of depth the rmsd of the zonal meridional subsurface current fig 16 h from letkf roms blue curves is smaller in the upper layers of the arabian sea compared to o roms red curves it is evident that letkf roms is better in reproducing the amplitude and the variability of the subsurface current off the coast of goa it is important to estimate a realistic phase and amplitude of the currents which are responsible in transporting heat and mass and which is the most important ingredient to leeway models breivik et al 2013 breivik and allen 2007 that helps in saving lives through search and rescue operations 5 surface layer temperature inversions in the bay of bengal surface layer temperature inversion which is a warm layer of water sandwiched between surface and subsurface cold waters is the hidden pocket of heat content in the ocean it can influence sst variability on a range of time scales e g wang et al 2012 show for subseasonal girishkumar et al 2013 and nagura et al 2015 show for seasonal to inter annual time scales it can also potentially influence tropical cyclones neetu et al 2012 bay of bengal hosts thick layers of temperature inversions e g thadathil et al 2016 2007 2002 girishkumar et al 2013 these inversions are concentrated mostly in the northern parts of the bob and are particularly large during winter october march thadathil et al 2016 here we analyze temperature inversions from model and observations in northern parts of the bay of bengal during winter to demonstrate the skill of letkf roms in capturing sub seasonal variations fig 17 displays depth time evolution of temperature during the winter of 2016 17 at two locations viz 15 n 90 e and 18 n 90 e that also host moored buoys the top most panels depict the evolution of observed temperature at these two locations the second row and the third row in fig 17 depict the evolution of temperature at these two locations derived from letkf roms and o roms respectively the bottom panel shows the number of tracer observations that were assimilated within a radius of 200 km around these two locations both the locations experience large temperature inversions during january february helped by thick barrier layer with warm waters of temperature reaching up to 28 c in letkf roms the magnitude of the temperature inversion reaches 2 c in both the systems the magnitude of the temperature inversion and the thickness of the barrier layer which is a difference between mixed layer depth and the isothermal layer depth hosting the inversion layer are comparatively larger at 18 n 90 e than at 15 n 90 e these results are consistent with the earlier studies e g thadathil et al 2016 the barrier layer thickness and magnitude of the temperature inversions are overestimated by o roms letkf roms improves the estimation of barrier layer thickness it can be inferred from fig 17 c and d that letkf roms represents the inversions at reasonable skill indirectly letkf roms manages to simulate the halocline better in northern bay of bengal during boreal winters compared to o roms which falls short of expectations owing to absence of river runoff in the model for instance the thickness of the large temperature inversion during january february at 15 n 90 e and the timing of the events are well simulated in letkf roms the timing of the events and thickness of these inversion layers are not that accurate in o roms the sharp rise in isothermal layer depth ild in fig 17 which is the depth at which the temperature decreases by 1 c from the sst during early october 2016 and late february 2017 at 15 n 90 e a signature of strong mixing is not captured in o roms whereas letkf roms manages to simulate it to a reasonable extent at 18 n 90 e o roms estimates a deeper ild and hence a larger barrier layer thickness inaccurate salinity stratification and subdued mixing compared to the observation and letkf roms at all times the improvement is more pronounced at 15 n 90 e compared to at 18 n 90 e which lies closer to the river mouth of ganges and brahmaputra this may be attributed to the absence of river discharge fluxes in the model and no sparse temperature and salinity observations as presented in fig 17 g and fig 17 h results from letkf roms are likely to improve further with improved observation coverage in the region 6 summary and discussion an ensemble kalman filter based indian ocean data assimilation system is implemented using letkf and 1 12 1 12 roms this system comprises of 80 ensemble members and assimilates in situ temperature and salinity profiles from a plethora of observation networks ranging from moored buoys to argo floats it also assimilates satellite track data of sst coarse grained over a length scale of 25 km in order to downscale the dense data this system is different from many existing data assimilation systems even though a healthy atmospheric ensemble forces the oceanic system letkf roms the spread of the oceanic ensemble often collapses unless appropriate measures are taken in order to maintain a healthy spread the model configuration among the oceanic ensemble members were tweaked one half of the ensemble members follow kpp mixing scheme whereas the other half follow mellor yamada mixing scheme the diffusion coefficient of the tracers and the viscosity coefficients of the momentum variables were perturbed around a mean value in each of the ensemble members this strategy in addition to a covariance inflation of 10 after each analysis step ensures that the spread does not collapse in time this approach is metaphorically identical to perturbations in atmospheric fluxes or lateral boundary conditions or river fluxes to contain filter divergence lima et al 2019 this assimilation system exhibits an improved ocean state evolution when compared to the non assimilated system the sst analysis is much better correlated to the avhrr sst and significantly decreases the rmsd across the entire domain in comparison to the assimilation free model unlike the non assimilated system letkf roms facilitates a more robust vertical estimation of temperature profile in terms of correlation standard deviation and rmsd when compared to the rama moorings letkf roms simulates sst anomaly to a better degree of accuracy in comparison to that of o roms in the time scales less than 200 days we have shown that the likely reason lies in the over estimation of rossby wave amplitude in the o roms leading to deepening of the 20 c isotherm and consequently a degraded sst anomaly this will likely ensure an improved simulation of biogeochemistry if inducted into the model there is no localization in the vertical direction the satellite swath data is assimilated to the top layer of the model temperature and the information is subsequently passed to the vertical layers below the weightage at each layer depends on the model error covariance the effect of assimilating satellite sst swath data therefore does not only affect the surface temperature of the model but runs deep into the subsurface layers in order to understand the effect of sst assimilation on subsurface layers we have performed a controlled experiment wherein only sst observations were assimilated in fig 18 we show vertical profile of temperature from the analysis posterior red curve and background prior blue curve state of the controlled experiment at an arbitrary location of 46 75 e and 11 75 n on nov 18 2016 the first day when sst observations were available and assimilated the analysis expectedly moves towards the observed sst value depicted as a star in fig 18 it is also apparent that the innovation is relayed immediately to deeper layers the innovation decreases with increasing depth the mixed layer depth is altered immediately deepened by about 1 5 m leading to a change in the instantaneous subduction rate which depends on the rate of change of mixed layer depth williams et al 1995 this facilitates a volume flux into out of the thermocline from the mixed layer if the subduction rate is positive negative thereby modulating the thermocline as well the most profound improvements were seen in currents whose observations were not assimilated o roms often overestimates the currents particularly during strong events in the indian ocean like the wyrtki jet propagations in may june and october november assimilation of temperature and salinity in letkf roms restores the estimation of currents closer to reality both in surface and subsurface regions and both in the open ocean and in the coastal region the improvements are however nominal in the southern indian ocean we have shown that the cause can be attributed to sparse observations of temperature and salinity profiles in the southern indian ocean compared to other parts of the basin the non availability of temperature and salinity observations likely does not improve the steric effects in sea surface height which is also not assimilated resulting in incorrect estimation of geostrophic currents in the southern indian ocean an improved estimation of currents in the ocean is vital to represent the transport of mass and tracers also it should result in a better evaluation of probable search zones of lost persons objects predicted by models during search and rescue operations breivik and allen 2007 the estimation of currents can be further improved if sla is also assimilated particularly we believe that the surface currents in the southern indian ocean can potentially improve significantly if satellite track data of sla is assimilated inclusion of sla assimilation into this system is reserved for future work another major improvement is seen in the temperature and salinity estimates in northern bay of bengal a region known for its propensity to give birth to temperature inversions particularly during boreal winters even though the ocean model is not injected with river water discharges assimilation of in situ temperature and salinity profiles and satellite sst takes care of the absence of fresh water discharges to a reasonable extent and realistically simulates the barrier layer thickness and isothermal layer depths however the improvements are less profound close to the river mouths because of sparse data coverage one major caveat of the present study is the limited period of study which amounts to about 778 days we had to settle for such a limited period because of the non availability of ensemble of atmospheric fluxes the initial condition of the ensembles in letkf roms is generated through perturbation of a stable initial condition obtained from o roms it is expected that the ensemble members in letkf roms will generate transients which is likely to subside with the evolution of time while the barotropic transients shall subside in a few weeks owing to its faster propagation the baroclinic transients near the surface is likely to take a few years to dissipate these transients shall contaminate the ocean state during the initial period of our study and hence limit the scope of improvement letkf roms on the other hand shows significant improvements over o roms even during this transient period and it is therefore expected that letkf roms will further improve once the transients die down completely a larger forecast length compared to the data assimilation cycle provides increased confidence on the system in the present scenario the length of the data assimilation cycle and the forecast length is identical i e 5 days however our system has been developed for short term forecasts typically amounting to 3 5 days to be used for operational use in this context the performance of letkf roms has been convincingly shown to be better than o roms inclusion of two mixing schemes in letkf roms as a measure to arrest filter divergence has its own disadvantages the controlled experiments show that the use of two mixing schemes across the ensembles increases the spread in sst and turns the initial gaussian distribution in sst into a bimodal distribution however assimilation subdues the bimodality of the distribution dashed black curve in fig 2 thereby restoring it close but not entirely to a gaussian distribution as long as the departure from gaussian distribution is not very strong this strategy is supposed to work without any ado this system therefore demands periodic inspection of the distribution of spread however a long integration is needed to look at the stability of the system that periodically deviates from gaussian distribution a more systematic effort is needed to convincingly establish the merit of using multiple mixing scheme a more refined way to include multiple mixing schemes in a data assimilation framework shall be to explore ways to reduce the severity of the differences in the mixing schemes our approach though crude in nature is probably the first approach in this direction and would likely stimulate further research in this direction the assimilation of observations 104 using this technique in a 80 member setup takes only about a couple of minutes using 256 processors xeon e5 2670 8c 2 6 ghz with linpack performance rmax 719 22 tflop s most of the time is consumed in evolving the model state in time even then one assimilation cycle takes about 60 min using 2000 processors for model evolution and 256 processors for assimilation this is operationally not very extensive declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments funded by ministry of earth sciences moes ssr and ap acknowledge the training on letkf mom by prof eugenia kalnay and her team dr travis sluka and dr steve penny at the university of maryland under the monsoon mission i project funded by ministry of earth sciences moes govt of india the data assimilation system is configured as a part of the ocean modeling data assimilation and process specific observations o mascot programme coordinated by the indian national centre for ocean information services incois and funded by the ministry of earth sciences moes govt of india ref moes 36 oois o mascot 2017 the authors wish to thank dr kunal chakraborty for providing the boundary condition for roms and dr francis pavanathara and dr abhisek chatterjee for setting up the operational model o roms authors also thank the osf team incois for providing the ocean state forecasts from the present operational roms against which letkf roms is compared with the authors would also like to thank dr munmun das gupta for providing the ocean observations from ncmrwf and the data management team from incois for providing the data all the experiments were conducted on the high performance computer aditya iitm pune india the support from aditya hpc team is highly appreciated matlab was used for generating the figures this is incois contribution number 355 
24013,a high resolution ocean circulation model for the indian ocean io using regional ocean modeling system roms is operational at indian national centre for ocean information services incois which provides ocean state forecasts for the bay of bengal bob and the arabian sea as to the indian ocean rim countries to provide an improved estimate of ocean state a variant of ensemble kalman filter enkf viz the local ensemble transform kalman filter letkf has been developed and interfaced with the present basin wide operational roms this system assimilates in situ temperature and salinity profiles and satellite track data of sea surface temperature sst the ensemble members of the assimilation system are initialized with different parameters like diffusion and viscosity coefficients and are subjected to an ensemble of atmospheric fluxes in addition one half of the ensemble members respond to k profile parameterization mixing scheme while the other half is subjected to mellor yamada mixing scheme this strategy aids in arresting the filter divergence which has always been a challenging task the assimilated system simulates the ocean state better than the present operational roms improvements permeate to deeper ocean depths with better correlation and reduced root mean squared deviation rmsd with respect to observations particularly in the northern indian ocean which is data rich in density analysis shows domain averaged rmsd reduction of about 0 2 0 4 c in sea surface temperature and 2 4 cm in sea level anomaly the assimilated system also manages to significantly improve the thickness of the temperature inversion layers and the duration of its occurrence in northern bay of bengal the most profound improvements are seen in currents with an error reduction of 15 cm s in zonal currents of central bay of bengal keywords roms ocean data assimilation letkf indian ocean 1 introduction improving ocean state forecast is necessary because a large proportion of the world population roughly about 40 resides within 100 km from the coastline or in the low elevation coastal zone center for international earth science information network ciesin of columbia university 2006 csd coastal population indicator data and methodology page http sedac ciesin columbia edu es csdcoastal html quite a few of them venture out into the sea for fishing and other marine activities and they require periodical information about the state of the ocean it is therefore imperative to provide ocean state forecasts bereft of errors lest it might adversely affect the coastal population the ocean state is estimated through general circulation models that numerically solve the dynamical equations for the momentum sea level anomaly and tracer variables like temperature and salinity the errors in the estimation however tend to grow in time due to various reasons ranging from inaccurate initial condition approximated parameterizations inaccurate atmospheric fluxes and boundary conditions if it is a regional model leading to a different realization of intrinsic ocean variability at a multitude of spatio temporal scales sérazin et al 2015 the inherent inability of the model to resolve length scales finer than its prescribed grid size also tends to contribute to these errors all these errors tend to diverge the model state from truth thereby gradually driving the forecasts far from the observed reality data assimilation da can however arrest the divergence of the model trajectory by periodically correcting the simulated ocean state using observations thereby providing improved estimates of the state of the ocean da is a method that imbibes information from both the model state and observation statistically churns them to reduce the cost function errors and spits out an initial condition for the model commonly known as analysis that is closer to the truth than either the input prior model state or the input observation in terms of uncertainty and root mean squared error asch et al 2016 bennett 2002 daley 1991 evensen 2009 kalnay 2003 lewis et al 2009 malanotte rizzoli 1996 park and xu 2009 the model is then restarted with this initial condition that ensures that the trajectory remains closer to reality for at least a short span of time this cycle is repeated typically at a fixed frequency an advanced data assimilation system has been developed using local ensemble transform kalman filter letkf hunt et al 2007 letkf is an intuitive amalgamation of local ensemble kalman filter lekf ott et al 2004 and ensemble transform kalman filter etkf bishop et al 2002 wang and bishop 2003 wherein letkf exploits the qualitative superiority of lekf and computational efficiency of etkf letkf has been used in multiple systems ranging from meteorology hoffman et al 2010 miyoshi et al 2010 2007 miyoshi and kunii 2012a b miyoshi and yamane 2007 seko et al 2011 szunyogh et al 2005 to oceanography penny 2014 2011 penny et al 2015 2013 with abundant success it has been used in regional ocean models like roms hoffman et al 2012 ecom hoffman et al 2008 pom miyazawa et al 2012 xu et al 2013 xu and oey 2014 and in global models like mom penny 2011 penny et al 2015 presently a high resolution operational model hencefortho roms using regional ocean modeling system roms provides ocean state forecasts to the indian ocean rim countries francis et al 2013 the o roms suffers from large biases in temperature and salinity particularly in the north indian ocean further there are large scope of improvements for surface and sub surface currents in the equatorial indian ocean the present system i e o roms does not assimilate any observations and hence is prone to accumulate and propagate errors in time in order to improve the performance of o roms letkf developed as a separate module is interfaced with this operational model for indian ocean region the in situ temperature and salinity profiles from multiple observation networks like argo floats xbts mooring ship tracks etc and satellite derived sea surface temperature sst from amsr e are assimilated results from this newly developed assimilation system for the basin wide indian ocean is presented in this work we henceforth call this assimilated system as letkf roms this assimilated system introduces a combination of mixing schemes and model parameters across the ensemble members as an additional prescription for arresting filter divergence the objective of this work is two fold to highlight that such a strategy appears to be beneficial to the system and indeed stabilizes the spread in the ensemble at least during short integration times which is susceptible to collapses in ensemble based schemes and to highlight that letkf roms is better suited to estimate ocean states in the indian ocean compared to o roms the improvements in the state estimation of prognostic variables from letkf roms in comparison to o roms are presented in this paper however the primary emphasis has been put on ocean currents which is a non assimilated variable section 2 provides a brief description of the roms model setup section 3 describes the letkf assimilation system section 4 presents results and improvements in state estimation of temperature salinity sea level anomaly and currents over o roms in section 5 we show that letkf roms in comparison to o roms is better equipped to estimate thermal inversions in the northern bay of bengal a region where temperature inversions are found in abundance girishkumar et al 2013 thadathil et al 2016 a summary and discussion of the results of this study is outlined in section 6 2 brief description of o roms the model used for estimating the ocean state of the indian ocean is the regional ocean modeling system roms version 3 7 roms is a free surface terrain following general circulation model which solves a set of primitive equations in an orthogonal curvilinear coordinate system haidvogel et al 2000 haidvogel and beckmann 1999 shchepetkin and mcwilliams 2005 song and haidvogel 1994 the domain of the indian ocean model based on roms i e o roms extends from 30 e to 120 e in the east west direction and from 30 n to 30 s in the north south direction the horizontal grid spacing of o roms is 1 12 approximately 9 km and it has 40 sigma levels in the vertical the vertical stretching parameters are chosen such that the vertical scales are finely resolved in the upper part of the ocean this is important because processes like diurnal radiation surface mixed layer restratification cycles bernie et al 2005 brainerd and gregg 1993 need to be judiciously resolved in the upper layers of the ocean the lateral boundary in the east is connected to the pacific ocean through the indonesian throughflow itf while the southern boundary is interfaced with the southern ocean on the other hand the northern and western boundaries are guarded by coastlines and no slip boundary conditions are employed the model exchanges information with the boundary every 5 days it is to be noted that due to these low frequency exchanges across the boundary fast moving barotropic waves that originates outside the stated indian ocean domain will not be efficiently resolved at the boundaries and remote effects due to these modes will be ignored resulting in likely escalation of errors in ocean state estimate fu 2003 rohith et al 2019 weijer 2010 this may be significant at times rohith et al 2019 particularly during the boreal winters when madden julian oscillation is known to generate strong wind stress anomalies and hence large barotropic fluctuations close to the north west coast of australia marshall and hendon 2014 wheeler et al 2009 wheeler and mcbride 2005 zhang 2013 10 day delayed ocean reanalysis derived from the incois godas analysis sivareddy 2015 provides the boundary condition to the tracer and momentum fields the eastern and southern boundaries in o roms therefore respond to an assimilated product and hence strictly speaking o roms is not absolutely free from assimilation however the effects of assimilation penetrating through boundary effects are likely to be negligible compared to the assimilation of observations in the interior of the domain and hence we pretend that o roms is free from assimilation in addition the ensemble models in letkf roms also respond to the same boundary conditions derived from incois godas therefore any improvements observed in letkf roms can be attributed to the assimilation of observations in the interior of the domain the model uses unless otherwise mentioned the k profile parameterization kpp mixing scheme large et al 1994 to parameterize the vertical mixing smagorinsky type viscosity along with harmonic and biharmonic horizontal mixing and diffusion schemes muschinski 1996 are chosen for horizontal mixing and a bulk parameterization scheme fairall et al 1996 is chosen for the computation of air sea fluxes of heat six hourly atmospheric fields obtained from national centre for medium range weather forecast ncmrwf george et al 2016 derived from ncmrwf unified model ncum at a horizontal resolution of 25 km are used to force the ocean state in o roms in addition the tidal forcing from tpx07 2 model is incorporated at the southern and eastern open boundaries the model however does not account for river runoff instead the model states are relaxed to monthly climatology of sea surface salinity antonov et al 2010 locarini et al 2010 with a relaxation time scale of 30 days o roms is able to capture the spatio temporal variabilities in the ocean with a fair amount of success including the salinity field in spite of the absence of river runoff fluxes there is however still room for improvement in the state variables francis et al 2013 and hence it calls for corrective measures particularly because this model is used for short term ocean state forecasts 3 letkf roms we implement local ensemble transform kalman filter hunt et al 2007 as the data assimilation scheme to regional ocean modeling system as the corrective measure to improve the quality of the ocean states it is a variant of ensemble kalman filter wherein the model error covariance is approximated as a sample covariance derived from the ensemble members rank is k 1 where k is number of ensemble members the sample covariance asymptotically converges to model error covariance when the ensemble number approaches infinity we have used 80 ensemble members in our study the inaccuracy introduced due to this approximation is traded off with the sharp decrease in computational resources and runtime the 80 member ensemble atmospheric fields obtained from gfs model implemented at ncmrwf prasad et al 2016 was used to force the ocean model component of letkf roms we prepare the initial ensemble by random sampling xu and oey 2014 from an ocean state derived from o roms the ocean state is perturbed with a gaussian random number whose mean is zero and standard deviation is σ x y z daily σ x y z is estimated from a set of 56 ensemble member ocean states obtained through an ensemble run of roms forced with 20th century atmospheric reanalysis ensemble fluxes compo et al 2011 for five years from 2003 2008 the spread of the ensemble during those five years of run was robust σ x y z of aug 15 2008 was chosen to generate the spread of the initial ensemble of the present letkf roms set up that takes off from aug 15 2016 this ensured that the ensemble spread of the initial ensemble so generated remains consistent with the previous experiment and hence it is reasonable to expect that the ensemble members do not collapse in time we also divide the ensemble members into two equal halves one half uses kpp mixing scheme large et al 1994 while the other uses mellor yamada mixing scheme mellor and yamada 1982 this is done to ensure that the advantages of both the mixing schemes are exploited this will also facilitate in arresting filter divergence we also arbitrarily spread the value of coefficients like tracer diffusion constant and horizontal viscosity coefficients about the value used in o roms across the ensembles table 1 lists the values used in the o roms experiment this division in mixing schemes and using a range of coefficients in tracer and momentum dynamics aids further in keeping the ensemble spread intact and improves background error covariance structure du et al 2014 kwon et al 2016 meng and zhang 2007 stensrud et al 2000 vandenbulcke and barth 2015 however there is a likely caveat the sample error covariance in this system no longer necessarily asymptotically approaches the model error covariance when the number of ensemble members approach infinity we however pretend that it does to further safeguard ensemble collapse we inflate the ensembles after each analysis by 10 inflation an exercise routinely performed across data assimilation community to prevent ensemble collapse anderson and anderson 1999 bocquet and sakov 2012 miyoshi 2011 however there are reports that show that use of constant multiplicative inflation can lead to filter divergence particularly in sparsely observed areas miyoshi et al 2010 penny 2011 over long integration time periods even though there are areas which are sparsely observed in our system particularly in the southern hemisphere we do not observe such filter divergence likely because of the use ensemble fluxes and mixture of schemes or because the integration time duration is not large we use a gaussian profile for localization with an e folding scale of 200 km anderson 2007 nurujjaman et al 2013 ying et al 2018 i e the model state variables at any particular grid are minimally influenced by observations that lies beyond 700 km from that grid i e around 3 5 times the localization radius the multivariate covariance error matrix allows the observation of any particular sort to influence all the prognostic variables unless specifically designed to restrict the influence in letkf roms each observation influences all the prognostic variables that fall within the localization radius of its location in many assimilation systems a constant observational error oe is attached to the observations i e the error covariance of the observation is uniform in space and time which is an oversimplification in some systems the observation error has a vertical structure behringer et al 1998 penny 2011 balmaseda et al 2013 sivareddy 2015 but lacks horizontal and temporal variability we however estimate oe following the method suggested by etherton and bishop 2004 wherein a model is used to estimate the errors due to unresolved scales and processes thereby introducing spatio temporal variations into oe through representation error re etherton and bishop 2004 hodyss and nichols 2015 janjić et al 2017 janjić and cohn 2006 oke and sakov 2008 van leeuwen 2015 which partly occurs due to the deficiency of the model to accurately project its state on to the observation space this estimated re is included as a component of observation error along with the instrument error to represent the uncertainty of observations it is to be noted that observations are not error free and hence do not represent the truth the performance of our assimilation system improves once we include re into the observation error statistics sanikommu et al 2019 we have used 1 12 degree model as the high resolution model and 1 4 degree model as a low resolution model to estimate the spatio temporal characteristics of re ideally this should have been used as the representation error in a lower resolution system but instead we have used this re for the 1 12 resolution experiment being presented here we however would like to highlight that the origin of representation error of a 1 4 degree model is characteristically different from that of a 1 12 degree model calculating re from a higher resolution 1 12 model to be used for the present 1 12 resolution model is computationally taxing and has been avoided the off diagonal elements in the observational error covariance matrix are assumed to be zero i e the observational errors do not influence each other this is a reasonable approximation particularly for in situ observations however this approximation is questionable for satellite observations bormann et al 2010 stewart 2009 stewart et al 2013 2008 in spite of this we assume the off diagonal elements of observation error covariance of sst to be zero this assumption is intended to simplify inversion of error covariance matrices we assimilate in situ temperature and salinity profiles from observation networks that include argo floats and moored buoys within the indian ocean domain these are the same quality controlled observations prepared originally for assimilation in incois godas sivareddy 2015 sivareddy et al 2017 we however discard assimilating observations that are too close to the domain boundaries or too close to any landmass we also assimilate satellite track data of sea surface temperature sst obtained from amsr e however since the data along the track is very dense we coarse grain the data over a length scale of 50 km before feeding it into the assimilation system this superobbing of observations is a common practice while dealing with dense data cummings 2005 fu 2016 oke and sakov 2008 we assimilate observations every 5 days except the observations on the 5th day all observations of the previous four days are discarded the advantage is that assimilation does not affect the ocean state daily and allows the ocean to respond to the fluxes unhindered the daily output from letkf roms therefore comprises of five days of free run followed by assimilation on fifth day we are correcting the initial condition of the model once in 5 days before proffering the results from the validation we demonstrate the integrity of the letkf roms by examining the ensemble spread in the model variables fig 1 a f shows the ensemble spread in the surface temperature 5 m in the top panel fig 1 a c and subsurface temperature 100 m in bottom panel fig 1 d e at three stages of the period of our study the spread is larger in the sub surface than in the near surface owing to usual large uncertainties in the thermocline layers further the domain averaged spread averaged only over x and y in temperature appears to be fluctuating about some mean value both at the surface blue curve fig 1 g and at the subsurface red curve fig 1 g the average spread is robust in time and does not collapse the spread is maintained at reasonable magnitudes because of the strategies ensemble atmospheric forcing change of diffusion schemes etc outlined in the previous section each of these strategies i e ensemble fluxes and the combination of mixing schemes play a role in maintaining the spread across the ensemble in order to understand the contribution of each of these two strategies viz atmospheric ensemble fluxes and combination of mixing schemes three controlled experiments were performed using 80 ensemble members starting from the same initial condition as that of letkf roms the initial spread in any variable is same for all the experiments no data were assimilated in any of these three controlled experiments the configuration of the first experiment exp1 is similar to letkf roms except that no data were assimilated this experiment is meant to quantify the combined role of the two strategies in the estimation of ensemble spread in absence of any assimilation in the second controlled experiment exp2 the mixing combination was kept intact but all the ocean members of the ensemble were driven by the mean of the 80 member ensemble atmospheric fluxes the purpose of exp2 is to quantify the role of the combination of mixing schemes in the estimation of ensemble spread in the third experiment exp3 all ocean members of the 80 member ensemble respond to kpp mixing scheme but is driven by the 80 ensemble atmospheric fluxes the kpp mixing scheme is chosen out of the two mixing scheme to make it commensurate with o roms evidently the purpose of exp3 is to quantify the role of the ensemble atmospheric fluxes in the estimation of ensemble spread in fig 2 we plot the initial distribution of sst black curve and the distribution of sst across the ensemble members after 600 days of run at an arbitrary location at 93 e 0 n from exp1 blue curve exp2 green curve and exp3 red curve along with the distribution of sst from the letkf roms dashed black curve the spread increases in all the three controlled experiments even though the distribution was initially gaussian the use of combination of mixing schemes in exp1 and exp2 rendered it into a bimodal distribution the departure from normal distribution in exp3 red curve is minimal compared to exp1 and exp2 it appears that the right peak of the blue green curve closer to the peak of exp3 is the manifestation of kpp mixing scheme in exp1 exp2 while the left peak is a result of mellor yamada mixing scheme the mean of the distribution in exp1 exp2 and exp3 is 30 46 0 21 30 59 0 17 and 30 67 0 08 respectively whereas the corresponding standard deviations are 1 85 1 57 and 0 73 the combination of mixing schemes facilitate a larger spread in the ensemble members compared to the ensemble fluxes 4 results both o roms and letkf roms were run from august 2016 to september 2018 the period of study is limited because of the unavailability of ensemble surface fluxes the first day forecast of o roms is compared here with the ocean states derived from letkf roms it is to be noted that in every assimilation cycle of 5 days the first five days of the letkf roms is a free run akin to forecast followed by correction of initial condition on the fifth day sea surface temperature sst is validated against multiple sources of observations i e daily quarter degree gridded observation of avhrr satellite reynolds et al 2007 in situ daily observations of rama moorings mcphaden et al 2009 and niot buoys we have compared results obtained from letkf roms and o roms both the model results are projected on to the observation space and compared this is true for all the subsequent validation exercises carried out in this study fig 3 a and b shows correlation of sst from letkf roms and o roms respectively with avhrr sst while fig 3 c which is simply a difference of fig 3 a and b is plotted to highlight the improvements degradations in correlation due to assimilation positive negative values correspond to improvements degradations the correlation is more than 0 8 in most of the domain for both letkf roms fig 3 a and o roms fig 3 b however the correlation of sst from o roms in the eastern bob eastern equatorial indian ocean and parts of western equatorial indian ocean is less than 0 6 assimilation manages to improve the temporal variability of sst in most of the aforementioned region except eastern equatorial io the reason is that the observation density ratio of the number of observations per day and the area of the boxed region excluding land region over the eastern indian ocean region r2 in fig 3 a and b is significantly low compared to the regions where significant improvements are seen for example region r1 in fig 3 a and b this is illustrated in fig 3 g such sparse observations coupled with a limited localization radius of 200 km limits the scope of improvement in the eastern equatorial indian ocean we have refrained from using a larger localization radius because that may introduce spurious correlation length scales kirchgessner et al 2014 root mean squared deviation rmsd of sst which quantifies the deviations of the model states from the observations are shown for letkf roms fig 3 d and o roms fig 3 e in fig 3 f the difference between fig 3 d and e is plotted to highlight the improvements and degradations due to assimilation negative positive values correspond to improvements degradations due to assimilation there appear to be significant improvements in most of the region except over some patches in the central and eastern part of the domain fig 3 f both the systems show that the model manages to estimate the amplitude of sst in the north indian ocean within an error of 1 c the rmsd in sst in o roms however reaches as large as 2 c in the southern indian ocean fig 3 e such a large error in sst is likely to be translated into an error in sea level anomaly sla through steric effects followed by an erroneous estimation of geostrophic currents the assimilation has significantly reduced the error to less than 1 c over most of the south indian ocean region fig 3 d the sst over the eastern coast of sumatra in letkf roms looks inferior to o roms possibly due to the contamination of satellite estimates by territorial boundaries and undetected clouds ricciardulli and wentz 2004 the period of our study is a little more than two years the correlation with observed sst anomaly and model sst anomaly from o roms at these locations is about 0 91 assimilation enhances this correlation to 0 95 the improvement in correlation due to assimilation is not very apparent however if we focus on the high frequency variability of sst by high pass filtering the observation and model sst with a cutoff of 200 days the improvements are striking the model sst anomaly derived from o roms and letkf roms at time scales smaller than 200 days is plotted against in situ observations spread across the indian ocean in fig 4 a and b respectively it is not surprising to see that the spread is reduced in letkf roms which has anomaly correlation coefficient acc of 0 75 compared to an acc of 0 66 in o roms the improvement is particularly evident if we focus on the moored buoy located in the eastern indian ocean yellow dot in the inset of fig 4 b and bay of bengal pink dot in the inset of fig 4 b the large spread in fig 4 a is reduced extensively by assimilation in fig 4 b this is corroborating the results obtained in fig 3 a and b but from a different observation network letkf roms is therefore more skillful in simulating the high frequency sst response of the ocean compared to o roms the ability of the model to capture the subsurface temperature structure is also analyzed we focus mainly in the upper 180 m this is due to the fact that the assimilation free o roms satisfactorily simulates the deeper layers figure not shown whereas it has large scope for improvement in the upper 180 m this analysis is done using temperature data obtained from the rama buoy at equator 80 5 e fig 5 both the systems o roms and letkf roms are able to reproduce the time averaged temperature variations similar to that of the observations fig 5 a the assimilation on the other hand has reduced the error throughout the depth fig 5 b whereas the reduction is about 0 2 c at the surface it progressively improves to 0 5 c at the thermocline the assimilated model however still falls short in predicting the thermocline by 1 c the standard deviation std of the observations and the two systems reveal that assimilation experiments are able to better reproduce the variability throughout the depth fig 5 b in fig 5 c we plot the correlation of temperature estimated from o roms and letkf roms with rama observation across depths both the systems show positive correlation throughout the depth of our analysis the correlation at the surface is 0 98 for letkf roms compared to 0 8 for o roms which we believe is a large improvement this significant difference is maintained throughout the depth of our analysis we have also looked into the skill of letkf in simulating the temperature profile along an argo trajectory fig 6 in the bay of bengal o roms simulates a warmer sst during may 2017 fig 6 c whereas letkf roms manages to reduce this unusual warmness fig 6 d which is not observed in the argo profile fig 6 a letkf roms not only simulates the surface temperature variability better than o roms fig 6 d it also manages to simulate an improved magnitude fig 6 e it appears that o roms better simulates the sub surface temperature variability between the depths of 50 m and 70 m but the performance deteriorates beyond this depth the temperature profile from o roms even gets decorrelated beyond 80 m in contrast the vertical variability in temperature reproduced by letkf roms is more robust whereas the rmsd in sst in o roms is larger than 1 c letkf reduces that difference down to less than 0 5 c which is a significant reduction the rmsd from letkf roms is consistently smaller than o roms all along the vertical section however both the systems fail to reproduce the observed strong rise in the 20 c during march 2017 in fig 7 a fig 7 b we have plotted the time evolution of root mean squared differences between the model temperature salinity and observed temperature salinity which were assimilated in the observation space averaged over all the three spatial dimensions of the entire indian ocean domain for both letkf roms blue curve and o roms red curve we see that there is a sharp reduction in global rmsd in temperature salinity by 0 5 c 0 1 psu we have also plotted the vertical profile of rmsd between the model temperature salinity and observed temperature salinity in the observation space averaged over all the spatial dimensions in the horizontal direction and time in fig 8 a fig 8 b for both letkf roms blue curve and o roms red curve we observe that there is a similar reduction of 0 5 c 0 08 psu in the thermocline region beyond the thermocline the difference in both temperature and salinity gradually decreases with depth but does not disappear altogether because of the absence of vertical localization i e even the deeper layers are affected by assimilation in the absence of vertical localization sea level anomaly sla estimates from o roms and letkf roms are validated using archiving validation and interpretation of satellite oceanographic aviso quarterly gridded daily product aviso 2009 no observations of sla were assimilated correlation of sla from letkf roms and o roms against aviso are shown in fig 9 a and b respectively the differences in the correlation pattern is highlighted in fig 9 c assimilation has improved the correlation significantly over western bob and parts of south indian ocean however there are multiple patches in the southern extreme of the domain that suffer from negative correlation fig 9 a b the southern indian ocean is connected to the southern ocean through the southern boundary of our domain at 30 s barotropic dynamics is prevalent in the southern ocean fu 2003 rohith et al 2019 weijer 2010 since these dynamics are fast an exchange of information through the southern boundary every 5 days does not suffice to simulate the southern part of our domain assimilation has reduced the error in o roms over western bob western equatorial indian ocean and south indian ocean by 2 4 cm fig 9 d and e to highlight the improvements degradations in rmsd a difference of the two rmsds is plotted in fig 9 f it is seen that neither letkf roms nor o roms is particularly skilled in simulating the sla amplitudes this is not a surprise because these models are not known to accurately simulate sla over finer length scales which would require accurate initial conditions and accurate representation of processes in the model dynamics a minor shift in eddy location can potentially give rise to several centimeters of rmsd in sla however a reasonably good model should be able to simulate the variability in sla in fig 9 g i the standard deviation of sla is plotted from letkf roms o roms and aviso respectively it is seen that letkf roms is more adept in simulating the standard deviation in sla compared to o roms it however fails to simulate the variability in the southern section of the domain possibly because of the inadequate low frequency boundary exchanges discussed earlier hovmoller diagram of sla along 8 s from aviso fig 10 a letkf roms fig 10 b and o roms fig 10 c show that rossby waves are better captured in letkf roms than o roms o roms tends to simulate a more intense rossby wave resulting in an anomalous deepening of the thermocline fig 10 d e and f corresponds to rama observation letkf roms and o roms respectively and consequently the sst anomaly is larger at the surface assimilation prevents such contaminations to a large degree resulting in an improved estimation of surface and subsurface temperature properties this result is supported by the correlation and rmsd of letkf roms blue curve fig 10 g and h and o roms red curve fig 10 g and h with respect to the observation ocean currents both the systems o roms and letkf roms are able to capture the robust seasonal current systems in the indian ocean such as the eastward flowing equatorial counter current during winter westward flowing north equatorial current during winter the seasonally reversing current systems such as east india coastal current west india coastal current somali current and also the permanent westward flowing south equatorial current the eastward flowing equatorially trapped wyrtki jets that develop during inter monsoon periods october november and may june are well captured in both the systems fig 11 the o roms overestimates jets fig 11 b and shows larger bias than letkf roms when compared to ocean surface current analysis real time oscar that combines geostrophic ekman and stommel shear dynamics using the prescription of bonjean and lagerloef 2002 involving quasi linear approximation and steady state momentum dynamics i e it neglects local acceleration in the formulation it is however generally accepted that the quality of oscar current is debatable in the equatorial belt the inferences from comparisons using oscar currents in the equatorial belt are therefore less trustworthy in general o roms shows strong westward current anomaly during winter season along northern part of the equator fig 11 b in contrast letkf roms shows fewer anomalies during the summer and winter monsoon periods the o roms overestimates the strength of the equatorial currents as compared to the oscar currents whereas the bias is less in the letkf roms fig 11 however oscar currents are known to be underestimated during may and october sikhakolli et al 2013 fig 12 a and b shows the correlation of zonal current from letkf roms and o roms with respect to oscar zonal current respectively fig 12 c highlights the differences in these two correlation patterns there are major improvements in the correlation across most regions in the basin i e letkf roms better simulates the variability in the zonal current assimilation has improved the correlation in the bay of bengal and arabian sea this result is also corroborated by the plot of the standard deviation of zonal current from letkf roms o roms and oscar current in fig 12 g h and i respectively whereas o roms shows a much stronger variability of zonal current in the equatorial belt and the eastern coast of africa than the observed variability assimilation weakens it to a good extent and pushes it towards observed variability in fig 12 d and e we plot the rmsd of zonal current from letkf roms and o roms with respect to oscar zonal current fig 12 f highlights the difference in rmsd of the patterns of fig 12 d and e a negative positive difference indicates improvements degradations due to assimilation there has been significant improvement not only in the equatorial belt but also along both the coast of india and along major fraction of the african coast except the mozambique channel the rmsd is less than 20 cm s in the arabian sea and the south indian ocean in both the systems the assimilation reduces the error considerably in the central bay of bengal the reduction is as large as 15 cm s in the equatorial indian ocean and east coast of africa the rmsd is as large as 45 cm s for o roms and 30 cm s for letkf roms there has been a minimal improvement in rmsd in the southern indian ocean as well the reason is evident if we inspect the observations that have been assimilated during the period of our study fig 12 j the southern indian ocean has the least observation density of in situ t and s profiles and that undermines the steric height correction in sla reflected in the analysis of sla as well in fig 9 and consequently no significant improvement in geostrophic currents which constitutes a major fraction of the current we choose to validate the performance of the currents with an adcp located at equator 80 5 e this adcp is chosen particularly because of uninterrupted data availability during the first year of our study we plot cos θ across depths where θ is the angle made by the current vector obtained from this adcp with the equator fig 13 a values close to 1 indicate that the direction of currents is primarily zonal while values close to zero indicate the direction as mostly meridional it is evident from fig 13 a that the current is mostly zonal during the period of our study across depths hence in the subsequent analysis we focus only on the zonal component of the current unless otherwise mentioned letkf roms manages to reproduce the observed time averaged vertical variability of zonal current from adcp located at equator 80 5 e up to 120 m beyond 120 m it follows the depth profile of zonal current estimated by o roms whereas the error in the estimation of mean zonal current at the surface by o roms is about 18 cm s fig 13 b the rmsd of the zonal current estimation by o roms during the period of our study is large at the surface 37 cm s and monotonically decreases to about 20 cm s at a depth of 140 m letkf roms on the other hand exhibits a lesser rmsd at the surface 28 cm s the rmsd decreases with depth before merging with the o roms profile at about 150 m fig 13 c solid lines the standard deviation of zonal current in letkf roms follows closely the standard deviation of the adcp observation while the standard deviation in o roms is larger till about top 100 m following which it mimics the observation fig 13 c dotted lines the zonal current in o roms exhibits a slightly higher correlation at the surface than letkf roms fig 13 d red curve however the vertical profile of the correlation exhibits a large variability and almost gets decorrelated at deeper depths less than 0 15 in contrast the correlation of zonal current from letkf roms shows a more robust vertical profile even at deeper depths it shows a correlation of about 0 4 fig 13 d blue curve time depth section of zonal current profiles obtained from the same adcp observation letkf roms and o roms shows that letkf roms manages to reproduce the observed spatio temporal variability to a much better degree than o roms fig 14 o roms tend to overestimate the currents the large currents seen during october december wyrtki jet is reflected in both letkf roms and o roms however this intense current persists for a longer time interval in o roms letkf roms manages to better simulate the current reversals both in surface and subsurface brought forth by the changing winds the high frequency hf radar records hourly surface currents every 6 km over a radial distance of 200 km from its stationed location in the present validation we have compared the daily averaged surface currents from letkf roms and o roms with respect to an hf radar observations situated on the east coast of india at kalpakkam 12 5 n 80 2 e and cuddalore 11 7 n 79 8 e the data is available only for the first year of our study the surface current is along the coast throughout the year schott et al 2009 shankar et al 2002 either poleward during summer or equatorward during winter and hence the meridional current dominates on a daily time scale we plot the correlation rmsd of the meridional surface current obtained from hf radar with respect to the meridional surface current from letkf roms in fig 15 a fig 15 d and o roms in fig 15 b fig 15 e while the differences between fig 15 a fig 15 d and fig 15 b fig 15 e in these patterns are plotted in fig 15 c fig 15 f the large positive difference in correlation fig 15 c and large negative difference seen in rmsd pattern fig 15 f indicates that the letkf roms manages to better estimate the amplitude of the surface meridional current in fig 15 g we plot the time evolution of spatially averaged rmsd of the meridional surface current from letkf roms blue curve and o roms red curve there is an appreciable reduction in rmsd particularly during the latter half of the period of our study letkf roms likely improves the coastal circulation along the eastern coast of india this inference however is precariously based on a single observation covering a small fraction of the entire eastern coast of india we have also verified the performance of letkf roms along the west coast of india against an adcp observation 15 2 n 72 7 e situated near goa india we could only manage to analyze the performance for the period aug 2016 to sep 2017 due to non availability of data beyond that period there appears to be a strong off shore sub surface zonal current directed away from the coast in the observation during early october to end of february fig 16 a o roms on the other hand estimates a strong off shore zonal current that is directed towards the coast during mid november to january fig 16 e letkf roms manages to suppress this to a good extent and even estimates a weak off shore subsurface zonal current directed away from the coast during this period fig 16 c however both letkf roms and o roms fail to estimate the strong sub surface zonal current beyond 150 m of depth during the month of december the meridional sub surface current from letkf roms fig 16 d better mimics the observations fig 16 b than o roms fig 16 f letkf roms manages to reproduce the strong poleward sub surface meridional current during september february to a reasonable extent whereas o roms occasionally reverses the direction to equatorward during this period however none of the system managed to reproduce the observed strong equatorward subsurface current beyond 150 m during november a quick look at the statistics confirm that letkf roms perform a much better job in estimating the subsurface currents the subsurface zonal current in o roms is decorrelated or even shows a negative correlation with the adcp observation solid red curve in fig 16 g whereas subsurface zonal current from letkf roms solid blue curve in fig 16 g shows a positive correlation ranging from 0 4 to 0 2 the subsurface meridional current from o roms shows an improved correlation compared to its zonal counterpart that progressively decreases with increasing depth dashed red curve in fig 16 g the performance of letkf roms is even better dashed blue curve in fig 16 g the correlation at 50 m is about 0 8 and it monotonically decreases with increasing depth there appears to be no difference between letkf roms and o roms beyond 200 m of depth the rmsd of the zonal meridional subsurface current fig 16 h from letkf roms blue curves is smaller in the upper layers of the arabian sea compared to o roms red curves it is evident that letkf roms is better in reproducing the amplitude and the variability of the subsurface current off the coast of goa it is important to estimate a realistic phase and amplitude of the currents which are responsible in transporting heat and mass and which is the most important ingredient to leeway models breivik et al 2013 breivik and allen 2007 that helps in saving lives through search and rescue operations 5 surface layer temperature inversions in the bay of bengal surface layer temperature inversion which is a warm layer of water sandwiched between surface and subsurface cold waters is the hidden pocket of heat content in the ocean it can influence sst variability on a range of time scales e g wang et al 2012 show for subseasonal girishkumar et al 2013 and nagura et al 2015 show for seasonal to inter annual time scales it can also potentially influence tropical cyclones neetu et al 2012 bay of bengal hosts thick layers of temperature inversions e g thadathil et al 2016 2007 2002 girishkumar et al 2013 these inversions are concentrated mostly in the northern parts of the bob and are particularly large during winter october march thadathil et al 2016 here we analyze temperature inversions from model and observations in northern parts of the bay of bengal during winter to demonstrate the skill of letkf roms in capturing sub seasonal variations fig 17 displays depth time evolution of temperature during the winter of 2016 17 at two locations viz 15 n 90 e and 18 n 90 e that also host moored buoys the top most panels depict the evolution of observed temperature at these two locations the second row and the third row in fig 17 depict the evolution of temperature at these two locations derived from letkf roms and o roms respectively the bottom panel shows the number of tracer observations that were assimilated within a radius of 200 km around these two locations both the locations experience large temperature inversions during january february helped by thick barrier layer with warm waters of temperature reaching up to 28 c in letkf roms the magnitude of the temperature inversion reaches 2 c in both the systems the magnitude of the temperature inversion and the thickness of the barrier layer which is a difference between mixed layer depth and the isothermal layer depth hosting the inversion layer are comparatively larger at 18 n 90 e than at 15 n 90 e these results are consistent with the earlier studies e g thadathil et al 2016 the barrier layer thickness and magnitude of the temperature inversions are overestimated by o roms letkf roms improves the estimation of barrier layer thickness it can be inferred from fig 17 c and d that letkf roms represents the inversions at reasonable skill indirectly letkf roms manages to simulate the halocline better in northern bay of bengal during boreal winters compared to o roms which falls short of expectations owing to absence of river runoff in the model for instance the thickness of the large temperature inversion during january february at 15 n 90 e and the timing of the events are well simulated in letkf roms the timing of the events and thickness of these inversion layers are not that accurate in o roms the sharp rise in isothermal layer depth ild in fig 17 which is the depth at which the temperature decreases by 1 c from the sst during early october 2016 and late february 2017 at 15 n 90 e a signature of strong mixing is not captured in o roms whereas letkf roms manages to simulate it to a reasonable extent at 18 n 90 e o roms estimates a deeper ild and hence a larger barrier layer thickness inaccurate salinity stratification and subdued mixing compared to the observation and letkf roms at all times the improvement is more pronounced at 15 n 90 e compared to at 18 n 90 e which lies closer to the river mouth of ganges and brahmaputra this may be attributed to the absence of river discharge fluxes in the model and no sparse temperature and salinity observations as presented in fig 17 g and fig 17 h results from letkf roms are likely to improve further with improved observation coverage in the region 6 summary and discussion an ensemble kalman filter based indian ocean data assimilation system is implemented using letkf and 1 12 1 12 roms this system comprises of 80 ensemble members and assimilates in situ temperature and salinity profiles from a plethora of observation networks ranging from moored buoys to argo floats it also assimilates satellite track data of sst coarse grained over a length scale of 25 km in order to downscale the dense data this system is different from many existing data assimilation systems even though a healthy atmospheric ensemble forces the oceanic system letkf roms the spread of the oceanic ensemble often collapses unless appropriate measures are taken in order to maintain a healthy spread the model configuration among the oceanic ensemble members were tweaked one half of the ensemble members follow kpp mixing scheme whereas the other half follow mellor yamada mixing scheme the diffusion coefficient of the tracers and the viscosity coefficients of the momentum variables were perturbed around a mean value in each of the ensemble members this strategy in addition to a covariance inflation of 10 after each analysis step ensures that the spread does not collapse in time this approach is metaphorically identical to perturbations in atmospheric fluxes or lateral boundary conditions or river fluxes to contain filter divergence lima et al 2019 this assimilation system exhibits an improved ocean state evolution when compared to the non assimilated system the sst analysis is much better correlated to the avhrr sst and significantly decreases the rmsd across the entire domain in comparison to the assimilation free model unlike the non assimilated system letkf roms facilitates a more robust vertical estimation of temperature profile in terms of correlation standard deviation and rmsd when compared to the rama moorings letkf roms simulates sst anomaly to a better degree of accuracy in comparison to that of o roms in the time scales less than 200 days we have shown that the likely reason lies in the over estimation of rossby wave amplitude in the o roms leading to deepening of the 20 c isotherm and consequently a degraded sst anomaly this will likely ensure an improved simulation of biogeochemistry if inducted into the model there is no localization in the vertical direction the satellite swath data is assimilated to the top layer of the model temperature and the information is subsequently passed to the vertical layers below the weightage at each layer depends on the model error covariance the effect of assimilating satellite sst swath data therefore does not only affect the surface temperature of the model but runs deep into the subsurface layers in order to understand the effect of sst assimilation on subsurface layers we have performed a controlled experiment wherein only sst observations were assimilated in fig 18 we show vertical profile of temperature from the analysis posterior red curve and background prior blue curve state of the controlled experiment at an arbitrary location of 46 75 e and 11 75 n on nov 18 2016 the first day when sst observations were available and assimilated the analysis expectedly moves towards the observed sst value depicted as a star in fig 18 it is also apparent that the innovation is relayed immediately to deeper layers the innovation decreases with increasing depth the mixed layer depth is altered immediately deepened by about 1 5 m leading to a change in the instantaneous subduction rate which depends on the rate of change of mixed layer depth williams et al 1995 this facilitates a volume flux into out of the thermocline from the mixed layer if the subduction rate is positive negative thereby modulating the thermocline as well the most profound improvements were seen in currents whose observations were not assimilated o roms often overestimates the currents particularly during strong events in the indian ocean like the wyrtki jet propagations in may june and october november assimilation of temperature and salinity in letkf roms restores the estimation of currents closer to reality both in surface and subsurface regions and both in the open ocean and in the coastal region the improvements are however nominal in the southern indian ocean we have shown that the cause can be attributed to sparse observations of temperature and salinity profiles in the southern indian ocean compared to other parts of the basin the non availability of temperature and salinity observations likely does not improve the steric effects in sea surface height which is also not assimilated resulting in incorrect estimation of geostrophic currents in the southern indian ocean an improved estimation of currents in the ocean is vital to represent the transport of mass and tracers also it should result in a better evaluation of probable search zones of lost persons objects predicted by models during search and rescue operations breivik and allen 2007 the estimation of currents can be further improved if sla is also assimilated particularly we believe that the surface currents in the southern indian ocean can potentially improve significantly if satellite track data of sla is assimilated inclusion of sla assimilation into this system is reserved for future work another major improvement is seen in the temperature and salinity estimates in northern bay of bengal a region known for its propensity to give birth to temperature inversions particularly during boreal winters even though the ocean model is not injected with river water discharges assimilation of in situ temperature and salinity profiles and satellite sst takes care of the absence of fresh water discharges to a reasonable extent and realistically simulates the barrier layer thickness and isothermal layer depths however the improvements are less profound close to the river mouths because of sparse data coverage one major caveat of the present study is the limited period of study which amounts to about 778 days we had to settle for such a limited period because of the non availability of ensemble of atmospheric fluxes the initial condition of the ensembles in letkf roms is generated through perturbation of a stable initial condition obtained from o roms it is expected that the ensemble members in letkf roms will generate transients which is likely to subside with the evolution of time while the barotropic transients shall subside in a few weeks owing to its faster propagation the baroclinic transients near the surface is likely to take a few years to dissipate these transients shall contaminate the ocean state during the initial period of our study and hence limit the scope of improvement letkf roms on the other hand shows significant improvements over o roms even during this transient period and it is therefore expected that letkf roms will further improve once the transients die down completely a larger forecast length compared to the data assimilation cycle provides increased confidence on the system in the present scenario the length of the data assimilation cycle and the forecast length is identical i e 5 days however our system has been developed for short term forecasts typically amounting to 3 5 days to be used for operational use in this context the performance of letkf roms has been convincingly shown to be better than o roms inclusion of two mixing schemes in letkf roms as a measure to arrest filter divergence has its own disadvantages the controlled experiments show that the use of two mixing schemes across the ensembles increases the spread in sst and turns the initial gaussian distribution in sst into a bimodal distribution however assimilation subdues the bimodality of the distribution dashed black curve in fig 2 thereby restoring it close but not entirely to a gaussian distribution as long as the departure from gaussian distribution is not very strong this strategy is supposed to work without any ado this system therefore demands periodic inspection of the distribution of spread however a long integration is needed to look at the stability of the system that periodically deviates from gaussian distribution a more systematic effort is needed to convincingly establish the merit of using multiple mixing scheme a more refined way to include multiple mixing schemes in a data assimilation framework shall be to explore ways to reduce the severity of the differences in the mixing schemes our approach though crude in nature is probably the first approach in this direction and would likely stimulate further research in this direction the assimilation of observations 104 using this technique in a 80 member setup takes only about a couple of minutes using 256 processors xeon e5 2670 8c 2 6 ghz with linpack performance rmax 719 22 tflop s most of the time is consumed in evolving the model state in time even then one assimilation cycle takes about 60 min using 2000 processors for model evolution and 256 processors for assimilation this is operationally not very extensive declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments funded by ministry of earth sciences moes ssr and ap acknowledge the training on letkf mom by prof eugenia kalnay and her team dr travis sluka and dr steve penny at the university of maryland under the monsoon mission i project funded by ministry of earth sciences moes govt of india the data assimilation system is configured as a part of the ocean modeling data assimilation and process specific observations o mascot programme coordinated by the indian national centre for ocean information services incois and funded by the ministry of earth sciences moes govt of india ref moes 36 oois o mascot 2017 the authors wish to thank dr kunal chakraborty for providing the boundary condition for roms and dr francis pavanathara and dr abhisek chatterjee for setting up the operational model o roms authors also thank the osf team incois for providing the ocean state forecasts from the present operational roms against which letkf roms is compared with the authors would also like to thank dr munmun das gupta for providing the ocean observations from ncmrwf and the data management team from incois for providing the data all the experiments were conducted on the high performance computer aditya iitm pune india the support from aditya hpc team is highly appreciated matlab was used for generating the figures this is incois contribution number 355 
24014,this paper summarizes the findings of a workshop convened in the united states in 2018 to discuss methods in coastal and estuarine modeling and to propose key areas of research and development needed to improve their accuracy and reliability the focus of this paper is on physical processes and we provide an overview of the current state of the art based on presentations and discussions at the meeting which revolved around the four primary themes of parameterizations numerical methods in situ and remote sensing measurements and high performance computing a primary outcome of the workshop was agreement on the need to reduce subjectivity and improve reproducibility in modeling of physical processes in the coastal ocean reduction of subjectivity can be accomplished through development of standards for benchmarks grid generation and validation and reproducibility can be improved through development of standards for input output coupling and model nesting and reporting subjectivity can also be reduced through more engagement with the applied mathematics and computer science communities to develop methods for robust parameter estimation and uncertainty quantification such engagement could be encouraged through more collaboration between the forward and inverse modeling communities and integration of more applied math and computer science into oceanography curricula another outcome of the workshop was agreement on the need to develop high resolution models that scale on advanced hpc systems to resolve rather than parameterize processes with horizontal scales that range between the depth and the internal rossby deformation scale unsurprisingly more research is needed on parameterizations of processes at scales smaller than the depth including parameterizations for drag including bottom roughness bedforms vegetation and corals wave breaking and air sea interactions under strong wind conditions other topics that require significantly more work to better parameterize include nearshore wave modeling sediment transport modeling and morphodynamics finally it was agreed that coastal models should be considered as key infrastructure needed to support research just like laboratory facilities field instrumentation and research vessels this will require a shift in the way proposals related to coastal ocean modeling are reviewed and funded keywords coastal ocean modeling physical processes model subjectivity development of standards high resolution modeling parameter estimation 1 introduction coastal and estuarine modeling is concerned with understanding and predicting marine processes in coastal oceans and estuaries although this includes physical and biogeochemical processes the focus of this paper is on the physical processes impacted by tides winds surface waves and hydrological processes including fresh water and sediment laden flows one component of coastal and estuarine modeling is the prediction of sediment transport including both fine sediments in shallow estuaries and coarser sediments in nearshore wave driven environments over long time scales sediment transport governs morphodynamics which strongly impacts coastal and estuarine flows unique to coastal and estuarine modeling is the connection to human influences particularly in densely populated coastal regions where flows can be altered by coastal structures dredging and sand nourishment operations and anthropogenic sources of contaminants and nutrients significantly impact coastal biogeochemistry given that roughly 60 the world s population lives within 60 km of the coast and this is expected to rise to 75 within a few decades rao et al 2008 accurate coastal and estuarine modeling is an essential component of efficient management for the sustainability of natural coastal systems and the development and improvement of sustainable urban infrastructure particularly in the face of rapid urbanization of coastal cities and changing climate including sea level rise accurate coastal and estuarine modeling is also a critical component of climate modeling because coastal shelves contain roughly the same amount of primary productivity and biomass as the open ocean whittle 1997 sharp 1988 yool and fasham 2001 the focus on physical processes in this paper rests on the assumption that they are fundamental to modeling nearly all other processes in the coastal ocean including pollution transport water quality biogeochemistry and coastal ecology since modeling each of these requires coupling to a circulation model that computes the transport and mixing in the context of modeling of physical processes in this paper we will distinguish between two distinct types of ocean modeling at larger or regional scales regional ocean models typically assume a geostrophic balance to leading order i e rotation in balance with pressure gradients and are weakly dissipative at smaller coastal and estuarine scales coastal models are fundamentally ageostrophic three dimensional and driven by boundary layer processes in the shallowest regions of the coastal environment coastal models must accurately capture frictional balances such as between the barotropic or baroclinic pressure gradients and bottom friction when simulating strong tides or storm surges coastal models must also account for wetting and drying and hydrological forcing to account for the effects of runoff from precipitation coastal models meant to capture the transitional nature between shallow environments and regional scales must be able to simulate both highly frictional ageostrophic motions and balanced flows models of physical processes in coastal environments have seen significant advances in the past two decades owing to increases in computational power and improved numerical methods including unstructured grids model nesting data assimilation and model coupling furthermore advances in remote sensing and in situ observational technologies have led to substantially larger and more accurate datasets which have significantly improved the ability to assess model performance multiple coastal models have been developed in the past two decades although there is no dominant model in sharp contrast to regional modeling for which the roms model shchepetkin and mcwilliams 2005 is the most common in the united states or wave modeling for which the swan model booij et al 1999 is the most common for coastal wave problems in the united states as an example while few if any models other than roms have been used to simulate regional circulation on the u s west coast e g neveu et al 2016 chao et al 2009 2018 circulation in san francisco bay has been modeled with at least six different models in the past ten years 1 suntans fringer et al 2006 was applied by chua and fringer 2011 and holleman et al 2013 2 untrim casulli and zanolli 2002 2005 was applied by macwilliams et al 2015 2016 3 trim3d casulli and cattani 1994 was applied by gross et al 2009 4 schism zhang et al 2016 was applied by chao et al 2017b and 5 delft3d flow and 6 delft3d fm oss deltares nl web deflt3d home were applied by erikson et al 2013 and martyr koller et al 2017 respectively although there have been no systematic comparisons of these models the most detailed calibration and best performing model appears to be the untrim implementation by macwilliams et al 2015 2016 although it is difficult to argue that the performance can be attributed to the model itself as opposed to superior grids bathymetry and forcing i e winds tides river inflows etc the fact that there is no dominant coastal ocean modeling strategy like that seen in regional modeling presents an opportunity to determine whether there is a need for a unified approach in coastal ocean modeling obviously there is no need for such an approach if existing strategies are efficient and accurate enough to answer pressing questions related to coastal processes to this end a workshop was held to determine the current state of the art in coastal ocean modeling and to form a consensus on key areas of research and development needed to improve the accuracy and reliability of such models key questions posed to workshop participants and that will be addressed in this paper are 1 where should we focus our efforts related to improved parameterizations in coastal modeling 2 what aspects of numerical methods related to coastal modeling can be improved 3 how can in situ and remote sensing measurements be used and improved to benefit coastal modeling 4 how can coastal modeling better leverage hpc resources there have been other recent workshops with similar objectives the paper by wilkin et al 2017 summarizes the outcomes of an ioos sponsored workshop that was held to advance coastal ocean modelling analysis and prediction as a complement to the observing and data management activities of the coastal components of the u s integrated ocean observing system ioos and the u s global ocean observing system goos the findings of that workshop concluded that the community should focus on the following seven topical areas 1 model coupling 2 data assimilation 3 nearshore processes 4 cyberinfrastructure and model skill assessment 5 modeling for observing system design and operation 6 probabilistic prediction methods 7 fast predictors as will be discussed below the findings of the workshop discussed in this paper are similar although the recommendations focus more on process and forward modeling rather than predictive and data assimilative modeling for observing systems like ioos and goos there are also coastal modeling initiatives in europe with similar objectives such as the german coastal modeling working group 1 1 http www deutsche meeresforschung de en coastalmodelling that is charged with defining challenges for coastal modeling encouraging cooperation between developers and users developing a national forum for coastal ocean modelling and developing common infrastructure as part of that working group a workshop was held in germany 2 2 https www io warnemuende de comod2018 html in february 2018 with the goal of increasing the communication between coastal ocean modellers in german marine research institutions and focused on answering the following questions 1 what are the future challenges in coastal ocean modeling 2 do we need better coordination between model developers and model applicants on the national level 3 could we profit from a common repository of reference model results 4 would we profit from a coastal ocean model intercomparison study coastalmip 5 do we need to develop new models or are we happy with what we have 6 do we need common interfaces for model and module coupling 7 are the national super computing resources sufficient rather than providing a comprehensive review of the state of the art in modeling of physical processes in the coastal ocean this paper summarizes key issues as presented and discussed by workshop attendees these issues range from numerical methods to parameterizations to observational technologies for a comprehensive overview of numerical methods for coastal models the reader should consult the review article by klingbeil et al 2018 klingbeil presents details of time discretization and wetting and drying schemes topics not mentioned in this report but that are crucial to coastal ocean modeling the reader should refer to the review article by medeiros and hagen 2013 or the paper by candy 2017 for a detailed discussion of wetting and drying algorithms in addition to a discussion of wetting and drying no overview of ocean modeling would be complete without a discussion of the state of the art in lagrangian particle tracking a review of which is given by van sebille et al 2018 although nearshore wave and sediment transport modeling were discussed at the workshop and in this report a more detailed review can be found in kirby 2017 2 workshop organization and attendees a four day workshop funded by the u s national science foundation nsf physical oceanography program was held during june 18 21 2018 at the stateview hotel and conference center in raleigh nc u s a on the campus of north carolina state university a total of 40 participants attended the workshop 29 of whom were more senior and gave 15 min presentations see appendix the senior researchers nominated 11 junior scientists who were allotted 30 min for their presentations research interests among the participants reflected a balance between model developers and users among the developers and users interests were equally divided between those with a stronger coastal focus and those with a stronger estuarine focus roughly half of those focusing on coastal processes had interests in storm surge modeling while six researchers had specific interests in coastal engineering and or nearshore processes finally there were three biogeochemists and two researchers focusing on wetlands most of the attendees were forward or process modelers and hence the outcomes focused less on data assimilation techniques more commonly employed in operational modeling indeed an important outcome of the workshop is the need for greater collaboration between forward and operational modelers and for forward modelers to adopt more techniques commonly employed in the operational and predictive modeling communities many of the original developers of most of the popular coastal models used in the united states were present at the meeting see table 1 including the finite element adcirc model luettich et al 1992 westerink et al 1994 the roms based coupled ocean atmosphere wave sediment transport modeling system coawst warner et al 2008 2010 the finite volume unstructured grid model fvcom chen et al 2003 the curvilinear coordinate finite volume model getm burchard and bolding 2002 the general ocean turbulence modeling framework gotm burchard et al 1999 the mixed finite element finite volume models selfe zhang and baptista 2008 and schism zhang et al 2016 and the finite volume unstructured grid and nonhydrostatic suntans model fringer et al 2006 also present was the developer of the biogeochemical model cosine chai et al 2002 2003 2007 other popular models are also discussed in this paper as listed in table 1 3 state of the art 3 1 parameterizations 3 1 1 the coastal submesoscale and turbulence modeling unresolved processes that must be parameterized in coastal modeling can be regarded as those that are smaller than the estuarine submesoscale or coastal submesoscale geyer this workshop in analogy to ocean submesoscale processes in regional or global ocean modeling like the ocean submesoscale coastal submesoscale processes can be thought of as those with horizontal scales that are smaller than the internal rossby deformation scale however unlike ocean submesoscale processes submesoscale coastal processes are constrained by bathymetric and coastline scales which are typically smaller than the rossby deformation scale therefore coastal submesoscale processes possess horizontal scales that are larger than the depth but smaller than the relevant horizontal bathymetric scale as a result they depend heavily on coastline geometry and might include processes like lateral vertical flow separation headland eddies secondary flows and fronts for an example of the importance of bathymetry in coastal and estuarine processes see ye et al 2018 unlike regional scales for which there is active research on parameterization of submesoscale processes e g pearson et al 2017 mcwilliams 2016 thomas et al 2013 there has been little work on parameterizing such processes in coastal models because they are so site specific instead efforts have focused on resolving these processes with high resolution for example giddings et al 2012 showed that the suntans model could resolve using o 1 m horizontal resolution a front at a convergence zone between two tidal channels in the snohomish river estuary that was measured in situ and with remote sensing giddings this workshop showed that coupled roms swan model results accurately capture frontal behavior of a small river plume front interacting with the surf zone at the mouth of the tijuana river estuary this model employed five nested roms model grids ranging from the regional scale down to the surf zone with a resolution of o 10 m on the finest grid these examples demonstrate the need for extremely high resolution to resolve rather than parameterize so called coastal submesoscale processes while parameterization of coastal submesoscale processes is difficult if not impossible there are many parameterizations of small scale processes in coastal models with scales that are on the order of the depth i e either the bottom or the mixed layer depth or smaller including turbulence coastal models compute the low frequency large scale motions dictated by the reynolds averaged navier stokes rans equations along with the hydrostatic approximation see section 3 2 9 typically turbulence models focus on the vertical turbulent reynolds stress arising from the averaging horizontal reynolds stresses are typically ignored in coastal models given the dominance of horizontal transport compared to horizontal turbulent mixing in most problems of interest e g blumberg and mellor 1987 the remaining vertical turbulent reynolds stress is modeled with a turbulent viscosity hypothesis which assumes the reynolds stress is a product of a turbulent eddy viscosity and the mean vertical shear pope 2000 most parameterizations of the turbulent eddy viscosity in coastal modeling assume that it is a product of turbulent length and velocity scales that are inferred from two equation turbulence closure schemes for a review see umlauf and burchard 2005 the first of these equations is an evolution equation for the turbulent kinetic energy tke from which the turbulent velocity scale can be extracted and the second equation is needed to compute the turbulent length scale examples include the mellor yamada level 2 5 scheme mellor and yamada 1982 the k epsilon model jones and launder 1972 launder and sharma 1974 rodi 1984 and the k omega model originally proposed by saffman 1970 and extended to oceanic applications by umlauf et al 2003 although these models have similar tke equations they differ in the implementation of the length scale related equation umlauf and burchard 2003 show that this second equation can be generalized as a generic length scale gls model that exhibits more flexibility than the traditional models in that it performs well when applied to a much broader variety of problems the gls model is written in a form that recovers the traditional models through alteration of the parameters in the governing equation for the generic length scale making it straightforward to compare all commonly used two equation models the gls approach was incorporated into the roms model warner et al 2005 and in the general ocean turbulence model gotm burchard et al 1999 gotm net the standard platform for turbulence parameterizations in coastal modeling a fundamental difficulty related to turbulence modeling for coastal problems concerns the relationship between stratification and turbulence see the review by umlauf and burchard 2005 although parameterization of stratified turbulent mixing remains an active area of research e g gregg et al 2018 monismith et al 2018 most coastal models produce reasonable results with stability functions that damp the turbulence due to stratification in this approach a critical or steady state richardson number is specified below which turbulence grows exponentially and above which turbulence decays exponentially burchard and baumert 1995 theory miles 1961 howard 1961 and experiments rohr et al 1988 have found the steady state richardson number to be around 0 25 a lower steady state richardson number requires stronger shear to incur vertical mixing and hence will produce a more strongly stratified environment although the steady state richardson number is predicted from theory it can be tuned to account for modeling errors like numerical mixing as discussed in section 3 2 5 3 1 2 bottom drag other than the turbulence model the most common parameterization of small scale processes in coastal models is that related to bottom drag most models compute a bottom stress that is dictated by a prescribed bottom roughness and the assumption that the horizontal velocity in the first grid cell above the bed satisfies a logarithmic velocity profile although this assumes that the bottom most cell is within the log law region in practice it is often relaxed most notably when calculating the drag coefficient for two dimensional depth averaged models which assume a log law throughout the water column as an example the drag coefficient for the external or barotropic mode in getm assumes a log law velocity profile at mid depth burchard and bolding 2002 the bottom roughness can be parameterized as a function of the grain size distribution and less commonly the presence of bedforms while bottom drag in steady flat rough boundary layers can be accurately parameterized if the median grain size is known there are few parameterizations for the bottom roughness in the presence of bedforms the most common being the wave dominated parameterization of wiberg and harris 1994 that is implemented in the wave current and sediment transport component described in warner et al 2008 of the coawst model there are few if any coastal models that employ parameterizations for bedforms in steady flows although there is evidence that the bottom drag coefficient depends on the tidal phase owing to bedform asymmetry fong et al 2009 bottom roughness parameterizations in wave models are similar to those in circulation models in that the wave friction factor is a function of the properties of the bed however wave models include dissipation by wave breaking and bottom dissipation due to viscous damping in mud which absorbs wave energy e g komen et al 1994 such models are highly uncertain given the difficulty in predicting the behavior of bottom mud layers in coastal regions models for the bottom drag that include the combined effects of currents and surface waves employ more complicated parameterizations like the theory of grant and madsen 1979 which parameterizes wave effects with an augmented roughness or mellor 2002 in which the waves are accounted for with an augmented shear production the augmented roughness of grant and madsen 1979 is typically further modified based on the effects of sediment induced stratification which acts to reduce near bed turbulence and the effective bottom drag e g glenn and grant 1987 styles and glenn 2000 much work on bottom drag has been done with large eddy simulation les and direct numerical simulation dns with a focus on understanding sediment transport which is highly sensitive to the bottom drag parameterization examples include steady current simulations cantero et al 2009a b and purely wave driven simulations ozdemir et al 2010 yu et al 2013 cheng et al 2015 although there is little les or dns work on wave current flows parameterizations accounting for waves typically augment the mean bottom stress or roughness under the assumption of turbulent wave boundary layers over rough beds while this is common in coastal nearshore environments in estuaries where waves are generally weaker laminar wave boundary layers are possible and can reduce the mean or effective roughness nelson and fringer 2018 3 1 3 vegetation kelp and coral drag bottom drag parameterizations for coastal modeling are often of second order importance when compared to the need for accurate boundary conditions and forcing see section 3 2 3 and in many cases the bottom drag is heavily tuned see section 3 2 5 drag parameterizations can be more important where the impact of larger scale roughness features on flow and waves is significant such as vegetation kelp or corals a recent example of the state of the art in parameterizing vegetation drag is the coupled roms swan flow wave vegetation model of beudin et al 2017 implemented in the coawst model the vegetation model includes three dimensional vegetation drag that extracts momentum from the flow in the region of the water column that is influenced by vegetation through a quadratic drag law nepf 2012 this drag decelerates the flow that is blocked by vegetation while accelerating it above submerged vegetation which in turn acts to locally decrease the effective water column depth the shear layer that develops at the interface between the submerged vegetation and the flow contributes to turbulence and is added as a production term to the tke equation in the gotm model following the approach of uittenbogaard 2003 at the same time fine scale eddies generated by separated flow around vegetation stems extract kinetic energy from the turbulence a process that is modeled with a dissipation term in the tke equation in a similar vein vegetation acts to damp waves with an energy dissipation term in the wave action equation in the swan model following the method described by mendez and losada 2004 the model of beudin et al 2017 also includes the effect of wave streaming observed by luhar et al 2010 and luhar and nepf 2013 in which a force is added to the horizontal momentum equations in the roms model to account for the contribution of wave induced mean flow within the vegetation like streaming in the wave boundary layer without vegetation further complicating the dynamics is the bending of submerged vegetation such as seagrass which leads to a reduction in the drag coefficient with increased flow strength an effect that is described by luhar and nepf 2011 such complexities are accentuated when parameterizing drag coefficients for flow through kelp for which the drag coefficient varies with the tidal cycle and seasonal changes in kelp density rosman et al 2010 wang et al 2018 coral reefs represent an added modeling challenge given the dominance of waves and wave breaking in those environments monismith 2007 models of circulation in coral reef environments can incorporate detailed spatial variability of the effective roughness derived from remote sensing and in situ measurements such as the coawst model of the palmyra atoll by rogers et al 2017 ultimately although there are numerous parameterizations for vegetation kelp and coral reef effects most parameterizations are based on idealized laboratory experiments with the drag elements represented by simplified arrays of rigid columns e g lowe et al 2005a b model seagrass blades e g zeller et al 2014 or model kelp rosman et al 2013 some studies focus on flow around the skeletal structure of real coral experimentally e g reidenbach et al 2006 or numerically e g chang et al 2014 nevertheless there are no parameterizations that account for the spatially heterogeneous nature of real vegetation kelp or corals such as cross sectional geometry drag coefficient density height area density young s modulus etc although such parameterizations are badly needed the primary difficulty of implementing them in coastal models is related to accurately measuring the distribution of such properties in the field 3 1 4 sediment transport modeling like modeling vegetation induced impacts sediment transport modeling is limited in large part by a lack of knowledge of the spatio temporal distribution of sediment properties in coastal environments examples of the current state of the art in sediment transport modeling for coastal problems can be found in the delft3d flow delft3d fm oss deltares nl web deflt3d home and coawst warner et al 2008 models in these models it is assumed that the suspended sediment can be treated as an eulerian concentration field because the grain sizes and flow regimes ensure that the sediment grains effectively follow the flow i e they possess a small stokes number which is a ratio of the particle relaxation time scale to the fine scale turbulent shearing time scale and the concentration is small enough less than roughly 1 g l 1 to ignore interactions between sediment grains balachandar and eaton 2010 this allows coastal models to use existing momentum and or scalar transport schemes to transport sediment with the addition of a term to account for gravitational settling to represent transport of a particle size distribution psd most models transport three or more size classes each with the theoretical settling velocity for that grain size examples of models with multiple size class distributions are the mekong river two size class sediment transport study of xue et al 2012 using coawst the skagit river tidal flats three dimensional model of ralston et al 2013 which employed three size classes fine sand silt and fine silt using fvcom the san francisco bay sediment transport model of bever and macwilliams 2013 using untrim which accounted for four size classes silt flocculated clay and silt sand and gravel and the seine estuary sediment transport model of grasso et al 2018 using mars3d lazure and dumas 2008 which accounted for five size classes gravel 3 sand sizes and one mud size class the choice of a limited number of size classes implies a coarse representation of the actual psds therefore the settling velocities are based on representative grain sizes and are largely tunable the settling velocity is particularly important in estuarine environments which possess fine grained sediments silts and clays with the propensity to flocculate or aggregate due to cohesive forces arising from salinity or biological effects flocculation is in turn countered by breakup in the presence of turbulent shear implying that the psd cannot be specified a priori because it evolves in time the most common approach to account for these effects is to parameterize the average settling velocity as a function of the flow sediment and turbulence properties as reviewed by soulsby et al 2013 as examples mengual et al 2017 and grasso et al 2018 used the mars3d hydrodynamic model lazure and dumas 2008 and parameterized the settling velocity as a function of the suspended sediment concentration ssc and turbulent shear rate using the formula of van leussen 1994 the next level of complexity is to explicitly simulate the evolution of the average floc diameter through parameterizations that account for the effects of concentration and turbulent shear on the flocculation and breakup processes e g winterwerp et al 2006 the average settling velocity is then computed with knowledge of the average floc diameter and assumptions about the floc density using fractal theory the disadvantage of this approach is that it does not account for the existence of a psd and its variation in time due to flocculation and breakup this can be accounted for by exchanging mass between different size classes or flocs with the population balance approach lick et al 1992 sterling et al 2005 wherein smaller size classes can interact flocculate and lose mass to larger size classes and larger size classes can lose mass to smaller size classes through turbulent breakup the primary advantage of the population balance approach is that it is based on first principles although it requires numerous parameterizations with many coefficients to model the aggregation and breakup interaction dynamics between the different size classes a good example is the flocmod model of verney et al 2011 that was incorporated into coawst by sherwood et al 2018 although this model shows great promise population balance models remain in their infancy owing to the need for extensive calibration of the many unknown parameters in addition to the difficulty of modeling the physical processes flocculation also depends critically on biological material in the water column which can promote aggregation kranck and milligan 1980 mietta et al 2009 however no coastal models explicitly couple biological models to sediment transport models to account for this interestingly such coupling is inherently two way given that the biology is modified by light availability which is a strong function of the ssc cloern 1987 because of the difficulty in parameterizing flocculation some coastal models ignore flocculation parameterizations and assume static floc sizes with behavior that is essentially tuned to match observations for example chou et al 2018 showed that the ssc in south san francisco bay could be reproduced reasonably well with a sediment transport model in suntans after tuning the relative erosion rates of two size classes which were referred to as microflocs and macroflocs an additional complication of sediment transport modeling is the erosion of sediment from the bed which is typically parameterized empirically with a power law as a function of the ratio of the bottom stress τ b to the critical bottom stress below which no erosion is expected to occur τ c sanford and maa 2001 pointed out that there are many variants of these empirical expressions and all appear to behave similarly in practice the most common form is e m τ b τ c 1 n winterwerp and van kesteren 2004 where models focusing on cohesive sediments set n 1 e g warner et al 2008 bever and macwilliams 2013 chou et al 2018 and models incorporating both cohesive and non cohesive sediments use n 1 for the cohesive sediments and n 1 5 for the non cohesive sediments or sands e g mengual et al 2017 van kessel et al 2011 delft3d flow 2019 uses the erosion formula with n 1 for cohesive sediments and a reference concentration approach van rijn 1993 for erosion of non cohesives the bottom stress τ b is typically obtained with parameterizations based on the bottom roughness and wave properties with the additional complication that the near bed sediment induced stratification can reduce the bottom stress see section 3 1 2 the variability with depth in the bed is accounted for by incorporating multiple sediment layers with varying critical stresses τ c erosion rates m and other sediment properties such as mud and sand mud mixtures e g warner et al 2008 sherwood et al 2018 delft3d flow 2019 it is often the case that just two layers are sufficient to account for the existence of an easily erodible top fluff layer lick 2009 composed of fine muddy sediments and a more consolidated sandy lower layer that is less erodible e g van kessel et al 2011 delft3d flow 2019 has a separate mud module that computes the momentum conservation equations in the mud layer and mass conservation is governed by the horizontal transport of mud consolidation and entrainment and deposition to from the flow the critical stresses for erosion τ c and the erosion rates m are often obtained from core samples in laboratory settings such as sedflume mcneil et al 1996 erosion rates and critical shear stresses respectively decrease and increase with time owing to consolidation of the bed in fine grained muddy environments these effects that can be accounted for with empirical approaches such as the model of sanford 2008 that was incorporated into coawst sherwood et al 2018 while suspended sediment transport modeling has its limitations accurate bed load transport modeling is even more limited by inaccurate parameterizations and a lack of knowledge of bed properties particularly in fine grained or muddy estuarine environments bed load is better defined in sandy environments because movement of sand grains under steady flow can be parameterized with models like the meyer peter and müeller formula 1948 wherein the bed load transport rate is given by a power law as a function of τ b τ c much like the parameterization for erosion the formula by soulsby and damgaard 2005 computes the time averaged bed load transport in sandy beds due to wave current flows and accounts for misalignment between waves and currents these bed load transport formulas require calculation of bottom stresses due to wave current flows as described in section 3 1 2 several parameterizations account for modifications in bed load transport due to bed slope the critical stress for erosion can be increased with increasing slope to effectively decrease the bed load transport in the upslope direction whitehouse and hardisty 1988 alternatively the bed load transport can be directly modified as a function of the bed slope to yield similar behavior lesser et al 2004 morphodynamic evolution of the bed is dictated by both suspended and bed load transport through the exner sediment mass balance equation in which the bed height evolves due to deposition and erosion of suspended load transport and divergence of the bed load transport implementations of the exner equation require a smoothing or diffusion term which is typically derived with the avalanching approach for which a bed load flux causes a decrease in bed slope if it exceeds the local angle of repose e g chou and fringer 2010 guerin et al 2016 without this term grid scale oscillations appear in the bed height given that the exner equation otherwise has no mechanism to smooth out such oscillations the most difficult aspect of morphodynamics modeling is that the bed evolves over time scales that are much longer than typical time scales in coastal models therefore to reduce the computational cost associated with the hydrodynamics most morphodynamics studies are run in two dimensions with depth averaged models e g van der wegen and roelvink 2008 while computationally less expensive two dimensional models do not capture subtidal estuarine dynamics which are largely baroclinically driven only recently have three dimensional morphodynamics studies been implemented to assess the role of density driven currents olabarrieta et al 2018 to study long term morphodynamics over decades or even centuries it is common to employ a morphological scale factor roelvink 2006 in which the bed evolution is multiplied by a factor during each time step to accelerate its motion relative to that of the flow olabarrieta et al used a factor of 50 van der wegen and roelvink used 400 owing to the potential for extensive erosion over long morphological time scales an added difficulty of long term morphological modeling is its dependence on sediment properties deep within the bed while these can be measured with core samples core sampling can be extremely expensive and may not provide adequate horizontal spatial resolution a more extensive discussion of different bed load transport and morphodynamics models can be found in the user manual for the delft3d family of models delft3d flow 2019 3 1 5 wave modeling a full description of the state of the art and recommendations for future research in wave and nearshore modeling would warrant a workshop in and of itself therefore here we discuss features of wave modeling that are most relevant for larger scale i e larger than nearshore scales coastal circulation modeling in the context of coupling of wave models to three dimensional circulation models a review of nearshore wave modeling is provided by kirby 2017 surface gravity wave time and length scales are too small to resolve in coastal models instead the waves are modeled with the conservation of wave action equation which governs the evolution of the wave energy spectrum due to wind input wave wave interactions and breaking these models can accurately capture refraction by bathymetry and currents although diffraction is extremely difficult to capture and hence associated parameterizations are not very reliable most coastal models include the effects of waves using the wave action approach the most popular being wavewatch iii tolman 2009 and swan booij et al 1999 which are coupled to currents in the coawst and delft3d flow delft3d fm models models with their own approaches to solving the wave action equation are similar to the swan approach such as the unstructured grid wave models in fvcom qi et al 2009 suntans chou et al 2015 or schism roland et al 2012 solution of the wave action equation is computationally costly given that the directional spectrum is typically resolved with roughly 30 angles and 30 frequencies thus incurring o 1000 additional two dimensional transport equations to compute transport of wave action by the group velocity and currents in many cases because the wave spectrum can evolve more slowly than the currents this computational cost can be reduced by computing the waves less often than the currents there are two approaches to coupling the time averaged effect of waves to the currents the first is the radiation stress formalism in which the waves drive currents with the divergence of the excess wave momentum flux and has been the most common approach e g warner et al 2008 2010 kumar et al 2011 the second and recently more popular approach is the vortex force formalism in which the advective term in the horizontal momentum equations is written in terms of the divergence of the kinetic energy and a vortex force mcwilliams et al 2004 bennis et al 2011 and has been implemented in the structured grid roms uchiyama et al 2010 and coawst kumar et al 2012 models and the unstructured grid schism model guerin et al 2018 this approach has the advantage that it naturally decomposes the wave force into conservative and non conservative parts which gives better results in the presence of wave breaking particularly in the nearshore of the many active areas of research in wave modeling parameterizing the effects of transient rip currents by nearshore wave breaking is an important component of coastal modeling such currents are critical to accurately representing cross shore transport past the breaker zone an important mechanism for transport of tracers from small discharge streams giddings this workshop rip currents are not resolved in coastal models because wave models do not resolve the vertical vorticity arising from finite crest length breaking to incorporate these effects into roms in the coawst model kumar and feddersen 2016 2017 directly computed the vertical vorticity with the funwavec model feddersen et al 2011 a two dimensional boussinesq wave model that resolves finite crest length breaking the resulting vertical vorticity can be directly computed in funwavec and added as a source term to the roms model which then produces transient rip currents while this approach is costly because it requires computation of waves with funwavec the ultimate objective is to develop parameterizations for these effects that can be incorporated into the circulation model at a fraction of the computational cost coupling of winds and waves is a critical component of coastal wave modeling under weak to moderate wind settings wave models can accurately reproduce wind wave generation given the relatively accurate parameterizations of equilibrium and depth or fetch limited wave spectra however accurate wave modeling is elusive under extreme conditions particularly in storms as can be expected modeling of such extreme events is highly dependent on accurate modeling of the wind field by the overlying atmospheric model and requires dynamic coupling of ocean atmosphere and wave fields warner et al 2010 olabarrieta et al 2012 hegermiller this workshop points out that wave modeling is also limited by inaccurate parameterizations of wave breaking and wave current interactions under strong wind conditions ardhuin et al 2010 inaccuracies in wave models under extreme conditions are accentuated by feedback into the atmospheric and ocean models to which they are coupled because of errors in predictions of air sea fluxes under strong wave conditions including breaking zambon et al 2014a allahdadi et al 2019 3 2 numerical methods and modeling frameworks 3 2 1 a unified modeling framework the sense that emerged from the workshop was that the existence of multiple on going approaches to coastal and estuarine modeling is due to some basic challenges related to the coastal and estuarine parameter space in contrast to the more unified modeling framework like that seen in the regional modeling community i e roms coastal and estuarine model applications are highly dependent on resolving bathymetric coastline and forcing variability features that can be highly site specific as a result the community felt that it is important to ensure model diversity see table 1 to encourage application and testing of a wide variety of methods to understand resolution requirements related to the site specific parameters despite the aversion to a unified modeling framework there is a clear need for a common framework for model setup and analysis to reduce barriers for new users and facilitate more direct comparisons between approaches such a framework is sorely needed across all model classes including coupled and uncoupled and structured and unstructured grids the community felt there is a significant lack of standards for model coupling despite the wealth of coupled models similarly a more unified approach to model inputs including grid generation and boundary forcing files would greatly reduce the overhead and expertise required to apply a new model to a particular problem in addition to simplifying the process of model implementation and analysis a unified approach would significantly improve the ability to compare models model intercomparison would be encouraged because differences in model results depend critically on grid quality and accuracy of initial conditions forcing and boundary conditions features that are typically not highlighted in the peer reviewed literature as much as parameterizations and numerical methods development of a unified framework would be a daunting task given the extensive variety with which users implement models including compilers and operating systems data formats e g binary netcdf etc and the scripts that are employed for model setup and analysis e g bash python matlab etc this variety is further complicated by the need to update software to ensure compatibility with continuous advances in software engineering tools 3 2 2 a standard model test bed despite the importance of model diversity there are few if any studies that systematically compare the accuracy and efficiency of different models to gain insight into their advantages and disadvantages this is largely a result of the lack of a set of agreed upon benchmarks or test cases that can be applied to assess model performance it is standard practice to demonstrate model accuracy and efficiency through simplified test cases as examples the thacker test case is standard for wetting and drying e g casulli 2009 the lock exchange is standard for nonhydrostatic models e g fringer et al 2006 and the channel flow wind driven mixed layer and simplified estuary are standard cases for turbulence models warner et al 2005 although simplified test cases abound they test model performance in regimes that are expected to give smooth or converged results and so do not demonstrate model performance in scenarios that might be found in real problems simplified test cases also do not test model performance related to uncertainties in initial and boundary conditions the lack of benchmarks for real problems is likely a result of the subjectivity related to choosing parameters for model setup however even if a model setup is consistent between two model implementations there is substantial subjectivity in devising validation metrics to compare predictions to observations for example the skill score of murphy 1988 is common in coastal modeling and normalizes the difference between model results and observations by a measure of the difference between the observations and a reference model which is often taken as the mean of the observations or climatological values while this is a reasonable metric since it implies that a model achieves a better skill score if the difference between the observations and reference model is greater hetland this workshop showed that there is subjectivity in defining the reference model as it can require a time averaged or low passed signal about which the variance is defined as a result the skill score can vary significantly depending on how the error is normalized even if the absolute difference between the predictions and observations remains unchanged 3 2 3 higher order accuracy as they are currently implemented coastal ocean models do not always take full advantage of higher order accuracy in this context model accuracy is defined as the rate at which the error decreases with respect to spatial or temporal refinement for example a second order accurate model is one in which the error decreases quadratically with respect to grid refinement here the error can also be defined as the difference between the solution on one grid and the solution on a refined grid it is important to note that deterministic chaos may prevent convergence with respect to spatial or temporal refinement particularly when resolving horizontal spatial scales that are finer than the internal rossby deformation scale therefore strictly speaking one would need to conduct grid refinement studies of ensemble average simulations this would be prohibitively expensive from a computational point of view and so refinement studies can realistically only be conducted on deterministic problems nevertheless an advantage of using accuracy to gauge model fidelity is that in principle it does not require observations or truth because model accuracy is a test of whether the discrete equations converge to the exact governing partial differential equations therefore although one model may be more accurate than another because it has more advanced numerical discretization techniques this usually does not imply better agreement with observations owing to the dominance of errors related to forcing and boundary conditions furthermore model accuracy is only assured when the spatial scales over which the solution varies are at least one order of magnitude larger than the grid spacing satisfying such a constraint requires grid resolutions and problem sizes that are beyond the reaches of existing computational resources therefore most coastal applications are run with grid resolutions that allow grid scale variability resulting in grid dependent solutions that are generally not expected to improve with grid refinement without calibration of tunable parameters although the advantages of higher order methods have not yet been exhibited for coastal problems it is essential to employ at least second order accurate flux limiting schemes both for finite volume and discontinuous galerkin methods for scalar transport first order methods exhibit excessive numerical diffusion and cannot accurately predict physical processes with sharp horizontal gradients such as gravitational circulation river plumes or other frontal processes despite their importance for scalar transport flux limiting schemes are not as important for momentum or continuity overall it was agreed that although models can vary widely in the discretization schemes no model behaves as a second order accurate model in practice because of the overriding uncertainties from the bathymetry and forcing at best spatial and temporal accuracy of coastal models is somewhere between first and second order and in many cases spatio temporal resolution is limited by the resolution of the boundary conditions and forcing and available observations for validation most importantly high resolution coastal models cannot be accurate without accompanying high resolution accurate bathymetry furthermore accurate forcing is needed to accurately model the effects of such forcing for example the effects of the spatial variability of wind on estuarine circulation would not be possible without measurements of winds at many stations surrounding the estuary or accurate winds from an atmospheric model with sufficient resolution to resolve the spatial variability similarly an accurate model also needs more detailed measurements for validation for example validation of flooding with an accurate coastal storm surge model is not possible without accurate water level records during strong storm events particularly in regions that are normally dry 3 2 4 finite element vs finite volume methods both the finite element and finite volume method exhibit the potential for grid scale oscillations depending on mesh geometry and placement of discrete variables le roux et al 2007 korn and danilov 2017 while the spatial accuracy of both methods is sensitive to mesh quality in general the finite element method is better suited to the development of higher order spatial discretizations that are less sensitive to the grid the spatial accuracy of the finite volume method is typically restricted to first order on general unstructured meshes while second order or higher accuracy can only be achieved on cartesian or smoothly varying curvilinear grids spatial accuracy of finite volume methods can further degrade to less than first order on highly skewed meshes although the finite element method is more amenable to higher order spatial discretization schemes the finite volume method is much more common in coastal modeling see table 1 the finite volume method is more straightforward to implement and generally more computationally cost effective because the finite element method requires evaluation of costly numerical integrals and inversions of linear systems regardless of the time stepping scheme implicit time stepping schemes require inversions of linear systems on both grid types the finite volume method also has the advantage that it can guarantee local conservation of mass momentum and energy which is particularly attractive when enforcing monotonicity in scalar transport schemes finite element methods in contrast generally ensure global rather than local conservation although the discontinuous galerkin formulation can ensure local conservation some models such as schism employ the finite element method to discretize the momentum equations and the finite volume method to ensure mass conservation in scalar transport overall the community agreed that there is no clear advantage of finite volume vs finite element methods in existing popular coastal models given that it is less susceptible to grid quality the finite element method is superior in the implementation of adaptive mesh refinement amr as an example since the discretization error in the discontinuous galerkin method is related to jumps at element interfaces ainsworth 2004 this error naturally serves as a metric for local mesh refinement where and when it is needed bernard et al 2007 mesh refinement via the addition or removal of elements is referred to as h adaptivity while refinement via movement of element nodes without changing the number of elements is referred to as r adaptivity while both finite element and finite volume models can employ h or r adaptivity only finite element methods can employ p adaptivity wherein the order of accuracy of the discretization is varied in time and space without altering the mesh examples of ocean models that employ amr are the imperial college ocean model icom ford et al 2004 pain et al 2005 piggott et al 2005 and the nonhydrostatic unified model of the ocean numo which is currently under development and based on the nonhydrostatic unified model of the atmosphere numa giraldo and restelli 2008 giraldo et al 2010 although the amr approach is very powerful it is not yet common for coastal modeling because mesh resolution is often known a priori for most problems as it is dictated by bathymetry and coastlines ye et al 2018 however there is significant potential for application of amr to coastal flooding problems in which large portions of the domain that are normally dry can be active during flooding even without amr the performance of parallel storm surge computations can be optimized with load balancing strategies that account for the large variability of active cells during flooding roberts et al 2019 an important consideration related to high resolution modeling of coastal flooding with amr or high resolution grids is the need for accurate bathymetry vegetation and land use data in areas that are normally dry 3 2 5 tuning to account for unresolved processes and model error it is well established that coastal models must be tuned to account for unresolved processes and numerical errors perhaps the most ubiquitous of the many subjective parameter choices in coastal models is the bottom roughness which dictates the bottom drag imposed on momentum and waves and the production of turbulence by bottom shear see section 3 1 2 the distribution of bottom roughness is not known either because the physical parameters are not known or measured or because of inaccurate or nonexistent parameterizations the result is that most implementations require ad hoc tuning of the drag parameters particularly the bottom roughness to improve model skill such tuning not only accounts for unresolved physical processes impacting the drag but it also accounts for numerical errors as an example tuning of bottom roughness to account for numerical damping can lead to bottom roughness that is much smaller than the expected physical value since the numerical damping can overwhelm the physical damping the optimal bottom roughness also depends on the grid resolution due to its effect on the numerical damping and on differences between the resolved and unresolved scales e g ralston et al 2017 owing to the difficulty of predicting the distribution of vertical turbulent mixing as discussed in section 3 1 1 it often requires tuning of several parameters as an example ralston et al 2017 simulated the circulation in a salt wedge estuary with different bottom roughness coefficients grid resolutions and steady state richardson numbers they were able to tune the bottom roughness to give the best skill scores for water levels and depth averaged currents however a steady state richardson number of 0 1 which is smaller than the predicted value of 0 25 see section 3 1 1 was needed to give good model skill for baroclinic features in frontal regions to compensate for excess numerical mixing because the excess numerical mixing decreased with grid refinement the required steady state richardson number increased although it was estimated that a prohibitively high horizontal grid resolution of about 5 m would be needed to require the theoretically correct value of 0 25 these results suggest that the steady state richardson number is not a tuning parameter in the classical sense unlike the bottom roughness parameter there exists a grid resolution at which it is no longer justified to tune the steady state richardson number in a rans modeling framework because in principle the mean shear and stratification can be resolved a steady state richardson number parameterization is not needed for les see section 3 2 10 tuning of the bottom roughness or other parameters like the steady state richardson number to produce higher skill scores accounts for errors in both the parameterizations and numerical methods for scalar transport the principal error is numerical diffusion which can incur mixing that is larger than the physical mixing although higher order methods help to reduce spurious numerical diffusion in practice models exhibit near first order error in the presence of sharp fronts or around grid scale bathymetric variability the significant role that numerical diffusion plays in the distribution of mixing throughout an estuary makes it difficult to assess turbulence model performance because it is difficult to compare the vertical mixing computed by the model to the observed turbulent mixing recently finite volume methods have been developed to compute the spatio temporal distribution of numerical mixing burchard and rennau 2008 klingbeil et al 2014 which allows for a direct quantification of the amount of numerical relative to physical mixing the methods are non invasive in that they can quantify numerical mixing with minimal code alteration and can be applied for structured and unstructured grids in a finite volume framework results from several estuarine models at the workshop showed that a significant fraction of the mixing was numerical with the numerical mixing being greater than the physical mixing in some cases numerical diffusion in a model is highly dependent on the grid resolution numerical schemes being used and strength of material property gradients e g bathymetry velocity salinity being simulated although the notion of reducing the numerical mixing to a level that is smaller than the physical mixing poses a daunting challenge for coastal models in terms of resolution requirements such a challenge would be impossible without the ability to quantify the numerical mixing using methods like those developed by burchard and rennau 2008 and klingbeil et al 2014 3 2 6 grid generation and placement of variables the most important yet underappreciated aspect of coastal modeling is development of the computational grids grid generation is in some ways more difficult for structured curvilinear grids given the need for smoothness in curvilinear grids there are many grid generation tools for both unstructured and structured models e g gmsh gmsh info geuzaine and remacle 2009 sms aquaveo org janet smileconsult de yet there is no clear advantage of one over the other because grid generation continues to be a largely manual or tunable process grid resolution fundamentally dictates the accuracy and efficiency of the results yet grids can never fully resolve the complexity of the bathymetry and coastline in coastal problems in general unstructured grids are better at resolving complex bathymetric features than curvilinear or cartesian grids and unstructured grids can also more efficiently resolve multiscale features due to the flexibility of grid orientation and telescoping a good example of the advantage of unstructured meshes is the finite element model of the great barrier reef using slim second generation louvain la neuve iceocean model vallaeys et al 2018 which resolved the flow features throughout the detailed reef system with an extremely complex high resolution mesh legrand et al 2006 lambrechts et al 2008 to avoid significant grid stretching when using structured grids grid nesting must be used wherein grids with successively finer resolution are nested within one other e g roms agrif romsagrif gforge inria fr nesting enables use of smaller time step sizes on the finer grids thus reducing the number of time steps and the associated computational cost on the coarse grids it is possible to employ smaller time step sizes where cells are finer on single grids with the multirate approach as applied to the slim model seny et al 2013 however this method is difficult to implement and is not common in coastal ocean models although some effort has been made recently to more objectively construct grids and to ensure reproducibility candy and pietrzak 2018 generation of structured or unstructured grids is not fully automated because both require subjective decisions related to manual grid alteration in regions with degraded grid quality an additional advantage of unstructured grids is that grid edges are constrained to follow a specified coastline and so no masking is required to eliminate inactive cells over land from the computation this has the advantage that no memory or computational effort is wasted on masked cells more importantly however the process of grid masking is typically a manual process since it requires decisions about which grid cells will account for unresolved features such as narrow channels or headlands while unstructured grids do not need grid masking grid quality still typically degrades around complex bathymetric features since grids must be highly skewed when constrained by sharp coastline angles or grid scale bathymetric features either way both structured and unstructured grids require manual intervention in which grid nodes are moved to improve grid quality at the expense of poorer coastline resolution although finite element methods are more forgiving as discussed below the need for masking can be eliminated with use of subgrid bathymetry casulli 2009 a method that employs bathymetric resolution that is finer than the grid to ensure that the cell geometry follows the bathymetry without constraining the grid edges to follow coastlines see section 3 2 7 in general finite element methods are less sensitive to grid quality and so most finite element models can be run with little to no grid tuning finite volume methods can be highly sensitive to grid quality depending on arrangement of the variables on the grid on staggered or c grids arakawa and lamb 1977 pressure gradients are defined as normal to grid edges and are computed as the difference between the pressures at cell centers on either side of the edges therefore c grids must be orthogonal so that the lines connecting cell centers or voronoi edges are perpendicular to lines connecting cell vertices or delaunay edges generation of high quality orthogonal grids is extremely difficult when constrained by complex coastlines this can be alleviated with hybrid unstructured grids that can employ arbitrary sided cells thus allowing resolution of channelized features with quadrilateral grid cells and connecting the quadrilateral regions with triangles or other polygons e g macwilliams et al 2016 ye et al 2018 however there are no automated grid generation tools that employ both triangles and quadrilaterals other grid arrangements such as a all variables are collocated at cell centers or b velocity components stored at cell vertices grids alleviate the orthogonality constraint although no popular coastal models employ a grids because they tend to exhibit grid scale noise due to decoupling between the pressure and velocity danilov 2013 the fvcom model employs b grids and is one of the most robust coastal models regarding susceptibility to grid quality while the schism model avoids grid quality issues associated with c grids by employing finite element methods for the momentum equations owing to the problem of grid scale noise on a grids except for fvcom all popular regional or coastal models employ c grids roms being the most obvious example see table 1 interestingly weak grid scale noise is also a feature of unstructured triangular c grids korn and danilov 2017 and it can be amplified by poor grid quality wolfram and fringer 2013 however such noise is manifested in weakly dissipative settings over time scales that are much longer than those typically employed in coastal models danilov 2013 hexagonal c grids are a viable alternative that eliminate the noise although they can be difficult to generate in complex coastal geometries and have been better suited to global ocean modeling ringler et al 2010 3 2 7 subgrid bathymetry recent advances in bathymetric surveying have enabled extremely high resolution bathymetry at sub meter resolutions for entire coastal regions for example a digital elevation map is available for the entire country of the netherlands at a horizontal resolution of 0 5 m http www ahn nl such high resolution datasets provide bathymetry that will continue to be substantially higher than typical grid resolutions of coastal models for the foreseeable future with bathymetric resolution that is higher than the grid resolution typical coastal models subsample the bathymetry data through averaging to assign model depths at cell centers or edges while ensuring the same water volume relative to some datum on the subsampled bathymetry e g ye et al 2018 subsampling eliminates information about the high resolution or subgrid bathymetry that can be used to inform a more accurate simulation in the two dimensional subgrid bathymetry method of casulli 2009 adapted to three dimensions by casulli and stelling 2010 the finite volume framework uses the bathymetry data within a grid cell to obtain a more accurate representation of the cell geometry such as volume surface area and cross sectional area of cell faces as a result the cell geometry is independent of the computational grid resolution since it is only a function of the bathymetric resolution combined with more accurate volume and mass fluxes in most cases the subgrid method gives accurate solutions at a reduced computational cost because the computational grid can be coarsened without sacrificing accuracy related to the subgrid representation of the bathymetry as an example macwilliams et al 2016 simulated the san francisco estuary in three dimensions using the untrim model with the subgrid method of casulli 2009 and achieved similar accuracy as a previous high resolution simulation macwilliams et al 2015 with one order of magnitude fewer cells in the horizontal and a decrease in run time by a factor of 40 similarly sehili et al 2014 observed speedup by a factor of 20 using untrim with subgrid bathymetry to simulate the elbe estuary in addition to reduced computational cost the subgrid method also relieves the constraint on grid quality since grid boundaries do not need to be aligned with coastlines because the subgrid bathymetry ensures accurate geometric representation regardless of grid orientation as a result the subgrid method allows a three dimensional model to resolve channels with one grid cell in the cross section effectively providing for seamless transition between three and one dimensional modeling within the same framework this was demonstrated by stelling 2012 and sehili et al 2014 who show that subgrid modeling can resolve channels with one grid cell accurately as long as the cross channel variability in the friction term is appropriately accounted for in a way that gives the correct flow stage relationship an additional advantage of the subgrid bathymetry method is that it eliminates the stability restriction associated with wetting and drying thus further increasing computational efficiency by allowing larger time step sizes casulli 2009 while subgrid bathymetry can reduce the computational cost of computing flow and stage in channelized networks subgrid bathymetry may not necessarily give more accurate results for features with strong horizontal variability such as salt wedges fronts or flow features arising from strong cross channel bathymetric variability zhang 2017 accurate computation of these processes requires higher resolution of the base grid 3 2 8 the vertical coordinate system it is often argued that sigma s or in general terrain following vertical coordinates should be employed to accurately resolve along bottom flow in contrast to z levels or cartesian vertical coordinates that represent bottom topography with stair steps the stair steps give rise to grid scale variability in flow variables which can induce spurious numerical mixing legg et al 2006 and also lead to discontinuities in the bed shear stress platzek et al 2014 which can be particularly problematic for sediment transport another problem with z levels arises in the presence of large changes in water level due to strong tides which requires that the free surface cross over grid lines if sufficient vertical resolution is desired throughout the tidal cycle in shallow areas otherwise shallow areas might be resolved with just one grid cell in the vertical while deeper areas are resolved with many more despite these problems unlike sigma coordinates z coordinates do not exhibit the well known pressure gradient error particularly in the presence of steep bathymetry shchepetkin and mcwilliams 2003 the pressure gradient error is more problematic in coastal problems containing the shelf break seamounts or canyons it is typically less of a concern in estuaries where tidal currents are generally stronger than currents induced by the pressure gradient error and horizontal pressure gradients are weaker owing to stronger mixing and shallower water most popular coastal models employ sigma coordinates see table 1 except for untrim and suntans which employ z coordinates see table 1 the stair stepped nature of z levels can be reduced with partial stepping whereby the bottom face of the bottom most cell coincides with the bed or shaved cells adcroft et al 1997 in which the numerical discretization is rewritten about finite volume cells that are cut by the bathymetry so that one of the faces is coincident with the bed discontinuities in the bottom shear stress when using z levels can be avoided with remapping in which the velocity is remapped onto a terrain following grid at each time step to produce a continuous along slope velocity field and bottom stress distribution platzek et al 2014 the subgrid bathymetry method see section 3 2 7 can also be applied to improve the representation of the bottom bathymetry when using z levels practically speaking terrain following coordinates are more straightforward from a coding perspective because the number of active layers in the vertical is constant in time and space although the apparent advantages of each type of vertical coordinate are readily exhibited with idealized test cases there is no clear winner when applied to real coastal problems this is likely a result of other errors that make it difficult to quantitatively compare numerical discretization errors as discussed in section 3 2 3 nevertheless some models have shown great promise in application of hybrid vertical coordinates such as selfe which employs terrain following coordinates in the upper layers of the water column and z levels at depth zhang and baptista 2008 or slim which can employ a combination of fixed or adaptive z and generalized sigma coordinates delandmeter et al 2015 2018 vallaeys et al 2018 hybrid vertical coordinate models are advantageous in estuaries with broad shoals and gradual bathymetry in the shallows which is appropriate for sigma coordinates while z levels are employed in deeper regions where slopes may be steeper thus avoiding pressure gradient errors to reduce pressure gradient errors while retaining the advantages of both z and terrain following coordinates the schism model zhang et al 2015 employs localized sigma coordinates with shaved cells lsc2 wherein the slope of the sigma coordinates in the presence of steep slopes is reduced by adding more vertical layers near the bed despite the benefits of hybrid vertical coordinates all vertical coordinate approaches in popular coastal models exhibit spurious vertical numerical diffusion of scalars although numerical diffusion is reduced with lsc2 vertical coordinates the optimum approach is to employ arbitrary lagrangian eulerian vertical coordinates ale adcroft and hallberg 2006 in the ale approach the vertical coordinate moves with the flow via lagrangian trajectories followed by a correction to prevent grid distortion e g burchard and beckers 2004 hofmeister et al 2010 delandmeter et al 2018 in this way the vertical coordinate can naturally follow one or a combination of z levels s levels or isopycnal coordinates isopycnal coordinates by definition eliminate vertical numerical diffusion of scalars because there is no transport of scalars across isopycnal coordinate lines and hence no discrete vertical advection isopycnal coordinates are not well suited to coastal simulations given that there can be significant physical mixing and isopycnals are vertical at fronts however ale coordinates can be moved adaptively r adaptivity see section 3 2 4 to concentrate vertical coordinates in regions with strong stratification thus encouraging representation of sharper vertical density gradients quantification of spurious numerical diffusion with the techniques of burchard and rennau 2008 and klingbeil et al 2014 shows significant reduction of numerical mixing in coastal problems when using ale vertical coordinates gräwe et al 2015 3 2 9 nonhydrostatic modeling coastal models are typically hydrostatic because most processes of interest have long horizontal scales of motion relative to the vertical scales marshall et al 1997b the nonhydrostatic pressure is important only when considering processes that are short relative to the depth in order of decreasing horizontal scales these include solitary like internal gravity waves fronts and bores surface gravity waves convective overturning kelvin helmholtz like billows flow over short wavelength topography including small scale roughness such as dunes ripples vegetation etc and turbulence resolving such processes in coastal domains is computationally expensive because it requires very high resolution of the order of meters or smaller it is even daunting to resolve horizontal scales associated with internal solitary waves which are likely the largest scales for which the nonhydrostatic pressure is important in this regard the horizontal grid resolution must be smaller than the depth of the mixed layer in order to resolve the internal solitary wave dispersion vitousek and fringer 2011 which requires o 1 m grid resolutions in coastal problems the nonhydrostatic pressure is computed in many coastal and regional ocean models including mitgcm marshall et al 1997a trim casulli 1999 untrim casulli and zanolli 2002 suntans fringer et al 2006 roms kanarska et al 2007 auclair et al 2018 fvcom lai et al 2010a and getm klingbeil and burchard 2013 at smaller scales needed to resolve dispersion related to surface gravity waves several three dimensional nonhydrostatic models have been developed such as swash zijlema et al 2011 and nhwave ma et al 2012 shi et al 2015 in addition to the computational cost associated with the large number of grid cells needed to resolve nonhydrostatic effects computation of the nonhydrostatic pressure is expensive because it requires solution of an elliptic equation which can increase the computational cost of a coastal simulation by more than one order of magnitude fortunately because nonhydrostatic processes occur over short length scales they encompass a small fraction of the energy in most coastal problems and thus computation of the nonhydrostatic pressure may not necessarily show significant improvement of predictions over time scales greater than o 1 hr unresolved nonhydrostatic processes like convective overturning or shear instabilities are represented reasonably well by rans turbulence closure schemes despite the need for tuning as discussed in section 3 2 5 even the propagation speed of for example river plume fronts and short internal gravity waves are reasonably well predicted with hydrostatic models given that these propagate close to the hydrostatic long wave speed only when the details of such processes are of interest is the nonhydrostatic pressure important because of the relatively weak impact of small scale nonhydrostatic processes on large scale flows most nonhydrostatic studies are conducted in idealized domains most of these idealized studies are conducted in small domains so that the nonhydrostatic effects are dominant few studies have resolved small scale physics and their effects on large scale processes with a nonhydrostatic model examples include the study of shi et al 2017 who used the nonhydrostatic nhwave model to simulate the structure of a front near the mouth of the columbia river to resolve the details of fine scale nonhydrostatic features apparent in airborne imagery other field scale nonhydrostatic examples include simulation of nonlinear and nonhydrostatic internal waves in massachusetts bay by lai et al 2010b and sediment resuspension by internal bores in otsuchi bay japan by masunaga et al 2017 despite the realistic scales of these simulations only qualitative comparisons with field observations could be made because of the idealizations 3 2 10 large eddy simulation les in principle a coastal model could directly compute the turbulent scales of motion and eliminate the need for a turbulence model if it were nonhydrostatic since the turbulent scales are nonhydrostatic and the grid resolution was sufficient to resolve the turbulent scales of motion this could be accomplished with a direct numerical simulation dns for which the grid must resolve all of the turbulent scales of motion however dns is not feasible in coastal flows given that the grid spacing must be on the order of the kolmogorov dissipative scale or the batchelor scale if there is scalar transport which implies the need for an unrealistic number of grid points see e g ch 9 pope 2000 the computational cost can be alleviated with a large eddy simulation les in which the energy containing eddies are resolved by the grid and the small or subgrid scale eddies are parameterized with a so called subgrid scale sgs or subfilter scale sfs model pope suggests that 80 of the turbulent kinetic energy should be resolved the degree to which the computational cost is reduced for les when compared to dns depends on the flow of interest near boundaries the computational cost of les is still extremely high because of the need to resolve the small near wall turbulent scales that are proportional to the viscous wall unit ν u where u is the friction velocity to avoid the computational cost of resolving boundary layers the les can simulate the region away from the wall and parameterize the near wall region and the associated stress with so called wall layer modeling piomelli and balaras 2002 avoiding simulation of the near wall region decreases the needed grid resolution roughly by a factor of 10 in each direction leading to substantial savings in computational cost and the ability to simulate higher reynolds numbers piomelli and balaras 2002 as an example chou and fringer 2008 simulated suspended sediment transport in a channel with a reynolds number of 600 000 based on the channel height using les because the near wall physics were not resolved the wall stress was modeled with a quadratic drag law and the near wall vertical turbulent reynolds stress was augmented with the model of chow et al 2005 the augmented stress ensures that the near wall eddies are strong enough to vertically mix momentum to produce the correct mean logarithmic velocity profile an important constraint with this approach is the need for the first grid cell to fall within the lower end of the logarithmic velocity profile implying that the first grid point must be within roughly 10 100 wall units of the boundary piomelli and balaras 2002 this is similar to the requirement for rans based coastal modeling when the velocity in the bottom most grid cell is constrained to match the log law see section 3 1 2 however unlike in rans based coastal modeling in which horizontal grid resolutions are typically o 100 m 1 km the horizontal grid resolution in les is constrained by the need for the grid aspect ratio to be as close to unity as possible otherwise the accuracy will degrade due to numerical errors related to the sgs parameterization scotti et al 1993 and the nonhydrostatic pressure solver fringer et al 2006 santilli and scotti 2011 the accuracy of numerical methods used to discretize the governing equations also dictates the grid resolution in les an effect that is accentuated by high aspect ratio grids the importance of numerical methods in les is discussed by rodi et al 2013 who emphasize that central schemes for momentum advection should be used in les given that upwind biased schemes often produce too much numerical dissipation leading to an incorrect prediction of the turbulent kinetic energy spectrum although the cost of les is generally lower in regions where boundary layers are absent or not important stable stratification can also require high resolution for les since the grid resolution in the presence of stable stratification must resolve the ozmidov scale or the largest scale of turbulence before internal wave motions dominate the ratio of the ozmidov scale to the kolmogorov scale is proportional to r e b 3 4 where r e b ϵ v n 2 is the buoyancy reynolds number with dissipation ϵ and buoyancy frequency n e g smyth and moum 2000 with r e b as small as o 1 in the ocean thermocline ivey et al 2018 the resolution requirement based on the ozmidov scale in stably stratified boundary layers with les can be as limiting as resolving unstratified turbulent boundary layers this is a well known limitation in les of stable atmospheric boundary layers chow et al 2005 if the flow is well mixed i e there is no stratification and boundary layers are not important or can be modeled the grid resolution needed for les is less dependent on the reynolds pope 2000 or buoyancy reynolds numbers and instead is based on the need to resolve the energy containing scales of motion that are dictated by the problem of interest for example langmuir cells in the surface mixed layer can be simulated accurately with les if 10 20 grid points are used to resolve each langmuir cell skyllingstad and denbo 1995 similarly the convective atmospheric boundary layer is simulated accurately using les if the boundary layer is resolved with at least 50 grid points in the vertical sullivan and patton 2011 although wall models can be used to parameterize near wall physics simplified parameterizations like quadratic drag laws that are often used in wall modeling cannot account for the more complex dynamics associated with flow separation or stratification a novel method to reduce the cost of les in the presence of walls while retaining more complex near wall physics is to employ hybrid rans les approaches such as detached eddy simulation des rodi et al 2013 in this approach the near wall region is modeled with a rans approach see section 3 1 1 while the far field region is simulated with les the subgrid scale model in the les uses a similar eddy viscosity parameterization as the rans model except that the length scale in the les model is proportional to the grid resolution as examples in the des simulations of realistic river geometries of constantinescu et al 2011a b les was applied away from the walls to simulate the large scale flow separation and circulation while the turbulence in the hydraulically rough near wall regions was parameterized with the spalart allmaras sa one equation model spalart 2000 which only requires the near wall length scale to parameterize the eddy viscosity as opposed to the two equation models discussed in section 3 1 1 while hybrid rans les approaches and wall models are promising their application to coastal modeling is fundamentally limited by a lack of knowledge of the near wall physics which is ultimately stochastic in nature and not only difficult to model but also extremely difficult to measure therefore much of the uncertainty inherent in the traditional rans approach to coastal modeling discussed in this paper would not be eliminated with les modeling since it would still require tuning of unknown coefficients related to the parameterizations of near wall physics notwithstanding the difficulty with boundary conditions however the primary advantage of les modeling would be a direct calculation of turbulence away from boundaries thus eliminating the need to parameterize highly unsteady or stratified turbulence processes that can require tuning in a rans framework see section 3 2 5 3 3 field observations field observations form an integral component of modeling because they are needed for initialization forcing and validation this is particularly true for data assimilation and operational modeling as pointed out in the summary of the ioos sponsored workshop on operational ocean modeling by wilkin et al 2017 regarding forward and process modeling high resolution field observations are an essential component of understanding of physical processes that in turn leads to the development of improved parameterizations and modeling techniques like models observations are subject to errors and uncertainties and they are frequently modeled from the raw measurements 3 3 1 new instrumentation techniques among the many exciting new instrumentation techniques reid et al 2019 discuss measurements of nonlinear internal wave activity on the donghsa atoll in the south china sea using the distributed temperature sensor dts a 4 km long cable resting on the bed that recorded temperature every minute at a spatial resolution of 2 m while the most obvious benefit of such measurements is the ability to understand the details of high frequency processes related to nonlinear internal waves an additional advantage is that they lead to the development of high resolution simulations needed to understand those processes high resolution simulations would otherwise be difficult if not impossible without the necessary high resolution observations needed for validation davis this workshop presented results of high resolution nonhydrostatic simulations using the suntans model that were used to help interpret the high resolution observations which in turn were used to validate the model acoustic instruments to measure velocity like the acoustic doppler current profiler adcp which measures mean current profiles over the water column and the acoustic doppler velocimeter adv which measures high frequency currents i e turbulence at a point have been in use for several decades recent developments in acoustic instrumentation include the vectrino profiler which measures currents turbulence and ssc in 1 mm bins over 3 cm and can thus yield flow details within o 1 cm boundary layers in the environment e g wengrove and foster 2014 brand et al 2016 egan et al 2019 these detailed measurements can be used to directly test bottom stress and erosion parameterizations of wave current boundary layers in real field settings see sections 3 1 2 and 3 1 3 to broaden the parameter space and help develop parameterizations those detailed measurements can be compared to dns and les studies this presents a huge leap in our ability to use dns and les to develop parameterizations in real field scale settings such simulations have been restricted to laboratory scale experiments that until now were the only setting in which high resolution measurements like those with the vectrino could be obtained 3 3 2 remote sensing advances in high resolution remote sensing technologies are continuously increasing the resolution with which coastal processes can be measured for example giddings this workshop presented research on the large scale impacts of small scale coastal streams and the resulting plumes work that is motivated in large part by high resolution satellite remote sensing e g warrick and farnsworth 2017 although the high resolution imagery provides spatial detail models are needed to study the associated high frequency temporal resolution such as the high resolution simulations of romero et al 2016 which employed roms with four nested grids the finest had a resolution of 100 m to study the dispersion of a small river plume near santa barbara ca this resolution resolved the features of the submesoscale eddies and their interaction with the river plume although giddings and colleagues this workshop are employing roms nesting with a fifth nested grid that has 10 m horizontal resolution to study surf zone dispersion by waves interacting with the tijuana river plume near san diego ca these high resolution simulations are only possible with companion high resolution observations such as the aerial imagery and high speed jet ski transects employed by hally rosendahl et al 2014 or the infrared imagery by marmorino et al 2013 to study surf zone dispersion detailed observations like these form the impetus for the high resolution simulations of transient rip currents by kumar and feddersen 2016 2017 discussed in section 3 1 5 in a similar manner shi et al 2017 used the nonhydrostatic nhwave model with high horizontal resolution to understand the source of thermal fingers observed in aerial infrared imagery obtained at the mouth of the columbia river in addition to infrared remote sensing which reveals structures with a thermal signature recent remote sensing technologies have been developed to measure surface deflections with high spatial resolution as in the airborne lidar measurements of branch et al 2018 which could measure surface wave features at the columbia river mouth with o 0 5 1 0 m horizontal resolution structured from motion sfm photogrammetry using airborne drones dietrich 2017 is an exciting new technology to remotely measure high resolution bathymetry sfm methods can be used in combination with interferometric multibeam acoustic surveys to greatly increase the coverage and resolution of bathymetric data in shallow regions that are challenging to survey with traditional multibeam or single beam acoustic methods 3 3 3 acoustic backscatter recently methods have been devised using the backscatter signal from acoustic profilers to measure the vertical structure of flow density and sscs at extremely high resolution in coastal flows geyer et al 2013 for example geyer et al 2010 studied the dynamics of stratified shear instabilities in the connecticut river estuary with a high resolution broadband echo sounder that measured turbulent processes over vertical scales of o 10 cm in combination with adcps and advs to measure velocity horner devine and chickadel 2017 used infrared imagery to measure the surface features with o 1 m scale instabilities at the front in the merrimack river plume and these measurements were combined with subsurface backscatter measurements using adcps to infer the three dimensional structure of the instabilities and show that they are similar to lobe cleft instabilities found in gravity currents such high resolution field measurements naturally motivate high resolution nonhydrostatic simulations to further understand the underlying physics 3 4 high performance computing the coastal ocean models in use by the community today have been parallelized to some degree either using distributed memory message passing techniques such as mpi and or shared memory tools such as openmp it is well recognized that models that employ explicit methods in time or have simple matrix solves e g symmetric and diagonally dominant are typically easier to parallelize as they avoid the solution of potentially ill conditioned systems of linear and nonlinear equations commonly found in implicit methods however implicit solvers have become much more sophisticated in recent years with open source packages such as petsc now in wide use making them competitive for large scale parallel computing typical coastal models running large scale applications can scale to 100s or 1000s of cores on today s supercomputers as supercomputer architectures evolve with graphical processing unit gpu machines becoming more prevalent and hybrid cpu gpu machines coming online the algorithmic techniques must also evolve typical lower order methods in use today in most codes will probably not scale well on these machines due to low memory access to compute ratios higher order methods may actually perform better since more work is performed per cell meaning more local memory access however the difficulties with higher order methods mentioned in section 3 2 3 make this a challenging research area still if the coastal modeling community is going to play in the exascale computing arena of the future these challenges must be tackled head on and soon another high performance computing hpc arena that is rapidly evolving is the use of cloud computing cloud computing at least as it pertains to physics based simulations is still in its infancy and while nsf has funded cloud computing based research under some of its big data initiatives it remains to be seen what impact it will have on the coastal modeling community however cloud computing opens up entirely new frontiers in making computing resources available and more affordable to a larger community and will most certainly have a larger role in the future of hpc finally physics based simulators are becoming simply one part of simulation frameworks that merge big data uncertainty quantification and parameter estimation statistical inverse methods data assimilation and machine learning tools in these frameworks the simulator must often be executed tens to hundreds of times in order to generate statistical quantities of interest conditioned on uncertain data to be scalable and efficient these frameworks must utilize hpc and the physical based simulators must be optimized for performance the merging of data science tools with physics based simulation in hpc environments is another new frontier for the coastal modeling community to explore as our sources of data continue to advance at a rapid pace we must learn how to best utilize the data to improve predictive simulations 4 recommendations 4 1 collaboration engagement and education different groups within the coastal modeling community should more closely collaborate the original date of this workshop conflicted with the 15th estuarine and coastal modeling conference ecm15 which was held in seattle wa june 25 27 2018 despite the relevance of that workshop to the goals of this workshop it posed a conflict for just one of the forty invitees to our workshop this is indicative of the lack of communication in the coastal modeling community which appears to be divided into three groups 1 those working on development and application of forward models and process studies predominantly participants of this workshop 2 those working on assimilative models and state estimation techniques and 3 operational or applied modelers ecm15 including modelers from both academia and industry within these groups there are workshops and conferences with even more specificity such as focused workshops on data assimilation or unstructured grids e g imum international workshop on multi scale unstructured mesh numerical modeling for coastal shelf and global ocean dynamics while such specificity is natural and should be encouraged the community should also focus on workshops and collaborative initiatives that foster interaction of the different groups most importantly there is scant interaction between the forward and inverse modeling communities despite the importance of inverse methods in improving forward modeling capabilities and process studies similarly limited interactions between unstructured and structured grid or finite element and finite volume communities represent missed opportunities to synergistically address common problems in model development and application to what are often the same coastal regions the coastal modeling community should engage with the applied math computer science and hpc communities the lack of communication between the different groups within the coastal modeling community extends to a lack of communication between the coastal modeling community and the applied math and hpc communities coastal modeling can benefit significantly from the recent surge of methods in machine learning parameter estimation and inverse methods which can be used to quantify uncertainty and incorporate high resolution in situ and remote sensing techniques for improved predictions and parameter estimation into coastal ocean models see section 4 5 funding agencies should consider proposal calls which foster such engagement for example the collaboration in mathematical geosciences program was an nsf sponsored program that funded research collaborations between coastal modelers and applied mathematicians and led to the development of many of the techniques presented at this workshop academic programs should promote more applied math and computer science in their curricula the lack of collaboration between different groups in the coastal modeling community can be attributed to the academic backgrounds of those involved for example the prevalence of finite volume over finite element methods in coastal modeling is likely because traditional curricula in physical oceanography or coastal engineering focus on finite difference or finite volume methods because those require less mathematical background than finite element methods similarly data assimilation and machine learning techniques also require strong backgrounds in applied mathematics and computer science the coastal modeling community should integrate such methods into curricula to train the next generation of coastal modelers because coastal models have become sufficiently robust and accessible over recent decades scientists and engineers can now use coastal models in their research without having to develop their own codes or understand the details of a particular numerical method this precludes the need in many cases for coastal modelers to learn the applied math and computer science details involved however academic programs should broadly train students in the underlying principles and tradeoffs of different modeling approaches and introduce methods by which they might apply models to particular problems for example using the common framework described in the next section 4 2 a common framework for model setup validation and intercomparison discussions at the workshop focused extensively on model intercomparison to aid in model selection for a particular application and to assess accuracy of numerical methods model efficiency and the effectiveness of parameterizations employed in the models however it was noted that model implementation for real coastal problems requires many subjective choices that make it difficult if not impossible to quantitatively compare or reproduce results these subjective choices include for example the bathymetric resolution the structure of the grid model forcing datasets parameterization schemes numerical method options etc attempts to compare these different choices are difficult because they are generally not documented in academic papers largely because few if any scientific or engineering journals encourage publication of site specific modeling studies with technical details needed to reproduce model results instead academic papers focus on general descriptions of model setup with an emphasis on model validation that is also highly subjective this suggests an opportunity for the community to be more open to publications related to site specific model implementation and to give more credit for publications in technical journals of course a paper with all of the technical details related to a model application would be too boring for words therefore the community should embrace the open source model and use code repositories like github to promote completeness and transparency related to specific model applications although code repositories are common platforms to share source code for the models themselves it is rare to find references to repositories related to actual model applications in papers an added advantage of code repositories is the ability for individuals to modify model parameters and input data and post results of different implementations to the repository so that they can be made available to the community we note that academic papers focusing on numerical methods as opposed to model applications usually include details needed to reproduce the test cases however these test cases typically do not require subjective user choices because they are often simplified and designed to accentuate the behavior of a specific aspect of a model they also typically do not incorporate the spatial and temporal complexity of a realistic application to promote quantitative assessment of different models the community should develop a set of guidelines on how to report details related to coastal model implementation these details should include a list of all model parameters and subjective choices needed to reproduce the results including validation metrics and related data in addition the guidelines should promote sharing of datasets needed to initialize and force the models such as bathymetry grid wind data tidal data flow data etc ultimately it would be up to the community to define a set of standard reporting protocols to ensure all details needed to reproduce a model result are available these details would also include observational data used for model validation and the details of how validation metrics were created to encourage unified reporting of these model details the community should encourage proposal writers to document reporting strategies in for example the nsf data management plan to promote model intercomparison the community should agree on a set of i o standards and benchmarks based on idealized and real field scale test cases a unified reporting standard would not advance knowledge of the benefits and drawback of different models unless a set of standard benchmarks and test cases were agreed upon that would encourage model intercomparison studies while there are many idealized test cases that are reported in the literature there is no consensus on a set of standard idealized test cases that test different numerical aspects of coastal models such as advection schemes horizontal vertical gridding nonhydrostatic solvers free surface solvers etc the community should also agree upon specific study sites that form standard field scale benchmarks related to different estuarine or coastal regimes when possible grids bathymetry boundary conditions and forcing and computational cost should be standardized to eliminate the impact of subjective user choices in model intercomparison studies finally standard i o formats would need to be agreed upon to reduce barriers related to testing of new models and encourage application of many models to the benchmark sites although there are countless possibilities the community should form a consensus on benchmarks that are needed and specific sites that form the basis for those benchmarks as examples the columbia river estuary could be the benchmark for a salt wedge estuary while san francisco bay could be the benchmark for a partially mixed estuary a specific hurricane event could be the benchmark for storm surge models while a specific coastal region could be the benchmark for nearshore modeling the community should set standards for model coupling and nesting approaches several presentations at the workshop focused on simulations involving coupled modeling frameworks and nested approaches the most commonly coupled models are circulation and wave models such as swan roms some coastal circulation wave and storm surge prediction models are dynamically coupled to atmospheric models to obtain surface wind stresses and heat fluxes and have shown important improvement in both ocean prediction and storm track intensity forecasts zambon et al 2014a b nelson and he 2012 further accuracy can be achieved through coupling of circulation models with hydrology models that include surface runoff from precipitation these can be particularly important to predict compound flooding events when heavy rains and the associated runoff significantly increase water levels during strong storm surges e g silva araya et al 2018 dresback et al 2011 in addition to coupling of different modeling frameworks many applications benefit from the ability to nest higher resolution grids into coarser grids to study a specific region in more detail the most commonly employed example of grid nesting in ocean modeling is the roms based agrif approach romsagrif gforge inria fr which allows for one or two way nesting on successively refined grids despite the necessity of model coupling and nesting each implementation employs its own unique methodology for inter model communication for example some models employ the model coupling toolkit mct or the earth system modeling framework esmf which can handle communication on both serial and parallel implementations and are suitable for two way coupling model downscaling implementations involve either two way nesting in which boundary values are exchanged and updated by the parent and child models or one way nesting in which initial and boundary conditions generated by the parent model are written to files that are then treated as input for the child model a standardized approach to model coupling and nesting would provide several advantages over the existing relatively ad hoc paradigm first a standard approach would provide a framework that could be improved upon to make model coupling more efficient researchers could work with one framework and study the advantages and disadvantages of different approaches to help streamline the standard second the standard approach would enable coupling of a wider variety of models from which more model comparisons could be performed to assess the benefits of different models finally standardized model coupling approaches could provide a framework for use in applications that rely heavily on forcing and boundary conditions from different models such as hydrological and atmospheric forcing including winds and heating the community should devise standards for grid development and quality assessment it would be difficult to develop a grid generation tool that could be applied to general coastal modeling problems given the wide variety of grid types involved indeed there are countless grid generation tools and there is no obvious best choice nevertheless grid generation is largely subjective because of the difficulty in resolving grid scale coastal and bathymetric features see section 3 2 6 while it would be difficult to eliminate this subjectivity it could largely be reduced if such grid scale variability were eliminated either through bathymetric smoothing at subgrid scales or through higher resolution grids this may not be appropriate for many applications given the need to include grid scale coastal variability on some domains particularly in estuaries with complex island or channel networks however given the need to resolve the coastal submesoscale as discussed in section 4 3 higher grid resolution would inherently lead to less constraints on grid quality for grid generation tools in addition to higher resolution the concept of subgrid bathymetry section 3 2 7 also relieves the constraints associated with grid masking or coastline following grids in the shorter term the community should focus on encouraging more detailed reporting of grid generation strategies grid quality metrics and grid sensitivity studies grid generation strategies should more faithfully outline choices made regarding grid masking or adjustment particularly if there was a strategy that could be quantified over one that was subjective grid quality metrics should also be reported e g skewness telescoping fraction number of masked cells along with grid sensitivity studies the overall objective should be to encourage reproducibility of results regardless of the model or grid generation tool while noting that reproducibility can be affected by factors that cannot be controlled such as different variable precision among codes i e 32 vs 64 bit floating point arithmetic or stochasticity more open discussion of the process behind and results of grid generation will hopefully facilitate development of community tools that are robust and not specific to a particular model such as the shingle framework candy and pietrzak 2018 validation metrics should be standardized and include dynamically relevant integrated metrics that are representative of the overall utility of the model in addition to the skill score of murphy 1988 as discussed in section 3 2 2 there are many metrics used to compare coastal model predictions to observations such as the model skill metric of wilmott 1981 and standard statistical metrics such as the correlation coefficient and mean and root mean square errors because there is a general lack of agreement on standards for these metrics the community should make a concerted effort to create such standards with the understanding that these will likely be highly site and problem specific for example the choices and metrics used to evaluate model ability to reproduce sscs in a salt marsh would be very different from those used to evaluate model ability to reproduce the significant wave heights during a hurricane ultimately details of the validation metrics should be part of the aforementioned standards related to reporting guidelines and benchmarks in addition to standardizing existing validation metrics new validation metrics should be devised that incorporate integrated or dynamical quantities such as fluxes area integrated quantities or time averaged or low frequency metrics for example different components of the salt flux stokes drift mean etc might be more appropriate validation metrics than time series of bottom salinity the challenge with such metrics is that while they are readily computed from three dimensional model outputs it is harder to compute them with observations observational campaigns should therefore focus on methods to better validate such integrated quantities in models advances in remote sensing technologies are promising in this regard given their ability to measure spatial distributions of water properties at increasingly higher resolution see section 3 3 2 some integrated metrics do not require observations such as the ratio of numerical to physical mixing see sections 3 2 5 and 3 2 8 for which the ideal model would eliminate the numerical mixing entirely 4 3 resolve the coastal submesoscale the community should collaborate more closely with the high performance computing and applied mathematics communities to develop high resolution accurate models that directly compute the coastal submesoscale as discussed in section 3 1 1 it is unlikely that parameterizations for coastal submesoscale processes can be developed given that such processes are highly site specific and dependent on local geometry and other related physical processes therefore the coastal modeling community should focus on developing modeling tools that can directly simulate such processes in addition to attempting to parameterize them simulations at grid resolutions that would resolve the coastal submesoscale would require o 1 10 m horizontal grid resolution in estuaries and o 0 1 1 km in coastal shelf domains which would place a heavy burden on computational requirements however high performance computing platforms have advanced significantly in the past few decades and there is great potential for the development of high resolution coastal models that run efficiently on such platforms see section 3 4 with higher resolution models would be less susceptible to numerical error which would enable quantification of model uncertainty due to bathymetry and boundary conditions less numerical error would also allow for assessment of the benefits of more advanced computational techniques which are traditionally reserved for idealized problems 4 4 coordinate observational and modeling studies to improve parameterizations parameterizations should be tested and advanced using direct comparison between high resolution state of the art measurement technologies and focused modeling studies there is active research in many areas related to parameterizations of unresolved processes in coastal models see section 3 1 the community identified the following as among the most important and relevant for coastal modeling noting the importance of developing parameterizations that require as little tuning as possible 1 parameterizations of the spatial and temporal variability in horizontal diffusion and dispersion bottom roughness and unresolved drag including dependence on both physical bedform grain size vegetation kelp corals waves and model grid resolution advection scheme characteristics 2 nearshore wave modeling wave breaking parameterizations wave mud damping 3 air sea interaction under high wind conditions including air sea momentum and buoyancy flux exchanges wave breaking and wave current interactions 4 sediment transport modeling erosion parameterizations flocculation settling bed consolidation biological effects 5 morphodynamics while there has been extensive work on parameterizations within each of the categories listed it is not always clear whether the parameterizations improve the coastal models in which they are implemented therefore to be relevant for coastal scale processes improvements to parameterizations should clearly demonstrate improved predictive capability of coastal models development of parameterizations with clear connections to coastal model results requires stronger collaboration between observationalists experimentalists and modelers over different scales i e large scale vs les and dns and it also requires development and application of more advanced observational techniques as an example parameterizations of wind wave sediment resuspension are difficult to test in the field because of the difficulty in observing the true bed stress and the true near bed sediment erosion however it is now possible to directly measure turbulence mean flow and sscs in 1 mm bins near the bed using the profiling vectrino which allows for direct assessment of the accuracy of existing parameterizations of bottom drag and sediment erosion see section 3 3 1 such instruments also allow for integration of les or dns results into development of improved parameterizations because they can be validated recent advances in remote sensing technology also allow for tighter coupling between observations and models since remote sensing provides higher spatial resolution that can be used to test parameterizations see section 3 3 2 4 5 robust parameter estimation and uncertainty quantification coastal models should incorporate advanced tools to more robustly estimate parameters and quantify uncertainty the greatest impediment to the development of more accurate coastal models is a lack of knowledge of the uncertainty the uncertainty has numerous sources including parameterization error numerical error including the discretization and errors related to grid quality and errors from boundary conditions and forcing the path to reducing the errors related to each of these sources on their own is clear and many of the recommendations in this paper suggest strategies to reduce those errors however owing to an inability to quantify the relative contribution of different sources of uncertainty accurate coastal modeling relies more on subjective choices see section 3 2 5 than it does on quantitative metrics subjectivity plays a dominant role in much of coastal modeling given that accurate simulations require an experienced user to make decisions in an ad hoc manner during the modeling process including 1 the numerical methods or simply the choice of which coastal model to use 2 the parameterizations and their underlying constants 3 choice of suitable datasets for boundary conditions and forcing and the interpolation techniques to impose those conditions at model grid points 4 the model grid and 5 validation techniques the result is that accuracy of model results is typically attributed more to the experience of the model user than to the accuracy of the model itself to eliminate the subjectivity related to coastal modeling the community should incorporate advanced tools in applied mathematics and computer science to quantify uncertainty and develop robust techniques to objectively guide model choices and estimate optimal model parameters these advanced tools include uncertainty quantification data assimilation and machine learning data assimilation methods use observations to improve predictions and are well established in regional ocean models e g edwards et al 2015 largely owing to the prevalence of regional scales in operational modeling systems that must assimilate data to ensure predictability given its success at regional scales there is ample room for data assimilation in coastal modeling studies particularly in the context of parameter estimation data assimilation methods can inform the optimal parameter sets that minimize the difference between predictions and observations thus providing a quantitative methodology to estimate parameters that are unknown or difficult to measure for example zhang et al 2018 assimilated remote sensing data of surface ssc to estimate the spatial distribution of the settling velocity which is typically a tunable parameter as discussed in section 3 1 4 in a three dimensional cohesive sediment transport model similarly zhang et al 2011 used data assimilation to estimate the spatial distribution of bottom friction coefficients which are also typically tuned as discussed in section 3 2 5 in a regional tidal model while the focus of data assimilation is to use observations to minimize some measure of the model error the source of the error is determined with methods in uncertainty quantification which are also popular in regional ocean modeling e g lermusiaux et al 2006 as an example in the coastal ocean manderson et al 2019 quantified the uncertainty of the density stratification and demonstrated how the uncertainty is manifested in nonlinear internal gravity wave models given the many tunable parameters in coastal modeling particularly for sediment transport it is important to understand the uncertainty related to each parameter to avoid the potential for equifinality or the possibility of the same result arising from different sets of parameters van maren and cronin 2016 in addition to providing a robust framework to characterize the numerous sources of uncertainty in coastal models uncertainty quantification can also help prioritize future research directions based on where uncertainty in coastal models is greatest owing to the continued improvement of observational technologies and increased quantities of observational data see section 3 3 more machine learning should be incorporated into coastal ocean modeling because machine learning extracts relationships from datasets without the need for models based on first principles it can be used to estimate parameters that are difficult if not impossible to measure machine learning techniques have seen great success as a tool to understand and predict coastal sediment transport and morphodynamics as discussed in the review article by goldstein et al 2019 as examples yoon et al 2013 used an artificial neural network approach to determine the hydrodynamic parameters that best predicted ssc in the surf zone while goldstein and coco 2014 used machine learning to predict the particle settling velocity in a dataset derived from various suspended sediment flows this approach to determining parameters from data can be combined with models in what are referred to as hybrid models wherein machine learning is used to determine model parameters based on observational data goldstein et al 2019 like data assimilation this hybrid approach appears to be a promising method to reduce ad hoc tuning and subjective parameter choices in coastal ocean modeling 5 conclusions the primary outcome of the workshop was agreement on the need to reduce subjectivity in implementation of coastal ocean models this subjectivity arises from the need to make choices that rely on experience rather than quantitative metrics ironically because only experienced model developers and users attend a workshop of this kind the model results that are presented reflect the subjective choices that can only be made with extensive experience it was agreed that subjectivity should be reduced through development of a common framework for coastal model users and developers through stronger engagement with applied mathematics and computer science communities and through implementation of methods in data assimilation uncertainty quantification and machine learning to understand the sources of uncertainty and quantify parameter choices in coastal ocean modeling a second outcome of the workshop was an understanding of the importance of setting standards for numerous aspects of coastal modeling the lack of which is partially related to the subjectivity inherent to the current state of the art although workshop participants had extensive experience with models most lamented at the lack of standards to guide model development and dissemination of results the greatest advantage of setting standards is that they encourage the community to focus efforts in favor of continued model assessment and improvement it was thus agreed that the community should focus on setting standards for the following aspects of coastal modeling 1 implementation details needed to reproduce model results 2 input output standards for ease of model inter comparison 3 benchmarks to test and compare model performance 4 coupling and model nesting 5 grid generation and 6 model validation regarding technical details of coastal models it was agreed that it is difficult to assess the advantages of different numerical methods or parameterizations this is due in part to the complexity of coastal modeling and a lack of standards to assess and compare models and hence can be partially addressed with the outcomes discussed above however a basic theme emerged regarding the development of advanced numerical methods and was related to the coastal submesoscale throughout the workshop this refers to horizontal scales that are smaller than the horizontal bathymetric scale but larger than the depth in coastal modeling unlike ocean submesoscales these coastal submesoscales are strongly controlled by the coastal geometry and hence are highly site specific as a result there is little hope in developing parameterizations for them and hence the community should work toward resolving coastal submesoscales with high resolution simulations like the subjectivity problem this also warrants collaboration with the applied math and computer science communities but in this case to develop accurate numerical methods and high resolution efficient simulations on advanced hpc systems while it is possible to resolve the coastal submesoscale smaller scale processes with scales smaller than the depth will likely never be resolved not only are these scales prohibitively small but they are dictated by small scale features that are hard to measure and hence must be modeled or parameterized such as turbulence the community should continue to focus on developing parameterizations for such processes following the wealth of research that has already been done to date however it was agreed that there should be tighter coupling between observations laboratory experiments and modeling to focus specifically on developing and testing of parameterizations in coastal models in the past it has been difficult to test parameterizations in field scale models because of limitations in observational technologies which could not directly measure the parameters needed for the processes being parameterized e g the bottom stress or sediment erosion rates observational technologies have advanced significantly and hence it is now possible to directly test parameterizations in situ accomplishing the objectives laid out in this paper will require buy in from funding agencies to support critical components of modeling that have not been part of traditional funding streams in the past this could include support for research that focuses on inter comparison studies or development of benchmarks or modeling standards such as i o coupling or validation metrics such benchmarks or standards could then be the focus of future workshops on coastal modeling to foster collaboration among different groups within the coastal modeling community in a similar vein while model data is typically integrated into proposal data management plans model test cases and supporting documentation could be an integral part of these plans funding agencies should also encourage collaboration between applied mathematicians computer scientists and coastal modelers to help accomplish many of the objectives laid out in this paper funding agencies will play an important role in the future of coastal ocean modeling but buy in of the objectives laid out in this paper will ultimately come from coastal modeling community members who are tasked with reviewing proposals and recommending funding therefore the proposal submission and review process should involve new priorities and evaluation procedures and the community needs to identify and develop sustainable means of funding the initiatives proposed in this paper given the complexity of developing testing and maintaining coastal models they can be just as difficult and costly to develop and support as ships or state of the art equipment and instrumentation therefore models should be treated as a fundamental component of critical infrastructure needed to support research just like laboratory facilities field instrumentation and research vessels the notion that models constitute critical infrastructure implies that model maintenance and development should be an important component of infrastructure or facilities sections of proposals as with ships and major laboratory facilities that are used broadly by the science community coastal model development maintenance and support cannot be expected to be funded only through core science budgets that support hypothesis driven science it has often been the case that new models or parameterizations have emerged from such hypothesis driven research but this ad hoc approach is unsustainable as coastal models are now used broadly by non developers to advance basic science coastal models have become an important community asset that should be supported like other key infrastructure which will likely require commitment and coordination of resources across multiple funding agencies e g the federal agencies in the united states nsf noaa onr doe usgs a key result of this workshop was that the range of coastal model applications benefits from a diversity of modeling approaches but their accessibility and evaluation are hampered by legacy impediments developing the tools and frameworks to lower the structural barriers requires investment in order to realize an improved next generation of coastal ocean models acknowledgments we thank carmen torres at stanford university and jennifer warrillow at north carolina state university for their assistance with workshop logistics helpful comments and suggestions were provided by two anonymous reviewers and hans burchard and john warner the workshop and preparation of this paper were funded by u s national science foundation grant oce 1749613 appendix workshop participants and presentation titles invited early career scientists indicated with workshop presentations are available for download from web stanford edu fringer nsf workshop 2018 ateljevich eli cal dept of water resources from coast to estuary to channels challenges in cross scale modeling of the san francisco bay delta baptista antonio oregon health and science university is in silico estuarine oceanography here yet lessons from a humbling benchmark blain cheryl ann naval research laboratory approaches to capture freshwater influence in coastal and estuarine waters burchard hans leibniz institute for baltic sea research the concept of numerical mixing in coastal oceans chai fei u maine modeling nutrients and plankton dynamics of the san francisco bay chao yi ucla modeling the california coastal ocean and its interactions with san francisco bay chen changsheng u mass dartmouth importance of resolving coastal estuarine wetland interactions in estuarine modeling davis kristen u c irvine spatially continuous observations of shelf and estuarine processes what can new observational tools tell us about what we re getting right and wrong in coastal models dawson clint u t austin some hpc challenges in coastal modeling dietrich casey ncsu connecting coastal infrastructure to predictions of storm surge and flooding fringer oliver stanford will we ever simulate via nonhydrostatic les real estuarine and coastal problems ganju neil usgs progress and challenges in simulating coupled hydrodynamic vegetation processes georgas nickitas jupiter predicting risk in a changing climate geyer rocky whoi estuarine salinity variance and mixing is numerical diffusion trying to tell us something giddings sarah scripps inst of oceanography capturing the dynamics of ocean estuarine exchange when small discharge rivers matter gross edward u c davis hydrodynamic and particle tracking modeling to support fish migration studies he ruoying ncsu modeling air sea interactions during storms hegermiller christie whoi towards simulating extreme coastal morphological change using coupled models hetland robert texas a m university the whimsy of model data comparison in coastal ocean modeling hsu tom u delaware insights into several issues in sediment dynamics investigated by turbulence scale and wave scale models kirby jim u delaware surface waves the interface between phase resolving and phase averaged models and outstanding issues in each setting klingbeil knut leibniz institute for baltic sea research the problem of numerical mixing and its mitigation through adaptive vertical coordinates kumar nirnimesh u washington parameterizing the effect of surf zone eddies in 3d models implications for cross shore exchange and surf zone dispersion li ming u maryland climate downscaling projections for estuarine hypoxia and acidification using coupledhydrodynamic biogeochemical models luettich rick u north carolina challenges of moving from hindcasting to forecasting storm surge and inundation maccready parker u washington challenges in realistic modeling of estuary shelf connections moriarty julia usgs challenges and opportunities in regional scale hydrodynamic sediment transport modeling olabarrieta maitane u florida modeling the long term morphodynamic evolution of estuaries advances and challenges orton phillip stevens inst of tech the utility of fast accurate models stories from the front lines of disaster and adaptation ozkan haller tuba oregon state u nearshore modeling what s data got to do with it pietrzak julie t u delft lessons learnt from a tidal river plume the importance of frontal dynamics near the river mouth ralston david whoi modeling sharp salinity gradients in a tidal salt wedge and river plume scully malcolm whoi bathymetrically controlled inflow events in chesapeake bay shi fengyan u delaware simulations of river plumes using a sub meter resolution non hydrostatic model siedlecki samantha u connecticut marine sciences getting it right for the right reasons lessons from simulating regional biogeochemistry as part of decision support tool development signell rich usgs interactive scalable data proximate analysis of coastal ocean model data in the cloud vitousek sean u illinois chicago challenges in modeling waves turbulence and sediment transport across scales westerink joannes notre dame the evolution of process and scale coupling in coastal ocean hydrodynamic modeling wilkin john rutgers the iooc task team on coastal modelling for ioos a community consensus on research priorities for integrated analysis across littoral estuary and shelf regimes zhang joseph virginia institute of marine science simulating estuarine circulation in the chesapeake bay shelf system 
24014,this paper summarizes the findings of a workshop convened in the united states in 2018 to discuss methods in coastal and estuarine modeling and to propose key areas of research and development needed to improve their accuracy and reliability the focus of this paper is on physical processes and we provide an overview of the current state of the art based on presentations and discussions at the meeting which revolved around the four primary themes of parameterizations numerical methods in situ and remote sensing measurements and high performance computing a primary outcome of the workshop was agreement on the need to reduce subjectivity and improve reproducibility in modeling of physical processes in the coastal ocean reduction of subjectivity can be accomplished through development of standards for benchmarks grid generation and validation and reproducibility can be improved through development of standards for input output coupling and model nesting and reporting subjectivity can also be reduced through more engagement with the applied mathematics and computer science communities to develop methods for robust parameter estimation and uncertainty quantification such engagement could be encouraged through more collaboration between the forward and inverse modeling communities and integration of more applied math and computer science into oceanography curricula another outcome of the workshop was agreement on the need to develop high resolution models that scale on advanced hpc systems to resolve rather than parameterize processes with horizontal scales that range between the depth and the internal rossby deformation scale unsurprisingly more research is needed on parameterizations of processes at scales smaller than the depth including parameterizations for drag including bottom roughness bedforms vegetation and corals wave breaking and air sea interactions under strong wind conditions other topics that require significantly more work to better parameterize include nearshore wave modeling sediment transport modeling and morphodynamics finally it was agreed that coastal models should be considered as key infrastructure needed to support research just like laboratory facilities field instrumentation and research vessels this will require a shift in the way proposals related to coastal ocean modeling are reviewed and funded keywords coastal ocean modeling physical processes model subjectivity development of standards high resolution modeling parameter estimation 1 introduction coastal and estuarine modeling is concerned with understanding and predicting marine processes in coastal oceans and estuaries although this includes physical and biogeochemical processes the focus of this paper is on the physical processes impacted by tides winds surface waves and hydrological processes including fresh water and sediment laden flows one component of coastal and estuarine modeling is the prediction of sediment transport including both fine sediments in shallow estuaries and coarser sediments in nearshore wave driven environments over long time scales sediment transport governs morphodynamics which strongly impacts coastal and estuarine flows unique to coastal and estuarine modeling is the connection to human influences particularly in densely populated coastal regions where flows can be altered by coastal structures dredging and sand nourishment operations and anthropogenic sources of contaminants and nutrients significantly impact coastal biogeochemistry given that roughly 60 the world s population lives within 60 km of the coast and this is expected to rise to 75 within a few decades rao et al 2008 accurate coastal and estuarine modeling is an essential component of efficient management for the sustainability of natural coastal systems and the development and improvement of sustainable urban infrastructure particularly in the face of rapid urbanization of coastal cities and changing climate including sea level rise accurate coastal and estuarine modeling is also a critical component of climate modeling because coastal shelves contain roughly the same amount of primary productivity and biomass as the open ocean whittle 1997 sharp 1988 yool and fasham 2001 the focus on physical processes in this paper rests on the assumption that they are fundamental to modeling nearly all other processes in the coastal ocean including pollution transport water quality biogeochemistry and coastal ecology since modeling each of these requires coupling to a circulation model that computes the transport and mixing in the context of modeling of physical processes in this paper we will distinguish between two distinct types of ocean modeling at larger or regional scales regional ocean models typically assume a geostrophic balance to leading order i e rotation in balance with pressure gradients and are weakly dissipative at smaller coastal and estuarine scales coastal models are fundamentally ageostrophic three dimensional and driven by boundary layer processes in the shallowest regions of the coastal environment coastal models must accurately capture frictional balances such as between the barotropic or baroclinic pressure gradients and bottom friction when simulating strong tides or storm surges coastal models must also account for wetting and drying and hydrological forcing to account for the effects of runoff from precipitation coastal models meant to capture the transitional nature between shallow environments and regional scales must be able to simulate both highly frictional ageostrophic motions and balanced flows models of physical processes in coastal environments have seen significant advances in the past two decades owing to increases in computational power and improved numerical methods including unstructured grids model nesting data assimilation and model coupling furthermore advances in remote sensing and in situ observational technologies have led to substantially larger and more accurate datasets which have significantly improved the ability to assess model performance multiple coastal models have been developed in the past two decades although there is no dominant model in sharp contrast to regional modeling for which the roms model shchepetkin and mcwilliams 2005 is the most common in the united states or wave modeling for which the swan model booij et al 1999 is the most common for coastal wave problems in the united states as an example while few if any models other than roms have been used to simulate regional circulation on the u s west coast e g neveu et al 2016 chao et al 2009 2018 circulation in san francisco bay has been modeled with at least six different models in the past ten years 1 suntans fringer et al 2006 was applied by chua and fringer 2011 and holleman et al 2013 2 untrim casulli and zanolli 2002 2005 was applied by macwilliams et al 2015 2016 3 trim3d casulli and cattani 1994 was applied by gross et al 2009 4 schism zhang et al 2016 was applied by chao et al 2017b and 5 delft3d flow and 6 delft3d fm oss deltares nl web deflt3d home were applied by erikson et al 2013 and martyr koller et al 2017 respectively although there have been no systematic comparisons of these models the most detailed calibration and best performing model appears to be the untrim implementation by macwilliams et al 2015 2016 although it is difficult to argue that the performance can be attributed to the model itself as opposed to superior grids bathymetry and forcing i e winds tides river inflows etc the fact that there is no dominant coastal ocean modeling strategy like that seen in regional modeling presents an opportunity to determine whether there is a need for a unified approach in coastal ocean modeling obviously there is no need for such an approach if existing strategies are efficient and accurate enough to answer pressing questions related to coastal processes to this end a workshop was held to determine the current state of the art in coastal ocean modeling and to form a consensus on key areas of research and development needed to improve the accuracy and reliability of such models key questions posed to workshop participants and that will be addressed in this paper are 1 where should we focus our efforts related to improved parameterizations in coastal modeling 2 what aspects of numerical methods related to coastal modeling can be improved 3 how can in situ and remote sensing measurements be used and improved to benefit coastal modeling 4 how can coastal modeling better leverage hpc resources there have been other recent workshops with similar objectives the paper by wilkin et al 2017 summarizes the outcomes of an ioos sponsored workshop that was held to advance coastal ocean modelling analysis and prediction as a complement to the observing and data management activities of the coastal components of the u s integrated ocean observing system ioos and the u s global ocean observing system goos the findings of that workshop concluded that the community should focus on the following seven topical areas 1 model coupling 2 data assimilation 3 nearshore processes 4 cyberinfrastructure and model skill assessment 5 modeling for observing system design and operation 6 probabilistic prediction methods 7 fast predictors as will be discussed below the findings of the workshop discussed in this paper are similar although the recommendations focus more on process and forward modeling rather than predictive and data assimilative modeling for observing systems like ioos and goos there are also coastal modeling initiatives in europe with similar objectives such as the german coastal modeling working group 1 1 http www deutsche meeresforschung de en coastalmodelling that is charged with defining challenges for coastal modeling encouraging cooperation between developers and users developing a national forum for coastal ocean modelling and developing common infrastructure as part of that working group a workshop was held in germany 2 2 https www io warnemuende de comod2018 html in february 2018 with the goal of increasing the communication between coastal ocean modellers in german marine research institutions and focused on answering the following questions 1 what are the future challenges in coastal ocean modeling 2 do we need better coordination between model developers and model applicants on the national level 3 could we profit from a common repository of reference model results 4 would we profit from a coastal ocean model intercomparison study coastalmip 5 do we need to develop new models or are we happy with what we have 6 do we need common interfaces for model and module coupling 7 are the national super computing resources sufficient rather than providing a comprehensive review of the state of the art in modeling of physical processes in the coastal ocean this paper summarizes key issues as presented and discussed by workshop attendees these issues range from numerical methods to parameterizations to observational technologies for a comprehensive overview of numerical methods for coastal models the reader should consult the review article by klingbeil et al 2018 klingbeil presents details of time discretization and wetting and drying schemes topics not mentioned in this report but that are crucial to coastal ocean modeling the reader should refer to the review article by medeiros and hagen 2013 or the paper by candy 2017 for a detailed discussion of wetting and drying algorithms in addition to a discussion of wetting and drying no overview of ocean modeling would be complete without a discussion of the state of the art in lagrangian particle tracking a review of which is given by van sebille et al 2018 although nearshore wave and sediment transport modeling were discussed at the workshop and in this report a more detailed review can be found in kirby 2017 2 workshop organization and attendees a four day workshop funded by the u s national science foundation nsf physical oceanography program was held during june 18 21 2018 at the stateview hotel and conference center in raleigh nc u s a on the campus of north carolina state university a total of 40 participants attended the workshop 29 of whom were more senior and gave 15 min presentations see appendix the senior researchers nominated 11 junior scientists who were allotted 30 min for their presentations research interests among the participants reflected a balance between model developers and users among the developers and users interests were equally divided between those with a stronger coastal focus and those with a stronger estuarine focus roughly half of those focusing on coastal processes had interests in storm surge modeling while six researchers had specific interests in coastal engineering and or nearshore processes finally there were three biogeochemists and two researchers focusing on wetlands most of the attendees were forward or process modelers and hence the outcomes focused less on data assimilation techniques more commonly employed in operational modeling indeed an important outcome of the workshop is the need for greater collaboration between forward and operational modelers and for forward modelers to adopt more techniques commonly employed in the operational and predictive modeling communities many of the original developers of most of the popular coastal models used in the united states were present at the meeting see table 1 including the finite element adcirc model luettich et al 1992 westerink et al 1994 the roms based coupled ocean atmosphere wave sediment transport modeling system coawst warner et al 2008 2010 the finite volume unstructured grid model fvcom chen et al 2003 the curvilinear coordinate finite volume model getm burchard and bolding 2002 the general ocean turbulence modeling framework gotm burchard et al 1999 the mixed finite element finite volume models selfe zhang and baptista 2008 and schism zhang et al 2016 and the finite volume unstructured grid and nonhydrostatic suntans model fringer et al 2006 also present was the developer of the biogeochemical model cosine chai et al 2002 2003 2007 other popular models are also discussed in this paper as listed in table 1 3 state of the art 3 1 parameterizations 3 1 1 the coastal submesoscale and turbulence modeling unresolved processes that must be parameterized in coastal modeling can be regarded as those that are smaller than the estuarine submesoscale or coastal submesoscale geyer this workshop in analogy to ocean submesoscale processes in regional or global ocean modeling like the ocean submesoscale coastal submesoscale processes can be thought of as those with horizontal scales that are smaller than the internal rossby deformation scale however unlike ocean submesoscale processes submesoscale coastal processes are constrained by bathymetric and coastline scales which are typically smaller than the rossby deformation scale therefore coastal submesoscale processes possess horizontal scales that are larger than the depth but smaller than the relevant horizontal bathymetric scale as a result they depend heavily on coastline geometry and might include processes like lateral vertical flow separation headland eddies secondary flows and fronts for an example of the importance of bathymetry in coastal and estuarine processes see ye et al 2018 unlike regional scales for which there is active research on parameterization of submesoscale processes e g pearson et al 2017 mcwilliams 2016 thomas et al 2013 there has been little work on parameterizing such processes in coastal models because they are so site specific instead efforts have focused on resolving these processes with high resolution for example giddings et al 2012 showed that the suntans model could resolve using o 1 m horizontal resolution a front at a convergence zone between two tidal channels in the snohomish river estuary that was measured in situ and with remote sensing giddings this workshop showed that coupled roms swan model results accurately capture frontal behavior of a small river plume front interacting with the surf zone at the mouth of the tijuana river estuary this model employed five nested roms model grids ranging from the regional scale down to the surf zone with a resolution of o 10 m on the finest grid these examples demonstrate the need for extremely high resolution to resolve rather than parameterize so called coastal submesoscale processes while parameterization of coastal submesoscale processes is difficult if not impossible there are many parameterizations of small scale processes in coastal models with scales that are on the order of the depth i e either the bottom or the mixed layer depth or smaller including turbulence coastal models compute the low frequency large scale motions dictated by the reynolds averaged navier stokes rans equations along with the hydrostatic approximation see section 3 2 9 typically turbulence models focus on the vertical turbulent reynolds stress arising from the averaging horizontal reynolds stresses are typically ignored in coastal models given the dominance of horizontal transport compared to horizontal turbulent mixing in most problems of interest e g blumberg and mellor 1987 the remaining vertical turbulent reynolds stress is modeled with a turbulent viscosity hypothesis which assumes the reynolds stress is a product of a turbulent eddy viscosity and the mean vertical shear pope 2000 most parameterizations of the turbulent eddy viscosity in coastal modeling assume that it is a product of turbulent length and velocity scales that are inferred from two equation turbulence closure schemes for a review see umlauf and burchard 2005 the first of these equations is an evolution equation for the turbulent kinetic energy tke from which the turbulent velocity scale can be extracted and the second equation is needed to compute the turbulent length scale examples include the mellor yamada level 2 5 scheme mellor and yamada 1982 the k epsilon model jones and launder 1972 launder and sharma 1974 rodi 1984 and the k omega model originally proposed by saffman 1970 and extended to oceanic applications by umlauf et al 2003 although these models have similar tke equations they differ in the implementation of the length scale related equation umlauf and burchard 2003 show that this second equation can be generalized as a generic length scale gls model that exhibits more flexibility than the traditional models in that it performs well when applied to a much broader variety of problems the gls model is written in a form that recovers the traditional models through alteration of the parameters in the governing equation for the generic length scale making it straightforward to compare all commonly used two equation models the gls approach was incorporated into the roms model warner et al 2005 and in the general ocean turbulence model gotm burchard et al 1999 gotm net the standard platform for turbulence parameterizations in coastal modeling a fundamental difficulty related to turbulence modeling for coastal problems concerns the relationship between stratification and turbulence see the review by umlauf and burchard 2005 although parameterization of stratified turbulent mixing remains an active area of research e g gregg et al 2018 monismith et al 2018 most coastal models produce reasonable results with stability functions that damp the turbulence due to stratification in this approach a critical or steady state richardson number is specified below which turbulence grows exponentially and above which turbulence decays exponentially burchard and baumert 1995 theory miles 1961 howard 1961 and experiments rohr et al 1988 have found the steady state richardson number to be around 0 25 a lower steady state richardson number requires stronger shear to incur vertical mixing and hence will produce a more strongly stratified environment although the steady state richardson number is predicted from theory it can be tuned to account for modeling errors like numerical mixing as discussed in section 3 2 5 3 1 2 bottom drag other than the turbulence model the most common parameterization of small scale processes in coastal models is that related to bottom drag most models compute a bottom stress that is dictated by a prescribed bottom roughness and the assumption that the horizontal velocity in the first grid cell above the bed satisfies a logarithmic velocity profile although this assumes that the bottom most cell is within the log law region in practice it is often relaxed most notably when calculating the drag coefficient for two dimensional depth averaged models which assume a log law throughout the water column as an example the drag coefficient for the external or barotropic mode in getm assumes a log law velocity profile at mid depth burchard and bolding 2002 the bottom roughness can be parameterized as a function of the grain size distribution and less commonly the presence of bedforms while bottom drag in steady flat rough boundary layers can be accurately parameterized if the median grain size is known there are few parameterizations for the bottom roughness in the presence of bedforms the most common being the wave dominated parameterization of wiberg and harris 1994 that is implemented in the wave current and sediment transport component described in warner et al 2008 of the coawst model there are few if any coastal models that employ parameterizations for bedforms in steady flows although there is evidence that the bottom drag coefficient depends on the tidal phase owing to bedform asymmetry fong et al 2009 bottom roughness parameterizations in wave models are similar to those in circulation models in that the wave friction factor is a function of the properties of the bed however wave models include dissipation by wave breaking and bottom dissipation due to viscous damping in mud which absorbs wave energy e g komen et al 1994 such models are highly uncertain given the difficulty in predicting the behavior of bottom mud layers in coastal regions models for the bottom drag that include the combined effects of currents and surface waves employ more complicated parameterizations like the theory of grant and madsen 1979 which parameterizes wave effects with an augmented roughness or mellor 2002 in which the waves are accounted for with an augmented shear production the augmented roughness of grant and madsen 1979 is typically further modified based on the effects of sediment induced stratification which acts to reduce near bed turbulence and the effective bottom drag e g glenn and grant 1987 styles and glenn 2000 much work on bottom drag has been done with large eddy simulation les and direct numerical simulation dns with a focus on understanding sediment transport which is highly sensitive to the bottom drag parameterization examples include steady current simulations cantero et al 2009a b and purely wave driven simulations ozdemir et al 2010 yu et al 2013 cheng et al 2015 although there is little les or dns work on wave current flows parameterizations accounting for waves typically augment the mean bottom stress or roughness under the assumption of turbulent wave boundary layers over rough beds while this is common in coastal nearshore environments in estuaries where waves are generally weaker laminar wave boundary layers are possible and can reduce the mean or effective roughness nelson and fringer 2018 3 1 3 vegetation kelp and coral drag bottom drag parameterizations for coastal modeling are often of second order importance when compared to the need for accurate boundary conditions and forcing see section 3 2 3 and in many cases the bottom drag is heavily tuned see section 3 2 5 drag parameterizations can be more important where the impact of larger scale roughness features on flow and waves is significant such as vegetation kelp or corals a recent example of the state of the art in parameterizing vegetation drag is the coupled roms swan flow wave vegetation model of beudin et al 2017 implemented in the coawst model the vegetation model includes three dimensional vegetation drag that extracts momentum from the flow in the region of the water column that is influenced by vegetation through a quadratic drag law nepf 2012 this drag decelerates the flow that is blocked by vegetation while accelerating it above submerged vegetation which in turn acts to locally decrease the effective water column depth the shear layer that develops at the interface between the submerged vegetation and the flow contributes to turbulence and is added as a production term to the tke equation in the gotm model following the approach of uittenbogaard 2003 at the same time fine scale eddies generated by separated flow around vegetation stems extract kinetic energy from the turbulence a process that is modeled with a dissipation term in the tke equation in a similar vein vegetation acts to damp waves with an energy dissipation term in the wave action equation in the swan model following the method described by mendez and losada 2004 the model of beudin et al 2017 also includes the effect of wave streaming observed by luhar et al 2010 and luhar and nepf 2013 in which a force is added to the horizontal momentum equations in the roms model to account for the contribution of wave induced mean flow within the vegetation like streaming in the wave boundary layer without vegetation further complicating the dynamics is the bending of submerged vegetation such as seagrass which leads to a reduction in the drag coefficient with increased flow strength an effect that is described by luhar and nepf 2011 such complexities are accentuated when parameterizing drag coefficients for flow through kelp for which the drag coefficient varies with the tidal cycle and seasonal changes in kelp density rosman et al 2010 wang et al 2018 coral reefs represent an added modeling challenge given the dominance of waves and wave breaking in those environments monismith 2007 models of circulation in coral reef environments can incorporate detailed spatial variability of the effective roughness derived from remote sensing and in situ measurements such as the coawst model of the palmyra atoll by rogers et al 2017 ultimately although there are numerous parameterizations for vegetation kelp and coral reef effects most parameterizations are based on idealized laboratory experiments with the drag elements represented by simplified arrays of rigid columns e g lowe et al 2005a b model seagrass blades e g zeller et al 2014 or model kelp rosman et al 2013 some studies focus on flow around the skeletal structure of real coral experimentally e g reidenbach et al 2006 or numerically e g chang et al 2014 nevertheless there are no parameterizations that account for the spatially heterogeneous nature of real vegetation kelp or corals such as cross sectional geometry drag coefficient density height area density young s modulus etc although such parameterizations are badly needed the primary difficulty of implementing them in coastal models is related to accurately measuring the distribution of such properties in the field 3 1 4 sediment transport modeling like modeling vegetation induced impacts sediment transport modeling is limited in large part by a lack of knowledge of the spatio temporal distribution of sediment properties in coastal environments examples of the current state of the art in sediment transport modeling for coastal problems can be found in the delft3d flow delft3d fm oss deltares nl web deflt3d home and coawst warner et al 2008 models in these models it is assumed that the suspended sediment can be treated as an eulerian concentration field because the grain sizes and flow regimes ensure that the sediment grains effectively follow the flow i e they possess a small stokes number which is a ratio of the particle relaxation time scale to the fine scale turbulent shearing time scale and the concentration is small enough less than roughly 1 g l 1 to ignore interactions between sediment grains balachandar and eaton 2010 this allows coastal models to use existing momentum and or scalar transport schemes to transport sediment with the addition of a term to account for gravitational settling to represent transport of a particle size distribution psd most models transport three or more size classes each with the theoretical settling velocity for that grain size examples of models with multiple size class distributions are the mekong river two size class sediment transport study of xue et al 2012 using coawst the skagit river tidal flats three dimensional model of ralston et al 2013 which employed three size classes fine sand silt and fine silt using fvcom the san francisco bay sediment transport model of bever and macwilliams 2013 using untrim which accounted for four size classes silt flocculated clay and silt sand and gravel and the seine estuary sediment transport model of grasso et al 2018 using mars3d lazure and dumas 2008 which accounted for five size classes gravel 3 sand sizes and one mud size class the choice of a limited number of size classes implies a coarse representation of the actual psds therefore the settling velocities are based on representative grain sizes and are largely tunable the settling velocity is particularly important in estuarine environments which possess fine grained sediments silts and clays with the propensity to flocculate or aggregate due to cohesive forces arising from salinity or biological effects flocculation is in turn countered by breakup in the presence of turbulent shear implying that the psd cannot be specified a priori because it evolves in time the most common approach to account for these effects is to parameterize the average settling velocity as a function of the flow sediment and turbulence properties as reviewed by soulsby et al 2013 as examples mengual et al 2017 and grasso et al 2018 used the mars3d hydrodynamic model lazure and dumas 2008 and parameterized the settling velocity as a function of the suspended sediment concentration ssc and turbulent shear rate using the formula of van leussen 1994 the next level of complexity is to explicitly simulate the evolution of the average floc diameter through parameterizations that account for the effects of concentration and turbulent shear on the flocculation and breakup processes e g winterwerp et al 2006 the average settling velocity is then computed with knowledge of the average floc diameter and assumptions about the floc density using fractal theory the disadvantage of this approach is that it does not account for the existence of a psd and its variation in time due to flocculation and breakup this can be accounted for by exchanging mass between different size classes or flocs with the population balance approach lick et al 1992 sterling et al 2005 wherein smaller size classes can interact flocculate and lose mass to larger size classes and larger size classes can lose mass to smaller size classes through turbulent breakup the primary advantage of the population balance approach is that it is based on first principles although it requires numerous parameterizations with many coefficients to model the aggregation and breakup interaction dynamics between the different size classes a good example is the flocmod model of verney et al 2011 that was incorporated into coawst by sherwood et al 2018 although this model shows great promise population balance models remain in their infancy owing to the need for extensive calibration of the many unknown parameters in addition to the difficulty of modeling the physical processes flocculation also depends critically on biological material in the water column which can promote aggregation kranck and milligan 1980 mietta et al 2009 however no coastal models explicitly couple biological models to sediment transport models to account for this interestingly such coupling is inherently two way given that the biology is modified by light availability which is a strong function of the ssc cloern 1987 because of the difficulty in parameterizing flocculation some coastal models ignore flocculation parameterizations and assume static floc sizes with behavior that is essentially tuned to match observations for example chou et al 2018 showed that the ssc in south san francisco bay could be reproduced reasonably well with a sediment transport model in suntans after tuning the relative erosion rates of two size classes which were referred to as microflocs and macroflocs an additional complication of sediment transport modeling is the erosion of sediment from the bed which is typically parameterized empirically with a power law as a function of the ratio of the bottom stress τ b to the critical bottom stress below which no erosion is expected to occur τ c sanford and maa 2001 pointed out that there are many variants of these empirical expressions and all appear to behave similarly in practice the most common form is e m τ b τ c 1 n winterwerp and van kesteren 2004 where models focusing on cohesive sediments set n 1 e g warner et al 2008 bever and macwilliams 2013 chou et al 2018 and models incorporating both cohesive and non cohesive sediments use n 1 for the cohesive sediments and n 1 5 for the non cohesive sediments or sands e g mengual et al 2017 van kessel et al 2011 delft3d flow 2019 uses the erosion formula with n 1 for cohesive sediments and a reference concentration approach van rijn 1993 for erosion of non cohesives the bottom stress τ b is typically obtained with parameterizations based on the bottom roughness and wave properties with the additional complication that the near bed sediment induced stratification can reduce the bottom stress see section 3 1 2 the variability with depth in the bed is accounted for by incorporating multiple sediment layers with varying critical stresses τ c erosion rates m and other sediment properties such as mud and sand mud mixtures e g warner et al 2008 sherwood et al 2018 delft3d flow 2019 it is often the case that just two layers are sufficient to account for the existence of an easily erodible top fluff layer lick 2009 composed of fine muddy sediments and a more consolidated sandy lower layer that is less erodible e g van kessel et al 2011 delft3d flow 2019 has a separate mud module that computes the momentum conservation equations in the mud layer and mass conservation is governed by the horizontal transport of mud consolidation and entrainment and deposition to from the flow the critical stresses for erosion τ c and the erosion rates m are often obtained from core samples in laboratory settings such as sedflume mcneil et al 1996 erosion rates and critical shear stresses respectively decrease and increase with time owing to consolidation of the bed in fine grained muddy environments these effects that can be accounted for with empirical approaches such as the model of sanford 2008 that was incorporated into coawst sherwood et al 2018 while suspended sediment transport modeling has its limitations accurate bed load transport modeling is even more limited by inaccurate parameterizations and a lack of knowledge of bed properties particularly in fine grained or muddy estuarine environments bed load is better defined in sandy environments because movement of sand grains under steady flow can be parameterized with models like the meyer peter and müeller formula 1948 wherein the bed load transport rate is given by a power law as a function of τ b τ c much like the parameterization for erosion the formula by soulsby and damgaard 2005 computes the time averaged bed load transport in sandy beds due to wave current flows and accounts for misalignment between waves and currents these bed load transport formulas require calculation of bottom stresses due to wave current flows as described in section 3 1 2 several parameterizations account for modifications in bed load transport due to bed slope the critical stress for erosion can be increased with increasing slope to effectively decrease the bed load transport in the upslope direction whitehouse and hardisty 1988 alternatively the bed load transport can be directly modified as a function of the bed slope to yield similar behavior lesser et al 2004 morphodynamic evolution of the bed is dictated by both suspended and bed load transport through the exner sediment mass balance equation in which the bed height evolves due to deposition and erosion of suspended load transport and divergence of the bed load transport implementations of the exner equation require a smoothing or diffusion term which is typically derived with the avalanching approach for which a bed load flux causes a decrease in bed slope if it exceeds the local angle of repose e g chou and fringer 2010 guerin et al 2016 without this term grid scale oscillations appear in the bed height given that the exner equation otherwise has no mechanism to smooth out such oscillations the most difficult aspect of morphodynamics modeling is that the bed evolves over time scales that are much longer than typical time scales in coastal models therefore to reduce the computational cost associated with the hydrodynamics most morphodynamics studies are run in two dimensions with depth averaged models e g van der wegen and roelvink 2008 while computationally less expensive two dimensional models do not capture subtidal estuarine dynamics which are largely baroclinically driven only recently have three dimensional morphodynamics studies been implemented to assess the role of density driven currents olabarrieta et al 2018 to study long term morphodynamics over decades or even centuries it is common to employ a morphological scale factor roelvink 2006 in which the bed evolution is multiplied by a factor during each time step to accelerate its motion relative to that of the flow olabarrieta et al used a factor of 50 van der wegen and roelvink used 400 owing to the potential for extensive erosion over long morphological time scales an added difficulty of long term morphological modeling is its dependence on sediment properties deep within the bed while these can be measured with core samples core sampling can be extremely expensive and may not provide adequate horizontal spatial resolution a more extensive discussion of different bed load transport and morphodynamics models can be found in the user manual for the delft3d family of models delft3d flow 2019 3 1 5 wave modeling a full description of the state of the art and recommendations for future research in wave and nearshore modeling would warrant a workshop in and of itself therefore here we discuss features of wave modeling that are most relevant for larger scale i e larger than nearshore scales coastal circulation modeling in the context of coupling of wave models to three dimensional circulation models a review of nearshore wave modeling is provided by kirby 2017 surface gravity wave time and length scales are too small to resolve in coastal models instead the waves are modeled with the conservation of wave action equation which governs the evolution of the wave energy spectrum due to wind input wave wave interactions and breaking these models can accurately capture refraction by bathymetry and currents although diffraction is extremely difficult to capture and hence associated parameterizations are not very reliable most coastal models include the effects of waves using the wave action approach the most popular being wavewatch iii tolman 2009 and swan booij et al 1999 which are coupled to currents in the coawst and delft3d flow delft3d fm models models with their own approaches to solving the wave action equation are similar to the swan approach such as the unstructured grid wave models in fvcom qi et al 2009 suntans chou et al 2015 or schism roland et al 2012 solution of the wave action equation is computationally costly given that the directional spectrum is typically resolved with roughly 30 angles and 30 frequencies thus incurring o 1000 additional two dimensional transport equations to compute transport of wave action by the group velocity and currents in many cases because the wave spectrum can evolve more slowly than the currents this computational cost can be reduced by computing the waves less often than the currents there are two approaches to coupling the time averaged effect of waves to the currents the first is the radiation stress formalism in which the waves drive currents with the divergence of the excess wave momentum flux and has been the most common approach e g warner et al 2008 2010 kumar et al 2011 the second and recently more popular approach is the vortex force formalism in which the advective term in the horizontal momentum equations is written in terms of the divergence of the kinetic energy and a vortex force mcwilliams et al 2004 bennis et al 2011 and has been implemented in the structured grid roms uchiyama et al 2010 and coawst kumar et al 2012 models and the unstructured grid schism model guerin et al 2018 this approach has the advantage that it naturally decomposes the wave force into conservative and non conservative parts which gives better results in the presence of wave breaking particularly in the nearshore of the many active areas of research in wave modeling parameterizing the effects of transient rip currents by nearshore wave breaking is an important component of coastal modeling such currents are critical to accurately representing cross shore transport past the breaker zone an important mechanism for transport of tracers from small discharge streams giddings this workshop rip currents are not resolved in coastal models because wave models do not resolve the vertical vorticity arising from finite crest length breaking to incorporate these effects into roms in the coawst model kumar and feddersen 2016 2017 directly computed the vertical vorticity with the funwavec model feddersen et al 2011 a two dimensional boussinesq wave model that resolves finite crest length breaking the resulting vertical vorticity can be directly computed in funwavec and added as a source term to the roms model which then produces transient rip currents while this approach is costly because it requires computation of waves with funwavec the ultimate objective is to develop parameterizations for these effects that can be incorporated into the circulation model at a fraction of the computational cost coupling of winds and waves is a critical component of coastal wave modeling under weak to moderate wind settings wave models can accurately reproduce wind wave generation given the relatively accurate parameterizations of equilibrium and depth or fetch limited wave spectra however accurate wave modeling is elusive under extreme conditions particularly in storms as can be expected modeling of such extreme events is highly dependent on accurate modeling of the wind field by the overlying atmospheric model and requires dynamic coupling of ocean atmosphere and wave fields warner et al 2010 olabarrieta et al 2012 hegermiller this workshop points out that wave modeling is also limited by inaccurate parameterizations of wave breaking and wave current interactions under strong wind conditions ardhuin et al 2010 inaccuracies in wave models under extreme conditions are accentuated by feedback into the atmospheric and ocean models to which they are coupled because of errors in predictions of air sea fluxes under strong wave conditions including breaking zambon et al 2014a allahdadi et al 2019 3 2 numerical methods and modeling frameworks 3 2 1 a unified modeling framework the sense that emerged from the workshop was that the existence of multiple on going approaches to coastal and estuarine modeling is due to some basic challenges related to the coastal and estuarine parameter space in contrast to the more unified modeling framework like that seen in the regional modeling community i e roms coastal and estuarine model applications are highly dependent on resolving bathymetric coastline and forcing variability features that can be highly site specific as a result the community felt that it is important to ensure model diversity see table 1 to encourage application and testing of a wide variety of methods to understand resolution requirements related to the site specific parameters despite the aversion to a unified modeling framework there is a clear need for a common framework for model setup and analysis to reduce barriers for new users and facilitate more direct comparisons between approaches such a framework is sorely needed across all model classes including coupled and uncoupled and structured and unstructured grids the community felt there is a significant lack of standards for model coupling despite the wealth of coupled models similarly a more unified approach to model inputs including grid generation and boundary forcing files would greatly reduce the overhead and expertise required to apply a new model to a particular problem in addition to simplifying the process of model implementation and analysis a unified approach would significantly improve the ability to compare models model intercomparison would be encouraged because differences in model results depend critically on grid quality and accuracy of initial conditions forcing and boundary conditions features that are typically not highlighted in the peer reviewed literature as much as parameterizations and numerical methods development of a unified framework would be a daunting task given the extensive variety with which users implement models including compilers and operating systems data formats e g binary netcdf etc and the scripts that are employed for model setup and analysis e g bash python matlab etc this variety is further complicated by the need to update software to ensure compatibility with continuous advances in software engineering tools 3 2 2 a standard model test bed despite the importance of model diversity there are few if any studies that systematically compare the accuracy and efficiency of different models to gain insight into their advantages and disadvantages this is largely a result of the lack of a set of agreed upon benchmarks or test cases that can be applied to assess model performance it is standard practice to demonstrate model accuracy and efficiency through simplified test cases as examples the thacker test case is standard for wetting and drying e g casulli 2009 the lock exchange is standard for nonhydrostatic models e g fringer et al 2006 and the channel flow wind driven mixed layer and simplified estuary are standard cases for turbulence models warner et al 2005 although simplified test cases abound they test model performance in regimes that are expected to give smooth or converged results and so do not demonstrate model performance in scenarios that might be found in real problems simplified test cases also do not test model performance related to uncertainties in initial and boundary conditions the lack of benchmarks for real problems is likely a result of the subjectivity related to choosing parameters for model setup however even if a model setup is consistent between two model implementations there is substantial subjectivity in devising validation metrics to compare predictions to observations for example the skill score of murphy 1988 is common in coastal modeling and normalizes the difference between model results and observations by a measure of the difference between the observations and a reference model which is often taken as the mean of the observations or climatological values while this is a reasonable metric since it implies that a model achieves a better skill score if the difference between the observations and reference model is greater hetland this workshop showed that there is subjectivity in defining the reference model as it can require a time averaged or low passed signal about which the variance is defined as a result the skill score can vary significantly depending on how the error is normalized even if the absolute difference between the predictions and observations remains unchanged 3 2 3 higher order accuracy as they are currently implemented coastal ocean models do not always take full advantage of higher order accuracy in this context model accuracy is defined as the rate at which the error decreases with respect to spatial or temporal refinement for example a second order accurate model is one in which the error decreases quadratically with respect to grid refinement here the error can also be defined as the difference between the solution on one grid and the solution on a refined grid it is important to note that deterministic chaos may prevent convergence with respect to spatial or temporal refinement particularly when resolving horizontal spatial scales that are finer than the internal rossby deformation scale therefore strictly speaking one would need to conduct grid refinement studies of ensemble average simulations this would be prohibitively expensive from a computational point of view and so refinement studies can realistically only be conducted on deterministic problems nevertheless an advantage of using accuracy to gauge model fidelity is that in principle it does not require observations or truth because model accuracy is a test of whether the discrete equations converge to the exact governing partial differential equations therefore although one model may be more accurate than another because it has more advanced numerical discretization techniques this usually does not imply better agreement with observations owing to the dominance of errors related to forcing and boundary conditions furthermore model accuracy is only assured when the spatial scales over which the solution varies are at least one order of magnitude larger than the grid spacing satisfying such a constraint requires grid resolutions and problem sizes that are beyond the reaches of existing computational resources therefore most coastal applications are run with grid resolutions that allow grid scale variability resulting in grid dependent solutions that are generally not expected to improve with grid refinement without calibration of tunable parameters although the advantages of higher order methods have not yet been exhibited for coastal problems it is essential to employ at least second order accurate flux limiting schemes both for finite volume and discontinuous galerkin methods for scalar transport first order methods exhibit excessive numerical diffusion and cannot accurately predict physical processes with sharp horizontal gradients such as gravitational circulation river plumes or other frontal processes despite their importance for scalar transport flux limiting schemes are not as important for momentum or continuity overall it was agreed that although models can vary widely in the discretization schemes no model behaves as a second order accurate model in practice because of the overriding uncertainties from the bathymetry and forcing at best spatial and temporal accuracy of coastal models is somewhere between first and second order and in many cases spatio temporal resolution is limited by the resolution of the boundary conditions and forcing and available observations for validation most importantly high resolution coastal models cannot be accurate without accompanying high resolution accurate bathymetry furthermore accurate forcing is needed to accurately model the effects of such forcing for example the effects of the spatial variability of wind on estuarine circulation would not be possible without measurements of winds at many stations surrounding the estuary or accurate winds from an atmospheric model with sufficient resolution to resolve the spatial variability similarly an accurate model also needs more detailed measurements for validation for example validation of flooding with an accurate coastal storm surge model is not possible without accurate water level records during strong storm events particularly in regions that are normally dry 3 2 4 finite element vs finite volume methods both the finite element and finite volume method exhibit the potential for grid scale oscillations depending on mesh geometry and placement of discrete variables le roux et al 2007 korn and danilov 2017 while the spatial accuracy of both methods is sensitive to mesh quality in general the finite element method is better suited to the development of higher order spatial discretizations that are less sensitive to the grid the spatial accuracy of the finite volume method is typically restricted to first order on general unstructured meshes while second order or higher accuracy can only be achieved on cartesian or smoothly varying curvilinear grids spatial accuracy of finite volume methods can further degrade to less than first order on highly skewed meshes although the finite element method is more amenable to higher order spatial discretization schemes the finite volume method is much more common in coastal modeling see table 1 the finite volume method is more straightforward to implement and generally more computationally cost effective because the finite element method requires evaluation of costly numerical integrals and inversions of linear systems regardless of the time stepping scheme implicit time stepping schemes require inversions of linear systems on both grid types the finite volume method also has the advantage that it can guarantee local conservation of mass momentum and energy which is particularly attractive when enforcing monotonicity in scalar transport schemes finite element methods in contrast generally ensure global rather than local conservation although the discontinuous galerkin formulation can ensure local conservation some models such as schism employ the finite element method to discretize the momentum equations and the finite volume method to ensure mass conservation in scalar transport overall the community agreed that there is no clear advantage of finite volume vs finite element methods in existing popular coastal models given that it is less susceptible to grid quality the finite element method is superior in the implementation of adaptive mesh refinement amr as an example since the discretization error in the discontinuous galerkin method is related to jumps at element interfaces ainsworth 2004 this error naturally serves as a metric for local mesh refinement where and when it is needed bernard et al 2007 mesh refinement via the addition or removal of elements is referred to as h adaptivity while refinement via movement of element nodes without changing the number of elements is referred to as r adaptivity while both finite element and finite volume models can employ h or r adaptivity only finite element methods can employ p adaptivity wherein the order of accuracy of the discretization is varied in time and space without altering the mesh examples of ocean models that employ amr are the imperial college ocean model icom ford et al 2004 pain et al 2005 piggott et al 2005 and the nonhydrostatic unified model of the ocean numo which is currently under development and based on the nonhydrostatic unified model of the atmosphere numa giraldo and restelli 2008 giraldo et al 2010 although the amr approach is very powerful it is not yet common for coastal modeling because mesh resolution is often known a priori for most problems as it is dictated by bathymetry and coastlines ye et al 2018 however there is significant potential for application of amr to coastal flooding problems in which large portions of the domain that are normally dry can be active during flooding even without amr the performance of parallel storm surge computations can be optimized with load balancing strategies that account for the large variability of active cells during flooding roberts et al 2019 an important consideration related to high resolution modeling of coastal flooding with amr or high resolution grids is the need for accurate bathymetry vegetation and land use data in areas that are normally dry 3 2 5 tuning to account for unresolved processes and model error it is well established that coastal models must be tuned to account for unresolved processes and numerical errors perhaps the most ubiquitous of the many subjective parameter choices in coastal models is the bottom roughness which dictates the bottom drag imposed on momentum and waves and the production of turbulence by bottom shear see section 3 1 2 the distribution of bottom roughness is not known either because the physical parameters are not known or measured or because of inaccurate or nonexistent parameterizations the result is that most implementations require ad hoc tuning of the drag parameters particularly the bottom roughness to improve model skill such tuning not only accounts for unresolved physical processes impacting the drag but it also accounts for numerical errors as an example tuning of bottom roughness to account for numerical damping can lead to bottom roughness that is much smaller than the expected physical value since the numerical damping can overwhelm the physical damping the optimal bottom roughness also depends on the grid resolution due to its effect on the numerical damping and on differences between the resolved and unresolved scales e g ralston et al 2017 owing to the difficulty of predicting the distribution of vertical turbulent mixing as discussed in section 3 1 1 it often requires tuning of several parameters as an example ralston et al 2017 simulated the circulation in a salt wedge estuary with different bottom roughness coefficients grid resolutions and steady state richardson numbers they were able to tune the bottom roughness to give the best skill scores for water levels and depth averaged currents however a steady state richardson number of 0 1 which is smaller than the predicted value of 0 25 see section 3 1 1 was needed to give good model skill for baroclinic features in frontal regions to compensate for excess numerical mixing because the excess numerical mixing decreased with grid refinement the required steady state richardson number increased although it was estimated that a prohibitively high horizontal grid resolution of about 5 m would be needed to require the theoretically correct value of 0 25 these results suggest that the steady state richardson number is not a tuning parameter in the classical sense unlike the bottom roughness parameter there exists a grid resolution at which it is no longer justified to tune the steady state richardson number in a rans modeling framework because in principle the mean shear and stratification can be resolved a steady state richardson number parameterization is not needed for les see section 3 2 10 tuning of the bottom roughness or other parameters like the steady state richardson number to produce higher skill scores accounts for errors in both the parameterizations and numerical methods for scalar transport the principal error is numerical diffusion which can incur mixing that is larger than the physical mixing although higher order methods help to reduce spurious numerical diffusion in practice models exhibit near first order error in the presence of sharp fronts or around grid scale bathymetric variability the significant role that numerical diffusion plays in the distribution of mixing throughout an estuary makes it difficult to assess turbulence model performance because it is difficult to compare the vertical mixing computed by the model to the observed turbulent mixing recently finite volume methods have been developed to compute the spatio temporal distribution of numerical mixing burchard and rennau 2008 klingbeil et al 2014 which allows for a direct quantification of the amount of numerical relative to physical mixing the methods are non invasive in that they can quantify numerical mixing with minimal code alteration and can be applied for structured and unstructured grids in a finite volume framework results from several estuarine models at the workshop showed that a significant fraction of the mixing was numerical with the numerical mixing being greater than the physical mixing in some cases numerical diffusion in a model is highly dependent on the grid resolution numerical schemes being used and strength of material property gradients e g bathymetry velocity salinity being simulated although the notion of reducing the numerical mixing to a level that is smaller than the physical mixing poses a daunting challenge for coastal models in terms of resolution requirements such a challenge would be impossible without the ability to quantify the numerical mixing using methods like those developed by burchard and rennau 2008 and klingbeil et al 2014 3 2 6 grid generation and placement of variables the most important yet underappreciated aspect of coastal modeling is development of the computational grids grid generation is in some ways more difficult for structured curvilinear grids given the need for smoothness in curvilinear grids there are many grid generation tools for both unstructured and structured models e g gmsh gmsh info geuzaine and remacle 2009 sms aquaveo org janet smileconsult de yet there is no clear advantage of one over the other because grid generation continues to be a largely manual or tunable process grid resolution fundamentally dictates the accuracy and efficiency of the results yet grids can never fully resolve the complexity of the bathymetry and coastline in coastal problems in general unstructured grids are better at resolving complex bathymetric features than curvilinear or cartesian grids and unstructured grids can also more efficiently resolve multiscale features due to the flexibility of grid orientation and telescoping a good example of the advantage of unstructured meshes is the finite element model of the great barrier reef using slim second generation louvain la neuve iceocean model vallaeys et al 2018 which resolved the flow features throughout the detailed reef system with an extremely complex high resolution mesh legrand et al 2006 lambrechts et al 2008 to avoid significant grid stretching when using structured grids grid nesting must be used wherein grids with successively finer resolution are nested within one other e g roms agrif romsagrif gforge inria fr nesting enables use of smaller time step sizes on the finer grids thus reducing the number of time steps and the associated computational cost on the coarse grids it is possible to employ smaller time step sizes where cells are finer on single grids with the multirate approach as applied to the slim model seny et al 2013 however this method is difficult to implement and is not common in coastal ocean models although some effort has been made recently to more objectively construct grids and to ensure reproducibility candy and pietrzak 2018 generation of structured or unstructured grids is not fully automated because both require subjective decisions related to manual grid alteration in regions with degraded grid quality an additional advantage of unstructured grids is that grid edges are constrained to follow a specified coastline and so no masking is required to eliminate inactive cells over land from the computation this has the advantage that no memory or computational effort is wasted on masked cells more importantly however the process of grid masking is typically a manual process since it requires decisions about which grid cells will account for unresolved features such as narrow channels or headlands while unstructured grids do not need grid masking grid quality still typically degrades around complex bathymetric features since grids must be highly skewed when constrained by sharp coastline angles or grid scale bathymetric features either way both structured and unstructured grids require manual intervention in which grid nodes are moved to improve grid quality at the expense of poorer coastline resolution although finite element methods are more forgiving as discussed below the need for masking can be eliminated with use of subgrid bathymetry casulli 2009 a method that employs bathymetric resolution that is finer than the grid to ensure that the cell geometry follows the bathymetry without constraining the grid edges to follow coastlines see section 3 2 7 in general finite element methods are less sensitive to grid quality and so most finite element models can be run with little to no grid tuning finite volume methods can be highly sensitive to grid quality depending on arrangement of the variables on the grid on staggered or c grids arakawa and lamb 1977 pressure gradients are defined as normal to grid edges and are computed as the difference between the pressures at cell centers on either side of the edges therefore c grids must be orthogonal so that the lines connecting cell centers or voronoi edges are perpendicular to lines connecting cell vertices or delaunay edges generation of high quality orthogonal grids is extremely difficult when constrained by complex coastlines this can be alleviated with hybrid unstructured grids that can employ arbitrary sided cells thus allowing resolution of channelized features with quadrilateral grid cells and connecting the quadrilateral regions with triangles or other polygons e g macwilliams et al 2016 ye et al 2018 however there are no automated grid generation tools that employ both triangles and quadrilaterals other grid arrangements such as a all variables are collocated at cell centers or b velocity components stored at cell vertices grids alleviate the orthogonality constraint although no popular coastal models employ a grids because they tend to exhibit grid scale noise due to decoupling between the pressure and velocity danilov 2013 the fvcom model employs b grids and is one of the most robust coastal models regarding susceptibility to grid quality while the schism model avoids grid quality issues associated with c grids by employing finite element methods for the momentum equations owing to the problem of grid scale noise on a grids except for fvcom all popular regional or coastal models employ c grids roms being the most obvious example see table 1 interestingly weak grid scale noise is also a feature of unstructured triangular c grids korn and danilov 2017 and it can be amplified by poor grid quality wolfram and fringer 2013 however such noise is manifested in weakly dissipative settings over time scales that are much longer than those typically employed in coastal models danilov 2013 hexagonal c grids are a viable alternative that eliminate the noise although they can be difficult to generate in complex coastal geometries and have been better suited to global ocean modeling ringler et al 2010 3 2 7 subgrid bathymetry recent advances in bathymetric surveying have enabled extremely high resolution bathymetry at sub meter resolutions for entire coastal regions for example a digital elevation map is available for the entire country of the netherlands at a horizontal resolution of 0 5 m http www ahn nl such high resolution datasets provide bathymetry that will continue to be substantially higher than typical grid resolutions of coastal models for the foreseeable future with bathymetric resolution that is higher than the grid resolution typical coastal models subsample the bathymetry data through averaging to assign model depths at cell centers or edges while ensuring the same water volume relative to some datum on the subsampled bathymetry e g ye et al 2018 subsampling eliminates information about the high resolution or subgrid bathymetry that can be used to inform a more accurate simulation in the two dimensional subgrid bathymetry method of casulli 2009 adapted to three dimensions by casulli and stelling 2010 the finite volume framework uses the bathymetry data within a grid cell to obtain a more accurate representation of the cell geometry such as volume surface area and cross sectional area of cell faces as a result the cell geometry is independent of the computational grid resolution since it is only a function of the bathymetric resolution combined with more accurate volume and mass fluxes in most cases the subgrid method gives accurate solutions at a reduced computational cost because the computational grid can be coarsened without sacrificing accuracy related to the subgrid representation of the bathymetry as an example macwilliams et al 2016 simulated the san francisco estuary in three dimensions using the untrim model with the subgrid method of casulli 2009 and achieved similar accuracy as a previous high resolution simulation macwilliams et al 2015 with one order of magnitude fewer cells in the horizontal and a decrease in run time by a factor of 40 similarly sehili et al 2014 observed speedup by a factor of 20 using untrim with subgrid bathymetry to simulate the elbe estuary in addition to reduced computational cost the subgrid method also relieves the constraint on grid quality since grid boundaries do not need to be aligned with coastlines because the subgrid bathymetry ensures accurate geometric representation regardless of grid orientation as a result the subgrid method allows a three dimensional model to resolve channels with one grid cell in the cross section effectively providing for seamless transition between three and one dimensional modeling within the same framework this was demonstrated by stelling 2012 and sehili et al 2014 who show that subgrid modeling can resolve channels with one grid cell accurately as long as the cross channel variability in the friction term is appropriately accounted for in a way that gives the correct flow stage relationship an additional advantage of the subgrid bathymetry method is that it eliminates the stability restriction associated with wetting and drying thus further increasing computational efficiency by allowing larger time step sizes casulli 2009 while subgrid bathymetry can reduce the computational cost of computing flow and stage in channelized networks subgrid bathymetry may not necessarily give more accurate results for features with strong horizontal variability such as salt wedges fronts or flow features arising from strong cross channel bathymetric variability zhang 2017 accurate computation of these processes requires higher resolution of the base grid 3 2 8 the vertical coordinate system it is often argued that sigma s or in general terrain following vertical coordinates should be employed to accurately resolve along bottom flow in contrast to z levels or cartesian vertical coordinates that represent bottom topography with stair steps the stair steps give rise to grid scale variability in flow variables which can induce spurious numerical mixing legg et al 2006 and also lead to discontinuities in the bed shear stress platzek et al 2014 which can be particularly problematic for sediment transport another problem with z levels arises in the presence of large changes in water level due to strong tides which requires that the free surface cross over grid lines if sufficient vertical resolution is desired throughout the tidal cycle in shallow areas otherwise shallow areas might be resolved with just one grid cell in the vertical while deeper areas are resolved with many more despite these problems unlike sigma coordinates z coordinates do not exhibit the well known pressure gradient error particularly in the presence of steep bathymetry shchepetkin and mcwilliams 2003 the pressure gradient error is more problematic in coastal problems containing the shelf break seamounts or canyons it is typically less of a concern in estuaries where tidal currents are generally stronger than currents induced by the pressure gradient error and horizontal pressure gradients are weaker owing to stronger mixing and shallower water most popular coastal models employ sigma coordinates see table 1 except for untrim and suntans which employ z coordinates see table 1 the stair stepped nature of z levels can be reduced with partial stepping whereby the bottom face of the bottom most cell coincides with the bed or shaved cells adcroft et al 1997 in which the numerical discretization is rewritten about finite volume cells that are cut by the bathymetry so that one of the faces is coincident with the bed discontinuities in the bottom shear stress when using z levels can be avoided with remapping in which the velocity is remapped onto a terrain following grid at each time step to produce a continuous along slope velocity field and bottom stress distribution platzek et al 2014 the subgrid bathymetry method see section 3 2 7 can also be applied to improve the representation of the bottom bathymetry when using z levels practically speaking terrain following coordinates are more straightforward from a coding perspective because the number of active layers in the vertical is constant in time and space although the apparent advantages of each type of vertical coordinate are readily exhibited with idealized test cases there is no clear winner when applied to real coastal problems this is likely a result of other errors that make it difficult to quantitatively compare numerical discretization errors as discussed in section 3 2 3 nevertheless some models have shown great promise in application of hybrid vertical coordinates such as selfe which employs terrain following coordinates in the upper layers of the water column and z levels at depth zhang and baptista 2008 or slim which can employ a combination of fixed or adaptive z and generalized sigma coordinates delandmeter et al 2015 2018 vallaeys et al 2018 hybrid vertical coordinate models are advantageous in estuaries with broad shoals and gradual bathymetry in the shallows which is appropriate for sigma coordinates while z levels are employed in deeper regions where slopes may be steeper thus avoiding pressure gradient errors to reduce pressure gradient errors while retaining the advantages of both z and terrain following coordinates the schism model zhang et al 2015 employs localized sigma coordinates with shaved cells lsc2 wherein the slope of the sigma coordinates in the presence of steep slopes is reduced by adding more vertical layers near the bed despite the benefits of hybrid vertical coordinates all vertical coordinate approaches in popular coastal models exhibit spurious vertical numerical diffusion of scalars although numerical diffusion is reduced with lsc2 vertical coordinates the optimum approach is to employ arbitrary lagrangian eulerian vertical coordinates ale adcroft and hallberg 2006 in the ale approach the vertical coordinate moves with the flow via lagrangian trajectories followed by a correction to prevent grid distortion e g burchard and beckers 2004 hofmeister et al 2010 delandmeter et al 2018 in this way the vertical coordinate can naturally follow one or a combination of z levels s levels or isopycnal coordinates isopycnal coordinates by definition eliminate vertical numerical diffusion of scalars because there is no transport of scalars across isopycnal coordinate lines and hence no discrete vertical advection isopycnal coordinates are not well suited to coastal simulations given that there can be significant physical mixing and isopycnals are vertical at fronts however ale coordinates can be moved adaptively r adaptivity see section 3 2 4 to concentrate vertical coordinates in regions with strong stratification thus encouraging representation of sharper vertical density gradients quantification of spurious numerical diffusion with the techniques of burchard and rennau 2008 and klingbeil et al 2014 shows significant reduction of numerical mixing in coastal problems when using ale vertical coordinates gräwe et al 2015 3 2 9 nonhydrostatic modeling coastal models are typically hydrostatic because most processes of interest have long horizontal scales of motion relative to the vertical scales marshall et al 1997b the nonhydrostatic pressure is important only when considering processes that are short relative to the depth in order of decreasing horizontal scales these include solitary like internal gravity waves fronts and bores surface gravity waves convective overturning kelvin helmholtz like billows flow over short wavelength topography including small scale roughness such as dunes ripples vegetation etc and turbulence resolving such processes in coastal domains is computationally expensive because it requires very high resolution of the order of meters or smaller it is even daunting to resolve horizontal scales associated with internal solitary waves which are likely the largest scales for which the nonhydrostatic pressure is important in this regard the horizontal grid resolution must be smaller than the depth of the mixed layer in order to resolve the internal solitary wave dispersion vitousek and fringer 2011 which requires o 1 m grid resolutions in coastal problems the nonhydrostatic pressure is computed in many coastal and regional ocean models including mitgcm marshall et al 1997a trim casulli 1999 untrim casulli and zanolli 2002 suntans fringer et al 2006 roms kanarska et al 2007 auclair et al 2018 fvcom lai et al 2010a and getm klingbeil and burchard 2013 at smaller scales needed to resolve dispersion related to surface gravity waves several three dimensional nonhydrostatic models have been developed such as swash zijlema et al 2011 and nhwave ma et al 2012 shi et al 2015 in addition to the computational cost associated with the large number of grid cells needed to resolve nonhydrostatic effects computation of the nonhydrostatic pressure is expensive because it requires solution of an elliptic equation which can increase the computational cost of a coastal simulation by more than one order of magnitude fortunately because nonhydrostatic processes occur over short length scales they encompass a small fraction of the energy in most coastal problems and thus computation of the nonhydrostatic pressure may not necessarily show significant improvement of predictions over time scales greater than o 1 hr unresolved nonhydrostatic processes like convective overturning or shear instabilities are represented reasonably well by rans turbulence closure schemes despite the need for tuning as discussed in section 3 2 5 even the propagation speed of for example river plume fronts and short internal gravity waves are reasonably well predicted with hydrostatic models given that these propagate close to the hydrostatic long wave speed only when the details of such processes are of interest is the nonhydrostatic pressure important because of the relatively weak impact of small scale nonhydrostatic processes on large scale flows most nonhydrostatic studies are conducted in idealized domains most of these idealized studies are conducted in small domains so that the nonhydrostatic effects are dominant few studies have resolved small scale physics and their effects on large scale processes with a nonhydrostatic model examples include the study of shi et al 2017 who used the nonhydrostatic nhwave model to simulate the structure of a front near the mouth of the columbia river to resolve the details of fine scale nonhydrostatic features apparent in airborne imagery other field scale nonhydrostatic examples include simulation of nonlinear and nonhydrostatic internal waves in massachusetts bay by lai et al 2010b and sediment resuspension by internal bores in otsuchi bay japan by masunaga et al 2017 despite the realistic scales of these simulations only qualitative comparisons with field observations could be made because of the idealizations 3 2 10 large eddy simulation les in principle a coastal model could directly compute the turbulent scales of motion and eliminate the need for a turbulence model if it were nonhydrostatic since the turbulent scales are nonhydrostatic and the grid resolution was sufficient to resolve the turbulent scales of motion this could be accomplished with a direct numerical simulation dns for which the grid must resolve all of the turbulent scales of motion however dns is not feasible in coastal flows given that the grid spacing must be on the order of the kolmogorov dissipative scale or the batchelor scale if there is scalar transport which implies the need for an unrealistic number of grid points see e g ch 9 pope 2000 the computational cost can be alleviated with a large eddy simulation les in which the energy containing eddies are resolved by the grid and the small or subgrid scale eddies are parameterized with a so called subgrid scale sgs or subfilter scale sfs model pope suggests that 80 of the turbulent kinetic energy should be resolved the degree to which the computational cost is reduced for les when compared to dns depends on the flow of interest near boundaries the computational cost of les is still extremely high because of the need to resolve the small near wall turbulent scales that are proportional to the viscous wall unit ν u where u is the friction velocity to avoid the computational cost of resolving boundary layers the les can simulate the region away from the wall and parameterize the near wall region and the associated stress with so called wall layer modeling piomelli and balaras 2002 avoiding simulation of the near wall region decreases the needed grid resolution roughly by a factor of 10 in each direction leading to substantial savings in computational cost and the ability to simulate higher reynolds numbers piomelli and balaras 2002 as an example chou and fringer 2008 simulated suspended sediment transport in a channel with a reynolds number of 600 000 based on the channel height using les because the near wall physics were not resolved the wall stress was modeled with a quadratic drag law and the near wall vertical turbulent reynolds stress was augmented with the model of chow et al 2005 the augmented stress ensures that the near wall eddies are strong enough to vertically mix momentum to produce the correct mean logarithmic velocity profile an important constraint with this approach is the need for the first grid cell to fall within the lower end of the logarithmic velocity profile implying that the first grid point must be within roughly 10 100 wall units of the boundary piomelli and balaras 2002 this is similar to the requirement for rans based coastal modeling when the velocity in the bottom most grid cell is constrained to match the log law see section 3 1 2 however unlike in rans based coastal modeling in which horizontal grid resolutions are typically o 100 m 1 km the horizontal grid resolution in les is constrained by the need for the grid aspect ratio to be as close to unity as possible otherwise the accuracy will degrade due to numerical errors related to the sgs parameterization scotti et al 1993 and the nonhydrostatic pressure solver fringer et al 2006 santilli and scotti 2011 the accuracy of numerical methods used to discretize the governing equations also dictates the grid resolution in les an effect that is accentuated by high aspect ratio grids the importance of numerical methods in les is discussed by rodi et al 2013 who emphasize that central schemes for momentum advection should be used in les given that upwind biased schemes often produce too much numerical dissipation leading to an incorrect prediction of the turbulent kinetic energy spectrum although the cost of les is generally lower in regions where boundary layers are absent or not important stable stratification can also require high resolution for les since the grid resolution in the presence of stable stratification must resolve the ozmidov scale or the largest scale of turbulence before internal wave motions dominate the ratio of the ozmidov scale to the kolmogorov scale is proportional to r e b 3 4 where r e b ϵ v n 2 is the buoyancy reynolds number with dissipation ϵ and buoyancy frequency n e g smyth and moum 2000 with r e b as small as o 1 in the ocean thermocline ivey et al 2018 the resolution requirement based on the ozmidov scale in stably stratified boundary layers with les can be as limiting as resolving unstratified turbulent boundary layers this is a well known limitation in les of stable atmospheric boundary layers chow et al 2005 if the flow is well mixed i e there is no stratification and boundary layers are not important or can be modeled the grid resolution needed for les is less dependent on the reynolds pope 2000 or buoyancy reynolds numbers and instead is based on the need to resolve the energy containing scales of motion that are dictated by the problem of interest for example langmuir cells in the surface mixed layer can be simulated accurately with les if 10 20 grid points are used to resolve each langmuir cell skyllingstad and denbo 1995 similarly the convective atmospheric boundary layer is simulated accurately using les if the boundary layer is resolved with at least 50 grid points in the vertical sullivan and patton 2011 although wall models can be used to parameterize near wall physics simplified parameterizations like quadratic drag laws that are often used in wall modeling cannot account for the more complex dynamics associated with flow separation or stratification a novel method to reduce the cost of les in the presence of walls while retaining more complex near wall physics is to employ hybrid rans les approaches such as detached eddy simulation des rodi et al 2013 in this approach the near wall region is modeled with a rans approach see section 3 1 1 while the far field region is simulated with les the subgrid scale model in the les uses a similar eddy viscosity parameterization as the rans model except that the length scale in the les model is proportional to the grid resolution as examples in the des simulations of realistic river geometries of constantinescu et al 2011a b les was applied away from the walls to simulate the large scale flow separation and circulation while the turbulence in the hydraulically rough near wall regions was parameterized with the spalart allmaras sa one equation model spalart 2000 which only requires the near wall length scale to parameterize the eddy viscosity as opposed to the two equation models discussed in section 3 1 1 while hybrid rans les approaches and wall models are promising their application to coastal modeling is fundamentally limited by a lack of knowledge of the near wall physics which is ultimately stochastic in nature and not only difficult to model but also extremely difficult to measure therefore much of the uncertainty inherent in the traditional rans approach to coastal modeling discussed in this paper would not be eliminated with les modeling since it would still require tuning of unknown coefficients related to the parameterizations of near wall physics notwithstanding the difficulty with boundary conditions however the primary advantage of les modeling would be a direct calculation of turbulence away from boundaries thus eliminating the need to parameterize highly unsteady or stratified turbulence processes that can require tuning in a rans framework see section 3 2 5 3 3 field observations field observations form an integral component of modeling because they are needed for initialization forcing and validation this is particularly true for data assimilation and operational modeling as pointed out in the summary of the ioos sponsored workshop on operational ocean modeling by wilkin et al 2017 regarding forward and process modeling high resolution field observations are an essential component of understanding of physical processes that in turn leads to the development of improved parameterizations and modeling techniques like models observations are subject to errors and uncertainties and they are frequently modeled from the raw measurements 3 3 1 new instrumentation techniques among the many exciting new instrumentation techniques reid et al 2019 discuss measurements of nonlinear internal wave activity on the donghsa atoll in the south china sea using the distributed temperature sensor dts a 4 km long cable resting on the bed that recorded temperature every minute at a spatial resolution of 2 m while the most obvious benefit of such measurements is the ability to understand the details of high frequency processes related to nonlinear internal waves an additional advantage is that they lead to the development of high resolution simulations needed to understand those processes high resolution simulations would otherwise be difficult if not impossible without the necessary high resolution observations needed for validation davis this workshop presented results of high resolution nonhydrostatic simulations using the suntans model that were used to help interpret the high resolution observations which in turn were used to validate the model acoustic instruments to measure velocity like the acoustic doppler current profiler adcp which measures mean current profiles over the water column and the acoustic doppler velocimeter adv which measures high frequency currents i e turbulence at a point have been in use for several decades recent developments in acoustic instrumentation include the vectrino profiler which measures currents turbulence and ssc in 1 mm bins over 3 cm and can thus yield flow details within o 1 cm boundary layers in the environment e g wengrove and foster 2014 brand et al 2016 egan et al 2019 these detailed measurements can be used to directly test bottom stress and erosion parameterizations of wave current boundary layers in real field settings see sections 3 1 2 and 3 1 3 to broaden the parameter space and help develop parameterizations those detailed measurements can be compared to dns and les studies this presents a huge leap in our ability to use dns and les to develop parameterizations in real field scale settings such simulations have been restricted to laboratory scale experiments that until now were the only setting in which high resolution measurements like those with the vectrino could be obtained 3 3 2 remote sensing advances in high resolution remote sensing technologies are continuously increasing the resolution with which coastal processes can be measured for example giddings this workshop presented research on the large scale impacts of small scale coastal streams and the resulting plumes work that is motivated in large part by high resolution satellite remote sensing e g warrick and farnsworth 2017 although the high resolution imagery provides spatial detail models are needed to study the associated high frequency temporal resolution such as the high resolution simulations of romero et al 2016 which employed roms with four nested grids the finest had a resolution of 100 m to study the dispersion of a small river plume near santa barbara ca this resolution resolved the features of the submesoscale eddies and their interaction with the river plume although giddings and colleagues this workshop are employing roms nesting with a fifth nested grid that has 10 m horizontal resolution to study surf zone dispersion by waves interacting with the tijuana river plume near san diego ca these high resolution simulations are only possible with companion high resolution observations such as the aerial imagery and high speed jet ski transects employed by hally rosendahl et al 2014 or the infrared imagery by marmorino et al 2013 to study surf zone dispersion detailed observations like these form the impetus for the high resolution simulations of transient rip currents by kumar and feddersen 2016 2017 discussed in section 3 1 5 in a similar manner shi et al 2017 used the nonhydrostatic nhwave model with high horizontal resolution to understand the source of thermal fingers observed in aerial infrared imagery obtained at the mouth of the columbia river in addition to infrared remote sensing which reveals structures with a thermal signature recent remote sensing technologies have been developed to measure surface deflections with high spatial resolution as in the airborne lidar measurements of branch et al 2018 which could measure surface wave features at the columbia river mouth with o 0 5 1 0 m horizontal resolution structured from motion sfm photogrammetry using airborne drones dietrich 2017 is an exciting new technology to remotely measure high resolution bathymetry sfm methods can be used in combination with interferometric multibeam acoustic surveys to greatly increase the coverage and resolution of bathymetric data in shallow regions that are challenging to survey with traditional multibeam or single beam acoustic methods 3 3 3 acoustic backscatter recently methods have been devised using the backscatter signal from acoustic profilers to measure the vertical structure of flow density and sscs at extremely high resolution in coastal flows geyer et al 2013 for example geyer et al 2010 studied the dynamics of stratified shear instabilities in the connecticut river estuary with a high resolution broadband echo sounder that measured turbulent processes over vertical scales of o 10 cm in combination with adcps and advs to measure velocity horner devine and chickadel 2017 used infrared imagery to measure the surface features with o 1 m scale instabilities at the front in the merrimack river plume and these measurements were combined with subsurface backscatter measurements using adcps to infer the three dimensional structure of the instabilities and show that they are similar to lobe cleft instabilities found in gravity currents such high resolution field measurements naturally motivate high resolution nonhydrostatic simulations to further understand the underlying physics 3 4 high performance computing the coastal ocean models in use by the community today have been parallelized to some degree either using distributed memory message passing techniques such as mpi and or shared memory tools such as openmp it is well recognized that models that employ explicit methods in time or have simple matrix solves e g symmetric and diagonally dominant are typically easier to parallelize as they avoid the solution of potentially ill conditioned systems of linear and nonlinear equations commonly found in implicit methods however implicit solvers have become much more sophisticated in recent years with open source packages such as petsc now in wide use making them competitive for large scale parallel computing typical coastal models running large scale applications can scale to 100s or 1000s of cores on today s supercomputers as supercomputer architectures evolve with graphical processing unit gpu machines becoming more prevalent and hybrid cpu gpu machines coming online the algorithmic techniques must also evolve typical lower order methods in use today in most codes will probably not scale well on these machines due to low memory access to compute ratios higher order methods may actually perform better since more work is performed per cell meaning more local memory access however the difficulties with higher order methods mentioned in section 3 2 3 make this a challenging research area still if the coastal modeling community is going to play in the exascale computing arena of the future these challenges must be tackled head on and soon another high performance computing hpc arena that is rapidly evolving is the use of cloud computing cloud computing at least as it pertains to physics based simulations is still in its infancy and while nsf has funded cloud computing based research under some of its big data initiatives it remains to be seen what impact it will have on the coastal modeling community however cloud computing opens up entirely new frontiers in making computing resources available and more affordable to a larger community and will most certainly have a larger role in the future of hpc finally physics based simulators are becoming simply one part of simulation frameworks that merge big data uncertainty quantification and parameter estimation statistical inverse methods data assimilation and machine learning tools in these frameworks the simulator must often be executed tens to hundreds of times in order to generate statistical quantities of interest conditioned on uncertain data to be scalable and efficient these frameworks must utilize hpc and the physical based simulators must be optimized for performance the merging of data science tools with physics based simulation in hpc environments is another new frontier for the coastal modeling community to explore as our sources of data continue to advance at a rapid pace we must learn how to best utilize the data to improve predictive simulations 4 recommendations 4 1 collaboration engagement and education different groups within the coastal modeling community should more closely collaborate the original date of this workshop conflicted with the 15th estuarine and coastal modeling conference ecm15 which was held in seattle wa june 25 27 2018 despite the relevance of that workshop to the goals of this workshop it posed a conflict for just one of the forty invitees to our workshop this is indicative of the lack of communication in the coastal modeling community which appears to be divided into three groups 1 those working on development and application of forward models and process studies predominantly participants of this workshop 2 those working on assimilative models and state estimation techniques and 3 operational or applied modelers ecm15 including modelers from both academia and industry within these groups there are workshops and conferences with even more specificity such as focused workshops on data assimilation or unstructured grids e g imum international workshop on multi scale unstructured mesh numerical modeling for coastal shelf and global ocean dynamics while such specificity is natural and should be encouraged the community should also focus on workshops and collaborative initiatives that foster interaction of the different groups most importantly there is scant interaction between the forward and inverse modeling communities despite the importance of inverse methods in improving forward modeling capabilities and process studies similarly limited interactions between unstructured and structured grid or finite element and finite volume communities represent missed opportunities to synergistically address common problems in model development and application to what are often the same coastal regions the coastal modeling community should engage with the applied math computer science and hpc communities the lack of communication between the different groups within the coastal modeling community extends to a lack of communication between the coastal modeling community and the applied math and hpc communities coastal modeling can benefit significantly from the recent surge of methods in machine learning parameter estimation and inverse methods which can be used to quantify uncertainty and incorporate high resolution in situ and remote sensing techniques for improved predictions and parameter estimation into coastal ocean models see section 4 5 funding agencies should consider proposal calls which foster such engagement for example the collaboration in mathematical geosciences program was an nsf sponsored program that funded research collaborations between coastal modelers and applied mathematicians and led to the development of many of the techniques presented at this workshop academic programs should promote more applied math and computer science in their curricula the lack of collaboration between different groups in the coastal modeling community can be attributed to the academic backgrounds of those involved for example the prevalence of finite volume over finite element methods in coastal modeling is likely because traditional curricula in physical oceanography or coastal engineering focus on finite difference or finite volume methods because those require less mathematical background than finite element methods similarly data assimilation and machine learning techniques also require strong backgrounds in applied mathematics and computer science the coastal modeling community should integrate such methods into curricula to train the next generation of coastal modelers because coastal models have become sufficiently robust and accessible over recent decades scientists and engineers can now use coastal models in their research without having to develop their own codes or understand the details of a particular numerical method this precludes the need in many cases for coastal modelers to learn the applied math and computer science details involved however academic programs should broadly train students in the underlying principles and tradeoffs of different modeling approaches and introduce methods by which they might apply models to particular problems for example using the common framework described in the next section 4 2 a common framework for model setup validation and intercomparison discussions at the workshop focused extensively on model intercomparison to aid in model selection for a particular application and to assess accuracy of numerical methods model efficiency and the effectiveness of parameterizations employed in the models however it was noted that model implementation for real coastal problems requires many subjective choices that make it difficult if not impossible to quantitatively compare or reproduce results these subjective choices include for example the bathymetric resolution the structure of the grid model forcing datasets parameterization schemes numerical method options etc attempts to compare these different choices are difficult because they are generally not documented in academic papers largely because few if any scientific or engineering journals encourage publication of site specific modeling studies with technical details needed to reproduce model results instead academic papers focus on general descriptions of model setup with an emphasis on model validation that is also highly subjective this suggests an opportunity for the community to be more open to publications related to site specific model implementation and to give more credit for publications in technical journals of course a paper with all of the technical details related to a model application would be too boring for words therefore the community should embrace the open source model and use code repositories like github to promote completeness and transparency related to specific model applications although code repositories are common platforms to share source code for the models themselves it is rare to find references to repositories related to actual model applications in papers an added advantage of code repositories is the ability for individuals to modify model parameters and input data and post results of different implementations to the repository so that they can be made available to the community we note that academic papers focusing on numerical methods as opposed to model applications usually include details needed to reproduce the test cases however these test cases typically do not require subjective user choices because they are often simplified and designed to accentuate the behavior of a specific aspect of a model they also typically do not incorporate the spatial and temporal complexity of a realistic application to promote quantitative assessment of different models the community should develop a set of guidelines on how to report details related to coastal model implementation these details should include a list of all model parameters and subjective choices needed to reproduce the results including validation metrics and related data in addition the guidelines should promote sharing of datasets needed to initialize and force the models such as bathymetry grid wind data tidal data flow data etc ultimately it would be up to the community to define a set of standard reporting protocols to ensure all details needed to reproduce a model result are available these details would also include observational data used for model validation and the details of how validation metrics were created to encourage unified reporting of these model details the community should encourage proposal writers to document reporting strategies in for example the nsf data management plan to promote model intercomparison the community should agree on a set of i o standards and benchmarks based on idealized and real field scale test cases a unified reporting standard would not advance knowledge of the benefits and drawback of different models unless a set of standard benchmarks and test cases were agreed upon that would encourage model intercomparison studies while there are many idealized test cases that are reported in the literature there is no consensus on a set of standard idealized test cases that test different numerical aspects of coastal models such as advection schemes horizontal vertical gridding nonhydrostatic solvers free surface solvers etc the community should also agree upon specific study sites that form standard field scale benchmarks related to different estuarine or coastal regimes when possible grids bathymetry boundary conditions and forcing and computational cost should be standardized to eliminate the impact of subjective user choices in model intercomparison studies finally standard i o formats would need to be agreed upon to reduce barriers related to testing of new models and encourage application of many models to the benchmark sites although there are countless possibilities the community should form a consensus on benchmarks that are needed and specific sites that form the basis for those benchmarks as examples the columbia river estuary could be the benchmark for a salt wedge estuary while san francisco bay could be the benchmark for a partially mixed estuary a specific hurricane event could be the benchmark for storm surge models while a specific coastal region could be the benchmark for nearshore modeling the community should set standards for model coupling and nesting approaches several presentations at the workshop focused on simulations involving coupled modeling frameworks and nested approaches the most commonly coupled models are circulation and wave models such as swan roms some coastal circulation wave and storm surge prediction models are dynamically coupled to atmospheric models to obtain surface wind stresses and heat fluxes and have shown important improvement in both ocean prediction and storm track intensity forecasts zambon et al 2014a b nelson and he 2012 further accuracy can be achieved through coupling of circulation models with hydrology models that include surface runoff from precipitation these can be particularly important to predict compound flooding events when heavy rains and the associated runoff significantly increase water levels during strong storm surges e g silva araya et al 2018 dresback et al 2011 in addition to coupling of different modeling frameworks many applications benefit from the ability to nest higher resolution grids into coarser grids to study a specific region in more detail the most commonly employed example of grid nesting in ocean modeling is the roms based agrif approach romsagrif gforge inria fr which allows for one or two way nesting on successively refined grids despite the necessity of model coupling and nesting each implementation employs its own unique methodology for inter model communication for example some models employ the model coupling toolkit mct or the earth system modeling framework esmf which can handle communication on both serial and parallel implementations and are suitable for two way coupling model downscaling implementations involve either two way nesting in which boundary values are exchanged and updated by the parent and child models or one way nesting in which initial and boundary conditions generated by the parent model are written to files that are then treated as input for the child model a standardized approach to model coupling and nesting would provide several advantages over the existing relatively ad hoc paradigm first a standard approach would provide a framework that could be improved upon to make model coupling more efficient researchers could work with one framework and study the advantages and disadvantages of different approaches to help streamline the standard second the standard approach would enable coupling of a wider variety of models from which more model comparisons could be performed to assess the benefits of different models finally standardized model coupling approaches could provide a framework for use in applications that rely heavily on forcing and boundary conditions from different models such as hydrological and atmospheric forcing including winds and heating the community should devise standards for grid development and quality assessment it would be difficult to develop a grid generation tool that could be applied to general coastal modeling problems given the wide variety of grid types involved indeed there are countless grid generation tools and there is no obvious best choice nevertheless grid generation is largely subjective because of the difficulty in resolving grid scale coastal and bathymetric features see section 3 2 6 while it would be difficult to eliminate this subjectivity it could largely be reduced if such grid scale variability were eliminated either through bathymetric smoothing at subgrid scales or through higher resolution grids this may not be appropriate for many applications given the need to include grid scale coastal variability on some domains particularly in estuaries with complex island or channel networks however given the need to resolve the coastal submesoscale as discussed in section 4 3 higher grid resolution would inherently lead to less constraints on grid quality for grid generation tools in addition to higher resolution the concept of subgrid bathymetry section 3 2 7 also relieves the constraints associated with grid masking or coastline following grids in the shorter term the community should focus on encouraging more detailed reporting of grid generation strategies grid quality metrics and grid sensitivity studies grid generation strategies should more faithfully outline choices made regarding grid masking or adjustment particularly if there was a strategy that could be quantified over one that was subjective grid quality metrics should also be reported e g skewness telescoping fraction number of masked cells along with grid sensitivity studies the overall objective should be to encourage reproducibility of results regardless of the model or grid generation tool while noting that reproducibility can be affected by factors that cannot be controlled such as different variable precision among codes i e 32 vs 64 bit floating point arithmetic or stochasticity more open discussion of the process behind and results of grid generation will hopefully facilitate development of community tools that are robust and not specific to a particular model such as the shingle framework candy and pietrzak 2018 validation metrics should be standardized and include dynamically relevant integrated metrics that are representative of the overall utility of the model in addition to the skill score of murphy 1988 as discussed in section 3 2 2 there are many metrics used to compare coastal model predictions to observations such as the model skill metric of wilmott 1981 and standard statistical metrics such as the correlation coefficient and mean and root mean square errors because there is a general lack of agreement on standards for these metrics the community should make a concerted effort to create such standards with the understanding that these will likely be highly site and problem specific for example the choices and metrics used to evaluate model ability to reproduce sscs in a salt marsh would be very different from those used to evaluate model ability to reproduce the significant wave heights during a hurricane ultimately details of the validation metrics should be part of the aforementioned standards related to reporting guidelines and benchmarks in addition to standardizing existing validation metrics new validation metrics should be devised that incorporate integrated or dynamical quantities such as fluxes area integrated quantities or time averaged or low frequency metrics for example different components of the salt flux stokes drift mean etc might be more appropriate validation metrics than time series of bottom salinity the challenge with such metrics is that while they are readily computed from three dimensional model outputs it is harder to compute them with observations observational campaigns should therefore focus on methods to better validate such integrated quantities in models advances in remote sensing technologies are promising in this regard given their ability to measure spatial distributions of water properties at increasingly higher resolution see section 3 3 2 some integrated metrics do not require observations such as the ratio of numerical to physical mixing see sections 3 2 5 and 3 2 8 for which the ideal model would eliminate the numerical mixing entirely 4 3 resolve the coastal submesoscale the community should collaborate more closely with the high performance computing and applied mathematics communities to develop high resolution accurate models that directly compute the coastal submesoscale as discussed in section 3 1 1 it is unlikely that parameterizations for coastal submesoscale processes can be developed given that such processes are highly site specific and dependent on local geometry and other related physical processes therefore the coastal modeling community should focus on developing modeling tools that can directly simulate such processes in addition to attempting to parameterize them simulations at grid resolutions that would resolve the coastal submesoscale would require o 1 10 m horizontal grid resolution in estuaries and o 0 1 1 km in coastal shelf domains which would place a heavy burden on computational requirements however high performance computing platforms have advanced significantly in the past few decades and there is great potential for the development of high resolution coastal models that run efficiently on such platforms see section 3 4 with higher resolution models would be less susceptible to numerical error which would enable quantification of model uncertainty due to bathymetry and boundary conditions less numerical error would also allow for assessment of the benefits of more advanced computational techniques which are traditionally reserved for idealized problems 4 4 coordinate observational and modeling studies to improve parameterizations parameterizations should be tested and advanced using direct comparison between high resolution state of the art measurement technologies and focused modeling studies there is active research in many areas related to parameterizations of unresolved processes in coastal models see section 3 1 the community identified the following as among the most important and relevant for coastal modeling noting the importance of developing parameterizations that require as little tuning as possible 1 parameterizations of the spatial and temporal variability in horizontal diffusion and dispersion bottom roughness and unresolved drag including dependence on both physical bedform grain size vegetation kelp corals waves and model grid resolution advection scheme characteristics 2 nearshore wave modeling wave breaking parameterizations wave mud damping 3 air sea interaction under high wind conditions including air sea momentum and buoyancy flux exchanges wave breaking and wave current interactions 4 sediment transport modeling erosion parameterizations flocculation settling bed consolidation biological effects 5 morphodynamics while there has been extensive work on parameterizations within each of the categories listed it is not always clear whether the parameterizations improve the coastal models in which they are implemented therefore to be relevant for coastal scale processes improvements to parameterizations should clearly demonstrate improved predictive capability of coastal models development of parameterizations with clear connections to coastal model results requires stronger collaboration between observationalists experimentalists and modelers over different scales i e large scale vs les and dns and it also requires development and application of more advanced observational techniques as an example parameterizations of wind wave sediment resuspension are difficult to test in the field because of the difficulty in observing the true bed stress and the true near bed sediment erosion however it is now possible to directly measure turbulence mean flow and sscs in 1 mm bins near the bed using the profiling vectrino which allows for direct assessment of the accuracy of existing parameterizations of bottom drag and sediment erosion see section 3 3 1 such instruments also allow for integration of les or dns results into development of improved parameterizations because they can be validated recent advances in remote sensing technology also allow for tighter coupling between observations and models since remote sensing provides higher spatial resolution that can be used to test parameterizations see section 3 3 2 4 5 robust parameter estimation and uncertainty quantification coastal models should incorporate advanced tools to more robustly estimate parameters and quantify uncertainty the greatest impediment to the development of more accurate coastal models is a lack of knowledge of the uncertainty the uncertainty has numerous sources including parameterization error numerical error including the discretization and errors related to grid quality and errors from boundary conditions and forcing the path to reducing the errors related to each of these sources on their own is clear and many of the recommendations in this paper suggest strategies to reduce those errors however owing to an inability to quantify the relative contribution of different sources of uncertainty accurate coastal modeling relies more on subjective choices see section 3 2 5 than it does on quantitative metrics subjectivity plays a dominant role in much of coastal modeling given that accurate simulations require an experienced user to make decisions in an ad hoc manner during the modeling process including 1 the numerical methods or simply the choice of which coastal model to use 2 the parameterizations and their underlying constants 3 choice of suitable datasets for boundary conditions and forcing and the interpolation techniques to impose those conditions at model grid points 4 the model grid and 5 validation techniques the result is that accuracy of model results is typically attributed more to the experience of the model user than to the accuracy of the model itself to eliminate the subjectivity related to coastal modeling the community should incorporate advanced tools in applied mathematics and computer science to quantify uncertainty and develop robust techniques to objectively guide model choices and estimate optimal model parameters these advanced tools include uncertainty quantification data assimilation and machine learning data assimilation methods use observations to improve predictions and are well established in regional ocean models e g edwards et al 2015 largely owing to the prevalence of regional scales in operational modeling systems that must assimilate data to ensure predictability given its success at regional scales there is ample room for data assimilation in coastal modeling studies particularly in the context of parameter estimation data assimilation methods can inform the optimal parameter sets that minimize the difference between predictions and observations thus providing a quantitative methodology to estimate parameters that are unknown or difficult to measure for example zhang et al 2018 assimilated remote sensing data of surface ssc to estimate the spatial distribution of the settling velocity which is typically a tunable parameter as discussed in section 3 1 4 in a three dimensional cohesive sediment transport model similarly zhang et al 2011 used data assimilation to estimate the spatial distribution of bottom friction coefficients which are also typically tuned as discussed in section 3 2 5 in a regional tidal model while the focus of data assimilation is to use observations to minimize some measure of the model error the source of the error is determined with methods in uncertainty quantification which are also popular in regional ocean modeling e g lermusiaux et al 2006 as an example in the coastal ocean manderson et al 2019 quantified the uncertainty of the density stratification and demonstrated how the uncertainty is manifested in nonlinear internal gravity wave models given the many tunable parameters in coastal modeling particularly for sediment transport it is important to understand the uncertainty related to each parameter to avoid the potential for equifinality or the possibility of the same result arising from different sets of parameters van maren and cronin 2016 in addition to providing a robust framework to characterize the numerous sources of uncertainty in coastal models uncertainty quantification can also help prioritize future research directions based on where uncertainty in coastal models is greatest owing to the continued improvement of observational technologies and increased quantities of observational data see section 3 3 more machine learning should be incorporated into coastal ocean modeling because machine learning extracts relationships from datasets without the need for models based on first principles it can be used to estimate parameters that are difficult if not impossible to measure machine learning techniques have seen great success as a tool to understand and predict coastal sediment transport and morphodynamics as discussed in the review article by goldstein et al 2019 as examples yoon et al 2013 used an artificial neural network approach to determine the hydrodynamic parameters that best predicted ssc in the surf zone while goldstein and coco 2014 used machine learning to predict the particle settling velocity in a dataset derived from various suspended sediment flows this approach to determining parameters from data can be combined with models in what are referred to as hybrid models wherein machine learning is used to determine model parameters based on observational data goldstein et al 2019 like data assimilation this hybrid approach appears to be a promising method to reduce ad hoc tuning and subjective parameter choices in coastal ocean modeling 5 conclusions the primary outcome of the workshop was agreement on the need to reduce subjectivity in implementation of coastal ocean models this subjectivity arises from the need to make choices that rely on experience rather than quantitative metrics ironically because only experienced model developers and users attend a workshop of this kind the model results that are presented reflect the subjective choices that can only be made with extensive experience it was agreed that subjectivity should be reduced through development of a common framework for coastal model users and developers through stronger engagement with applied mathematics and computer science communities and through implementation of methods in data assimilation uncertainty quantification and machine learning to understand the sources of uncertainty and quantify parameter choices in coastal ocean modeling a second outcome of the workshop was an understanding of the importance of setting standards for numerous aspects of coastal modeling the lack of which is partially related to the subjectivity inherent to the current state of the art although workshop participants had extensive experience with models most lamented at the lack of standards to guide model development and dissemination of results the greatest advantage of setting standards is that they encourage the community to focus efforts in favor of continued model assessment and improvement it was thus agreed that the community should focus on setting standards for the following aspects of coastal modeling 1 implementation details needed to reproduce model results 2 input output standards for ease of model inter comparison 3 benchmarks to test and compare model performance 4 coupling and model nesting 5 grid generation and 6 model validation regarding technical details of coastal models it was agreed that it is difficult to assess the advantages of different numerical methods or parameterizations this is due in part to the complexity of coastal modeling and a lack of standards to assess and compare models and hence can be partially addressed with the outcomes discussed above however a basic theme emerged regarding the development of advanced numerical methods and was related to the coastal submesoscale throughout the workshop this refers to horizontal scales that are smaller than the horizontal bathymetric scale but larger than the depth in coastal modeling unlike ocean submesoscales these coastal submesoscales are strongly controlled by the coastal geometry and hence are highly site specific as a result there is little hope in developing parameterizations for them and hence the community should work toward resolving coastal submesoscales with high resolution simulations like the subjectivity problem this also warrants collaboration with the applied math and computer science communities but in this case to develop accurate numerical methods and high resolution efficient simulations on advanced hpc systems while it is possible to resolve the coastal submesoscale smaller scale processes with scales smaller than the depth will likely never be resolved not only are these scales prohibitively small but they are dictated by small scale features that are hard to measure and hence must be modeled or parameterized such as turbulence the community should continue to focus on developing parameterizations for such processes following the wealth of research that has already been done to date however it was agreed that there should be tighter coupling between observations laboratory experiments and modeling to focus specifically on developing and testing of parameterizations in coastal models in the past it has been difficult to test parameterizations in field scale models because of limitations in observational technologies which could not directly measure the parameters needed for the processes being parameterized e g the bottom stress or sediment erosion rates observational technologies have advanced significantly and hence it is now possible to directly test parameterizations in situ accomplishing the objectives laid out in this paper will require buy in from funding agencies to support critical components of modeling that have not been part of traditional funding streams in the past this could include support for research that focuses on inter comparison studies or development of benchmarks or modeling standards such as i o coupling or validation metrics such benchmarks or standards could then be the focus of future workshops on coastal modeling to foster collaboration among different groups within the coastal modeling community in a similar vein while model data is typically integrated into proposal data management plans model test cases and supporting documentation could be an integral part of these plans funding agencies should also encourage collaboration between applied mathematicians computer scientists and coastal modelers to help accomplish many of the objectives laid out in this paper funding agencies will play an important role in the future of coastal ocean modeling but buy in of the objectives laid out in this paper will ultimately come from coastal modeling community members who are tasked with reviewing proposals and recommending funding therefore the proposal submission and review process should involve new priorities and evaluation procedures and the community needs to identify and develop sustainable means of funding the initiatives proposed in this paper given the complexity of developing testing and maintaining coastal models they can be just as difficult and costly to develop and support as ships or state of the art equipment and instrumentation therefore models should be treated as a fundamental component of critical infrastructure needed to support research just like laboratory facilities field instrumentation and research vessels the notion that models constitute critical infrastructure implies that model maintenance and development should be an important component of infrastructure or facilities sections of proposals as with ships and major laboratory facilities that are used broadly by the science community coastal model development maintenance and support cannot be expected to be funded only through core science budgets that support hypothesis driven science it has often been the case that new models or parameterizations have emerged from such hypothesis driven research but this ad hoc approach is unsustainable as coastal models are now used broadly by non developers to advance basic science coastal models have become an important community asset that should be supported like other key infrastructure which will likely require commitment and coordination of resources across multiple funding agencies e g the federal agencies in the united states nsf noaa onr doe usgs a key result of this workshop was that the range of coastal model applications benefits from a diversity of modeling approaches but their accessibility and evaluation are hampered by legacy impediments developing the tools and frameworks to lower the structural barriers requires investment in order to realize an improved next generation of coastal ocean models acknowledgments we thank carmen torres at stanford university and jennifer warrillow at north carolina state university for their assistance with workshop logistics helpful comments and suggestions were provided by two anonymous reviewers and hans burchard and john warner the workshop and preparation of this paper were funded by u s national science foundation grant oce 1749613 appendix workshop participants and presentation titles invited early career scientists indicated with workshop presentations are available for download from web stanford edu fringer nsf workshop 2018 ateljevich eli cal dept of water resources from coast to estuary to channels challenges in cross scale modeling of the san francisco bay delta baptista antonio oregon health and science university is in silico estuarine oceanography here yet lessons from a humbling benchmark blain cheryl ann naval research laboratory approaches to capture freshwater influence in coastal and estuarine waters burchard hans leibniz institute for baltic sea research the concept of numerical mixing in coastal oceans chai fei u maine modeling nutrients and plankton dynamics of the san francisco bay chao yi ucla modeling the california coastal ocean and its interactions with san francisco bay chen changsheng u mass dartmouth importance of resolving coastal estuarine wetland interactions in estuarine modeling davis kristen u c irvine spatially continuous observations of shelf and estuarine processes what can new observational tools tell us about what we re getting right and wrong in coastal models dawson clint u t austin some hpc challenges in coastal modeling dietrich casey ncsu connecting coastal infrastructure to predictions of storm surge and flooding fringer oliver stanford will we ever simulate via nonhydrostatic les real estuarine and coastal problems ganju neil usgs progress and challenges in simulating coupled hydrodynamic vegetation processes georgas nickitas jupiter predicting risk in a changing climate geyer rocky whoi estuarine salinity variance and mixing is numerical diffusion trying to tell us something giddings sarah scripps inst of oceanography capturing the dynamics of ocean estuarine exchange when small discharge rivers matter gross edward u c davis hydrodynamic and particle tracking modeling to support fish migration studies he ruoying ncsu modeling air sea interactions during storms hegermiller christie whoi towards simulating extreme coastal morphological change using coupled models hetland robert texas a m university the whimsy of model data comparison in coastal ocean modeling hsu tom u delaware insights into several issues in sediment dynamics investigated by turbulence scale and wave scale models kirby jim u delaware surface waves the interface between phase resolving and phase averaged models and outstanding issues in each setting klingbeil knut leibniz institute for baltic sea research the problem of numerical mixing and its mitigation through adaptive vertical coordinates kumar nirnimesh u washington parameterizing the effect of surf zone eddies in 3d models implications for cross shore exchange and surf zone dispersion li ming u maryland climate downscaling projections for estuarine hypoxia and acidification using coupledhydrodynamic biogeochemical models luettich rick u north carolina challenges of moving from hindcasting to forecasting storm surge and inundation maccready parker u washington challenges in realistic modeling of estuary shelf connections moriarty julia usgs challenges and opportunities in regional scale hydrodynamic sediment transport modeling olabarrieta maitane u florida modeling the long term morphodynamic evolution of estuaries advances and challenges orton phillip stevens inst of tech the utility of fast accurate models stories from the front lines of disaster and adaptation ozkan haller tuba oregon state u nearshore modeling what s data got to do with it pietrzak julie t u delft lessons learnt from a tidal river plume the importance of frontal dynamics near the river mouth ralston david whoi modeling sharp salinity gradients in a tidal salt wedge and river plume scully malcolm whoi bathymetrically controlled inflow events in chesapeake bay shi fengyan u delaware simulations of river plumes using a sub meter resolution non hydrostatic model siedlecki samantha u connecticut marine sciences getting it right for the right reasons lessons from simulating regional biogeochemistry as part of decision support tool development signell rich usgs interactive scalable data proximate analysis of coastal ocean model data in the cloud vitousek sean u illinois chicago challenges in modeling waves turbulence and sediment transport across scales westerink joannes notre dame the evolution of process and scale coupling in coastal ocean hydrodynamic modeling wilkin john rutgers the iooc task team on coastal modelling for ioos a community consensus on research priorities for integrated analysis across littoral estuary and shelf regimes zhang joseph virginia institute of marine science simulating estuarine circulation in the chesapeake bay shelf system 
