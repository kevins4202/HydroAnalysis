index,text
25590,how and when to upscale and finance nature based solutions in watersheds in order to improve downstream urban water security is a global concern in this paper we address such challenge by providing a novel method for planning the expansion of a chosen set of nature based solutions that couples optimization of sequential decision making hydrology modelling and hydroeconomics in a single modeling framework the benefits were considered the avoided costs of water scarcity and water treatment perceived downstream the optimal expansion schedule of decisions was then defined under the objective function of minimizing total cost we demonstrated the framework by applying it to a set of benchmark alternative scenarios for the sinos river watershed brazil we found that the optimal schedule of nature based solutions expansion is sensitive to the initial land use and future drivers treatment cost reduction was the greatest feature in some scenarios confirming it as a strategy for addressing water quality issues of degraded watersheds but the accounting of water scarcity cost introduced a tradeoff expansion of nature based solutions in well preserved watersheds may not worth the investment future research may improve current limitations in scope and models without structural changes in the framework practical implications of this work arise from the funding of watershed payments for environmental services projects by institutions and governments which demand evidence based policies to support investment of public resources graphical abstract image 1 keywords nature based solutions expansion planning direct benefits water utility abbreviations dp dynamic programming lulc land use and land cover nbs nature based solutions pes payments for ecosystem environmental services software availability name planning nature based solutions plans available on www github com ipo exe plans2 developer and contact information iporã possantti contact ipora possantti ufrgs br year first available 2020 software required python 3 6 dependencies numpy scipy pandas matplotlib availability and cost free and open source under gnu gpl 3 0 license program language python 1 introduction urban water security is a global concern since the risk of water crisis in cities escalates worldwide driven by forces both on the demand side such as the growth of economy and population and on the supply side such as the changes in climate and land use wada et al 2016 flörke et al 2018 wwap un water 2019 many sustainable development agendas highlight the use of nature based solutions nbs among available technologies for better reliability in urban water supply boelee et al 2017 keesstra et al 2018 raymond et al 2017 sonneveld et al 2018 wwap un water 2018 these are solutions inspired by natural processes that improve water management food production and ecosystems conservation iucn 2012 european commission 2015 wwap un water 2018 sonneveld et al 2018 comprising the already existing but scattered concepts of soil and water conservation practices and ecological economics eggermont et al 2015 nesshover et al 2017 hanson et al 2020 such attention on nbs is justified since it affects hydrology by changing land use and land cover lulc potentially enhancing water provision water quality control soil protection and hydro meteorological hazards mitigation foley et al 2005 kalantari et al 2018 keesstra et al 2018 sahani et al 2019 for instance in the case of forests not only less runoff is produced but the annual water yield tends to be relatively lower than conventional land use andréassian 2004 filoso et al 2017 zhang et al 2017 which is associated to higher evapotranspiration zhang et al 2001 streamflow of forested watersheds however tends to be of higher quality as water gets filtered by litter rich top soils neary et al 2009 likewise soil and water conservation practices in food production tends to reduce runoff remove nutrients and entrap sediments liu et al 2017 mekonnen et al 2015 the expected benefits perceived downstream is the main reason why cities deploy programmes such as payments for environmental services pes or watershed pes and induce upstream lulc change by the expansion of nbs brauman et al 2007 bremer et al 2016 dudley and stolton 2003 smith et al 2006 un water 2018 indeed watershed pes grows worldwide counting up to 387 programmes in 62 countries and annual payments estimated up to us 24 7 billion salzman et al 2018 in this sense valuation of the environmental services into direct benefits can be defined as the avoided costs perceived by the urban downstream water utility such as treatment costs and scarcity costs cunha et al 2016 kroeger 2013 postel and thompson 2005 price and heberling 2018 from the side of upstream landholders these water users would not accept as payment less than the operational cost plus the land use opportunity cost engel et al 2008 smith et al 2006 for wunder 2007 the efficiency of pes schemes should be assessed by evaluating the additional environmental services against a baseline scenario of not deploying the pes strategy following this reasoning decision making may rely on simulated hydrological performance an array of computer tools are available for this task such as swat neitsch et al 2011 and gis based like invest and others natural capital project 2016 carvalho santos et al 2014 grilli et al 2020 these allow quantitative assessment of environmental services francesconi et al 2016 tuppad et al 2010 cong et al 2020 and improvements in the decision making process arabi et al 2006 2007 strauch et al 2012 2013 ullrich and volk 2009 dynamic modelling and design may couple hydrology simulation into optimization algorithms babbar sebens et al 2013 chu et al 2010 zhang and chui 2018 zhang and song 2014 by introducing economic constraints the optimization algorithm maximizes cost effectiveness providing solutions that consider the costs of nbs expansion arabi et al 2006 beverly et al 2016 cibin chaubey 2015 yang and best 2015 beyond cost effectiveness optimization valuation of direct benefits using bio economic or hydro economic models allows the assessment of return on investments castro et al 2018 haavisto et al 2019 kroeger et al 2019 momblanch et al 2016 secchi et al 2007 none of the mentioned tools has yet included optimitization of decisions in long term planning horizons knowledge of when how and if it is necessary to deploy nbs in a watershed given a long term planning horizon is still limited this context allowed us to develop a modeling framework focused on urban water uses public utility decisions and upstream landholders all connected by the watershed s hydrologic response demand growth water value and costs associate to investment decisions on specific nbs despite the limited scope we address a key question faced by several cities driven by the necessity to secure future water supplies and negotiate solutions with upstream watershed users our aim is to push research forward by providing a novel method for planning the expansion of a chosen set of nbs that couples into a single and flexible modeling framework 1 optimization of sequential decision making 2 simulation of hydrological processes and 3 direct benefit assessment for downstream urban users for a better discussion we also illustrate the framework by applying it to three alternative water systems under different future projections the paper is structured as follows section 2 presents the details of the proposed method and models section 3 illustrates the framework in action by determining the optimal nbs expansion policies for three alternative water system under a set of future projections section 4 discuss the method strengths and limitations section 5 finally states our conclusions 2 methods 2 1 conceptual framework we designed a conceptual framework so the water system contains two subsystems as components the water producing system and the water consuming system as depicted in fig 1 on each side we represent drivers that evolve into the future following the water system s dynamics and transient state on the supply side the climate driver defines the water net inputs while the land cover and land use lulc drivers define the partitioning of incoming water among hydrological water stocks impacting the availability and quality of the water received by the consuming system on the demand side the social and economic drivers combine to produce the water consumption flow those drivers include population growth and consumption behavior under this framework a downstream urban water utility looks for water security from the perspective of this user the main nbs types of concern are reforestation best management practices for pastures and best management practices for crops because those are related to runoff generation and infiltration to groundwater also for this user investment in nbs expansion must yield an economic benefit when balancing avoided costs of both water scarcity and water treatment this user want to know if expanding those nbs is worthwhile given 1 a target water system 2 a planning horizon and 3 a future set of projections if it is when the expansion of a given nbs type or mixture of types should be made as well as their amount to ensure optimal cost effectiveness and a better financing schedule 2 2 direct benefit assessment following the principle of additionality wunder 2007 the direct benefit of the environmental services perceived by the downstream water utility is the avoided cost defined as the difference between the cost of the baseline policy not expanding nbs and the cost of expanding nbs 1 b c 0 c where b is the direct benefit c 0 is the cost of the baseline policy and c is the cost when expanding nbs all units are monetary the cost of the baseline policy is defined as the sum of water scarcity cost and water treatment cost 2 c 0 s c 0 t c 0 where sc 0 and tc 0 are the water scarcity cost and water treatment cost respectively both under the baseline policy by expanding nbs the cost c of the system is the sum of sc tc and xc which represent respectively the water scarcity cost treatment cost and nbs expansion cost 3 c s c t c x c note that other water supply costs not related to the expected nbs impact can be disregarded since it would be cancelled out in eq 1 in order to get a positive benefit eqs 1 3 requires the investment in nbs expansion xc to produce a sufficient decrease in the water scarcity cost sc and water treatment cost tc relatively to the respective costs in the baseline policy sc 0 and tc 0 so that c c 0 also to yield the highest possible benefit in a given planning horizon the nbs expansion should also be scheduled in a way that costs are minimized this optimal policy can be found by an algorithm that optimizes the expansion of the nbs having c sc tc and xc as decision variables computing sc tc and xc is a complex task requiring separate simulation models this is explained in the next sections 2 3 optimization of nbs expansion for a given set of nbs water system and defined future projection there is an optimal schedule to expand nbs including no expansion which means expanding a given area of those nbs at a given time so that the total direct benefits are maximized this yields a cost effective strategy not only because the nbs areas and their effect are adjusted following the water demand growth but also because it schedules investment decisions in time considering the time value of money we structured the problem of finding the optimal schedule as a sequential decision making process solved by dynamic programming for loucks and van beek 2017 this is known as the capacity expansion problem and has been proposed to solve problems of conventional water infrastructure expansion arancibia et al 2016 fraga et al 2017 rosenberg et al 2008 introduced by bellman 1957 dynamic programming dp is a computational technique for solving problems defined by sequential decision making processes the solution of the procedure is the optimal combination of decisions or the optimal policy hillier and liebman 2009 the procedure is based on the bellman s principle of optimality that the optimal policy must have optimal sub policies lew and mauch 2007 dp divides sequential decision making problems in stages states and decisions in each stage the system might be in a variety of discrete states which can change by decisions when moving forward or backward stage by stage those decisions are limited by the set of possible states and other constrains related to the problem a recursion relationship keeps track of previous optimal policies and it is used to find the optimal policy decision for each state when moving forward or backward along the stages once finished the recursion procedure it is possible to retrieve the optimal policy for the whole sequence of stages i e the whole problem this optimization technique allows the partitioning of the nbs expansion problem into multiple and simpler small problems which answer how much nbs should be expanded now considering the current conditions and past decisions the result from a decision in a given stage is then carried out to the next ones in the recursive term of the procedure this allows representation of time coupled decisions in problems where the objective function is discontinuous or non continuously differentiable which limits application of other optimization methods the tradeoff is that the state space needs to be discretized which limits the problem size due to computational constraints for the dp problem of nbs optimal expansion depicted in fig 2 a the planning horizon is divided in t planning cycles which are smaller periods of time t or stages the discrete possible states s of the system in any given stage t are defined by a set of positive integer values ranging from 0 to 100 and representing percent fractions of the initial available area to nbs expansion the total number of possible states s depends on expansion step resolution y which is a submultiple of 100 discrete expansions of nbs are represented by the decision variable x which adds up to the state s of the system after any stage t the optimal expansion schedule or optimal policy is determined by minimizing the total system s cost of all sequential stages which is computed by the following objective function 4 m i n i m i z e t 1 t c t s t x t f t s t 1 subject to 5 s t 1 s t x t t and to 6 s t 0 y n y 100 n n t in which t is the number of stages t is the stage index c t is the system s cost in present value for a given stage t given by eq 3 s t is the system s state as a percent fraction of available area for a given stage t x t is the expansion decision of nbs as a percent fraction of available area for a given stage t and y is a submultiple of 100 defined as the expansion step resolution the recursive relationship used to minimize the total system s cost is characterized by finding the minimum cost f to reach every discrete state s considering the accumulated cost after the first stage 7 f t s t 1 c t s t x t t 1 m i n c t s t x t f t 1 s t t 1 in which f t s t 1 is the minimum cost of achieving the state s t 1 at the end of stage t given the three types of nbs chosen here to expand at any stage t each x t expansion decision is a set of multiple combinations of individual nbs types that adds up to x t for example consider an expansion decision x t of 20 in a dp problem where the expansion step resolution y is of 10 one option may be expanding 10 of reforestation nbsf 10 of pastures with best management practices nbsp and 0 of crops with best management practices nbsc likewise another alternative is expanding 20 of reforestation and none of the remaining and so on for every policy decision x t we introduced in the dp an algorithm to compute the set d of all possible nbs expansion combinations this unfolds the x t variable into several x t d expansion decision sets fig 2b where d is a given combinations of x t 8 x t d n b s f d n b s p d n b s c d d d t hence x t d is the expansion decision set for the nbs expansion combination d in a given stage t d is the set of all possible nbs expansion combinations nbsf is the reforestation expansion nbsp is the pasture with best management practices expansion and nbsc is the crop with best management practices expansion for a given stage t and for any expansion decision set x t d the system s cost c t were computed in present value 9 c t f v t 1 r n t in which fv t is the future value of the system s cost r is the interest rate and n is the number of years from the beginning until the end of stage t from eq 3 the future value of the system s cost was set as the sum of three terms 10 f v t s c s t x t d t t c s t x t d t x c s t x t d t t in which sc is the water scarcity cost tc is the water treatment cost xc is the cost of nbs expansion s t is the system s state as a percent fraction of available area for a given stage t and x t d is the expansion decision set for the nbs expansion combination d in a given stage t to compute fv t for any system s state s t the decision set x t d is organized into a vector of land use classes of the system s watershed represented by lulc t this vector is then relayed to the simulation routines hydrology scarcity cost and expansion cost which feedbacks to the dp algorithm fig 2c the simulated series of streamflow on a daily time step for the full period of years comprised by the stage is then fed into a hydro economic model to simulate the stage s scarcity cost similarly a bio economic model computes the water supply treatment costs scarcity costs and treatment costs are computed on a daily time step and integrated for the stage period the expansion cost is finally computed directly from the updated lulc t given the baseline policy is a sequence of x decisions equal to zero expansion the system s cost in this case was computed as the accumulated costs c t every time the dp procedure assessed this possibility this can be visualized as the bottom red line of the dp network of fig 2a 2 4 simulation models the optimization procedure is independent from the simulation sub routines so any model could be in theory plugged in as a first attempt we developed our own models for addressing the problem of nbs expansion and describe them in this section 2 4 1 hydrology this model aims to simulate the daily streamflow for a given a land use and land cover lulc arrangement which is the necessary input for scarcity cost and treatment cost calculation our simulation model is conceptual semi distributed and deterministic that performs explicit soil moisture accounting beven 2012 as depicted in fig 3 a the model perceives the whole watershed as the combined effect of hydrologic response units defined by seven lulc classes urban water forest conventional pastures conventional crops and the nbs counterparts of the last three restoration forests pastures with best management practices and crops with best management practices a conceptual diagram of the model is depicted in fig 3b in the land phase there are two water stocks the surface comprising all forms of water abstraction above the surface level and the subsurface comprising all forms of water storage below the surface level when the surface water stock is saturated the excess precipitation spills as runoff to the channel transport phase runoff routing is then performed by a nash cascade of linear reservoirs beven 2012 and stormflow in finally computed at the end of the cascade the output streamflow is therefore defined as the sum of baseflow and stormflow in the scs method of runoff estimation the surface water initial abstraction is set to be 20 of a storage variable rallison and miller 1981 soil conservation service 1972 we used this concept in our model by letting the percentage coefficient to be a calibration factor leading to the following relationship 11 i a m a x u i a f 25400 c n u 254 where iamax u is the surface water stock capacity for a given hydrologic response unit u in millimeters iaf is the calibration factor varying from 0 to 1 and cn u is the scs method parameter for a given hydrologic response unit u and is derived from the area prevalence of soil types other parameters of the model are the root zone depth rzd subsurface s water stock capacity sw max exfiltration maximum rate gw max number n of reservoirs of the channel s cascade and mean residence time k of the channel s reservoir cascade the root zone depth rzd u for a giver hydrologic response unit u is like cn a lulc sensitive parameter it defines how much of the subsurface s saturation capacity sw max is available to plant transpiration and is set in our model to have the same effective magnitude of the respective surface water stock capacity iamax u 2 4 2 scarcity cost the concept of scarcity cost is based on the loss of economic value consumer surplus when water is not available for the water urban user projected full supply marques et al 2006 jenkins et al 2004 the analysis of efficient choices by consumers is more difficult than for a firm given the former does not seek tangible profits in this sense water demanded by a household is not a particular quantity but rather a value dependent function griffin 2006 and this value increases with scarcity in this work we estimate such value dependent function through a marginal benefit or willingness to pay curve calculated based on observed data on quantity price and elasticity the function exhibits negative slope which captures the decreasing water value to the user and its integration yields the scarcity cost which is based on the consumer surplus lost when access to water is reduced or gained when access to water is increased while we agree this does not fully account to the total water value it is a lower bound and a concept applied elsewhere as in young and loomis 2014 and dolan et al 2021 following griffin 2006 a linear model of the marginal benefit curve is set to represent the water demand in the form of 12 w a p b in which w is the consumed water in a given time interval and p is the water price parameters a and b are then calculated based on estimated water price elasticity ε and paired observed data on average daily water consumption w 0 and the respective price paid by urban users p 0 hence 13 a ε w 0 p 0 b w 0 a p 0 in the problem of nbs expansion given a planning horizon the marginal benefits curve is not static but sensitive to the projections of price price elasticity and full water demand therefore for every stage an updated marginal benefit curve is calculated based on equations 12 and 13 also scarcity only occurs when the available water q is lower than the projected full water demand w p in such conditions the scarcity cost sc is defined by the area beneath the marginal benefit curve and between available water q and the projected full water demand w p eq 14 illustrated in fig 2d 14 s c 0 q w p p p p q w p 2 q w p in which sc is the daily scarcity cost calculated for every day of a given stage t and added to result in the total scarcity cost of the stage q is the daily available water calculated based on the simulated streamflow and time step duration daily p is the calculated water price for the simulated streamflow w p is the projected full water demand and p p is the projected water price this model assumes the observed urban average daily water consumption to be equal to the full water demand this assumption is reasonable when a the water system is operating at full capacity i e unconstrained most of time by water availability at the river b the users water tariffs to the local public utilities are low nearly 1 00 us m3 and c the water supply service coverage is high nearly 90 of the population hence as population grows water withdrawals should follow to maintain full supply conditions otherwise water scarcity is registered 2 4 3 treatment cost the treatment cost for the water utility is evaluated by a bio economic model we conceived it as an empirical surrogate model that links the fraction of soil conservative land cover in the watershed directly to costs of using chemicals at the water treatment plant see fig 2e for schematic illustration this model is based on two assumptions the first is that baseflow is the cleanest water produced by the system as highlighted by neary et al 2009 and a watershed fully covered by soil conservative land use produce baseflow most of the time so the unitary treatment cost are nearly the same in both situations 15 t c u q b t c u s c f 100 where tc u qb is the unitary treatment cost for baseflow and tc u scf100 is the unitary treatment cost for a watershed 100 covered by soil conservative land use the second assumption is that unitary treatment cost rises nonlinearly as the watershed is deforested or loses soil conservation practices such as nbs such behavior has been observed in reis 2004 and bassi 2002 hence the unitary treatment cost is considered inversely proportional to the fraction of soil conservative area of the watershed 16 t c u a s c f b in which tc u is the unitary treatment cost scf is soil conservative area fraction in percent and a and b are model parameters calibrated by curve fitting to data therefore 17 t c w t c u where tc is the treatment cost and w is the volume of treatment water while streamflow is a mixture of baseflow and stormflow both components are treated separately so the treatment costs were weighted by the flow type dominance thus considering equations 15 and 17 18 t c w q b q t c u q b 1 q b q t c u for a given stage t the treatment cost tc is computed on a daily basis using the projected full water demand wp as the volume of treatment water w however under water scarcity conditions the simulated streamflow q generated by the hydrological model is used instead 2 4 4 expansion cost for each decision set taken at any given stage t the costs of expanding nbs were also computed dividing it in installation costs and operation costs of the three different j nbs types 19 x c t j 1 3 x c i j t j 1 3 x c o j t in which xc t is the stage t expansion cost xci j t is the installation cost of the nbs type j and xco j t is the operation cost of the nbs type j the cost of installing new nbs reflects the costs of materials and labor this cost is represented by linear functions of expansion area for each type of nbs 20 x c i j a j i x a j b j i in which xci j is the installation cost of the nbs type j xa j is the expanded area of the nbs type j and a i j and b i j are parameters calibrated by curve fitting to data the operation cost is the annual payment for environmental services pes for a given nbs which may be composed by yearly estimated maintenance costs of best management practices or the opportunity cost negotiated for reforestation unlike installation costs it was conceived as functions of how much each nbs type has already being expanded and of how many years n the stage t period lasts 21 x c o j n a j o a j b j o in which xco j is the operation cost of the nbs type j a j is the total covered area of the nbs type j and a o j and b o j are parameters calibrated by curve fitting to data 2 5 software the modeling framework described above was programmed into a software called plans which stands for planning nature based solutions this software is free and open source under gnu gpl 3 0 license available on www github com ipo exe plans2 it takes the form of a python 3 package that rely on numpy pandas scipy and matplotlib libraries a terminal based user interface was developed addressing the needs of regular users although all software s dependencies are accessible via python s import statements 3 application and results applying the proposed framework to a full scale water problem is beyond the scope of this paper given constrains in resources and time datasets were used from a real watershed system as a reference the sinos river basin but we introduced modifications to create alternative development scenarios for benchmark testing also when data were not readily available we retrieved it from regional and national sources hence results are intended for presenting and discussing the framework itself but do not represent a conventional case study main water policiy changes in the sinos river basin included the creation of formal watershed committe in 1987 the creation of the pro sinos association of counties in 2007 which fostered conditions to improve regional planning to reduce pollution from 2008 to 2010 according to pedde et al 2015 several actions mostly funded by the federal government were implemented by the pro sinos committee basic sanitation plan for the sinos river basin sinos plan regional plan for the integrated management of solid wastes environmental education program collective educators this was followed by additional ruling by the state deparment of the environment fepam in 2006 which prohibited the installation of plants that discharge effluents into the sinos river all these actions led to the formal incorporation of water and environmentl agendas into public planning but public participation is still lacking and the insufficient sanitation planning and infrastructure is still a major contributor to the river pollution 3 1 datasets used and reference watershed the water system chosen as a main source of datasets was the sinos river watershed brazil which appears on fig 4 a a summary of datasets sources is presented in table a1 the urban water demand system is characterized by a cluster of ten cities counting up to 1 million inhabitants those cities depend on the water provided by the sinos river basin a water producing system comprising nearly 2 9 thousands square kilometers of drainage area the observed conservation status of the upstream watershed is relatively high souza et al 2020 with forests covering up to 67 of land leaving 17 of land to conventional crops 13 to conventional pastures and 3 to upland urban settlements fig 4b the conversion procedure of sartori et al 2005 and a map overlay analysis of lulc and soils classes allowed us to estimate the average scs curve number cn for each land cover class daily streamflow and precipitation datasets were retrieved from a national database the nearest gauging station is placed upstream the system s pumping station so we evaluated land use and soils prevalence in the gauged catchment for proper model calibration daily potential evapotranspiration pet datasets were not directly available so we derived it from annual and monthly datasets those observed datasets can be visualized in the appendix on fig a1a precipitation fig a1b potential evapotranspiration and fig a1d streamflow on the demand side we assessed datasets of population water consumption rates water tariffs and price elasticity the total population size is estimated in 1 million inhabitants at the year of 2020 from a local water utility reports we estimated the water consumption rate in 120 l hab day which yields 120 thousand cubic meters of water being pumped off the river every day at the year of 2020 this same water utility is currently issuing a water tariff of nearly us 1 00 m3 water price elasticity value of 0 17 for household water use were retrieved from a national data source andrade et al 1995 the treatment cost model was calibrated against data retrieved from a regional study of treatment costs in water utilities and the respective watershed land cover in the appendix fig a2 depicts the fitted model the available data and the linearized correlation coefficient currency values were adjusted to 2020 and computed in us dollars the expansion model cost for installation and operation of all nbs types was calibrated against datasets retrieved from a national databases and regional sources currency values were adjusted to 2020 and computed in us dollars installation costs for reforestation pastures with nbs and crops with nbs were of 34 3 43 6 and 24 3 us ha respectively for reforestation installation we considered the method of natural regeneration which is the cheapest costing only labor and fencing for nbs for crops and pastures we considered reported costs in labor and materials for terracing operation costs for reforestation pastures with nbs and crops with nbs were of 3 71 4 3 and 2 4 us ha year respectively the operation cost of reforestation were based on the average reported pes values in regional and national data sources for nbs for crops and pastures we considered 10 of installation costs for yearly maintenance of the installed terraces fig a3 in the appendix displays a bar chart for both installation and operation costs 3 2 alternative water systems and projections a designed set of three alternative water systems and future projections of 30 year long planning horizons going from 2020 to 2050 illustrate possible contexts and future outcomes of watershed conservation and water demands for the sake of result comparability the return rate for present value calculations were set in 7 for all scenarios fig 6 depicts the scenarios projections alongside the observed data used in the extrapolations for the climate projections see fig a4 in the appendix conservation status of watershed were divided in preserved condition with 66 forest cover and degraded condition with nearly 7 of forest cover the latter is representative of many watersheds of the major urbanized region of brazil which is in the south the southeast and the northeast coastline the biome of these regions the atlantic forest has been severely reduced from its original land cover prior to european colonization as presented in rezende et al 2018 which indicate that 28 is the average and some regions are far more degraded with virtually full deforestation hence the division adopted in the scenarios is representative of the extremes found in the biome region of the sinos river watershed in fact the full extension of the sinos river basin had its original 75 atlantic forest cover reduced to fragments covering 18 sos mata atlântica 2019 da costa et al 2017 mostly due to the mountainous terrain that prevented agricultural and urban expansion in the early phases of colonization however the situation is fragile and environmental risks persist given ongoing urbanization without proper planning nunes et al 2015 spilki and tundisi 2010 according to ioris et al 2008 system stability for the sinos showed wetter catchment conditions 1973 2001 with often higher and more variable flows which may reflect changes in soil use due to deforestation and urbanization scenario 1 preserved watershed under growing water scarcity pressures in this scenario the watershed s initial land use and land cover is the same as the observed in the upstream parts of the sinos river watershed which is very well preserved 66 of forest cover therefore the available area for nbs expansion is nearly 889 square kilometers the future projected climate gets drier and the projected water consumption grows exponentially driven by the compound effect of population and per capita consumption rate growth the public utilities water tariffs also grow fast in this scenario scenario 2 degraded watershed under growing water scarcity pressures this scenario has the same projected climate and demand growth as scenario 1 but with initial land use and land cover representing a degraded condition where forests accounts only for 6 7 of the landscape and conventional crops and pastures are dominant nearly 90 of basin land cover this results in a larger area available for nbs expansion almost 2 6 thousands square kilometers scenario 3 degraded watershed under decreasing water scarcity pressures in this scenario the watershed has the same initial land use and land cover representing a degraded condition as scenario 2 however water scarcity risks are decreasing since the climate gets wetter population growth rate diminishes and the per capita water consumption rate remains stable water tariffs also are stable over the planning horizon by following the principle of additionality wunder 2007 each scenario is run in two modes an optimal policy when the schedule for nbs expansion is optimized with the proposed method and a baseline policy when no nbs are implemented during the planning period 3 3 hydrology model calibration and sensitivity to land use change the hydrological model was calibrated against observed data from a reference time frame of six years 2010 2016 in a way the model was able to reproduce the observed streamflow regime the stock parameters were automatically calibrated by a random restart hill climbing optimization algorithm that minimize the root of mean squared error rmse of cumulative frequency curves of streamflow finally the number n of reservoirs of the channel s nash cascade was manually adjusted in the appendix fig a1 shows a panel of detailed calibration results including simulated flows of evapotranspiration and averaged land phase water stocks once calibrated the hydrology simulation model was subject to a land use and land cover change sensitivity analysis to provide a preliminary assessment of the hydrological performance of nbs we run the model under five different land use conditions beyond the observed reference land use 1 full expansion of reforestation with nbs 2 full expansion of pastures with nbs 3 full expansion of crops with nbs 4 50 deforestation to conventional crops and 5 90 deforestation to conventional crops results appear on fig 5 which shows cumulative frequency curves for the total streamflow the stormflow component the baseflow component and the baseflow dominance defined by the fraction of baseflow relative to total streamflow the model was sensitive to land use change from the range of moderate to low discharges of streamflow i e high exceedance probabilities as shown by the slightly farther apart curves in fig 5a in the range of higher streamflow discharges i e low exceedance probabilities the curves for all land use changes are nearly coincident fig 5a the expansion of nbs decreased simulated stormflow this can be noticed in fig 5b where the curves of nbs land use condition l1 l2 l3 all indicate less flow for a given exceedance probability compared to the curve of reference land use l0 in blue the curves in fig 5b also show that the land change to forests and nbs for pastures resulted in higher flow change compared to the change to nbs for crops on the other hand deforestation progressively increased stormflow i e overland runoff as shown in fig 5b by the l4 and l5 orange and red curves indicating higher flows for a given exceedance probability compared to the reference land use curve l0 in blue the expansion of nbs increased simulated baseflow this is demonstrated in fig 5c where the curves for nbs land use condition l1 l2 l3 all indicate more baseflow for a given exceedance probability compared to the curve of reference land use l0 in blue on the other hand deforestation progressively decreased baseflow i e infiltration shown in fig 5c by the l4 and l5 orange and red curves indicating lower flows for a given exceedance probability compared to the reference land use curve l0 in blue this effect is more significant for low flows higher exceedance probabilities fig 5c finally baseflow dominance was sensitive to land use change in all ranges fig 5d the expansion of nbs increased the dominance of baseflow which is noticed by the curves showing higher flow values than the reference land use curve in fig 5d for a given exceedance probability conversely the curves showing lower flow values than the reference land use in fig 5d indicate that deforestation progressively decreased baseflow dominance 3 4 optimal nbs expansion policies the optimal nbs expansion policies for the set of scenarios are depicted in fig 7 a along with the resulting watershed land use change fig 7b the system s costs for each planning stage fig 7c and the accumulated total system cost fig 7d the costs are further detailed in fig 8 currency values are presented in future value in the appendix table a2 provides the numerical results in present and future values under scenario 1 preserved watershed under growing water scarcity pressures the optimization procedure resulted in no nbs expansion which is the same as the baseline policy shown by the overlapping lines of expansion schedules in fig 7a i this means that investment in nbs is not worth from the perspective of direct benefits to the downstream water utility as a consequence land use and land cover remained the same during the planning horizon fig 7b i and the system s costs for the optimal policy also overlapped with the baseline policy fig 7c i and fig 7d i by the end of the planning horizon year of 2050 the system in scenario 1 presented an accumulated total cost of u 10 3 million fig 7d i which was mainly due to a rapid rise of the scarcity cost in the last ten years of the planning horizon from 2040 to 2050 in fig 7c i in fact the scarcity cost surpassed the treatment cost by the year 2040 and then dominated the total cost of the system by 2050 reaching the value of u 6 3 million year making the relative contribution of treatment cost marginal by 2050 u 0 2 million under scenario 2 degraded watershed under growing water scarcity pressures the optimization procedure determined that nbs for crops should be expanded at 20 of the available area this means that the highest return on investment is obtained when transforming nearly 520 km2 of conventional land by practices of soil and water conservation this is shown in fig 7a ii where the optimal schedule line detaches from the baseline policy the optimal policy determined a single stage leap of 20 nbs expansion that changed the land use of the watershed in the first stage of the planning horizon from 2020 to 2025 fig 7b ii the scarcity cost in scenario 2 has similar behavior as scenario 1 a fast rise near the end of the planning horizon reaching an annual cost of us 6 4 million by 2050 fig 7c ii however most likely due to the initial degraded watershed conditions scenario 2 has a u 1 7 million total investment in nbs expansion which resulted in a six fold drop in treatment costs relative to the baseline policy by the 2050 us 3 7 to 0 6 million this means that for every dollar invested in nbs in 2050 1 8 dollars could be saved in treatment costs as a consequence the accumulated total cost under the optimal policy is lower than under the baseline policy fig 7d ii from 2025 to 2030 and this difference increases along the planning horizon reaching us 12 6 million in 2050 which represents nearly 50 avoided costs relative to the baseline policy us 26 6 to 14 million given the total nbs expansion cost of us 1 7 million fig a4b this investment delivered an eight fold return in terms of avoided treatment costs us 14 2 million under scenario 3 degraded watershed under decreasing water scarcity pressures the optimization procedure resulted in a mix of nbs pastures and forests expanded to 30 of the initial available area fig 7a iii by 2050 nbs forests accounted for 20 and nbs for pastures accounted for 10 fig 7b iii by 2050 the optimal expansion policy expanded 10 of nbs for pastures in the first stage from 2020 to 2025 and hold the nbs forest expansions for later in the planning horizon 10 in the second stage from 2025 to 2030 and 10 more in the fourth stage from 2035 to 2040 the scarcity cost was negligible for both baseline and optimal policies never rising during the planning horizon fig 7c iii as water scarcity pressure drops in this scenario resulting from the combination of smaller population growth and stabilized water consumption per capita fig 6c and d treatment cost under the baseline policy rose up to us 2 3 million by the year of 2050 responding for most of the contribution 98 of the total accumulated cost of us 12 4 million for this policy the optimal policy reduced the total accumulated cost from 12 4 to 9 3 us million by investing up to us 7 3 million in nbs expansion along the planning period this expansion provided a seven fold drop in the accumulated treatment cost from us 12 2 to us 1 7 million which leaded to a total accumulated direct benefit of us 3 1 million nearly 25 of avoided total accumulated costs over the 30 year period relative to the baseline policy fig 7d iii since the total nbs expansion cost was us 7 3 million this investment delivered an average return of us 1 43 for every dollar invested in nbs in terms of avoided treatment costs us 10 5 million scenario 3 also made clear that even under a lower water demand pressure when compared to scenarios 1 and 2 nbs are relevant to meet objectives of lowering treatment costs 4 discussion we provided an optimization method that minimizes costs by allocating nbs expansion decisions along a planning horizon given a future projection our framework is able to determine the optimal expansion schedule when and how much to expand nbs from the perspective of direct benefits to a downstream water utility by developing simulation models and applying the proposed method to a set of alternative scenarios we demonstrate that the optimal schedule of nbs expansion depends on the initial land use and future drivers including population growth per capita consumption and climate change also we show that knowing how long delaying expansion decisions is key for efficiency since our method optimizes the present value of future costs planning ahead saves money the expansion schedule is an output from the modelling framework and a valuable information to water utilities and related investment institutions concerned with watershed pes and water security and efficiency the reduction of treatment cost was the greatest feature of nbs expansion in the scenarios it was worth confirming it might be a cost effective strategy for addressing water quality issues of degraded watersheds as pointed by kroeger et al 2019 price and heberling 2018 and bassi 2002 however our results suggest that the type of nbs chosen and the decision schedule may change depending on the water pressures of the system interestingly the accounting of scarcity cost introduced a tradeoff in the problem expansion of nbs in a well preserved watershed may not be worth the investment depending on how scarce and valuable water is this insight gained by applying the framework addresses the issues of nbs upscaling and financing wwap un water 2018 kroeger et al 2019 also it highlights the potential contribution of other water management measures not modelled to curb increase in water consumption which could be implemented in parallel with nbs additionally we found diminishing returns for nbs expansion in the scenarios when it was worth which is caused by the balance between a linear cost function the expansion cost and the non linear responses of scarcity cost treatment cost and land use impact in hydrology this finding suggest that decreasing returns of nbs expansion might be expected for any water system future scenario and even different simulation models from those we developed for some conditions like in scenario 1 the optimal policy might be not expanding at all on the side of hydrology the sensitivity assessment demonstrated that baseflow is expected to increase by nbs expansion but total streamflow however is expected to decrease for most of moderate to high discharge rates this relates to a higher capacity of forests and soil conservation practices to store water making it more available to infiltration and evapotranspiration those results are coherent with reported results from empirical studies of catchment hydrology in the same or similar biomes of the application basin cruz et al 2016 gomes et al 2012 moreover this is in agreement with the growing consensus of forest hydrology jackson et al 2009 ilstedt et al 2007 2016 ellison et al 2012 filoso et al 2017 although system complexity scale variability and the relatively few long term studies limit strong generalizations ellison et al 2012 liu et al 2017 zhang et al 2017 besides the expected increase in baseflow discharges the optimal nbs expansion schedules in the scenarios it was worth provided a negligible reduction in water scarcity costs relatively to the baseline policies this suggest that nbs expansion alone may not be robust enough to avoid scarcity costs and to prevent the impacts of climate change extreme events that triggers water scarcity we decided to demonstrate the framework focusing on 1 land use nbs types and on 2 a conventional pes scheme payments for landowners only to ensure the reproducibility of the framework in other watersheds and contexts the nbs chosen reflect choices that are more easily available to local landholders for which there is already a common knowledge base and for which there is more data available for modeling this allows us not only to provide not only better modeling results but also to focus on nbs that are already being discussed for implementation which makes the results more readily useful to current planning however as outlined in the methods section the simulation models used by us are independent from the optimization procedure and can be improved or simply replaced in further developments in fact this is the main strength of dynamic programming simulation models can be plugged as external routines this means that once unconstrained by data availability and computing power the framework might be customized for other types of nbs such as wetland restoration agricultural and urban retention pounds rural roads sediment traps etc such nbs types does not fit the conventional pes scheme since most are related to public goods e g roads or large areas e g floodplains there is also the possibility of merging our approach with spatial optimization strategies and tools like those revised by zhang and chui 2018 this would require modifying the hydrology model to a distributed version that could assess soil losses and the connectivity of forest patches providing biodiversity conservation metrics to the optimization problem as presented by domingue et al 2020 the treatment cost simulation model would also be changed to a more direct approach based on water quality modelling in the water bodies by determining the optimal nbs spatial arrangement at the decision level it would be possible to map the optimal nbs expansion schedule in the watershed along the planning horizon including conventional infrastructure expansion is another way of further improvement an artificial reservoir at the catchment outlet may change the overall performance of nbs by preventing the extreme low flow events to happen integrating such new feature in the capacity expansion optimization problem may leads to a hybrid green grey infrastructure approach as recommended by wwap un water 2018 despite financing many pes programmes citizens covered by water utilities are only a part of the downstream beneficiaries of watershed services given that nbs especially involving forests provide other ecosystem services which value is not considered here smith et al 2006 for instance soil conservation by nbs may lead to increased soil productivity a benefit to landowners and avoided dredging costs an indirect benefit to many downstream users sonneveld et al 2018 un water 2018 bassi 2002 while the motivation is to propose an useful tool for planners to improve the downstream upstream cycle depicted in fig 1 we realize that the partial valuation of the indirect use of nbs using the avoided cost method brander et al 2010 as used here still underestimates other values avoided treatment costs and avoided scarcity costs are appealing to water utilities but are a fraction of the use and non use values of nbs the determination of other values is broad and beyond the scope of the paper however the modelling framework can be relaxed to include new benefit components eq 2 and eq 3 to other users in the objective function of the optimization algorithm the proposed tool has the underlying assumption that the simulation models used are optimally fitted but any simulation model plugged into the dynamic programming would actually present model or epistemic uncertainties the problem of equifinality in hydrological modelling as summarised by beven 2006 postulates that the observed information available is not enough to justify a set of parameter values or even structural hypothesis of a model in other words the equifinality of a model implies that it is possible to get it right for the wrong reasons the solution for this problem as proposed by beven and binley 1992 is to hold a selection of behavioural models such model uncertainty might affect output of our framework and must be addressed in further developments a second important assumption is regarding the nbs values which can be quite broad while the choice of measuring the benefits based on avoided costs is a pragmatic easy to understand quantify and communicate approach which are all necessary for defining clear planning objectives it undervalues the nbs depending on the regional context finally future uncertainty is an important aspect of water resources planning that our framework is not designed to deal the optimal expansion is computed based on deterministic future projections therefore for a given case study the most probable optimal expansion schedule for nbs expansion might be better understood via exploratory analysis of scenarios haan et al 2016 indeed when considering future uncertainty planning nbs expansion and other decisions might be an adaptive strategy considering feedback loops of information in the process when the next stage arrives a new model run is executed with updated projections and boundary conditions walker et al 2010 loucks and van beek 2017 an attractive form of further improvement in this sense is integrating it with the haasnoot et al 2011 2012 2013 2014 approach for dynamic adaptive management buurman and babovic 2016 kingsborough et al 2016 this would put the nbs strategy under a broader set of decision options or adaptation pathways for maximizing future water security a main water governance context in the brazilian case involves the integration of water resources management and sanitation planning water management has the watershed committee as a key player in charge of approving the watershed plans and the water resources authority wra which issues the water permits to all users sanitation planning is undertaken by the public utility which requests a special water permit to the wra to withdraw water from the river so it is also considered a user in this context all initiatives to improve water quality and quantity would benefit both the utility with lower costs the urban users with lower tariffs and more security and the landholders which could be compensated as pointed by dalcin and marques 2020 the current management gap in this context is how to integrate sanitation and water resources management we expect our approach provides to the public utility and urban users a broader view of the watershed system and a greater motivation to take action beyond the city limits in order to improve water security 5 conclusions we provided a novel modellig framework to address the worldwide challenge of upscaling and financing nature based solutions in watersheds from the perspective of a downstream urban water utility the proposed method innovates by integrating optimization of sequential decision making hydrology modelling and hydroeconomics in a single modeling framework the application of it to three alternative water systems under different future projections allowed us to identify when and how land use in the watershed can be dynamically changed by nature based solutions to accommodate present and future water demands with economic efficiency practical implications of this work arise from the funding of watershed pes projects by institutions and governments which demand evidence based policies to support investment of public resources some policies and decisions can be devised with modeling tools including the one provided here which is both conceptually and procedurally flexible to allow continuous improvement from future research declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are grateful for the financial support provided by the brazilian national counsel of technological and scientific development cnpq grant number 132697 2018 1 and cnpq grant number 308549 2019 8 d myfiles elsevier enso 00105381 s ceediting gs3 appendix this section provides supplemental material to support our methods and results the figures and tables are referred in the main text fig a1 hydrology calibration panel the hydrological model was calibrated against observed data to adjust the simulated flow regime and reproduce the observed cumulative frequency curve r 0 996 shown in f and g a observed precipitation from porto alegre s weather station b simulated evapotranspiration flows c simulated water stocks both in surface and subsurface episodes of saturation occurs during the wet winters which is reflected in higher streamflow discharges shown in d fig a1 table a1 summary of datasets sources due to our scope we used data retrieved from these sources to built three alternative water systems in order to illustrate the proposed framework in action and discuss it an application to a real system by water resources managers would prioritize sampling data at least for treatment and expansion costs table a1 dataset source source range source type land use and land cover souza et al 2020 regional gis dataset soil classes ibge instituto brasileiro de geografia e estatística 2020 banco de dados de informações ambientais tema pedologia https bdiaweb ibge gov br consulta pedologia national official gis dataset streamflow agência nacional de águas ana 2018 hidroweb sistemas de informações hidrológicas http hidroweb ana gov br hidroweb national official dataset precipitation agência nacional de águas ana 2018 hidroweb sistemas de informações hidrológicas http hidroweb ana gov br hidroweb national official dataset evapotranspiration cordeiro a p a 2010 tendências climáticas das variáveis meteorológicas originais estimadas e das derivadas do balanço hídrico seriado no rio grande do sul universidade federal do rio grande do sul mota d a silveira f goederr o 1966 evapotranspiração potential no rio grande do sul pesquisa agropecuária brasileira 1 1 155 163 regional thesis population rio grande do sul fee estimativas populacionais municipais do rio grande do sul revisão 2018 fundação de economia e estatística 2018 regional official dataset water consumption novo hamburgo 2017 plano municipal de saneamento básico de novo hamburgo local official report water tariff novo hamburgo 2020 local official report treatment cost reis l cobertura florestal e custo do tratamento de águas em bacias hidrográficas de abastecimento público caso do manancial do município de piracicaba 2004 universidade de são paulo piracicaba 2004 http www teses usp br teses disponiveis 11 11150 tde 14122004 113308 regional thesis expansion costs agência nacional de águas ana 2020 programa produtor de água projetos https www ana gov br programas e projetos programa produtor de agua projetos griebeler et al 2000 benini and adeodato 2017 certi fundação centros de referência em tecnologias inovadoras plano de ação para ampliação do programa produtor de água do rio vermelho florianópolis 2018 and santa catarina sds elaboração do planejamento estratégico do programa produtor de água na bacia hidrográfica do rio cubatão curitiba 2019 national regional official and non official reports price elasticity andrade et al 1995 national non official report fig a2 treatment model calibration currency values are in us dollars data source reis 2004 fig a2 fig a3 expansion costs for each type of nbs a installation costs b operation costs currency values are in us dollars data sources ana 2020 griebeler et al 2000 benini and adeodato 2017 and santa catarina 2019 fig a3 fig a4 climate driver scenarios precipitation in a and potential evapotranspiration in b were projected based on preset future scenarios scenario 1 and 2 which are the same for the climate projects an increasing drier condition scenario 3 on the other hand projects an increasing wetter future the projected time series were generated by sampling observed years biased by the three wetness indicators depicted in c fig a4 table a2 optimal and baseline policies for scenario 1 2 and 3 currency are in million units of 2020 us and in future value table a2 scenario 1 stage t 2020 2025 2030 2035 2040 2045 2050 optimal policy state s 0 0 0 0 0 0 0 0 0 0 0 0 0 0 decision x 0 0 0 0 0 0 0 0 0 0 0 0 0 0 cost 0 000 0 282 0 234 0 368 0 748 2 118 6 548 10 299 scarcity cost 0 000 0 182 0 118 0 235 0 599 1 940 6 342 9 415 treatment cost 0 000 0 100 0 116 0 133 0 149 0 179 0 206 0 884 expansion cost 0 000 0 000 0 000 0 000 0 000 0 000 0 000 0 000 baseline policy cost 0 000 0 282 0 234 0 368 0 748 2 118 6 548 10 299 scarcity cost 0 000 0 182 0 118 0 235 0 599 1 940 6 342 9 415 treatment cost 0 000 0 100 0 116 0 133 0 149 0 179 0 206 0 884 expansion cost 0 000 0 000 0 000 0 000 0 000 0 000 0 000 0 000 scenario 2 stage t 2020 2025 2030 2035 2040 2045 2050 optimal policy state s 0 0 20 0 20 0 20 0 20 0 20 0 20 0 decision x 0 0 20 0 0 0 0 0 0 0 0 0 0 0 cost 0 000 1 945 0 599 0 748 1 142 2 554 7 042 14 030 scarcity cost 0 000 0 255 0 162 0 264 0 640 1 958 6 376 9 656 treatment cost 0 000 0 322 0 372 0 419 0 437 0 532 0 601 2 683 expansion cost 0 000 1 368 0 065 0 065 0 065 0 065 0 065 1 692 baseline policy cost 0 000 2 325 2 563 2 948 3 368 5 280 10 142 26 626 scarcity cost 0 000 0 257 0 177 0 272 0 653 1 966 6 391 9 716 treatment cost 0 000 2 068 2 386 2 676 2 715 3 314 3 751 16 910 expansion cost 0 000 0 000 0 000 0 000 0 000 0 000 0 000 0 000 scenario 3 stage t 2020 2025 2030 2035 2040 2045 2050 optimal policy state s 0 0 10 0 10 0 20 0 30 0 30 0 30 0 decision x 0 0 10 0 0 0 10 0 10 0 0 0 0 0 cost 0 000 1 736 1 766 0 957 2 218 1 257 1 355 9 288 scarcity cost 0 000 0 000 0 000 0 088 0 024 0 000 0 071 0 183 treatment cost 0 000 0 507 0 290 0 313 0 220 0 203 0 232 1 764 expansion cost 0 000 1 229 1 476 0 555 1 974 1 053 1 053 7 341 baseline policy cost 0 000 1 767 1 909 2 156 2 263 1 937 2 369 12 402 scarcity cost 0 000 0 000 0 000 0 088 0 020 0 000 0 070 0 178 treatment cost 0 000 1 767 1 909 2 068 2 243 1 937 2 299 12 224 expansion cost 0 000 0 000 0 000 0 000 0 000 0 000 0 000 0 000 
25590,how and when to upscale and finance nature based solutions in watersheds in order to improve downstream urban water security is a global concern in this paper we address such challenge by providing a novel method for planning the expansion of a chosen set of nature based solutions that couples optimization of sequential decision making hydrology modelling and hydroeconomics in a single modeling framework the benefits were considered the avoided costs of water scarcity and water treatment perceived downstream the optimal expansion schedule of decisions was then defined under the objective function of minimizing total cost we demonstrated the framework by applying it to a set of benchmark alternative scenarios for the sinos river watershed brazil we found that the optimal schedule of nature based solutions expansion is sensitive to the initial land use and future drivers treatment cost reduction was the greatest feature in some scenarios confirming it as a strategy for addressing water quality issues of degraded watersheds but the accounting of water scarcity cost introduced a tradeoff expansion of nature based solutions in well preserved watersheds may not worth the investment future research may improve current limitations in scope and models without structural changes in the framework practical implications of this work arise from the funding of watershed payments for environmental services projects by institutions and governments which demand evidence based policies to support investment of public resources graphical abstract image 1 keywords nature based solutions expansion planning direct benefits water utility abbreviations dp dynamic programming lulc land use and land cover nbs nature based solutions pes payments for ecosystem environmental services software availability name planning nature based solutions plans available on www github com ipo exe plans2 developer and contact information iporã possantti contact ipora possantti ufrgs br year first available 2020 software required python 3 6 dependencies numpy scipy pandas matplotlib availability and cost free and open source under gnu gpl 3 0 license program language python 1 introduction urban water security is a global concern since the risk of water crisis in cities escalates worldwide driven by forces both on the demand side such as the growth of economy and population and on the supply side such as the changes in climate and land use wada et al 2016 flörke et al 2018 wwap un water 2019 many sustainable development agendas highlight the use of nature based solutions nbs among available technologies for better reliability in urban water supply boelee et al 2017 keesstra et al 2018 raymond et al 2017 sonneveld et al 2018 wwap un water 2018 these are solutions inspired by natural processes that improve water management food production and ecosystems conservation iucn 2012 european commission 2015 wwap un water 2018 sonneveld et al 2018 comprising the already existing but scattered concepts of soil and water conservation practices and ecological economics eggermont et al 2015 nesshover et al 2017 hanson et al 2020 such attention on nbs is justified since it affects hydrology by changing land use and land cover lulc potentially enhancing water provision water quality control soil protection and hydro meteorological hazards mitigation foley et al 2005 kalantari et al 2018 keesstra et al 2018 sahani et al 2019 for instance in the case of forests not only less runoff is produced but the annual water yield tends to be relatively lower than conventional land use andréassian 2004 filoso et al 2017 zhang et al 2017 which is associated to higher evapotranspiration zhang et al 2001 streamflow of forested watersheds however tends to be of higher quality as water gets filtered by litter rich top soils neary et al 2009 likewise soil and water conservation practices in food production tends to reduce runoff remove nutrients and entrap sediments liu et al 2017 mekonnen et al 2015 the expected benefits perceived downstream is the main reason why cities deploy programmes such as payments for environmental services pes or watershed pes and induce upstream lulc change by the expansion of nbs brauman et al 2007 bremer et al 2016 dudley and stolton 2003 smith et al 2006 un water 2018 indeed watershed pes grows worldwide counting up to 387 programmes in 62 countries and annual payments estimated up to us 24 7 billion salzman et al 2018 in this sense valuation of the environmental services into direct benefits can be defined as the avoided costs perceived by the urban downstream water utility such as treatment costs and scarcity costs cunha et al 2016 kroeger 2013 postel and thompson 2005 price and heberling 2018 from the side of upstream landholders these water users would not accept as payment less than the operational cost plus the land use opportunity cost engel et al 2008 smith et al 2006 for wunder 2007 the efficiency of pes schemes should be assessed by evaluating the additional environmental services against a baseline scenario of not deploying the pes strategy following this reasoning decision making may rely on simulated hydrological performance an array of computer tools are available for this task such as swat neitsch et al 2011 and gis based like invest and others natural capital project 2016 carvalho santos et al 2014 grilli et al 2020 these allow quantitative assessment of environmental services francesconi et al 2016 tuppad et al 2010 cong et al 2020 and improvements in the decision making process arabi et al 2006 2007 strauch et al 2012 2013 ullrich and volk 2009 dynamic modelling and design may couple hydrology simulation into optimization algorithms babbar sebens et al 2013 chu et al 2010 zhang and chui 2018 zhang and song 2014 by introducing economic constraints the optimization algorithm maximizes cost effectiveness providing solutions that consider the costs of nbs expansion arabi et al 2006 beverly et al 2016 cibin chaubey 2015 yang and best 2015 beyond cost effectiveness optimization valuation of direct benefits using bio economic or hydro economic models allows the assessment of return on investments castro et al 2018 haavisto et al 2019 kroeger et al 2019 momblanch et al 2016 secchi et al 2007 none of the mentioned tools has yet included optimitization of decisions in long term planning horizons knowledge of when how and if it is necessary to deploy nbs in a watershed given a long term planning horizon is still limited this context allowed us to develop a modeling framework focused on urban water uses public utility decisions and upstream landholders all connected by the watershed s hydrologic response demand growth water value and costs associate to investment decisions on specific nbs despite the limited scope we address a key question faced by several cities driven by the necessity to secure future water supplies and negotiate solutions with upstream watershed users our aim is to push research forward by providing a novel method for planning the expansion of a chosen set of nbs that couples into a single and flexible modeling framework 1 optimization of sequential decision making 2 simulation of hydrological processes and 3 direct benefit assessment for downstream urban users for a better discussion we also illustrate the framework by applying it to three alternative water systems under different future projections the paper is structured as follows section 2 presents the details of the proposed method and models section 3 illustrates the framework in action by determining the optimal nbs expansion policies for three alternative water system under a set of future projections section 4 discuss the method strengths and limitations section 5 finally states our conclusions 2 methods 2 1 conceptual framework we designed a conceptual framework so the water system contains two subsystems as components the water producing system and the water consuming system as depicted in fig 1 on each side we represent drivers that evolve into the future following the water system s dynamics and transient state on the supply side the climate driver defines the water net inputs while the land cover and land use lulc drivers define the partitioning of incoming water among hydrological water stocks impacting the availability and quality of the water received by the consuming system on the demand side the social and economic drivers combine to produce the water consumption flow those drivers include population growth and consumption behavior under this framework a downstream urban water utility looks for water security from the perspective of this user the main nbs types of concern are reforestation best management practices for pastures and best management practices for crops because those are related to runoff generation and infiltration to groundwater also for this user investment in nbs expansion must yield an economic benefit when balancing avoided costs of both water scarcity and water treatment this user want to know if expanding those nbs is worthwhile given 1 a target water system 2 a planning horizon and 3 a future set of projections if it is when the expansion of a given nbs type or mixture of types should be made as well as their amount to ensure optimal cost effectiveness and a better financing schedule 2 2 direct benefit assessment following the principle of additionality wunder 2007 the direct benefit of the environmental services perceived by the downstream water utility is the avoided cost defined as the difference between the cost of the baseline policy not expanding nbs and the cost of expanding nbs 1 b c 0 c where b is the direct benefit c 0 is the cost of the baseline policy and c is the cost when expanding nbs all units are monetary the cost of the baseline policy is defined as the sum of water scarcity cost and water treatment cost 2 c 0 s c 0 t c 0 where sc 0 and tc 0 are the water scarcity cost and water treatment cost respectively both under the baseline policy by expanding nbs the cost c of the system is the sum of sc tc and xc which represent respectively the water scarcity cost treatment cost and nbs expansion cost 3 c s c t c x c note that other water supply costs not related to the expected nbs impact can be disregarded since it would be cancelled out in eq 1 in order to get a positive benefit eqs 1 3 requires the investment in nbs expansion xc to produce a sufficient decrease in the water scarcity cost sc and water treatment cost tc relatively to the respective costs in the baseline policy sc 0 and tc 0 so that c c 0 also to yield the highest possible benefit in a given planning horizon the nbs expansion should also be scheduled in a way that costs are minimized this optimal policy can be found by an algorithm that optimizes the expansion of the nbs having c sc tc and xc as decision variables computing sc tc and xc is a complex task requiring separate simulation models this is explained in the next sections 2 3 optimization of nbs expansion for a given set of nbs water system and defined future projection there is an optimal schedule to expand nbs including no expansion which means expanding a given area of those nbs at a given time so that the total direct benefits are maximized this yields a cost effective strategy not only because the nbs areas and their effect are adjusted following the water demand growth but also because it schedules investment decisions in time considering the time value of money we structured the problem of finding the optimal schedule as a sequential decision making process solved by dynamic programming for loucks and van beek 2017 this is known as the capacity expansion problem and has been proposed to solve problems of conventional water infrastructure expansion arancibia et al 2016 fraga et al 2017 rosenberg et al 2008 introduced by bellman 1957 dynamic programming dp is a computational technique for solving problems defined by sequential decision making processes the solution of the procedure is the optimal combination of decisions or the optimal policy hillier and liebman 2009 the procedure is based on the bellman s principle of optimality that the optimal policy must have optimal sub policies lew and mauch 2007 dp divides sequential decision making problems in stages states and decisions in each stage the system might be in a variety of discrete states which can change by decisions when moving forward or backward stage by stage those decisions are limited by the set of possible states and other constrains related to the problem a recursion relationship keeps track of previous optimal policies and it is used to find the optimal policy decision for each state when moving forward or backward along the stages once finished the recursion procedure it is possible to retrieve the optimal policy for the whole sequence of stages i e the whole problem this optimization technique allows the partitioning of the nbs expansion problem into multiple and simpler small problems which answer how much nbs should be expanded now considering the current conditions and past decisions the result from a decision in a given stage is then carried out to the next ones in the recursive term of the procedure this allows representation of time coupled decisions in problems where the objective function is discontinuous or non continuously differentiable which limits application of other optimization methods the tradeoff is that the state space needs to be discretized which limits the problem size due to computational constraints for the dp problem of nbs optimal expansion depicted in fig 2 a the planning horizon is divided in t planning cycles which are smaller periods of time t or stages the discrete possible states s of the system in any given stage t are defined by a set of positive integer values ranging from 0 to 100 and representing percent fractions of the initial available area to nbs expansion the total number of possible states s depends on expansion step resolution y which is a submultiple of 100 discrete expansions of nbs are represented by the decision variable x which adds up to the state s of the system after any stage t the optimal expansion schedule or optimal policy is determined by minimizing the total system s cost of all sequential stages which is computed by the following objective function 4 m i n i m i z e t 1 t c t s t x t f t s t 1 subject to 5 s t 1 s t x t t and to 6 s t 0 y n y 100 n n t in which t is the number of stages t is the stage index c t is the system s cost in present value for a given stage t given by eq 3 s t is the system s state as a percent fraction of available area for a given stage t x t is the expansion decision of nbs as a percent fraction of available area for a given stage t and y is a submultiple of 100 defined as the expansion step resolution the recursive relationship used to minimize the total system s cost is characterized by finding the minimum cost f to reach every discrete state s considering the accumulated cost after the first stage 7 f t s t 1 c t s t x t t 1 m i n c t s t x t f t 1 s t t 1 in which f t s t 1 is the minimum cost of achieving the state s t 1 at the end of stage t given the three types of nbs chosen here to expand at any stage t each x t expansion decision is a set of multiple combinations of individual nbs types that adds up to x t for example consider an expansion decision x t of 20 in a dp problem where the expansion step resolution y is of 10 one option may be expanding 10 of reforestation nbsf 10 of pastures with best management practices nbsp and 0 of crops with best management practices nbsc likewise another alternative is expanding 20 of reforestation and none of the remaining and so on for every policy decision x t we introduced in the dp an algorithm to compute the set d of all possible nbs expansion combinations this unfolds the x t variable into several x t d expansion decision sets fig 2b where d is a given combinations of x t 8 x t d n b s f d n b s p d n b s c d d d t hence x t d is the expansion decision set for the nbs expansion combination d in a given stage t d is the set of all possible nbs expansion combinations nbsf is the reforestation expansion nbsp is the pasture with best management practices expansion and nbsc is the crop with best management practices expansion for a given stage t and for any expansion decision set x t d the system s cost c t were computed in present value 9 c t f v t 1 r n t in which fv t is the future value of the system s cost r is the interest rate and n is the number of years from the beginning until the end of stage t from eq 3 the future value of the system s cost was set as the sum of three terms 10 f v t s c s t x t d t t c s t x t d t x c s t x t d t t in which sc is the water scarcity cost tc is the water treatment cost xc is the cost of nbs expansion s t is the system s state as a percent fraction of available area for a given stage t and x t d is the expansion decision set for the nbs expansion combination d in a given stage t to compute fv t for any system s state s t the decision set x t d is organized into a vector of land use classes of the system s watershed represented by lulc t this vector is then relayed to the simulation routines hydrology scarcity cost and expansion cost which feedbacks to the dp algorithm fig 2c the simulated series of streamflow on a daily time step for the full period of years comprised by the stage is then fed into a hydro economic model to simulate the stage s scarcity cost similarly a bio economic model computes the water supply treatment costs scarcity costs and treatment costs are computed on a daily time step and integrated for the stage period the expansion cost is finally computed directly from the updated lulc t given the baseline policy is a sequence of x decisions equal to zero expansion the system s cost in this case was computed as the accumulated costs c t every time the dp procedure assessed this possibility this can be visualized as the bottom red line of the dp network of fig 2a 2 4 simulation models the optimization procedure is independent from the simulation sub routines so any model could be in theory plugged in as a first attempt we developed our own models for addressing the problem of nbs expansion and describe them in this section 2 4 1 hydrology this model aims to simulate the daily streamflow for a given a land use and land cover lulc arrangement which is the necessary input for scarcity cost and treatment cost calculation our simulation model is conceptual semi distributed and deterministic that performs explicit soil moisture accounting beven 2012 as depicted in fig 3 a the model perceives the whole watershed as the combined effect of hydrologic response units defined by seven lulc classes urban water forest conventional pastures conventional crops and the nbs counterparts of the last three restoration forests pastures with best management practices and crops with best management practices a conceptual diagram of the model is depicted in fig 3b in the land phase there are two water stocks the surface comprising all forms of water abstraction above the surface level and the subsurface comprising all forms of water storage below the surface level when the surface water stock is saturated the excess precipitation spills as runoff to the channel transport phase runoff routing is then performed by a nash cascade of linear reservoirs beven 2012 and stormflow in finally computed at the end of the cascade the output streamflow is therefore defined as the sum of baseflow and stormflow in the scs method of runoff estimation the surface water initial abstraction is set to be 20 of a storage variable rallison and miller 1981 soil conservation service 1972 we used this concept in our model by letting the percentage coefficient to be a calibration factor leading to the following relationship 11 i a m a x u i a f 25400 c n u 254 where iamax u is the surface water stock capacity for a given hydrologic response unit u in millimeters iaf is the calibration factor varying from 0 to 1 and cn u is the scs method parameter for a given hydrologic response unit u and is derived from the area prevalence of soil types other parameters of the model are the root zone depth rzd subsurface s water stock capacity sw max exfiltration maximum rate gw max number n of reservoirs of the channel s cascade and mean residence time k of the channel s reservoir cascade the root zone depth rzd u for a giver hydrologic response unit u is like cn a lulc sensitive parameter it defines how much of the subsurface s saturation capacity sw max is available to plant transpiration and is set in our model to have the same effective magnitude of the respective surface water stock capacity iamax u 2 4 2 scarcity cost the concept of scarcity cost is based on the loss of economic value consumer surplus when water is not available for the water urban user projected full supply marques et al 2006 jenkins et al 2004 the analysis of efficient choices by consumers is more difficult than for a firm given the former does not seek tangible profits in this sense water demanded by a household is not a particular quantity but rather a value dependent function griffin 2006 and this value increases with scarcity in this work we estimate such value dependent function through a marginal benefit or willingness to pay curve calculated based on observed data on quantity price and elasticity the function exhibits negative slope which captures the decreasing water value to the user and its integration yields the scarcity cost which is based on the consumer surplus lost when access to water is reduced or gained when access to water is increased while we agree this does not fully account to the total water value it is a lower bound and a concept applied elsewhere as in young and loomis 2014 and dolan et al 2021 following griffin 2006 a linear model of the marginal benefit curve is set to represent the water demand in the form of 12 w a p b in which w is the consumed water in a given time interval and p is the water price parameters a and b are then calculated based on estimated water price elasticity ε and paired observed data on average daily water consumption w 0 and the respective price paid by urban users p 0 hence 13 a ε w 0 p 0 b w 0 a p 0 in the problem of nbs expansion given a planning horizon the marginal benefits curve is not static but sensitive to the projections of price price elasticity and full water demand therefore for every stage an updated marginal benefit curve is calculated based on equations 12 and 13 also scarcity only occurs when the available water q is lower than the projected full water demand w p in such conditions the scarcity cost sc is defined by the area beneath the marginal benefit curve and between available water q and the projected full water demand w p eq 14 illustrated in fig 2d 14 s c 0 q w p p p p q w p 2 q w p in which sc is the daily scarcity cost calculated for every day of a given stage t and added to result in the total scarcity cost of the stage q is the daily available water calculated based on the simulated streamflow and time step duration daily p is the calculated water price for the simulated streamflow w p is the projected full water demand and p p is the projected water price this model assumes the observed urban average daily water consumption to be equal to the full water demand this assumption is reasonable when a the water system is operating at full capacity i e unconstrained most of time by water availability at the river b the users water tariffs to the local public utilities are low nearly 1 00 us m3 and c the water supply service coverage is high nearly 90 of the population hence as population grows water withdrawals should follow to maintain full supply conditions otherwise water scarcity is registered 2 4 3 treatment cost the treatment cost for the water utility is evaluated by a bio economic model we conceived it as an empirical surrogate model that links the fraction of soil conservative land cover in the watershed directly to costs of using chemicals at the water treatment plant see fig 2e for schematic illustration this model is based on two assumptions the first is that baseflow is the cleanest water produced by the system as highlighted by neary et al 2009 and a watershed fully covered by soil conservative land use produce baseflow most of the time so the unitary treatment cost are nearly the same in both situations 15 t c u q b t c u s c f 100 where tc u qb is the unitary treatment cost for baseflow and tc u scf100 is the unitary treatment cost for a watershed 100 covered by soil conservative land use the second assumption is that unitary treatment cost rises nonlinearly as the watershed is deforested or loses soil conservation practices such as nbs such behavior has been observed in reis 2004 and bassi 2002 hence the unitary treatment cost is considered inversely proportional to the fraction of soil conservative area of the watershed 16 t c u a s c f b in which tc u is the unitary treatment cost scf is soil conservative area fraction in percent and a and b are model parameters calibrated by curve fitting to data therefore 17 t c w t c u where tc is the treatment cost and w is the volume of treatment water while streamflow is a mixture of baseflow and stormflow both components are treated separately so the treatment costs were weighted by the flow type dominance thus considering equations 15 and 17 18 t c w q b q t c u q b 1 q b q t c u for a given stage t the treatment cost tc is computed on a daily basis using the projected full water demand wp as the volume of treatment water w however under water scarcity conditions the simulated streamflow q generated by the hydrological model is used instead 2 4 4 expansion cost for each decision set taken at any given stage t the costs of expanding nbs were also computed dividing it in installation costs and operation costs of the three different j nbs types 19 x c t j 1 3 x c i j t j 1 3 x c o j t in which xc t is the stage t expansion cost xci j t is the installation cost of the nbs type j and xco j t is the operation cost of the nbs type j the cost of installing new nbs reflects the costs of materials and labor this cost is represented by linear functions of expansion area for each type of nbs 20 x c i j a j i x a j b j i in which xci j is the installation cost of the nbs type j xa j is the expanded area of the nbs type j and a i j and b i j are parameters calibrated by curve fitting to data the operation cost is the annual payment for environmental services pes for a given nbs which may be composed by yearly estimated maintenance costs of best management practices or the opportunity cost negotiated for reforestation unlike installation costs it was conceived as functions of how much each nbs type has already being expanded and of how many years n the stage t period lasts 21 x c o j n a j o a j b j o in which xco j is the operation cost of the nbs type j a j is the total covered area of the nbs type j and a o j and b o j are parameters calibrated by curve fitting to data 2 5 software the modeling framework described above was programmed into a software called plans which stands for planning nature based solutions this software is free and open source under gnu gpl 3 0 license available on www github com ipo exe plans2 it takes the form of a python 3 package that rely on numpy pandas scipy and matplotlib libraries a terminal based user interface was developed addressing the needs of regular users although all software s dependencies are accessible via python s import statements 3 application and results applying the proposed framework to a full scale water problem is beyond the scope of this paper given constrains in resources and time datasets were used from a real watershed system as a reference the sinos river basin but we introduced modifications to create alternative development scenarios for benchmark testing also when data were not readily available we retrieved it from regional and national sources hence results are intended for presenting and discussing the framework itself but do not represent a conventional case study main water policiy changes in the sinos river basin included the creation of formal watershed committe in 1987 the creation of the pro sinos association of counties in 2007 which fostered conditions to improve regional planning to reduce pollution from 2008 to 2010 according to pedde et al 2015 several actions mostly funded by the federal government were implemented by the pro sinos committee basic sanitation plan for the sinos river basin sinos plan regional plan for the integrated management of solid wastes environmental education program collective educators this was followed by additional ruling by the state deparment of the environment fepam in 2006 which prohibited the installation of plants that discharge effluents into the sinos river all these actions led to the formal incorporation of water and environmentl agendas into public planning but public participation is still lacking and the insufficient sanitation planning and infrastructure is still a major contributor to the river pollution 3 1 datasets used and reference watershed the water system chosen as a main source of datasets was the sinos river watershed brazil which appears on fig 4 a a summary of datasets sources is presented in table a1 the urban water demand system is characterized by a cluster of ten cities counting up to 1 million inhabitants those cities depend on the water provided by the sinos river basin a water producing system comprising nearly 2 9 thousands square kilometers of drainage area the observed conservation status of the upstream watershed is relatively high souza et al 2020 with forests covering up to 67 of land leaving 17 of land to conventional crops 13 to conventional pastures and 3 to upland urban settlements fig 4b the conversion procedure of sartori et al 2005 and a map overlay analysis of lulc and soils classes allowed us to estimate the average scs curve number cn for each land cover class daily streamflow and precipitation datasets were retrieved from a national database the nearest gauging station is placed upstream the system s pumping station so we evaluated land use and soils prevalence in the gauged catchment for proper model calibration daily potential evapotranspiration pet datasets were not directly available so we derived it from annual and monthly datasets those observed datasets can be visualized in the appendix on fig a1a precipitation fig a1b potential evapotranspiration and fig a1d streamflow on the demand side we assessed datasets of population water consumption rates water tariffs and price elasticity the total population size is estimated in 1 million inhabitants at the year of 2020 from a local water utility reports we estimated the water consumption rate in 120 l hab day which yields 120 thousand cubic meters of water being pumped off the river every day at the year of 2020 this same water utility is currently issuing a water tariff of nearly us 1 00 m3 water price elasticity value of 0 17 for household water use were retrieved from a national data source andrade et al 1995 the treatment cost model was calibrated against data retrieved from a regional study of treatment costs in water utilities and the respective watershed land cover in the appendix fig a2 depicts the fitted model the available data and the linearized correlation coefficient currency values were adjusted to 2020 and computed in us dollars the expansion model cost for installation and operation of all nbs types was calibrated against datasets retrieved from a national databases and regional sources currency values were adjusted to 2020 and computed in us dollars installation costs for reforestation pastures with nbs and crops with nbs were of 34 3 43 6 and 24 3 us ha respectively for reforestation installation we considered the method of natural regeneration which is the cheapest costing only labor and fencing for nbs for crops and pastures we considered reported costs in labor and materials for terracing operation costs for reforestation pastures with nbs and crops with nbs were of 3 71 4 3 and 2 4 us ha year respectively the operation cost of reforestation were based on the average reported pes values in regional and national data sources for nbs for crops and pastures we considered 10 of installation costs for yearly maintenance of the installed terraces fig a3 in the appendix displays a bar chart for both installation and operation costs 3 2 alternative water systems and projections a designed set of three alternative water systems and future projections of 30 year long planning horizons going from 2020 to 2050 illustrate possible contexts and future outcomes of watershed conservation and water demands for the sake of result comparability the return rate for present value calculations were set in 7 for all scenarios fig 6 depicts the scenarios projections alongside the observed data used in the extrapolations for the climate projections see fig a4 in the appendix conservation status of watershed were divided in preserved condition with 66 forest cover and degraded condition with nearly 7 of forest cover the latter is representative of many watersheds of the major urbanized region of brazil which is in the south the southeast and the northeast coastline the biome of these regions the atlantic forest has been severely reduced from its original land cover prior to european colonization as presented in rezende et al 2018 which indicate that 28 is the average and some regions are far more degraded with virtually full deforestation hence the division adopted in the scenarios is representative of the extremes found in the biome region of the sinos river watershed in fact the full extension of the sinos river basin had its original 75 atlantic forest cover reduced to fragments covering 18 sos mata atlântica 2019 da costa et al 2017 mostly due to the mountainous terrain that prevented agricultural and urban expansion in the early phases of colonization however the situation is fragile and environmental risks persist given ongoing urbanization without proper planning nunes et al 2015 spilki and tundisi 2010 according to ioris et al 2008 system stability for the sinos showed wetter catchment conditions 1973 2001 with often higher and more variable flows which may reflect changes in soil use due to deforestation and urbanization scenario 1 preserved watershed under growing water scarcity pressures in this scenario the watershed s initial land use and land cover is the same as the observed in the upstream parts of the sinos river watershed which is very well preserved 66 of forest cover therefore the available area for nbs expansion is nearly 889 square kilometers the future projected climate gets drier and the projected water consumption grows exponentially driven by the compound effect of population and per capita consumption rate growth the public utilities water tariffs also grow fast in this scenario scenario 2 degraded watershed under growing water scarcity pressures this scenario has the same projected climate and demand growth as scenario 1 but with initial land use and land cover representing a degraded condition where forests accounts only for 6 7 of the landscape and conventional crops and pastures are dominant nearly 90 of basin land cover this results in a larger area available for nbs expansion almost 2 6 thousands square kilometers scenario 3 degraded watershed under decreasing water scarcity pressures in this scenario the watershed has the same initial land use and land cover representing a degraded condition as scenario 2 however water scarcity risks are decreasing since the climate gets wetter population growth rate diminishes and the per capita water consumption rate remains stable water tariffs also are stable over the planning horizon by following the principle of additionality wunder 2007 each scenario is run in two modes an optimal policy when the schedule for nbs expansion is optimized with the proposed method and a baseline policy when no nbs are implemented during the planning period 3 3 hydrology model calibration and sensitivity to land use change the hydrological model was calibrated against observed data from a reference time frame of six years 2010 2016 in a way the model was able to reproduce the observed streamflow regime the stock parameters were automatically calibrated by a random restart hill climbing optimization algorithm that minimize the root of mean squared error rmse of cumulative frequency curves of streamflow finally the number n of reservoirs of the channel s nash cascade was manually adjusted in the appendix fig a1 shows a panel of detailed calibration results including simulated flows of evapotranspiration and averaged land phase water stocks once calibrated the hydrology simulation model was subject to a land use and land cover change sensitivity analysis to provide a preliminary assessment of the hydrological performance of nbs we run the model under five different land use conditions beyond the observed reference land use 1 full expansion of reforestation with nbs 2 full expansion of pastures with nbs 3 full expansion of crops with nbs 4 50 deforestation to conventional crops and 5 90 deforestation to conventional crops results appear on fig 5 which shows cumulative frequency curves for the total streamflow the stormflow component the baseflow component and the baseflow dominance defined by the fraction of baseflow relative to total streamflow the model was sensitive to land use change from the range of moderate to low discharges of streamflow i e high exceedance probabilities as shown by the slightly farther apart curves in fig 5a in the range of higher streamflow discharges i e low exceedance probabilities the curves for all land use changes are nearly coincident fig 5a the expansion of nbs decreased simulated stormflow this can be noticed in fig 5b where the curves of nbs land use condition l1 l2 l3 all indicate less flow for a given exceedance probability compared to the curve of reference land use l0 in blue the curves in fig 5b also show that the land change to forests and nbs for pastures resulted in higher flow change compared to the change to nbs for crops on the other hand deforestation progressively increased stormflow i e overland runoff as shown in fig 5b by the l4 and l5 orange and red curves indicating higher flows for a given exceedance probability compared to the reference land use curve l0 in blue the expansion of nbs increased simulated baseflow this is demonstrated in fig 5c where the curves for nbs land use condition l1 l2 l3 all indicate more baseflow for a given exceedance probability compared to the curve of reference land use l0 in blue on the other hand deforestation progressively decreased baseflow i e infiltration shown in fig 5c by the l4 and l5 orange and red curves indicating lower flows for a given exceedance probability compared to the reference land use curve l0 in blue this effect is more significant for low flows higher exceedance probabilities fig 5c finally baseflow dominance was sensitive to land use change in all ranges fig 5d the expansion of nbs increased the dominance of baseflow which is noticed by the curves showing higher flow values than the reference land use curve in fig 5d for a given exceedance probability conversely the curves showing lower flow values than the reference land use in fig 5d indicate that deforestation progressively decreased baseflow dominance 3 4 optimal nbs expansion policies the optimal nbs expansion policies for the set of scenarios are depicted in fig 7 a along with the resulting watershed land use change fig 7b the system s costs for each planning stage fig 7c and the accumulated total system cost fig 7d the costs are further detailed in fig 8 currency values are presented in future value in the appendix table a2 provides the numerical results in present and future values under scenario 1 preserved watershed under growing water scarcity pressures the optimization procedure resulted in no nbs expansion which is the same as the baseline policy shown by the overlapping lines of expansion schedules in fig 7a i this means that investment in nbs is not worth from the perspective of direct benefits to the downstream water utility as a consequence land use and land cover remained the same during the planning horizon fig 7b i and the system s costs for the optimal policy also overlapped with the baseline policy fig 7c i and fig 7d i by the end of the planning horizon year of 2050 the system in scenario 1 presented an accumulated total cost of u 10 3 million fig 7d i which was mainly due to a rapid rise of the scarcity cost in the last ten years of the planning horizon from 2040 to 2050 in fig 7c i in fact the scarcity cost surpassed the treatment cost by the year 2040 and then dominated the total cost of the system by 2050 reaching the value of u 6 3 million year making the relative contribution of treatment cost marginal by 2050 u 0 2 million under scenario 2 degraded watershed under growing water scarcity pressures the optimization procedure determined that nbs for crops should be expanded at 20 of the available area this means that the highest return on investment is obtained when transforming nearly 520 km2 of conventional land by practices of soil and water conservation this is shown in fig 7a ii where the optimal schedule line detaches from the baseline policy the optimal policy determined a single stage leap of 20 nbs expansion that changed the land use of the watershed in the first stage of the planning horizon from 2020 to 2025 fig 7b ii the scarcity cost in scenario 2 has similar behavior as scenario 1 a fast rise near the end of the planning horizon reaching an annual cost of us 6 4 million by 2050 fig 7c ii however most likely due to the initial degraded watershed conditions scenario 2 has a u 1 7 million total investment in nbs expansion which resulted in a six fold drop in treatment costs relative to the baseline policy by the 2050 us 3 7 to 0 6 million this means that for every dollar invested in nbs in 2050 1 8 dollars could be saved in treatment costs as a consequence the accumulated total cost under the optimal policy is lower than under the baseline policy fig 7d ii from 2025 to 2030 and this difference increases along the planning horizon reaching us 12 6 million in 2050 which represents nearly 50 avoided costs relative to the baseline policy us 26 6 to 14 million given the total nbs expansion cost of us 1 7 million fig a4b this investment delivered an eight fold return in terms of avoided treatment costs us 14 2 million under scenario 3 degraded watershed under decreasing water scarcity pressures the optimization procedure resulted in a mix of nbs pastures and forests expanded to 30 of the initial available area fig 7a iii by 2050 nbs forests accounted for 20 and nbs for pastures accounted for 10 fig 7b iii by 2050 the optimal expansion policy expanded 10 of nbs for pastures in the first stage from 2020 to 2025 and hold the nbs forest expansions for later in the planning horizon 10 in the second stage from 2025 to 2030 and 10 more in the fourth stage from 2035 to 2040 the scarcity cost was negligible for both baseline and optimal policies never rising during the planning horizon fig 7c iii as water scarcity pressure drops in this scenario resulting from the combination of smaller population growth and stabilized water consumption per capita fig 6c and d treatment cost under the baseline policy rose up to us 2 3 million by the year of 2050 responding for most of the contribution 98 of the total accumulated cost of us 12 4 million for this policy the optimal policy reduced the total accumulated cost from 12 4 to 9 3 us million by investing up to us 7 3 million in nbs expansion along the planning period this expansion provided a seven fold drop in the accumulated treatment cost from us 12 2 to us 1 7 million which leaded to a total accumulated direct benefit of us 3 1 million nearly 25 of avoided total accumulated costs over the 30 year period relative to the baseline policy fig 7d iii since the total nbs expansion cost was us 7 3 million this investment delivered an average return of us 1 43 for every dollar invested in nbs in terms of avoided treatment costs us 10 5 million scenario 3 also made clear that even under a lower water demand pressure when compared to scenarios 1 and 2 nbs are relevant to meet objectives of lowering treatment costs 4 discussion we provided an optimization method that minimizes costs by allocating nbs expansion decisions along a planning horizon given a future projection our framework is able to determine the optimal expansion schedule when and how much to expand nbs from the perspective of direct benefits to a downstream water utility by developing simulation models and applying the proposed method to a set of alternative scenarios we demonstrate that the optimal schedule of nbs expansion depends on the initial land use and future drivers including population growth per capita consumption and climate change also we show that knowing how long delaying expansion decisions is key for efficiency since our method optimizes the present value of future costs planning ahead saves money the expansion schedule is an output from the modelling framework and a valuable information to water utilities and related investment institutions concerned with watershed pes and water security and efficiency the reduction of treatment cost was the greatest feature of nbs expansion in the scenarios it was worth confirming it might be a cost effective strategy for addressing water quality issues of degraded watersheds as pointed by kroeger et al 2019 price and heberling 2018 and bassi 2002 however our results suggest that the type of nbs chosen and the decision schedule may change depending on the water pressures of the system interestingly the accounting of scarcity cost introduced a tradeoff in the problem expansion of nbs in a well preserved watershed may not be worth the investment depending on how scarce and valuable water is this insight gained by applying the framework addresses the issues of nbs upscaling and financing wwap un water 2018 kroeger et al 2019 also it highlights the potential contribution of other water management measures not modelled to curb increase in water consumption which could be implemented in parallel with nbs additionally we found diminishing returns for nbs expansion in the scenarios when it was worth which is caused by the balance between a linear cost function the expansion cost and the non linear responses of scarcity cost treatment cost and land use impact in hydrology this finding suggest that decreasing returns of nbs expansion might be expected for any water system future scenario and even different simulation models from those we developed for some conditions like in scenario 1 the optimal policy might be not expanding at all on the side of hydrology the sensitivity assessment demonstrated that baseflow is expected to increase by nbs expansion but total streamflow however is expected to decrease for most of moderate to high discharge rates this relates to a higher capacity of forests and soil conservation practices to store water making it more available to infiltration and evapotranspiration those results are coherent with reported results from empirical studies of catchment hydrology in the same or similar biomes of the application basin cruz et al 2016 gomes et al 2012 moreover this is in agreement with the growing consensus of forest hydrology jackson et al 2009 ilstedt et al 2007 2016 ellison et al 2012 filoso et al 2017 although system complexity scale variability and the relatively few long term studies limit strong generalizations ellison et al 2012 liu et al 2017 zhang et al 2017 besides the expected increase in baseflow discharges the optimal nbs expansion schedules in the scenarios it was worth provided a negligible reduction in water scarcity costs relatively to the baseline policies this suggest that nbs expansion alone may not be robust enough to avoid scarcity costs and to prevent the impacts of climate change extreme events that triggers water scarcity we decided to demonstrate the framework focusing on 1 land use nbs types and on 2 a conventional pes scheme payments for landowners only to ensure the reproducibility of the framework in other watersheds and contexts the nbs chosen reflect choices that are more easily available to local landholders for which there is already a common knowledge base and for which there is more data available for modeling this allows us not only to provide not only better modeling results but also to focus on nbs that are already being discussed for implementation which makes the results more readily useful to current planning however as outlined in the methods section the simulation models used by us are independent from the optimization procedure and can be improved or simply replaced in further developments in fact this is the main strength of dynamic programming simulation models can be plugged as external routines this means that once unconstrained by data availability and computing power the framework might be customized for other types of nbs such as wetland restoration agricultural and urban retention pounds rural roads sediment traps etc such nbs types does not fit the conventional pes scheme since most are related to public goods e g roads or large areas e g floodplains there is also the possibility of merging our approach with spatial optimization strategies and tools like those revised by zhang and chui 2018 this would require modifying the hydrology model to a distributed version that could assess soil losses and the connectivity of forest patches providing biodiversity conservation metrics to the optimization problem as presented by domingue et al 2020 the treatment cost simulation model would also be changed to a more direct approach based on water quality modelling in the water bodies by determining the optimal nbs spatial arrangement at the decision level it would be possible to map the optimal nbs expansion schedule in the watershed along the planning horizon including conventional infrastructure expansion is another way of further improvement an artificial reservoir at the catchment outlet may change the overall performance of nbs by preventing the extreme low flow events to happen integrating such new feature in the capacity expansion optimization problem may leads to a hybrid green grey infrastructure approach as recommended by wwap un water 2018 despite financing many pes programmes citizens covered by water utilities are only a part of the downstream beneficiaries of watershed services given that nbs especially involving forests provide other ecosystem services which value is not considered here smith et al 2006 for instance soil conservation by nbs may lead to increased soil productivity a benefit to landowners and avoided dredging costs an indirect benefit to many downstream users sonneveld et al 2018 un water 2018 bassi 2002 while the motivation is to propose an useful tool for planners to improve the downstream upstream cycle depicted in fig 1 we realize that the partial valuation of the indirect use of nbs using the avoided cost method brander et al 2010 as used here still underestimates other values avoided treatment costs and avoided scarcity costs are appealing to water utilities but are a fraction of the use and non use values of nbs the determination of other values is broad and beyond the scope of the paper however the modelling framework can be relaxed to include new benefit components eq 2 and eq 3 to other users in the objective function of the optimization algorithm the proposed tool has the underlying assumption that the simulation models used are optimally fitted but any simulation model plugged into the dynamic programming would actually present model or epistemic uncertainties the problem of equifinality in hydrological modelling as summarised by beven 2006 postulates that the observed information available is not enough to justify a set of parameter values or even structural hypothesis of a model in other words the equifinality of a model implies that it is possible to get it right for the wrong reasons the solution for this problem as proposed by beven and binley 1992 is to hold a selection of behavioural models such model uncertainty might affect output of our framework and must be addressed in further developments a second important assumption is regarding the nbs values which can be quite broad while the choice of measuring the benefits based on avoided costs is a pragmatic easy to understand quantify and communicate approach which are all necessary for defining clear planning objectives it undervalues the nbs depending on the regional context finally future uncertainty is an important aspect of water resources planning that our framework is not designed to deal the optimal expansion is computed based on deterministic future projections therefore for a given case study the most probable optimal expansion schedule for nbs expansion might be better understood via exploratory analysis of scenarios haan et al 2016 indeed when considering future uncertainty planning nbs expansion and other decisions might be an adaptive strategy considering feedback loops of information in the process when the next stage arrives a new model run is executed with updated projections and boundary conditions walker et al 2010 loucks and van beek 2017 an attractive form of further improvement in this sense is integrating it with the haasnoot et al 2011 2012 2013 2014 approach for dynamic adaptive management buurman and babovic 2016 kingsborough et al 2016 this would put the nbs strategy under a broader set of decision options or adaptation pathways for maximizing future water security a main water governance context in the brazilian case involves the integration of water resources management and sanitation planning water management has the watershed committee as a key player in charge of approving the watershed plans and the water resources authority wra which issues the water permits to all users sanitation planning is undertaken by the public utility which requests a special water permit to the wra to withdraw water from the river so it is also considered a user in this context all initiatives to improve water quality and quantity would benefit both the utility with lower costs the urban users with lower tariffs and more security and the landholders which could be compensated as pointed by dalcin and marques 2020 the current management gap in this context is how to integrate sanitation and water resources management we expect our approach provides to the public utility and urban users a broader view of the watershed system and a greater motivation to take action beyond the city limits in order to improve water security 5 conclusions we provided a novel modellig framework to address the worldwide challenge of upscaling and financing nature based solutions in watersheds from the perspective of a downstream urban water utility the proposed method innovates by integrating optimization of sequential decision making hydrology modelling and hydroeconomics in a single modeling framework the application of it to three alternative water systems under different future projections allowed us to identify when and how land use in the watershed can be dynamically changed by nature based solutions to accommodate present and future water demands with economic efficiency practical implications of this work arise from the funding of watershed pes projects by institutions and governments which demand evidence based policies to support investment of public resources some policies and decisions can be devised with modeling tools including the one provided here which is both conceptually and procedurally flexible to allow continuous improvement from future research declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are grateful for the financial support provided by the brazilian national counsel of technological and scientific development cnpq grant number 132697 2018 1 and cnpq grant number 308549 2019 8 d myfiles elsevier enso 00105381 s ceediting gs3 appendix this section provides supplemental material to support our methods and results the figures and tables are referred in the main text fig a1 hydrology calibration panel the hydrological model was calibrated against observed data to adjust the simulated flow regime and reproduce the observed cumulative frequency curve r 0 996 shown in f and g a observed precipitation from porto alegre s weather station b simulated evapotranspiration flows c simulated water stocks both in surface and subsurface episodes of saturation occurs during the wet winters which is reflected in higher streamflow discharges shown in d fig a1 table a1 summary of datasets sources due to our scope we used data retrieved from these sources to built three alternative water systems in order to illustrate the proposed framework in action and discuss it an application to a real system by water resources managers would prioritize sampling data at least for treatment and expansion costs table a1 dataset source source range source type land use and land cover souza et al 2020 regional gis dataset soil classes ibge instituto brasileiro de geografia e estatística 2020 banco de dados de informações ambientais tema pedologia https bdiaweb ibge gov br consulta pedologia national official gis dataset streamflow agência nacional de águas ana 2018 hidroweb sistemas de informações hidrológicas http hidroweb ana gov br hidroweb national official dataset precipitation agência nacional de águas ana 2018 hidroweb sistemas de informações hidrológicas http hidroweb ana gov br hidroweb national official dataset evapotranspiration cordeiro a p a 2010 tendências climáticas das variáveis meteorológicas originais estimadas e das derivadas do balanço hídrico seriado no rio grande do sul universidade federal do rio grande do sul mota d a silveira f goederr o 1966 evapotranspiração potential no rio grande do sul pesquisa agropecuária brasileira 1 1 155 163 regional thesis population rio grande do sul fee estimativas populacionais municipais do rio grande do sul revisão 2018 fundação de economia e estatística 2018 regional official dataset water consumption novo hamburgo 2017 plano municipal de saneamento básico de novo hamburgo local official report water tariff novo hamburgo 2020 local official report treatment cost reis l cobertura florestal e custo do tratamento de águas em bacias hidrográficas de abastecimento público caso do manancial do município de piracicaba 2004 universidade de são paulo piracicaba 2004 http www teses usp br teses disponiveis 11 11150 tde 14122004 113308 regional thesis expansion costs agência nacional de águas ana 2020 programa produtor de água projetos https www ana gov br programas e projetos programa produtor de agua projetos griebeler et al 2000 benini and adeodato 2017 certi fundação centros de referência em tecnologias inovadoras plano de ação para ampliação do programa produtor de água do rio vermelho florianópolis 2018 and santa catarina sds elaboração do planejamento estratégico do programa produtor de água na bacia hidrográfica do rio cubatão curitiba 2019 national regional official and non official reports price elasticity andrade et al 1995 national non official report fig a2 treatment model calibration currency values are in us dollars data source reis 2004 fig a2 fig a3 expansion costs for each type of nbs a installation costs b operation costs currency values are in us dollars data sources ana 2020 griebeler et al 2000 benini and adeodato 2017 and santa catarina 2019 fig a3 fig a4 climate driver scenarios precipitation in a and potential evapotranspiration in b were projected based on preset future scenarios scenario 1 and 2 which are the same for the climate projects an increasing drier condition scenario 3 on the other hand projects an increasing wetter future the projected time series were generated by sampling observed years biased by the three wetness indicators depicted in c fig a4 table a2 optimal and baseline policies for scenario 1 2 and 3 currency are in million units of 2020 us and in future value table a2 scenario 1 stage t 2020 2025 2030 2035 2040 2045 2050 optimal policy state s 0 0 0 0 0 0 0 0 0 0 0 0 0 0 decision x 0 0 0 0 0 0 0 0 0 0 0 0 0 0 cost 0 000 0 282 0 234 0 368 0 748 2 118 6 548 10 299 scarcity cost 0 000 0 182 0 118 0 235 0 599 1 940 6 342 9 415 treatment cost 0 000 0 100 0 116 0 133 0 149 0 179 0 206 0 884 expansion cost 0 000 0 000 0 000 0 000 0 000 0 000 0 000 0 000 baseline policy cost 0 000 0 282 0 234 0 368 0 748 2 118 6 548 10 299 scarcity cost 0 000 0 182 0 118 0 235 0 599 1 940 6 342 9 415 treatment cost 0 000 0 100 0 116 0 133 0 149 0 179 0 206 0 884 expansion cost 0 000 0 000 0 000 0 000 0 000 0 000 0 000 0 000 scenario 2 stage t 2020 2025 2030 2035 2040 2045 2050 optimal policy state s 0 0 20 0 20 0 20 0 20 0 20 0 20 0 decision x 0 0 20 0 0 0 0 0 0 0 0 0 0 0 cost 0 000 1 945 0 599 0 748 1 142 2 554 7 042 14 030 scarcity cost 0 000 0 255 0 162 0 264 0 640 1 958 6 376 9 656 treatment cost 0 000 0 322 0 372 0 419 0 437 0 532 0 601 2 683 expansion cost 0 000 1 368 0 065 0 065 0 065 0 065 0 065 1 692 baseline policy cost 0 000 2 325 2 563 2 948 3 368 5 280 10 142 26 626 scarcity cost 0 000 0 257 0 177 0 272 0 653 1 966 6 391 9 716 treatment cost 0 000 2 068 2 386 2 676 2 715 3 314 3 751 16 910 expansion cost 0 000 0 000 0 000 0 000 0 000 0 000 0 000 0 000 scenario 3 stage t 2020 2025 2030 2035 2040 2045 2050 optimal policy state s 0 0 10 0 10 0 20 0 30 0 30 0 30 0 decision x 0 0 10 0 0 0 10 0 10 0 0 0 0 0 cost 0 000 1 736 1 766 0 957 2 218 1 257 1 355 9 288 scarcity cost 0 000 0 000 0 000 0 088 0 024 0 000 0 071 0 183 treatment cost 0 000 0 507 0 290 0 313 0 220 0 203 0 232 1 764 expansion cost 0 000 1 229 1 476 0 555 1 974 1 053 1 053 7 341 baseline policy cost 0 000 1 767 1 909 2 156 2 263 1 937 2 369 12 402 scarcity cost 0 000 0 000 0 000 0 088 0 020 0 000 0 070 0 178 treatment cost 0 000 1 767 1 909 2 068 2 243 1 937 2 299 12 224 expansion cost 0 000 0 000 0 000 0 000 0 000 0 000 0 000 0 000 
25591,in various watershed based hydrological models river topography information is generated by utilizing empirical expressions developed for large catchments therefore identified the impact of channel geometry factors on hydrologic simulation in small watersheds and evaluation of reproducibility of river flow rate are required we applied the soil and water assessment tool swat to small agricultural watersheds and compared measurements obtained from a river with dem generated values the channel geometry factors determined using dem were underestimated in comparison with measurements however using the measured channel geometry as the model input led to a 14 7 increase in the r2 and nash sutcliffe coefficient nse values in the daily average flow in addition the r2 and nse values were more accurate by approximately 26 and 47 than the dem based optimized values obtained through swat calibration and uncertainty program cup sequential uncertainty fitting ver 2 sufi 2 optimization based on measured riverbed topography keywords riverbed topography measurement digital elevation model global sensitivity analysis streamflow soil and water assessment tool swat sequential uncertainty fitting ver 2 1 introduction environmental models have been used as tools to solve ecological problems stemming from climate or land use variations abbaspour et al 2009 zuo et al 2016 the soil and water assessment tool swat is an environmental model that is widely used to simulate water quality and semi distributed continuous long term hydrological parameters douglas mankin et al 2010 malagó et al 2017 for small 250 km2 to large scale 2500 km2 watersheds bonumá et al 2013 leng et al 2020 the swat is based on watershed level simulations which use digital elevation models dems to delineate watersheds and generate channel networks han et al 2019 using the swat the watersheds of the same size can be modeled using different geological data on the sub watershed and channel network depending on the differences in the spatial resolution of the dem gautam et al 2019 the reliability of the simulations increases with the resolution of the dems chaubey et al 2005 lin et al 2013 dems provide upward elevations only above the water surface except vegetation and artificial structures thus data on channel geometric factors i e the width depth and slope generated by watershed delineation inevitably include greater uncertainty than overland geometry in environmental models river morphology is simply geometricized richards 1997 and channel geometry is generated using a channel extraction algorithm with dem data bieger et al 2015 in swat the river cross section is assumed to be trapezoidal such that the channel hillslope for all rivers is defined as 45 neitsch et al 2011 the channel extraction algorithm accurately generates the river slope and length but not the river depth or width ames et al 2009 because the channel extraction algorithm was originally developed for large scale watersheds it has limitations in terms of its application to small watersheds staley et al 2006 comparative studies of the riverbed topographies generated by dems have compared characteristics of hydrologic response units hrus or catchment numbers and have discussed differences in channel slope soil function and subsurface effluence change reddy and reddy 2015 tan et al 2018 rocha et al 2020 as the empirical expression responsible for generating river width and depth values has been developed for large catchment sizes it was deemed unnecessary to evaluate the error and reproducibility of the actual riverbed information obtained via the use of small catchments han et al 2019 identified the spatiotemporal variability of channel geometry in small watersheds by analyzing the river width they applied a revised channel extraction algorithm for widths channel and floodplain to improve the simulated streamflow however channel geometric factors such as width depth and slope vary depending on the direction of the river flow smaller rivers can have relatively more complex shapes than larger rivers therefore it is necessary to clearly identify differences in river morphology in swat the simulation results differ not only in terms of the geometric factors of the delineated watershed i e size slope of sub watersheds and channel precision but also in terms of the hydrological parameters i e surface evapotranspiration groundwater and routing kannan et al 2007 white and chaubey 2005 tasdighi et al 2018 koo et al 2020 to improve the reliability of the simulation results both these geometric factors and hydrological parameters should be considered together however most studies have performed uncertainty sensitivity and optimization analyses focusing only on the process related parameters to improve hydrological simulations the swat calibration and uncertainty program cup supports the evaluation of uncertainty and the optimization of parameters related to the swat model swat cup was developed to facilitate the selection of five algorithms i e the generalized likelihood uncertainty analysis glue sequential uncertainty fitting ver 2 sufi 2 particle swarm optimization pso parameter solution parasol and markov chain monte carlo mcmc abbaspour 2015 the swat cup program has the advantage of allowing selective consideration of all parameters and geometric factors included in swat abbaspour 2015 swat cup supports global sensitivity analysis gsa which is a method that simultaneously considers the changes in parameters and evaluates their effects on the simulation results abbaspour 2015 in this study we evaluated the parameters that influence hydrological simulations including overland and channel topography data using the swat cup program for small agricultural watersheds and by analyzing the global sensitivity of the parameters in addition we compared the riverbed topography i e the width depth and slope of a channel generated by dems in small watersheds with the measured data by inputting the measurements into the optimized model set we assessed the improvements in the simulation results compared to the dem generated riverbed topography data 2 material and methods 2 1 study site the gwangcheon stream watershed is a part of the geum river basin one of the major rivers in korea located adjacent to the yellow sea into which the geum river flows fig 1 a the sang ji stream tributary flows into gwangcheon at the end of the stream watershed as shown in the images of measurement point mp 3 and 9 located in gwangcheon and sang ji stream the gwangcheon and sang ji streams have a sluggish reach due to their heavy and deep pools fig 1a the basin is a small watershed of approximately 65 2 km2 the annual cumulative precipitation level of the target watershed ranges from 1116 6 to 1318 6 mm with 56 4 of total precipitation occurring between july and september during the monsoon season the daily temperature of the target watershed ranges from 12 4 to 31 4 c and the average annual temperature is 12 5 c as a typical agricultural region 86 4 of the overall area of the basin encompasses agricultural and forest land fig 1b agricultural land forest urban land and pasture account for 44 7 41 7 5 0 and 4 0 of the land use respectively thirty five soil series are distributed within this region including the asan series fine loamy mesic family of typic dystrudepts red yellow soils jisan series fine loamy mesic family of fluvaquentic endoaquepts grey soils and the cheongsam series loamy skeletal mesic family of typic dystrudepts lithosols altogether these three soil series account for 52 of the total soil composition fig 1c the remaining 32 soil series represent 4 each 2 2 swat the swat was developed by the agricultural research service ars at the u s department of agriculture usda arnold et al 1998 the swat structure comprises two phases land and stream neitsch et al 2011 in the land phase the water sediment and nutrient cycles are calculated in the hydrologic response unit hru by superimposing the slope land use and soil data for each sub watershed in the stream phase the streamflow and the sediment and nutrient loading at the sub watershed outlet are calculated through routing in the land phase the soil conservation service scs curve number cn method is used to calculate surface flow and the penman method is used to calculate evapotranspiration in the stream phase the variable storage method is used to calculate channel routing in this study we used arcswat arcgis arcview extension and interface for swat version 2012 10 4 19 which is linked to arcgis v 10 4 1 2 3 input data and model set up 2 3 1 geospatial data to delineate watershed and hru spatial data including the topography land use and soil series are required winchell et al 2013 we divided the gwangcheon stream watershed into 12 sub watersheds using dem data with a spatial resolution of 5 m which were generated by the national geographic information institute of korea in 2015 national spatial data infrastructure portal 2020 the distribution area of each sub watershed ranged from 0 59 to 14 90 km2 with a mean of 4 90 km2 the land use data with a spatial resolution of 5 m were constructed using aerial ortho imageries derived from the kompsat 2 satellite in 2010 provided by the national institute of environmental research of korea environmental geographic information service 2020 the soil series data with a spatial resolution of 5 m were generated by the national institute of agricultural sciences in 2010 provided by the korean soil information system ksis korean soil information system 2020 the hrus generated by combining the unique characteristics of topography land use and soil series for each sub watershed were discretized into a total number of 312 units 2 3 2 hydrometeorological data meteorological data necessary for swat execution include precipitation temperature e g minimum and maximum wind speed solar radiation and daily relative humidity winchell et al 2013 as no observatory is located in the gwangcheon stream watershed we used data from the hongseong automated synoptic observing system asos which is located about 10 km from the boundary of the gwangcheon watershed fig 1d the data collected from 2010 to 2018 were provided by the meteorological information portal operated by the korea meteorological administration kma korea meteorological adiministration 2020 2 3 3 point source data the point sources of pollution were classified into residential industrial and livestock sources national institute of environmental research 2019 residential sources can be categorized as on site wastewater treatment systems owts and public wastewater treatment plants pwtps the outflow data from each pwtp were provided by the national wastewater information system nwis national wastewater information system 2020 industrial wastewater treatment plant iwtp data which refer to the outflow data from the wastewater treatment facilities within industrial complexes were provided by the nwis national wastewater information system 2020 livestock data refer to the outflow data from on site swine wastewater treatment oswt the processing volume per oswt facility was used as the daily average outflow the owts and oswt data were obtained from the korean public data portal public data portal 2020 2 4 measurement of riverbed topography there were nine measurement points including four points in the gwangcheon stream main stream and five points in the sang ji tributary fig 1a the measurements were obtained at 5 m intervals when the bankfull width of the river was 200 m and at 10 m intervals when the bankfull width was 200 m the channel width measurements were obtained at uniform intervals within a maximum distance of 4 m measurements were obtained at least twice during each field investigation in october 2017 and july 2018 to minimize errors 2 5 development of stage discharge rating curve for the calibration and validation of hydrological data continuous flow rate monitoring was conducted by installing real time water level observation equipment at the pohang monitoring point located in the gc2 sub watershed outlet for generating a stage discharge rating curve sdrc fig 1a a pressure type automatic water level meter ott orpheus mini water level logger ott hydromet kempton london uk was installed and set to record real time water level data every 10 min from august 2017 to august 2019 to secure the flow rate data for each level to develop the sdrc more than 30 survey of flow rates by various water level were conducted for approximately one year after installing the real time water level equipment to investigate the flow rate the velocity area method was used with a flowmeter the number and interval of segments at the channel width were in accordance with the standards set by the international organization for standardization iso 748 2007 international organization for standardization 2020 the measurement uncertainty was maintained at the good grade of 5 or less as suggested by the united states geological survey 1992 sauer and meyer 1992 to ensure accurate measurements the sdrc was prepared in two parts based on the 75th percentile of the flow rate results the water level measurements were extrapolated in areas where the water level measured with the automatic meter was lower or higher than the level measured in the flow survey table 1 the maximum water level at the pohang monitoring point was 2 5 m recorded on july 2 2018 the accumulated precipitation was 200 mm between july 1 and 2 2018 the largest daily flow rate using the sdrc during the survey period was approximately 14 2 m3 s which occurred at the time when the water level was the highest fig 2 b 2 6 calibration and validation of the model in this study calibration and verification were performed over three stages the first step was parameter optimization using the gsa and swat cup sufi 2 method based on the riverbed topography information generated through dem and the calibration and verification of the observations at pohang station second by reflecting the measured riverbed topographic information in the optimized model the error improvement for the inputted riverbed topography generated by dem was analyzed and compared with the resulting accuracy evaluation of the flow rate simulation results finally by reflecting the actual river topography information in the model and then performing parameter optimization again using the swat cup sufi 2 method the effect of optimal parameters under dem generated river topography conditions was analyzed thereby improving the accuracy of the outflow simulation optimization and gsa were conducted on the hydrological parameters using the sufi 2 method and swat cup program abbaspour 2015 in the sufi 2 method the optimal parameters were selected through the comparison with the measured data by uniformly dividing the minimum and maximum ranges of the parameters based on the number of iterations followed by sequentially simulating them while randomly changing the median of each section abbaspour et al 2007 sufi 2 can optimize the parameters with sufficient accuracy even with a few iterations yang et al 2008 this method has been applied in various regions including agricultural or urban area dominated watersheds raihan et al 2020 shivhare et al 2018 for parameter optimization a total of 32 parameters related to hydrological simulations in the land and stream phases were selected neitsch et al 2011 in the land phase 27 parameters were included which were discretized into five hydrological processes including surface water lateral flow groundwater evapotranspiration and snow processes in the stream phase five parameters related to channel routing were included a description of each parameter used in the gsa is provided in table 3 to analyze the effect of topography on the hydrological simulation a sensitivity analysis was performed for eight factors these eight factors of topography for swat included three geometries that affect the surface runoff simulation in the land phase i e the average slope length slsubbsn steepness hru slp channel slope ch s1 and slope length slsoil for lateral subsurface flow the other four parameters in the stream phase were channel width ch w2 depth ch d length ch l2 and slope along the main channel ch s2 the analysis entailed comparing the daily stream flow rate simulation results in the output of the entire watershed based on a 20 change for each factor sensitive parameters can be determined using the p value and t stat values calculated through gsa parameters with a large t stat and small p value were labeled as sensitive parameters in this study parameters with high confidence levels i e p values of less than 0 05 were selected as sensitive parameters thavhana et al 2018 further the relative magnitude of the sensitivity of the parameters was compared using the t stat value parameter optimization was performed with parameter sensitivity p 0 05 a total of 1500 iterations were performed for parameter optimization and sensitivity analysis using the sufi 2 method where the nash sutcliffe coefficient nse was applied as the objective function yang et al 2008 aggregate parameters that are spatially different due to geological characteristics such as soil series and land use were varied within the 20 range the remaining parameters were replaced within the minimum and maximum range abbaspour 2015 the simulation period was 2010 2019 spin up was performed for seven years 2010 2016 to stabilize the model the calibration period was one year from august 2017 to july 2018 the validation was conducted using data from august 2018 to june 2019 as the efficiency criteria of the simulation results the coefficient of determination r2 nse and percent bias pbias were calculated moriasi et al 2015 the simulated stream flow rate was compared with the recommended statistical performance criteria proposed by moriasi et al 2015 table 2 the evaluation of the simulation results for flow rate can be judged as satisfactory if the r2 nse and pbias values are 0 6 0 5 or more and 15 or less respectively to be evaluated as good the values of r2 nse and pbias should be simulated to be 0 75 0 7 or more and 10 or less respectively to be evaluated as a very good grade the values of r2 nse and pbias must be simulated to be 0 85 more than 0 8 and less than 5 respectively 3 results and discussion 3 1 sensitivity analysis the sensitivity analysis of the hydrological parameters showed that 19 out of 32 parameters were sensitive with p values below 0 05 fig 3 a fifteen parameters in the land phase five parameters related to the groundwater process and two related to the lateral flow process were indicated as sensitive parameters therefore the parameters that directly affected the base flow accounted for the largest proportion in the plant uptake and evapotranspiration process which affects the soil water content four parameters were indicated as sensitive parameters as the forest and agricultural areas in the gwangcheon stream watershed account for 84 1 of the total area parameters related to the interception of precipitation on the leaf surface owing to the vegetation growth and consumption of moisture in the soil were also considered as sensitive parameters meaurio et al 2015 in the surface flow process cn2 scs cn in the antecedent moisture condition ii and ov n manning s n value for overland flow were classified as sensitive parameters these parameters were also ranked highly sensitive in previous studies and considered as key parameters for the correction and verification of the models wu and chen 2015 kushwaha and jain 2013 narsimlu et al 2015 in the snow process two of the six parameters were indicated as sensitive parameters approximately 60 of the total precipitation volume in the area occurs between july and september and the precipitation volume in winter december february is low 7 on average therefore parameters related to the direct river inflow such as smtmp which controls the volume of melted snow entering the river and timp which is a lag coefficient are not sensitive parameters consequently the amount of soil water content the amount of water content transferred among soil shallow and deep aquifers and flow estimation processes of the return to the stream play an important role in the hydrological simulation in the land phase of the study site which is a small agricultural watershed affected by the monsoon climate among the five parameters related to routing in the stream phase four were sensitive trnsrch which controls the base flow volume in the floodplain alpha bnk ch k2 and ch n2 from manning s equation that calculates the channel flow fig 4 a evrch a parameter related to the calculation of evaporation from water surface was classified as a non sensitive parameter the loss of water by evaporation from the surface in a small river in a small watershed had little effect on the total runoff that reached the watershed exit among the 19 parameters selected those with the most substantial impact on hydrological changes were esco evaporation compensation coefficient and trnsrch controls the inflow that exceeds the river cross section to the bank storage during a flood in the land and stream phases fig 3b the sensitivity analysis of the parameters related to the swat hydrology in regions with a similar watershed area and land use characteristics also showed that esco is a highly sensitive parameter narsimlu et al 2015 furthermore esco showed high levels of sensitivity in large watersheds that are approximately 100 times the size of the gwangcheon stream watershed kaiser et al 2010 goyal et al 2018 therefore esco is an important parameter in the swat hydrologic process regardless of the watershed size the high sensitivity of trnsrch is believed to be due to the effects of the monsoon climate resulting in empty rainfall similar to that of the dry season thus securing bank storage inflow flow rates in the land and stream phases as well as groundwater the results of the sensitivity analysis on topography showed that seven out of eight parameters were sensitive three in the land phase and all riverbed topography in the stream phase the sensitive parameters in the land phase slsoil hru slp and ch s1 were used to calculate the lateral flow and surface lag time this is similar to the sensitivity analysis result i e parameters related to the base flow process accounted for a substantial proportion of the land phase the channel geometric factors i e the width depth and slope are major input data that define the size of the cross section which directly affects the flow rate and velocity in the channel moreover parameters related to the base flow process accounted for the largest proportion among 15 sensitive parameters selected in the land phase therefore as the stream flow in the river cross section had a major effect on the results all the channel geometry factors were selected as sensitive parameters 3 2 model calibration and validation parameter optimization was performed using the sufi 2 method on 32 parameters related to the hydrological simulation for calibration and validation evaluations were performed separately for daily and monthly averages the parameter optimization results for surface water in the land phase were that cn2 increased by approximately 5 and ov n which affects the amount of surface runoff into the main channel and overland flow time of concentration was optimized to 0 019 similar to the minimum value of the parameter range see table 3 therefore the calibration was performed by increasing the surface flow affected by increasing the cn2 and reducing the lag of the amount of surface runoff affected by lower value of the ov n so that the surface flow from the land phase can reach the river in a short time in the groundwater process that affects the base flow gwqmn which is the threshold of the groundwater level returning from the shallow aquifer to the base flow was 490 8 mm this corresponds to approximately within 10th percentile of the possible range 0 5000 mm see table 3 swat model was calculated using the base flow return to the channel under conditions satisfying groundwater levels above gwqmn neitsch et al 2011 so it was calibrated under conditions that allowed the base flow to be simulated quickly during rainfall regarding the base flow in the stream phase trnsrch which controls the quantity of water lost to the deep aquifer from bank storage and alpha bnk which controls the recession to the base flow were optimized to 0 10 and 0 96 respectively table 3 therefore the calibration was completed such that conditions favorable to the base spill of groundwater and loss of water content of the bank storage in the channel phase were minimized while the amount of return to the base flow was increased to ensure sufficient base spill flow as the calibrated values of the parameters are influenced by various regional characteristics such as precipitation patterns as well as land cover and soil the parameters cannot represent the same value however the outflow characteristics can be similar to those of the watersheds with similar spatiotemporal environmental conditions in small agricultural watersheds gwqmn and the surface lag coefficient were calibrated to small values as a result they showed hydrological characteristics exhibiting favorable base flow from groundwater and rapid flow from surface water into the river similar to previous studies kaiser et al 2010 narsimlu et al 2015 contrasting results were observed in large scale watersheds with dominant agricultural areas because gwqmn and the surface lag coefficient were calibrated to large values malagó et al 2017 leng et al 2020 raihan et al 2020 table 3 lists the calibration values for the parameters by runoff characteristics in the order of sensitive parameters based on p value the hydrologic simulation results were evaluated using three indicators of the swat model runoff simulation results the flow rate observation values were used that were calculated using the water level flow relation equation at the pohang monitoring point table 4 three performance criteria related to the daily average discharge achieved the satisfactory level where the r2 and nse values showed good or very good accuracy levels table 4 in terms of validation the r2 and nse values met the satisfactory criterion whereas pbias was lower than the performance criterion this result can be ascribed to the rapid decrease in the recession limb of the simulated hydrograph at each event fig 4a in terms of the monthly average discharge three efficiency criteria were simulated at satisfactory levels in the calibration periods table 4 as simulation results were overestimated after october 2018 the pbias value did not meet the satisfactory level during the validation period fig 4b 3 3 comparison of dems and measurement of riverbed topography for small watersheds channel geometry factors were sensitive to the swat hydrological simulation results riverbed topography generated with the channel extraction algorithm using dems included errors in their width and depth values ames et al 2009 the widths of the river at five monitoring points in the sang ji stream mp1 mp5 ranged from 11 82 to 37 98 m with an average of 30 73 m fig 5 a the width changed irregularly in the downstream direction where the difference between points mp4 and mp5 showed a sharp change of more than 25 m table 5 and fig 5a the water depths were distributed between 0 39 and 1 08 m with an average of approximately 0 73 m similar to the width the depth showed an irregular pattern of change in the downstream direction fig 5a the river widths at four monitoring points of the gwangcheon stream mp7 mp9 showed a distribution range of 25 86 44 64 m with an average of 34 62 m fig 5b similar to the sang ji stream irregular changes were observed in the downstream direction where the difference between sections mp8 and mp9 was large 20 m the water depths showed a distribution that ranged from 0 91 to 1 32 m forming similar water depths in different sections we measured 1 5 measurements points per sub watershed in this study and observed significant differences in the depth and width of each cross section fig 5 the width and depth of each cross section in the sub watersheds and the channel slope were averaged in the downstream direction for each cross section to construct data that reflected the measurements the dem generated data were lower than the measured values of river width depth and slope table 5 3 4 comparison of stream flow using dem and measurements of riverbed topography the comparison of the daily average stream flow between the dem and measured riverbed topography for the optimized model set showed differences depending on precipitation and conditions for exceeding actual river depth for simulated water level fig 6 a the maximum difference in the stream flow during precipitation was approximately 5 m3 s 17 3 increase after applied measured topography whereas the minimum difference in runoff in the absence of precipitation was 1 in particular when the dem based water level simulation results lead line in fig 6b exceeded the measured river depths black line in fig 6b the outflow rate changed significantly yellow shaded region in fig 6b the reason for the relatively large difference in flow during rainfall is the increase in the amount of flow from the land phase to the steam phase leading to an overflow that exceeds the cross section of the river such that it is directly affected by the changes in channel storage under the conditions where water level exceeds the river cross section various hydrological characteristics such as routing processes and runoff from flood plains and hillslopes determine the stream flow since the width of the river created by dem is generally smaller than the measured cross section it is judged that the change in the flow rate of the flood plain and hillslope due to the increase of the flood plain through the application of the measured cross section has a large effect kirkby 1988 when the dem based water level simulation did not exceed the measured river cross section no difference in the flow rate was observed this suggests that the simulated stream flow is insensitive to the changes in channel storage during periods when surface flow from rainfall is not generated and lateral and groundwater flows are dominant the comparison of the total monthly runoff volume for two datasets showed minor changes in runoff characteristics fig 6c during the period of intense rainfall the difference in the runoff volume between the two datasets was less than 1 the same value was observed for the base flow in the absence of rainfall goyal et al 2018 reported that the monthly average discharge based on the changes in the dem resolution was insensitive when hydrological parameters were fixed shen et al 2012 therefore when the hydrological parameters in the land phase are fixed the accumulated runoff is insensitive to the changes in the stream phase the measured river cross section data width depth and channel slope were applied to the optimized parameter model set improved error of dem generated river cross section information by reflecting measured riverbed information in the model and the accuracy was compared with the simulation that applied the dem generated river cross section during the calibration period the r2 value increased by 14 3 and pbias by 9 7 regarding the daily average value therefore reflecting the river cross section measurements improved the accuracy of the hydrological results table 6 as the hydrological results were obtained for the period of intense rainfall due to the differences in the river cross section the peak flow simulation accurately reproduced the measured values thereby increasing the efficiency criteria fig 7 the results of re optimization using swat cup sufi 2 on the 19 sensitive parameters based on the measured river information showed that the r2 and nse values in the calibration period were reproduced at a similar level to the simulated results that reflected the measured river information in the dem based calibration results during the verification period the r2 nse and pbais values improved by 26 2 46 8 and 102 8 respectively table 6 in the comparison of the scatter plots linear expressions of observations and simulated values were adjacent to the 1 1 line grey dot line in fig 7 thus improving the reproducibility of the actual flow rate to the simulated results overall fig 7 the comparison between the 19 sensitive parameters optimized after the application of the measured river cross section and the dem based optimization parameters is shown in fig 8 the rate of change compared with the optimal value based on dem by parameter was 378 to 88 the values of gw revap which regulates the evaporation amount in the shallow aquifer and of gwqmn which regulates the flow rate back into the stream were most significantly changed to 378 and 125 respectively the esco with the greatest sensitivity increased by approximately 39 to 0 97 which resulted in a change in optimization conditions because of increased evaporation of water in the soil the values of ov n cn2 and canmx which affect the amount of surface runoff due to rainfall changed to about 88 1 and 34 respectively the decrease in canmx affected the reduction of rainfall blocking by the vegetation in the soil resulting in increased rainfall reaching the surface in addition the optimization was changed to the condition that the surface and peak flow was increased due to the increase in the ov n and cn2 parameters 4 conclusions the swat was applied to small watersheds in which agricultural land and forest areas were dominant optimization was performed for 32 hydrology related parameters using the sufi 2 method the daily and monthly averages of r2 and nse both achieved the satisfactory cut off level of 0 7 or higher moriasi et al 2015 therefore the swat is considered highly suitable for understanding the hydrological characteristics of the small agricultural watersheds in which heavy rainfall events occur over a short period sensitivity analysis was performed by applying the sufi 2 method which is a gsa method to topographical factors generated by dems along with other hydrological parameters the results of the sensitivity analysis in the land and stream phases showed that the parameters related to the soil water transferred among the soil shallow aquifer deep aquifer and the base flow were sensitive parameters this is considered to be the result of the climatic conditions as the base flow in the river is relatively dominant because of the intense precipitation that occurs in the region between july and september and most of the land use in the watershed is cropland and forest all the riverbed morphological factors used in the swat such as depth width and slope were highly sensitive to the river flow simulation results the riverbed topography at nine measurement points in the region were compared with the channel geometry values generated by the dems with a dense spatial resolution of 5 m overall the widths of the rivers generated by the dem were lower than the actual river data values with a substantial difference up to 3 m although high accuracy can be achieved for channel slope values generated through the channel extraction algorithm using a dem ames et al 2009 significant differences were observed in this study therefore additional research and optimization algorithms are required to improve channel extraction algorithms for small watersheds the measured riverbed topography data were added to the calibrated model set and the results were compared with the dem simulation results the performance criteria generally improved in the calibration and validation periods during the calibration period r2 increased by 14 7 and pbias by 9 7 during the validation period both r2 and nse increased by 14 7 this improved the accuracy of the simulation results and the peak flow during precipitation represented the measured values more accurately however the simulated monthly flow rate was similar to the measured and generated riverbed topography therefore when using the swat to utilize daily simulated simulated results the measurements of the river geometry should be reviewed and input into the model to reflect the practical conditions as closely as possible the optimal flow simulation results reflecting the measured river cross section showed that r2 nse and pbias values increased to 26 2 46 8 and 102 8 respectively compared to the accuracy of the flow simulation results for dem based river topography input conditions accurate information regarding river bed topography can be obtained by surveying the area but it is expensive and time consuming lyzenga et al 2009 various studies are being conducted to extract riverbed topographic information using image data such as satellites and aerial photographs or through various statistical methods such as random forest bures et al 2019 kasbi et al 2019 furthermore studies that evaluate the accuracy of such topographical information generally focus on hydrological analysis using numerical analysis models sanders 2007 bures et al 2018 therefore the findings of the present study that difference of the measured river topography information generated through dem for a small watershed with the improved the accuracy of stream flow through the application of the measured river bed topography can be utilized in various studies using the watershed model also research on environmental changes caused by the variations in climate and land use requires the establishment and implementation of disaster prevention technologies for soil leakages and non point source pollution this requires a reliable interpretation of the daily simulated and peak flow rates however topographic data generated using high resolution dem data in small watersheds differ significantly from the actual data which affects the sensitivity and accuracy of hydrological simulations therefore accurate channel geometry is key in ensuring the accuracy of the simulation results funding this work was funded by the ministry of environment moe of the republic of korea grant number 1900 1946 303 201 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments thanks to author members of livestock excreta management research team including hyun jeong lee un il baek and seon jeong kim 
25591,in various watershed based hydrological models river topography information is generated by utilizing empirical expressions developed for large catchments therefore identified the impact of channel geometry factors on hydrologic simulation in small watersheds and evaluation of reproducibility of river flow rate are required we applied the soil and water assessment tool swat to small agricultural watersheds and compared measurements obtained from a river with dem generated values the channel geometry factors determined using dem were underestimated in comparison with measurements however using the measured channel geometry as the model input led to a 14 7 increase in the r2 and nash sutcliffe coefficient nse values in the daily average flow in addition the r2 and nse values were more accurate by approximately 26 and 47 than the dem based optimized values obtained through swat calibration and uncertainty program cup sequential uncertainty fitting ver 2 sufi 2 optimization based on measured riverbed topography keywords riverbed topography measurement digital elevation model global sensitivity analysis streamflow soil and water assessment tool swat sequential uncertainty fitting ver 2 1 introduction environmental models have been used as tools to solve ecological problems stemming from climate or land use variations abbaspour et al 2009 zuo et al 2016 the soil and water assessment tool swat is an environmental model that is widely used to simulate water quality and semi distributed continuous long term hydrological parameters douglas mankin et al 2010 malagó et al 2017 for small 250 km2 to large scale 2500 km2 watersheds bonumá et al 2013 leng et al 2020 the swat is based on watershed level simulations which use digital elevation models dems to delineate watersheds and generate channel networks han et al 2019 using the swat the watersheds of the same size can be modeled using different geological data on the sub watershed and channel network depending on the differences in the spatial resolution of the dem gautam et al 2019 the reliability of the simulations increases with the resolution of the dems chaubey et al 2005 lin et al 2013 dems provide upward elevations only above the water surface except vegetation and artificial structures thus data on channel geometric factors i e the width depth and slope generated by watershed delineation inevitably include greater uncertainty than overland geometry in environmental models river morphology is simply geometricized richards 1997 and channel geometry is generated using a channel extraction algorithm with dem data bieger et al 2015 in swat the river cross section is assumed to be trapezoidal such that the channel hillslope for all rivers is defined as 45 neitsch et al 2011 the channel extraction algorithm accurately generates the river slope and length but not the river depth or width ames et al 2009 because the channel extraction algorithm was originally developed for large scale watersheds it has limitations in terms of its application to small watersheds staley et al 2006 comparative studies of the riverbed topographies generated by dems have compared characteristics of hydrologic response units hrus or catchment numbers and have discussed differences in channel slope soil function and subsurface effluence change reddy and reddy 2015 tan et al 2018 rocha et al 2020 as the empirical expression responsible for generating river width and depth values has been developed for large catchment sizes it was deemed unnecessary to evaluate the error and reproducibility of the actual riverbed information obtained via the use of small catchments han et al 2019 identified the spatiotemporal variability of channel geometry in small watersheds by analyzing the river width they applied a revised channel extraction algorithm for widths channel and floodplain to improve the simulated streamflow however channel geometric factors such as width depth and slope vary depending on the direction of the river flow smaller rivers can have relatively more complex shapes than larger rivers therefore it is necessary to clearly identify differences in river morphology in swat the simulation results differ not only in terms of the geometric factors of the delineated watershed i e size slope of sub watersheds and channel precision but also in terms of the hydrological parameters i e surface evapotranspiration groundwater and routing kannan et al 2007 white and chaubey 2005 tasdighi et al 2018 koo et al 2020 to improve the reliability of the simulation results both these geometric factors and hydrological parameters should be considered together however most studies have performed uncertainty sensitivity and optimization analyses focusing only on the process related parameters to improve hydrological simulations the swat calibration and uncertainty program cup supports the evaluation of uncertainty and the optimization of parameters related to the swat model swat cup was developed to facilitate the selection of five algorithms i e the generalized likelihood uncertainty analysis glue sequential uncertainty fitting ver 2 sufi 2 particle swarm optimization pso parameter solution parasol and markov chain monte carlo mcmc abbaspour 2015 the swat cup program has the advantage of allowing selective consideration of all parameters and geometric factors included in swat abbaspour 2015 swat cup supports global sensitivity analysis gsa which is a method that simultaneously considers the changes in parameters and evaluates their effects on the simulation results abbaspour 2015 in this study we evaluated the parameters that influence hydrological simulations including overland and channel topography data using the swat cup program for small agricultural watersheds and by analyzing the global sensitivity of the parameters in addition we compared the riverbed topography i e the width depth and slope of a channel generated by dems in small watersheds with the measured data by inputting the measurements into the optimized model set we assessed the improvements in the simulation results compared to the dem generated riverbed topography data 2 material and methods 2 1 study site the gwangcheon stream watershed is a part of the geum river basin one of the major rivers in korea located adjacent to the yellow sea into which the geum river flows fig 1 a the sang ji stream tributary flows into gwangcheon at the end of the stream watershed as shown in the images of measurement point mp 3 and 9 located in gwangcheon and sang ji stream the gwangcheon and sang ji streams have a sluggish reach due to their heavy and deep pools fig 1a the basin is a small watershed of approximately 65 2 km2 the annual cumulative precipitation level of the target watershed ranges from 1116 6 to 1318 6 mm with 56 4 of total precipitation occurring between july and september during the monsoon season the daily temperature of the target watershed ranges from 12 4 to 31 4 c and the average annual temperature is 12 5 c as a typical agricultural region 86 4 of the overall area of the basin encompasses agricultural and forest land fig 1b agricultural land forest urban land and pasture account for 44 7 41 7 5 0 and 4 0 of the land use respectively thirty five soil series are distributed within this region including the asan series fine loamy mesic family of typic dystrudepts red yellow soils jisan series fine loamy mesic family of fluvaquentic endoaquepts grey soils and the cheongsam series loamy skeletal mesic family of typic dystrudepts lithosols altogether these three soil series account for 52 of the total soil composition fig 1c the remaining 32 soil series represent 4 each 2 2 swat the swat was developed by the agricultural research service ars at the u s department of agriculture usda arnold et al 1998 the swat structure comprises two phases land and stream neitsch et al 2011 in the land phase the water sediment and nutrient cycles are calculated in the hydrologic response unit hru by superimposing the slope land use and soil data for each sub watershed in the stream phase the streamflow and the sediment and nutrient loading at the sub watershed outlet are calculated through routing in the land phase the soil conservation service scs curve number cn method is used to calculate surface flow and the penman method is used to calculate evapotranspiration in the stream phase the variable storage method is used to calculate channel routing in this study we used arcswat arcgis arcview extension and interface for swat version 2012 10 4 19 which is linked to arcgis v 10 4 1 2 3 input data and model set up 2 3 1 geospatial data to delineate watershed and hru spatial data including the topography land use and soil series are required winchell et al 2013 we divided the gwangcheon stream watershed into 12 sub watersheds using dem data with a spatial resolution of 5 m which were generated by the national geographic information institute of korea in 2015 national spatial data infrastructure portal 2020 the distribution area of each sub watershed ranged from 0 59 to 14 90 km2 with a mean of 4 90 km2 the land use data with a spatial resolution of 5 m were constructed using aerial ortho imageries derived from the kompsat 2 satellite in 2010 provided by the national institute of environmental research of korea environmental geographic information service 2020 the soil series data with a spatial resolution of 5 m were generated by the national institute of agricultural sciences in 2010 provided by the korean soil information system ksis korean soil information system 2020 the hrus generated by combining the unique characteristics of topography land use and soil series for each sub watershed were discretized into a total number of 312 units 2 3 2 hydrometeorological data meteorological data necessary for swat execution include precipitation temperature e g minimum and maximum wind speed solar radiation and daily relative humidity winchell et al 2013 as no observatory is located in the gwangcheon stream watershed we used data from the hongseong automated synoptic observing system asos which is located about 10 km from the boundary of the gwangcheon watershed fig 1d the data collected from 2010 to 2018 were provided by the meteorological information portal operated by the korea meteorological administration kma korea meteorological adiministration 2020 2 3 3 point source data the point sources of pollution were classified into residential industrial and livestock sources national institute of environmental research 2019 residential sources can be categorized as on site wastewater treatment systems owts and public wastewater treatment plants pwtps the outflow data from each pwtp were provided by the national wastewater information system nwis national wastewater information system 2020 industrial wastewater treatment plant iwtp data which refer to the outflow data from the wastewater treatment facilities within industrial complexes were provided by the nwis national wastewater information system 2020 livestock data refer to the outflow data from on site swine wastewater treatment oswt the processing volume per oswt facility was used as the daily average outflow the owts and oswt data were obtained from the korean public data portal public data portal 2020 2 4 measurement of riverbed topography there were nine measurement points including four points in the gwangcheon stream main stream and five points in the sang ji tributary fig 1a the measurements were obtained at 5 m intervals when the bankfull width of the river was 200 m and at 10 m intervals when the bankfull width was 200 m the channel width measurements were obtained at uniform intervals within a maximum distance of 4 m measurements were obtained at least twice during each field investigation in october 2017 and july 2018 to minimize errors 2 5 development of stage discharge rating curve for the calibration and validation of hydrological data continuous flow rate monitoring was conducted by installing real time water level observation equipment at the pohang monitoring point located in the gc2 sub watershed outlet for generating a stage discharge rating curve sdrc fig 1a a pressure type automatic water level meter ott orpheus mini water level logger ott hydromet kempton london uk was installed and set to record real time water level data every 10 min from august 2017 to august 2019 to secure the flow rate data for each level to develop the sdrc more than 30 survey of flow rates by various water level were conducted for approximately one year after installing the real time water level equipment to investigate the flow rate the velocity area method was used with a flowmeter the number and interval of segments at the channel width were in accordance with the standards set by the international organization for standardization iso 748 2007 international organization for standardization 2020 the measurement uncertainty was maintained at the good grade of 5 or less as suggested by the united states geological survey 1992 sauer and meyer 1992 to ensure accurate measurements the sdrc was prepared in two parts based on the 75th percentile of the flow rate results the water level measurements were extrapolated in areas where the water level measured with the automatic meter was lower or higher than the level measured in the flow survey table 1 the maximum water level at the pohang monitoring point was 2 5 m recorded on july 2 2018 the accumulated precipitation was 200 mm between july 1 and 2 2018 the largest daily flow rate using the sdrc during the survey period was approximately 14 2 m3 s which occurred at the time when the water level was the highest fig 2 b 2 6 calibration and validation of the model in this study calibration and verification were performed over three stages the first step was parameter optimization using the gsa and swat cup sufi 2 method based on the riverbed topography information generated through dem and the calibration and verification of the observations at pohang station second by reflecting the measured riverbed topographic information in the optimized model the error improvement for the inputted riverbed topography generated by dem was analyzed and compared with the resulting accuracy evaluation of the flow rate simulation results finally by reflecting the actual river topography information in the model and then performing parameter optimization again using the swat cup sufi 2 method the effect of optimal parameters under dem generated river topography conditions was analyzed thereby improving the accuracy of the outflow simulation optimization and gsa were conducted on the hydrological parameters using the sufi 2 method and swat cup program abbaspour 2015 in the sufi 2 method the optimal parameters were selected through the comparison with the measured data by uniformly dividing the minimum and maximum ranges of the parameters based on the number of iterations followed by sequentially simulating them while randomly changing the median of each section abbaspour et al 2007 sufi 2 can optimize the parameters with sufficient accuracy even with a few iterations yang et al 2008 this method has been applied in various regions including agricultural or urban area dominated watersheds raihan et al 2020 shivhare et al 2018 for parameter optimization a total of 32 parameters related to hydrological simulations in the land and stream phases were selected neitsch et al 2011 in the land phase 27 parameters were included which were discretized into five hydrological processes including surface water lateral flow groundwater evapotranspiration and snow processes in the stream phase five parameters related to channel routing were included a description of each parameter used in the gsa is provided in table 3 to analyze the effect of topography on the hydrological simulation a sensitivity analysis was performed for eight factors these eight factors of topography for swat included three geometries that affect the surface runoff simulation in the land phase i e the average slope length slsubbsn steepness hru slp channel slope ch s1 and slope length slsoil for lateral subsurface flow the other four parameters in the stream phase were channel width ch w2 depth ch d length ch l2 and slope along the main channel ch s2 the analysis entailed comparing the daily stream flow rate simulation results in the output of the entire watershed based on a 20 change for each factor sensitive parameters can be determined using the p value and t stat values calculated through gsa parameters with a large t stat and small p value were labeled as sensitive parameters in this study parameters with high confidence levels i e p values of less than 0 05 were selected as sensitive parameters thavhana et al 2018 further the relative magnitude of the sensitivity of the parameters was compared using the t stat value parameter optimization was performed with parameter sensitivity p 0 05 a total of 1500 iterations were performed for parameter optimization and sensitivity analysis using the sufi 2 method where the nash sutcliffe coefficient nse was applied as the objective function yang et al 2008 aggregate parameters that are spatially different due to geological characteristics such as soil series and land use were varied within the 20 range the remaining parameters were replaced within the minimum and maximum range abbaspour 2015 the simulation period was 2010 2019 spin up was performed for seven years 2010 2016 to stabilize the model the calibration period was one year from august 2017 to july 2018 the validation was conducted using data from august 2018 to june 2019 as the efficiency criteria of the simulation results the coefficient of determination r2 nse and percent bias pbias were calculated moriasi et al 2015 the simulated stream flow rate was compared with the recommended statistical performance criteria proposed by moriasi et al 2015 table 2 the evaluation of the simulation results for flow rate can be judged as satisfactory if the r2 nse and pbias values are 0 6 0 5 or more and 15 or less respectively to be evaluated as good the values of r2 nse and pbias should be simulated to be 0 75 0 7 or more and 10 or less respectively to be evaluated as a very good grade the values of r2 nse and pbias must be simulated to be 0 85 more than 0 8 and less than 5 respectively 3 results and discussion 3 1 sensitivity analysis the sensitivity analysis of the hydrological parameters showed that 19 out of 32 parameters were sensitive with p values below 0 05 fig 3 a fifteen parameters in the land phase five parameters related to the groundwater process and two related to the lateral flow process were indicated as sensitive parameters therefore the parameters that directly affected the base flow accounted for the largest proportion in the plant uptake and evapotranspiration process which affects the soil water content four parameters were indicated as sensitive parameters as the forest and agricultural areas in the gwangcheon stream watershed account for 84 1 of the total area parameters related to the interception of precipitation on the leaf surface owing to the vegetation growth and consumption of moisture in the soil were also considered as sensitive parameters meaurio et al 2015 in the surface flow process cn2 scs cn in the antecedent moisture condition ii and ov n manning s n value for overland flow were classified as sensitive parameters these parameters were also ranked highly sensitive in previous studies and considered as key parameters for the correction and verification of the models wu and chen 2015 kushwaha and jain 2013 narsimlu et al 2015 in the snow process two of the six parameters were indicated as sensitive parameters approximately 60 of the total precipitation volume in the area occurs between july and september and the precipitation volume in winter december february is low 7 on average therefore parameters related to the direct river inflow such as smtmp which controls the volume of melted snow entering the river and timp which is a lag coefficient are not sensitive parameters consequently the amount of soil water content the amount of water content transferred among soil shallow and deep aquifers and flow estimation processes of the return to the stream play an important role in the hydrological simulation in the land phase of the study site which is a small agricultural watershed affected by the monsoon climate among the five parameters related to routing in the stream phase four were sensitive trnsrch which controls the base flow volume in the floodplain alpha bnk ch k2 and ch n2 from manning s equation that calculates the channel flow fig 4 a evrch a parameter related to the calculation of evaporation from water surface was classified as a non sensitive parameter the loss of water by evaporation from the surface in a small river in a small watershed had little effect on the total runoff that reached the watershed exit among the 19 parameters selected those with the most substantial impact on hydrological changes were esco evaporation compensation coefficient and trnsrch controls the inflow that exceeds the river cross section to the bank storage during a flood in the land and stream phases fig 3b the sensitivity analysis of the parameters related to the swat hydrology in regions with a similar watershed area and land use characteristics also showed that esco is a highly sensitive parameter narsimlu et al 2015 furthermore esco showed high levels of sensitivity in large watersheds that are approximately 100 times the size of the gwangcheon stream watershed kaiser et al 2010 goyal et al 2018 therefore esco is an important parameter in the swat hydrologic process regardless of the watershed size the high sensitivity of trnsrch is believed to be due to the effects of the monsoon climate resulting in empty rainfall similar to that of the dry season thus securing bank storage inflow flow rates in the land and stream phases as well as groundwater the results of the sensitivity analysis on topography showed that seven out of eight parameters were sensitive three in the land phase and all riverbed topography in the stream phase the sensitive parameters in the land phase slsoil hru slp and ch s1 were used to calculate the lateral flow and surface lag time this is similar to the sensitivity analysis result i e parameters related to the base flow process accounted for a substantial proportion of the land phase the channel geometric factors i e the width depth and slope are major input data that define the size of the cross section which directly affects the flow rate and velocity in the channel moreover parameters related to the base flow process accounted for the largest proportion among 15 sensitive parameters selected in the land phase therefore as the stream flow in the river cross section had a major effect on the results all the channel geometry factors were selected as sensitive parameters 3 2 model calibration and validation parameter optimization was performed using the sufi 2 method on 32 parameters related to the hydrological simulation for calibration and validation evaluations were performed separately for daily and monthly averages the parameter optimization results for surface water in the land phase were that cn2 increased by approximately 5 and ov n which affects the amount of surface runoff into the main channel and overland flow time of concentration was optimized to 0 019 similar to the minimum value of the parameter range see table 3 therefore the calibration was performed by increasing the surface flow affected by increasing the cn2 and reducing the lag of the amount of surface runoff affected by lower value of the ov n so that the surface flow from the land phase can reach the river in a short time in the groundwater process that affects the base flow gwqmn which is the threshold of the groundwater level returning from the shallow aquifer to the base flow was 490 8 mm this corresponds to approximately within 10th percentile of the possible range 0 5000 mm see table 3 swat model was calculated using the base flow return to the channel under conditions satisfying groundwater levels above gwqmn neitsch et al 2011 so it was calibrated under conditions that allowed the base flow to be simulated quickly during rainfall regarding the base flow in the stream phase trnsrch which controls the quantity of water lost to the deep aquifer from bank storage and alpha bnk which controls the recession to the base flow were optimized to 0 10 and 0 96 respectively table 3 therefore the calibration was completed such that conditions favorable to the base spill of groundwater and loss of water content of the bank storage in the channel phase were minimized while the amount of return to the base flow was increased to ensure sufficient base spill flow as the calibrated values of the parameters are influenced by various regional characteristics such as precipitation patterns as well as land cover and soil the parameters cannot represent the same value however the outflow characteristics can be similar to those of the watersheds with similar spatiotemporal environmental conditions in small agricultural watersheds gwqmn and the surface lag coefficient were calibrated to small values as a result they showed hydrological characteristics exhibiting favorable base flow from groundwater and rapid flow from surface water into the river similar to previous studies kaiser et al 2010 narsimlu et al 2015 contrasting results were observed in large scale watersheds with dominant agricultural areas because gwqmn and the surface lag coefficient were calibrated to large values malagó et al 2017 leng et al 2020 raihan et al 2020 table 3 lists the calibration values for the parameters by runoff characteristics in the order of sensitive parameters based on p value the hydrologic simulation results were evaluated using three indicators of the swat model runoff simulation results the flow rate observation values were used that were calculated using the water level flow relation equation at the pohang monitoring point table 4 three performance criteria related to the daily average discharge achieved the satisfactory level where the r2 and nse values showed good or very good accuracy levels table 4 in terms of validation the r2 and nse values met the satisfactory criterion whereas pbias was lower than the performance criterion this result can be ascribed to the rapid decrease in the recession limb of the simulated hydrograph at each event fig 4a in terms of the monthly average discharge three efficiency criteria were simulated at satisfactory levels in the calibration periods table 4 as simulation results were overestimated after october 2018 the pbias value did not meet the satisfactory level during the validation period fig 4b 3 3 comparison of dems and measurement of riverbed topography for small watersheds channel geometry factors were sensitive to the swat hydrological simulation results riverbed topography generated with the channel extraction algorithm using dems included errors in their width and depth values ames et al 2009 the widths of the river at five monitoring points in the sang ji stream mp1 mp5 ranged from 11 82 to 37 98 m with an average of 30 73 m fig 5 a the width changed irregularly in the downstream direction where the difference between points mp4 and mp5 showed a sharp change of more than 25 m table 5 and fig 5a the water depths were distributed between 0 39 and 1 08 m with an average of approximately 0 73 m similar to the width the depth showed an irregular pattern of change in the downstream direction fig 5a the river widths at four monitoring points of the gwangcheon stream mp7 mp9 showed a distribution range of 25 86 44 64 m with an average of 34 62 m fig 5b similar to the sang ji stream irregular changes were observed in the downstream direction where the difference between sections mp8 and mp9 was large 20 m the water depths showed a distribution that ranged from 0 91 to 1 32 m forming similar water depths in different sections we measured 1 5 measurements points per sub watershed in this study and observed significant differences in the depth and width of each cross section fig 5 the width and depth of each cross section in the sub watersheds and the channel slope were averaged in the downstream direction for each cross section to construct data that reflected the measurements the dem generated data were lower than the measured values of river width depth and slope table 5 3 4 comparison of stream flow using dem and measurements of riverbed topography the comparison of the daily average stream flow between the dem and measured riverbed topography for the optimized model set showed differences depending on precipitation and conditions for exceeding actual river depth for simulated water level fig 6 a the maximum difference in the stream flow during precipitation was approximately 5 m3 s 17 3 increase after applied measured topography whereas the minimum difference in runoff in the absence of precipitation was 1 in particular when the dem based water level simulation results lead line in fig 6b exceeded the measured river depths black line in fig 6b the outflow rate changed significantly yellow shaded region in fig 6b the reason for the relatively large difference in flow during rainfall is the increase in the amount of flow from the land phase to the steam phase leading to an overflow that exceeds the cross section of the river such that it is directly affected by the changes in channel storage under the conditions where water level exceeds the river cross section various hydrological characteristics such as routing processes and runoff from flood plains and hillslopes determine the stream flow since the width of the river created by dem is generally smaller than the measured cross section it is judged that the change in the flow rate of the flood plain and hillslope due to the increase of the flood plain through the application of the measured cross section has a large effect kirkby 1988 when the dem based water level simulation did not exceed the measured river cross section no difference in the flow rate was observed this suggests that the simulated stream flow is insensitive to the changes in channel storage during periods when surface flow from rainfall is not generated and lateral and groundwater flows are dominant the comparison of the total monthly runoff volume for two datasets showed minor changes in runoff characteristics fig 6c during the period of intense rainfall the difference in the runoff volume between the two datasets was less than 1 the same value was observed for the base flow in the absence of rainfall goyal et al 2018 reported that the monthly average discharge based on the changes in the dem resolution was insensitive when hydrological parameters were fixed shen et al 2012 therefore when the hydrological parameters in the land phase are fixed the accumulated runoff is insensitive to the changes in the stream phase the measured river cross section data width depth and channel slope were applied to the optimized parameter model set improved error of dem generated river cross section information by reflecting measured riverbed information in the model and the accuracy was compared with the simulation that applied the dem generated river cross section during the calibration period the r2 value increased by 14 3 and pbias by 9 7 regarding the daily average value therefore reflecting the river cross section measurements improved the accuracy of the hydrological results table 6 as the hydrological results were obtained for the period of intense rainfall due to the differences in the river cross section the peak flow simulation accurately reproduced the measured values thereby increasing the efficiency criteria fig 7 the results of re optimization using swat cup sufi 2 on the 19 sensitive parameters based on the measured river information showed that the r2 and nse values in the calibration period were reproduced at a similar level to the simulated results that reflected the measured river information in the dem based calibration results during the verification period the r2 nse and pbais values improved by 26 2 46 8 and 102 8 respectively table 6 in the comparison of the scatter plots linear expressions of observations and simulated values were adjacent to the 1 1 line grey dot line in fig 7 thus improving the reproducibility of the actual flow rate to the simulated results overall fig 7 the comparison between the 19 sensitive parameters optimized after the application of the measured river cross section and the dem based optimization parameters is shown in fig 8 the rate of change compared with the optimal value based on dem by parameter was 378 to 88 the values of gw revap which regulates the evaporation amount in the shallow aquifer and of gwqmn which regulates the flow rate back into the stream were most significantly changed to 378 and 125 respectively the esco with the greatest sensitivity increased by approximately 39 to 0 97 which resulted in a change in optimization conditions because of increased evaporation of water in the soil the values of ov n cn2 and canmx which affect the amount of surface runoff due to rainfall changed to about 88 1 and 34 respectively the decrease in canmx affected the reduction of rainfall blocking by the vegetation in the soil resulting in increased rainfall reaching the surface in addition the optimization was changed to the condition that the surface and peak flow was increased due to the increase in the ov n and cn2 parameters 4 conclusions the swat was applied to small watersheds in which agricultural land and forest areas were dominant optimization was performed for 32 hydrology related parameters using the sufi 2 method the daily and monthly averages of r2 and nse both achieved the satisfactory cut off level of 0 7 or higher moriasi et al 2015 therefore the swat is considered highly suitable for understanding the hydrological characteristics of the small agricultural watersheds in which heavy rainfall events occur over a short period sensitivity analysis was performed by applying the sufi 2 method which is a gsa method to topographical factors generated by dems along with other hydrological parameters the results of the sensitivity analysis in the land and stream phases showed that the parameters related to the soil water transferred among the soil shallow aquifer deep aquifer and the base flow were sensitive parameters this is considered to be the result of the climatic conditions as the base flow in the river is relatively dominant because of the intense precipitation that occurs in the region between july and september and most of the land use in the watershed is cropland and forest all the riverbed morphological factors used in the swat such as depth width and slope were highly sensitive to the river flow simulation results the riverbed topography at nine measurement points in the region were compared with the channel geometry values generated by the dems with a dense spatial resolution of 5 m overall the widths of the rivers generated by the dem were lower than the actual river data values with a substantial difference up to 3 m although high accuracy can be achieved for channel slope values generated through the channel extraction algorithm using a dem ames et al 2009 significant differences were observed in this study therefore additional research and optimization algorithms are required to improve channel extraction algorithms for small watersheds the measured riverbed topography data were added to the calibrated model set and the results were compared with the dem simulation results the performance criteria generally improved in the calibration and validation periods during the calibration period r2 increased by 14 7 and pbias by 9 7 during the validation period both r2 and nse increased by 14 7 this improved the accuracy of the simulation results and the peak flow during precipitation represented the measured values more accurately however the simulated monthly flow rate was similar to the measured and generated riverbed topography therefore when using the swat to utilize daily simulated simulated results the measurements of the river geometry should be reviewed and input into the model to reflect the practical conditions as closely as possible the optimal flow simulation results reflecting the measured river cross section showed that r2 nse and pbias values increased to 26 2 46 8 and 102 8 respectively compared to the accuracy of the flow simulation results for dem based river topography input conditions accurate information regarding river bed topography can be obtained by surveying the area but it is expensive and time consuming lyzenga et al 2009 various studies are being conducted to extract riverbed topographic information using image data such as satellites and aerial photographs or through various statistical methods such as random forest bures et al 2019 kasbi et al 2019 furthermore studies that evaluate the accuracy of such topographical information generally focus on hydrological analysis using numerical analysis models sanders 2007 bures et al 2018 therefore the findings of the present study that difference of the measured river topography information generated through dem for a small watershed with the improved the accuracy of stream flow through the application of the measured river bed topography can be utilized in various studies using the watershed model also research on environmental changes caused by the variations in climate and land use requires the establishment and implementation of disaster prevention technologies for soil leakages and non point source pollution this requires a reliable interpretation of the daily simulated and peak flow rates however topographic data generated using high resolution dem data in small watersheds differ significantly from the actual data which affects the sensitivity and accuracy of hydrological simulations therefore accurate channel geometry is key in ensuring the accuracy of the simulation results funding this work was funded by the ministry of environment moe of the republic of korea grant number 1900 1946 303 201 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments thanks to author members of livestock excreta management research team including hyun jeong lee un il baek and seon jeong kim 
25592,the cyberwater project is created to develop an open data and open model integration framework for studying complex environmental and water problems where diverse online data sources can be directly accessed by diverse models without any need of users extra effort on the tedious tasks of data preparation for their models we present our design and development of a novel generic model agent toolkit in the context of cyberwater which enables users to integrate their models into the cyberwater system without writing any new code significantly simplifying the data and model integration task cyberwater adopts a visual scientific workflow system vistrails which also supports provenance and reproducible computing our approach and the developed generic model agent toolkit are demonstrated via cyberwater framework with automated and flexible workflows through integrating data and models using real world use cases two popular hydrological models vic and dhsvm are used for illustrations keywords open data and model model integration generic model agent scientific workflow reproducible process software data availability name author contact year first available format language cost size availability vistrails new york university https www vistrails org index php people 2007 python free 300 mb https www vistrails org index php main page vic university of washington http uw hydro github io team 1994 c free 2 mb https vic readthedocs io en master dhsvm pacific northwest national laboratory ning sun pnnl gov mark wigmosta pnnl gov 1994 c free 60 mb https www pnnl gov source code usgs water services usgs https water usgs gov contact gsanswers n a xml webservices free n a https waterdata usgs gov nwis dv nca ldas nasa gsfc dl help disc mail nasa gov 2018 tiff grib or netcdfwebservices free n a https doi org 10 5067 7v3n5do04mas nldas nasa gsfc dl help disc mail nasa gov 2009 tiff or netcdfwebservices free n a https doi org 10 5067 6j5lhhohzhn4 1 introduction with diverse data across disciplines being increasingly available on today s internet it has become more urgent and important to have a framework facilitating data and computational model integration for studying problems involved in many complex processes e g physical hydrological biological and atmospheric such as predictions of floods droughts water quality and air quality while there are many online data sources available on the internet the key challenge is how to make these data sources directly accessible to various models without users additional effort on data preparation or data preprocessing for their models such model input data preparation task is usually very time consuming tedious and error prone due to the fact that individual data sources offer their different access protocols and have their different data organizations and formats previous research focused more on model integration argent 2004 belete et al 2017 and the papers therein with little consideration or treatment on the general problem of integrating data and models together for example existing modeling systems for different needs in environmental and water fields include open modeling interface openmi 1 1 https www openmi org moore and tindall 2005 gregersen et al 2007 knapen et al 2013 harpham et al 2019 community surface dynamics modeling system csdms 2 2 https csdms colorado edu wiki main page peckham et al 2013 university of colorado boulder 2012 object modeling system oms 3 3 https alm engr clolstate edu cb wiki 16961 ahuja et al 2005 david et al 2013 earth system modeling framework esmf 4 4 https earthsystemmodeling org collins et al 2005 deluca et al 2012 hill et al 2004 the invisible modeling environment time rahman et al 2003 stenson et al 2011 and geographic modeling and simulation systems opengms chen et al 2019 zhang et al 2019 wang et al 2020 these modeling systems do not directly address the challenge of enabling users models to access the heterogeneous data sources automatically and seamlessly over the internet to carry out their modeling studies while some of these systems support public sharing of data modeling and or simulation resources they are generally limited to some specific models and data rather than providing a general framework to integrate diverse data and models like cyberwater although openmi2 0 and opengms for example increase the flexibility by turning numerical models into linkable components to receive exchange data from each other there is remaining work for these systems to wrap up the existing models to be feasible components running in these frameworks salas et al 2020 provided comprehensive reviews of these systems and compared them with the earlier version of the cyberwater framework a recent effort aiming at developing a systemic solution to this challenge the key idea and feature of the cyberwater system is its focus on building an open architecture framework to facilitate realizations of open science salas et al 2020 which facilitates resource sharing e g data and models reproducibility of the work and community participation to this end an open data and open modeling framework salas et al 2020 referred to as msm meta scientific modeling that provides the foundation for the cyberwater framework is developed msm is designed to offer scientific researchers and practitioners a sophisticated open modeling environment where data agents are developed to access heterogeneous online data products provided by different data sources so that the data can directly flow seamlessly from the data sources to the users models without any need for the users to preprocess the input data for their models the model input data preparation has been performed by each individual data agent developed in msm for its corresponding data source such as the usgs and nasa data products the implementation of msm is called msm which incorporates the workflow system of vistrails 5 5 https www vistrails org index php main page to provide not only a graphical workflow mechanism for achieving the modeling task but also data provenance to ensure traceability and reproducibility salas et al 2020 to make use of the facilities of data agents in msm however one needs to integrate his her model into the msm system by means of writing a model agent based on the interface provided by msm once a user s model is integrated into msm via its agent the user s model can use all the data agents available in the msm system for direct data and model integration and can also couple his her model with another model already integrated into msm for model to model coupling integration without coding another recent work on the model input data preparation is hydrods web services gichamo et al 2020 which is applicable to two specific models the utah energy balance ueb snowmelt model tarboton et al 2014 and the topnet hydrologic model bandaragoda et al 2004 the use of hydrods an integration of model ueb with data is demonstrated through the development of a web application gan et al 2020 the strength of hydrods a webservice based data and model integration approach is that there is no need for local software installation its limitations include not supporting other models beyond ueb and topnet nor providing users with any mechanism to integrate new models into hydrods cyberwater is a standalone system at present which avoids the potential bottleneck of the centralized server facilitating scalability and sustainability cyberwater will be extended to include web services as well in the future to further complement its standalone system for the webservice based systems chen et al 2020 provided a comprehensive review while the msm framework has its unique merit for open data and open model integration it nevertheless still requires programming a model agent per new model integration to write a model agent means that the user needs to do programming coding to overcome such a limitation this work extends the previous msm framework here we present a general approach to construct a generic model agent template using workflow for handling models input interfaces for the environmental and hydrological models we then design and develop a set of tools called generic model agent toolkit for our approach with which a user can construct his her model agent by simply doing parameter based configuration steps without coding for most of the environmental and hydrological models this effort together with other extensions to and improvements on the msm system including accessing high performance computing on demand li et al 2021 constitutes the latest development of our cyberwater framework software system in this paper we present the design and development of the generic model agent toolkit in cyberwater we then demonstrate how this set of tools can significantly eliminate the user s task of writing a model agent in order to integrate a new model into the system and thus further enhance the usability and effectiveness of the cyberwater framework for open data and model integration and greatly improve the capability of the community to share diverse models for model validation evaluation scientific explorations etc different versions of the variable infiltration capacity vic model hamman et al 2018 liang et al 1994 1996a 1996b liang and xie 2003 and the distributed hydrology soil vegetation model dhsvm wigmosta et al 1994 are used in this study for illustration via use cases over several watersheds in pennsylvania usa the remainder of this paper is organized as follows section 2 briefly overviews the background to set up the context section 3 presents our approach and design while section 4 describes the implementation of the generic model agent toolkit section 5 provides two use cases of reproducible end to end model simulations to demonstrate the use of the developed generic model agent toolkit for open data and model integration in cyberwater and offers our insights through discussions finally section 6 concludes the presented work and provides planned future work 2 background 2 1 msm architecture msm meta scientific modeling is an open data open model framework that provides a sophisticated workflow controlled modeling environment for heterogeneous data and model integration without a need for a central administration salas et al 2020 the implementation of the msm framework having vistrails bavoil et al 2005 incorporated in it is called msm system which running on the user s local desktop or laptop constitutes several components including the core i e msm core data agents and model agents basically the msm system interacts with the workflow engine of vistrails accesses remote heterogeneous data sources through data agents and invokes users computational models for simulations through model agents the msm core is the center of all the components in the open data open model framework through its connection with the workflow engine of vistrails the msm system invokes external models and couples various data sources with respective model agents and data agents via a friendly and feasible workflow controlled working environment therefore the msm enables access to external data of diverse sources conveniently and effectively and execution of sophisticated numerical models efficiently the open model feature allows various models or modules to be smoothly integrated into the msm system via model agents the architecture of msm offers a novel information bus connection pattern of linear complexity i e the complexity of the development of data and model agents is o m n for integration of m data sources and n computational models the lowest possible complexity among all independent models and external heterogeneous data sources as opposed to quadratic complexity involved in a pair wise based one to one integration architecture i e the complexity of o mn n 2 for integration of m data sources and n computational models salas et al 2020 in the msm framework the modeling processes are dynamically decided by the end users through a workflow engine users can customize the sequence of activities and construct any workflow series for their modeling process based on the building blocks or modules provided by the msm system each workflow activity conducts interactively with the other workflow activities in the same workflow msm s design criteria enable a creation of a general and flexible system so that the msm framework can be easily plugged into various open source workflow engines the msm takes advantage of the existing workflow engines and avoids reinventing the wheel the overall architecture of the msm system is shown in fig 1 2 2 model agent in msm various computational models are added into the msm modeling framework by means of their corresponding model agents any model added into msm can then be freely integrated with various data agents as well as other models in a workflow constructed by the user where model agents represent and execute the corresponding external models in the workflow salas et al 2020 thus the effort to add a model into the msm modeling framework is to develop the model s agent since no modification of the original model code is required in the msm system a model agent needs to perform three main tasks as follows preparing model s inputs the agent collects the model s required input data from the workflow and transforms them into the model s input files which are needed by the model s execution executing model the agent invokes the model executable file according to the workflow control and runs the model retrieving model s outputs after finishing the model s execution the agent needs to retrieve the model s output files and transform the output data into the msm system in the format of msmdataset before the subsequent workflow item gets to be executed the generic agent is a class developed in the msm system to assist users in their development process of model agents in the generic agent base class a generic run model method is declared each model s agent is inherited from the generic agent class and overrides the generic run model method enabling the customized model s invocation from the workflow the model agent uses services offered by the generic agent class to read the inputs from and write the outputs into the msm core datastore which simplifies the agent s coding for integrating an external model into the msm system the model agent s primary responsibilities are to be implemented by overriding the run model function that is first read the model inputs from the msm core and save the data into the model s input files then run the model executable file finally read the model s output files after the model s execution transform them into the msm dataset format and save the model s outputs into the msm core to write a model agent the user has to create a text file including a class in python as outlined here image 1 in the run model function the user writes all the necessary codes to prepare the inputs execute the model and store the outputs these are the three main tasks that the model agent is supposed to perform to integrate the user s model into the msm system for more details readers are referred to salas et al 2020 3 approach 3 1 the idea while the generic agent class in the msm system reduces users burden to develop their model agents to some extent it would be desirable if users model agents can be constructed without any coding since in general any model agent will conduct the three tasks described in section 2 2 our idea is to construct a generic model agent template for most environmental and hydrological models which predefines outlines tasks of preparing model inputs executing model and storing outputs in a workflow environment in the msm system all workflow modules can be dynamically defined and configured thus using the divide and conquer technique a generic model agent can be achieved by first composing a generic workflow segment template of predefined generic modules as part of the overall workflow in the msm system such a workflow segment template will then be configured by individual users for their specific models where each model s variability is accommodated via the configuration of the generic workflow segment template the predefined generic modules represent the abstraction of each aspect of the generic model agent this way the variability of an individual model is treated and represented through parameter based configuration options of the workflow segment where specific information concerning an individual model such as the model s parameter values is configured via a graphic user interface of the input panel associated with the configuration option therefore a user no longer needs to write any specific model agent code to integrate an external model into the msm system which greatly enhances the usability of the msm modeling framework for achieving the data and model integration 3 2 the design to fulfill our idea we attempt to systematically predefine and design a set of generic workflow modules i e components to be used for constructing a model agent for an individual model in the form of a generic workflow segment i e template where each component represents and fulfills an individual task that a user s specific model agent would accomplish to this end our design strategy is as follows a we developed a set of basic template based workflow components based on an analysis of the representative model interface structures and organizations of most environmental and hydrological models those workflow components are used as building blocks for constructing a generic model agent where each component i e building block is called a generic model agent tool b a specific model agent for a given scientific model is constructed by composing the predeveloped component templates into a workflow segment template which is then incorporated into the user s overall modeling workflow sequence c the workflow segment template is configured by the user for the model simulation needs to serve as the model agent this way the model agent s construction and its corresponding configurations are realized within the msm system via an interactive graphical workflow interface without any coding our design includes the following six major generic model agent tools i e components users are only expected to be knowledgeable about their own models input output details when they use these generic model agent tools for integrating their models into the cyberwater system a model s inputs are typically classified into three categories static parameters forcing data and the initial states of the model 1 maingenerator this is the first component i e the root component to build up a model agent in the form of a workflow segment template for a user s specific model this component is responsible for setting up a working directory for performing the user s model simulation inside this working directory folder all the input information needed for executing the user s model and the output information after the model s execution will be placed the path of the working directory is specified by the user this component is also responsible for receiving the forcing information required for the model s execution the order in which these forcing data are collected to the maingenerator as datasets through its module information panel matters since the final forcing data files created and placed in the working directory folder for the model execution will have these forcing data organized in columns following the same order in addition this maingenerator reads in other important information needed for setting up the required execution condition environment for the user s specific model for example information specifying certain options such as the number of soil layers to be used by the model the water or energy balance mode to be executed etc such information is typically provided through a model s global file or model s configuration file whose path is provided to this component and also through the use of its module information panel this module outputs two pieces of information through its two output ports one is called wd path which provides the working directory s path information where all the model s needed input information and the model s output information will reside the other is called dataset class which is a list of all the datasets comprising the model s forcings 2 areawiseparamgenerator this component organizes parameter files e g vegetation and soil related parameter files and places the imported parameter files into the parameter folder created by it this parameter folder will be automatically placed under the working folder provided by the maingenerator module information on all the paths of the parameter files needed for executing the user s model is collected in this module by specifying them through the module s information panel from the user interface this module outputs a ready signal which will be discussed later 3 forcingdatafilegenerator this component is responsible for creating the forcing data for the user s model specifically this component organizes the forcing data brought in by the maingenerator the user should be aware of the expected format of the model s forcing data by default this module creates the forcing data to be used by the user s model in a folder named forcing inside the working directory such forcing data are always created as a time series where different columns hold different variables according to the order specified in the maingenerator the data are divided into separate files where each one represents a single modeling cell unit of the forcing inputs this module has two input ports one is the working directory and the other is the list of forcing datasets both of which are provided by the maingenerator module like the areawiseparamgenerator this module also outputs a ready signal 4 initialstatefilegenerator this is an optional component which organizes the data of initial states for the user s model as the initial state files are not always required for the execution of a given model however if a user would like to conduct data assimilation to improve the user s model forecast reliability the user may need to use this module in any event if required this module receives the information of the working directory from the maingenerator module also the initial states file paths will be specified through this module s information panel and be placed either inside the working directory or a new directory created by the initialstatefilegenerator with a name given as an input from the user the output of this module is again a ready signal 5 runmoduleagent this component is responsible for invoking the user s model e g executable on the local machine and for retrieving the model s outputs it executes the model in the same way as the model runs alone without the msm system environment where the user must manually prepare all the input files e g forcing data parameter files initial state files of the model this module has three pieces of input information first it connects to the output of the areawiseparamgenerator forcingdatafilegenerator and initialstatefilegenerator if required separately to receive their ready signals second it receives the information of the working directory from the maingenerator third it receives the dataset information outputted from the maingenerator this module also requires the path where the model s executable file is located by setting up all the aforementioned information and receiving all the required information passed onto it from the other modules components this runmoduleagent component is now ready to execute the user s model the model s simulated results will be written back to the cyberwater system for the workflow to continue 6 hpc this component is the module responsible for accessing remote high performance computing hpc facility on demand this module aims to provide high performance computing capacity for executing the user s model by seamlessly connecting it to either academic supercomputers or commercial cloud platforms li et al 2021 it retrieves the results back to the workflow when the execution of the user s model on the remote hpc platform is completed in a nutshell regarding the three tasks that a model agent needs to perform sec 2 2 preparing model s input is fulfilled by a combination of three corresponding generic components i e components 2 3 and 4 described above while the tasks of both executing model and retrieving model s outputs are combined and fulfilled by the generic component runmoduleagent component 5 above or hpc component 6 above depending on whether the model is executed on a local machine or a remote hpc platform based on our approach users construct their model agents using the generic model agent toolkit i e the six modules or building components described above at the same time as they employ the msm system to build their overall modeling workflows fig 2 illustrates a typical workflow construction for hydrological environmental modeling in the msm system with a graphical workflow interface the dotted block indicates how the generic workflow components presented above are used to construct and configure a user s model agent in the workflow after the completion of the user s construction and configuration of the workflow the workflow is then executed in the msm system a sequence diagram of a typical modeling workflow with the constructed model agent using the generic model agent toolkit is illustrated in fig 3 in summary the generic model agent toolkit is designed to help the user construct a model agent to integrate the user s model into cyberwater without writing any codes such a goal is achieved through some combinations of the six modules in the toolkit together to perform the following main functionalities set up the user s model working directory write data sets retrieved from data agent s or local files into model input files that is prepare input data files for the user s model invoke the user s model codes to run the user s model write the user s model output results back to the cyberwater system 4 implementation the generic model agent toolkit called genericmodelagenttools is developed and implemented in python outside of the msm system as an extensive vistrails package which is a significant extension to the original msm system in the overall cyberwater system however the genericmodelagenttools need the support of the corresponding dao and cache modules of the msm system to access data sets flowed into the workflow modules the hierarchical architecture of the cyberwater system is shown in fig 4 each generic model agent tool in the toolkit is designed and implemented as a template based workflow module to illustrate we present in the following the implementation of the template based generic model agent tool maingenerator in sections 4 1 and its configuration in section 4 2 respectively 4 1 generic model agent tool as a template the maingenerator creates a working directory folder as a workspace for the user s model where it hosts all the input information required for executing the user s model and the output results of the model after its execution copies the model s global parameter file or configuration file into this created working directory for carrying out the model s execution task and converts the datasets into dataset class elements in the form of python dictionaries this component also gathers all the forcing datasets required by running the user s model the maximum number of the maingenerator s input ports for datasets is set to be 15 as a default for convenience however the input ports of the maingenerator can be expanded if needed hence in the template based component maingenerator the path of the working directory the path of the model s global parameter file and the datasets of the forcing to be used are all configurable by the user for illustration the pseudocode of the maingenerator is given in fig 5 in general there are three types of input files needed for executing a numerical model namely parameter files forcing data files and initial state files the areawiseparamgenerator organizes the parameter files for performing the numerical model the forcingdatafilegenerator creates the model s forcing data files and the initialstatefilegenerator is engaged in preparing the initial state files if needed for performing the numerical model simulation the runmoduleagent module is for setting up the path where the model s executable file is located it also obtains the information of the arguments for executing the model and the configuration information for the model s output variables during the workflow execution the areawiseparamgenerator forcingdatafilegenerator and initialstatefilegenerator modules process the datasets to generate the needed input files for the user s model the module of the runmoduleagent invokes the user s model execution based on the datasets passed the model s simulated results will be written back to the cyberwater system similar to the maingenerator each of these template based components also has its places configurable for the user to construct part of the user s model agent 4 2 configuration of generic model agent modules a configuration example with the maingenerator fig 6 is used here to illustrate how the user can configure a generic model agent module a input port specification 01 path the path of the working directory folder where files for executing the user s model will reside 02 gpf the path of the global or configuration file if such a file is required by the user s model this file is commonly used for setting up the variables for the model s execution dataset x collecting the forcing dataset x x 01 02 03 15 from the msm system in which x indicates the index of the dataset and passing the forcing datasets to the forcingdatafilegenerator to generate the required forcing files for the user s model to run b output port specification wd path an output port that passes the working directory path information to the next module dataset class another output port that passes the information of a dataset cluster i e the information of the forcing datasets to the next module 5 use cases there are two approaches to integrate a new model into cyberwater one is to program a model agent by the user while the other is to use the generic model agent toolkit provided in cyberwater to build the model agent without coding this section shows how to achieve the integration of a user s model into the cyberwater system using our developed generic model agent toolkit without writing a single line of code the user s model integration will be illustrated using our generic model agent toolkit versus adopting the user s manually programmed model agent two use cases with the vic model and dhsvm model integrations are provided in sections 5 1 and 5 2 respectively 5 1 vic model simulation cyberwater provides various modules that help users construct their model simulation workflow to directly access online data from diverse data sources e g forcing data from nasa and streamflow data from usgs execute models and produce model simulation results in this use case the variable infiltration capacity vic model liang et al 1994 1996a 1996b liang and xie 2003 is employed for illustration first we show the integration of the vic model version 4 0 vic4 into the cyberwater system via the user s manually coded model agent called vic agent the integrated vic4 runs a water balance simulation of the french creek basin in cyberwater this is a small watershed with a drainage area of 153 km2 located in the southeast of pennsylvania united states note that all the model parameters in this simulation are default values that the user can further modify through a parameter calibration process 5 1 1 workflow with manually developed vic model agent we first show how to build a complete example of a model simulation workflow using the vic model via a user s manually developed vic agent in general creating a model simulation workflow starts with model input data preparation based on various data sources for a given study area over a given time period thus the workflow starts first with a timerange module which is used to specify the simulation time extend for this example the simulation will go from 2010 01 01 00 00 00 time start to 2011 01 01 00 00 00 time end then a spacerange module is added to specify the study area with four bounds i e east west north and south as 75 58 75 86 40 24 40 10 x max x min y max y min respectively to access the forcing information required for this simulation the ncaldas data agent will be used this agent accesses nasa ncaldas 6 6 https ldas gsfc nasa gov nca ldas forcing earth data database to automatically bring nasa s spatially organized daily hydrometric information to the local machine four ncaldasagent boxes are added into the workflow area each one of these four new boxes will be used to retrieve a different variable of the forcing data the four forcing variables required to execute vic4 include wind speed m s total precipitation rate mm s temperature max k and temperature min k now bring the vic4 model into the workflow the user s model is added into the workflow through its model agent so the vicagent module developed by us salas et al 2020 is added to the workflow area basically this vicagent enables the usage of the vic model in the cyberwater workflow it collects vic forcing information either extracted via cyberwater s data agents e g wind speed and precipitation online or reads in the forcing data from files locally provided by the user it also reads in all the parameter files provided by the user to run the vic model if these parameter files are not provided the vicagent will generate all the parameter files required for the simulation using default values automatically values of the parameters in these files can later be modified or calibrated by the user for re running the vic model the vicagent also provides users the flexibility of selecting their desired output variables from the vicagent interface for visualizing them through the workflow execution the workflow chart shown in fig 7 represents the main functionalities accomplished by the vicagent code note that vic like any other model requires inputs to be included in specific units depending on the individual variables details of such input information are included in the vicagent documentation available via the documentation button in cyberwater for example the temperature must be in celsius and for this case study the precipitation will need to be in mm per day since the expected results will be daily thus the user only needs to check if the units of the retrieved data from data providers match the units required by the user s model if not unit transformations are needed the msmunitconversion module can be used to perform such transformations for instance to convert the temperature from the ncaldas dataset in kelvin into celsius the user needs to set the input operation to be x 273 15 this operation allows the user to execute simple mathematical operations using x as the variable representing the dataset given as an input to transform the input precipitation data from mm per second to mm per day for example the user just needs to set the operation to x 86400 by creating another msmunitconversion box this module also allows the user to indicate the new resulting units of the transformed dataset the last step for creating this workflow is to set up visualization tools which will allow the user to see the model simulated results for example the baseflow and surface runoff this visualization can be achieved by adding two msmshowchart boxes into the workflow and naming them baseflow and surface runoff respectively the completely created workflow for this vic simulation case is shown in fig 8 after executing this workflow two charts shown in fig 9 a and fig 9 b will be prompted this example produces outputs of these two variables offered by default in the vic agent however if the user goes to the outputs tab in the module info panel more model output results available to be displayed are listed 5 1 2 constructing vic4 model agent using generic model agent toolkit this section shows how to use the generic model agent toolkit presented in sections 3 and 4 to construct configure the vic4 model agent for the model integration instead of manually writing the vic4 model agent code the important benefit here is that a user can integrate the user s own model into the cyberwater environment using this generic toolkit with little or no programming effort to use this toolkit users are only expected to be knowledgeable about their own models inputs and outputs organizations our following example of the vic4 model illustrates a simulation for the same period of time over the same study area described in section 5 1 1 thus the time range space range and forcing data construction part of the workflow is the same as those described in section 5 1 1 here we focus on steps to construct the vic4 model agent using the toolkit 5 1 2 1 maingenerator to start constructing the vic model agent first bring a maingenerator box the main control component of the generic model agent toolkit into the workflow working area this component is responsible for setting up the working directory for the model simulation where it receives all the forcing datasets as inputs the users can adjust the paths accordingly for their specific folder structure for the working directory for this example this working directory folder will be saved on c temp cyberwater gt vic4 french creek thus the inputs of path and gpf can be set up respectively as c temp cyberwater gt vic4 french creek mainagent and c temp cyberwater gt vic4 french creek source vic global file val in addition all forcing data are collected by the maingenerator by connecting the forcing data modules to this maingenerator the working directory path information will be passed via its wd path output port to all the other components in the toolkit to be used for constructing the user s model agent 5 1 2 2 areawiseparamgenerator add an areawiseparamgenerator which is in charge of setting up the parameter files e g soil parameter files and vegetation parameter files of the model these files need to be previously created by the user the user needs to enter the information of the file s path for each of the parameter files required as the model s inputs in the areawiseparamgenerator module for this example the files used are stored in the folder c temp cyberwater gt vic4 french creek source 5 1 2 3 forcingdatafilegenerator add a forcingdatafilegenerator which is responsible for the creation of the forcing data files of the model this component takes the forcing information gathered from the maingenerator and saves it into the user s specified folder 5 1 2 4 initialstatefilegenerator add an initialstatefilegenerator this module is responsible for placing the initial state files in the right folder in the working directory 5 1 2 5 runmoduleagent add the runmoduleagent it is responsible for setting up the path where cyberwater can locate the model s executable file e g c fortran python java and matlab which have been tested so far also the arguments of the execution as well as the model output variables selected and their formats are configured here the user s model can only be executed when all of its inputs are ready which is guaranteed by connecting the output port of each of the three previous components i e areawiseparamgenerator forcingdatafilegenerator and initialstatefilegenerator to the ready list input port of the runmoduleagent as in section 5 1 1 the user can now add the msmshowchart modules to plot the surface runoff and the baseflow for viewing after adding two msmshowchart modules to plot the output01 surface runoff and output02 baseflow ports of the runmoduleagent the user will be able to execute the created workflow shown in fig 10 the vic4 model simulated baseflow and surface runoff results obtained using the generic model agent toolkit are exactly the same as shown in fig 9 5 1 3 coupling the routing agent with the vic4 model simulation using generic model agent toolkit the generic model agent toolkit can also be used to couple different models in the workflow to illustrate we use the outputs of the vic model as the inputs to the routing model this routing model takes datasets of the surface and subsurface runoff i e baseflow as inputs through the maingenerator and the forcingdatafilegenerator and computes the resulting streamflow of a given watershed following the muskingum method other required input parameter files are provided to the routing model through the areawiseparametergenerator module which are prepared offline including those related to geographic information the routing model s core functionalities are compiled in a jar file which requires the presence of java in the local machine to be able to work the runmoduleagent component takes this jar file through the exe input as java jar jar compiled model file jar the workflow of coupling these two models using the generic model agent toolkit is given in fig 11 the simulated streamflow results from the coupled models are depicted in fig 12 5 1 4 constructing model agent for vic5 the more recent available version of the vic model is vic5 vic version5 0 which has some significant differences in the model s i o from those in the vic4 version consequently the model agent previously developed or coded for vic4 no longer works for vic5 v5 0 however with the generic model agent toolkit a user can easily reconstruct a new model agent for vic5 the following example illustrates the construction of the vic5 model agent and its associated workflow to simulate the water and energy fluxes for a different and large watershed using the cyberwater system in this example an hourly model simulation of a large watershed inside the state of pennsylvania is conducted the watershed is the west branch susquehanna wbs river basin covering more than 17 700 square kilometers this river basin is selected to illustrate that cyberwater can scale up to handle large watersheds in this case a new vic agent is constructed using the generic model agent toolkit and is executed in an energy balance mode using vic5 this watershed includes 299 modeling cells at 1 8th degree resolution like the french creek watershed example all the parameter values used in this simulation are default values that can be modified later by the user to perform parameter calibrations the simulation is set up for a period of time between 2010 01 01 00 00 00 and 2010 03 01 00 00 00 since this simulation is hourly it requires more than 1 400 data files per forcing variable as each data file represents 1 h i e one time step covering the entire watershed with 299 modeling cells at a spatial resolution of 1 8 per cell in other words each forcing data file is a map with 299 cells representing 1 h time step to be automatically retrieved from nasa the space range of the wbs watershed is given by the limits 76 213 78 9155 41 933 and 40 454 i e x max x min y max y min respectively eight nldasagent boxes are used to bring hourly forcing data from the nasa data source into cyberwater for the following eight variables temperature k pressure pa radiation flux long wave w m2 radiation flux short wave w m2 total precipitation mm h specific humidity kg kg u wind m s and v wind m s cyberwater offers the msmdatasetoperation module facilitating performing operations between two datasets thus it can be used to compute the magnitude of the wind speed based on information of the two components associated with two directions of the wind speed i e u wind and v wind given by the nasa data source for this example add the msmdatasetoperation module to compute the magnitude of the wind speed x 2 y 2 1 2 required as a forcing variable by vic5 where x and y represent obtained u wind and v wind from the nasa data source respectively then use the msmunitconversion to transform the units of the temperature and pressure datasets in addition the energy balance mode of vic requires the user to provide water vapor pressure as a forcing variable to do so the user needs to read in the specific humidity information provided by nldas data product using the nldasagent then convert it to vapor pressure using the equation below as an approximation 1 v p 1 61 q h p where vp is the water vapor pressure kpa qh is the specific humidity kg kg and p is the pressure kpa this operation can be performed using again the msmdatasetoperation module 5 1 4 1 maingenerator after the data preparation one is ready to construct the vic5 model agent using the generic model agent toolkit first bring a maingenerator box the main control component of the generic model agent toolkit into the working area then have each required input information provided in a similar way as it is illustrated in section 5 1 2 note that the order of the input datasets is determined by the order given in the vic model s global file vic global file val cyberwater vistrails offers a nice feature that allows the user to group multiple modules to facilitate displaying a larger workflow select all the modules between spacerange and maingenerator and go to the workflow group menu on the top toolbar as shown in fig 13 to group these modules 5 1 4 2 areawiseparamgenerator add an areawiseparamgenerator to set up the parameter files e g soil parameter files and vegetation parameter files for the model these files need to be previously created by the user before using the areawiseparamgenerator module again the user needs to provide the required information in a similar way as it is illustrated in section 5 1 2 for the areawiseparamgenerator module 5 1 4 3 forcingdatafilegenerator add a forcingdatafilegenerator for taking care of the forcing data file preparation for the model this component takes the forcing information collected from the maingenerator and saves it into the user specified folder 5 1 4 4 initialstatefilegenerator add an initialstatefilegenerator to place the initial state files into the right folder in the working directory 5 1 4 5 runmoduleagent add the final component the runmoduleagent to complete the construction of the vic5 model agent it is responsible for setting up the path where the executable file of the model is located also the arguments of the execution as well as the selected outputs should be configured here now the user can add the msmshowchart module to plot the surface runoff and baseflow for visualization after adding the two msmshowchart modules to plot the output01 surface runoff and output02 baseflow ports of the runmoduleagent the user will be able to execute the workflow shown in fig 14 the two time series obtained are depicted in fig 15 a and fig 15 b 5 2 dhsvm model simulation the distributed hydrology soil vegetation model dhsvm is a high resolution hydrological model that simulates water and energy fluxes by considering the effects of terrain features wigmosta et al 1994 one of the main differences in the modeling structures between vic and dhsvm is that in the latter terrain or topography information is needed as routing process is part of dhsvm i e the routing process is internally coupled to other processes inside dhsvm in this use case we illustrate simulations of the dhsvm model through both its manually developed dhsvmagent and the agent constructed using the generic model agent toolkit separately the indiantown run basin a small watershed in pennsylvania is used to illustrate these features seven types of forcing data are required for dhsvm pressure precipitation relative humidity temperature wind speed longwave radiation and shortwave radiation since the simulation will be performed at an hourly time step the nldas data agent will be used again however this data source does not provide relative humidity but the specific humidity thus one needs to calculate the relative humidity based on the other given forcing data employing an approach similar to that used in the vic5 example with the msmdatasetoperation module 5 2 1 constructing dhsvm model agent using generic model agent toolkit in this section we show how to integrate the dhsvm model by using the generic model agent toolkit and escape steps that are similar to those described before to construct a workflow running the dhsvm model the user needs to add timerange spacerange nldasagent modules like before and use msmdatasetoperation and msmunitconversion modules to calculate certain forcing input variables based on available input data and to convert units for this example the user needs to use eight nldasagent boxes to retrieve eight different hourly forcing variables which are temperature k pressure pa radiation flux long wave w m2 radiation flux short wave w m2 specific humidity kg kg total precipitation mm h u wind m s and v wind m s and then to compute the relative humidity and magnitude of wind required by dhsvm to run the dhsvm model one needs to use gis 7 7 https en wikipedia org wiki geographic information system geographic information system tools to prepare data for the routing related processes like those involved in the routing model described in subsection 5 1 3 as these routing related processes are part of the dhsvm model one can either prepare these gis related files offline and have them inputted through the areawiseparametergenerator as illustrated in subsection 5 1 3 or prepare them as part of the overall workflow as in this example to facilitate the in workflow preparation of these gis related files required for the integration of models such as the dhsvm model cyberwater has incorporated grass gis 8 8 https grass osgeo org an open source gis software in its msm system a module called gisengine is developed to interface between the msm system and the grass gis system through this gisengine module and other gis related modules developed in cyberwater one can automatically and seamlessly access the grass gis system and easily and effectively make use of the various functionalities offered by grass gis to accomplish the user s various tasks such as identifying the flow directions flow paths and the stream network within the studied area given its dem 9 9 https en wikipedia org wiki usgs dem digital elevation model map file the resulting files from gis are then automatically passed back to the msm system thus all the input files required to be produced by gis can be obtained using our developed gis related modules luna et al 2020 2021a details on the integration of grass gis and its relevant modules developed in the cyberwater system are presented in luna et al 2021b and will be discussed in detail in a separate paper luna et al 2022 in this example most of the gis related input files are automatically prepared and are passed to the areawiseparametergenerator as indicated by the dashed line box parameter file preparation in fig 16 which is achieved by using our developed gisengine and the static parameter agent modules luna et al 2021b 2022 which are grouped together in the parameter file preparation box and described below 5 2 1 1 parameter files preparation this preparation mainly includes the following major steps 1 use four staticbinarymapagent modules which are configured with the soil depth map soil map vegetation map and dem map paths respectively 2 use a gisdefinewatershed module configured with the path of the dem map 3 use a streammapandroutingfiles dhsvm module configured with the north coordinator south coordinator west coordinator east coordinator as mask limits 4 group all the modules above in a module box for a neat and concise workflow outlook and name these grouped components as parameter file preparation as shown in fig 16 5 2 1 2 dhsvm model agent construction with the toolkit 1 add a maingenerator module and create a new working directory folder on the user s computer where the dhsvm simulation will occur for this example this folder will be saved on c temp cyberwater the inputs are set up in a similar way as described in subsection 5 1 4 for vic5 but the variables corresponding to each dataset are different since they are determined by the model s global file or configuration file 2 add an areawiseparamgenerator module and the inputs are set up in a similar way as described in subsection 5 1 4 for vic5 3 add forcingdatafilegenerator to take care of the forcing data file preparation for the dhsvm model this component takes the forcing information collected from the maingenerator and saves it into the user specified folder 4 add an initialstatefilegenerator module and let the path be the folder path of c temp cyberwater gt dhsvm irun10years source 5 add a runmoduleagent module and its inputs are set up in a similar way as described in subsection 5 1 4 for vic5 the newly created workflow after execution is shown in fig 16 the resulting plot after the execution of this workflow is depicted in fig 17 in contrast fig 18 shows the complete workflow if the user employs the manually written dhsvm model agent for integrating the dhsvm model 5 3 discussion based on our investigation of the popular hydrological and environmental models it shows that the complexity of these models interface structures often lies in the organizations of models input files with many different options types e g soil and vegetation related libraries and parameter files snow related information and gis related information rather than the organizations and structures of the models output files thus we designed three components to handle a given model s input interface organizations i e areawiseparamgenerator forcingdatafilegenerator and initialstatefilegenerator so that each component would not be too complicated for users to configure these three components correspond to the three major types that a typical model would have on the other hand as the model s output interface organizations and structures are usually straightforward e g values of computed state variables fluxes and input information organized either in time series or spatial maps we thus combined the receiving of the model s outputs into the component handling the model s execution e g runmoduleagent rather than creating a dedicated component for that furthermore the organizations and structures of the computational models interfaces can be briefly classified into the following two categories 1 the model itself does not include the use of gis related information and or functionalities such as vic 2 the model itself utilizes gis related information and or functionalities as well such as the routing model and dhsvm where the routing processes are included the former does not directly involve gis to handle its area wise parameter files while the latter usually directly involves the use of gis for handling its area wise parameter files to facilitate the latter case the grass gis is integrated into the cyberwater system via our developed gisengine and its related modules in addition the static parameter agent modules are developed to automatically prepare gis related input files for such models which will be discussed in a separate paper luna et al 2022 once the gis related input files are created they are passed to the areawiseparametergenerator as indicated in the use case of subsection 5 2 1 we note that among the three types of the input data files the structure and format interfaces of the forcing data files are less heterogeneous and complex compared to those of the parameter files and initial state files among the different hydrological and environmental models for the forcing data files the majority of the models e g vic dhsvm routing and casa cnp have their formats fall into two types namely single point time series or spatio temporal maps these two types are both taken care of inside cyberwater s forcingdatafilegenerator module which also allows for customizable time stamps for the data in addition the forcingdatafilegenerator module allows the users to modify the order of the columns inside the forcing data files to represent different forcing variables read in so that the order of the variables involved in the forcing data files can be adjusted to match the structure requirements of different models forcing data files for the input parameter files and initial state files however interfaces of the structures and formats of the individual files are then taken care of by the static parameter agent modules developed in cyberwater luna et al 2021b 2022 we also note that the generic model agent toolkit not only makes the integration of one s model an easier task but also significantly saves user s time and efforts and reduces the error prone process of writing the model agent codes since one does not need to write any code to integrate a model into the cyberwater system by manually writing a model agent to integrate a model one needs to not only write codes initially but also to re write the codes whenever the model s i o interface changes as shown in the example of using vic4 version 4 versus vic5 version 5 on the other hand due to the existing large number of diverse types of models it is possible that some unusual model structures and features might not be covered by the generic model agent toolkit we have developed in such a case the option of manually writing a model agent in cyberwater provides more flexibility consequently cyberwater provides users with the flexibility to either use the generic model agent toolkit for most models or to manually write their own model agents when needed for any model with unusual i o structures from the illustrations of the use cases in sections 5 1 and 5 2 we can see the flexibility and straightforwardness of using the generic model agent toolkit in integrating various computational models with significantly different model input structures and formats without the need of writing any code the strengths of using the visual drag and drop approach of the generic model agent toolkit to construct a model agent for integrating a model into the cyberwater system as opposed to the traditional approach of writing programming codes e g the vicagent and dhsvmagent include 1 usability ease of use one can easily construct a model agent with the toolkit to integrate a model into cyberwater without writing computer codes in which the functionalities and responsibilities of the individual modules are clear the relationships connections between different modules in the toolkit provide a clear overview and big picture of a model s data file structure organization logic and its workflow processes making model integration an easier task reducing the error prone code writing process and saving time 2 clarity the organizations and structures of a model s input output files can be clearly seen from the five six modules included in the generic model agent toolkit for example from the configuration panel associated with each module users can easily see where the model s input files are located and what they are such a neat visual presentation helps users identify missing information and debugging 3 generality our toolkit is applicable to a wide range of computational models since most of the models have similar input file organizations e g global file configuration file parameter file forcing data file and initial state file in fact we have been successfully applied the toolkit to integrate other hydrological and environmental models with ease luna et al 2020 such as the biogeochemical model casa cnp the carnegie ames stanford approach model with carbon nitrogen and phosphorous cycles which computes the net primary productivity of terrestrial ecosystems together with detailed distributions of the amount of carbon nitrogen and phosphorous present in the biosphere and its surroundings wang et al 2010 and the usgs environmental model phreeqc model which is used to calculate a variety of aqueous geochemical reactions and processes in natural waters or laboratory experiments parkhurst and appelo 1999 4 agility any changes in a model s interface can be easily accommodated with the toolkit for example when a model s inputs structure is changed as shown in section 5 1 vic4 versus vic5 one can easily reconstruct the model agent for vic5 using the toolkit instead of rewriting hundreds of new codes to have a new vicagent 6 conclusion in the fields of sciences and engineering the need for open data and open model integration is urgent while our previous work on the open data and open modeling framework of msm lays a solid foundation for effectively and efficiently addressing the challenge faced by the broad community this new work on the development of the generic model agent toolkit enables modelers to construct their own model agents without coding therefore further simplifying the data and model integration task by eliminating users effort of writing model agent code this is achieved by the innovative design and development of a suite of feature oriented code templates that extract each individual model agent s characteristics and therefore reduce the model agent programming work to the template based components configuration task this approach of the generic model agent toolkit in cyberwater is capable of applying to diverse models developed in the broad community without modifying any code of the original models or writing any model agents code with cyberwater users can individually and easily make their models pluggable into comprehensive end to end workflows with automated and seamless access to various online data sources for their scientific study without any need for a central control administration or service therefore our presented approach not only provides a general and elegant solution for open data and open model integration for complex modeling systems but also offers a truly sustainable and scalable solution for broad applications the current limitation of the generic model agent toolkit is that it only works for grid based models as we know there are hydrological models developed based on hillslope units or sub watershed units all of which have irregular modeling grids our future work is to extend the generic model agent toolkit to handle irregular grid based models we also plan to extend cyberwater to include web services to further complement its current standalone system which can better support education in colleges and universities as most universities have very strict limitations for software installations on their laboratory computers declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the u s national science foundation under oac 1835817 to indiana university purdue university indianapolis iupui and oac 1835785 to the university of pittsburgh respectively 
25592,the cyberwater project is created to develop an open data and open model integration framework for studying complex environmental and water problems where diverse online data sources can be directly accessed by diverse models without any need of users extra effort on the tedious tasks of data preparation for their models we present our design and development of a novel generic model agent toolkit in the context of cyberwater which enables users to integrate their models into the cyberwater system without writing any new code significantly simplifying the data and model integration task cyberwater adopts a visual scientific workflow system vistrails which also supports provenance and reproducible computing our approach and the developed generic model agent toolkit are demonstrated via cyberwater framework with automated and flexible workflows through integrating data and models using real world use cases two popular hydrological models vic and dhsvm are used for illustrations keywords open data and model model integration generic model agent scientific workflow reproducible process software data availability name author contact year first available format language cost size availability vistrails new york university https www vistrails org index php people 2007 python free 300 mb https www vistrails org index php main page vic university of washington http uw hydro github io team 1994 c free 2 mb https vic readthedocs io en master dhsvm pacific northwest national laboratory ning sun pnnl gov mark wigmosta pnnl gov 1994 c free 60 mb https www pnnl gov source code usgs water services usgs https water usgs gov contact gsanswers n a xml webservices free n a https waterdata usgs gov nwis dv nca ldas nasa gsfc dl help disc mail nasa gov 2018 tiff grib or netcdfwebservices free n a https doi org 10 5067 7v3n5do04mas nldas nasa gsfc dl help disc mail nasa gov 2009 tiff or netcdfwebservices free n a https doi org 10 5067 6j5lhhohzhn4 1 introduction with diverse data across disciplines being increasingly available on today s internet it has become more urgent and important to have a framework facilitating data and computational model integration for studying problems involved in many complex processes e g physical hydrological biological and atmospheric such as predictions of floods droughts water quality and air quality while there are many online data sources available on the internet the key challenge is how to make these data sources directly accessible to various models without users additional effort on data preparation or data preprocessing for their models such model input data preparation task is usually very time consuming tedious and error prone due to the fact that individual data sources offer their different access protocols and have their different data organizations and formats previous research focused more on model integration argent 2004 belete et al 2017 and the papers therein with little consideration or treatment on the general problem of integrating data and models together for example existing modeling systems for different needs in environmental and water fields include open modeling interface openmi 1 1 https www openmi org moore and tindall 2005 gregersen et al 2007 knapen et al 2013 harpham et al 2019 community surface dynamics modeling system csdms 2 2 https csdms colorado edu wiki main page peckham et al 2013 university of colorado boulder 2012 object modeling system oms 3 3 https alm engr clolstate edu cb wiki 16961 ahuja et al 2005 david et al 2013 earth system modeling framework esmf 4 4 https earthsystemmodeling org collins et al 2005 deluca et al 2012 hill et al 2004 the invisible modeling environment time rahman et al 2003 stenson et al 2011 and geographic modeling and simulation systems opengms chen et al 2019 zhang et al 2019 wang et al 2020 these modeling systems do not directly address the challenge of enabling users models to access the heterogeneous data sources automatically and seamlessly over the internet to carry out their modeling studies while some of these systems support public sharing of data modeling and or simulation resources they are generally limited to some specific models and data rather than providing a general framework to integrate diverse data and models like cyberwater although openmi2 0 and opengms for example increase the flexibility by turning numerical models into linkable components to receive exchange data from each other there is remaining work for these systems to wrap up the existing models to be feasible components running in these frameworks salas et al 2020 provided comprehensive reviews of these systems and compared them with the earlier version of the cyberwater framework a recent effort aiming at developing a systemic solution to this challenge the key idea and feature of the cyberwater system is its focus on building an open architecture framework to facilitate realizations of open science salas et al 2020 which facilitates resource sharing e g data and models reproducibility of the work and community participation to this end an open data and open modeling framework salas et al 2020 referred to as msm meta scientific modeling that provides the foundation for the cyberwater framework is developed msm is designed to offer scientific researchers and practitioners a sophisticated open modeling environment where data agents are developed to access heterogeneous online data products provided by different data sources so that the data can directly flow seamlessly from the data sources to the users models without any need for the users to preprocess the input data for their models the model input data preparation has been performed by each individual data agent developed in msm for its corresponding data source such as the usgs and nasa data products the implementation of msm is called msm which incorporates the workflow system of vistrails 5 5 https www vistrails org index php main page to provide not only a graphical workflow mechanism for achieving the modeling task but also data provenance to ensure traceability and reproducibility salas et al 2020 to make use of the facilities of data agents in msm however one needs to integrate his her model into the msm system by means of writing a model agent based on the interface provided by msm once a user s model is integrated into msm via its agent the user s model can use all the data agents available in the msm system for direct data and model integration and can also couple his her model with another model already integrated into msm for model to model coupling integration without coding another recent work on the model input data preparation is hydrods web services gichamo et al 2020 which is applicable to two specific models the utah energy balance ueb snowmelt model tarboton et al 2014 and the topnet hydrologic model bandaragoda et al 2004 the use of hydrods an integration of model ueb with data is demonstrated through the development of a web application gan et al 2020 the strength of hydrods a webservice based data and model integration approach is that there is no need for local software installation its limitations include not supporting other models beyond ueb and topnet nor providing users with any mechanism to integrate new models into hydrods cyberwater is a standalone system at present which avoids the potential bottleneck of the centralized server facilitating scalability and sustainability cyberwater will be extended to include web services as well in the future to further complement its standalone system for the webservice based systems chen et al 2020 provided a comprehensive review while the msm framework has its unique merit for open data and open model integration it nevertheless still requires programming a model agent per new model integration to write a model agent means that the user needs to do programming coding to overcome such a limitation this work extends the previous msm framework here we present a general approach to construct a generic model agent template using workflow for handling models input interfaces for the environmental and hydrological models we then design and develop a set of tools called generic model agent toolkit for our approach with which a user can construct his her model agent by simply doing parameter based configuration steps without coding for most of the environmental and hydrological models this effort together with other extensions to and improvements on the msm system including accessing high performance computing on demand li et al 2021 constitutes the latest development of our cyberwater framework software system in this paper we present the design and development of the generic model agent toolkit in cyberwater we then demonstrate how this set of tools can significantly eliminate the user s task of writing a model agent in order to integrate a new model into the system and thus further enhance the usability and effectiveness of the cyberwater framework for open data and model integration and greatly improve the capability of the community to share diverse models for model validation evaluation scientific explorations etc different versions of the variable infiltration capacity vic model hamman et al 2018 liang et al 1994 1996a 1996b liang and xie 2003 and the distributed hydrology soil vegetation model dhsvm wigmosta et al 1994 are used in this study for illustration via use cases over several watersheds in pennsylvania usa the remainder of this paper is organized as follows section 2 briefly overviews the background to set up the context section 3 presents our approach and design while section 4 describes the implementation of the generic model agent toolkit section 5 provides two use cases of reproducible end to end model simulations to demonstrate the use of the developed generic model agent toolkit for open data and model integration in cyberwater and offers our insights through discussions finally section 6 concludes the presented work and provides planned future work 2 background 2 1 msm architecture msm meta scientific modeling is an open data open model framework that provides a sophisticated workflow controlled modeling environment for heterogeneous data and model integration without a need for a central administration salas et al 2020 the implementation of the msm framework having vistrails bavoil et al 2005 incorporated in it is called msm system which running on the user s local desktop or laptop constitutes several components including the core i e msm core data agents and model agents basically the msm system interacts with the workflow engine of vistrails accesses remote heterogeneous data sources through data agents and invokes users computational models for simulations through model agents the msm core is the center of all the components in the open data open model framework through its connection with the workflow engine of vistrails the msm system invokes external models and couples various data sources with respective model agents and data agents via a friendly and feasible workflow controlled working environment therefore the msm enables access to external data of diverse sources conveniently and effectively and execution of sophisticated numerical models efficiently the open model feature allows various models or modules to be smoothly integrated into the msm system via model agents the architecture of msm offers a novel information bus connection pattern of linear complexity i e the complexity of the development of data and model agents is o m n for integration of m data sources and n computational models the lowest possible complexity among all independent models and external heterogeneous data sources as opposed to quadratic complexity involved in a pair wise based one to one integration architecture i e the complexity of o mn n 2 for integration of m data sources and n computational models salas et al 2020 in the msm framework the modeling processes are dynamically decided by the end users through a workflow engine users can customize the sequence of activities and construct any workflow series for their modeling process based on the building blocks or modules provided by the msm system each workflow activity conducts interactively with the other workflow activities in the same workflow msm s design criteria enable a creation of a general and flexible system so that the msm framework can be easily plugged into various open source workflow engines the msm takes advantage of the existing workflow engines and avoids reinventing the wheel the overall architecture of the msm system is shown in fig 1 2 2 model agent in msm various computational models are added into the msm modeling framework by means of their corresponding model agents any model added into msm can then be freely integrated with various data agents as well as other models in a workflow constructed by the user where model agents represent and execute the corresponding external models in the workflow salas et al 2020 thus the effort to add a model into the msm modeling framework is to develop the model s agent since no modification of the original model code is required in the msm system a model agent needs to perform three main tasks as follows preparing model s inputs the agent collects the model s required input data from the workflow and transforms them into the model s input files which are needed by the model s execution executing model the agent invokes the model executable file according to the workflow control and runs the model retrieving model s outputs after finishing the model s execution the agent needs to retrieve the model s output files and transform the output data into the msm system in the format of msmdataset before the subsequent workflow item gets to be executed the generic agent is a class developed in the msm system to assist users in their development process of model agents in the generic agent base class a generic run model method is declared each model s agent is inherited from the generic agent class and overrides the generic run model method enabling the customized model s invocation from the workflow the model agent uses services offered by the generic agent class to read the inputs from and write the outputs into the msm core datastore which simplifies the agent s coding for integrating an external model into the msm system the model agent s primary responsibilities are to be implemented by overriding the run model function that is first read the model inputs from the msm core and save the data into the model s input files then run the model executable file finally read the model s output files after the model s execution transform them into the msm dataset format and save the model s outputs into the msm core to write a model agent the user has to create a text file including a class in python as outlined here image 1 in the run model function the user writes all the necessary codes to prepare the inputs execute the model and store the outputs these are the three main tasks that the model agent is supposed to perform to integrate the user s model into the msm system for more details readers are referred to salas et al 2020 3 approach 3 1 the idea while the generic agent class in the msm system reduces users burden to develop their model agents to some extent it would be desirable if users model agents can be constructed without any coding since in general any model agent will conduct the three tasks described in section 2 2 our idea is to construct a generic model agent template for most environmental and hydrological models which predefines outlines tasks of preparing model inputs executing model and storing outputs in a workflow environment in the msm system all workflow modules can be dynamically defined and configured thus using the divide and conquer technique a generic model agent can be achieved by first composing a generic workflow segment template of predefined generic modules as part of the overall workflow in the msm system such a workflow segment template will then be configured by individual users for their specific models where each model s variability is accommodated via the configuration of the generic workflow segment template the predefined generic modules represent the abstraction of each aspect of the generic model agent this way the variability of an individual model is treated and represented through parameter based configuration options of the workflow segment where specific information concerning an individual model such as the model s parameter values is configured via a graphic user interface of the input panel associated with the configuration option therefore a user no longer needs to write any specific model agent code to integrate an external model into the msm system which greatly enhances the usability of the msm modeling framework for achieving the data and model integration 3 2 the design to fulfill our idea we attempt to systematically predefine and design a set of generic workflow modules i e components to be used for constructing a model agent for an individual model in the form of a generic workflow segment i e template where each component represents and fulfills an individual task that a user s specific model agent would accomplish to this end our design strategy is as follows a we developed a set of basic template based workflow components based on an analysis of the representative model interface structures and organizations of most environmental and hydrological models those workflow components are used as building blocks for constructing a generic model agent where each component i e building block is called a generic model agent tool b a specific model agent for a given scientific model is constructed by composing the predeveloped component templates into a workflow segment template which is then incorporated into the user s overall modeling workflow sequence c the workflow segment template is configured by the user for the model simulation needs to serve as the model agent this way the model agent s construction and its corresponding configurations are realized within the msm system via an interactive graphical workflow interface without any coding our design includes the following six major generic model agent tools i e components users are only expected to be knowledgeable about their own models input output details when they use these generic model agent tools for integrating their models into the cyberwater system a model s inputs are typically classified into three categories static parameters forcing data and the initial states of the model 1 maingenerator this is the first component i e the root component to build up a model agent in the form of a workflow segment template for a user s specific model this component is responsible for setting up a working directory for performing the user s model simulation inside this working directory folder all the input information needed for executing the user s model and the output information after the model s execution will be placed the path of the working directory is specified by the user this component is also responsible for receiving the forcing information required for the model s execution the order in which these forcing data are collected to the maingenerator as datasets through its module information panel matters since the final forcing data files created and placed in the working directory folder for the model execution will have these forcing data organized in columns following the same order in addition this maingenerator reads in other important information needed for setting up the required execution condition environment for the user s specific model for example information specifying certain options such as the number of soil layers to be used by the model the water or energy balance mode to be executed etc such information is typically provided through a model s global file or model s configuration file whose path is provided to this component and also through the use of its module information panel this module outputs two pieces of information through its two output ports one is called wd path which provides the working directory s path information where all the model s needed input information and the model s output information will reside the other is called dataset class which is a list of all the datasets comprising the model s forcings 2 areawiseparamgenerator this component organizes parameter files e g vegetation and soil related parameter files and places the imported parameter files into the parameter folder created by it this parameter folder will be automatically placed under the working folder provided by the maingenerator module information on all the paths of the parameter files needed for executing the user s model is collected in this module by specifying them through the module s information panel from the user interface this module outputs a ready signal which will be discussed later 3 forcingdatafilegenerator this component is responsible for creating the forcing data for the user s model specifically this component organizes the forcing data brought in by the maingenerator the user should be aware of the expected format of the model s forcing data by default this module creates the forcing data to be used by the user s model in a folder named forcing inside the working directory such forcing data are always created as a time series where different columns hold different variables according to the order specified in the maingenerator the data are divided into separate files where each one represents a single modeling cell unit of the forcing inputs this module has two input ports one is the working directory and the other is the list of forcing datasets both of which are provided by the maingenerator module like the areawiseparamgenerator this module also outputs a ready signal 4 initialstatefilegenerator this is an optional component which organizes the data of initial states for the user s model as the initial state files are not always required for the execution of a given model however if a user would like to conduct data assimilation to improve the user s model forecast reliability the user may need to use this module in any event if required this module receives the information of the working directory from the maingenerator module also the initial states file paths will be specified through this module s information panel and be placed either inside the working directory or a new directory created by the initialstatefilegenerator with a name given as an input from the user the output of this module is again a ready signal 5 runmoduleagent this component is responsible for invoking the user s model e g executable on the local machine and for retrieving the model s outputs it executes the model in the same way as the model runs alone without the msm system environment where the user must manually prepare all the input files e g forcing data parameter files initial state files of the model this module has three pieces of input information first it connects to the output of the areawiseparamgenerator forcingdatafilegenerator and initialstatefilegenerator if required separately to receive their ready signals second it receives the information of the working directory from the maingenerator third it receives the dataset information outputted from the maingenerator this module also requires the path where the model s executable file is located by setting up all the aforementioned information and receiving all the required information passed onto it from the other modules components this runmoduleagent component is now ready to execute the user s model the model s simulated results will be written back to the cyberwater system for the workflow to continue 6 hpc this component is the module responsible for accessing remote high performance computing hpc facility on demand this module aims to provide high performance computing capacity for executing the user s model by seamlessly connecting it to either academic supercomputers or commercial cloud platforms li et al 2021 it retrieves the results back to the workflow when the execution of the user s model on the remote hpc platform is completed in a nutshell regarding the three tasks that a model agent needs to perform sec 2 2 preparing model s input is fulfilled by a combination of three corresponding generic components i e components 2 3 and 4 described above while the tasks of both executing model and retrieving model s outputs are combined and fulfilled by the generic component runmoduleagent component 5 above or hpc component 6 above depending on whether the model is executed on a local machine or a remote hpc platform based on our approach users construct their model agents using the generic model agent toolkit i e the six modules or building components described above at the same time as they employ the msm system to build their overall modeling workflows fig 2 illustrates a typical workflow construction for hydrological environmental modeling in the msm system with a graphical workflow interface the dotted block indicates how the generic workflow components presented above are used to construct and configure a user s model agent in the workflow after the completion of the user s construction and configuration of the workflow the workflow is then executed in the msm system a sequence diagram of a typical modeling workflow with the constructed model agent using the generic model agent toolkit is illustrated in fig 3 in summary the generic model agent toolkit is designed to help the user construct a model agent to integrate the user s model into cyberwater without writing any codes such a goal is achieved through some combinations of the six modules in the toolkit together to perform the following main functionalities set up the user s model working directory write data sets retrieved from data agent s or local files into model input files that is prepare input data files for the user s model invoke the user s model codes to run the user s model write the user s model output results back to the cyberwater system 4 implementation the generic model agent toolkit called genericmodelagenttools is developed and implemented in python outside of the msm system as an extensive vistrails package which is a significant extension to the original msm system in the overall cyberwater system however the genericmodelagenttools need the support of the corresponding dao and cache modules of the msm system to access data sets flowed into the workflow modules the hierarchical architecture of the cyberwater system is shown in fig 4 each generic model agent tool in the toolkit is designed and implemented as a template based workflow module to illustrate we present in the following the implementation of the template based generic model agent tool maingenerator in sections 4 1 and its configuration in section 4 2 respectively 4 1 generic model agent tool as a template the maingenerator creates a working directory folder as a workspace for the user s model where it hosts all the input information required for executing the user s model and the output results of the model after its execution copies the model s global parameter file or configuration file into this created working directory for carrying out the model s execution task and converts the datasets into dataset class elements in the form of python dictionaries this component also gathers all the forcing datasets required by running the user s model the maximum number of the maingenerator s input ports for datasets is set to be 15 as a default for convenience however the input ports of the maingenerator can be expanded if needed hence in the template based component maingenerator the path of the working directory the path of the model s global parameter file and the datasets of the forcing to be used are all configurable by the user for illustration the pseudocode of the maingenerator is given in fig 5 in general there are three types of input files needed for executing a numerical model namely parameter files forcing data files and initial state files the areawiseparamgenerator organizes the parameter files for performing the numerical model the forcingdatafilegenerator creates the model s forcing data files and the initialstatefilegenerator is engaged in preparing the initial state files if needed for performing the numerical model simulation the runmoduleagent module is for setting up the path where the model s executable file is located it also obtains the information of the arguments for executing the model and the configuration information for the model s output variables during the workflow execution the areawiseparamgenerator forcingdatafilegenerator and initialstatefilegenerator modules process the datasets to generate the needed input files for the user s model the module of the runmoduleagent invokes the user s model execution based on the datasets passed the model s simulated results will be written back to the cyberwater system similar to the maingenerator each of these template based components also has its places configurable for the user to construct part of the user s model agent 4 2 configuration of generic model agent modules a configuration example with the maingenerator fig 6 is used here to illustrate how the user can configure a generic model agent module a input port specification 01 path the path of the working directory folder where files for executing the user s model will reside 02 gpf the path of the global or configuration file if such a file is required by the user s model this file is commonly used for setting up the variables for the model s execution dataset x collecting the forcing dataset x x 01 02 03 15 from the msm system in which x indicates the index of the dataset and passing the forcing datasets to the forcingdatafilegenerator to generate the required forcing files for the user s model to run b output port specification wd path an output port that passes the working directory path information to the next module dataset class another output port that passes the information of a dataset cluster i e the information of the forcing datasets to the next module 5 use cases there are two approaches to integrate a new model into cyberwater one is to program a model agent by the user while the other is to use the generic model agent toolkit provided in cyberwater to build the model agent without coding this section shows how to achieve the integration of a user s model into the cyberwater system using our developed generic model agent toolkit without writing a single line of code the user s model integration will be illustrated using our generic model agent toolkit versus adopting the user s manually programmed model agent two use cases with the vic model and dhsvm model integrations are provided in sections 5 1 and 5 2 respectively 5 1 vic model simulation cyberwater provides various modules that help users construct their model simulation workflow to directly access online data from diverse data sources e g forcing data from nasa and streamflow data from usgs execute models and produce model simulation results in this use case the variable infiltration capacity vic model liang et al 1994 1996a 1996b liang and xie 2003 is employed for illustration first we show the integration of the vic model version 4 0 vic4 into the cyberwater system via the user s manually coded model agent called vic agent the integrated vic4 runs a water balance simulation of the french creek basin in cyberwater this is a small watershed with a drainage area of 153 km2 located in the southeast of pennsylvania united states note that all the model parameters in this simulation are default values that the user can further modify through a parameter calibration process 5 1 1 workflow with manually developed vic model agent we first show how to build a complete example of a model simulation workflow using the vic model via a user s manually developed vic agent in general creating a model simulation workflow starts with model input data preparation based on various data sources for a given study area over a given time period thus the workflow starts first with a timerange module which is used to specify the simulation time extend for this example the simulation will go from 2010 01 01 00 00 00 time start to 2011 01 01 00 00 00 time end then a spacerange module is added to specify the study area with four bounds i e east west north and south as 75 58 75 86 40 24 40 10 x max x min y max y min respectively to access the forcing information required for this simulation the ncaldas data agent will be used this agent accesses nasa ncaldas 6 6 https ldas gsfc nasa gov nca ldas forcing earth data database to automatically bring nasa s spatially organized daily hydrometric information to the local machine four ncaldasagent boxes are added into the workflow area each one of these four new boxes will be used to retrieve a different variable of the forcing data the four forcing variables required to execute vic4 include wind speed m s total precipitation rate mm s temperature max k and temperature min k now bring the vic4 model into the workflow the user s model is added into the workflow through its model agent so the vicagent module developed by us salas et al 2020 is added to the workflow area basically this vicagent enables the usage of the vic model in the cyberwater workflow it collects vic forcing information either extracted via cyberwater s data agents e g wind speed and precipitation online or reads in the forcing data from files locally provided by the user it also reads in all the parameter files provided by the user to run the vic model if these parameter files are not provided the vicagent will generate all the parameter files required for the simulation using default values automatically values of the parameters in these files can later be modified or calibrated by the user for re running the vic model the vicagent also provides users the flexibility of selecting their desired output variables from the vicagent interface for visualizing them through the workflow execution the workflow chart shown in fig 7 represents the main functionalities accomplished by the vicagent code note that vic like any other model requires inputs to be included in specific units depending on the individual variables details of such input information are included in the vicagent documentation available via the documentation button in cyberwater for example the temperature must be in celsius and for this case study the precipitation will need to be in mm per day since the expected results will be daily thus the user only needs to check if the units of the retrieved data from data providers match the units required by the user s model if not unit transformations are needed the msmunitconversion module can be used to perform such transformations for instance to convert the temperature from the ncaldas dataset in kelvin into celsius the user needs to set the input operation to be x 273 15 this operation allows the user to execute simple mathematical operations using x as the variable representing the dataset given as an input to transform the input precipitation data from mm per second to mm per day for example the user just needs to set the operation to x 86400 by creating another msmunitconversion box this module also allows the user to indicate the new resulting units of the transformed dataset the last step for creating this workflow is to set up visualization tools which will allow the user to see the model simulated results for example the baseflow and surface runoff this visualization can be achieved by adding two msmshowchart boxes into the workflow and naming them baseflow and surface runoff respectively the completely created workflow for this vic simulation case is shown in fig 8 after executing this workflow two charts shown in fig 9 a and fig 9 b will be prompted this example produces outputs of these two variables offered by default in the vic agent however if the user goes to the outputs tab in the module info panel more model output results available to be displayed are listed 5 1 2 constructing vic4 model agent using generic model agent toolkit this section shows how to use the generic model agent toolkit presented in sections 3 and 4 to construct configure the vic4 model agent for the model integration instead of manually writing the vic4 model agent code the important benefit here is that a user can integrate the user s own model into the cyberwater environment using this generic toolkit with little or no programming effort to use this toolkit users are only expected to be knowledgeable about their own models inputs and outputs organizations our following example of the vic4 model illustrates a simulation for the same period of time over the same study area described in section 5 1 1 thus the time range space range and forcing data construction part of the workflow is the same as those described in section 5 1 1 here we focus on steps to construct the vic4 model agent using the toolkit 5 1 2 1 maingenerator to start constructing the vic model agent first bring a maingenerator box the main control component of the generic model agent toolkit into the workflow working area this component is responsible for setting up the working directory for the model simulation where it receives all the forcing datasets as inputs the users can adjust the paths accordingly for their specific folder structure for the working directory for this example this working directory folder will be saved on c temp cyberwater gt vic4 french creek thus the inputs of path and gpf can be set up respectively as c temp cyberwater gt vic4 french creek mainagent and c temp cyberwater gt vic4 french creek source vic global file val in addition all forcing data are collected by the maingenerator by connecting the forcing data modules to this maingenerator the working directory path information will be passed via its wd path output port to all the other components in the toolkit to be used for constructing the user s model agent 5 1 2 2 areawiseparamgenerator add an areawiseparamgenerator which is in charge of setting up the parameter files e g soil parameter files and vegetation parameter files of the model these files need to be previously created by the user the user needs to enter the information of the file s path for each of the parameter files required as the model s inputs in the areawiseparamgenerator module for this example the files used are stored in the folder c temp cyberwater gt vic4 french creek source 5 1 2 3 forcingdatafilegenerator add a forcingdatafilegenerator which is responsible for the creation of the forcing data files of the model this component takes the forcing information gathered from the maingenerator and saves it into the user s specified folder 5 1 2 4 initialstatefilegenerator add an initialstatefilegenerator this module is responsible for placing the initial state files in the right folder in the working directory 5 1 2 5 runmoduleagent add the runmoduleagent it is responsible for setting up the path where cyberwater can locate the model s executable file e g c fortran python java and matlab which have been tested so far also the arguments of the execution as well as the model output variables selected and their formats are configured here the user s model can only be executed when all of its inputs are ready which is guaranteed by connecting the output port of each of the three previous components i e areawiseparamgenerator forcingdatafilegenerator and initialstatefilegenerator to the ready list input port of the runmoduleagent as in section 5 1 1 the user can now add the msmshowchart modules to plot the surface runoff and the baseflow for viewing after adding two msmshowchart modules to plot the output01 surface runoff and output02 baseflow ports of the runmoduleagent the user will be able to execute the created workflow shown in fig 10 the vic4 model simulated baseflow and surface runoff results obtained using the generic model agent toolkit are exactly the same as shown in fig 9 5 1 3 coupling the routing agent with the vic4 model simulation using generic model agent toolkit the generic model agent toolkit can also be used to couple different models in the workflow to illustrate we use the outputs of the vic model as the inputs to the routing model this routing model takes datasets of the surface and subsurface runoff i e baseflow as inputs through the maingenerator and the forcingdatafilegenerator and computes the resulting streamflow of a given watershed following the muskingum method other required input parameter files are provided to the routing model through the areawiseparametergenerator module which are prepared offline including those related to geographic information the routing model s core functionalities are compiled in a jar file which requires the presence of java in the local machine to be able to work the runmoduleagent component takes this jar file through the exe input as java jar jar compiled model file jar the workflow of coupling these two models using the generic model agent toolkit is given in fig 11 the simulated streamflow results from the coupled models are depicted in fig 12 5 1 4 constructing model agent for vic5 the more recent available version of the vic model is vic5 vic version5 0 which has some significant differences in the model s i o from those in the vic4 version consequently the model agent previously developed or coded for vic4 no longer works for vic5 v5 0 however with the generic model agent toolkit a user can easily reconstruct a new model agent for vic5 the following example illustrates the construction of the vic5 model agent and its associated workflow to simulate the water and energy fluxes for a different and large watershed using the cyberwater system in this example an hourly model simulation of a large watershed inside the state of pennsylvania is conducted the watershed is the west branch susquehanna wbs river basin covering more than 17 700 square kilometers this river basin is selected to illustrate that cyberwater can scale up to handle large watersheds in this case a new vic agent is constructed using the generic model agent toolkit and is executed in an energy balance mode using vic5 this watershed includes 299 modeling cells at 1 8th degree resolution like the french creek watershed example all the parameter values used in this simulation are default values that can be modified later by the user to perform parameter calibrations the simulation is set up for a period of time between 2010 01 01 00 00 00 and 2010 03 01 00 00 00 since this simulation is hourly it requires more than 1 400 data files per forcing variable as each data file represents 1 h i e one time step covering the entire watershed with 299 modeling cells at a spatial resolution of 1 8 per cell in other words each forcing data file is a map with 299 cells representing 1 h time step to be automatically retrieved from nasa the space range of the wbs watershed is given by the limits 76 213 78 9155 41 933 and 40 454 i e x max x min y max y min respectively eight nldasagent boxes are used to bring hourly forcing data from the nasa data source into cyberwater for the following eight variables temperature k pressure pa radiation flux long wave w m2 radiation flux short wave w m2 total precipitation mm h specific humidity kg kg u wind m s and v wind m s cyberwater offers the msmdatasetoperation module facilitating performing operations between two datasets thus it can be used to compute the magnitude of the wind speed based on information of the two components associated with two directions of the wind speed i e u wind and v wind given by the nasa data source for this example add the msmdatasetoperation module to compute the magnitude of the wind speed x 2 y 2 1 2 required as a forcing variable by vic5 where x and y represent obtained u wind and v wind from the nasa data source respectively then use the msmunitconversion to transform the units of the temperature and pressure datasets in addition the energy balance mode of vic requires the user to provide water vapor pressure as a forcing variable to do so the user needs to read in the specific humidity information provided by nldas data product using the nldasagent then convert it to vapor pressure using the equation below as an approximation 1 v p 1 61 q h p where vp is the water vapor pressure kpa qh is the specific humidity kg kg and p is the pressure kpa this operation can be performed using again the msmdatasetoperation module 5 1 4 1 maingenerator after the data preparation one is ready to construct the vic5 model agent using the generic model agent toolkit first bring a maingenerator box the main control component of the generic model agent toolkit into the working area then have each required input information provided in a similar way as it is illustrated in section 5 1 2 note that the order of the input datasets is determined by the order given in the vic model s global file vic global file val cyberwater vistrails offers a nice feature that allows the user to group multiple modules to facilitate displaying a larger workflow select all the modules between spacerange and maingenerator and go to the workflow group menu on the top toolbar as shown in fig 13 to group these modules 5 1 4 2 areawiseparamgenerator add an areawiseparamgenerator to set up the parameter files e g soil parameter files and vegetation parameter files for the model these files need to be previously created by the user before using the areawiseparamgenerator module again the user needs to provide the required information in a similar way as it is illustrated in section 5 1 2 for the areawiseparamgenerator module 5 1 4 3 forcingdatafilegenerator add a forcingdatafilegenerator for taking care of the forcing data file preparation for the model this component takes the forcing information collected from the maingenerator and saves it into the user specified folder 5 1 4 4 initialstatefilegenerator add an initialstatefilegenerator to place the initial state files into the right folder in the working directory 5 1 4 5 runmoduleagent add the final component the runmoduleagent to complete the construction of the vic5 model agent it is responsible for setting up the path where the executable file of the model is located also the arguments of the execution as well as the selected outputs should be configured here now the user can add the msmshowchart module to plot the surface runoff and baseflow for visualization after adding the two msmshowchart modules to plot the output01 surface runoff and output02 baseflow ports of the runmoduleagent the user will be able to execute the workflow shown in fig 14 the two time series obtained are depicted in fig 15 a and fig 15 b 5 2 dhsvm model simulation the distributed hydrology soil vegetation model dhsvm is a high resolution hydrological model that simulates water and energy fluxes by considering the effects of terrain features wigmosta et al 1994 one of the main differences in the modeling structures between vic and dhsvm is that in the latter terrain or topography information is needed as routing process is part of dhsvm i e the routing process is internally coupled to other processes inside dhsvm in this use case we illustrate simulations of the dhsvm model through both its manually developed dhsvmagent and the agent constructed using the generic model agent toolkit separately the indiantown run basin a small watershed in pennsylvania is used to illustrate these features seven types of forcing data are required for dhsvm pressure precipitation relative humidity temperature wind speed longwave radiation and shortwave radiation since the simulation will be performed at an hourly time step the nldas data agent will be used again however this data source does not provide relative humidity but the specific humidity thus one needs to calculate the relative humidity based on the other given forcing data employing an approach similar to that used in the vic5 example with the msmdatasetoperation module 5 2 1 constructing dhsvm model agent using generic model agent toolkit in this section we show how to integrate the dhsvm model by using the generic model agent toolkit and escape steps that are similar to those described before to construct a workflow running the dhsvm model the user needs to add timerange spacerange nldasagent modules like before and use msmdatasetoperation and msmunitconversion modules to calculate certain forcing input variables based on available input data and to convert units for this example the user needs to use eight nldasagent boxes to retrieve eight different hourly forcing variables which are temperature k pressure pa radiation flux long wave w m2 radiation flux short wave w m2 specific humidity kg kg total precipitation mm h u wind m s and v wind m s and then to compute the relative humidity and magnitude of wind required by dhsvm to run the dhsvm model one needs to use gis 7 7 https en wikipedia org wiki geographic information system geographic information system tools to prepare data for the routing related processes like those involved in the routing model described in subsection 5 1 3 as these routing related processes are part of the dhsvm model one can either prepare these gis related files offline and have them inputted through the areawiseparametergenerator as illustrated in subsection 5 1 3 or prepare them as part of the overall workflow as in this example to facilitate the in workflow preparation of these gis related files required for the integration of models such as the dhsvm model cyberwater has incorporated grass gis 8 8 https grass osgeo org an open source gis software in its msm system a module called gisengine is developed to interface between the msm system and the grass gis system through this gisengine module and other gis related modules developed in cyberwater one can automatically and seamlessly access the grass gis system and easily and effectively make use of the various functionalities offered by grass gis to accomplish the user s various tasks such as identifying the flow directions flow paths and the stream network within the studied area given its dem 9 9 https en wikipedia org wiki usgs dem digital elevation model map file the resulting files from gis are then automatically passed back to the msm system thus all the input files required to be produced by gis can be obtained using our developed gis related modules luna et al 2020 2021a details on the integration of grass gis and its relevant modules developed in the cyberwater system are presented in luna et al 2021b and will be discussed in detail in a separate paper luna et al 2022 in this example most of the gis related input files are automatically prepared and are passed to the areawiseparametergenerator as indicated by the dashed line box parameter file preparation in fig 16 which is achieved by using our developed gisengine and the static parameter agent modules luna et al 2021b 2022 which are grouped together in the parameter file preparation box and described below 5 2 1 1 parameter files preparation this preparation mainly includes the following major steps 1 use four staticbinarymapagent modules which are configured with the soil depth map soil map vegetation map and dem map paths respectively 2 use a gisdefinewatershed module configured with the path of the dem map 3 use a streammapandroutingfiles dhsvm module configured with the north coordinator south coordinator west coordinator east coordinator as mask limits 4 group all the modules above in a module box for a neat and concise workflow outlook and name these grouped components as parameter file preparation as shown in fig 16 5 2 1 2 dhsvm model agent construction with the toolkit 1 add a maingenerator module and create a new working directory folder on the user s computer where the dhsvm simulation will occur for this example this folder will be saved on c temp cyberwater the inputs are set up in a similar way as described in subsection 5 1 4 for vic5 but the variables corresponding to each dataset are different since they are determined by the model s global file or configuration file 2 add an areawiseparamgenerator module and the inputs are set up in a similar way as described in subsection 5 1 4 for vic5 3 add forcingdatafilegenerator to take care of the forcing data file preparation for the dhsvm model this component takes the forcing information collected from the maingenerator and saves it into the user specified folder 4 add an initialstatefilegenerator module and let the path be the folder path of c temp cyberwater gt dhsvm irun10years source 5 add a runmoduleagent module and its inputs are set up in a similar way as described in subsection 5 1 4 for vic5 the newly created workflow after execution is shown in fig 16 the resulting plot after the execution of this workflow is depicted in fig 17 in contrast fig 18 shows the complete workflow if the user employs the manually written dhsvm model agent for integrating the dhsvm model 5 3 discussion based on our investigation of the popular hydrological and environmental models it shows that the complexity of these models interface structures often lies in the organizations of models input files with many different options types e g soil and vegetation related libraries and parameter files snow related information and gis related information rather than the organizations and structures of the models output files thus we designed three components to handle a given model s input interface organizations i e areawiseparamgenerator forcingdatafilegenerator and initialstatefilegenerator so that each component would not be too complicated for users to configure these three components correspond to the three major types that a typical model would have on the other hand as the model s output interface organizations and structures are usually straightforward e g values of computed state variables fluxes and input information organized either in time series or spatial maps we thus combined the receiving of the model s outputs into the component handling the model s execution e g runmoduleagent rather than creating a dedicated component for that furthermore the organizations and structures of the computational models interfaces can be briefly classified into the following two categories 1 the model itself does not include the use of gis related information and or functionalities such as vic 2 the model itself utilizes gis related information and or functionalities as well such as the routing model and dhsvm where the routing processes are included the former does not directly involve gis to handle its area wise parameter files while the latter usually directly involves the use of gis for handling its area wise parameter files to facilitate the latter case the grass gis is integrated into the cyberwater system via our developed gisengine and its related modules in addition the static parameter agent modules are developed to automatically prepare gis related input files for such models which will be discussed in a separate paper luna et al 2022 once the gis related input files are created they are passed to the areawiseparametergenerator as indicated in the use case of subsection 5 2 1 we note that among the three types of the input data files the structure and format interfaces of the forcing data files are less heterogeneous and complex compared to those of the parameter files and initial state files among the different hydrological and environmental models for the forcing data files the majority of the models e g vic dhsvm routing and casa cnp have their formats fall into two types namely single point time series or spatio temporal maps these two types are both taken care of inside cyberwater s forcingdatafilegenerator module which also allows for customizable time stamps for the data in addition the forcingdatafilegenerator module allows the users to modify the order of the columns inside the forcing data files to represent different forcing variables read in so that the order of the variables involved in the forcing data files can be adjusted to match the structure requirements of different models forcing data files for the input parameter files and initial state files however interfaces of the structures and formats of the individual files are then taken care of by the static parameter agent modules developed in cyberwater luna et al 2021b 2022 we also note that the generic model agent toolkit not only makes the integration of one s model an easier task but also significantly saves user s time and efforts and reduces the error prone process of writing the model agent codes since one does not need to write any code to integrate a model into the cyberwater system by manually writing a model agent to integrate a model one needs to not only write codes initially but also to re write the codes whenever the model s i o interface changes as shown in the example of using vic4 version 4 versus vic5 version 5 on the other hand due to the existing large number of diverse types of models it is possible that some unusual model structures and features might not be covered by the generic model agent toolkit we have developed in such a case the option of manually writing a model agent in cyberwater provides more flexibility consequently cyberwater provides users with the flexibility to either use the generic model agent toolkit for most models or to manually write their own model agents when needed for any model with unusual i o structures from the illustrations of the use cases in sections 5 1 and 5 2 we can see the flexibility and straightforwardness of using the generic model agent toolkit in integrating various computational models with significantly different model input structures and formats without the need of writing any code the strengths of using the visual drag and drop approach of the generic model agent toolkit to construct a model agent for integrating a model into the cyberwater system as opposed to the traditional approach of writing programming codes e g the vicagent and dhsvmagent include 1 usability ease of use one can easily construct a model agent with the toolkit to integrate a model into cyberwater without writing computer codes in which the functionalities and responsibilities of the individual modules are clear the relationships connections between different modules in the toolkit provide a clear overview and big picture of a model s data file structure organization logic and its workflow processes making model integration an easier task reducing the error prone code writing process and saving time 2 clarity the organizations and structures of a model s input output files can be clearly seen from the five six modules included in the generic model agent toolkit for example from the configuration panel associated with each module users can easily see where the model s input files are located and what they are such a neat visual presentation helps users identify missing information and debugging 3 generality our toolkit is applicable to a wide range of computational models since most of the models have similar input file organizations e g global file configuration file parameter file forcing data file and initial state file in fact we have been successfully applied the toolkit to integrate other hydrological and environmental models with ease luna et al 2020 such as the biogeochemical model casa cnp the carnegie ames stanford approach model with carbon nitrogen and phosphorous cycles which computes the net primary productivity of terrestrial ecosystems together with detailed distributions of the amount of carbon nitrogen and phosphorous present in the biosphere and its surroundings wang et al 2010 and the usgs environmental model phreeqc model which is used to calculate a variety of aqueous geochemical reactions and processes in natural waters or laboratory experiments parkhurst and appelo 1999 4 agility any changes in a model s interface can be easily accommodated with the toolkit for example when a model s inputs structure is changed as shown in section 5 1 vic4 versus vic5 one can easily reconstruct the model agent for vic5 using the toolkit instead of rewriting hundreds of new codes to have a new vicagent 6 conclusion in the fields of sciences and engineering the need for open data and open model integration is urgent while our previous work on the open data and open modeling framework of msm lays a solid foundation for effectively and efficiently addressing the challenge faced by the broad community this new work on the development of the generic model agent toolkit enables modelers to construct their own model agents without coding therefore further simplifying the data and model integration task by eliminating users effort of writing model agent code this is achieved by the innovative design and development of a suite of feature oriented code templates that extract each individual model agent s characteristics and therefore reduce the model agent programming work to the template based components configuration task this approach of the generic model agent toolkit in cyberwater is capable of applying to diverse models developed in the broad community without modifying any code of the original models or writing any model agents code with cyberwater users can individually and easily make their models pluggable into comprehensive end to end workflows with automated and seamless access to various online data sources for their scientific study without any need for a central control administration or service therefore our presented approach not only provides a general and elegant solution for open data and open model integration for complex modeling systems but also offers a truly sustainable and scalable solution for broad applications the current limitation of the generic model agent toolkit is that it only works for grid based models as we know there are hydrological models developed based on hillslope units or sub watershed units all of which have irregular modeling grids our future work is to extend the generic model agent toolkit to handle irregular grid based models we also plan to extend cyberwater to include web services to further complement its current standalone system which can better support education in colleges and universities as most universities have very strict limitations for software installations on their laboratory computers declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the u s national science foundation under oac 1835817 to indiana university purdue university indianapolis iupui and oac 1835785 to the university of pittsburgh respectively 
25593,we present machine learning methods to predict hydrologic features such as streamflow and soil moisture from spatially and temporally varying hydrological and meteorological data we used a temporal reduction technique to reduce computation and memory requirements and trained a long short term memory lstm network to predict soil moisture and streamflow over multiple watersheds we show lstm networks can be trained in a fraction of the time required by complex process based and attention based models such as soil and water assessment tool swat and geoman without sacrificing accuracy we also demonstrate that outside data sourced from a watershed other than the target can be used to train lstm to comparable or even superior prediction accuracy the success of lstm in such spatially inductive settings shows hydrologic features can be predicted with minimal prior knowledge of the watershed in question finally we make all methodologies of this work publicly available as an end to end software pipeline that facilitates rapid prototyping of hydrologic learners graphical abstract image 1 keywords hydrology machine learning long short term memory wabash river little river watershed 1 introduction hydrologic features such as streamflow and soil moisture in river basins are important for agriculture managing reservoirs and hydropower operations predicting future streamflow and soil moisture levels are also used to predict extreme hydrologic events such as floods and droughts that are among the most severe weather related disasters that cause widespread damage in agriculture wildlife habitats and human properties for example the 2012 drought in the continental united states resulted in an estimated 30 billion in mostly agricultural losses rippey 2015 hence forecasting hydrologic features and predicting the occurrence frequency and severity of hydrologic events are extremely important for societal well being reichstein et al 2019 there are two fundamental approaches to predict hydrological features and events process based and data driven models process based models fatichi et al 2016 nash 1957 diskin et al 1984 arnold et al 1995 abbott et al 1986 fatichi et al 2016 simulate hydrologic processes often referred to as rainfall runoff models and have a long history dating back to the 1960s it has been observed that rainfall runoff models often exhibit low transferability meaning that the model performance declines when simulating events outside of the calibration period guo et al 2020 hartmann and bárdossy 2005 in contrast data driven models remesan and mathew 2016 solomatine et al 2009 learn functions that map input features to output variables based on training data and do not explicitly simulate physical or conceptual representation of the hydrologic processes despite a long standing discussion klemeš 1986 sungmin et al 2020 zheng et al 2018 about the relative merits of process based and data driven models both methods have individually delivered reliable predictions of various hydrologic features in this paper we aim to utilize the complementary merits of both methods to make accurate temporal and spatial predictions of soil moisture and streamflow one particular example is in predicting streamflow from ungauged basins for which no training data exists in this case we can train machine learning ml models using simulated data from process based models and forecast hydrologic events in ungauged basins we demonstrate that process based and ml models when working in tandem improve spatial and temporal generalization of the underlying hydrologic processes and help us improve spatial validation one of many major open challenges in hydrology blöschl et al 2019 the primary objective of this paper is to show the applicability of simple lstm models in cooperation with swat when predicting hydrologic events in two watersheds despite the recent surge in using ml methods in water resource studies we identify and address four aspects that received less attention in the literature 1 spatiotemporal learning it is not adequately studied how ml models could use historic data from one basin to predict hydrologic variables in other basins we study this spatiotemporal learning task extensively at multiple levels including cross subbasin and cross basin 2 limited historic data it is not well studied how ml models could be trained when only limited historic data is available previous work has mostly focused on streamflow due to the abundance of historic streamflow data however the literature is sparse on predicting soil moisture for which historic data is limited we demonstrate that adequate predictions of soil moisture and streamflow for both gauged and ungauged subbasins may be achieved by training ml models on swat simulated data 3 reducing computational time it is computationally expensive to train complex ml models on hundreds of years of multivariate data originating from thousands of subbasins we demonstrate that the relative low complexity lstm model can be trained much faster than complex ml models such as geoman liang et al 2018 without sacrificing prediction accuracy 4 an end to end software pipeline domain scientists often face a stiff learning curve when processing hydrology data in ml frameworks such as pytorch paszke et al 2019 and tensorflow abadi et al 2016 we developed a pipeline that packages data processing model training evaluation and experimental design and analysis this pipeline was designed for ease of use but remains highly modular to meet the needs of both non experts and experts alike this paper addresses the aforementioned objectives by 1 extensively studying spatiotemporal learning tasks using ml models 2 leveraging simulated data to produce reasonable predictions for basins of limited historical data 3 utilizing a model of relatively low complexity lstm and reduction techniques to reduce computational requirements while retaining accuracy and 4 by making our end to end pipeline publicly available to facilitate the development of ml models for hydrological prediction by experts and non experts 2 a brief background of machine learning in hydrology machine learning refers to a set of data driven methods that learn patterns present in data a supervised ml model tunes its parameters to learn input output relations present in the training data after training the model one can predict outputs or events from test data that is not seen by the model during training in recent years ml models especially artificial neural networks anns have been widely used in hydrologic predictions solomatine et al 2009 solomatine and ostfeld 2008 schaap and leij 1998 parisouj et al 2020 hu et al 2020 adikari et al 2021 for the purpose of this paper we broadly divide ml methods in hydrology into four groups a regression models that do not use anns b various ann models excluding recurrent neural networks rnns c methods based on rnns and d combination of process based and ml models classical ml methods such as logistic regression k nearest neighbors support vector machine svm and random forest have been widely used in various hydrologic predictions such as classifying water supply vulnerability robinson et al 2020 predicting streamflow solomatine et al 2008 worland et al 2018 flood forecasting solomatine and xue 2004 chapi et al 2017 and approximating process based models zhang et al 2009b however recent work cheng et al 2020 kratzert et al 2019b a have demonstrated that anns capture complex input output relations better than simple regression models this is not surprising given that deep neural networks are exponentially expressive because deeper networks can memorise very large dataset montúfar et al 2014 hydrology researchers used simple multi layer perceptron and fully connected neural networks to predict soil moisture retention and streamflow patterns cheng et al 2020 schaap and leij 1998 cai et al used deep learning regression network dnnr to develop a soil moisture prediction model cai et al 2019 however considering the temporal nature of hydrology data rnn and its variants are more reasonable choices given rnn s ability to model sequence data indeed long short term memory lstm models were trained on hundreds of basins to demonstrate that lstm outperformed traditional process based models both in temporal and spatial predictions kratzert et al 2019b a majeske et al 2020 custom built rnn models such as neural runoff model nrm xiang and demir 2020 and geoman liang et al 2018 also performed much better than regression models and other anns finally several recent work combined process based and ml models to take complimentary advantage of both approaches for example essenfelder et al coupled the soil and water assessment tool swat arnold et al 1995 1998 and ann models to predict inter basin water transfer essenfelder and giupponi 2020 3 materials and methods 3 1 study area and data 3 1 1 wabash river basin the wabash river basin wrb in the us midwest is the largest northern tributary of the ohio river the wrb drains over 90 000 km2 and has a humid continental climate with a mean annual temperature of 11 3 c and mean annual precipitation of 122 7 cm over the 1971 2000 period the basin shown in fig 1 a spans most of the state of indiana part of eastern illinois and a small section of western ohio major land use types include 52 cropland and 19 forest with the remaining 29 covered by generic agriculture tall fescue residential and water the wrb has a mean slope of 0 009 with elevation and subbasin area ranging from 75m to 376m and 21 km2 200 km2 respectively the wrb time series consists of daily hydrological and meteorological records spanning january 01 1929 to 12 31 2013 for t 31 046 time steps the basin is subdivided into s 1 276 distinct subbasins each of which record f 5 features 2 hydrological soil moisture and streamflow and 3 meteorological precipitation minimum and maximum temperature hydrological features of the time series were obtained from simulation using the swat model arnold et al 1995 1998 meteorological features are observed records obtained from the university of notre dame boryan et al 2011 from this basin we also have daily observed streamflow records from s 5 subbasins spanning january 01 1985 to 12 31 2013 for t 12 424 time steps these streamflow records are combined with observed meteorological records resulting in f 4 features for each time step and subbasin we denote swat simulated and observed wrb time series formally by equation 1 1 wabash swat r 31 046 1 276 5 wabash gt r 12 424 5 4 3 1 2 little river watershed the little river watershed lrw fig 1b is the upper 334 km2 of the little river in georgia us and part of us department of agriculture usda agricultural research service ars long term agriculture research ltar https ltar ars usda gov sites gacp the region is of low relief and characterized by flat uplands and alluvial floodplains and located in the humid subtropical climate zone with typical annual precipitation of 1167 mm major land use types include 50 forest and 31 cropland with the remaining 19 covered by pasture urban and water the lrw has a mean slope of 0 08 with elevation and subbasin area ranging from 104m to 146m and 3 km2 65 km2 respectively daily precipitation temperature and streamflow data used in this study were obtained from little river experimental watershed database bosch et al 2007 and processed in our previous studies zhang et al 2009a 2011 the lrw time series consists of daily hydrological and meteorological records spanning january 01 1968 to 12 31 2004 for t 13 515 time steps the watershed is subdivided into s 8 distinct subbasins each of which record f 4 features 1 hydrological streamflow and 3 meteorological precipitation minimum and maximum temperature this time series possesses many missing records with one subbasin missing as many as 8 521 13515 entries we handle missing records by replacement via forward propagation which involves replacing all missing values with the last known value we denote the lrw time series formally by equation 2 2 little gt r 13 515 8 4 3 2 types of inductive predictions among various prediction tasks we particularly focus on inductive predictions that draw conclusions about future events from past and current samples the primary challenge in inductively predicting hydrological variables stems from the nature of data which varies both temporally being dynamic across time and spatially originating from multiple geographical locations consider hydrological and meteorological records x 1 x t 1 x t and y 1 y t 1 y t collected over t discrete time steps from two subbasins x and y when x t is predicted from a model trained with observed data x 1 x t 1 it is called temporal induction in contrast when y t is predicted from a model trained with observed data x 1 x t 1 it is called spatiotemporal induction fig 2 visualizes and explains these concepts further we develop ml models that work well for both temporal and spatiotemporal induction within and across watersheds 3 3 swat configuration in addition to historical streamflow from five wrb subbasins and eight lrw subbasins we use swat simulated streamflow and soil moisture values from other ungauged subbasins swat is a semi distributed watershed modeling program developed by the agricultural research service ars of the u s department of agriculture arnold et al 1995 1998 in this paper we use previously generated data dierauer and zhu 2020 from a calibrated swat model available at https futurewater indiana edu the calibration and prediction from swat rely on historical time series including minimum maximum temperature precipitation and wind speed additionally swat model construction uses inputs of hydrography topography soils and landuse landcover compiled from multiple governmental agencies the additional information coming from hydrographic and topographic data helps swat capture the hydrologic cycle 3 4 ml software pipeline 3 4 1 temporal reduction the daily time step resolution of wrb and lrw time series is precise but computationally cumbersome and superfluous in predicting extreme events such as drought with this in mind we apply temporal reduction to reduce time step resolution and significantly decrease memory computation and model complexity with minimal cost to prediction accuracy let t be time step count of the original time series w be reduction window size and s be reduction window stride in the trivial case a weekly reduction uses w s 7 but this fails to consider all possible weeks and thus we generalize temporal reduction for any s this allows control over the resulting time step resolution by varying w but also time step distinctiveness by varying s since s determines the extent of window overlap the sliding window method fig 3 which we use to extract input output window samples requires time step contiguity to ensure contiguity persists through temporal reduction w s contiguous reduced temporal channels are created each channel is formed by applying the trivial w s reduction at offsets c offset 0 1 w s 1 lastly input output window samples are extracted from each channel separately and pooled to create the final window sample sets x and y 3 4 2 feature pre and post processing due to the distribution of features across spatial and temporal domains proper pre and post processing is essential for model optimization and induction in induction it is critical to transform features into a common range so that the learned parameters of the ml model are invariant to feature magnitude for example subbasin 772 of the wrb sees mean streamflow of approximately 0 169 m3 s while mean streamflow of subbasin 52 is approximately 842 187 m3 s in this case optimizing on subbasin 772 and predicting on subbasin 52 or vice versa will cause severe and systematic under over prediction to make induction feasible and alleviate various other numerical challenges overflow exploding gradients etc all features are normalized to the common range 1 1 since each model is optimized over input outputs in the range 1 1 raw model outputs may not be used as final predictions thus post processing involves transforming feature predictions back into their original range via inverse normalization normalization for feature f of subbasin s to interval a b and inverse normalization are defined in equations 3 and 4 3 x s f b a x s f m i n s f m a x s f m i n s f a 4 x s f m a x s f m i n s f x s f a b a m i n s f 3 4 3 the lstm network the lstm network is a recurrent neural network rnn that utilizes the lstm cell for its recurrent unit the lstm cell proposed in hochreiter and schmidhuber 1997 is designed specifically to handle the challenge of learning long term dependencies from temporally distributed data pascanu et al 2013 in this work we employ a simple lstm network architecture consisting of three layers 1 a temporal encoding layer to encode a sequence of i temporally distributed predictors x x 1 x 2 x i into latent representation h enc 2 a temporal decoding layer to decode representation h enc into a sequence of o latent representations h dec h 1 dec h 2 dec h o dec and 3 a linear layer to convert from decoding dimension d enc to response dimension d y for all representations in h dec fig 4 shows a schematic diagram of the lstm network model used in this paper the steps of forward propagation for the lstm network are defined in equation 5 where θ θ enc θ dec θ lin denotes all learned network parameters 5 h enc l s t m enc x θ enc h dec l s t m dec h enc θ dec y l i n e a r h dec θ lin the relation between response dependent and predictor independent variables is statistical and formally defined by the conditional probability p y x t p x t p x where x t p denotes the pth predictor variable at time step t in optimizing the lstm network the parameters of a non linear function mapping past predictors precipitation soil moisture streamflow etc into a future response soil moisture or streamflow are learned more specifically lstm optimization learns a conditional density function cdf that approximates the true underlying conditional probability p y x t p θ p y x t p parameterized by θ we formally define lstm in terms of cdf estimation as a premise for observed induction performance and analysis in section 4 3 4 4 training and testing settings in order to facilitate optimization and validate model efficacy we partition each original time series into training validation and testing sets the training set is used to optimize model parameters directly validation to guide model optimization towards better generalization and testing to evaluate final model performance for the wrb time series we assigned the non overlapping periods january 01 1985 12 31 1997 january 01 1998 12 31 2005 and january 01 2006 12 31 2013 to training validation and testing sets respectively for the lrw time series training validation and testing sets were assigned to the non overlapping periods january 01 1968 12 31 1995 january 01 1996 12 31 1999 and january 01 2000 12 31 2004 the training validation and testing periods above were assigned to occupy approximately 80 10 and 10 percent of the original time series a common split ratio 8 1 1 in machine learning in this work we define model efficacy in terms of prediction quality over the testing set since this most accurately reflects model generalization we quantify prediction quality via normalized root mean square error nrmse defined in equation 6 where t is a time step of the target period testing set s is the target subbasin and r is the predicted response we chose nrmse because it defines error in a consistent range 0 1 and allows for meaningful comparison between predictions independent of target period subbasin and response all experiments were executed on an acer predator g3 710 equipped with an intel core i7 7700 at 3 6 ghz 32 gb ram and nvidia s geforce 1070 1920 cuda cores 1 5 ghz 8 gb memory when predicting o 1 week into the future best testing set nrmse for lstm was achieved using i 8 weeks of past predictors d enc d dec 128 hidden units adadelta optimizer zeiler 2012 with mini batch size 128 learning rate η 0 1 xavier initialization glorot and bengio 2010 and 100 epochs of training with optimal predictors defined in section 4 2 6 rmse s r t 1 t y t s r y t s r 2 t nrmse s r rmse s r m a x s r m i n s r 4 results 4 1 experimental goals before discussing the results in detail it is important to first clarify our experimental goals the results section is organized into four major subsections that aim to answer the following questions 4 2 what are the optimal predictors for soil moisture and streamflow 4 3 what is the predictive performance of lstm in the baseline temporal induction setting and how does this compare to the process based model swat 4 4 how does lstm perform in more complex induction settings such as spatiotemporal and multi subbasin induction 4 5 how does lstm performance compare to more and less complex ml methods for temporal and spatiotemporal induction settings each subsection defines our motivation for the experiment the settings and results and finally offers an analysis of the observed results 4 2 predictor informativeness in order to minimize prediction error it is important to analyze the contribution of each predictor so that we may establish an optimal set table 1 enumerates soil moisture and streamflow prediction error for various predictor sets for this experiment we separate predictors into two categories supplementary and primary supplementary predictors are variables not included in the response soil moisture or streamflow while primary predictors are variables included in the response these categories reflect our assumption that past response values are most informative or primary to future response values while all other variables are supplementary or non primary predictors table 1 lists results arranged into the following three groups blocks of rows that show prediction error when 1 optimizing using only one of all possible predictors 2 optimizing using one supplementary and the primary predictor 3 optimizing using all possible predictors as expected we find the best performance from group 1 emboldened when using the primary predictor confirming past responses are most informative to future responses in group 2 we find precipitation to be the most informative supplementary predictor for both soil moisture and streamflow supplementary predictors day of year and soil moisture were the second most informative for soil moisture and streamflow respectively and in group 3 we observe the lowest prediction error for soil moisture and streamflow when using all possible predictors it should be noted that all possible predictor subsets were tested and three instances were found to reduce error over all possible predictors all three instances included five of six possible predictors and were missing either minimum or maximum temperature but never both this would suggest that while both are informative these variables possess significant mutual information and the inclusion of both is likely redundant given that error decrease was marginal and predictor dimension d x from group 3 does not warrant reduction all possible predictors were chosen as the optimal set and used in all proceeding experiments unless otherwise stated 4 3 temporal induction in this section we analyze the predictive capability of lstm for temporal induction in temporal induction the parameters of each model are optimized to training period samples of a subbasin the optimized model is then used to predict over samples of the mutually exclusive future testing period of the same subbasin in all instances of this section samples from the testing period are never used during optimization and thus prediction error reflects quality of induction across the temporal domain 4 3 1 comparison with swat we evaluate the relative predictive capability of the swat and lstm models on wrb ground truth gt data as previously stated in section 3 ground truth streamflow records are available for 5 wrb subbasins which include 43 169 348 529 and 757 table 2 enumerates swat and lstm prediction error for all 5 wrb subbasins with ground truth streamflow records each row shows prediction performance under the following scenarios 1 swat optimization and prediction on gt 2 lstm optimization and prediction on gt 3 lstm optimization to swat predictions from period 01 01 1985 12 31 1997 and prediction on gt 4 lstm optimization to swat predictions from period 01 01 1929 12 31 1997 and prediction on gt in rows 1 and 2 we directly compare lstm and swat to gauge lstm prediction performance in rows 3 and 4 we test lstm prediction performance when optimized to synthetic swat prediction data in most cases swat out performs lstm but lstm can perform nearly as well or better than swat figs 5 and 6 show lstm a and swat b predictions against gt streamflow for subbasins 169 and 529 respectively when training and testing on gt data in fig 5 we see lstm struggles to adequately capture extreme streamflow values compared to swat resulting in systematic under prediction in fig 6 lstm still struggles to fully capture extreme values but does not systematically over predict as is the case with swat prediction streamflow prediction appears to be particularly difficult for lstm as it tends to under predict but in cases where swat over predicts subbasin 529 lstm can deliver predictions superior to swat it is promising that lstm should produce any superior comparable result when we consider the many factors that favor swat prediction for example swat model construction uses inputs of hydrography topography soils and landuse landcover compiled from multiple governmental agencies in contrast the lstm model only uses a handful of predictors shown in table 1 despite swat s apparent advantage lstm outperforms swat in subbasin 529 and 757 more importantly as we will see in the following swat simulations may be used to optimize the lstm model and predict soil moisture and streamflow values quite accurately in row 3 we consider the case of synthetic data substitution but select only a subset from the complete synthetic training period that coincides with the available gt training period 01 01 1985 12 31 1997 our rationale for selecting this reduced training period is to avoid introducing bias by optimizing lstm over more synthetic training samples than what is available for gt data we find performance is generally worse compared to optimization on gt data but note that performance degradation is mostly insignificant this result demonstrates that swat predictions can be used to train lstm without significant increase in prediction error in row 4 we again consider synthetic data substitution but instead use the complete synthetic training period 01 01 1929 12 31 1997 given that we are required to substitute synthetic data we would like to quantify the benefit of optimizing lstm over more data as a consequence the use of more data for optimization is a clear advantage and we see reduction in prediction error for all but subbasin 43 relative to row 3 this result though expected demonstrates that the greater abundance of synthetic data relative to gt which is often the case when synthetic data is easier to acquire can be an advantage overall the predictive performance of lstm is not strictly superior to swat and we would not use this model as a replacement this is expected since the lstm is a highly generalized function approximator and far less complex than swat however in certain circumstances the lstm model can improve over swat for gt prediction and can be optimized at a fraction of the run time in all cases 4 3 2 overall temporal induction performance to thoroughly understand the capability of lstm to perform temporal induction we temporally induct soil moisture and streamflow for all subbasins of the wrb we use the swat simulated time series because data is available for all 1276 wrb subbasins and as shown in section 4 3 1 swat predictions are reasonably close to ground truth this experiment includes the optimization of 1276 separate lstm models to predict soil moisture and an additional 1276 to predict streamflow settings were identical across all lstm models with the exception of selected response variable soil moisture or streamflow fig 7 shows the distribution of temporal induction error a and feature value b for soil moisture and streamflow the error distributions show streamflow prediction to be a significantly more challenging task than soil moisture prediction the stark difference between error distributions appears to be a consequence of the difference in distribution of feature values at first glance soil moisture appears to be normally distributed while streamflow appears to be log or log normally distributed soil moisture shows slight negative skew at μ 3 0 0552 but streamflow shows heavy positive skew at μ 3 3 141 to explain why this presents a challenge we first note that lstm though non linear as an end to end model uses a linear transformation with identity activation as its output layer thus lstm may be equivalently defined as a non linear regression function f x θ g φ x θ enc θ dec θ lin where g is a linear regression function and φ is some non linear basis function it is well established that the minimization of least squares for regression is equivalent to maximum likelihood estimation with gaussian error assumption given that lstm is trained via mean square error mse loss we effectively assume the response is gaussian normally distributed this appears to be correct for soil moisture but incorrect for streamflow and explains at least partially the discrepancy between temporal induction errors 4 4 spatiotemporal induction in this section we analyze the predictive capability of lstm for spatiotemporal induction in spatiotemporal induction the parameters of each model are optimized to training period samples of a training subbasin the optimized model is then used to predict over samples of the mutually exclusive future testing period of a separate testing subbasin in all instances of this section samples from the testing period of the testing subbasin are never used during optimization and thus prediction error reflects quality of induction across both spatial and temporal domains 4 4 1 intra watershed spatiotemporal induction we first test the capability of lstm to perform spatiotemporal induction within a watershed to do this we randomly select two mutually exclusive sets of subbasins designating one for training s train and the other for testing s test for each training subbasin s i s train we optimize an lstm to its training period and then predict over the testing period of all testing subbasins s i s test i we again use the swat simulated wrb time series for the motivations addressed in section 4 3 2 to keep this experiment concise and visual analysis practical forty wrb subbasins were selected at random and split evenly into training and testing sets in total spatiotemporal induction is performed over all possible train test subbasin pairs resulting in s train s test 400 inductions for soil moisture and streamflow fig 8 visualizes the distribution of error over the 400 inductions for soil moisture and streamflow overall spatiotemporal induction is significantly more erroneous than temporal induction for both response variables we believe the primary driver of performance loss is the significant increase in model misspecification error for any learning task there exist several underlying sources of error that limit final prediction performance in zhu and laptev 2017 the authors attempt to capture prediction uncertainty by modeling three underlying sources of error referred to as model uncertainty inherent noise and model misspecification though model uncertainty and inherent noise certainly contribute to our prediction error we believe model misspecification is particularly impactful for spatiotemporal induction model misspecification refers to the error assumed when optimizing and predicting over samples drawn from non identical divergent distributions this causes the testing error of any model even if optimized to exactly reproduce the distribution of training samples to be lower bounded by the divergence of training and testing sample distributions in temporal induction we assume model misspecification error since training and testing samples are drawn from separate time periods and thus are non identically distributed spatiotemporal induction effectively compounds this error because training and testing samples are drawn from separate time periods and geographical locations in other words induction performance is limited by the similarity of training and testing samples and we expect similarity to decrease when training and testing on different subbasins in addition to different time periods fig 9 provides an example of the divergence between training testing distributions and its effect on induction when training and testing on subbasins 1 and 2 respectively there is clear divergence between distribution of soil moisture in the training set of subbasin 1 and the testing set of subbasin 2 as a result we see the distribution of testing set predictions over subbasin 2 falls somewhere between training and testing distributions namely lstm tends to under predict streamflow in the normalized range 0 5 1 0 because the training set distribution is significantly less left skewed we visualize this discrepancy in the plotted curves of fig 10 with left and right sub figures showing prediction from temporal and spatiotemporal induction over subbasin 2 the upper quartile of soil moisture values in subbasin 2 are not captured in spatiotemporal induction because soil moisture values in this range are not adequately represented by the training set of subbasin 1 for streamflow we again see the distribution of predictions fall somewhere between training testing distributions but does not capture the normalized range 0 0 1 0 of either we also see many testing set streamflow values that far exceed the 69 year training set maximum causing normalized values upwards of x 1 58 these extreme values appear to cause lstm to over predict resulting in a prediction distribution that possesses a significantly heavier tail than training and testing distributions we again visualize the discrepancy in the plotted curves of fig 11 with left and right sub figures showing prediction from temporal and spatiotemporal induction over subbasin 2 predictions from spatiotemporal induction do not entirely capture the lower quartile of streamflow values because extreme values in the testing set of subbasin 2 bias lstm towards over prediction in summary lstm can perform spatiotemporal induction with success but the resulting predictions are biased towards its knowledge learned expected value of the training set alternatively predictions can be biased towards the testing set if the testing set contains enough extreme values of sufficient magnitude in both cases the divergence between training and testing set distributions clearly effects prediction accuracy moreover the potential for training testing divergence is increased for spatiotemporal induction thus making it on average more challenging than temporal induction we additionally visualize spatiotemporal induction errors for soil moisture and streamflow as clustered heatmaps in figs 12 and 13 each column of the heatmap shows induction error across all testing subbasins when lstm is optimized to a given training subbasin rows and columns of each heatmap are hierarchically clustered using pearson correlation distance and shown as dendrograms on the left and top respectively our motivation for clustering induction errors is to better understand the relationship between induction error and choice of train and test subbasins if it is the case for example that spatial proximity of chosen subbasins is correlated to spatiotemporal induction error we can make informed decisions about the training subbasins we select and potentially reduce error before discussing the results in detail it is important to precisely define the semantics behind training and testing subbasin clustering first the clustering of training subbasins indicates significant similarity in induction error when predicting over any of the 20 testing subbasins and as defined earlier we derive from lstm optimization a conditional density function approximation y f x θ p y x t p x t p x thus clustering represents significant similarity in learned conditional density functions and by dependence the distribution of predictor and response variables of training subbasins this follows for testing subbasin clustering with the exception that only similarity in predictor variable distributions is indicated since prediction depends exclusively on these variables finally low induction error indicates significantly similarity in the distribution of predictor and response variables low divergence between train and test subbasins for soil moisture we find a region of low induction error in which training subbasins s train 178 61 170 82 82 112 and testing subbasins s test 81 5 311 100 314 84 878 form a cluster fig 12 shows the wrb with highlighted train and test subbasins and a magnified area containing the six clustered training subbasins above from this region we see all six clustered training subbasins are relatively close to each other and five of the seven clustered testing subbasins are relatively close to these training subbasins given the majority of these low induction error subbasin pairs are relatively close there appears to be some correlation between subbasin proximity and feature similarity in summary spatiotemporal induction for soil moisture appears to benefit from the selection of training subbasins with proximity to target testing subbasins for streamflow we see induction error is generally consistent across training subbasins with the exception of an outlier train and outlier test subbasin in these outlier cases training subbasin 894 and testing subbasin 750 produce extreme or significantly higher induction errors compared to all other train and test subbasins however for testing subbasin 750 we find two exceptions in which training subbasins 777 and 894 produce significantly lower induction error taking a closer look at the wrb in fig 12 we see training subbasins 777 and 894 co occur with testing subbasin 750 on the tippecanoe river this suggests at least for subbasin 750 there is some correlation between the co occurrence of subbasins on a river and feature similarity in summary spatiotemporal induction for streamflow appears to benefit from the selection of training subbasins that co occur on the river s of target testing subbasins 4 4 2 inter watershed spatiotemporal induction to test the predictive capability of lstm in induction across watersheds we perform spatiotemporal induction from wrb to lrw by optimizing lstm to a subbasin of the wrb and predicting over each subbasin of the lrw we also perform spatiotemporal induction from lrw to wrb but predict over eight randomly selected subbasins of the wrb to keep results concise as a baseline we also perform temporal induction for each subbasin of the lrw and each of the eight randomly selected subbasins of the wrb it is important to note that because soil moisture is not available in the lrw time series it was not used as a predictor for any of the following tests table 3 shows temporal induction row 1 and inter watershed spatiotemporal induction row 2 error across all subbasins of the lrw results in row 2 were achieved from optimizing lstm to subbasin 186 selected for best performance of the wrb we find inter watershed spatiotemporal induction performs on par or slightly worse than temporal induction with one instance subbasin n showing significant error increase fig 14 shows ground truth and predicted streamflow values from temporal and inter watershed spatiotemporal induction on subbasin f in inter watershed spatiotemporal induction lstm rarely over predicts small streamflow values but tends to under predict large streamflow values this runs counter to temporal induction where lstm tends to over predict small streamflow values but rarely under predicts large streamflow values these tendencies for under and over prediction partially explains the relatively low and high error rates in subbasins m and n which possess many missing records specifically missing values replaced in m are near zero but missing values replaced in n are well above zero leading to periods of predictions that are more and less accurate for subbasins m and n respectively overall the results show that lstm can induct from the wrb to lrw with accuracy comparable to induction within the lrw alone table 4 shows temporal induction row 1 and inter watershed spatiotemporal induction row 2 error across the eight randomly selected wrb subbasins results in row 2 were achieved from optimizing lstm to subbasin b again selected for best performance of the lrw we find inter watershed spatiotemporal induction performs worse than temporal induction across all tested subbasins but performance is comparable for some comparable prediction error is promising considering wrb contains 69 years of training data while lrw contains only 28 years in fig 15 we observe the same proclivity of lstm to over predict small streamflow values when it is optimized to lrw data this is likely a consequence of the discrepancy in training sample amounts between wrb and lrw to validate these results we test the similarity of meteorological and hydrological variables between subbasins within and across the wrb and lrw if it is the case that these variables are sufficiently similar the task of inter watershed spatiotemporal induction is no more challenging than temporal induction fig 16 shows the distribution of streamflow left subfigure and minimum temperature right subfigure from four subbasins two from the wrb and two from the lrw amongst the 16 tested subbasins we find subbasins belonging to the same watershed generally share greater feature similarity than subbasins across watersheds and while exceptions exist left subfigure lrw b wrb871 we did not find these to coincide with any significant performance loss relative to other subbasins in summary inter watershed spatiotemporal induction between wrb and lrw appears to be a significantly complex task and lstm shows promising performance overall the results demonstrate lstm can leverage outside data to successfully predict over a target watershed this is particularly useful since data is often missing or severely limited making predictions within the target watershed inaccurate however superior or even comparable results should not be expected if the outside data contains less training samples than the target watershed 4 4 3 induction using multi subbasin training as we saw in the heatmaps of section 4 4 1 low spatiotemporal induction error can be achieved from individual optimization on a variety of training subbasins thus it follows that multiple subbasins with pertinent feature information exist and induction quality may benefit from multi subbasin optimization to learn the effect of optimization over multiple subbasins we perform individual and multi subbasin optimization on subbasins s train 1 141 1275 and predict over subbasins s test 1 2 141 142 1275 1276 table 5 shows the resulting temporal and spatiotemporal induction errors from individual and multi subbasin optimization for soil moisture sm and streamflow sf rows 1 3 show temporal and spatiotemporal induction results from individual optimization while rows 4 6 show induction results from multi subbasin optimization in temporal induction error is generally made worse with just two instances of soil moisture producing negligible improvement conversely in spatiotemporal induction we see significant error reduction in all but one instance where soil moisture error remains unchanged though there may be multiple reasons for this result we believe model over fitting is the primary contributing factor a model becomes over fit when it is optimized to the extent that its learned distribution too closely reflects the distribution of training samples as a result if the divergence between training and testing sample distributions is significant the divergence between prediction and testing distributions is also significant and produces excess error by optimizing over multiple subbasins the lstm is constrained to learn a cdf that is generalized to equally reflect the distribution of all training subbasins this naturally prevents over fitting since fitting too closely to the distribution of any one subbasin incurs significant loss over all other subbasins as such this generalized cdf does not produce a superior fit for any one subbasin compared to individual optimization but does counteract over fitting as a consequence we see either increased or unchanged error for temporal induction and almost exclusive error decrease for spatiotemporal induction this is intuitive since we expect a generalized cdf to fit unseen subbasins better at the expense of fitting training subbasins worse lastly multi subbasin training also benefits from model consolidation which significantly reduces training run time producing an approximate 2 74 speed up in this instance 4 5 comparison with other ml methods we have analyzed the predictive capability of lstm relative to swat and found in some cases lstm can out perform swat however there exist a myriad of ml models with varying degrees of complexity capable of time series prediction that may produce results superior to lstm to further evaluate the efficacy of lstm we train and test two other ml models for comparison arima box and pierce 1970 a classical auto regressive model for time series forecasting geoman liang et al 2018 a complex lstm network with temporal and spatial attention we specifically test the temporal and spatiotemporal induction capabilities of arima geoman and lstm for subbasin pairs 1 2 141 142 1275 1276 each model is optimized to the training period of subbasin s train 1 141 1275 and then used to predict over the testing period of subbasins s test 1 2 141 142 1275 1276 errors from temporal and spatiotemporal induction over soil moisture sm and streamflow sf after individual optimization to each of the three training subbasins are listed for each model in table 6 4 5 1 temporal induction results arima delivers predictions of significantly higher error across all trained subbasins compared to geoman and lstm an expected result given its limited complexity after closely considering the plotted predictions optimized arima models appear to predict a mean value approximated over the given eight weeks of predictors soil moisture and streamflow vary significantly at weekly resolution causing systematic over and under prediction by arima alternatively the difference in temporal induction error moving from geoman to lstm is either negligible subbasin 1 or slightly improved subbasin 141 and 1275 for both soil moisture and streamflow this performance is promising for lstm given geoman leverages additional exogenous variables employs multiple attention mechanisms and learns significantly more parameters and because it is a far simpler model lstm benefits from an approximate 3 6 times speed up in training run time over geoman 4 5 2 spatiotemporal induction results for spatiotemporal induction geoman and lstm again out perform arima however on soil moisture lstm gives higher induction error compared to geoman for all testing subbasins with only one showing negligible error difference lstm produces predictions of lower induction error than geoman for two of the three testing subbasins with subbasin 1276 seeing significant error reduction improved induction capability for lstm over geoman is understandable given the significant increase in model complexity and the susceptibility to over fitting that follows as we saw in section 4 4 3 over fitting is especially problematic for spatiotemporal induction since fitting a model too closely to the distribution of training samples is likely to maximize model misspecification error in summary the improvement of lstm over geoman is somewhat expected since the increased complexity of geoman increases the potential for over fitting and poor generalization 4 6 temporal reduction in this section we demonstrate the trade off between prediction accuracy and training run time for various temporal reduction settings we consider reductions to weekly bi weekly and monthly resolutions w 7 14 28 and compare with the original daily resolution w 1 testing set nrmse is shown in table 7 for arima geoman and lstm when predicting soil moisture sm and streamflow sf we selected subbasin 529 from the wrb and utilized observed data for streamflow and swat simulation data for soil moisture to keep results concise we show only the training run time of lstm and select soil moisture prediction since swat simulated data has significant size we find error generally increases with decreasing temporal resolution and this effect is amplified by reduction stride this is expected in geoman and lstm since training sample count is reduced with lower resolution and reduction stride is the primary factor for sample loss to better understand the relationship between temporal reduction and error we consider percentage error increase pei from daily w s 1 to monthly resolution w 28 s 1 in each model for streamflow arima shows least pei at 368 with geoman and lstm having peis of 984 and 453 respectively for soil moisture pei differences between models are less significant at 557 454 and 379 for arima geoman and lstm we believe these results stem from the discrepancy in noise between soil moisture and streamflow and the benefit of denoising for models of lower complexity we first note that arima lstm and geoman represent three levels of model complexity with arima and geoman holding the lowest and highest ends moreover model complexity is strongly correlated with prediction accuracy as models of higher complexity are more capable of capturing noisier variables mu et al 2020 considering the distribution raw values and model errors it is clear that streamflow possesses significantly more noise than soil moisture lastly we note that temporal reduction has a significant and clear denoising effect on these variables demonstrated in fig 17 this dynamic leads to arima and geoman seeing the least and greatest pei on streamflow due to the denoising of temporal reduction which favors models of lower complexity accordingly soil moisture pei is less significant due to the lower degree of noise which decreases to extent of denoising by temporal reduction in summary temporal reduction decreases prediction accuracy but allows for the consideration of lower complexity models and decreases resource consumption 5 discussion 5 1 application to hydrologic event forecasting a key application of lstm predicted values is in forecasting the occurrence and severity of extreme hydrologic events such as droughts and floods even though we do not focus on extreme event forecasting we show that the lstm model can faithfully predict extreme values that in turn help forecast extreme events to demonstrate this capability we train and test an lstm model just as before but classify predicted soil moisture and streamflow values according to a standardized index then each predicted time step t for subbasin s is converted to its z score z s t via equation 7 where mean μ s t and standard deviation σ s t are calculated to each subbasin s 1 1276 and day of year t 1 366 7 z s t x s t μ s t σ s t we classify hydrologic events into 3 broad categories wet dry and normal wet and dry categories are each split into three severity sub categories resulting in seven total classes that occupy the following z score intervals extremely dry z s t 2 severely dry z s t 2 1 5 moderately dry z s t 1 5 1 near normal z s t 1 1 moderately wet z s t 1 1 5 severely wet z s t 1 5 2 extremely wet z s t 2 fig 18 shows a subsection of predictions over the testing period for subbasin 1 in the wabash river watershed with extreme events marked for the purpose of readability wet and dry events are pooled into their corresponding severity categories moderate sever and extreme while near normal events are omitted entirely blue regions enclose near normal moderate and severe intervals with extreme events z s t 2 being those that fall outside all regions for soil moisture lstm has a slight tendency to under predict wet conditions resulting in the majority of incorrect wet classifications to be less severe than the actual no significant bias is shown in miss classifications during dry conditions as a consequence of over and under prediction overall lstm achieves an approximate 80 accuracy in soil moisture event classification with relatively strong accuracy acc 50 across all classes for streamflow lstm does not appear to have any significant bias towards under or over prediction but clearly struggles to correctly predict wet condition events overall lstm achieves an approximate 86 accuracy in streamflow event classification but this result is overwhelmingly a consequence of superior classification accuracy for near normal events in summary lstm can accurately forecast the occurrence of non normal soil moisture and streamflow events but accurately predicting the severity of such events remains a challenge 5 2 the use of high performance computing modern deep learning models used in image processing or language modeling often have billions of parameters thus it is fairly common to use high performance computing resources such as graphical processing unit gpus or distributed supercomputers to train complex models with terabytes of training data you et al 2018 shoeybi et al 2019 since we used small scale datasets to train a relatively simple lstm model we utilized a single gpu to train our ml model however our software is built on pytorch that makes it very easy to use distributed cyberinfrastructure whenever necessary in the next phase of our software we will incorporate distributed training and inference options in our software pipeline and then train deep learning models with national scale hydrology data 5 3 limitations even though we present a comprehensive evaluation of soil moisture and streamflow predictions the presented ml methods could be improved further in two aspects first the lstm model is trained only with temporally varying data such as precipitation and temperature thus our software currently does not take advantage of static features such as hydrography topography soils and landuse landcover such static features can directly be added in the last fully connected layer in fig 4 or via a combination of convolutional and recurrent neural networks future releases of our software will incorporate both static and temporal features second we only measured the accuracy of various methods using nmse eq 6 because we wanted to compare relative errors among different methods however there are a variety of metrics to quantify the error when predicting streamflow and soil moisture gupta et al 1998 the nash sutcliffe efficiency nse is a widely used metric that measures the coefficient of determination r 2 of the observed and predicted streamflow nash and sutcliffe 1970 hydrologists also use hydrologic signatures of prediction and observation to measure model performance mcmillan 2020 though each of these metrics could be used to evaluate lstm s predictions nmse allows us to remain focused on the relative performance of various methods 6 conclusions lstm models are robust in temporal and spatiotemporal predictions of hydrologic features in this paper we have clearly demonstrated this robustness in the prediction of soil moisture and streamflow in two watershreds in indiana and georgia while lstm models are known to perform well for time series data their notable performance in spatiotemporal predictions suggest the resemblance among hydrologic processes observed in different subbasins ml models can be trained much faster than complex process based models the time required for building a swat model and calibrate its parameters often span from weeks to months or even years depending on the size and complexity of a watershed for the wabash river basin case we spent 2 years to collect data climate forcing soils land use terrain and hydrography and calibrate the model to achieve acceptable results for simulating streamflow this compare to several weeks we spend on designing the lstm model and only a few minutes needed to train the model with the wabash river data table 6 despite that the extra efforts in preparing the swat model help it to perform slightly better than lstm the high efficiency and comparable accuracy of lstm make it an appealing and practical tool for hydrologic predictions particularly when limited time and resources are available process based and ml models can work together to overcome the scarcity of historic data to achieve acceptable predictions from any ml model one needs adequate data to train the model hence limited historic data can severely impede the applicability of ml methods in ungauged subbasins we demonstrate that simulated data from the swat model can adequately train lstm models for predicting both soil moisture and streamflow for both gauged and ungauged subbasins reduced model and data complexity do not reduce prediction accuracy we applied temporal reduction to time series data and decreased memory computational and model complexity section 3 4 1 while the reduced data decreased the time needed to train all models the prediction accuracy did not drop significantly furthermore the relatively simple lstm model fig 4 performs comparably to the highly complex geoman liang et al 2018 model at a fraction of the run time table 6 thus simpler ml models and sub sampling of available data are often sufficient in data driven predictions of hydrologic events editorial conflict of interest statement given his role as environmental modeling software editorial board member xuesong zhang was not involved in the peer review of this article and has no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to journal editor daniel p ames software availability software hydrolearn developer nicholas majeske operating systems windows linux and mac osx dependent software our software pipeline is implemented in python 3 7 and leverages the pytorch package version 1 5 0 for ml model construction and optimization availability the code base is publicly available and can be cloned from the github repository at https github com hipgraph hydrolearn to reproduce results the repository includes wabash river basin and little river watershed ground truth time series data and a driver script to optimize and evaluate the lstm for either of these data sets the repository also includes a step by step guide on integrating new spatiotemporal data and models into the pipeline memory requirements of the swat simulated wabash time series data prevent its inclusion into the repository but this data set may be accessed at https futurewater indiana edu declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements nm and aa were supported by the applied mathematics program of the doe office of advanced scientific computing research under contract number de sc0022098 lg and cz acknowledge support from the eri xuesong zhang was supported by the u s department of agriculture agricultural research service 
25593,we present machine learning methods to predict hydrologic features such as streamflow and soil moisture from spatially and temporally varying hydrological and meteorological data we used a temporal reduction technique to reduce computation and memory requirements and trained a long short term memory lstm network to predict soil moisture and streamflow over multiple watersheds we show lstm networks can be trained in a fraction of the time required by complex process based and attention based models such as soil and water assessment tool swat and geoman without sacrificing accuracy we also demonstrate that outside data sourced from a watershed other than the target can be used to train lstm to comparable or even superior prediction accuracy the success of lstm in such spatially inductive settings shows hydrologic features can be predicted with minimal prior knowledge of the watershed in question finally we make all methodologies of this work publicly available as an end to end software pipeline that facilitates rapid prototyping of hydrologic learners graphical abstract image 1 keywords hydrology machine learning long short term memory wabash river little river watershed 1 introduction hydrologic features such as streamflow and soil moisture in river basins are important for agriculture managing reservoirs and hydropower operations predicting future streamflow and soil moisture levels are also used to predict extreme hydrologic events such as floods and droughts that are among the most severe weather related disasters that cause widespread damage in agriculture wildlife habitats and human properties for example the 2012 drought in the continental united states resulted in an estimated 30 billion in mostly agricultural losses rippey 2015 hence forecasting hydrologic features and predicting the occurrence frequency and severity of hydrologic events are extremely important for societal well being reichstein et al 2019 there are two fundamental approaches to predict hydrological features and events process based and data driven models process based models fatichi et al 2016 nash 1957 diskin et al 1984 arnold et al 1995 abbott et al 1986 fatichi et al 2016 simulate hydrologic processes often referred to as rainfall runoff models and have a long history dating back to the 1960s it has been observed that rainfall runoff models often exhibit low transferability meaning that the model performance declines when simulating events outside of the calibration period guo et al 2020 hartmann and bárdossy 2005 in contrast data driven models remesan and mathew 2016 solomatine et al 2009 learn functions that map input features to output variables based on training data and do not explicitly simulate physical or conceptual representation of the hydrologic processes despite a long standing discussion klemeš 1986 sungmin et al 2020 zheng et al 2018 about the relative merits of process based and data driven models both methods have individually delivered reliable predictions of various hydrologic features in this paper we aim to utilize the complementary merits of both methods to make accurate temporal and spatial predictions of soil moisture and streamflow one particular example is in predicting streamflow from ungauged basins for which no training data exists in this case we can train machine learning ml models using simulated data from process based models and forecast hydrologic events in ungauged basins we demonstrate that process based and ml models when working in tandem improve spatial and temporal generalization of the underlying hydrologic processes and help us improve spatial validation one of many major open challenges in hydrology blöschl et al 2019 the primary objective of this paper is to show the applicability of simple lstm models in cooperation with swat when predicting hydrologic events in two watersheds despite the recent surge in using ml methods in water resource studies we identify and address four aspects that received less attention in the literature 1 spatiotemporal learning it is not adequately studied how ml models could use historic data from one basin to predict hydrologic variables in other basins we study this spatiotemporal learning task extensively at multiple levels including cross subbasin and cross basin 2 limited historic data it is not well studied how ml models could be trained when only limited historic data is available previous work has mostly focused on streamflow due to the abundance of historic streamflow data however the literature is sparse on predicting soil moisture for which historic data is limited we demonstrate that adequate predictions of soil moisture and streamflow for both gauged and ungauged subbasins may be achieved by training ml models on swat simulated data 3 reducing computational time it is computationally expensive to train complex ml models on hundreds of years of multivariate data originating from thousands of subbasins we demonstrate that the relative low complexity lstm model can be trained much faster than complex ml models such as geoman liang et al 2018 without sacrificing prediction accuracy 4 an end to end software pipeline domain scientists often face a stiff learning curve when processing hydrology data in ml frameworks such as pytorch paszke et al 2019 and tensorflow abadi et al 2016 we developed a pipeline that packages data processing model training evaluation and experimental design and analysis this pipeline was designed for ease of use but remains highly modular to meet the needs of both non experts and experts alike this paper addresses the aforementioned objectives by 1 extensively studying spatiotemporal learning tasks using ml models 2 leveraging simulated data to produce reasonable predictions for basins of limited historical data 3 utilizing a model of relatively low complexity lstm and reduction techniques to reduce computational requirements while retaining accuracy and 4 by making our end to end pipeline publicly available to facilitate the development of ml models for hydrological prediction by experts and non experts 2 a brief background of machine learning in hydrology machine learning refers to a set of data driven methods that learn patterns present in data a supervised ml model tunes its parameters to learn input output relations present in the training data after training the model one can predict outputs or events from test data that is not seen by the model during training in recent years ml models especially artificial neural networks anns have been widely used in hydrologic predictions solomatine et al 2009 solomatine and ostfeld 2008 schaap and leij 1998 parisouj et al 2020 hu et al 2020 adikari et al 2021 for the purpose of this paper we broadly divide ml methods in hydrology into four groups a regression models that do not use anns b various ann models excluding recurrent neural networks rnns c methods based on rnns and d combination of process based and ml models classical ml methods such as logistic regression k nearest neighbors support vector machine svm and random forest have been widely used in various hydrologic predictions such as classifying water supply vulnerability robinson et al 2020 predicting streamflow solomatine et al 2008 worland et al 2018 flood forecasting solomatine and xue 2004 chapi et al 2017 and approximating process based models zhang et al 2009b however recent work cheng et al 2020 kratzert et al 2019b a have demonstrated that anns capture complex input output relations better than simple regression models this is not surprising given that deep neural networks are exponentially expressive because deeper networks can memorise very large dataset montúfar et al 2014 hydrology researchers used simple multi layer perceptron and fully connected neural networks to predict soil moisture retention and streamflow patterns cheng et al 2020 schaap and leij 1998 cai et al used deep learning regression network dnnr to develop a soil moisture prediction model cai et al 2019 however considering the temporal nature of hydrology data rnn and its variants are more reasonable choices given rnn s ability to model sequence data indeed long short term memory lstm models were trained on hundreds of basins to demonstrate that lstm outperformed traditional process based models both in temporal and spatial predictions kratzert et al 2019b a majeske et al 2020 custom built rnn models such as neural runoff model nrm xiang and demir 2020 and geoman liang et al 2018 also performed much better than regression models and other anns finally several recent work combined process based and ml models to take complimentary advantage of both approaches for example essenfelder et al coupled the soil and water assessment tool swat arnold et al 1995 1998 and ann models to predict inter basin water transfer essenfelder and giupponi 2020 3 materials and methods 3 1 study area and data 3 1 1 wabash river basin the wabash river basin wrb in the us midwest is the largest northern tributary of the ohio river the wrb drains over 90 000 km2 and has a humid continental climate with a mean annual temperature of 11 3 c and mean annual precipitation of 122 7 cm over the 1971 2000 period the basin shown in fig 1 a spans most of the state of indiana part of eastern illinois and a small section of western ohio major land use types include 52 cropland and 19 forest with the remaining 29 covered by generic agriculture tall fescue residential and water the wrb has a mean slope of 0 009 with elevation and subbasin area ranging from 75m to 376m and 21 km2 200 km2 respectively the wrb time series consists of daily hydrological and meteorological records spanning january 01 1929 to 12 31 2013 for t 31 046 time steps the basin is subdivided into s 1 276 distinct subbasins each of which record f 5 features 2 hydrological soil moisture and streamflow and 3 meteorological precipitation minimum and maximum temperature hydrological features of the time series were obtained from simulation using the swat model arnold et al 1995 1998 meteorological features are observed records obtained from the university of notre dame boryan et al 2011 from this basin we also have daily observed streamflow records from s 5 subbasins spanning january 01 1985 to 12 31 2013 for t 12 424 time steps these streamflow records are combined with observed meteorological records resulting in f 4 features for each time step and subbasin we denote swat simulated and observed wrb time series formally by equation 1 1 wabash swat r 31 046 1 276 5 wabash gt r 12 424 5 4 3 1 2 little river watershed the little river watershed lrw fig 1b is the upper 334 km2 of the little river in georgia us and part of us department of agriculture usda agricultural research service ars long term agriculture research ltar https ltar ars usda gov sites gacp the region is of low relief and characterized by flat uplands and alluvial floodplains and located in the humid subtropical climate zone with typical annual precipitation of 1167 mm major land use types include 50 forest and 31 cropland with the remaining 19 covered by pasture urban and water the lrw has a mean slope of 0 08 with elevation and subbasin area ranging from 104m to 146m and 3 km2 65 km2 respectively daily precipitation temperature and streamflow data used in this study were obtained from little river experimental watershed database bosch et al 2007 and processed in our previous studies zhang et al 2009a 2011 the lrw time series consists of daily hydrological and meteorological records spanning january 01 1968 to 12 31 2004 for t 13 515 time steps the watershed is subdivided into s 8 distinct subbasins each of which record f 4 features 1 hydrological streamflow and 3 meteorological precipitation minimum and maximum temperature this time series possesses many missing records with one subbasin missing as many as 8 521 13515 entries we handle missing records by replacement via forward propagation which involves replacing all missing values with the last known value we denote the lrw time series formally by equation 2 2 little gt r 13 515 8 4 3 2 types of inductive predictions among various prediction tasks we particularly focus on inductive predictions that draw conclusions about future events from past and current samples the primary challenge in inductively predicting hydrological variables stems from the nature of data which varies both temporally being dynamic across time and spatially originating from multiple geographical locations consider hydrological and meteorological records x 1 x t 1 x t and y 1 y t 1 y t collected over t discrete time steps from two subbasins x and y when x t is predicted from a model trained with observed data x 1 x t 1 it is called temporal induction in contrast when y t is predicted from a model trained with observed data x 1 x t 1 it is called spatiotemporal induction fig 2 visualizes and explains these concepts further we develop ml models that work well for both temporal and spatiotemporal induction within and across watersheds 3 3 swat configuration in addition to historical streamflow from five wrb subbasins and eight lrw subbasins we use swat simulated streamflow and soil moisture values from other ungauged subbasins swat is a semi distributed watershed modeling program developed by the agricultural research service ars of the u s department of agriculture arnold et al 1995 1998 in this paper we use previously generated data dierauer and zhu 2020 from a calibrated swat model available at https futurewater indiana edu the calibration and prediction from swat rely on historical time series including minimum maximum temperature precipitation and wind speed additionally swat model construction uses inputs of hydrography topography soils and landuse landcover compiled from multiple governmental agencies the additional information coming from hydrographic and topographic data helps swat capture the hydrologic cycle 3 4 ml software pipeline 3 4 1 temporal reduction the daily time step resolution of wrb and lrw time series is precise but computationally cumbersome and superfluous in predicting extreme events such as drought with this in mind we apply temporal reduction to reduce time step resolution and significantly decrease memory computation and model complexity with minimal cost to prediction accuracy let t be time step count of the original time series w be reduction window size and s be reduction window stride in the trivial case a weekly reduction uses w s 7 but this fails to consider all possible weeks and thus we generalize temporal reduction for any s this allows control over the resulting time step resolution by varying w but also time step distinctiveness by varying s since s determines the extent of window overlap the sliding window method fig 3 which we use to extract input output window samples requires time step contiguity to ensure contiguity persists through temporal reduction w s contiguous reduced temporal channels are created each channel is formed by applying the trivial w s reduction at offsets c offset 0 1 w s 1 lastly input output window samples are extracted from each channel separately and pooled to create the final window sample sets x and y 3 4 2 feature pre and post processing due to the distribution of features across spatial and temporal domains proper pre and post processing is essential for model optimization and induction in induction it is critical to transform features into a common range so that the learned parameters of the ml model are invariant to feature magnitude for example subbasin 772 of the wrb sees mean streamflow of approximately 0 169 m3 s while mean streamflow of subbasin 52 is approximately 842 187 m3 s in this case optimizing on subbasin 772 and predicting on subbasin 52 or vice versa will cause severe and systematic under over prediction to make induction feasible and alleviate various other numerical challenges overflow exploding gradients etc all features are normalized to the common range 1 1 since each model is optimized over input outputs in the range 1 1 raw model outputs may not be used as final predictions thus post processing involves transforming feature predictions back into their original range via inverse normalization normalization for feature f of subbasin s to interval a b and inverse normalization are defined in equations 3 and 4 3 x s f b a x s f m i n s f m a x s f m i n s f a 4 x s f m a x s f m i n s f x s f a b a m i n s f 3 4 3 the lstm network the lstm network is a recurrent neural network rnn that utilizes the lstm cell for its recurrent unit the lstm cell proposed in hochreiter and schmidhuber 1997 is designed specifically to handle the challenge of learning long term dependencies from temporally distributed data pascanu et al 2013 in this work we employ a simple lstm network architecture consisting of three layers 1 a temporal encoding layer to encode a sequence of i temporally distributed predictors x x 1 x 2 x i into latent representation h enc 2 a temporal decoding layer to decode representation h enc into a sequence of o latent representations h dec h 1 dec h 2 dec h o dec and 3 a linear layer to convert from decoding dimension d enc to response dimension d y for all representations in h dec fig 4 shows a schematic diagram of the lstm network model used in this paper the steps of forward propagation for the lstm network are defined in equation 5 where θ θ enc θ dec θ lin denotes all learned network parameters 5 h enc l s t m enc x θ enc h dec l s t m dec h enc θ dec y l i n e a r h dec θ lin the relation between response dependent and predictor independent variables is statistical and formally defined by the conditional probability p y x t p x t p x where x t p denotes the pth predictor variable at time step t in optimizing the lstm network the parameters of a non linear function mapping past predictors precipitation soil moisture streamflow etc into a future response soil moisture or streamflow are learned more specifically lstm optimization learns a conditional density function cdf that approximates the true underlying conditional probability p y x t p θ p y x t p parameterized by θ we formally define lstm in terms of cdf estimation as a premise for observed induction performance and analysis in section 4 3 4 4 training and testing settings in order to facilitate optimization and validate model efficacy we partition each original time series into training validation and testing sets the training set is used to optimize model parameters directly validation to guide model optimization towards better generalization and testing to evaluate final model performance for the wrb time series we assigned the non overlapping periods january 01 1985 12 31 1997 january 01 1998 12 31 2005 and january 01 2006 12 31 2013 to training validation and testing sets respectively for the lrw time series training validation and testing sets were assigned to the non overlapping periods january 01 1968 12 31 1995 january 01 1996 12 31 1999 and january 01 2000 12 31 2004 the training validation and testing periods above were assigned to occupy approximately 80 10 and 10 percent of the original time series a common split ratio 8 1 1 in machine learning in this work we define model efficacy in terms of prediction quality over the testing set since this most accurately reflects model generalization we quantify prediction quality via normalized root mean square error nrmse defined in equation 6 where t is a time step of the target period testing set s is the target subbasin and r is the predicted response we chose nrmse because it defines error in a consistent range 0 1 and allows for meaningful comparison between predictions independent of target period subbasin and response all experiments were executed on an acer predator g3 710 equipped with an intel core i7 7700 at 3 6 ghz 32 gb ram and nvidia s geforce 1070 1920 cuda cores 1 5 ghz 8 gb memory when predicting o 1 week into the future best testing set nrmse for lstm was achieved using i 8 weeks of past predictors d enc d dec 128 hidden units adadelta optimizer zeiler 2012 with mini batch size 128 learning rate η 0 1 xavier initialization glorot and bengio 2010 and 100 epochs of training with optimal predictors defined in section 4 2 6 rmse s r t 1 t y t s r y t s r 2 t nrmse s r rmse s r m a x s r m i n s r 4 results 4 1 experimental goals before discussing the results in detail it is important to first clarify our experimental goals the results section is organized into four major subsections that aim to answer the following questions 4 2 what are the optimal predictors for soil moisture and streamflow 4 3 what is the predictive performance of lstm in the baseline temporal induction setting and how does this compare to the process based model swat 4 4 how does lstm perform in more complex induction settings such as spatiotemporal and multi subbasin induction 4 5 how does lstm performance compare to more and less complex ml methods for temporal and spatiotemporal induction settings each subsection defines our motivation for the experiment the settings and results and finally offers an analysis of the observed results 4 2 predictor informativeness in order to minimize prediction error it is important to analyze the contribution of each predictor so that we may establish an optimal set table 1 enumerates soil moisture and streamflow prediction error for various predictor sets for this experiment we separate predictors into two categories supplementary and primary supplementary predictors are variables not included in the response soil moisture or streamflow while primary predictors are variables included in the response these categories reflect our assumption that past response values are most informative or primary to future response values while all other variables are supplementary or non primary predictors table 1 lists results arranged into the following three groups blocks of rows that show prediction error when 1 optimizing using only one of all possible predictors 2 optimizing using one supplementary and the primary predictor 3 optimizing using all possible predictors as expected we find the best performance from group 1 emboldened when using the primary predictor confirming past responses are most informative to future responses in group 2 we find precipitation to be the most informative supplementary predictor for both soil moisture and streamflow supplementary predictors day of year and soil moisture were the second most informative for soil moisture and streamflow respectively and in group 3 we observe the lowest prediction error for soil moisture and streamflow when using all possible predictors it should be noted that all possible predictor subsets were tested and three instances were found to reduce error over all possible predictors all three instances included five of six possible predictors and were missing either minimum or maximum temperature but never both this would suggest that while both are informative these variables possess significant mutual information and the inclusion of both is likely redundant given that error decrease was marginal and predictor dimension d x from group 3 does not warrant reduction all possible predictors were chosen as the optimal set and used in all proceeding experiments unless otherwise stated 4 3 temporal induction in this section we analyze the predictive capability of lstm for temporal induction in temporal induction the parameters of each model are optimized to training period samples of a subbasin the optimized model is then used to predict over samples of the mutually exclusive future testing period of the same subbasin in all instances of this section samples from the testing period are never used during optimization and thus prediction error reflects quality of induction across the temporal domain 4 3 1 comparison with swat we evaluate the relative predictive capability of the swat and lstm models on wrb ground truth gt data as previously stated in section 3 ground truth streamflow records are available for 5 wrb subbasins which include 43 169 348 529 and 757 table 2 enumerates swat and lstm prediction error for all 5 wrb subbasins with ground truth streamflow records each row shows prediction performance under the following scenarios 1 swat optimization and prediction on gt 2 lstm optimization and prediction on gt 3 lstm optimization to swat predictions from period 01 01 1985 12 31 1997 and prediction on gt 4 lstm optimization to swat predictions from period 01 01 1929 12 31 1997 and prediction on gt in rows 1 and 2 we directly compare lstm and swat to gauge lstm prediction performance in rows 3 and 4 we test lstm prediction performance when optimized to synthetic swat prediction data in most cases swat out performs lstm but lstm can perform nearly as well or better than swat figs 5 and 6 show lstm a and swat b predictions against gt streamflow for subbasins 169 and 529 respectively when training and testing on gt data in fig 5 we see lstm struggles to adequately capture extreme streamflow values compared to swat resulting in systematic under prediction in fig 6 lstm still struggles to fully capture extreme values but does not systematically over predict as is the case with swat prediction streamflow prediction appears to be particularly difficult for lstm as it tends to under predict but in cases where swat over predicts subbasin 529 lstm can deliver predictions superior to swat it is promising that lstm should produce any superior comparable result when we consider the many factors that favor swat prediction for example swat model construction uses inputs of hydrography topography soils and landuse landcover compiled from multiple governmental agencies in contrast the lstm model only uses a handful of predictors shown in table 1 despite swat s apparent advantage lstm outperforms swat in subbasin 529 and 757 more importantly as we will see in the following swat simulations may be used to optimize the lstm model and predict soil moisture and streamflow values quite accurately in row 3 we consider the case of synthetic data substitution but select only a subset from the complete synthetic training period that coincides with the available gt training period 01 01 1985 12 31 1997 our rationale for selecting this reduced training period is to avoid introducing bias by optimizing lstm over more synthetic training samples than what is available for gt data we find performance is generally worse compared to optimization on gt data but note that performance degradation is mostly insignificant this result demonstrates that swat predictions can be used to train lstm without significant increase in prediction error in row 4 we again consider synthetic data substitution but instead use the complete synthetic training period 01 01 1929 12 31 1997 given that we are required to substitute synthetic data we would like to quantify the benefit of optimizing lstm over more data as a consequence the use of more data for optimization is a clear advantage and we see reduction in prediction error for all but subbasin 43 relative to row 3 this result though expected demonstrates that the greater abundance of synthetic data relative to gt which is often the case when synthetic data is easier to acquire can be an advantage overall the predictive performance of lstm is not strictly superior to swat and we would not use this model as a replacement this is expected since the lstm is a highly generalized function approximator and far less complex than swat however in certain circumstances the lstm model can improve over swat for gt prediction and can be optimized at a fraction of the run time in all cases 4 3 2 overall temporal induction performance to thoroughly understand the capability of lstm to perform temporal induction we temporally induct soil moisture and streamflow for all subbasins of the wrb we use the swat simulated time series because data is available for all 1276 wrb subbasins and as shown in section 4 3 1 swat predictions are reasonably close to ground truth this experiment includes the optimization of 1276 separate lstm models to predict soil moisture and an additional 1276 to predict streamflow settings were identical across all lstm models with the exception of selected response variable soil moisture or streamflow fig 7 shows the distribution of temporal induction error a and feature value b for soil moisture and streamflow the error distributions show streamflow prediction to be a significantly more challenging task than soil moisture prediction the stark difference between error distributions appears to be a consequence of the difference in distribution of feature values at first glance soil moisture appears to be normally distributed while streamflow appears to be log or log normally distributed soil moisture shows slight negative skew at μ 3 0 0552 but streamflow shows heavy positive skew at μ 3 3 141 to explain why this presents a challenge we first note that lstm though non linear as an end to end model uses a linear transformation with identity activation as its output layer thus lstm may be equivalently defined as a non linear regression function f x θ g φ x θ enc θ dec θ lin where g is a linear regression function and φ is some non linear basis function it is well established that the minimization of least squares for regression is equivalent to maximum likelihood estimation with gaussian error assumption given that lstm is trained via mean square error mse loss we effectively assume the response is gaussian normally distributed this appears to be correct for soil moisture but incorrect for streamflow and explains at least partially the discrepancy between temporal induction errors 4 4 spatiotemporal induction in this section we analyze the predictive capability of lstm for spatiotemporal induction in spatiotemporal induction the parameters of each model are optimized to training period samples of a training subbasin the optimized model is then used to predict over samples of the mutually exclusive future testing period of a separate testing subbasin in all instances of this section samples from the testing period of the testing subbasin are never used during optimization and thus prediction error reflects quality of induction across both spatial and temporal domains 4 4 1 intra watershed spatiotemporal induction we first test the capability of lstm to perform spatiotemporal induction within a watershed to do this we randomly select two mutually exclusive sets of subbasins designating one for training s train and the other for testing s test for each training subbasin s i s train we optimize an lstm to its training period and then predict over the testing period of all testing subbasins s i s test i we again use the swat simulated wrb time series for the motivations addressed in section 4 3 2 to keep this experiment concise and visual analysis practical forty wrb subbasins were selected at random and split evenly into training and testing sets in total spatiotemporal induction is performed over all possible train test subbasin pairs resulting in s train s test 400 inductions for soil moisture and streamflow fig 8 visualizes the distribution of error over the 400 inductions for soil moisture and streamflow overall spatiotemporal induction is significantly more erroneous than temporal induction for both response variables we believe the primary driver of performance loss is the significant increase in model misspecification error for any learning task there exist several underlying sources of error that limit final prediction performance in zhu and laptev 2017 the authors attempt to capture prediction uncertainty by modeling three underlying sources of error referred to as model uncertainty inherent noise and model misspecification though model uncertainty and inherent noise certainly contribute to our prediction error we believe model misspecification is particularly impactful for spatiotemporal induction model misspecification refers to the error assumed when optimizing and predicting over samples drawn from non identical divergent distributions this causes the testing error of any model even if optimized to exactly reproduce the distribution of training samples to be lower bounded by the divergence of training and testing sample distributions in temporal induction we assume model misspecification error since training and testing samples are drawn from separate time periods and thus are non identically distributed spatiotemporal induction effectively compounds this error because training and testing samples are drawn from separate time periods and geographical locations in other words induction performance is limited by the similarity of training and testing samples and we expect similarity to decrease when training and testing on different subbasins in addition to different time periods fig 9 provides an example of the divergence between training testing distributions and its effect on induction when training and testing on subbasins 1 and 2 respectively there is clear divergence between distribution of soil moisture in the training set of subbasin 1 and the testing set of subbasin 2 as a result we see the distribution of testing set predictions over subbasin 2 falls somewhere between training and testing distributions namely lstm tends to under predict streamflow in the normalized range 0 5 1 0 because the training set distribution is significantly less left skewed we visualize this discrepancy in the plotted curves of fig 10 with left and right sub figures showing prediction from temporal and spatiotemporal induction over subbasin 2 the upper quartile of soil moisture values in subbasin 2 are not captured in spatiotemporal induction because soil moisture values in this range are not adequately represented by the training set of subbasin 1 for streamflow we again see the distribution of predictions fall somewhere between training testing distributions but does not capture the normalized range 0 0 1 0 of either we also see many testing set streamflow values that far exceed the 69 year training set maximum causing normalized values upwards of x 1 58 these extreme values appear to cause lstm to over predict resulting in a prediction distribution that possesses a significantly heavier tail than training and testing distributions we again visualize the discrepancy in the plotted curves of fig 11 with left and right sub figures showing prediction from temporal and spatiotemporal induction over subbasin 2 predictions from spatiotemporal induction do not entirely capture the lower quartile of streamflow values because extreme values in the testing set of subbasin 2 bias lstm towards over prediction in summary lstm can perform spatiotemporal induction with success but the resulting predictions are biased towards its knowledge learned expected value of the training set alternatively predictions can be biased towards the testing set if the testing set contains enough extreme values of sufficient magnitude in both cases the divergence between training and testing set distributions clearly effects prediction accuracy moreover the potential for training testing divergence is increased for spatiotemporal induction thus making it on average more challenging than temporal induction we additionally visualize spatiotemporal induction errors for soil moisture and streamflow as clustered heatmaps in figs 12 and 13 each column of the heatmap shows induction error across all testing subbasins when lstm is optimized to a given training subbasin rows and columns of each heatmap are hierarchically clustered using pearson correlation distance and shown as dendrograms on the left and top respectively our motivation for clustering induction errors is to better understand the relationship between induction error and choice of train and test subbasins if it is the case for example that spatial proximity of chosen subbasins is correlated to spatiotemporal induction error we can make informed decisions about the training subbasins we select and potentially reduce error before discussing the results in detail it is important to precisely define the semantics behind training and testing subbasin clustering first the clustering of training subbasins indicates significant similarity in induction error when predicting over any of the 20 testing subbasins and as defined earlier we derive from lstm optimization a conditional density function approximation y f x θ p y x t p x t p x thus clustering represents significant similarity in learned conditional density functions and by dependence the distribution of predictor and response variables of training subbasins this follows for testing subbasin clustering with the exception that only similarity in predictor variable distributions is indicated since prediction depends exclusively on these variables finally low induction error indicates significantly similarity in the distribution of predictor and response variables low divergence between train and test subbasins for soil moisture we find a region of low induction error in which training subbasins s train 178 61 170 82 82 112 and testing subbasins s test 81 5 311 100 314 84 878 form a cluster fig 12 shows the wrb with highlighted train and test subbasins and a magnified area containing the six clustered training subbasins above from this region we see all six clustered training subbasins are relatively close to each other and five of the seven clustered testing subbasins are relatively close to these training subbasins given the majority of these low induction error subbasin pairs are relatively close there appears to be some correlation between subbasin proximity and feature similarity in summary spatiotemporal induction for soil moisture appears to benefit from the selection of training subbasins with proximity to target testing subbasins for streamflow we see induction error is generally consistent across training subbasins with the exception of an outlier train and outlier test subbasin in these outlier cases training subbasin 894 and testing subbasin 750 produce extreme or significantly higher induction errors compared to all other train and test subbasins however for testing subbasin 750 we find two exceptions in which training subbasins 777 and 894 produce significantly lower induction error taking a closer look at the wrb in fig 12 we see training subbasins 777 and 894 co occur with testing subbasin 750 on the tippecanoe river this suggests at least for subbasin 750 there is some correlation between the co occurrence of subbasins on a river and feature similarity in summary spatiotemporal induction for streamflow appears to benefit from the selection of training subbasins that co occur on the river s of target testing subbasins 4 4 2 inter watershed spatiotemporal induction to test the predictive capability of lstm in induction across watersheds we perform spatiotemporal induction from wrb to lrw by optimizing lstm to a subbasin of the wrb and predicting over each subbasin of the lrw we also perform spatiotemporal induction from lrw to wrb but predict over eight randomly selected subbasins of the wrb to keep results concise as a baseline we also perform temporal induction for each subbasin of the lrw and each of the eight randomly selected subbasins of the wrb it is important to note that because soil moisture is not available in the lrw time series it was not used as a predictor for any of the following tests table 3 shows temporal induction row 1 and inter watershed spatiotemporal induction row 2 error across all subbasins of the lrw results in row 2 were achieved from optimizing lstm to subbasin 186 selected for best performance of the wrb we find inter watershed spatiotemporal induction performs on par or slightly worse than temporal induction with one instance subbasin n showing significant error increase fig 14 shows ground truth and predicted streamflow values from temporal and inter watershed spatiotemporal induction on subbasin f in inter watershed spatiotemporal induction lstm rarely over predicts small streamflow values but tends to under predict large streamflow values this runs counter to temporal induction where lstm tends to over predict small streamflow values but rarely under predicts large streamflow values these tendencies for under and over prediction partially explains the relatively low and high error rates in subbasins m and n which possess many missing records specifically missing values replaced in m are near zero but missing values replaced in n are well above zero leading to periods of predictions that are more and less accurate for subbasins m and n respectively overall the results show that lstm can induct from the wrb to lrw with accuracy comparable to induction within the lrw alone table 4 shows temporal induction row 1 and inter watershed spatiotemporal induction row 2 error across the eight randomly selected wrb subbasins results in row 2 were achieved from optimizing lstm to subbasin b again selected for best performance of the lrw we find inter watershed spatiotemporal induction performs worse than temporal induction across all tested subbasins but performance is comparable for some comparable prediction error is promising considering wrb contains 69 years of training data while lrw contains only 28 years in fig 15 we observe the same proclivity of lstm to over predict small streamflow values when it is optimized to lrw data this is likely a consequence of the discrepancy in training sample amounts between wrb and lrw to validate these results we test the similarity of meteorological and hydrological variables between subbasins within and across the wrb and lrw if it is the case that these variables are sufficiently similar the task of inter watershed spatiotemporal induction is no more challenging than temporal induction fig 16 shows the distribution of streamflow left subfigure and minimum temperature right subfigure from four subbasins two from the wrb and two from the lrw amongst the 16 tested subbasins we find subbasins belonging to the same watershed generally share greater feature similarity than subbasins across watersheds and while exceptions exist left subfigure lrw b wrb871 we did not find these to coincide with any significant performance loss relative to other subbasins in summary inter watershed spatiotemporal induction between wrb and lrw appears to be a significantly complex task and lstm shows promising performance overall the results demonstrate lstm can leverage outside data to successfully predict over a target watershed this is particularly useful since data is often missing or severely limited making predictions within the target watershed inaccurate however superior or even comparable results should not be expected if the outside data contains less training samples than the target watershed 4 4 3 induction using multi subbasin training as we saw in the heatmaps of section 4 4 1 low spatiotemporal induction error can be achieved from individual optimization on a variety of training subbasins thus it follows that multiple subbasins with pertinent feature information exist and induction quality may benefit from multi subbasin optimization to learn the effect of optimization over multiple subbasins we perform individual and multi subbasin optimization on subbasins s train 1 141 1275 and predict over subbasins s test 1 2 141 142 1275 1276 table 5 shows the resulting temporal and spatiotemporal induction errors from individual and multi subbasin optimization for soil moisture sm and streamflow sf rows 1 3 show temporal and spatiotemporal induction results from individual optimization while rows 4 6 show induction results from multi subbasin optimization in temporal induction error is generally made worse with just two instances of soil moisture producing negligible improvement conversely in spatiotemporal induction we see significant error reduction in all but one instance where soil moisture error remains unchanged though there may be multiple reasons for this result we believe model over fitting is the primary contributing factor a model becomes over fit when it is optimized to the extent that its learned distribution too closely reflects the distribution of training samples as a result if the divergence between training and testing sample distributions is significant the divergence between prediction and testing distributions is also significant and produces excess error by optimizing over multiple subbasins the lstm is constrained to learn a cdf that is generalized to equally reflect the distribution of all training subbasins this naturally prevents over fitting since fitting too closely to the distribution of any one subbasin incurs significant loss over all other subbasins as such this generalized cdf does not produce a superior fit for any one subbasin compared to individual optimization but does counteract over fitting as a consequence we see either increased or unchanged error for temporal induction and almost exclusive error decrease for spatiotemporal induction this is intuitive since we expect a generalized cdf to fit unseen subbasins better at the expense of fitting training subbasins worse lastly multi subbasin training also benefits from model consolidation which significantly reduces training run time producing an approximate 2 74 speed up in this instance 4 5 comparison with other ml methods we have analyzed the predictive capability of lstm relative to swat and found in some cases lstm can out perform swat however there exist a myriad of ml models with varying degrees of complexity capable of time series prediction that may produce results superior to lstm to further evaluate the efficacy of lstm we train and test two other ml models for comparison arima box and pierce 1970 a classical auto regressive model for time series forecasting geoman liang et al 2018 a complex lstm network with temporal and spatial attention we specifically test the temporal and spatiotemporal induction capabilities of arima geoman and lstm for subbasin pairs 1 2 141 142 1275 1276 each model is optimized to the training period of subbasin s train 1 141 1275 and then used to predict over the testing period of subbasins s test 1 2 141 142 1275 1276 errors from temporal and spatiotemporal induction over soil moisture sm and streamflow sf after individual optimization to each of the three training subbasins are listed for each model in table 6 4 5 1 temporal induction results arima delivers predictions of significantly higher error across all trained subbasins compared to geoman and lstm an expected result given its limited complexity after closely considering the plotted predictions optimized arima models appear to predict a mean value approximated over the given eight weeks of predictors soil moisture and streamflow vary significantly at weekly resolution causing systematic over and under prediction by arima alternatively the difference in temporal induction error moving from geoman to lstm is either negligible subbasin 1 or slightly improved subbasin 141 and 1275 for both soil moisture and streamflow this performance is promising for lstm given geoman leverages additional exogenous variables employs multiple attention mechanisms and learns significantly more parameters and because it is a far simpler model lstm benefits from an approximate 3 6 times speed up in training run time over geoman 4 5 2 spatiotemporal induction results for spatiotemporal induction geoman and lstm again out perform arima however on soil moisture lstm gives higher induction error compared to geoman for all testing subbasins with only one showing negligible error difference lstm produces predictions of lower induction error than geoman for two of the three testing subbasins with subbasin 1276 seeing significant error reduction improved induction capability for lstm over geoman is understandable given the significant increase in model complexity and the susceptibility to over fitting that follows as we saw in section 4 4 3 over fitting is especially problematic for spatiotemporal induction since fitting a model too closely to the distribution of training samples is likely to maximize model misspecification error in summary the improvement of lstm over geoman is somewhat expected since the increased complexity of geoman increases the potential for over fitting and poor generalization 4 6 temporal reduction in this section we demonstrate the trade off between prediction accuracy and training run time for various temporal reduction settings we consider reductions to weekly bi weekly and monthly resolutions w 7 14 28 and compare with the original daily resolution w 1 testing set nrmse is shown in table 7 for arima geoman and lstm when predicting soil moisture sm and streamflow sf we selected subbasin 529 from the wrb and utilized observed data for streamflow and swat simulation data for soil moisture to keep results concise we show only the training run time of lstm and select soil moisture prediction since swat simulated data has significant size we find error generally increases with decreasing temporal resolution and this effect is amplified by reduction stride this is expected in geoman and lstm since training sample count is reduced with lower resolution and reduction stride is the primary factor for sample loss to better understand the relationship between temporal reduction and error we consider percentage error increase pei from daily w s 1 to monthly resolution w 28 s 1 in each model for streamflow arima shows least pei at 368 with geoman and lstm having peis of 984 and 453 respectively for soil moisture pei differences between models are less significant at 557 454 and 379 for arima geoman and lstm we believe these results stem from the discrepancy in noise between soil moisture and streamflow and the benefit of denoising for models of lower complexity we first note that arima lstm and geoman represent three levels of model complexity with arima and geoman holding the lowest and highest ends moreover model complexity is strongly correlated with prediction accuracy as models of higher complexity are more capable of capturing noisier variables mu et al 2020 considering the distribution raw values and model errors it is clear that streamflow possesses significantly more noise than soil moisture lastly we note that temporal reduction has a significant and clear denoising effect on these variables demonstrated in fig 17 this dynamic leads to arima and geoman seeing the least and greatest pei on streamflow due to the denoising of temporal reduction which favors models of lower complexity accordingly soil moisture pei is less significant due to the lower degree of noise which decreases to extent of denoising by temporal reduction in summary temporal reduction decreases prediction accuracy but allows for the consideration of lower complexity models and decreases resource consumption 5 discussion 5 1 application to hydrologic event forecasting a key application of lstm predicted values is in forecasting the occurrence and severity of extreme hydrologic events such as droughts and floods even though we do not focus on extreme event forecasting we show that the lstm model can faithfully predict extreme values that in turn help forecast extreme events to demonstrate this capability we train and test an lstm model just as before but classify predicted soil moisture and streamflow values according to a standardized index then each predicted time step t for subbasin s is converted to its z score z s t via equation 7 where mean μ s t and standard deviation σ s t are calculated to each subbasin s 1 1276 and day of year t 1 366 7 z s t x s t μ s t σ s t we classify hydrologic events into 3 broad categories wet dry and normal wet and dry categories are each split into three severity sub categories resulting in seven total classes that occupy the following z score intervals extremely dry z s t 2 severely dry z s t 2 1 5 moderately dry z s t 1 5 1 near normal z s t 1 1 moderately wet z s t 1 1 5 severely wet z s t 1 5 2 extremely wet z s t 2 fig 18 shows a subsection of predictions over the testing period for subbasin 1 in the wabash river watershed with extreme events marked for the purpose of readability wet and dry events are pooled into their corresponding severity categories moderate sever and extreme while near normal events are omitted entirely blue regions enclose near normal moderate and severe intervals with extreme events z s t 2 being those that fall outside all regions for soil moisture lstm has a slight tendency to under predict wet conditions resulting in the majority of incorrect wet classifications to be less severe than the actual no significant bias is shown in miss classifications during dry conditions as a consequence of over and under prediction overall lstm achieves an approximate 80 accuracy in soil moisture event classification with relatively strong accuracy acc 50 across all classes for streamflow lstm does not appear to have any significant bias towards under or over prediction but clearly struggles to correctly predict wet condition events overall lstm achieves an approximate 86 accuracy in streamflow event classification but this result is overwhelmingly a consequence of superior classification accuracy for near normal events in summary lstm can accurately forecast the occurrence of non normal soil moisture and streamflow events but accurately predicting the severity of such events remains a challenge 5 2 the use of high performance computing modern deep learning models used in image processing or language modeling often have billions of parameters thus it is fairly common to use high performance computing resources such as graphical processing unit gpus or distributed supercomputers to train complex models with terabytes of training data you et al 2018 shoeybi et al 2019 since we used small scale datasets to train a relatively simple lstm model we utilized a single gpu to train our ml model however our software is built on pytorch that makes it very easy to use distributed cyberinfrastructure whenever necessary in the next phase of our software we will incorporate distributed training and inference options in our software pipeline and then train deep learning models with national scale hydrology data 5 3 limitations even though we present a comprehensive evaluation of soil moisture and streamflow predictions the presented ml methods could be improved further in two aspects first the lstm model is trained only with temporally varying data such as precipitation and temperature thus our software currently does not take advantage of static features such as hydrography topography soils and landuse landcover such static features can directly be added in the last fully connected layer in fig 4 or via a combination of convolutional and recurrent neural networks future releases of our software will incorporate both static and temporal features second we only measured the accuracy of various methods using nmse eq 6 because we wanted to compare relative errors among different methods however there are a variety of metrics to quantify the error when predicting streamflow and soil moisture gupta et al 1998 the nash sutcliffe efficiency nse is a widely used metric that measures the coefficient of determination r 2 of the observed and predicted streamflow nash and sutcliffe 1970 hydrologists also use hydrologic signatures of prediction and observation to measure model performance mcmillan 2020 though each of these metrics could be used to evaluate lstm s predictions nmse allows us to remain focused on the relative performance of various methods 6 conclusions lstm models are robust in temporal and spatiotemporal predictions of hydrologic features in this paper we have clearly demonstrated this robustness in the prediction of soil moisture and streamflow in two watershreds in indiana and georgia while lstm models are known to perform well for time series data their notable performance in spatiotemporal predictions suggest the resemblance among hydrologic processes observed in different subbasins ml models can be trained much faster than complex process based models the time required for building a swat model and calibrate its parameters often span from weeks to months or even years depending on the size and complexity of a watershed for the wabash river basin case we spent 2 years to collect data climate forcing soils land use terrain and hydrography and calibrate the model to achieve acceptable results for simulating streamflow this compare to several weeks we spend on designing the lstm model and only a few minutes needed to train the model with the wabash river data table 6 despite that the extra efforts in preparing the swat model help it to perform slightly better than lstm the high efficiency and comparable accuracy of lstm make it an appealing and practical tool for hydrologic predictions particularly when limited time and resources are available process based and ml models can work together to overcome the scarcity of historic data to achieve acceptable predictions from any ml model one needs adequate data to train the model hence limited historic data can severely impede the applicability of ml methods in ungauged subbasins we demonstrate that simulated data from the swat model can adequately train lstm models for predicting both soil moisture and streamflow for both gauged and ungauged subbasins reduced model and data complexity do not reduce prediction accuracy we applied temporal reduction to time series data and decreased memory computational and model complexity section 3 4 1 while the reduced data decreased the time needed to train all models the prediction accuracy did not drop significantly furthermore the relatively simple lstm model fig 4 performs comparably to the highly complex geoman liang et al 2018 model at a fraction of the run time table 6 thus simpler ml models and sub sampling of available data are often sufficient in data driven predictions of hydrologic events editorial conflict of interest statement given his role as environmental modeling software editorial board member xuesong zhang was not involved in the peer review of this article and has no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to journal editor daniel p ames software availability software hydrolearn developer nicholas majeske operating systems windows linux and mac osx dependent software our software pipeline is implemented in python 3 7 and leverages the pytorch package version 1 5 0 for ml model construction and optimization availability the code base is publicly available and can be cloned from the github repository at https github com hipgraph hydrolearn to reproduce results the repository includes wabash river basin and little river watershed ground truth time series data and a driver script to optimize and evaluate the lstm for either of these data sets the repository also includes a step by step guide on integrating new spatiotemporal data and models into the pipeline memory requirements of the swat simulated wabash time series data prevent its inclusion into the repository but this data set may be accessed at https futurewater indiana edu declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements nm and aa were supported by the applied mathematics program of the doe office of advanced scientific computing research under contract number de sc0022098 lg and cz acknowledge support from the eri xuesong zhang was supported by the u s department of agriculture agricultural research service 
25594,today computational fluid dynamics approaches have a high level of spatial temporal accuracy in modelling atmospheric transport and dispersion in very complex environments several numerical models require however heavy computational resources and prolonged simulation time up to several days this time constraint is specifically crucial for intervention planning in case of accidental or malevolent toxic releases in a city in this paper we propose to use synthetic data generated by a realistic 3 d transport dispersion simulator to train a learning framework called mcxm the latter relies on a sequence of masking and correction operations to progressively apply the spatial constraints and underlying physics of transport and dispersion the learning phase uses the urban geometry of the french city grenoble we then test the effectiveness of the trained mcxm in a different french city paris the results show that the mcxm s forecasts are virtually instantaneous and generalize successfully to unseen conditions graphical abstract image 1 keywords deep learning hazardous pollutant dispersion simulation surrogate model synthetic data abbreviations ade advection diffusion equation ai artificial intelligence ann artificial neural network cfd computational fluid dynamics dl deep learning dnn deep neural network ml machine learning mse mean squared error pmss parallel micro swift spray 1 introduction 1 1 context air pollutants are substances of different natures that raise many issues for the population being exposed to such pollutants can provoke several effects from minor discomforts e g bad odor to serious health problems such as cancer lung infection and birth defects particularly accidental or hostile emissions of contaminants are highly dangerous events that necessitate an immediate intervention strategy from the authorities to protect the population and limit the material and environmental casualties for this purpose dispersion modelling is used to forecast the concentration of a pollutant at different distances and directions from the sources the models can compare exposures to some selected benchmark such as a state pollution standard or a level with a known health effect to provide recommendations to decision makers e g us legislation united states environmental protection agency for example in an emergency planning study lindell 1995 it was noted that hazardous plumes emanating from a waste incinerator in the usa could arrive in populated areas before people could evacuate or while they are evacuating a shelter in place strategy would be recommended instead sorensen 2004 state of the art models of air pollution transport and dispersion which belong to computational fluid dynamics cfd family are characterized by large memory needs and computational complexity they usually require high performance hardware and several hours of runtime zannetti 1990 accidental pollutant release would ideally require an immediate response from the authorities to contain any potential hazard for the population and the environment in this regard the runtime overhead of atmospheric transport and dispersion simulations may hinder them from being consistent with decision making under time constraints especially if such simulations have to be executed several times for sensitivity or uncertainty evaluations 1 2 related work in the past decade many researchers explored the idea of leveraging machine learning ml to predict atmospheric pollution aguilera et al 2011 zhang et al 2020 araujo et al 2020 this trend stems from the ability of artificial intelligence ai to approximate extremely complex systems from observational data compared to cfd a trained artificial neural network ann is generally time efficient and less demanding in terms of computational resources these properties make it a suitable solution for predicting the exposure to a hazardous release where the assessment time frame is a serious restriction a comprehensive review cabaneros et al 2019 listed recent research activities on air quality with recommendations on how to build anns for predicting long term air pollution regarding short term pollution events lauret et al 2016 solved the advection diffusion equation ade using cellular automata whose transition function is learned by an ann the wind velocity field is however considered known in advance as an input variable in another study wang et al compared two ml models using the prairie grass dataset which refers to a typical hazardous gas release with low stack emission in a flat terrain wang et al 2018 with 20 meteorological inputs the selected neural network architecture two hidden layers of 38 and 4 neurons respectively showed the best performance for a learning phase of 4 4 s and a prediction time of 8 ms a subsequent study ni et al 2020 complemented wang s work identical dataset by adding two deep learning dl models to the benchmark results showed a significant leap in performance with deep neural networks dnns compared to classic ml models for a learning phase of 406 3 s and a prediction time of 36 ms in the aforementioned works dl has a better performance compared to classic ml techniques in fitting atmospheric dispersion data however the used datasets are associated with simple flat terrains whose properties are completely different from those in urban areas the learning problem is therefore simpler and its feasibility cannot generalize to complex urban geometries few studies assessed the performance of ai applied to fluid mechanics in presence of turbulence lauret et al 2014 used the same combination of cellular automata with anns to solve the ade lauret et al 2016 but this time with a new consideration the impact of a simple obstacle on the turbulent diffusion coefficient this parameter is learned and estimated by another ann prior to solving the ade the obtained results showed that the proposed model runs 1 5 times faster than cfd calculations and the diffusion coefficient is fairly predicted the main drawback here is the assumption that the wind velocity field is a precomputed via cfd input of the ann yet transport simulation requires a significant amount of time for example for fluent 6 3 99 of the time is dedicated to wind field computation and 1 for the dispersion vendel 2011 in a study entitled hidden fluid mechanics raissi et al 2018 a dnn was trained to predict the concentration and latent quantities such as velocity and pressure by including into the cost function residual errors from the conservation equations mass and momentum a showcase of 2 d and 3 d experiments revealed the capacity of dl techniques to capture with high accuracy the turbulence caused by various obstacles however the algorithm or alternatively the discovery of partial differential equations affected by boundary conditions has to be trained for every new terrain making it highly unpractical given the geometrical diversity of urban domains 1 3 contributions and organization to the best of our knowledge there is a serious lack of ml based surrogate models that provide a fast and reliable assessment of pollution incidents in urban areas regarding this limitation our main contributions are the following 1 we propose a novel learning framework called mcxm to estimate the concentration level of pollutants released in urban areas in case of an emergency event such surrogate model is provided with actual meteorological and topographical data to infer the pollutant dispersion 2 unlike purely data centric approaches in dl we propose a physically interpretable design of our learning framework mainly two key stages are considered a masking straightforward enforcement of the spatial constraints generated by the urban topology on the airflow b correction modelling of the effects due to the obstacles buildings and progressive estimation of the pollutant distribution 3 we propose to use synthetic data produced by a realistic 3d multi scale atmospheric dispersion simulator pmss for model training and testing specifically the simulations are configured to cover several wind conditions and source locations in the cities of grenoble and paris france the rest of the paper is organized as follows the proposed learning approach is provided in section 2 in section 3 we explain how the training dataset is generated and give details about the test procedure section 4 discusses the obtained results highlighting the benefits and limitations of the proposed framework finally conclusions are drawn in section 5 2 methods 2 1 problem scope the aim of this paper is to provide a ml based surrogate model of atmospheric transport and dispersion that is able to quickly and accurately predict the pollutant s concentration dose released by accident in an urban area the studied area is a neighborhood of a surface up to few squared kilometers the pollutants doses are computed over an integration time window of maximum a couple of hours therefore we consider only small scale turbulence in the dispersion phenomena due to mechanical phenomena for the most part zannetti 1990 mechanical turbulence comes from surface rugosity mainly of the obstacles and manifests as a deflection and or a vortex circulation of the airflow at the interface of obstacles hunter et al 1992 we summarize the previous considerations in the three following hypotheses to characterize the dispersion conditions hypothesis 1 the release conditions consist in an instantaneous emission of the pollutant from a single point source x 0 y 0 z 0 at human height hypothesis 2 the weather conditions are restricted to the wind speed and direction over the urban canopy hypothesis 3 the environment consists of obstacles buildings of different heights and shapes located inside the urban area within these hypotheses we propose in the sequel a learning approach based on dl to forecast the integrated pollutant concentration on a 2 d section at the same height z 0 as the emission source a brief overview of dl is provided before describing the learning workflow 2 2 introduction to dl a dnn is a non linear statistical data modelling system that transforms a set of inputs into a set of outputs through multiple layers sequence of computations the goal of dnns is to infer from data the underlying phenomena that process the inputs values into the resulting outputs for instance they are used for pattern recognition and regression in various fields such as physics health and finance in a dnn the predictor variables are assigned to the input layer l 0 fig 1 illustrates the operations performed in all subsequent layers each output of a given layer l i is the result of applying a non linear function called activation to a linear combination of l i s inputs which are also the outputs of the preceding layer l i 1 thanks to the universal approximation property hornik et al 1989 this structure is able to fit any non linear function by adjusting the weights and offsets of the linear combinations in the network the second property that allows dnns to learn efficiently is the principle of parsimony parsimonious models are presumably simple models with great explanatory predictive power they explain data with a minimum number of predictor variables hypotheses by limiting the number of input variables we avoid building complex models that ultimately lead to overfitting the learning process consists in calibrating the parameters of the network weights w li and biases b l i to decrease the model s error the error function represents a distance metric between the model s output and observational data in the learning phase the parameters are updated by applying a gradient descent algorithm using the training set the latter step called back propagation uses automatic differentiation to compute the gradients of the error function with respect to the weights and biases of the dnn automatic differentiation relies on combining the derivatives of the constituent operations through the chain rule that gives the derivative of the overall composition enabling accurate evaluation at machine precision baydin et al 2018 after each training cycle the dnn model is validated on a disjoined set of data validation set lastly the performance of the model is evaluated on another set test set never used during training or validation the validation step is necessary to adjust the complexity of the dnn that is the number of layers neurons and connections called synapses between neurons of consecutive layers vapnik 1999 introduced the notion of model capacity which conceptually represents the space of functions the dnn can fit increasing the number of layers and or their neural density enables it to fit more complex non linear transformations however the obtained function may model the random noise in the training data rather than the governing principles this problem called overfitting results in building models that explain well the data at hand but fail in out of sample predictions on the other hand dnns with low capacity are impractical to solve complex tasks and tend to underfit this trade off is also known in statistics as the bias variance trade off both overfitting and underfitting models lack the ability to forecast the correct output 2 3 mcxm framework 2 3 1 overview fig 2 represents an example of the learning approach namely mcxm composed of a sequence of four masking and three correction stages that we explain later that incrementally build the pollution field through multiple non linear transformations the input data are as follows the wind s speed ν and direction θ above the urban canopy and the 2 d binary map of the urban area denoted by m c that encodes the presence of obstacles as 0 and the fluid zones as 1 note that this is a 2 d section at height z 0 of a 3 d building map the output is a prediction of a 2 d integrated concentration map at the source height z 0 over a window 0 t starting from the instant of release up to a duration t note that the position of the emission source varies from one experiment to the other for example possible source locations are represented by red dots in fig 4 and fig 5 in spite of that the source location is considered at the center of the computation domain at every stage of our approach with this assumption the source coordinates are not needed as input parameters thereby simplifying the learning model the counterpart is a pre processing effort of the urban area maps and the associated concentration fields to center them on the location of the source further details in section 3 2 we are in presence of a multivariate regression task with two heterogeneous input data types a column vector and a matrix a straightforward approach to data fusion would be to vectorize the urban area map and concatenate it with the wind vector this results in mixing high level physical parameters with the local spatial representation of the urban terrain preliminary experiments showed that this solution performs poorly as heterogeneous data sources require different processing an alternative relies on creating two separate processing streams that construct after a succession of learned operations mergeable data representations zhang et al 2018 tatarchenko et al 2015 audebert et al 2017 this approach offers better performances but increases the learning complexity besides in our case it is clear that constructing a two dimensional array based only on two wind scalars will lead to a poor and redundant representation instead we propose to build a physics informed representation from the meteorological data using the gaussian plume model 2 3 2 gaussian plume model the gaussian plume model describes the three dimensional concentration field generated by a point source under stationary meteorological conditions the dispersion is considered here in an ideal flat environment principally caused by the airflow transport given x the co linear axis with the wind direction vector at angle θ the gaussian plume formula is as follows 1 c x y z 0 t q 2 π σ x σ y exp x v t 2 2 σ x 2 exp y 2 2 σ y 2 where c x y z 0 t is the concentration at x y z 0 and instant t due to the emission at x 0 y 0 z 0 q is the emission rate v is the wind speed and σ x σ y are the horizontal standard deviations of the spatial gaussian plume distribution which are computed following pasquill turner correlation formula σ x σ y a x b c hanna et al 1982 the numerical expression of these parameters is given is section 3 2 thereafter we calculate the integrated concentration over a time window of duration t for the 2 d section at the elevation of the source z z 0 2 c i x y z 0 0 t c x y z 0 t d t the obtained cartography of the concentration doses noted u is so far unaffected by the urban geometry 2 3 3 masking and corrections the masking stage accounts for some physical obstacles of the urban area it operates an element wise matrix multiplication between the binary map m c and a pollutant distribution u x to zero the concentration level where obstacles are located 3 u xm u x m c where u x u u mc 1 u mc 2 u mc 3 the first masking applied to the gaussian plume u is a way to merge the heterogeneous input data wind scalars and 2 d urban map by restricting the pollution to the fluid zones encoded in m c subsequent masking stages are applied to the corrected concentration fields we explain this below with the same objective of enforcing the spatial constraints associated to the urban area however the masking process itself does not capture any physical interaction between the airflow and the buildings such as trajectory change and mechanical turbulence furthermore it introduces many non physical artifacts and abrupt discontinuities in the concentration field e g see u m and u mc 1 in fig 2 the correction stages are introduced specifically to mitigate these inconsistencies in the correction stage the physics related to the transport and dispersion of the pollutant are approximated it operates learned non linear transformations on masked pollutant distributions u m u mc 1 m and u mc 2 m and outputs their corrected counterparts u mc 1 u mc 2 and u mc 3 respectively one consequence of the correction stage is the modification of the area reached by the pollutant it is mainly due to the buildings deflecting the airflow that transports the pollutant outside of its initial range this observation is important for two reasons first the pollutant distribution likely requires several iterations of correction before reaching its final extent second there is a need for an additional masking after each correction to apply spatial constraints to the newly polluted areas the latter update is necessary because the binary masking only identifies obstacles within the reach of the pollutant that becomes obsolete as the pollutant distribution extends after every correction stage we end up with a succession of masking and correction stages that progressively model the impact of different obstacles on the expanding polluted areas hence the architecture s name mcxm where the number after x stands for the length of the masking m correction c sequence when using the mcxm framework each correction stage can consist of a separate dnn but requires a joint training during the learning phase i e the weights and biases have to be tuned together for the sake of consistency and interpretability we propose to share the same weights and biases between the correction dnns in all stages as a way to have them model universal physics of atmospheric transport and dispersion this is also a regularization technique to reduce the computational complexity an example of architecture for the correction dnn is given hereafter and the length of the masking and correction sequence will be motivated in section 4 2 3 4 example of a correction dnn when dealing with 2 d data most common learning architectures either use image processing operations such as convolutions or process encoded data in less complex subspaces before recasting them into the original space for the sake of illustration we will use an encoder decoder dnn represented in fig 3 to learn the correction operations this is not necessarily the optimal architecture but only a practical example more generally the proposed approach has the advantage of being interoperable with various non linear ml models the encoder block starts with a vectorization of a m m matrix with m 100 for example it creates a considerable amount of features m 2 10 000 to be processed the following hidden layers use significantly fewer features 512 units per layer which will be motivated in section 4 by transforming them into compact high level representations in the hidden layers of the encoder and decoder blocks we use the rectified linear unit relu as non linear activation function agarap 2018 as mentioned before stacking multiple hidden layers is what makes dl approximate complex non linear transformations however if deeper networks are more powerful they also suffer from vanishing or exploding gradients problems that seriously limit the learning performance glorot and bengio 2010 the use of relu instead of saturated functions like sigmoid partially mitigates this problem additionally we rely on batch normalization ioffe and szegedy 2015 and he initialization henormal he et al 2015 that achieve and maintain comparable variances of incident and outgoing weights throughout the encoder dnn the batch normalization layers are placed before and after each of the four hidden layers of the encoder network finally the decoder reconstructs the data in the original space through a full connected layer of m 2 features rearranged in a matrix m m 3 experiment settings study case of hazardous pollutant release in french urban areas 3 1 synthetic data and cfd simulation the dataset in dl is of a vital importance a dnn mainly finds recurring patterns on the training data to build a governing relationship between the input and output variables the more situations are covered the more powerful the inference capability the model will have in case of transport and dispersion modelling data can be collected from 1 real size experiments or 2 small scale experiments in wind tunnels the former solution is expensive data acquisition devices release systems gases to be released workforce and site availability for the experiment and needs long acquisition campaigns to capture natural cycles e g day night and seasons moreover it is impossible to control the weather conditions the latter alternative is not only costly but also suffers from scaling adjustments since the experiments are usually carried out in a 1 50 scale the database can also be generated from cfd calculations given that most sophisticated models provide a very high accuracy in the simulation environment the weather characteristics the properties of the chemical release and the geometry of the terrain can be configured with no constraints synthetic data have been used in many studies bolón canedo et al 2013 ayturan et al 2018 and are extremely efficient to train ai systems we use parallel micro swift spray pmss as the numerical modelling system of transport and dispersion it is a parallel space and time decomposition version of micro swift spray composed of micro swift an analytically modified mass consistent interpolator over complex terrain given topography meteorological data and buildings a mass consistent 3 d wind field is generated it is also able to derive diagnostic turbulence parameters to be used by micro spray inside the flow zones modified by obstacles micro spray a lagrangian particle dispersion model able to take into account the presence of obstacles the dispersion of a pollutant is simulated following the trajectories of a large number of fictitious particles the trajectories are obtained by integrating in time the particle velocity which is the sum of a transport component defined by the local averaged wind provided generally by micro swift and a stochastic component standing for the dispersion due to the atmospheric turbulence pmss modelling system has been thoroughly validated against wind tunnel and full scale experimental results oldrini et al 2017 castelli et al 2018 the parallelism was shown to be very efficient from a multicore laptop up to clusters with several hundreds or thousands of cores in the case of a high performance computing center oldrini et al 2019 armand et al 2021 in terms of computational time to give an order of magnitude considering a spatial domain of 1 km3 with a horizontal and vertical close to the ground resolution of 2 m pmss computational time is around 5 min for the flow and 10 more minutes for tracking a plume for 15 min using an optimal number of cores on a standard work station 3 2 pollution data we have selected two cities to experiment the mcxm framework grenoble and paris the learning phase training and validation of the mcxm is performed on synthetic pollution data in grenoble s city center while the generalization capabilities test are challenged in the district of opera in paris paris and grenoble are two french cities characterized by a typical european architecture where the majority of buildings are 20m 25m tall and date from the 19th and 20th centuries their urban density measured as the building square footage divided by land area is nearly similar 57 for grenoble s city center versus 41 for the opera district in paris the buildings in grenoble center are slightly more scattered than in opera see figs 4 and 5 overall the selected urban areas are comparable but still exhibit some architectural differences this enables to show how adaptive the mcxm is in predicting pollution exposure in new environments the configuration of pmss for training validation and test is summarized in table 1 and described in details in the following paragraphs the training and validation set 80 and 20 respectively is composed of 14 796 instances of integrated concentration over t 2 h generated by pmss for the following configuration as shown in fig 4 the urban area that constitutes the computation domain in pmss is the center of the french city grenoble located in the bounding box x y 913301 914301 6457391 6458391 expressed in lambert 93 coordinate reference system it is an 500 500 shaped grid with a space resolution of 2m 274 hypothetical different locations of the point source are considered 54 stationary weather conditions are considered built from a combination of 18 values of wind direction θ 0 20 40 340 and 3 values of the wind speed v m s 1 1 5 3 6 we assume neutral atmospheric stability conditions pasquill s class d to compute the semi empiric standard deviations σ x and σ y of the gaussian plume such that a 0 068 b 0 908 and c 0 for each experiment a unique point source is considered to produce an instantaneous emission of a unit mass of the pollutant gas or particulate matter for a given initialization source location and wind conditions pmss simulates the wind and dispersion fields for 2 h other unspecified attributes such as temperature or air pressure are assumed to be constant similarly the test set is composed of 11 988 instances generated with the same logic given the following configuration the computation domain of pmss is the opera district of paris located in the bounding box x y 650250 6863050 651450 6864050 see fig 5 it is an 600 500 shaped grid with a space resolution of 2m 222 hypothetical different locations of the point source same 54 stationary weather conditions used for grenoble as mentioned before the proposed learning procedure considers integrated concentration maps centered on the release source it is a way to simplify the model by reducing the dimension of the input space source coordinates are not input variables therefore we process the data to extract bounding boxes of dimensions 400m 400m centered on the source one of the consequences is the elimination of all source points that fall less than 200m from the borders 274 92 sources in grenoble and 220 102 in opera the training validation set is reduced to 4968 instances and the test set to 5508 afterwards the training validation data are augmented by rotation with respect to three angles 90 180 and 270 these rotations preserve the shape of the used square arrays so that no padding is required with this the training validation set counts approximately 20 000 instances 4 results and discussion all of the following results were obtained by using during the training phase the mean squared error mse as loss function and rmsprop tieleman hintonet al 2012 as the optimization algorithm with a batch size of 16 additionally two regularization strategies were used to make learning tractable and improve the generalization error 1 exponential decay of the learning rate we start the gradient descent algorithm with a higher learning rate to accelerate the training and avoid spurious local minima that is decreased in an exponential fashion to help the network converge to a local minimum and avoid oscillation 2 early stopping the validation error with respect to the training epoch is a u shaped curve assuming a sufficient representational capacity of the model to overfit during the training phase we stop the training when the validation error reaches a minimum achieving the best tradeoff between underfitting and overfitting 4 1 hyper parameters tuning as mentioned before the learning capacity of a dnn is directly connected to its hyper parameters a poor tuning leads either to underfitting or overfitting both reducing the generalization performance of the trained model we propose a straightforward approach based on a grid search to select the number of occurrence and the composition of the correction block s the goal here is not an exhaustive search of the optimal hyper parameters but just to tune them enough to demonstrate the feasibility of the proposed model first we want to find a suitable hyper parameter setting for the correction block introduced in section 2 the dnn is trained for different combinations of hidden layers count and their neural density after convergence the mse is evaluated on the validation dataset normalized with respect to the highest obtained error compiling the results in fig 6 we observe that the lowest mse is achieved by an encoder composed of four hidden layers each containing 512 neurons compared to the considered configurations this encoder configuration yields the best estimator of pollution concentration subsequent to a point hazardous release secondly we investigate what impact the length of the masking correction sequence has on the prediction quality recall that the succession of correction and masking stages enables to gradually construct the integrated concentration field that accounts for the urban topography fig 7 illustrates the normalized mse with respect to the highest error on the grenoble s validation dataset with eight mcxm architectures of different masking correction sequence lengths note that each correction operation is built with the hyper parameters selected previously i e an encoder composed of four hidden layers each containing 512 neurons the lowest error is obtained when using a sequence of three consecutive masking and correction stages 1 1 this sequence ends necessarily with a masking stage even if it is not shown in fig 7 in the remainder of the paper we consider the mcx3m model whose three correction blocks are identical constituted of four hidden layers and 512 neurons 4 2 performance the mcx3m model is trained on 16 000 synthetic examples of accidental pollution dispersion in the city of grenoble the wind direction θ and speed v are input variables used to compute the time integrated gaussian plume the latter represents the concentrations field as if no obstacles were hindering the dispersion of the pollutant as a reference for upcoming comparisons we consider the gaussian model as a baseline the proposed mcx3m has to achieve at least better performance than a simplistic gaussian plume and approximate the turbulence dynamic introduced by the urban geometry fig 8 represents different prediction stages of the trained mcx3m in a new area of grenoble not encountered during the training phase the gaussian plume u is narrow and assumes a flat terrain the first masking u m applies the spatial constraints on the support of the gaussian plume while the correction u mc 1 increases its dispersion after the second masking u mc 1 m we can clearly see that the emission source is located in a street intersection the next correction stage u mc 2 splits the pollutant mass into four directions with a higher concentration into the streets in the south east and north east the dispersion range is again extended reaching new areas where the masking u mc 2 m will reveal new intersections to the following correction stage u mc 3 and so on the three correction stages progressively alter the concentration field such that their normalized average relative deviations 2 2 the normalized average relative deviation from the gaussian plume of each correction stage n is 1 n r o w s n c o l s i j n r o w s n c o l s u mc n i j u i j u i j where n r o w s n c o l s are the dimensions of the concentration field matrices u i j is the gaussian plume value at coordinates i j and u mc n i j is the output of correction stage n at coordinates i j from the gaussian plume are 57 96 and 105 respectively note that there is a slower deviation rate of the correction in the last stage compared to the first ones hinting for a convergence process at the end this iterative process yields a good approximation of the actual concentration field as simulated by pmss considered as ground truth now we evaluate the generalization capacity of the mcx3m model by computing the prediction error in the city of paris the obtained prediction mse averaged over the whole testset is 2 3 times lower than the mse of the baseline method gaussian model consequently the consecutive masking and correction stages of the mcx3m drastically improve the precision of prediction in new urban areas in fig 9 we illustrate four examples of predicted by the mcx3m model and simulated by pmss considered as the ground truth integrated concentrations fields over 2 h each example corresponds to a hypothetical accidental release by a single point source in a specific location in paris because of the complex topography of urban areas the airflow can be very turbulent in particular the interactions between the buildings and the airflow as well as the effect of street intersections that change the trajectory of the pollutant are accurately modeled by the trained model the total pollutant mass deviation 3 3 the total pollutant mass m t o t a l is computed by integrating the concentration field over the surface of the 400m 400m bounding box centered on the source the total pollutant mass deviation of the prediction relatively to the simulation is m p r e d i c t i o n t o t a l m s i m u l a t i o n t o t a l m s i m u l a t i o n t o t a l of the predictions relatively to the simulations are 2 12 1 and 17 for examples a b c and d respectively the small values indicate a good approximation of the total pollutant mass by the mcx3m additionally the pollutant mass distribution throughout the whole domain is faithfully reproduced by the predictions the learning model can therefore successfully determine the high risk areas risky concentration levels of the pollutant that require evacuation for example besides the mcx3m is extremely fast as the computation duration per prediction is 0 75 ms on a latitude le5490 laptop intel core i5 16 gbytes ram the prediction is virtually instantaneous compared to cfd simulations even with pmss which provides a simplified cfd solution in a short time of 15 min for comparable spatial domains in addition to the accuracy of the prediction the mcx3m speed is adapted for emergency planning in the event of a hazardous release especially if the model has to be run thousands of times for uncertainty evaluation 4 3 limitations and perspectives based on the previous results the proposed mcxm framework has shown promising speed and accuracy for modelling transport and dispersion of pollution in urban areas however we have only demonstrated such performance in a specific set of hypotheses so far first the weather conditions over the urban canopy are considered stationary for the time required by the plume to cross the simulation domain which is not true in practice secondly the mcxm is necessarily sub optimal because it approaches a 3 d problem based on 2 d data 2 d section of an urban map a 2 d section of the city map at the input of the mcxm does not represent the vertical urban geometry therefore the vertical transport and dispersion phenomena over or across the buidings and obstacles cannot be correctly approximated by the learning model as an example a closed 2 d contour such as a private courtyard at the considered height section should unequivocally be inaccessible to the pollutant from the 2 d perspective of the learning model however we observed in many mcxm predictions that such areas are still reachable by the air flow in reality the learning model does not rely on any physical justification to such behavior but only on the statistical likelihood because it learned from 2 d sections of 3 d concentration fields simulated by pmss in that case if the model s prediction is accurate it is certainty due to overfitting or randomness rather than a successful approximation of the vertical transport and dispersion dynamics besides we have trained and tested the mcxm in urban areas characterized by an almost flat natural topography no hills mountains or canyons the vertical dispersion would otherwise have an even higher impact that is not embedded in the proposed 2 d model to address these limitations we plan in future works to extend the mcxm framework by 1 accounting for the mass conservation during the masking and correction stages 2 considering time changing weather conditions over the urban canopy 3 integrating atmosphere stability conditions into the model and the simulations 4 jointly considering multiple sections of the urban area s 3 d geometry in order to enable 3 d predictions of the pollution concentration fields and 5 experimenting our framework with a larger variety of topographies 5 conclusion in this paper we propose an atmospheric transport and dispersion surrogate learning model as a fast and accurate decision making tool during hazardous pollution events in urban areas the mcxm learning framework starts by approximating the initial concentration field as a gaussian plume the urban geometry is introduced during the masking operation followed by a correction stage that applies the transport and dispersion physics accordingly to the perceived spatial constraints these two stages are then iteratively repeated until the final pollution map is obtained for the sake of illustration the correction consists of an encoder decoder dnn whose parameters were fine tuned based on a grid search we note however that the mcxm framework is also compatible with other dnn architectures such as convnets the data used in this work are synthetic generated by the pmss modelling system pmss is constituted by the parallel versions of the swift 3d diagnostic flow model and the spray 3d lagrangian particle dispersion model the training validation dataset approx 20 000 instances is produced in the french city of grenoble under different weather and emission conditions once the learning phase is completed the prediction error of the mcx3m was averaged over 5000 instances of hypothetical accidental release events in a district of paris whose urban geometry was never encountered during the training the learned model exhibits a good accuracy and virtually instantaneous prediction time 0 75 ms compared to usual cfd simulators the actual mcxm forecasts a 2 d horizontal concentration field at the height of the pollution source in future works we plan to fully utilize the 3 d simulations of pmss to teach the proposed learning framework how to jointly predict the 3 d horizontal and vertical distributions of the pollutant also we plan to extend the actual framework to consider rigorous mass conservation more complex weather and atmospheric stability conditions and a larger variety of urban natural and mixed terrains software and or data availability section the findings of this study are obtained using a research software developed by the authors it is written in python 3 7 and uses tensorfow 2 2 library to implement the proposed deep learning based surrogate model the codebase is about 1000 lines covering data processing and the mcxm framework the learning phase is operated on a 32 gbytes nvidia quadro m4000 gpu the trained model is deployed and runs on a latitude le5490 laptop intel core i5 16 gbytes ram the software is a property of the french alternative energies and atomic energy commission cea and is not currently shared publicly raw data for model training validation and test consist of ascii files of total size 60 gbytes each file encodes the integrated pollutant concentration field as a matrix whose vertical and horizontal axes correspond to the geographic coordinates more details in section 3 they are generated by pmss a 3d multi scale flow and dispersion modelling system dedicated to the complex atmospheric environment it is a commercial software developed for fifteen years by the french alternative energies and atomic energy commission and by aria technologies all data supporting the findings of this study are available from the corresponding author on request declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors would like to acknowledge the financial support of the cross disciplinary program on numerical simulation of cea the french alternative energies and atomic energy commission 
25594,today computational fluid dynamics approaches have a high level of spatial temporal accuracy in modelling atmospheric transport and dispersion in very complex environments several numerical models require however heavy computational resources and prolonged simulation time up to several days this time constraint is specifically crucial for intervention planning in case of accidental or malevolent toxic releases in a city in this paper we propose to use synthetic data generated by a realistic 3 d transport dispersion simulator to train a learning framework called mcxm the latter relies on a sequence of masking and correction operations to progressively apply the spatial constraints and underlying physics of transport and dispersion the learning phase uses the urban geometry of the french city grenoble we then test the effectiveness of the trained mcxm in a different french city paris the results show that the mcxm s forecasts are virtually instantaneous and generalize successfully to unseen conditions graphical abstract image 1 keywords deep learning hazardous pollutant dispersion simulation surrogate model synthetic data abbreviations ade advection diffusion equation ai artificial intelligence ann artificial neural network cfd computational fluid dynamics dl deep learning dnn deep neural network ml machine learning mse mean squared error pmss parallel micro swift spray 1 introduction 1 1 context air pollutants are substances of different natures that raise many issues for the population being exposed to such pollutants can provoke several effects from minor discomforts e g bad odor to serious health problems such as cancer lung infection and birth defects particularly accidental or hostile emissions of contaminants are highly dangerous events that necessitate an immediate intervention strategy from the authorities to protect the population and limit the material and environmental casualties for this purpose dispersion modelling is used to forecast the concentration of a pollutant at different distances and directions from the sources the models can compare exposures to some selected benchmark such as a state pollution standard or a level with a known health effect to provide recommendations to decision makers e g us legislation united states environmental protection agency for example in an emergency planning study lindell 1995 it was noted that hazardous plumes emanating from a waste incinerator in the usa could arrive in populated areas before people could evacuate or while they are evacuating a shelter in place strategy would be recommended instead sorensen 2004 state of the art models of air pollution transport and dispersion which belong to computational fluid dynamics cfd family are characterized by large memory needs and computational complexity they usually require high performance hardware and several hours of runtime zannetti 1990 accidental pollutant release would ideally require an immediate response from the authorities to contain any potential hazard for the population and the environment in this regard the runtime overhead of atmospheric transport and dispersion simulations may hinder them from being consistent with decision making under time constraints especially if such simulations have to be executed several times for sensitivity or uncertainty evaluations 1 2 related work in the past decade many researchers explored the idea of leveraging machine learning ml to predict atmospheric pollution aguilera et al 2011 zhang et al 2020 araujo et al 2020 this trend stems from the ability of artificial intelligence ai to approximate extremely complex systems from observational data compared to cfd a trained artificial neural network ann is generally time efficient and less demanding in terms of computational resources these properties make it a suitable solution for predicting the exposure to a hazardous release where the assessment time frame is a serious restriction a comprehensive review cabaneros et al 2019 listed recent research activities on air quality with recommendations on how to build anns for predicting long term air pollution regarding short term pollution events lauret et al 2016 solved the advection diffusion equation ade using cellular automata whose transition function is learned by an ann the wind velocity field is however considered known in advance as an input variable in another study wang et al compared two ml models using the prairie grass dataset which refers to a typical hazardous gas release with low stack emission in a flat terrain wang et al 2018 with 20 meteorological inputs the selected neural network architecture two hidden layers of 38 and 4 neurons respectively showed the best performance for a learning phase of 4 4 s and a prediction time of 8 ms a subsequent study ni et al 2020 complemented wang s work identical dataset by adding two deep learning dl models to the benchmark results showed a significant leap in performance with deep neural networks dnns compared to classic ml models for a learning phase of 406 3 s and a prediction time of 36 ms in the aforementioned works dl has a better performance compared to classic ml techniques in fitting atmospheric dispersion data however the used datasets are associated with simple flat terrains whose properties are completely different from those in urban areas the learning problem is therefore simpler and its feasibility cannot generalize to complex urban geometries few studies assessed the performance of ai applied to fluid mechanics in presence of turbulence lauret et al 2014 used the same combination of cellular automata with anns to solve the ade lauret et al 2016 but this time with a new consideration the impact of a simple obstacle on the turbulent diffusion coefficient this parameter is learned and estimated by another ann prior to solving the ade the obtained results showed that the proposed model runs 1 5 times faster than cfd calculations and the diffusion coefficient is fairly predicted the main drawback here is the assumption that the wind velocity field is a precomputed via cfd input of the ann yet transport simulation requires a significant amount of time for example for fluent 6 3 99 of the time is dedicated to wind field computation and 1 for the dispersion vendel 2011 in a study entitled hidden fluid mechanics raissi et al 2018 a dnn was trained to predict the concentration and latent quantities such as velocity and pressure by including into the cost function residual errors from the conservation equations mass and momentum a showcase of 2 d and 3 d experiments revealed the capacity of dl techniques to capture with high accuracy the turbulence caused by various obstacles however the algorithm or alternatively the discovery of partial differential equations affected by boundary conditions has to be trained for every new terrain making it highly unpractical given the geometrical diversity of urban domains 1 3 contributions and organization to the best of our knowledge there is a serious lack of ml based surrogate models that provide a fast and reliable assessment of pollution incidents in urban areas regarding this limitation our main contributions are the following 1 we propose a novel learning framework called mcxm to estimate the concentration level of pollutants released in urban areas in case of an emergency event such surrogate model is provided with actual meteorological and topographical data to infer the pollutant dispersion 2 unlike purely data centric approaches in dl we propose a physically interpretable design of our learning framework mainly two key stages are considered a masking straightforward enforcement of the spatial constraints generated by the urban topology on the airflow b correction modelling of the effects due to the obstacles buildings and progressive estimation of the pollutant distribution 3 we propose to use synthetic data produced by a realistic 3d multi scale atmospheric dispersion simulator pmss for model training and testing specifically the simulations are configured to cover several wind conditions and source locations in the cities of grenoble and paris france the rest of the paper is organized as follows the proposed learning approach is provided in section 2 in section 3 we explain how the training dataset is generated and give details about the test procedure section 4 discusses the obtained results highlighting the benefits and limitations of the proposed framework finally conclusions are drawn in section 5 2 methods 2 1 problem scope the aim of this paper is to provide a ml based surrogate model of atmospheric transport and dispersion that is able to quickly and accurately predict the pollutant s concentration dose released by accident in an urban area the studied area is a neighborhood of a surface up to few squared kilometers the pollutants doses are computed over an integration time window of maximum a couple of hours therefore we consider only small scale turbulence in the dispersion phenomena due to mechanical phenomena for the most part zannetti 1990 mechanical turbulence comes from surface rugosity mainly of the obstacles and manifests as a deflection and or a vortex circulation of the airflow at the interface of obstacles hunter et al 1992 we summarize the previous considerations in the three following hypotheses to characterize the dispersion conditions hypothesis 1 the release conditions consist in an instantaneous emission of the pollutant from a single point source x 0 y 0 z 0 at human height hypothesis 2 the weather conditions are restricted to the wind speed and direction over the urban canopy hypothesis 3 the environment consists of obstacles buildings of different heights and shapes located inside the urban area within these hypotheses we propose in the sequel a learning approach based on dl to forecast the integrated pollutant concentration on a 2 d section at the same height z 0 as the emission source a brief overview of dl is provided before describing the learning workflow 2 2 introduction to dl a dnn is a non linear statistical data modelling system that transforms a set of inputs into a set of outputs through multiple layers sequence of computations the goal of dnns is to infer from data the underlying phenomena that process the inputs values into the resulting outputs for instance they are used for pattern recognition and regression in various fields such as physics health and finance in a dnn the predictor variables are assigned to the input layer l 0 fig 1 illustrates the operations performed in all subsequent layers each output of a given layer l i is the result of applying a non linear function called activation to a linear combination of l i s inputs which are also the outputs of the preceding layer l i 1 thanks to the universal approximation property hornik et al 1989 this structure is able to fit any non linear function by adjusting the weights and offsets of the linear combinations in the network the second property that allows dnns to learn efficiently is the principle of parsimony parsimonious models are presumably simple models with great explanatory predictive power they explain data with a minimum number of predictor variables hypotheses by limiting the number of input variables we avoid building complex models that ultimately lead to overfitting the learning process consists in calibrating the parameters of the network weights w li and biases b l i to decrease the model s error the error function represents a distance metric between the model s output and observational data in the learning phase the parameters are updated by applying a gradient descent algorithm using the training set the latter step called back propagation uses automatic differentiation to compute the gradients of the error function with respect to the weights and biases of the dnn automatic differentiation relies on combining the derivatives of the constituent operations through the chain rule that gives the derivative of the overall composition enabling accurate evaluation at machine precision baydin et al 2018 after each training cycle the dnn model is validated on a disjoined set of data validation set lastly the performance of the model is evaluated on another set test set never used during training or validation the validation step is necessary to adjust the complexity of the dnn that is the number of layers neurons and connections called synapses between neurons of consecutive layers vapnik 1999 introduced the notion of model capacity which conceptually represents the space of functions the dnn can fit increasing the number of layers and or their neural density enables it to fit more complex non linear transformations however the obtained function may model the random noise in the training data rather than the governing principles this problem called overfitting results in building models that explain well the data at hand but fail in out of sample predictions on the other hand dnns with low capacity are impractical to solve complex tasks and tend to underfit this trade off is also known in statistics as the bias variance trade off both overfitting and underfitting models lack the ability to forecast the correct output 2 3 mcxm framework 2 3 1 overview fig 2 represents an example of the learning approach namely mcxm composed of a sequence of four masking and three correction stages that we explain later that incrementally build the pollution field through multiple non linear transformations the input data are as follows the wind s speed ν and direction θ above the urban canopy and the 2 d binary map of the urban area denoted by m c that encodes the presence of obstacles as 0 and the fluid zones as 1 note that this is a 2 d section at height z 0 of a 3 d building map the output is a prediction of a 2 d integrated concentration map at the source height z 0 over a window 0 t starting from the instant of release up to a duration t note that the position of the emission source varies from one experiment to the other for example possible source locations are represented by red dots in fig 4 and fig 5 in spite of that the source location is considered at the center of the computation domain at every stage of our approach with this assumption the source coordinates are not needed as input parameters thereby simplifying the learning model the counterpart is a pre processing effort of the urban area maps and the associated concentration fields to center them on the location of the source further details in section 3 2 we are in presence of a multivariate regression task with two heterogeneous input data types a column vector and a matrix a straightforward approach to data fusion would be to vectorize the urban area map and concatenate it with the wind vector this results in mixing high level physical parameters with the local spatial representation of the urban terrain preliminary experiments showed that this solution performs poorly as heterogeneous data sources require different processing an alternative relies on creating two separate processing streams that construct after a succession of learned operations mergeable data representations zhang et al 2018 tatarchenko et al 2015 audebert et al 2017 this approach offers better performances but increases the learning complexity besides in our case it is clear that constructing a two dimensional array based only on two wind scalars will lead to a poor and redundant representation instead we propose to build a physics informed representation from the meteorological data using the gaussian plume model 2 3 2 gaussian plume model the gaussian plume model describes the three dimensional concentration field generated by a point source under stationary meteorological conditions the dispersion is considered here in an ideal flat environment principally caused by the airflow transport given x the co linear axis with the wind direction vector at angle θ the gaussian plume formula is as follows 1 c x y z 0 t q 2 π σ x σ y exp x v t 2 2 σ x 2 exp y 2 2 σ y 2 where c x y z 0 t is the concentration at x y z 0 and instant t due to the emission at x 0 y 0 z 0 q is the emission rate v is the wind speed and σ x σ y are the horizontal standard deviations of the spatial gaussian plume distribution which are computed following pasquill turner correlation formula σ x σ y a x b c hanna et al 1982 the numerical expression of these parameters is given is section 3 2 thereafter we calculate the integrated concentration over a time window of duration t for the 2 d section at the elevation of the source z z 0 2 c i x y z 0 0 t c x y z 0 t d t the obtained cartography of the concentration doses noted u is so far unaffected by the urban geometry 2 3 3 masking and corrections the masking stage accounts for some physical obstacles of the urban area it operates an element wise matrix multiplication between the binary map m c and a pollutant distribution u x to zero the concentration level where obstacles are located 3 u xm u x m c where u x u u mc 1 u mc 2 u mc 3 the first masking applied to the gaussian plume u is a way to merge the heterogeneous input data wind scalars and 2 d urban map by restricting the pollution to the fluid zones encoded in m c subsequent masking stages are applied to the corrected concentration fields we explain this below with the same objective of enforcing the spatial constraints associated to the urban area however the masking process itself does not capture any physical interaction between the airflow and the buildings such as trajectory change and mechanical turbulence furthermore it introduces many non physical artifacts and abrupt discontinuities in the concentration field e g see u m and u mc 1 in fig 2 the correction stages are introduced specifically to mitigate these inconsistencies in the correction stage the physics related to the transport and dispersion of the pollutant are approximated it operates learned non linear transformations on masked pollutant distributions u m u mc 1 m and u mc 2 m and outputs their corrected counterparts u mc 1 u mc 2 and u mc 3 respectively one consequence of the correction stage is the modification of the area reached by the pollutant it is mainly due to the buildings deflecting the airflow that transports the pollutant outside of its initial range this observation is important for two reasons first the pollutant distribution likely requires several iterations of correction before reaching its final extent second there is a need for an additional masking after each correction to apply spatial constraints to the newly polluted areas the latter update is necessary because the binary masking only identifies obstacles within the reach of the pollutant that becomes obsolete as the pollutant distribution extends after every correction stage we end up with a succession of masking and correction stages that progressively model the impact of different obstacles on the expanding polluted areas hence the architecture s name mcxm where the number after x stands for the length of the masking m correction c sequence when using the mcxm framework each correction stage can consist of a separate dnn but requires a joint training during the learning phase i e the weights and biases have to be tuned together for the sake of consistency and interpretability we propose to share the same weights and biases between the correction dnns in all stages as a way to have them model universal physics of atmospheric transport and dispersion this is also a regularization technique to reduce the computational complexity an example of architecture for the correction dnn is given hereafter and the length of the masking and correction sequence will be motivated in section 4 2 3 4 example of a correction dnn when dealing with 2 d data most common learning architectures either use image processing operations such as convolutions or process encoded data in less complex subspaces before recasting them into the original space for the sake of illustration we will use an encoder decoder dnn represented in fig 3 to learn the correction operations this is not necessarily the optimal architecture but only a practical example more generally the proposed approach has the advantage of being interoperable with various non linear ml models the encoder block starts with a vectorization of a m m matrix with m 100 for example it creates a considerable amount of features m 2 10 000 to be processed the following hidden layers use significantly fewer features 512 units per layer which will be motivated in section 4 by transforming them into compact high level representations in the hidden layers of the encoder and decoder blocks we use the rectified linear unit relu as non linear activation function agarap 2018 as mentioned before stacking multiple hidden layers is what makes dl approximate complex non linear transformations however if deeper networks are more powerful they also suffer from vanishing or exploding gradients problems that seriously limit the learning performance glorot and bengio 2010 the use of relu instead of saturated functions like sigmoid partially mitigates this problem additionally we rely on batch normalization ioffe and szegedy 2015 and he initialization henormal he et al 2015 that achieve and maintain comparable variances of incident and outgoing weights throughout the encoder dnn the batch normalization layers are placed before and after each of the four hidden layers of the encoder network finally the decoder reconstructs the data in the original space through a full connected layer of m 2 features rearranged in a matrix m m 3 experiment settings study case of hazardous pollutant release in french urban areas 3 1 synthetic data and cfd simulation the dataset in dl is of a vital importance a dnn mainly finds recurring patterns on the training data to build a governing relationship between the input and output variables the more situations are covered the more powerful the inference capability the model will have in case of transport and dispersion modelling data can be collected from 1 real size experiments or 2 small scale experiments in wind tunnels the former solution is expensive data acquisition devices release systems gases to be released workforce and site availability for the experiment and needs long acquisition campaigns to capture natural cycles e g day night and seasons moreover it is impossible to control the weather conditions the latter alternative is not only costly but also suffers from scaling adjustments since the experiments are usually carried out in a 1 50 scale the database can also be generated from cfd calculations given that most sophisticated models provide a very high accuracy in the simulation environment the weather characteristics the properties of the chemical release and the geometry of the terrain can be configured with no constraints synthetic data have been used in many studies bolón canedo et al 2013 ayturan et al 2018 and are extremely efficient to train ai systems we use parallel micro swift spray pmss as the numerical modelling system of transport and dispersion it is a parallel space and time decomposition version of micro swift spray composed of micro swift an analytically modified mass consistent interpolator over complex terrain given topography meteorological data and buildings a mass consistent 3 d wind field is generated it is also able to derive diagnostic turbulence parameters to be used by micro spray inside the flow zones modified by obstacles micro spray a lagrangian particle dispersion model able to take into account the presence of obstacles the dispersion of a pollutant is simulated following the trajectories of a large number of fictitious particles the trajectories are obtained by integrating in time the particle velocity which is the sum of a transport component defined by the local averaged wind provided generally by micro swift and a stochastic component standing for the dispersion due to the atmospheric turbulence pmss modelling system has been thoroughly validated against wind tunnel and full scale experimental results oldrini et al 2017 castelli et al 2018 the parallelism was shown to be very efficient from a multicore laptop up to clusters with several hundreds or thousands of cores in the case of a high performance computing center oldrini et al 2019 armand et al 2021 in terms of computational time to give an order of magnitude considering a spatial domain of 1 km3 with a horizontal and vertical close to the ground resolution of 2 m pmss computational time is around 5 min for the flow and 10 more minutes for tracking a plume for 15 min using an optimal number of cores on a standard work station 3 2 pollution data we have selected two cities to experiment the mcxm framework grenoble and paris the learning phase training and validation of the mcxm is performed on synthetic pollution data in grenoble s city center while the generalization capabilities test are challenged in the district of opera in paris paris and grenoble are two french cities characterized by a typical european architecture where the majority of buildings are 20m 25m tall and date from the 19th and 20th centuries their urban density measured as the building square footage divided by land area is nearly similar 57 for grenoble s city center versus 41 for the opera district in paris the buildings in grenoble center are slightly more scattered than in opera see figs 4 and 5 overall the selected urban areas are comparable but still exhibit some architectural differences this enables to show how adaptive the mcxm is in predicting pollution exposure in new environments the configuration of pmss for training validation and test is summarized in table 1 and described in details in the following paragraphs the training and validation set 80 and 20 respectively is composed of 14 796 instances of integrated concentration over t 2 h generated by pmss for the following configuration as shown in fig 4 the urban area that constitutes the computation domain in pmss is the center of the french city grenoble located in the bounding box x y 913301 914301 6457391 6458391 expressed in lambert 93 coordinate reference system it is an 500 500 shaped grid with a space resolution of 2m 274 hypothetical different locations of the point source are considered 54 stationary weather conditions are considered built from a combination of 18 values of wind direction θ 0 20 40 340 and 3 values of the wind speed v m s 1 1 5 3 6 we assume neutral atmospheric stability conditions pasquill s class d to compute the semi empiric standard deviations σ x and σ y of the gaussian plume such that a 0 068 b 0 908 and c 0 for each experiment a unique point source is considered to produce an instantaneous emission of a unit mass of the pollutant gas or particulate matter for a given initialization source location and wind conditions pmss simulates the wind and dispersion fields for 2 h other unspecified attributes such as temperature or air pressure are assumed to be constant similarly the test set is composed of 11 988 instances generated with the same logic given the following configuration the computation domain of pmss is the opera district of paris located in the bounding box x y 650250 6863050 651450 6864050 see fig 5 it is an 600 500 shaped grid with a space resolution of 2m 222 hypothetical different locations of the point source same 54 stationary weather conditions used for grenoble as mentioned before the proposed learning procedure considers integrated concentration maps centered on the release source it is a way to simplify the model by reducing the dimension of the input space source coordinates are not input variables therefore we process the data to extract bounding boxes of dimensions 400m 400m centered on the source one of the consequences is the elimination of all source points that fall less than 200m from the borders 274 92 sources in grenoble and 220 102 in opera the training validation set is reduced to 4968 instances and the test set to 5508 afterwards the training validation data are augmented by rotation with respect to three angles 90 180 and 270 these rotations preserve the shape of the used square arrays so that no padding is required with this the training validation set counts approximately 20 000 instances 4 results and discussion all of the following results were obtained by using during the training phase the mean squared error mse as loss function and rmsprop tieleman hintonet al 2012 as the optimization algorithm with a batch size of 16 additionally two regularization strategies were used to make learning tractable and improve the generalization error 1 exponential decay of the learning rate we start the gradient descent algorithm with a higher learning rate to accelerate the training and avoid spurious local minima that is decreased in an exponential fashion to help the network converge to a local minimum and avoid oscillation 2 early stopping the validation error with respect to the training epoch is a u shaped curve assuming a sufficient representational capacity of the model to overfit during the training phase we stop the training when the validation error reaches a minimum achieving the best tradeoff between underfitting and overfitting 4 1 hyper parameters tuning as mentioned before the learning capacity of a dnn is directly connected to its hyper parameters a poor tuning leads either to underfitting or overfitting both reducing the generalization performance of the trained model we propose a straightforward approach based on a grid search to select the number of occurrence and the composition of the correction block s the goal here is not an exhaustive search of the optimal hyper parameters but just to tune them enough to demonstrate the feasibility of the proposed model first we want to find a suitable hyper parameter setting for the correction block introduced in section 2 the dnn is trained for different combinations of hidden layers count and their neural density after convergence the mse is evaluated on the validation dataset normalized with respect to the highest obtained error compiling the results in fig 6 we observe that the lowest mse is achieved by an encoder composed of four hidden layers each containing 512 neurons compared to the considered configurations this encoder configuration yields the best estimator of pollution concentration subsequent to a point hazardous release secondly we investigate what impact the length of the masking correction sequence has on the prediction quality recall that the succession of correction and masking stages enables to gradually construct the integrated concentration field that accounts for the urban topography fig 7 illustrates the normalized mse with respect to the highest error on the grenoble s validation dataset with eight mcxm architectures of different masking correction sequence lengths note that each correction operation is built with the hyper parameters selected previously i e an encoder composed of four hidden layers each containing 512 neurons the lowest error is obtained when using a sequence of three consecutive masking and correction stages 1 1 this sequence ends necessarily with a masking stage even if it is not shown in fig 7 in the remainder of the paper we consider the mcx3m model whose three correction blocks are identical constituted of four hidden layers and 512 neurons 4 2 performance the mcx3m model is trained on 16 000 synthetic examples of accidental pollution dispersion in the city of grenoble the wind direction θ and speed v are input variables used to compute the time integrated gaussian plume the latter represents the concentrations field as if no obstacles were hindering the dispersion of the pollutant as a reference for upcoming comparisons we consider the gaussian model as a baseline the proposed mcx3m has to achieve at least better performance than a simplistic gaussian plume and approximate the turbulence dynamic introduced by the urban geometry fig 8 represents different prediction stages of the trained mcx3m in a new area of grenoble not encountered during the training phase the gaussian plume u is narrow and assumes a flat terrain the first masking u m applies the spatial constraints on the support of the gaussian plume while the correction u mc 1 increases its dispersion after the second masking u mc 1 m we can clearly see that the emission source is located in a street intersection the next correction stage u mc 2 splits the pollutant mass into four directions with a higher concentration into the streets in the south east and north east the dispersion range is again extended reaching new areas where the masking u mc 2 m will reveal new intersections to the following correction stage u mc 3 and so on the three correction stages progressively alter the concentration field such that their normalized average relative deviations 2 2 the normalized average relative deviation from the gaussian plume of each correction stage n is 1 n r o w s n c o l s i j n r o w s n c o l s u mc n i j u i j u i j where n r o w s n c o l s are the dimensions of the concentration field matrices u i j is the gaussian plume value at coordinates i j and u mc n i j is the output of correction stage n at coordinates i j from the gaussian plume are 57 96 and 105 respectively note that there is a slower deviation rate of the correction in the last stage compared to the first ones hinting for a convergence process at the end this iterative process yields a good approximation of the actual concentration field as simulated by pmss considered as ground truth now we evaluate the generalization capacity of the mcx3m model by computing the prediction error in the city of paris the obtained prediction mse averaged over the whole testset is 2 3 times lower than the mse of the baseline method gaussian model consequently the consecutive masking and correction stages of the mcx3m drastically improve the precision of prediction in new urban areas in fig 9 we illustrate four examples of predicted by the mcx3m model and simulated by pmss considered as the ground truth integrated concentrations fields over 2 h each example corresponds to a hypothetical accidental release by a single point source in a specific location in paris because of the complex topography of urban areas the airflow can be very turbulent in particular the interactions between the buildings and the airflow as well as the effect of street intersections that change the trajectory of the pollutant are accurately modeled by the trained model the total pollutant mass deviation 3 3 the total pollutant mass m t o t a l is computed by integrating the concentration field over the surface of the 400m 400m bounding box centered on the source the total pollutant mass deviation of the prediction relatively to the simulation is m p r e d i c t i o n t o t a l m s i m u l a t i o n t o t a l m s i m u l a t i o n t o t a l of the predictions relatively to the simulations are 2 12 1 and 17 for examples a b c and d respectively the small values indicate a good approximation of the total pollutant mass by the mcx3m additionally the pollutant mass distribution throughout the whole domain is faithfully reproduced by the predictions the learning model can therefore successfully determine the high risk areas risky concentration levels of the pollutant that require evacuation for example besides the mcx3m is extremely fast as the computation duration per prediction is 0 75 ms on a latitude le5490 laptop intel core i5 16 gbytes ram the prediction is virtually instantaneous compared to cfd simulations even with pmss which provides a simplified cfd solution in a short time of 15 min for comparable spatial domains in addition to the accuracy of the prediction the mcx3m speed is adapted for emergency planning in the event of a hazardous release especially if the model has to be run thousands of times for uncertainty evaluation 4 3 limitations and perspectives based on the previous results the proposed mcxm framework has shown promising speed and accuracy for modelling transport and dispersion of pollution in urban areas however we have only demonstrated such performance in a specific set of hypotheses so far first the weather conditions over the urban canopy are considered stationary for the time required by the plume to cross the simulation domain which is not true in practice secondly the mcxm is necessarily sub optimal because it approaches a 3 d problem based on 2 d data 2 d section of an urban map a 2 d section of the city map at the input of the mcxm does not represent the vertical urban geometry therefore the vertical transport and dispersion phenomena over or across the buidings and obstacles cannot be correctly approximated by the learning model as an example a closed 2 d contour such as a private courtyard at the considered height section should unequivocally be inaccessible to the pollutant from the 2 d perspective of the learning model however we observed in many mcxm predictions that such areas are still reachable by the air flow in reality the learning model does not rely on any physical justification to such behavior but only on the statistical likelihood because it learned from 2 d sections of 3 d concentration fields simulated by pmss in that case if the model s prediction is accurate it is certainty due to overfitting or randomness rather than a successful approximation of the vertical transport and dispersion dynamics besides we have trained and tested the mcxm in urban areas characterized by an almost flat natural topography no hills mountains or canyons the vertical dispersion would otherwise have an even higher impact that is not embedded in the proposed 2 d model to address these limitations we plan in future works to extend the mcxm framework by 1 accounting for the mass conservation during the masking and correction stages 2 considering time changing weather conditions over the urban canopy 3 integrating atmosphere stability conditions into the model and the simulations 4 jointly considering multiple sections of the urban area s 3 d geometry in order to enable 3 d predictions of the pollution concentration fields and 5 experimenting our framework with a larger variety of topographies 5 conclusion in this paper we propose an atmospheric transport and dispersion surrogate learning model as a fast and accurate decision making tool during hazardous pollution events in urban areas the mcxm learning framework starts by approximating the initial concentration field as a gaussian plume the urban geometry is introduced during the masking operation followed by a correction stage that applies the transport and dispersion physics accordingly to the perceived spatial constraints these two stages are then iteratively repeated until the final pollution map is obtained for the sake of illustration the correction consists of an encoder decoder dnn whose parameters were fine tuned based on a grid search we note however that the mcxm framework is also compatible with other dnn architectures such as convnets the data used in this work are synthetic generated by the pmss modelling system pmss is constituted by the parallel versions of the swift 3d diagnostic flow model and the spray 3d lagrangian particle dispersion model the training validation dataset approx 20 000 instances is produced in the french city of grenoble under different weather and emission conditions once the learning phase is completed the prediction error of the mcx3m was averaged over 5000 instances of hypothetical accidental release events in a district of paris whose urban geometry was never encountered during the training the learned model exhibits a good accuracy and virtually instantaneous prediction time 0 75 ms compared to usual cfd simulators the actual mcxm forecasts a 2 d horizontal concentration field at the height of the pollution source in future works we plan to fully utilize the 3 d simulations of pmss to teach the proposed learning framework how to jointly predict the 3 d horizontal and vertical distributions of the pollutant also we plan to extend the actual framework to consider rigorous mass conservation more complex weather and atmospheric stability conditions and a larger variety of urban natural and mixed terrains software and or data availability section the findings of this study are obtained using a research software developed by the authors it is written in python 3 7 and uses tensorfow 2 2 library to implement the proposed deep learning based surrogate model the codebase is about 1000 lines covering data processing and the mcxm framework the learning phase is operated on a 32 gbytes nvidia quadro m4000 gpu the trained model is deployed and runs on a latitude le5490 laptop intel core i5 16 gbytes ram the software is a property of the french alternative energies and atomic energy commission cea and is not currently shared publicly raw data for model training validation and test consist of ascii files of total size 60 gbytes each file encodes the integrated pollutant concentration field as a matrix whose vertical and horizontal axes correspond to the geographic coordinates more details in section 3 they are generated by pmss a 3d multi scale flow and dispersion modelling system dedicated to the complex atmospheric environment it is a commercial software developed for fifteen years by the french alternative energies and atomic energy commission and by aria technologies all data supporting the findings of this study are available from the corresponding author on request declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors would like to acknowledge the financial support of the cross disciplinary program on numerical simulation of cea the french alternative energies and atomic energy commission 
