index,text
23760,we propose the development of a multigrid open source operational forecast system with the oceanic region off rio de janeiro state as our first case study the system is composed by three nested roms domains employing mercator ocean at the lateral open boundaries of the outermost grid and gfs global forecast system at the surface open boundary of all grids two schemes were built for the operational forecast workflow free without data assimilation and assimilated with data assimilation for evaluation we performed hindcast simulations using mercator and era5 as input data for different parts of the southeastern brazilian oceanic region covering various regimes of the unique brazil current system the modelled data was compared against velocity components from three oceanic buoys sst from mur ultra high resolution sea surface temperature and ssh from aviso the comparison against the buoys showed a good representation of the velocity field by roms performing better than mercator for most scenarios roms also presented good performances for sst and ssh for the three nested forecast system the velocity fields presented comparable magnitudes against mercator without the presence of jet like instabilities indicating a stable system we also implemented the i4d var method in a 2 day analysis for the simulation of 7 day forecasts finding an improvement in the modelled forecasts initialized from the assimilated analysis for both velocity components with an overall reduction in the rmse and bias and an increase in correlation keywords computational modelling operational forecast system roms data availability data will be made available on request 1 introduction the development of an operational ocean forecast system and its important role for human activities including search and rescue at sea oil spill modelling and plankton transport have been described in a variety of studies breivik et al 2013 röhrs et al 2018 strand et al 2017 robust systems are successfully implemented for oceanic and coastal regions of the united states europe australia and japan in brazil most operational efforts on oceanic and regional modelling were executed within the scope of a research and development consortium called oceanographic modelling and observation network with the portuguese acronym remo marta almeida et al 2011 and the project azul dos santos et al 2015 remo currently delivers for the oceanic region off brazil hindcasts and assimilated forecasts employing the hycom hybrid coordinate ocean mode model in the resolutions of 1 4 1 12 and 1 24 project azul started in 2012 for the offshore area of santos basin and is based on three components observation network including gliders argo floats and svp surface velocity program drifters computational modeling and data assimilation in the phase i the first 4dvar assimilation system in a brazilian regional model was implemented for a hindcast improving the representation of the challenging mesoscale oceanographic features of the santos basin fragoso et al 2016 project azul is currently in the phase ii delivering operational forecasts at 1 12 with data assimilation and 1 36 grids focused on the offshore area of santos basin http www projetoazul eco br so far brazil still lacks an operational forecast system covering regional and shelf regions as the ones implemented for california https www cencoos org observations models forecasts moore et al 2013 peters and bokhorst 2001 delaware bay https tidesandcurrents noaa gov ofs dbofs dbofs html new jersey and new york shelf http www myroms org espresso and the mediterranean https www socib es seccion modelling facility forecast socib tintoré et al 2013 in common all these systems employ the regional ocean modeling system roms nested into a global circulation model at the outermost grid in this context following the best practices in the ocean forecast field we propose the development of a multigrid high resolution operational ocean forecast system off rio de janeiro state rj as part of a bigger project to be extended for other brazilian regions the shelf and oceanic region off rj are paramount for the economy of brazil as it presents major port structures and oil gas reserves this project main goal is modelling the ocean circulation by means of an open source operational forecast system considering limited computation resources to allow a wide range application of the system the codes for interpolation of the input data grid configuration ingestion of oceanic and atmospheric data workflow of the forecast system and creation of the observation file for data assimilation were developed with open source languages including python shell script grads and octave they are freely available for the scientific community at the following github page https github com fernandotcbarreto roms the brazil current bc is unique among subtropical western boundary currents with increasing vertical extension from its origin site around 15 s towards the confluence region with the malvinas current it goes from 200 m to 400 500 m at 20 s with the addition of the south atlantic central water sacw and to about 1200 m at 25 s with the addition of the antarctic intermediate water aaiw schmid et al 2000 silveira et al 2004 soutelino et al 2013 the modification of the bc as it flows poleward motivated the first part of the evaluation performed in this paper in which three experiments were established to test the modelled velocity field along three regions of the brazilian coast vitória 19 55 s and 39 41 w cabo frio 23 37 s and 42 12 w and itajaí 27 24 s and 47 15 w covering different regimes of the brazil current system in these experiments we performed hindcast simulations using on the lateral open boundaries data from mercator reanalysis cmems copernicus marine service phy 001 030 and on the surface data from era5 fifth generation ecmwf reanalysis the results were compared against u and v velocity components from the brazilian national buoy programme pnboia marinha do brasil 2017 in a second part of the evaluation results from the multigrid operational system were compared against the data of bacia de campos buoy part of pnboia to assess the improvement in 7 day forecasts initialized from analysis assimilated with the incremental strong constraint 4d variational method i4d var this paper describes the operational forecast system and presents an evaluation for the implemented experiments the paper is organized as follows the description of the evaluation set up and the development of the forecast system are presented in section 2 the results of the evaluation are presented and discussed in section 3 conclusions and future developments are presented in section 4 2 materials and methods 2 1 the ocean model the numerical model implemented in the operational system was the regional ocean modelling system roms 3 8 a free surface terrain following sigma coordinate primitive equation ocean model which is advantageous for regional applications shchepetkin and mcwilliams 2003 2005 the air sea fluxes for momentum and heat are calculated according to the bulk fluxes parameterization wind minus current in stress fairall et al 2003 vertical mixing is parameterized with the generic length scale gls umlauf and burchard 2003 implementation of the k kl turbulence closure mellor and yamada 1982 the simulation is configured to conserve volume with a free surface condition chapman 1985 a condition for the 2d momentum flather 1976 and a radiation boundary condition with nudging for the 3d momentum and tracers marchesiello et al 2001 the nudging timescales present a configuration of 1 and 30 days for the inflow and outflow respectively roms was chosen due to its advanced and robust rapidly evolving community code model being employed in a great number of forecast systems hirose et al 2019 mourre et al 2018 powell et al 2009 röhrs et al 2018 in the following sections will be described the system of three nested grids implemented for the operational modelling section 2 2 in section 2 3 1 the three domains established for the evaluation of the system will be described in section 2 3 2 the experiments for the evaluation of the assimilated scheme are presented all experiments are summarized in table 1 2 2 operational modelling set up a system composed by three nested grids was established with the innermost grid centered in the rio de janeiro state fig 1 the outermost grid is initialized and forced on the lateral open boundaries with data from the mercator analysis cmems global ocean 1 12 physics analysis and forecast updated daily chune et al 2021 and on the surface with data from ncep gfs 3 hourly average global forecast system at a base horizontal resolution of 28 kilometers gfs 2015 for mercator it is considered the time centered at 12 00 as in the original data for the outermost grid we employed the same grid used in fragoso et al 2016 as it was successfully applied to a regional ocean observing system in southeastern brazilian with the incremental strong constraint 4d variational i4d var method this grid will be named primary grid from now on and covers the domain from 50 w to 25 w and from 15 s to 30 s with a horizontal grid resolution of 1 12 the numerical grid presents three open boundaries in the model domain northern southern and eastern boundaries while the western is bounded by the coast the resolution of 1 12 for the outermost grid was chosen to minimize the numerical instabilities arising from the different bathymetry of mercator and the primary grid in the interpolation points the two inner grids nest 1 and nest 2 were rotated to maximize the sea land proportion and present the resolutions of 1 36 and 1 72 nest 1 covers the domain from 46 72 w to 37 58 w and from 18 53 s to 27 16 s and nest 2 from 44 91 w to 39 80 w and from 20 73 s to 25 30 s the positions of nest 1 and nest 2 were defined based on the availability of high resolution bathymetry the nesting method used in the system was based on the one way nesting technique described in marchesiello et al 2001 and mason et al 2010 the bacia de campos buoy indicated in fig 1 will be employed in the validation of the assimilated operational forecast system the baroclinic time steps for the grids with resolutions 1 12 1 36 and 1 72 are 300 s 150 s and 50 s respectively for the number of barotropic time steps between each baroclinic time step it was employed 30 10 and 10 respectively the same value of 5 m s2 for the harmonic horizontal viscosity coefficient was used for all grids for the vertical discretization it was employed the same configuration in the three grids 40 layers and an enhancement of the resolution at the surface using the surface and bottom stretching parameters θ s 5 0 and θ b 2 and the critical depth parameter t c l i n e 100 m the same minimum depth of 1 m was employed for all grids to avoid model instability and or spurious deep currents the final masked bathymetry of all three grids was smoothed to fulfil a requirement of the roms slope the rx0 factor beckmann and haidvogel 1993 the two child s grid bathymetry was modified so that it is equal to the parent one at the open boundaries linearly converging to the original high resolution bathymetry as in mason et al 2010 for all grids the target was a rx0 between 0 3 and 0 35 to reach the optimum balance between grid stability and the integrity of bathymetry we point out that more refined grids reach lower values of rx0 more stable with less modification of the bathymetry the interpolation and extrapolation of data from mercator to the roms grid and between roms grids for nesting were performed following the works of mason et al 2010 and marta almeida et al 2019 to ensure the conservation of volume the input data are initially horizontally interpolated for the roms grid with the vertical interpolation resolved for each node in the next step the extrapolation consisting of replicating the data closest to the missing node nan is applied in the two interpolation steps horizontally and vertically to the following three regions above the uppermost input data layer below the bottommost input data layer and on the coast within the landmask of the input data the codes for interpolation and extrapolation are available at the following github page https github com fernandotcbarreto roms tree master boundary ini the system does not have tides nor rivers and they are expected to be implemented soon for this reason as no calculations were performed to quantify the influence of tides and rivers in the study area we must consider this limitation in the current stage of the operational system two schemes were built for the operational forecast workflow free without data assimilation and assimilated with the data assimilation method i4d var in fig 2 is shown the general workflow in which we obtain the free scheme excluding the n day analysis this strategy was used to account for limited computational resources since i4d var requires a cluster or cloud computing as observational data for assimilation we employed sea surface temperature sst data from mur chin et al 2017 and altimetry data from aviso aviso 1996 the free scheme was based in the works of costa et al 2012 and juza et al 2016 and involves resolving two systems a hindcast spin up that runs at a specified frequency working as a restart and a forecast that runs daily the hindcast n days is forced on the surface by the gfs and uses mercator as initial and lateral boundary conditions roms linearly interpolate in time the surface and boundary conditions to match the time step the forecast system n days uses the mercator to build the boundary conditions and the roms data from the previous run to build the initial conditions there is the option to activate the nudging process on a strip of grid cells next to the boundaries linearly converging to 0 no nudging beyond this strip to minimize instabilities generated by mismatches between initial condition and boundaries and to keep the boundaries stable along the simulation the downside of nudging is the increase in running time if the previous run is the hindcast the last available time step is used in case of the forecast it uses the time step corresponding to the start day of the new run 1 day after the start day of the previous run this workflow is repeated for all nested grids the restart strategy was employed to ensure that the roms solution stays close to the real evolution of the ocean in scenarios where data assimilation is not resolved costa et al 2012 the frequency to perform the restart depends on the quality of input data and on the stability of the model bathymetry smoothing has an important role the codes for implementing this scheme are available at the following github https github com fernandotcbarreto roms tree master operational based on the works of mourre et al 2018 and hirose et al 2019 we developed the assimilated scheme presented in fig 2 the main difference between the two schemes is an analysis run a n day analysis where the data assimilation method i4d var is performed the forecast is then initialized using the last time step of this analysis as the initial condition simulating n days in the future in a 2 day assimilated analysis for instance the analysis starts from the midpoint of the running time in the previous one leading to an overlap of 50 in the incremental strong constraint 4d variational method i4d var the increments to the model state vector x t t s ζ u v t at a time t comprised of all prognostic variables potential temperature t salinity s sea surface displacement ζ and horizontal velocity components u and v is estimated and subject to surface forcing conditions f t for freshwater fluxes heat momentum and lateral open boundary conditions b t that minimizes in a least squared sense the difference between the model and observations li et al 2015 de paula et al 2021 the parameters for the data assimilation performed in this work are presented in section 2 3 2 for more details of i4d var consult moore et al 2011a b the codes for implementing this scheme are available at the following github page https github com fernandotcbarreto roms tree master operational assimilation 2 3 evaluation set up 2 3 1 hindcasts in order to evaluate the whole system including input data and model parameters we established three domains summarized in table 1 along the eastern brazilian coast fig 3 these domains aim to cover the modifications of the brazilian current system bc as it flows poleward with great changes in vertical structure volume transport and energy balance the domains involved different grids depending on the results obtained with the primary grid and the location of the buoy data used for evaluation for the buoy site located in southern brazil itajaí buoy for instance due to its location close to the boundary of the primary grid a new coarse grid with resolution of 1 12 was implemented named parent grid a nested grid was implemented when poor quality results were obtained with the coarse grid for all domains the same parameters were employed 40 layers and an enhancement of the resolution at the surface using the surface and bottom stretching parameters θ s 5 0 and θ b 2 and the critical depth parameter t c l i n e 100 m the minimum depth was of 1 m for the coarse grid 1 12 the time step was of 300 s and for the nested grid 1 36 of 150 s the first domain fig 3 a is composed by the single primary grid the same outermost grid presented in last section and was run for the year long period of 2016 with 2 months of spin up in november and december of 2015 using at boundaries data from mercator reanalysis drévillon et al 2021 and era5 hersbach et al 2018 this experiment was run for a year aiming the construction of new assimilation files in future works the u and v velocity components were compared against the near surface and bottom available current data from the vitória buoy located at 19 55 s e 39 41 w part of the brazilian national buoy programme pnboia marinha do brasil 2017 the velocity comparison was performed between 15 04 2016 and 15 06 2016 based on the availability of the vitória buoy data pnboia velocity data extends from the depth of 5 5 m to 75 5 m with a cell of 3 5 m since this domain presented highly satisfactory results against this buoy to be presented in next section and is outside the region of interest a nesting was not performed in this experiment as the primary grid covers the entire eastern brazilian coast the results from the year long period of 2016 were also compared against mur and aviso to evaluate the fields of sea surface temperature sst and sea surface height ssh respectively and against argo float to evaluate the vertical profiles of temperature and salinity the mur analysis chin et al 2017 presents a 1 km resolution and is produced as a retrospective data set four day latency and as a near real time data set one day latency at the jet propulsion laboratory jpl podaac using wavelets as basis functions in an optimal interpolation oi approach aviso is a 0 1 resolution gridded composite of the advanced very high resolution radiometer avhrr from the polar operational environmental satellites poes project aviso 1996 argo argo 2000 is an international program that collects information from inside the ocean using a fleet of robotic instruments that drift with the ocean currents and move up and down between the surface and a mid water level a grid with resolution 1 36 was nested to the previous grid creating a two grid nested domain fig 3 b centered in the rio de janeiro state this domain was executed from 01 09 2017 to 01 11 2017 employing a two month spin up 01 07 2017 to 01 09 2017 with the results compared against the u and v velocity components of the cabo frio buoy pnboia located at 23 37 s and 42 12 w this date was selected based on the availability of cabo frio buoy despite the analysis of the domain averaged kinetic energy showing a fast warm up in less than 5 days we chose two months to make sure that all the results evaluated has the minimum influence from the initial condition mercator a second domain composed by two nested grids with resolutions 1 12 and 1 36 was implemented for the southern region of brazil fig 3 c this system was executed from 01 11 2017 to 01 01 2018 employing a two month spin up 01 09 2017 to 01 11 2017 with results compared against the u and v velocity components of the itajaí buoy pnboia located at 27 24 s and 47 15 w this date was selected based on the availability of itajaí buoy the analysis for itajaí buoy required a new system of nested grids as this buoy is located too close to the southern boundary of primary grid the quality of the modelled results relative to the observations were evaluated with basic statistic such as the pearson s correlation coefficient the root mean square error rmse the quadratic scoring rule which measures the root average quadratic distance between the model and observations and bias these scores complement each other and give different statistical information about the errors while the pearson coefficient measures the linear correlation between two time series the rmse measures the spread of the distribution of the differences between the model and observations giving information about how wide is its 2 3 2 operational forecast and data assimilation systems for the operational forecast system presented in section 2 2 two experiments were performed to evaluate the results table 1 in the first experiment the system was executed with the free scheme for two months from 26 09 2020 to 26 11 2020 in serial model at a cpu 2 40ghz with 12 gb ram laptop the goal of this experiment was to evaluate the feasibility of executing the operational multigrid system without a cluster structure or cloud computing in this case the system performed in a daily basis 7 day forecasts of the three nested grids forced by mercator analysis and ncep gfs with each forecast taking approximately 4 h for conclusion to evaluate the stability of the roms solution inside the domain and close to the boundaries the surface velocity of the last day of simulation was compared against mercator in order to evaluate the improvement with the assimilated scheme it was executed on a weekly basis performing 8 cycles of 7 day forecasts initialized with 2 day assimilated analysis spanning from 08 02 2022 to 05 04 2022 due to availability of the bacia de campos buoy bacia de campos is a recent deployed buoy from pnboia located at 22 48 8 s and 40 06 2 w and delivered current velocity at three surface layers until 05 04 2022 when it suffered severe damage the forecast started at 08 02 2022 with the initial condition provided from a 2 day assimilated analysis which used as observational data for assimilation sea surface temperature sst from mur and altimetry data from aviso the next forecast started at the end of the previous simulation with the initial condition provided from the assimilated analysis of the previous 2 days this workflow performed 8 x was executed in cloud computation with a configuration of 32 cpus and 128 ram the data assimilation was only executed in the primary grid employing the same parameters and preprocessing from fragoso et al 2016 it was used a 100 km horizontal decorrelation length scale for sea surface height temperature salinity and all surface forcing variables and a 60 km length scale for momentum in the vertical a 100 m decorrelation scale was used for all variables the observation error was of 2 cm and 0 4 for ssh and sst respectively the experiments were performed using ten inner loops and two outer loops and took approximately 2 h to execute the 2 day assimilated analysis and 10 min the 7 day forecast the simulations with data assimilation were run with the free scheme to evaluate the improvements from the assimilated scheme with the 2 way analysis executed without data assimilation control run as the position of the bacia de campos buoy is close to the boundary of nest 2 higher resolution grid the statistical parameters were calculated for the nest 1 grid with the adcp cell closest to the surface 5 5 m used in the comparisons 3 results and discussion 3 1 evaluating the velocity components the modification of the brazil current bc as it flows poleward motivated the test of the modelling system for the buoys vitória cabo frio and itajaí as they cover different regimes of the bc the vitória buoy was moored at a depth of 200 m in a region with baroclinic mesoscale activity resultant from the strong vertical shear of the bc with the intermediate western boundary current iwbc soutelino et al 2013 in the region of the cabo frio buoy moored at 200 m besides the baroclinic mesoscale activity from the strong vertical shear of bc iwbc system mano et al 2009 silveira et al 2008 the dynamics is also modified by a change in coastal direction from north south to east west around 23 s which also promotes upwelling of deep south atlantic central water valentin 2001 resulting in a highly complex region the site of the itajaí buoy moored at 200 m is much less studied compared to the other two due to the distance from the pre salt oil reserves of the campos and santos basin in this region the bc is composed by the tropical water tw south atlantic central water sacw antarctic intermediate water aaiw and north atlantic deep water nadw flowing towards the malvinas confluence the u and v velocity components from roms were compared against the in situ data for two months at the depths of 5 5 m and 75 5 m top and bottom depths of pnboia data the modelled data are initially linear interpolated to the in situ higher frequency data to avoid loss of information a 12 hour low pass filter butterworth is then applied to the u and v time series to remove outliers and smooth out high frequency variations the mercator data was also compared against roms and pnboia since mercator assimilates real time ocean measurements from satellite and in situ observations it is a great dataset to use as quality control the pearson s correlation coefficient rmse and bias were calculated to evaluate the modelled data and are presented in table 2 for the three buoys 3 1 1 vitória buoy the modelled data from roms grid 1 12 primary grid and mercator are plotted against vitória buoy in figs 4 and 5 roms and mercator presented high correlations for all comparisons 0 85 with roms performing slightest better this can be confirmed in the plots with the models reproducing the main peaks there was no significant difference between surface and bottom values for correlation coefficients for rmse the worst value was observed for mercator v component at depth 5 5 m 0 1612 m s and the best for roms u component at depth 75 5 m 0 0781 both roms and mercator improved their rmse values at 75 5 m roms improved around 23 for u and 28 for v regarding bias both roms and mercator underestimated the velocity field since roms performed very well for the vitória buoy and this site is outside the region of interest for the operational system rj state it was not implemented a nested grid for this domain 3 1 2 cabo frio buoy the modelled data from roms 1 12 primary grid roms 1 36 and mercator are plotted against cabo frio buoy in figs 6 and 7 at 5 5 m mercator 0 4096 and roms primary 0 3887 presented better correlation for v and roms nest for u 0 5506 however as observed in the plots the models failed to represent the main peaks in the v component while the u component was well represented by the roms nest with exception of the feature between 2016 10 08 and 2016 10 13 for rmse roms nest performed well for both u and v with values smaller than 0 13 m s while mercator and roms primary presented values close to 0 2 m s for the u component for comparison in their work for the santos basin fragoso et al 2016 calculated the rmse with a minimum value around 0 2 m s for both free and assimilated run at 75 5 m mercator improved the correlation for v in 43 to 0 5877 and roms nest for u in 30 to 0 7206 roms primary performed worse in this depth again no clear correlation can be observed from the plots of v component for neither model for rmse the nested roms improved slightly for both u and v in overall mercator presented a better correlation for u component and roms nest for v however when calculating a depth average of these correlations we found a greater value for roms 0 63 against 0 5 for mercator besides the depth average of rmse for u and v are 0 20 and 0 08 m s respectively for mercator and 0 12 and 0 11 m s respectively for roms indicating a better representation of the pnboia data by the roms nest from the value of bias we observe that both mercator and roms primary and nest had the tendency to underestimate the velocity field 3 1 3 itajaí buoy the modelled data from roms 1 12 roms 1 36 and mercator are plotted against itajaí buoy in figs 8 and 9 at 5 5 m both roms and mercator presented greater correlations for v with the nested domain performing 60 better than mercator 0 8528 against 0 5314 and the parent performing 26 worse 0 3896 against 0 5314 the reason the nested domain performed better than mercator even though the parent performed worse is probably related to an oceanographic process better represented in a higher resolution grid as a submesoscale feature for u component all domains presented low correlations as can be confirmed in the plots with most of the peaks not represented in the simulation regarding rmse all cases presented values around 0 1 indicating a stable solution with roms nest performing best for both u 0 0926 m s and v 0 0852 m s at 75 5 m mercator and roms parent presented low correlations with the nested domain performing well for v 0 8309 and regular for u 0 4875 in the work of costa et al 2018 for the northwestern iberian coast correlations around 0 5 were deemed as decent despite the high correlation value for the nested domain in fig 9 we can observe a significant dampening in all modelled results for rmse roms nest again performed better for both u and v therefore roms nest performed better than mercator and parent domain for both depths regarding bias both models had the tendency to overestimate the u component and underestimate the v component from the analysis performed in this section we found that even though for the same resolution roms and mercator had the tendency to present comparable results the increase in the resolution by 3 times led to better results when compared to both the primary parent roms grid and mercator therefore in the region of interest the implementation of a non assimilated higher resolution model can improve the result of the global models with great applications for local activities 3 2 sea surface temperature altimetry and t s profiles for the year of 2016 it was calculated the maps of rmse for the sea surface temperature sst fig 10 a and the sea surface height ssh fig 10 b to be employed as a skill score in the evaluation of model performance table 1 this parameter was calculated for each grid node after the linear interpolation between the time series from roms mur sst and aviso ssh most of the values of rmse for sst remained below 1 as observed in costa et al 2012 for the iberian coast with greater values in the oceanic region below 25 s and next to the coastal region of rio de janeiro around 23 s the discrepancies in the coastal region around 23 s was expected and is probably a result of the wind driven coastal upwelling off cabo frio after comparison with in situ measurements pereira et al 2020 reported strong biases in l4 sst products like mur for this region particularly during upwelling days according to the authors these discrepancies result mainly from the spatial temporal interpolation in l4 sst analyses and the use of microwave sst the spatial average of rmse presented the value of 0 45 c indicating a good representation of the surface temperature field as can it be compared against the works of azhar et al 2016 for the arabian gulf and the sea of oman and of chakraborty et al 2019 for the indian ocean for the ssh most of the area presents values of rmse below 0 1 m indicating a better representation of the ssh field when comparing to the work of pereira et al 2013 for the same region which is probability related to the climatological open boundary conditions used in their simulation the spatial average of rmse presented the value of 0 026 m indicating a good representation of the ssh field at the date of 31 05 2016 we extracted vertical profiles of temperature and salinity from an argo float located at 22 780 s and 35 750 w fig 11 this date was chosen as the float was located approximately in the middle of the grid without direct influence from the boundaries and the model is enough warmed up 7 months of simulation the model performed well for both variables with a rmse calculated from the profiles of 0 27 c and 0 09 for temperature and salinity respectively this comparison is only an indicative of the good representation of the water masses with more evaluations necessary to a conclusion 3 3 operational system results in order to evaluate the stability of the solution inside the domain and close to the boundaries with the free scheme executed daily for two months we plotted in fig 12 the daily mean surface velocity for the forecast system in the last day of simulation 26 11 2020 and in fig 13 the surface velocity from mercator for the same date the velocity fields in the two nested grids were plotted in a lower resolution to facilitate the visualization the velocity fields presented comparable magnitudes for the three roms grids against mercator the brazil current is well formed in the region off rj state with magnitudes around 0 8 m s it is also observed a southward current adjacent to the coastline in all domains with higher velocities next to cabo frio 23 s the models also presented the same overall features like the cyclonic gyre centered at 24 80 s and 40 80 w and the anticyclonic gyre centered at 25 87 s and 44 51 w the cyclonic gyre was outside the domain of the smallest grid the boundaries remained stable for all roms domains without the presence of jet like instabilities deeper layers also presented comparable magnitudes and features between roms and mercator and were not shown for the sake of repeatability following to the evaluation of the improvements with the assimilated scheme tables 3 and 4 present the pearson s correlation coefficient rmse and bias calculated from the 7 day forecasts compared against the bacia de campos buoy for the control run without data assimilation and the assimilated run respectively since this buoy is located outside the mean axis of the brazil current smaller values of error would be expected when comparing with the results from previous section from the tables it is observed greater values of both correlation and error for the v velocity component which is probably related to the dominance of this component at the buoy site in general there is an improvement in the modelled forecasts initialized from the assimilated analysis for both velocity components with an overall reduction in the rmse and bias and an increase in correlation we obtained an increase of around 30 in the correlation for both velocity components for the rmse a decrease of around 25 and for bias a decrease of around 20 for both u and v we did not find a tendency in the improvement of one component it is important to mention that the 2 day analysis performed in the tests was chosen taking in account computation constraints in future tests we will employ a 7 day analysis as in fragoso et al 2016 and de paula et al 2021 with the results compared against the 2 day analysis considering the improvement in the statistical parameters in the 7 day forecasts initialized from assimilated analysis we can infer that the assimilation method is correctly implemented in the workflow of the operational system and led to a better representation of the ocean circulation in the study region 4 conclusion and future developments in this work we proposed to build a multigrid ocean forecast operational system for the region off rio de janeiro state based on regional systems implemented for places like california japan and mediterranean for that we developed a set of open source codes to create boundary and initial conditions grid data ingestion atmospheric forcing nesting and observation file for data assimilation two operational schemes were developed a free without data assimilation and an assimilated employing the data assimilation method 4d var with the main difference being an analysis run performed in the assimilated scheme to resolve da in order to evaluate the new codes we performed hindcast simulations using mercator and era5 as input data for different parts of the southeastern brazilian oceanic region covering various regimes of the unique brazil current system the modelled velocity components including mercator were compared against two months of data from the following buoys of the pnboia program vitória itajaí and cabo frio mercator and roms 1 12 performed very well for vitória buoy for both u and v components in itajaí buoy roms nested 1 36 performed better than mercator and roms parent 1 12 with better correlations for v component for cabo frio buoy mercator performed better for v component and roms nest for u with roms primary performing poorly overall the cabo frio buoy presented the lowest correlation values among the three which is not a surprise when considering the complexity of the region with baroclinic instability associated to the bc iwbc system change in the coast orientation and strong upwelling therefore roms performed better than mercator in most of the scenarios executed in this work even with mercator assimilating real time ocean measurements from satellite and in situ observation systems from a one year simulation with the primary grid we performed calculations of the statistical parameter rmse for the sea surface temperature and sea surface height comparing against mur and aviso respectively roms performed very well for both variables with space averaged values for rmse of 0 45 c for sst and 0 026 m for ssh for the evaluation of the improvements with the assimilated scheme it was performed eight weekly simulations of 7 day forecasts initialize with 2 day assimilated analysis spanning from 08 02 2022 to 05 04 2022 due to availability of the bacia de campos buoy located at 22 48 8 s and 40 06 2 w in general we could observe an improvement in the modelled forecasts initialized from the assimilated analysis for both velocity components with an overall reduction in the rmse and bias and an increase in correlation for future developments we will implement tides and rivers and continue the evaluation of the system including transects and vertical profiles of temperature salinity velocity argo floats and real time velocity data collected from a network of hf radars in current implementation in the scope of the cronos project for the oceanic region off rio de janeiro https oceanpact com en innovation projects the radials from the hf radars will also be assimilated in the system a novelty in brazil code availability the numerical model source code is available at https github com fernandotcbarreto roms credit authorship contribution statement fernando t c barreto conceptualization methodology software data curation writing original draft validation franciane e curbani visualization methodology investigation writing review editing gabriel m zielinsky writing revision generating new data matheus b l da silva revision kaio c lacerda visualization methodology investigation writing review editing douglas f rodrigues software validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author was supported by the rio de janeiro research foundation faperj brazil funding the first author was funded by the rio de janeiro research foundation faperj 
23760,we propose the development of a multigrid open source operational forecast system with the oceanic region off rio de janeiro state as our first case study the system is composed by three nested roms domains employing mercator ocean at the lateral open boundaries of the outermost grid and gfs global forecast system at the surface open boundary of all grids two schemes were built for the operational forecast workflow free without data assimilation and assimilated with data assimilation for evaluation we performed hindcast simulations using mercator and era5 as input data for different parts of the southeastern brazilian oceanic region covering various regimes of the unique brazil current system the modelled data was compared against velocity components from three oceanic buoys sst from mur ultra high resolution sea surface temperature and ssh from aviso the comparison against the buoys showed a good representation of the velocity field by roms performing better than mercator for most scenarios roms also presented good performances for sst and ssh for the three nested forecast system the velocity fields presented comparable magnitudes against mercator without the presence of jet like instabilities indicating a stable system we also implemented the i4d var method in a 2 day analysis for the simulation of 7 day forecasts finding an improvement in the modelled forecasts initialized from the assimilated analysis for both velocity components with an overall reduction in the rmse and bias and an increase in correlation keywords computational modelling operational forecast system roms data availability data will be made available on request 1 introduction the development of an operational ocean forecast system and its important role for human activities including search and rescue at sea oil spill modelling and plankton transport have been described in a variety of studies breivik et al 2013 röhrs et al 2018 strand et al 2017 robust systems are successfully implemented for oceanic and coastal regions of the united states europe australia and japan in brazil most operational efforts on oceanic and regional modelling were executed within the scope of a research and development consortium called oceanographic modelling and observation network with the portuguese acronym remo marta almeida et al 2011 and the project azul dos santos et al 2015 remo currently delivers for the oceanic region off brazil hindcasts and assimilated forecasts employing the hycom hybrid coordinate ocean mode model in the resolutions of 1 4 1 12 and 1 24 project azul started in 2012 for the offshore area of santos basin and is based on three components observation network including gliders argo floats and svp surface velocity program drifters computational modeling and data assimilation in the phase i the first 4dvar assimilation system in a brazilian regional model was implemented for a hindcast improving the representation of the challenging mesoscale oceanographic features of the santos basin fragoso et al 2016 project azul is currently in the phase ii delivering operational forecasts at 1 12 with data assimilation and 1 36 grids focused on the offshore area of santos basin http www projetoazul eco br so far brazil still lacks an operational forecast system covering regional and shelf regions as the ones implemented for california https www cencoos org observations models forecasts moore et al 2013 peters and bokhorst 2001 delaware bay https tidesandcurrents noaa gov ofs dbofs dbofs html new jersey and new york shelf http www myroms org espresso and the mediterranean https www socib es seccion modelling facility forecast socib tintoré et al 2013 in common all these systems employ the regional ocean modeling system roms nested into a global circulation model at the outermost grid in this context following the best practices in the ocean forecast field we propose the development of a multigrid high resolution operational ocean forecast system off rio de janeiro state rj as part of a bigger project to be extended for other brazilian regions the shelf and oceanic region off rj are paramount for the economy of brazil as it presents major port structures and oil gas reserves this project main goal is modelling the ocean circulation by means of an open source operational forecast system considering limited computation resources to allow a wide range application of the system the codes for interpolation of the input data grid configuration ingestion of oceanic and atmospheric data workflow of the forecast system and creation of the observation file for data assimilation were developed with open source languages including python shell script grads and octave they are freely available for the scientific community at the following github page https github com fernandotcbarreto roms the brazil current bc is unique among subtropical western boundary currents with increasing vertical extension from its origin site around 15 s towards the confluence region with the malvinas current it goes from 200 m to 400 500 m at 20 s with the addition of the south atlantic central water sacw and to about 1200 m at 25 s with the addition of the antarctic intermediate water aaiw schmid et al 2000 silveira et al 2004 soutelino et al 2013 the modification of the bc as it flows poleward motivated the first part of the evaluation performed in this paper in which three experiments were established to test the modelled velocity field along three regions of the brazilian coast vitória 19 55 s and 39 41 w cabo frio 23 37 s and 42 12 w and itajaí 27 24 s and 47 15 w covering different regimes of the brazil current system in these experiments we performed hindcast simulations using on the lateral open boundaries data from mercator reanalysis cmems copernicus marine service phy 001 030 and on the surface data from era5 fifth generation ecmwf reanalysis the results were compared against u and v velocity components from the brazilian national buoy programme pnboia marinha do brasil 2017 in a second part of the evaluation results from the multigrid operational system were compared against the data of bacia de campos buoy part of pnboia to assess the improvement in 7 day forecasts initialized from analysis assimilated with the incremental strong constraint 4d variational method i4d var this paper describes the operational forecast system and presents an evaluation for the implemented experiments the paper is organized as follows the description of the evaluation set up and the development of the forecast system are presented in section 2 the results of the evaluation are presented and discussed in section 3 conclusions and future developments are presented in section 4 2 materials and methods 2 1 the ocean model the numerical model implemented in the operational system was the regional ocean modelling system roms 3 8 a free surface terrain following sigma coordinate primitive equation ocean model which is advantageous for regional applications shchepetkin and mcwilliams 2003 2005 the air sea fluxes for momentum and heat are calculated according to the bulk fluxes parameterization wind minus current in stress fairall et al 2003 vertical mixing is parameterized with the generic length scale gls umlauf and burchard 2003 implementation of the k kl turbulence closure mellor and yamada 1982 the simulation is configured to conserve volume with a free surface condition chapman 1985 a condition for the 2d momentum flather 1976 and a radiation boundary condition with nudging for the 3d momentum and tracers marchesiello et al 2001 the nudging timescales present a configuration of 1 and 30 days for the inflow and outflow respectively roms was chosen due to its advanced and robust rapidly evolving community code model being employed in a great number of forecast systems hirose et al 2019 mourre et al 2018 powell et al 2009 röhrs et al 2018 in the following sections will be described the system of three nested grids implemented for the operational modelling section 2 2 in section 2 3 1 the three domains established for the evaluation of the system will be described in section 2 3 2 the experiments for the evaluation of the assimilated scheme are presented all experiments are summarized in table 1 2 2 operational modelling set up a system composed by three nested grids was established with the innermost grid centered in the rio de janeiro state fig 1 the outermost grid is initialized and forced on the lateral open boundaries with data from the mercator analysis cmems global ocean 1 12 physics analysis and forecast updated daily chune et al 2021 and on the surface with data from ncep gfs 3 hourly average global forecast system at a base horizontal resolution of 28 kilometers gfs 2015 for mercator it is considered the time centered at 12 00 as in the original data for the outermost grid we employed the same grid used in fragoso et al 2016 as it was successfully applied to a regional ocean observing system in southeastern brazilian with the incremental strong constraint 4d variational i4d var method this grid will be named primary grid from now on and covers the domain from 50 w to 25 w and from 15 s to 30 s with a horizontal grid resolution of 1 12 the numerical grid presents three open boundaries in the model domain northern southern and eastern boundaries while the western is bounded by the coast the resolution of 1 12 for the outermost grid was chosen to minimize the numerical instabilities arising from the different bathymetry of mercator and the primary grid in the interpolation points the two inner grids nest 1 and nest 2 were rotated to maximize the sea land proportion and present the resolutions of 1 36 and 1 72 nest 1 covers the domain from 46 72 w to 37 58 w and from 18 53 s to 27 16 s and nest 2 from 44 91 w to 39 80 w and from 20 73 s to 25 30 s the positions of nest 1 and nest 2 were defined based on the availability of high resolution bathymetry the nesting method used in the system was based on the one way nesting technique described in marchesiello et al 2001 and mason et al 2010 the bacia de campos buoy indicated in fig 1 will be employed in the validation of the assimilated operational forecast system the baroclinic time steps for the grids with resolutions 1 12 1 36 and 1 72 are 300 s 150 s and 50 s respectively for the number of barotropic time steps between each baroclinic time step it was employed 30 10 and 10 respectively the same value of 5 m s2 for the harmonic horizontal viscosity coefficient was used for all grids for the vertical discretization it was employed the same configuration in the three grids 40 layers and an enhancement of the resolution at the surface using the surface and bottom stretching parameters θ s 5 0 and θ b 2 and the critical depth parameter t c l i n e 100 m the same minimum depth of 1 m was employed for all grids to avoid model instability and or spurious deep currents the final masked bathymetry of all three grids was smoothed to fulfil a requirement of the roms slope the rx0 factor beckmann and haidvogel 1993 the two child s grid bathymetry was modified so that it is equal to the parent one at the open boundaries linearly converging to the original high resolution bathymetry as in mason et al 2010 for all grids the target was a rx0 between 0 3 and 0 35 to reach the optimum balance between grid stability and the integrity of bathymetry we point out that more refined grids reach lower values of rx0 more stable with less modification of the bathymetry the interpolation and extrapolation of data from mercator to the roms grid and between roms grids for nesting were performed following the works of mason et al 2010 and marta almeida et al 2019 to ensure the conservation of volume the input data are initially horizontally interpolated for the roms grid with the vertical interpolation resolved for each node in the next step the extrapolation consisting of replicating the data closest to the missing node nan is applied in the two interpolation steps horizontally and vertically to the following three regions above the uppermost input data layer below the bottommost input data layer and on the coast within the landmask of the input data the codes for interpolation and extrapolation are available at the following github page https github com fernandotcbarreto roms tree master boundary ini the system does not have tides nor rivers and they are expected to be implemented soon for this reason as no calculations were performed to quantify the influence of tides and rivers in the study area we must consider this limitation in the current stage of the operational system two schemes were built for the operational forecast workflow free without data assimilation and assimilated with the data assimilation method i4d var in fig 2 is shown the general workflow in which we obtain the free scheme excluding the n day analysis this strategy was used to account for limited computational resources since i4d var requires a cluster or cloud computing as observational data for assimilation we employed sea surface temperature sst data from mur chin et al 2017 and altimetry data from aviso aviso 1996 the free scheme was based in the works of costa et al 2012 and juza et al 2016 and involves resolving two systems a hindcast spin up that runs at a specified frequency working as a restart and a forecast that runs daily the hindcast n days is forced on the surface by the gfs and uses mercator as initial and lateral boundary conditions roms linearly interpolate in time the surface and boundary conditions to match the time step the forecast system n days uses the mercator to build the boundary conditions and the roms data from the previous run to build the initial conditions there is the option to activate the nudging process on a strip of grid cells next to the boundaries linearly converging to 0 no nudging beyond this strip to minimize instabilities generated by mismatches between initial condition and boundaries and to keep the boundaries stable along the simulation the downside of nudging is the increase in running time if the previous run is the hindcast the last available time step is used in case of the forecast it uses the time step corresponding to the start day of the new run 1 day after the start day of the previous run this workflow is repeated for all nested grids the restart strategy was employed to ensure that the roms solution stays close to the real evolution of the ocean in scenarios where data assimilation is not resolved costa et al 2012 the frequency to perform the restart depends on the quality of input data and on the stability of the model bathymetry smoothing has an important role the codes for implementing this scheme are available at the following github https github com fernandotcbarreto roms tree master operational based on the works of mourre et al 2018 and hirose et al 2019 we developed the assimilated scheme presented in fig 2 the main difference between the two schemes is an analysis run a n day analysis where the data assimilation method i4d var is performed the forecast is then initialized using the last time step of this analysis as the initial condition simulating n days in the future in a 2 day assimilated analysis for instance the analysis starts from the midpoint of the running time in the previous one leading to an overlap of 50 in the incremental strong constraint 4d variational method i4d var the increments to the model state vector x t t s ζ u v t at a time t comprised of all prognostic variables potential temperature t salinity s sea surface displacement ζ and horizontal velocity components u and v is estimated and subject to surface forcing conditions f t for freshwater fluxes heat momentum and lateral open boundary conditions b t that minimizes in a least squared sense the difference between the model and observations li et al 2015 de paula et al 2021 the parameters for the data assimilation performed in this work are presented in section 2 3 2 for more details of i4d var consult moore et al 2011a b the codes for implementing this scheme are available at the following github page https github com fernandotcbarreto roms tree master operational assimilation 2 3 evaluation set up 2 3 1 hindcasts in order to evaluate the whole system including input data and model parameters we established three domains summarized in table 1 along the eastern brazilian coast fig 3 these domains aim to cover the modifications of the brazilian current system bc as it flows poleward with great changes in vertical structure volume transport and energy balance the domains involved different grids depending on the results obtained with the primary grid and the location of the buoy data used for evaluation for the buoy site located in southern brazil itajaí buoy for instance due to its location close to the boundary of the primary grid a new coarse grid with resolution of 1 12 was implemented named parent grid a nested grid was implemented when poor quality results were obtained with the coarse grid for all domains the same parameters were employed 40 layers and an enhancement of the resolution at the surface using the surface and bottom stretching parameters θ s 5 0 and θ b 2 and the critical depth parameter t c l i n e 100 m the minimum depth was of 1 m for the coarse grid 1 12 the time step was of 300 s and for the nested grid 1 36 of 150 s the first domain fig 3 a is composed by the single primary grid the same outermost grid presented in last section and was run for the year long period of 2016 with 2 months of spin up in november and december of 2015 using at boundaries data from mercator reanalysis drévillon et al 2021 and era5 hersbach et al 2018 this experiment was run for a year aiming the construction of new assimilation files in future works the u and v velocity components were compared against the near surface and bottom available current data from the vitória buoy located at 19 55 s e 39 41 w part of the brazilian national buoy programme pnboia marinha do brasil 2017 the velocity comparison was performed between 15 04 2016 and 15 06 2016 based on the availability of the vitória buoy data pnboia velocity data extends from the depth of 5 5 m to 75 5 m with a cell of 3 5 m since this domain presented highly satisfactory results against this buoy to be presented in next section and is outside the region of interest a nesting was not performed in this experiment as the primary grid covers the entire eastern brazilian coast the results from the year long period of 2016 were also compared against mur and aviso to evaluate the fields of sea surface temperature sst and sea surface height ssh respectively and against argo float to evaluate the vertical profiles of temperature and salinity the mur analysis chin et al 2017 presents a 1 km resolution and is produced as a retrospective data set four day latency and as a near real time data set one day latency at the jet propulsion laboratory jpl podaac using wavelets as basis functions in an optimal interpolation oi approach aviso is a 0 1 resolution gridded composite of the advanced very high resolution radiometer avhrr from the polar operational environmental satellites poes project aviso 1996 argo argo 2000 is an international program that collects information from inside the ocean using a fleet of robotic instruments that drift with the ocean currents and move up and down between the surface and a mid water level a grid with resolution 1 36 was nested to the previous grid creating a two grid nested domain fig 3 b centered in the rio de janeiro state this domain was executed from 01 09 2017 to 01 11 2017 employing a two month spin up 01 07 2017 to 01 09 2017 with the results compared against the u and v velocity components of the cabo frio buoy pnboia located at 23 37 s and 42 12 w this date was selected based on the availability of cabo frio buoy despite the analysis of the domain averaged kinetic energy showing a fast warm up in less than 5 days we chose two months to make sure that all the results evaluated has the minimum influence from the initial condition mercator a second domain composed by two nested grids with resolutions 1 12 and 1 36 was implemented for the southern region of brazil fig 3 c this system was executed from 01 11 2017 to 01 01 2018 employing a two month spin up 01 09 2017 to 01 11 2017 with results compared against the u and v velocity components of the itajaí buoy pnboia located at 27 24 s and 47 15 w this date was selected based on the availability of itajaí buoy the analysis for itajaí buoy required a new system of nested grids as this buoy is located too close to the southern boundary of primary grid the quality of the modelled results relative to the observations were evaluated with basic statistic such as the pearson s correlation coefficient the root mean square error rmse the quadratic scoring rule which measures the root average quadratic distance between the model and observations and bias these scores complement each other and give different statistical information about the errors while the pearson coefficient measures the linear correlation between two time series the rmse measures the spread of the distribution of the differences between the model and observations giving information about how wide is its 2 3 2 operational forecast and data assimilation systems for the operational forecast system presented in section 2 2 two experiments were performed to evaluate the results table 1 in the first experiment the system was executed with the free scheme for two months from 26 09 2020 to 26 11 2020 in serial model at a cpu 2 40ghz with 12 gb ram laptop the goal of this experiment was to evaluate the feasibility of executing the operational multigrid system without a cluster structure or cloud computing in this case the system performed in a daily basis 7 day forecasts of the three nested grids forced by mercator analysis and ncep gfs with each forecast taking approximately 4 h for conclusion to evaluate the stability of the roms solution inside the domain and close to the boundaries the surface velocity of the last day of simulation was compared against mercator in order to evaluate the improvement with the assimilated scheme it was executed on a weekly basis performing 8 cycles of 7 day forecasts initialized with 2 day assimilated analysis spanning from 08 02 2022 to 05 04 2022 due to availability of the bacia de campos buoy bacia de campos is a recent deployed buoy from pnboia located at 22 48 8 s and 40 06 2 w and delivered current velocity at three surface layers until 05 04 2022 when it suffered severe damage the forecast started at 08 02 2022 with the initial condition provided from a 2 day assimilated analysis which used as observational data for assimilation sea surface temperature sst from mur and altimetry data from aviso the next forecast started at the end of the previous simulation with the initial condition provided from the assimilated analysis of the previous 2 days this workflow performed 8 x was executed in cloud computation with a configuration of 32 cpus and 128 ram the data assimilation was only executed in the primary grid employing the same parameters and preprocessing from fragoso et al 2016 it was used a 100 km horizontal decorrelation length scale for sea surface height temperature salinity and all surface forcing variables and a 60 km length scale for momentum in the vertical a 100 m decorrelation scale was used for all variables the observation error was of 2 cm and 0 4 for ssh and sst respectively the experiments were performed using ten inner loops and two outer loops and took approximately 2 h to execute the 2 day assimilated analysis and 10 min the 7 day forecast the simulations with data assimilation were run with the free scheme to evaluate the improvements from the assimilated scheme with the 2 way analysis executed without data assimilation control run as the position of the bacia de campos buoy is close to the boundary of nest 2 higher resolution grid the statistical parameters were calculated for the nest 1 grid with the adcp cell closest to the surface 5 5 m used in the comparisons 3 results and discussion 3 1 evaluating the velocity components the modification of the brazil current bc as it flows poleward motivated the test of the modelling system for the buoys vitória cabo frio and itajaí as they cover different regimes of the bc the vitória buoy was moored at a depth of 200 m in a region with baroclinic mesoscale activity resultant from the strong vertical shear of the bc with the intermediate western boundary current iwbc soutelino et al 2013 in the region of the cabo frio buoy moored at 200 m besides the baroclinic mesoscale activity from the strong vertical shear of bc iwbc system mano et al 2009 silveira et al 2008 the dynamics is also modified by a change in coastal direction from north south to east west around 23 s which also promotes upwelling of deep south atlantic central water valentin 2001 resulting in a highly complex region the site of the itajaí buoy moored at 200 m is much less studied compared to the other two due to the distance from the pre salt oil reserves of the campos and santos basin in this region the bc is composed by the tropical water tw south atlantic central water sacw antarctic intermediate water aaiw and north atlantic deep water nadw flowing towards the malvinas confluence the u and v velocity components from roms were compared against the in situ data for two months at the depths of 5 5 m and 75 5 m top and bottom depths of pnboia data the modelled data are initially linear interpolated to the in situ higher frequency data to avoid loss of information a 12 hour low pass filter butterworth is then applied to the u and v time series to remove outliers and smooth out high frequency variations the mercator data was also compared against roms and pnboia since mercator assimilates real time ocean measurements from satellite and in situ observations it is a great dataset to use as quality control the pearson s correlation coefficient rmse and bias were calculated to evaluate the modelled data and are presented in table 2 for the three buoys 3 1 1 vitória buoy the modelled data from roms grid 1 12 primary grid and mercator are plotted against vitória buoy in figs 4 and 5 roms and mercator presented high correlations for all comparisons 0 85 with roms performing slightest better this can be confirmed in the plots with the models reproducing the main peaks there was no significant difference between surface and bottom values for correlation coefficients for rmse the worst value was observed for mercator v component at depth 5 5 m 0 1612 m s and the best for roms u component at depth 75 5 m 0 0781 both roms and mercator improved their rmse values at 75 5 m roms improved around 23 for u and 28 for v regarding bias both roms and mercator underestimated the velocity field since roms performed very well for the vitória buoy and this site is outside the region of interest for the operational system rj state it was not implemented a nested grid for this domain 3 1 2 cabo frio buoy the modelled data from roms 1 12 primary grid roms 1 36 and mercator are plotted against cabo frio buoy in figs 6 and 7 at 5 5 m mercator 0 4096 and roms primary 0 3887 presented better correlation for v and roms nest for u 0 5506 however as observed in the plots the models failed to represent the main peaks in the v component while the u component was well represented by the roms nest with exception of the feature between 2016 10 08 and 2016 10 13 for rmse roms nest performed well for both u and v with values smaller than 0 13 m s while mercator and roms primary presented values close to 0 2 m s for the u component for comparison in their work for the santos basin fragoso et al 2016 calculated the rmse with a minimum value around 0 2 m s for both free and assimilated run at 75 5 m mercator improved the correlation for v in 43 to 0 5877 and roms nest for u in 30 to 0 7206 roms primary performed worse in this depth again no clear correlation can be observed from the plots of v component for neither model for rmse the nested roms improved slightly for both u and v in overall mercator presented a better correlation for u component and roms nest for v however when calculating a depth average of these correlations we found a greater value for roms 0 63 against 0 5 for mercator besides the depth average of rmse for u and v are 0 20 and 0 08 m s respectively for mercator and 0 12 and 0 11 m s respectively for roms indicating a better representation of the pnboia data by the roms nest from the value of bias we observe that both mercator and roms primary and nest had the tendency to underestimate the velocity field 3 1 3 itajaí buoy the modelled data from roms 1 12 roms 1 36 and mercator are plotted against itajaí buoy in figs 8 and 9 at 5 5 m both roms and mercator presented greater correlations for v with the nested domain performing 60 better than mercator 0 8528 against 0 5314 and the parent performing 26 worse 0 3896 against 0 5314 the reason the nested domain performed better than mercator even though the parent performed worse is probably related to an oceanographic process better represented in a higher resolution grid as a submesoscale feature for u component all domains presented low correlations as can be confirmed in the plots with most of the peaks not represented in the simulation regarding rmse all cases presented values around 0 1 indicating a stable solution with roms nest performing best for both u 0 0926 m s and v 0 0852 m s at 75 5 m mercator and roms parent presented low correlations with the nested domain performing well for v 0 8309 and regular for u 0 4875 in the work of costa et al 2018 for the northwestern iberian coast correlations around 0 5 were deemed as decent despite the high correlation value for the nested domain in fig 9 we can observe a significant dampening in all modelled results for rmse roms nest again performed better for both u and v therefore roms nest performed better than mercator and parent domain for both depths regarding bias both models had the tendency to overestimate the u component and underestimate the v component from the analysis performed in this section we found that even though for the same resolution roms and mercator had the tendency to present comparable results the increase in the resolution by 3 times led to better results when compared to both the primary parent roms grid and mercator therefore in the region of interest the implementation of a non assimilated higher resolution model can improve the result of the global models with great applications for local activities 3 2 sea surface temperature altimetry and t s profiles for the year of 2016 it was calculated the maps of rmse for the sea surface temperature sst fig 10 a and the sea surface height ssh fig 10 b to be employed as a skill score in the evaluation of model performance table 1 this parameter was calculated for each grid node after the linear interpolation between the time series from roms mur sst and aviso ssh most of the values of rmse for sst remained below 1 as observed in costa et al 2012 for the iberian coast with greater values in the oceanic region below 25 s and next to the coastal region of rio de janeiro around 23 s the discrepancies in the coastal region around 23 s was expected and is probably a result of the wind driven coastal upwelling off cabo frio after comparison with in situ measurements pereira et al 2020 reported strong biases in l4 sst products like mur for this region particularly during upwelling days according to the authors these discrepancies result mainly from the spatial temporal interpolation in l4 sst analyses and the use of microwave sst the spatial average of rmse presented the value of 0 45 c indicating a good representation of the surface temperature field as can it be compared against the works of azhar et al 2016 for the arabian gulf and the sea of oman and of chakraborty et al 2019 for the indian ocean for the ssh most of the area presents values of rmse below 0 1 m indicating a better representation of the ssh field when comparing to the work of pereira et al 2013 for the same region which is probability related to the climatological open boundary conditions used in their simulation the spatial average of rmse presented the value of 0 026 m indicating a good representation of the ssh field at the date of 31 05 2016 we extracted vertical profiles of temperature and salinity from an argo float located at 22 780 s and 35 750 w fig 11 this date was chosen as the float was located approximately in the middle of the grid without direct influence from the boundaries and the model is enough warmed up 7 months of simulation the model performed well for both variables with a rmse calculated from the profiles of 0 27 c and 0 09 for temperature and salinity respectively this comparison is only an indicative of the good representation of the water masses with more evaluations necessary to a conclusion 3 3 operational system results in order to evaluate the stability of the solution inside the domain and close to the boundaries with the free scheme executed daily for two months we plotted in fig 12 the daily mean surface velocity for the forecast system in the last day of simulation 26 11 2020 and in fig 13 the surface velocity from mercator for the same date the velocity fields in the two nested grids were plotted in a lower resolution to facilitate the visualization the velocity fields presented comparable magnitudes for the three roms grids against mercator the brazil current is well formed in the region off rj state with magnitudes around 0 8 m s it is also observed a southward current adjacent to the coastline in all domains with higher velocities next to cabo frio 23 s the models also presented the same overall features like the cyclonic gyre centered at 24 80 s and 40 80 w and the anticyclonic gyre centered at 25 87 s and 44 51 w the cyclonic gyre was outside the domain of the smallest grid the boundaries remained stable for all roms domains without the presence of jet like instabilities deeper layers also presented comparable magnitudes and features between roms and mercator and were not shown for the sake of repeatability following to the evaluation of the improvements with the assimilated scheme tables 3 and 4 present the pearson s correlation coefficient rmse and bias calculated from the 7 day forecasts compared against the bacia de campos buoy for the control run without data assimilation and the assimilated run respectively since this buoy is located outside the mean axis of the brazil current smaller values of error would be expected when comparing with the results from previous section from the tables it is observed greater values of both correlation and error for the v velocity component which is probably related to the dominance of this component at the buoy site in general there is an improvement in the modelled forecasts initialized from the assimilated analysis for both velocity components with an overall reduction in the rmse and bias and an increase in correlation we obtained an increase of around 30 in the correlation for both velocity components for the rmse a decrease of around 25 and for bias a decrease of around 20 for both u and v we did not find a tendency in the improvement of one component it is important to mention that the 2 day analysis performed in the tests was chosen taking in account computation constraints in future tests we will employ a 7 day analysis as in fragoso et al 2016 and de paula et al 2021 with the results compared against the 2 day analysis considering the improvement in the statistical parameters in the 7 day forecasts initialized from assimilated analysis we can infer that the assimilation method is correctly implemented in the workflow of the operational system and led to a better representation of the ocean circulation in the study region 4 conclusion and future developments in this work we proposed to build a multigrid ocean forecast operational system for the region off rio de janeiro state based on regional systems implemented for places like california japan and mediterranean for that we developed a set of open source codes to create boundary and initial conditions grid data ingestion atmospheric forcing nesting and observation file for data assimilation two operational schemes were developed a free without data assimilation and an assimilated employing the data assimilation method 4d var with the main difference being an analysis run performed in the assimilated scheme to resolve da in order to evaluate the new codes we performed hindcast simulations using mercator and era5 as input data for different parts of the southeastern brazilian oceanic region covering various regimes of the unique brazil current system the modelled velocity components including mercator were compared against two months of data from the following buoys of the pnboia program vitória itajaí and cabo frio mercator and roms 1 12 performed very well for vitória buoy for both u and v components in itajaí buoy roms nested 1 36 performed better than mercator and roms parent 1 12 with better correlations for v component for cabo frio buoy mercator performed better for v component and roms nest for u with roms primary performing poorly overall the cabo frio buoy presented the lowest correlation values among the three which is not a surprise when considering the complexity of the region with baroclinic instability associated to the bc iwbc system change in the coast orientation and strong upwelling therefore roms performed better than mercator in most of the scenarios executed in this work even with mercator assimilating real time ocean measurements from satellite and in situ observation systems from a one year simulation with the primary grid we performed calculations of the statistical parameter rmse for the sea surface temperature and sea surface height comparing against mur and aviso respectively roms performed very well for both variables with space averaged values for rmse of 0 45 c for sst and 0 026 m for ssh for the evaluation of the improvements with the assimilated scheme it was performed eight weekly simulations of 7 day forecasts initialize with 2 day assimilated analysis spanning from 08 02 2022 to 05 04 2022 due to availability of the bacia de campos buoy located at 22 48 8 s and 40 06 2 w in general we could observe an improvement in the modelled forecasts initialized from the assimilated analysis for both velocity components with an overall reduction in the rmse and bias and an increase in correlation for future developments we will implement tides and rivers and continue the evaluation of the system including transects and vertical profiles of temperature salinity velocity argo floats and real time velocity data collected from a network of hf radars in current implementation in the scope of the cronos project for the oceanic region off rio de janeiro https oceanpact com en innovation projects the radials from the hf radars will also be assimilated in the system a novelty in brazil code availability the numerical model source code is available at https github com fernandotcbarreto roms credit authorship contribution statement fernando t c barreto conceptualization methodology software data curation writing original draft validation franciane e curbani visualization methodology investigation writing review editing gabriel m zielinsky writing revision generating new data matheus b l da silva revision kaio c lacerda visualization methodology investigation writing review editing douglas f rodrigues software validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author was supported by the rio de janeiro research foundation faperj brazil funding the first author was funded by the rio de janeiro research foundation faperj 
23761,we build on the work of mayer and madsen 2000 and larsen and fuhrman 2018 to correct for overmixing in wave potential flow regions due to the linear instability of two equation closure systems we extend it to the generic length scale gls turbulent closure approach while testing it for the first time on a stratified nearshore problem the modified turbulence equations affect only the nonbreaking wave potential flow region where the vorticity is less than 10 of the strain rate and thus can preserve stratification on the innershelf under favorable heat and wind forcing conditions this work is a necessary step before engaging in full 3d wave resolving studies of surf shelf exchange keywords nearshore wave resolving model wave induced mixing turbulence closure data availability data will be made available on request 1 introduction several 3d wave resolving free surface and terrain following models based on the reynolds averaged navier stokes rans equations have emerged for the nearshore zone e g swash zijlema et al 2011 nhwave derakhti et al 2016 and croco marchesiello et al 2021 in these models the explicit overturning of the free surface is excluded and the breaking wave is modeled with a single valued free surface which behaves as a dissipating bore despite the absence of explicit overturning replaced by parameterized turbulence these models can be accurate as well as computationally efficient in the study of waves and wave driven mean and transient circulation they can thus be applied to the entire 3d nearshore zone including the innershelf area and be coupled with biogeochemical or sediment transport models to explicitly simulate the effects of realistic short crested and asymmetric waves marchesiello et al 2021 2022 but stratification or earth rotation has not been included in these earlier works even though they may be important on the shelf and in the exchange mechanisms with the surfzone in previous studies marchesiello et al 2021 2022 the coastal and regional ocean community model croco was used with the k ω or k ϵ two equation turbulence closure models as part of a generic length scale gls method warner et al 2005 the k ω model generally performed best in the surfzone without modification of the standard model parameters from wilcox 1988 this is a conclusion shared by several modeling studies since mayer and madsen 2000 possibly due to a more accurate near wall treatment by k ω models devolder et al 2018 which is needed in the wall bounded turbulent shear flows of the surfzone trowbridge and elgar 2001 however both models suffer from overmixing in areas of potential irrotational flow typically in regions outside the surfzone where waves are not breaking this problem has long been identified lin and liu 1998 mayer and madsen 2000 bradford 2011 brown et al 2016 devolder et al 2018 larsen and fuhrman 2018 and understood as a kind of paradox where vorticity rather than strain rate should be the source of turbulence even though the role of strain rate is theoretically justified by the reynolds averaging process kato and launder 1993 the generation of turbulence in unexpected regions may thus appear to challenge the validity of boussinesq eddy viscosity hypothesis on the local relationship between mean and turbulent quantities schmitt 2007 which led to the introduction of a realizability constraint based on physical assumptions durbin 2009 recently however larsen and fuhrman 2018 building on earlier analysis of mayer and madsen 2000 that went rather unnoticed have better identified the causes of excessive vertical mixing in areas of potential flow revealing a linear instability of the two equation closure system it is the interaction between the two turbulent prognostic variables in the system that can exponentially amplify the generation of turbulent energy in unexpected regions the resulting overmixing can affect the amplitude of waves propagating to the nearshore zone and degrade the prediction of the wave breaking point lin and liu 1998 bradford 2000 mayer and madsen 2000 we found in the present study that perhaps more importantly overmixing can also affect the maintenance of stratification on the innershelf the last point is generally overlooked because 3d wave resolving models have been primarily focused on the surfzone without considering exchanges with the innershelf in contrast to recently emerging wave averaged model studies kumar and feddersen 2017 wang et al 2021 which do not suffer from the overmixing problem with advances in non hydrostatic models and computational resources the study of surf shelf exchange describing the fate of pollutants biological matter and sediments in the nearshore zone e g brown et al 2015 can now also be accessed by representing phase resolved waves and stratification however to proceed in this direction the problem of excessive mixing in nonbreaking waves must first be corrected note that in wave averaged models a certain amount of nonbreaking wave induced mixing is actually introduced by semi empirical formulas qiao et al 2004 ghantous and babanin 2014 it is much smaller than that caused by wave breaking but it reaches deeper and can affect the sea surface temperature of ocean general circulation models babanin 2023 the wave induced turbulence involved in these parameterizations assumes various processes such as interaction with pre existing turbulence here the generation of intense innershelf turbulent mixing appears to be more of an artifact of the turbulent models under conditions of nearly potential flow to address this specific issue we build on the works of mayer and madsen 2000 and larsen and fuhrman 2018 and extend it to a more generic approach within gls while testing for the first time the effect that a stabilized turbulence model has on innershelf stratification in an idealized nearshore configuration 2 modification of the turbulence closure model croco is a nonhydrostatic free surface and terrain following model developed around the regional oceanic modeling system shchepetkin and mcwilliams 2005 debreu et al 2012 its capabilities include high performance computation of high order accurate discretized equations and coupling with atmosphere waves biogeochemistry sediment and turbulence models it has been applied to a variety of configurations from regional and shelf circulations to very fine scale processes such as wave induced nearshore circulation marchesiello et al 2015 2021 2022 it thus appears naturally suited for addressing surf shelf exchange processes in a 3d rotating and stratified framework however innershelf stratification cannot be preserved if the turbulence closure equations produce excessive mixing as is often the case for rans wave resolving models larsen and fuhrman 2018 2 1 turbulence closure the treatment of breaking waves in croco involves both a shock capturing advection scheme weno5 and a second order turbulence closure system a k ϵ or k ω model solving the equations for turbulent kinetic energy k and dissipation ϵ or dissipation rate ω ϵ k 1 as part of a generic length scale gls method warner et al 2005 in the absence of buoyancy forcing the turbulence equations express a balance between transport diffusion shear production and dissipation 1 ρ k t ρ v k d k ρ p ϵ 2 ρ ϵ t ρ v ϵ d ϵ ρ ϵ k c ϵ 1 p c ϵ 2 ϵ 3 or ρ ω t ρ v ω d ω ρ ω k c ω 1 p c ω 2 ϵ the eddy viscosity ν t c μ l k 1 2 is derived from these equations with coefficient c μ dependent on stability functions and mixing length l k 3 2 ϵ 1 according to the boussinesq eddy viscosity hypothesis the shear production term for k is p 2 ν t s 2 with s 2 s i j s i j and s i j 1 2 u i x j u j x i using einstein notation which is the mean 3d strain rate tensor 1 1 the use of a 3d strain rate tensor differs from the gls approach presented in warner et al 2005 where energy production is only due to vertical shear of horizontal flows all turbulence model parameters are given in warner et al 2005 based on burchard et al 1998 for k ϵ and wilcox 1988 for k ω the turbulence models perform very well in the surfzone compared with flume experiments marchesiello et al 2021 2022 however as in previous studies they tend to produce excessive mixing in potential flow regions i e on the innershelf 2 2 correction in potential flow regions in many studies of breaking waves see the literature reviews by devolder et al 2018 larsen and fuhrman 2018 the predicted turbulence levels from two equation closures are much higher than what has actually been measured while many attempts were made for limiting mixing in supposed low turbulence regions larsen and fuhrman 2018 more specifically identify the causes of overmixing in areas of potential irrotational flow typical of nonbreaking waves the problem is common to the entire class of 2 equation closure models k ω k ϵ and variants because it is related to an unconditional linear instability of the equation system under low vorticity conditions for the k ω model a linearized homogeneous system suitable for the potential flow region beneath surface waves where a fixed strain rate s and negligible advection and diffusion can be assumed is 4 k t p ϵ 5 ω t ω k c ω 1 p c ω 2 ϵ noting that the specific dissipation rate ω is related to the energy dissipation ϵ by ϵ c μ k ω and that the eddy viscosity ν t k ω we can rewrite the system of equations in terms of only the variables k and ω 6 k t 2 k ω s 2 c μ k ω 7 ω t 2 c ω 1 s 2 c ω 2 c μ ω 2 considering as larsen and fuhrman 2018 that ω tends asymptotically to ω as ω t tends to 0 and replacing ω by ω in the k equation gives an exponential growth k e σ t with σ a fraction of s always positive i e the model is unconditionally unstable the instability of this system thus corresponds to an exponential increase in the turbulent energy k particularly in areas of potential flow where dissipation is never strong enough to control it larsen and fuhrman 2018 propose to curb the excessive energy production where the strain rate s is much greater than the vorticity ω defined by ω 2 ω i j ω i j with ω i j 1 2 u i x j u j x i the rotation rate tensor and demonstrate that this correction stabilizes the solution of the linear system as σ becomes negative the limitation is written as a majoration of ω in the viscosity ν t k ω used in the production term p 2 ν t s 2 with the following function 8 ω max 1 λ c ω 2 c ω 1 s 2 ω 2 ω where λ is a limitation parameter taken as 0 05 in larsen and fuhrman 2018 a similar condition is given for the k ϵ model ν t c μ k 2 ϵ where 9 ϵ max 1 λ c ϵ 2 c ϵ 1 s 2 ω 2 ϵ this correction is reminiscent of the one proposed for the stagnation point anomaly by kato and launder 1993 in a different study context where the turbulent energy production p 2 ν t s 2 is replaced by p 2 ν t s ω it also resembles that of mayer and madsen 2000 who take p 2 ν t ω 2 thus making production vanish in pure straining flow according to larsen and fuhrman 2018 these attempts significantly reduce the growth of instability in the potential flow region but do not resolve its formal unconditionality in addition in our numerical experiments the use of vorticity to replace all or part of the strain rate in the production term p tends to under represent the production of turbulent energy in the surfzone which is undesirable we thus concur with mayer and madsen 2000 and durbin 2009 that altering the strain rate tensor in p is not a satisfactory solution and we choose instead to majorate ϵ when computing ν t to control the instability of the turbulence equation system the latter solution may appear merely as an alternative to limiting the mixing length l as in bradford 2011 or the coefficient c μ as in realizable models shih et al 1995 but a limitation based on the ratio s ω makes it a more effective constraint on the instability fuhrman and li 2020 2 3 implementation in gls for the implementation in the croco gls code we impose the larsen and fuhrman 2018 limitation on the variable ϵ whatever the choice of closure since the turbulent dissipation ϵ is computed in all types of two equation models handled by gls starting from the k ω model we use the relation between ϵ and ω ϵ c μ k ω to assume a limitation of the form 10 ϵ max 1 λ c ω 2 c ω 1 s 2 ω 2 ϵ this expression is similar to eq 9 for the k ϵ apart from the factor c ω 2 c ω 2 replacing c ϵ 2 c ϵ 1 however the fraction c 2 c 1 does not vary much with the choice of closure model warner et al 2005 and is 1 2 1 5 considering the uncertainty in choosing λ we can propose a generic limitation 11 ϵ max 1 λ s 2 ω 2 ϵ in our numerical experiments next section this formulation controlled the growth of instability on the innershelf with λ 0 1 in this case the correction is triggered as soon as the vorticity is less than about 10 of the strain rate we also note that increasing the power law of the correction function stiffens the intensity of the correction below the 10 threshold and further decreases the mixing on the innershelf therefore in the following we will test the modified formulation 12 ϵ max 1 λ s 4 ω 4 ϵ with a baseline λ value of 0 01 3 numerical experiments 3 1 ideal test case the idealized model configuration is representative of a nearly planar beach representative of southern california kumar and feddersen 2017 or elsewhere with a smooth sandbar that triggers wave breaking and a cross shore circulation fig 1 details of the numerical implementation of croco for wave resolving simulations is given in marchesiello et al 2021 2022 only the configuration is presented here for simplicity we use a 2d x z channel with no alongshore dimension depths range from 7 m offshore to 1 m above mean sea level at the coast and a gentle sandbar is present about 100 m from shore the shoreline position is allowed to vary with swash oscillation based on a wetting drying scheme warner et al 2013 initially the water is at rest and the stratification is given by a temperature profile t z 8 10 e z 50 fig 2 roughly representing the mid latitude summer situation a monochromatic wave is generated at the offshore boundary with 0 5 m wave amplitude and 10 s period a non slip condition is imposed on the boundaries of the channel sidewalls the horizontal grid spacing is 0 5 m the vertical grid has 10 vertical levels refined at the surface and bottom and the model time step is 0 025 s for bed shear stress the logarithmic law of the wall is used with a roughness length of 1 mm the comparison of velocity u temperature t and eddy viscosity ν t at 30 min after the start of the simulation is presented in fig 2 it can be seen that without limitation the eddy viscosity produced by the k ω model increases to values well above 0 01 m 2 s and the high viscosity signal propagates from the surfzone to the shelf and gradually invade the entire model area fig 3 with the corrected closure system the viscosity does not develop outside the surfzone i e in the potential flow region where waves do not break in this case the viscosity in the surfzone has the expected structure and magnitude ν t 0 01 0 05 m 2 s battjes 1975 it is important to note that in the breaker zone the viscosity and related velocity profile is similar in both the standard and corrected cases showing that the limitation does not seem to affect the assumed turbulent zone much which is desirable but this will be verified against laboratory data in the next section the effect of vertical mixing on stratification is dramatic fig 2 at viscosity values of about 0 01 m 2 s and above mixing can break up the stratification in a matter of minutes τ m i x l 2 ν t with l limited by bottom depth with the original k ω model such high viscosity values reach the entire domain within half an hour and stratification breakdown follows everywhere on the contrary stratification is maintained on the innershelf outside the surfzone with the corrected turbulence model in this case the result becomes similar to those presented by wave averaged models lacking a process to generate a nonbreaking wave eddy viscosity kumar and feddersen 2017 and to nearshore observations sinnett and feddersen 2019 where spring summer stratification is preserved outside the surfzone as long as winds are sufficiently light next we checked that the corrections made on the turbulent dissipation ϵ are working not only for the k ω model but also for the k ϵ and the generic model of umlauf et al 2003 which is referred to as g e n model in warner et al 2005 fig 4 shows the result of the three models it confirms the report of marchesiello et al 2021 that the k ϵ model produces less turbulent energy than the k ω model with as a consequence an over prediction of velocity shear and undertow magnitude the g e n model gives intermediate results most importantly here the correction has worked in all cases and no exponential growth of turbulent energy and viscosity has occurred outside of the surfzone our correction of turbulent dissipation thus appears appropriate for a gls approach 3 2 surfzone verification with lip experiment validation of surfzone currents by comparison with flume experiments are provided in marchesiello et al 2021 2022 to make sure that the change of mixing due to the gls corrections does not affect the surfzone excessively we present here again some comparison with the full scale lip experiment of roelvink and reniers 1995 following durbin 2009 we assume that since eddy viscosity models are intended for prediction of mean flow mean flow data are a good basis for calibration in the lip1b experiment a jonswap wave spectrum with significant wave height h s 1 4 m and period t p 5 is generated that produces wave breaking at various position around a sandbar the horizontal grid resolution is 25 cm with 20 vertical levels refined near the bottom for more details on the lip experiment and model configuration the reader is referred to the previous reports cited above fig 5 shows a comparison of the model cross shore currents with data and the match appears very good throughout the complex morphology of the beach the waves break everywhere depending on their height but the breaking is more intense on the sandbar where the undertow has a strong shear and maximum intensity of about 30 cm s the mean absolute error of all experiments fig 5 bottom panel is 3 4 cm s rmse 2 3 cm close to the measurement error of 2 cm s as discussed in marchesiello et al 2021 these results show the validity of a rans approach for estimating breaking induced turbulence via parameterization and calculating lower frequency circulation explicitly the experiments with correction of the k ω model do not modify much the results but differences appear between the case with strong limitation λ 0 1 and moderate limitation λ 0 01 in the former the mean error with measurements is 4 9 cm while the latter is 3 7 cm close to 3 6 cm for the standard case the error in the strong limitation case is most significant in the offshore region of the sandbar where only the largest waves in the spectrum break implying intense but transient turbulent energy however the error reduces again at the most offshore location where turbulent energy is supposedly weakening the moderate limitation case performs well and even reduces the error relative to the standard model at a few inshore locations as well as at the most offshore location the λ value of 0 01 confirms to be a good choice for an optimal correction level by further decreasing λ the instability reappears the genericity of the λ value is unclear we have increased the power law of the limiting function section 2 3 in order to make the correction more robust and less sensitive to the limiting value the fact that the correction works for different gls model types seems to confirm this idea we believe that the method is robust at least for the problem of waves propagating in the nearshore zone we also verified that adding wind stress to the model in the idealized nearshore configuration of section 3 1 allows mixing of the stratified innershelf as expected i e the correction does not prevent the generation of mixing outside the surfzone if the forcing conditions are favorable 4 conclusion in this paper we build on the work of mayer and madsen 2000 and larsen and fuhrman 2018 to correct for overmixing in potential flow regions due to the linear instability of two equation closure systems we extend it to a more generic gls approach while testing for the first time the corrected closure model on a stratified nearshore problem the corrected gls formulation changes little in the surfzone where high vorticity and turbulence are expected while successfully addressing the problem of overmixing in regions where the waves are not breaking allowing the model to maintain stratification on the shelf provided the vorticity is less than 10 of the strain rate this work is a necessary step before engaging in full wave resolving 3d studies of surf shelf exchange that involve transient rip currents stratification and also earth rotation this will be possible due to advances in non hydrostatic free surface models which are emerging as a cost effective and accurate alternative to models that fully resolve the wave breaking process marchesiello et al 2021 credit authorship contribution statement patrick marchesiello conceptualization methodology software validation writing original draft simon treillou methodology software validation writing review editing declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests marchesiello patrick reports financial support was provided by french navy hydrographic and oceanographic service marchesiello patrick reports a relationship with french national research institute for sustainable development that includes employment acknowledgment this research has received support from a consortium of french research agencies as part of croco s development project insu gdr n 2014 named croco and from the french naval hydrographic and oceanographic service shom grant 20cp05 we thank dano roelvink for sharing the lip data apart from these all data were acquired by the authors and the croco source code is freely available at www croco ocean org both laboratory and modeling data are available upon request 
23761,we build on the work of mayer and madsen 2000 and larsen and fuhrman 2018 to correct for overmixing in wave potential flow regions due to the linear instability of two equation closure systems we extend it to the generic length scale gls turbulent closure approach while testing it for the first time on a stratified nearshore problem the modified turbulence equations affect only the nonbreaking wave potential flow region where the vorticity is less than 10 of the strain rate and thus can preserve stratification on the innershelf under favorable heat and wind forcing conditions this work is a necessary step before engaging in full 3d wave resolving studies of surf shelf exchange keywords nearshore wave resolving model wave induced mixing turbulence closure data availability data will be made available on request 1 introduction several 3d wave resolving free surface and terrain following models based on the reynolds averaged navier stokes rans equations have emerged for the nearshore zone e g swash zijlema et al 2011 nhwave derakhti et al 2016 and croco marchesiello et al 2021 in these models the explicit overturning of the free surface is excluded and the breaking wave is modeled with a single valued free surface which behaves as a dissipating bore despite the absence of explicit overturning replaced by parameterized turbulence these models can be accurate as well as computationally efficient in the study of waves and wave driven mean and transient circulation they can thus be applied to the entire 3d nearshore zone including the innershelf area and be coupled with biogeochemical or sediment transport models to explicitly simulate the effects of realistic short crested and asymmetric waves marchesiello et al 2021 2022 but stratification or earth rotation has not been included in these earlier works even though they may be important on the shelf and in the exchange mechanisms with the surfzone in previous studies marchesiello et al 2021 2022 the coastal and regional ocean community model croco was used with the k ω or k ϵ two equation turbulence closure models as part of a generic length scale gls method warner et al 2005 the k ω model generally performed best in the surfzone without modification of the standard model parameters from wilcox 1988 this is a conclusion shared by several modeling studies since mayer and madsen 2000 possibly due to a more accurate near wall treatment by k ω models devolder et al 2018 which is needed in the wall bounded turbulent shear flows of the surfzone trowbridge and elgar 2001 however both models suffer from overmixing in areas of potential irrotational flow typically in regions outside the surfzone where waves are not breaking this problem has long been identified lin and liu 1998 mayer and madsen 2000 bradford 2011 brown et al 2016 devolder et al 2018 larsen and fuhrman 2018 and understood as a kind of paradox where vorticity rather than strain rate should be the source of turbulence even though the role of strain rate is theoretically justified by the reynolds averaging process kato and launder 1993 the generation of turbulence in unexpected regions may thus appear to challenge the validity of boussinesq eddy viscosity hypothesis on the local relationship between mean and turbulent quantities schmitt 2007 which led to the introduction of a realizability constraint based on physical assumptions durbin 2009 recently however larsen and fuhrman 2018 building on earlier analysis of mayer and madsen 2000 that went rather unnoticed have better identified the causes of excessive vertical mixing in areas of potential flow revealing a linear instability of the two equation closure system it is the interaction between the two turbulent prognostic variables in the system that can exponentially amplify the generation of turbulent energy in unexpected regions the resulting overmixing can affect the amplitude of waves propagating to the nearshore zone and degrade the prediction of the wave breaking point lin and liu 1998 bradford 2000 mayer and madsen 2000 we found in the present study that perhaps more importantly overmixing can also affect the maintenance of stratification on the innershelf the last point is generally overlooked because 3d wave resolving models have been primarily focused on the surfzone without considering exchanges with the innershelf in contrast to recently emerging wave averaged model studies kumar and feddersen 2017 wang et al 2021 which do not suffer from the overmixing problem with advances in non hydrostatic models and computational resources the study of surf shelf exchange describing the fate of pollutants biological matter and sediments in the nearshore zone e g brown et al 2015 can now also be accessed by representing phase resolved waves and stratification however to proceed in this direction the problem of excessive mixing in nonbreaking waves must first be corrected note that in wave averaged models a certain amount of nonbreaking wave induced mixing is actually introduced by semi empirical formulas qiao et al 2004 ghantous and babanin 2014 it is much smaller than that caused by wave breaking but it reaches deeper and can affect the sea surface temperature of ocean general circulation models babanin 2023 the wave induced turbulence involved in these parameterizations assumes various processes such as interaction with pre existing turbulence here the generation of intense innershelf turbulent mixing appears to be more of an artifact of the turbulent models under conditions of nearly potential flow to address this specific issue we build on the works of mayer and madsen 2000 and larsen and fuhrman 2018 and extend it to a more generic approach within gls while testing for the first time the effect that a stabilized turbulence model has on innershelf stratification in an idealized nearshore configuration 2 modification of the turbulence closure model croco is a nonhydrostatic free surface and terrain following model developed around the regional oceanic modeling system shchepetkin and mcwilliams 2005 debreu et al 2012 its capabilities include high performance computation of high order accurate discretized equations and coupling with atmosphere waves biogeochemistry sediment and turbulence models it has been applied to a variety of configurations from regional and shelf circulations to very fine scale processes such as wave induced nearshore circulation marchesiello et al 2015 2021 2022 it thus appears naturally suited for addressing surf shelf exchange processes in a 3d rotating and stratified framework however innershelf stratification cannot be preserved if the turbulence closure equations produce excessive mixing as is often the case for rans wave resolving models larsen and fuhrman 2018 2 1 turbulence closure the treatment of breaking waves in croco involves both a shock capturing advection scheme weno5 and a second order turbulence closure system a k ϵ or k ω model solving the equations for turbulent kinetic energy k and dissipation ϵ or dissipation rate ω ϵ k 1 as part of a generic length scale gls method warner et al 2005 in the absence of buoyancy forcing the turbulence equations express a balance between transport diffusion shear production and dissipation 1 ρ k t ρ v k d k ρ p ϵ 2 ρ ϵ t ρ v ϵ d ϵ ρ ϵ k c ϵ 1 p c ϵ 2 ϵ 3 or ρ ω t ρ v ω d ω ρ ω k c ω 1 p c ω 2 ϵ the eddy viscosity ν t c μ l k 1 2 is derived from these equations with coefficient c μ dependent on stability functions and mixing length l k 3 2 ϵ 1 according to the boussinesq eddy viscosity hypothesis the shear production term for k is p 2 ν t s 2 with s 2 s i j s i j and s i j 1 2 u i x j u j x i using einstein notation which is the mean 3d strain rate tensor 1 1 the use of a 3d strain rate tensor differs from the gls approach presented in warner et al 2005 where energy production is only due to vertical shear of horizontal flows all turbulence model parameters are given in warner et al 2005 based on burchard et al 1998 for k ϵ and wilcox 1988 for k ω the turbulence models perform very well in the surfzone compared with flume experiments marchesiello et al 2021 2022 however as in previous studies they tend to produce excessive mixing in potential flow regions i e on the innershelf 2 2 correction in potential flow regions in many studies of breaking waves see the literature reviews by devolder et al 2018 larsen and fuhrman 2018 the predicted turbulence levels from two equation closures are much higher than what has actually been measured while many attempts were made for limiting mixing in supposed low turbulence regions larsen and fuhrman 2018 more specifically identify the causes of overmixing in areas of potential irrotational flow typical of nonbreaking waves the problem is common to the entire class of 2 equation closure models k ω k ϵ and variants because it is related to an unconditional linear instability of the equation system under low vorticity conditions for the k ω model a linearized homogeneous system suitable for the potential flow region beneath surface waves where a fixed strain rate s and negligible advection and diffusion can be assumed is 4 k t p ϵ 5 ω t ω k c ω 1 p c ω 2 ϵ noting that the specific dissipation rate ω is related to the energy dissipation ϵ by ϵ c μ k ω and that the eddy viscosity ν t k ω we can rewrite the system of equations in terms of only the variables k and ω 6 k t 2 k ω s 2 c μ k ω 7 ω t 2 c ω 1 s 2 c ω 2 c μ ω 2 considering as larsen and fuhrman 2018 that ω tends asymptotically to ω as ω t tends to 0 and replacing ω by ω in the k equation gives an exponential growth k e σ t with σ a fraction of s always positive i e the model is unconditionally unstable the instability of this system thus corresponds to an exponential increase in the turbulent energy k particularly in areas of potential flow where dissipation is never strong enough to control it larsen and fuhrman 2018 propose to curb the excessive energy production where the strain rate s is much greater than the vorticity ω defined by ω 2 ω i j ω i j with ω i j 1 2 u i x j u j x i the rotation rate tensor and demonstrate that this correction stabilizes the solution of the linear system as σ becomes negative the limitation is written as a majoration of ω in the viscosity ν t k ω used in the production term p 2 ν t s 2 with the following function 8 ω max 1 λ c ω 2 c ω 1 s 2 ω 2 ω where λ is a limitation parameter taken as 0 05 in larsen and fuhrman 2018 a similar condition is given for the k ϵ model ν t c μ k 2 ϵ where 9 ϵ max 1 λ c ϵ 2 c ϵ 1 s 2 ω 2 ϵ this correction is reminiscent of the one proposed for the stagnation point anomaly by kato and launder 1993 in a different study context where the turbulent energy production p 2 ν t s 2 is replaced by p 2 ν t s ω it also resembles that of mayer and madsen 2000 who take p 2 ν t ω 2 thus making production vanish in pure straining flow according to larsen and fuhrman 2018 these attempts significantly reduce the growth of instability in the potential flow region but do not resolve its formal unconditionality in addition in our numerical experiments the use of vorticity to replace all or part of the strain rate in the production term p tends to under represent the production of turbulent energy in the surfzone which is undesirable we thus concur with mayer and madsen 2000 and durbin 2009 that altering the strain rate tensor in p is not a satisfactory solution and we choose instead to majorate ϵ when computing ν t to control the instability of the turbulence equation system the latter solution may appear merely as an alternative to limiting the mixing length l as in bradford 2011 or the coefficient c μ as in realizable models shih et al 1995 but a limitation based on the ratio s ω makes it a more effective constraint on the instability fuhrman and li 2020 2 3 implementation in gls for the implementation in the croco gls code we impose the larsen and fuhrman 2018 limitation on the variable ϵ whatever the choice of closure since the turbulent dissipation ϵ is computed in all types of two equation models handled by gls starting from the k ω model we use the relation between ϵ and ω ϵ c μ k ω to assume a limitation of the form 10 ϵ max 1 λ c ω 2 c ω 1 s 2 ω 2 ϵ this expression is similar to eq 9 for the k ϵ apart from the factor c ω 2 c ω 2 replacing c ϵ 2 c ϵ 1 however the fraction c 2 c 1 does not vary much with the choice of closure model warner et al 2005 and is 1 2 1 5 considering the uncertainty in choosing λ we can propose a generic limitation 11 ϵ max 1 λ s 2 ω 2 ϵ in our numerical experiments next section this formulation controlled the growth of instability on the innershelf with λ 0 1 in this case the correction is triggered as soon as the vorticity is less than about 10 of the strain rate we also note that increasing the power law of the correction function stiffens the intensity of the correction below the 10 threshold and further decreases the mixing on the innershelf therefore in the following we will test the modified formulation 12 ϵ max 1 λ s 4 ω 4 ϵ with a baseline λ value of 0 01 3 numerical experiments 3 1 ideal test case the idealized model configuration is representative of a nearly planar beach representative of southern california kumar and feddersen 2017 or elsewhere with a smooth sandbar that triggers wave breaking and a cross shore circulation fig 1 details of the numerical implementation of croco for wave resolving simulations is given in marchesiello et al 2021 2022 only the configuration is presented here for simplicity we use a 2d x z channel with no alongshore dimension depths range from 7 m offshore to 1 m above mean sea level at the coast and a gentle sandbar is present about 100 m from shore the shoreline position is allowed to vary with swash oscillation based on a wetting drying scheme warner et al 2013 initially the water is at rest and the stratification is given by a temperature profile t z 8 10 e z 50 fig 2 roughly representing the mid latitude summer situation a monochromatic wave is generated at the offshore boundary with 0 5 m wave amplitude and 10 s period a non slip condition is imposed on the boundaries of the channel sidewalls the horizontal grid spacing is 0 5 m the vertical grid has 10 vertical levels refined at the surface and bottom and the model time step is 0 025 s for bed shear stress the logarithmic law of the wall is used with a roughness length of 1 mm the comparison of velocity u temperature t and eddy viscosity ν t at 30 min after the start of the simulation is presented in fig 2 it can be seen that without limitation the eddy viscosity produced by the k ω model increases to values well above 0 01 m 2 s and the high viscosity signal propagates from the surfzone to the shelf and gradually invade the entire model area fig 3 with the corrected closure system the viscosity does not develop outside the surfzone i e in the potential flow region where waves do not break in this case the viscosity in the surfzone has the expected structure and magnitude ν t 0 01 0 05 m 2 s battjes 1975 it is important to note that in the breaker zone the viscosity and related velocity profile is similar in both the standard and corrected cases showing that the limitation does not seem to affect the assumed turbulent zone much which is desirable but this will be verified against laboratory data in the next section the effect of vertical mixing on stratification is dramatic fig 2 at viscosity values of about 0 01 m 2 s and above mixing can break up the stratification in a matter of minutes τ m i x l 2 ν t with l limited by bottom depth with the original k ω model such high viscosity values reach the entire domain within half an hour and stratification breakdown follows everywhere on the contrary stratification is maintained on the innershelf outside the surfzone with the corrected turbulence model in this case the result becomes similar to those presented by wave averaged models lacking a process to generate a nonbreaking wave eddy viscosity kumar and feddersen 2017 and to nearshore observations sinnett and feddersen 2019 where spring summer stratification is preserved outside the surfzone as long as winds are sufficiently light next we checked that the corrections made on the turbulent dissipation ϵ are working not only for the k ω model but also for the k ϵ and the generic model of umlauf et al 2003 which is referred to as g e n model in warner et al 2005 fig 4 shows the result of the three models it confirms the report of marchesiello et al 2021 that the k ϵ model produces less turbulent energy than the k ω model with as a consequence an over prediction of velocity shear and undertow magnitude the g e n model gives intermediate results most importantly here the correction has worked in all cases and no exponential growth of turbulent energy and viscosity has occurred outside of the surfzone our correction of turbulent dissipation thus appears appropriate for a gls approach 3 2 surfzone verification with lip experiment validation of surfzone currents by comparison with flume experiments are provided in marchesiello et al 2021 2022 to make sure that the change of mixing due to the gls corrections does not affect the surfzone excessively we present here again some comparison with the full scale lip experiment of roelvink and reniers 1995 following durbin 2009 we assume that since eddy viscosity models are intended for prediction of mean flow mean flow data are a good basis for calibration in the lip1b experiment a jonswap wave spectrum with significant wave height h s 1 4 m and period t p 5 is generated that produces wave breaking at various position around a sandbar the horizontal grid resolution is 25 cm with 20 vertical levels refined near the bottom for more details on the lip experiment and model configuration the reader is referred to the previous reports cited above fig 5 shows a comparison of the model cross shore currents with data and the match appears very good throughout the complex morphology of the beach the waves break everywhere depending on their height but the breaking is more intense on the sandbar where the undertow has a strong shear and maximum intensity of about 30 cm s the mean absolute error of all experiments fig 5 bottom panel is 3 4 cm s rmse 2 3 cm close to the measurement error of 2 cm s as discussed in marchesiello et al 2021 these results show the validity of a rans approach for estimating breaking induced turbulence via parameterization and calculating lower frequency circulation explicitly the experiments with correction of the k ω model do not modify much the results but differences appear between the case with strong limitation λ 0 1 and moderate limitation λ 0 01 in the former the mean error with measurements is 4 9 cm while the latter is 3 7 cm close to 3 6 cm for the standard case the error in the strong limitation case is most significant in the offshore region of the sandbar where only the largest waves in the spectrum break implying intense but transient turbulent energy however the error reduces again at the most offshore location where turbulent energy is supposedly weakening the moderate limitation case performs well and even reduces the error relative to the standard model at a few inshore locations as well as at the most offshore location the λ value of 0 01 confirms to be a good choice for an optimal correction level by further decreasing λ the instability reappears the genericity of the λ value is unclear we have increased the power law of the limiting function section 2 3 in order to make the correction more robust and less sensitive to the limiting value the fact that the correction works for different gls model types seems to confirm this idea we believe that the method is robust at least for the problem of waves propagating in the nearshore zone we also verified that adding wind stress to the model in the idealized nearshore configuration of section 3 1 allows mixing of the stratified innershelf as expected i e the correction does not prevent the generation of mixing outside the surfzone if the forcing conditions are favorable 4 conclusion in this paper we build on the work of mayer and madsen 2000 and larsen and fuhrman 2018 to correct for overmixing in potential flow regions due to the linear instability of two equation closure systems we extend it to a more generic gls approach while testing for the first time the corrected closure model on a stratified nearshore problem the corrected gls formulation changes little in the surfzone where high vorticity and turbulence are expected while successfully addressing the problem of overmixing in regions where the waves are not breaking allowing the model to maintain stratification on the shelf provided the vorticity is less than 10 of the strain rate this work is a necessary step before engaging in full wave resolving 3d studies of surf shelf exchange that involve transient rip currents stratification and also earth rotation this will be possible due to advances in non hydrostatic free surface models which are emerging as a cost effective and accurate alternative to models that fully resolve the wave breaking process marchesiello et al 2021 credit authorship contribution statement patrick marchesiello conceptualization methodology software validation writing original draft simon treillou methodology software validation writing review editing declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests marchesiello patrick reports financial support was provided by french navy hydrographic and oceanographic service marchesiello patrick reports a relationship with french national research institute for sustainable development that includes employment acknowledgment this research has received support from a consortium of french research agencies as part of croco s development project insu gdr n 2014 named croco and from the french naval hydrographic and oceanographic service shom grant 20cp05 we thank dano roelvink for sharing the lip data apart from these all data were acquired by the authors and the croco source code is freely available at www croco ocean org both laboratory and modeling data are available upon request 
23762,conditional nonlinear optimal perturbation cnop is widely used in atmospheric and oceanic predictability studies solving cnop is essentially a nonlinear optimization problem with certain constraints one method to solve cnop is the intelligent optimization algorithm but it may not always obtain the satisfactory solution and efficiency because of the local exploitation of individual particles the bayesian optimization algorithm can avoid this local stagnation by building a global probability grasp for the optimization space of cnop nonetheless the limit of approximately 20 dimensional optimization space hampers its application in solving cnop of high dimensional numerical models to overcome this bottleneck we propose a paralleled embedding high dimensional bayesian optimization with additive gaussian kernels peboa algorithm which mainly consists of the feature extraction process and low dimensional optimization process in peboa a feature extraction method deepfe using the convolutional autoencoder with residual connections and customized constraint preserved loss function is designed which compresses 10 million dimensional data to relatively low dimensional search spaces from tens to hundreds of dimensions then we propose the strategy of additive gaussian kernels with which the bayesian optimization algorithm can optimize in relatively low dimensional search spaces concretely a kernel handles a subspace and a subspace can handle dimensions approximately not more than 20 additionally the modified acquisition function can sample multiple candidates simultaneously with the aim of accelerating the optimization process we apply peboa to solve cnop of regional ocean modeling system roms for identifying optimal initial errors of upstream kuroshio transport variation which is a 10 million dimensional and time consuming problem the computational performance of peboa and physical mechanisms of the obtained cnop are analyzed experimental results indicate that deepfe excels principal component analysis pca in terms of relative error ratio the magnitude of objective function values and the obtained cnop pattern besides compared to deepfe based particle swarm optimization peboa has better solving efficiency with 3 4 larger objective function values and 2 2 times greater likelihood of obtaining the valid cnop furthermore the modified acquisition function reduces computation time by about 71 0 with four cores the physical mechanism analysis shows that cnops obtained by peboa are almost identical to the adjoint method which can cause an anomalous increase decrease in upstream kuroshio transport and maintain physics consistency keywords autoencoder based bayesian optimization additive gaussian kernels paralleled acquisition function cnop regional ocean modeling system upstream kuroshio transport data availability data will be made available on request 1 introduction conditional nonlinear optimal perturbation cnop initially proposed by mu et al 2003 stands for the initial perturbation that leads to the largest nonlinear evolution at the prediction time in a given constraint so far it has been broadly adopted to explore the nonlinear dynamics of atmospheric and oceanic systems mu and duan 2005 duan and mu 2009 study the predictability and sensitivity of events dijkstra and viebahn 2015 wang et al 2020 zhang et al 2017b liu et al 2018 tao et al 2017 2019 carry out ensemble forecasts huo et al 2019 huo and duan 2019 duan and huo 2016 and determine targeted observations dai et al 2019 liang et al 2019 mu et al 2015c hu et al 2021 solving cnop is essentially a nonlinear optimization problem with certain constraints there have been some works on methods for solving cnop one of the most popular approaches is the intelligent optimization algorithm a class of optimization algorithms mimicking natural phenomena its advantages lie in that it is adjoint free and is capable of overcoming the discontinuity problem for instance zheng et al 2017 and yuan et al 2015b applied particle swarm optimization pso and some pso variants to solve cnop of a two dimensional ikeda model and a simplified zebiak cane zc model however the intelligent optimization algorithm with local exploitation of individual particles tends to fall into local optima when encountering complex multimodal functions ye et al 2013 larsen et al 2016 xu et al 2015 which may result in obtaining unsatisfactory solutions and efficiency the bayesian optimization algorithm močkus 1975 mockus 1989 consisting of the surrogate model and the acquisition function can avoid this local stagnation by building a global probability grasp for the optimization space of cnop its capability of globally optimizing computationally expensive black box functions makes it outstanding among optimization methods up to now it has been utilized to solve various problems such as robotics nogueira et al 2016 junge et al 2020 martinez cantin 2017 jaquier et al 2020 calandra et al 2016 reinforcement learning barsce et al 2017 imani and ghoreishi 2022 balakrishnan et al 2020 young et al 2020 brochu et al 2010 and the design of engineering systems palar et al 2020 wang et al 2021 hebbal et al 2021 pelamatti et al 2021 lam et al 2018 nonetheless the limit of approximately 20 dimensional optimization space wang et al 2022 li et al 2016 wang et al 2016b nayebi et al 2019 caused by the restricted expression ability of the gaussian process based surrogate model hampers its application in solving cnop of high dimensional numerical models to overcome this bottleneck we propose a paralleled embedding high dimensional bayesian optimization with additive gaussian kernels peboa algorithm which mainly consists of the feature extraction process and low dimensional optimization process specifically in peboa a feature extraction method deepfe using the convolutional autoencoder ae baldi 2012 with residual connections and customized loss function is designed as the embedding component to reduce dimensions nonlinearly with retaining as much information as possible the deepfe allows for searching in low dimensional search spaces and reconstructing solutions to calculate the objective function values unlike the commonly used linear feature extraction method mu et al 2015a yang et al 2020 xu et al 2022 such as principal component analysis pca jolliffe 2005 and its variant candes et al 2009 the deepfe yields less information loss since pca finds a fixed low dimensional hyperplane and only provides linear transformations for data lee 2011 while the deepfe involving neural networks discovers sophisticated relations of hidden layers and learns a compressed representation of complicated data effectively unlike ordinary ae used in the literature yuan et al 2022 dealing with about 100 thousand dimensional double gyre variation problem the deepfe which can handle 10 million dimensional data has more elaborate structures alkhayrat et al 2020 such as more layers and longer widths with residual connections to avoid the vanishing gradients he et al 2015 and presents a customized loss function with the constraint preserved penalty term in this way the deepfe compresses 10 million dimensional data to relatively low dimensional search spaces from tens to hundreds of dimensions then we propose the strategy of additive gaussian kernels with which the bayesian optimization algorithm can optimize in relatively low dimensional search spaces since deepfe is not strong enough to compress the ultra high dimensional bayesian optimization to an effect suitable for optimization i e less than approximately 20 dimensions this strategy is inspired by the studies on structural assumptions for the high dimensional extension of bayesian optimization researchers kandasamy et al 2015 han et al 2020 wang et al 2018a b imposed additive structures on problems assuming that the objective function can be partitioned into functions with small disjoint sets of dimensions experiments revealed that this method outperforms standard bayesian optimization even on non additive functions therefore we propose the strategy of additive gaussian kernels to build the gaussian process for each variable concretely a kernel handles a subspace and a subspace can handle dimensions approximately not more than 20 additionally to speed up the optimization process we modify the acquisition function to sample multiple candidates simultaneously instead of the sequential sampling of the standard bayesian optimization algorithm the modified acquisition function combines the upper confidence bound strategy and the pure exploration strategy contal et al 2013 allowing for evaluating points in parallel in summary we propose the peboa algorithm for solving high dimensional cnop to overcome the problem of intelligent optimization algorithms falling into local optima and to exceed the 20 dimensional limit of ordinary bayesian optimization algorithm we apply peboa to solve cnop of regional ocean modeling system roms for identifying optimal initial errors of upstream kuroshio transport variation zhang et al 2016 which is a 10 million dimensional and time consuming problem our contributions can be summarized as follows we design the deepfe using the convolutional ae with residual connections and customized constraint preserved loss function in peboa the deepfe can compress 10 million dimensional data to relatively low dimensional search spaces from tens to hundreds of dimensions with superior feature learning performance and reconstruction capabilities in peboa we propose the strategy of additive gaussian kernels with which the bayesian optimization algorithm can optimize in relatively low dimensional search spaces concretely a kernel handles a subspace and a subspace can handle dimensions approximately not more than 20 to speed up the optimization process we modify the acquisition function which can sample multiple candidates simultaneously instead of sequential sampling peboa is applied to solve cnop of the roms model for identifying optimal initial errors of upstream kuroshio transport first compared to pca we evaluate deepfe from the relative error ratio the magnitude of objective function values and the obtained cnop pattern second we compare peboa and deepfe based pso deepfepso in terms of the quality of obtained cnop and solving efficiency the modified acquisition function is also assessed cnops obtained by peboa are almost identical to the adjoint method which can cause an anomalous increase decrease of upstream kuroshio transport and maintain physics consistency the rest of this paper is organized as follows section 2 first introduces the concepts of cnop and approaches for solving cnop then briefly summarizes feature extraction methods and pso followed by the theory of bayesian optimization algorithm the components of the proposed peboa are described in section 3 section 4 illustrates the case of upstream kuroshio transport in section 5 we figure out the parameter settings of deepfe compare deepfe with pca compare peboa with deepfepso analyze the performance of our proposed method analyze three acquisition functions conduct the gain analysis and probe into the physical mechanisms section 6 presents the conclusion and future work 2 related works 2 1 cnop cnop represents the initial perturbation that leads to the largest nonlinear evolution at the prediction time in a given constraint the mathematical definition of cnop can be described in eq 1 x 0 denotes the optimal initial perturbation x 0 is the initial state vector m t denotes the nonlinear propagation operator that propagates the evolution equation from the initial moment to moment t which is calculated via numerical models to be precise the roms model is used in this paper x 0 c δ denotes the given constraint of the initial perturbation and o b j denotes the objective function usually the ℓ 2 norm 1 x 0 argmax x 0 c δ f x 0 argmax x 0 c δ m t x 0 x 0 m t x 0 o b j a great deal of previous research has focused on approaches for solving cnop existing approaches can be roughly classified into two categories i e gradient based ones and intelligent optimization algorithm involved ones based on whether or not the gradient information is utilized a detailed and organized overview of existing work on solving cnop is covered to offer a clear structure for the extensive literature reported in this section as illustrated in table 1 gradient based methods considering rewrite f x 0 of eq 1 see eq 2 the gradient of f x 0 can be defined as eq 3 in eq 3 h denotes the tangent linear matrix of the nonlinear model m and h t known as the jacobian matrix of f x 0 is the transpose of matrix h 2 f x 0 min x 0 c δ m t x 0 x 0 m t x 0 2 3 f x 0 2 h t m t x 0 x 0 m t x 0 the gradient can be calculated via adjoint models and then gradient descent optimization algorithms such as spectral projected gradient spg birgin et al 2001 2000 and sequential quadratic programming sqp powell 1983 can be utilized to solve cnop for instance adjoint models with spg have been employed to solve cnop in a roms model of kuroshio intrusion liang et al 2019 a weather research forecasting wrf model of the tropical cyclone wang et al 2011 mu et al 2009 etc tang et al 2012 jiang 2006 sun et al 2010 adjoint models with sqp have been applied in a two dimensional quasigeostrophic model mu and zhang 2006 a theoretical ocean atmosphere coupled model duan et al 2004 etc sun et al 2010 mu and duan 2005 however there are two limits to how far such type of approach can be taken one is that most complicated numerical models do not contain adjoint models on their own and it is challenging and time consuming to develop adjoint models the other is the on off switch problem zheng et al 2012 concretely when the gradient of the objective function with respect to variables does not exist the gradient information will be computed incorrectly which is known as a non smooth optimization problem as for the former limitations some researchers proposed adjoint free methods to calculate the gradient such as the monte carlo ensemble projection algorithm wang and tan 2010 and the singular vector decomposition svd based ensemble projection algorithm yuan et al 2015a chen et al 2015 these methods bypass the construction of adjoint models but the on off switch problem remains intelligent optimization algorithm involved methods intelligent optimization algorithms which do not require gradient information are capable of handling non smooth strongly nonlinear optimization problems they are a class of optimization algorithms mimicking some natural phenomena such as pso genetic algorithm ga simulated annealing sa and wolf search algorithm wsa zheng et al 2012 adopted ga in modified lorenz models to study predictability moreover zheng et al 2017 and yuan et al 2015b applied pso and some pso variants to solve cnop of a two dimensional ikeda model and a simplified zc model similarly ren et al 2016 utilized a modified artificial bee colony algorithm mabc in a simplified zc model these cases confirmed that intelligent optimization algorithms could overcome the discontinuity problem nevertheless such cases were only conducted in low dimensional unimodal or simple multimodal objective functions difficulties arise when applying intelligent optimization algorithms to large scale numerical models and complex multimodal objective functions one popular approach to overcome the curse of dimensionality is combining feature extraction methods such as pca and its variant robust principal component analysis rpca with intelligent optimization algorithms mu et al 2015a put forward ppso to solve cnop in complicated air sea coupled numerical models and the ppso framework has been frequently used yang et al 2020 xu et al 2022 pca rpca based ga has been used in medium complexity zc models and fifth generation mesoscale models mm5 mu et al 2015b zhang et al 2017a wen et al 2015 besides the idea of hybridization emerged mu et al 2019a 2022 applied the pca based ga and pso hybrid algorithm in north atlantic oscillation nao with the community earth system model cesm likewise researchers zhang et al 2018 mu et al 2019c proposed the pca based adaptive cooperative coevolution of pso and wsa acpw to identify sensitive regions of tropical cyclone adaptive observations although this strategy alleviates the difficulty of high dimensional optimization search to some extent the information loss incurred by fixed subspaces of pca exists resulting in unsatisfactory results to deal with this drawback yuan et al 2022 proposed feature extraction based intelligent algorithm framework with neural network fnnia to solve double gyre variation in roms where well trained neural network based feature extraction components including basic ae and generative adversarial network gan are involved experimental results show that the fnnia framework has better objective function value and cnop pattern than the framework without neural networks nevertheless it is necessary to investigate suitable model structures and the applicability of neural network based feature extraction methods in higher dimensional meteorological problems in addition the above methods cannot tackle the problem of intelligent optimization algorithms falling into local optima when encountering complex multimodal functions 2 2 feature extraction methods the objective of feature extraction is to construct feature vectors with dimensionality smaller than the original dataset pca jolliffe 2005 is one of the most frequently used linear feature extraction techniques tadić et al 2019 geng et al 2021 duforet frebourg et al 2016 touzani et al 2020 arivudainambi et al 2019 chen and tian 2022 giuliani 2017 the objective of pca is to find a set of vector bases that maximize the data s projection in the direction of vector bases mathematically pca obtains this set of vector basis via the eigendecomposition of positive semi definite matrixes or the svd of rectangular matrices abdi and williams 2010 as for the implementation researchers have presented detailed tutorials shlens 2014 reris and brooks 2015 rodionova et al 2021 the strengths of pca are that it is easily understood and computationally simple however the drawback lies in its fixed latent subspace that only considers linear correlations that is to say that non principal components with relatively small eigenvalues are likely to contain critical information and discarding them may have a negative impact on the subsequent optimization process the rapid proliferation of deep learning technology gives rise to new ideas chen et al 2014 zhu et al 2014 gensler et al 2016 for feature extraction since some neural network architectures can construct latent subspaces and map efficiently between low dimensional and original spaces ae architecture was first proposed by rumelhart et al 1988 in 1986 with the purpose of learning potential representations of data as described in fig 1 it is composed of two components the encoder and the decoder the former encodes the input data to the low dimensional representation the latter reconstructs the low dimensional representation so that the output is as similar as possible to the input the formal definition baldi 2012 of ae is written as 4 argmin e n c o d e r d e c o d e r l x d e c o d e r e n c o d e r x where e n c o d e r encodes the input x r h i g h d i m to latent representation z r l o w d i m and d e c o d e r decodes z to x ˆ r h i g h d i m h i g h d i m and l o w d i m stand for the dimensionality of the original and compressed space respectively ae learns the e n c o d e r and d e c o d e r by minimizing the loss function denoted by l on the condition that ae has linear activation functions and only has one sigmoid hidden layer it is almost identical to pca therefore researchers bank et al 2021 consider ae as a nonlinear generalization of pca they differ in that pca finds a fixed low dimensional hyperplane and only provides linear transformations for data while ae discovers sophisticated relations of hidden layers and learns a compressed representation of complicated data studies have revealed that compared to pca ae excels in reconstruction due to its nonlinearity for example wang et al 2016a compared ae with state of the art dimensionality reduction methods and the results indicate that ae can learn something different and meaningful on some real datasets like the mnist dataset besides alsenan et al 2020 conducted experiments on qsar dataset and found that ae outperforms pca on the accuracy measure by comparing the merits and drawbacks of pca and ae it can be seen that although pca has fewer parameters and accordingly requires less computation its disadvantage lies in the large amount of information loss in the feature extraction process which is mainly due to two reasons 1 on the one hand pca only considers linear correlation when constructing the low dimensional feature space and ignores the nonlinear correlations of corresponding meteorological data 2 on the other hand non principal components with relatively small eigenvalues may contain key information which is conducive to optimization and directly discarding them by pca may have a negative impact on the subsequent optimization process ae can overcome the above shortcomings by learning the nonlinear relationships of data as well as effectively retaining the critical feature information in brief ae shows superior data compression ability and data reconstruction ability although ae requires higher hardware costs and time costs these shortcomings are no longer the major influencing factors with the improvement of computing power based on the above analysis it can be determined that ae s better feature extraction performance enables it to be a more effective feature extraction component in solving cnop recent years have witnessed a growing academic interest in ae for feature extraction back in 2006 researchers hinton and salakhutdinov 2006 came up with a novel idea of using ae for feature extraction on very large datasets afterward there are studies on the general ae framework for feature extraction wang et al 2014 and ae has proven to be a promising tool for feature extraction in network anomaly detection chen et al 2018 fault diagnosis san martin et al 2019 hyperspectral images ramamurthy et al 2020 and yearly electricity loads ryu et al 2020 nevertheless a suitable network structure such as the number of layers and the widths should be customized for data with different domains and meanings alkhayrat et al 2020 and so far there have been few studies using ae for feature extraction of high dimensional meteorological data such as kuroshio in roms 2 3 pso kennedy and eberhart 1995 proposed pso algorithm for solving nonlinear functions inspired by the social behaviors of animals in pso algorithm each particle in the population has a corresponding position velocity and fitness value objective function value besides the personal best positions and the global best position are also recorded during optimization the first step of pso is to initialize particles positions and velocities randomly after initialization the fitness value of each particle is calculated followed by updating the personal best position of the individual particle and the global best position of the particle swarm the velocities and positions of the particles are then updated according to eqs 5 and 6 where v e l k i denotes the velocity of the k th particle in the i th iteration p o s k i denotes the position of the k th particle in the i th iteration p b e s t k denotes the personal best position of the k th particle g b e s t denotes the global best position the total number of particles are l c 1 and c 2 are acceleration constants r a n d 0 1 represents random numbers in the range 0 to 1 and w denotes the inertia weight according to previous studies the inertia weight is first set high and decreases gradually with iterations shi and eberhart 1998 bansal et al 2011 this process is looped until the termination condition is satisfied the termination condition is usually related to the number of iterations or the objective function value 5 v e l k i w v e l k i 1 c 1 r a n d 0 1 p b e s t k p o s k i 1 c 2 r a n d 0 1 g b e s t p o s k i 1 k 1 2 l 6 pos k i p o s k i 1 v e l k i k 1 2 l it has been found that pso requires few parameters to be tuned and is easy to implement due to this simplicity pso algorithm is widely used in environmental kashani et al 2021 alrashidi and el hawary 2006 vishnu and t k 2020 naderi et al 2019 medical pervaiz et al 2021 and financial pradeepkumar and ravi 2017 fields nonetheless pso is likely to converge prematurely by falling into local optima when encountering complex multi modal functions ye et al 2013 larsen et al 2016 xu et al 2015 to be specific the personal best position of the individual particle and the global best position of the particle swarm may tend to converge to the local optimal position resulting in obtaining wrong optimization results and the failure of trading off between the global exploration and local exploitation even though researchers have introduced some modifications or improvements isiet and gadala 2019 liu et al 2020 tian et al 2019 zeng et al 2022 to pso premature convergence of pso remains a major challenge currently houssein et al 2021 2 4 bayesian optimization bayesian optimization is a technique for expensive black box global optimization mockus 1989 močkus 1975 bayesian optimization has two components the surrogate model and the acquisition function the surrogate model is a probabilistic model that approximates the objective function via a gaussian process regression a gaussian process featured by a prior mean function μ and a covariance function κ offers a distribution over functions f gp μ κ w l o g let μ 0 considering a collection of data pairs d n x f x where x x 1 x 2 x n is a set of input points f x f x 1 f x 2 f x n is the corresponding function values and n is the number of samples with new input data x the joint distribution of f x and f x is 7 f x f x n 0 κ x x κ x x κ x x κ x x then the conditional distribution denoted by p f x x x f x is a multivariant gaussian distribution and can be formulated as 8 p f x x x f x n μ x σ 2 x where 9 μ x κ x x κ x x 1 f x 10 σ 2 x κ x x κ x x κ x x 1 κ x x the squared exponential kernel duvenaud 2014 is frequently adopted as the covariance function and its hyperparameters including the length scale the signal variance and the noise variance are chosen via maximizing the log marginal likelihood the acquisition function denoted by α x is able to guide the search to reach the optimum since it determines the most promising point to sample during iterations see eq 11 and achieves the balance between exploration and exploitation commonly used acquisition functions frazier 2018 are expected improvement ei and upper confidence bound ucb 11 x argmax x x α x ei calculates the expected improvement associated with the current best function value denoted by f 12 α x e i x e max 0 f x f μ x f φ μ x f σ x σ x ϕ μ x f σ x where e φ and ϕ denote the expectation value the gaussian cdf and the gaussian pdf respectively ucb focuses on sampling points with high mean and high standard deviation 13 α x u c b x μ x β 1 2 σ x where β is a coefficient to weigh up exploitation the μ x term and exploration the σ x term β is first set high and then decreases gradually with iterations to enable more exploration in the early stage and more exploitation in the later stage algorithm 1 exhibits the pseudocode of the bayesian optimization algorithm in spite of its pervasive application to low dimensional black box problems less than approximately 20 dimensions bayesian optimization has encountered difficulties when extended to high dimensional domains as the search space grows exponentially with the dimensionality wang et al 2022 to tackle this issue researchers assuming that the objective function only depends on a handful of variables utilized the finite difference sequential likelihood ratio test fdt and the gp sequential likelihood ratio test gpt to determine the most contributing variables in high dimensional bayesian optimization chen et al 2012 this approach is usually summarized as the selection of variables besides the strategy of low dimensional embeddings has emerged such as random embedding bayesian optimization rembo wang et al 2016b its variants qian et al 2016 binois et al 2020 nayebi et al 2019 and so on moriconi et al 2020 dhamala et al 2020 and grosnit et al 2021 assuming that the data has underlying representations these methods can perform bayesian optimization in the low dimensional space and compute the objective function values in the original space moreover there has been a tactic based on certain structural assumptions for instance researchers kandasamy et al 2015 han et al 2020 wang et al 2018a b imposed additive structures on problems assuming that the objective function can be partitioned into functions with small disjoint sets of dimensions experiments revealed that this method outperforms standard bayesian optimization even on non additive functions some researchers oh et al 2019 also proposed bayesian optimization with cylindrical kernels to leverage cylindrical transformations 3 methods as presented in fig 2 the embedding optimization algorithm for solving cnop consists of four components which are data gathering feature extraction optimization in the low dimensional feature space and result acquisition among them the feature extraction step and the optimization step are the core parts the process of each step is explained in detail as follows step 1 obtain data the first step is the data gathering process where datasets with perturbation characteristics are usually constructed based on the specific numerical model constraint norm and physical mechanisms of the meteorological problem to be studied this step is the preparation for the subsequent feature extraction step the obtained samples are denoted by x x 1 x 2 x n where n is the total number of samples x n denotes the n th sample n 1 n and x n r h i g h d i m h i g h d i m is the dimensionality of the original space step 2 conduct feature extraction as for the feature extraction step we design a well structured deepfe model and compare it with pca to demonstrate its superior reconstruction performance step 2 1 preprocess the first procedure in this part is non dimensionalization conesa et al 2016 where pca utilizes standardization to make the mean and standard deviation of all data equal to zero and one respectively and the deepfe utilizes min max scaling of the range 0 1 step 2 2 generate low dimensional representation pca performs mappings between the original and low dimensional space through a fixed feature matrix while the deepfe performs mappings by an adequately trained encoder and decoder c p s and r c s are constructed in this step as shown in eqs 14 and 15 c p s is responsible for compressing the original data to the low dimensional space and r c s is responsible for reconstructing the low dimensional data to the original high dimensional space 14 z n c p s x n 15 x ˆ n r c s z n step 3 perform optimization in the low dimensional feature space regarding the third step we employ the paralleled bayesian optimization with additive kernels and make a comparison with pso after completing the feature extraction process step 2 the optimization of high dimensional cnop see eq 1 can be converted into the low dimensional optimization problem see eq 16 the reduction of the search space can significantly improve the search efficiency 16 x 0 r c s z 0 argmax r c s z 0 c δ f r c s z 0 step 3 1 sample a potential solution pso and our proposed peboa differ in the rules of sampling the potential solution in the low dimensional space specifically peboa relies on the acquisition function to determine the most promising candidate point in each iteration see eq 11 while pso relies on the personal best positions and the global best position to update the velocity and position of each particle see eqs 5 and 6 step 3 2 reconstruct the low dimensional point to the original space this step involves the feature matrix of pca or the decoder of the deepfe to reconstruct the selected low dimensional candidate point to the original space for the calculation of the objective function value step 3 3 compute the objective function value after conducting step 3 2 the selected candidate point denoted by x ˆ is superimposed on the reference state at the initial moment to generate the corresponding initial field and then the roms model is called to obtain the state vector o t at the prediction time shown in eq 17 the objective function value can be calculated using eq 18 17 o t m t x 0 x ˆ 18 f x ˆ o t m t x 0 o b j step 4 output cnop finally the optimal solution in the low dimensional feature space is reconstructed to the original space to obtain the cnop since the concepts of pca and pso have been mentioned in section 2 and the literature mu et al 2015a offers implementation of pca based pso the remainder focuses on peboa including descriptions of its components and the pseudocode 3 1 deepfe section 2 has introduced the basic concepts and applications of ae thus this section elaborates on deepfe s specific structure and details in the proposed peboa 3 1 1 model structures in studying the relations between network structures and performance researchers found that the deeper the network gets the more significant the negative impacts on representation learning performance become he et al 2015 this issue occurs due to the vanishing gradients of deep neural networks and can be addressed by incorporating residual connections zaeemzadeh et al 2020 or skip connections huang et al 2018 srivastava et al 2015 ronneberger et al 2015 thus we employ the residual network as the backbone for both the encoder and the decoder since it allows for a larger network without feature learning performance degradation fig 3 presents an overview of the proposed deepfe we design five models each is composed of the encoder and decoder for the five variables v u t e m p s a l t and z e t a refer to table 2 for the meaning and dimensionality respectively with the aim that components in the encoder and decoder correspond as closely as possible to each other these five models are trained together by the loss function involving five variables see appendix the designed ae structures of two variables z e t a and v are shown in fig 4 because the other three variables structures are nearly the same as v s ae structures we do not depict them graphically z e t a s ae structure is distinguished from others owing to the dimensionality i e the input channels for zeta are 1 while the input channels for the other four variables are 32 in addition z e t a l o w d i m and v l o w d i m in fig 4 indicate the dimensionality of z e t a s and v s hidden representation respectively and the meanings of u l o w d i m t e m p l o w d i m and s a l t l o w d i m are the same as above moreover z e t a h i g h d i m in fig 4 indicates the actual dimensionality of zeta and the meanings of v h i g h d i m u h i g h d i m t e m p h i g h d i m and s a l t h i g h d i m are the same as above the number of repeats of the basic block denoted by r e p e a t n u m in fig 4 determines the complexity of the models considering that the dimensionality of z e t a is smaller than others r e p e a t n u m in the ae structure of z e t a is also set lower and in other variables structures r e p e a t n u m is set higher to achieve satisfactory feature learning note that the last layer uses the sigmoid function as the activation function because in pre processing step the input data is scaled to the range from 0 to 1 this activation function ensures that the output data is also in this range as illustrated in fig 5a the encoder modules consist of three types of blocks the fc block with a series of full connection operations the conv block including two dimensional convolution batch normalization and the activation function relu and the basic encoder block including several conv blocks and residual connections represented by operator where the first connection is used to increase the number of channels in the network and the rest of connection operations do not change the number of channels in the network fig 5b displays the decoder modules they consist of three types of blocks the fc inverse block with a series of full connection operations that correspond to the fc block in the encoder modules the conv transpose block including two dimensional deconvolution batch normalization and the activation function relu and the basic decoder block including several conv transpose blocks and residual connections where the first connection is used to decrease the number of channels in the network and the rest of connection operations do not change the number of channels in the network 3 1 2 loss function since we want the input and output of the deepfe model to be as consistent as possible the reconstruction error the mean squared error is introduced into the loss function 19 m s e 1 d j 1 d x i j x i j 2 where x i j is the j th dimension of the i th input sample x i j is the j th dimension of the i th output sample and d denotes the dimensionality of samples besides to make sure the output namely the initial perturbation satisfies the constraint of cnop refer to eq 1 in section 2 a constraint preserved penalty term is added to the loss function 20 p e n a l t y λ p n t max 0 x ˆ i c 2 δ 2 where x i is the i th output sample c denotes the constraint norm δ 2 is the constraint value and λ p n t is a constant coefficient this item incorporates the physical constraint whose purpose is to keep the reconstructed data within the boundary of the constraint additionally the ℓ 2 regularization is brought into the loss function to prevent overfitting and enhance generalization 21 r e g λ r e g i 1 ɛ θ i 2 where θ i represents the i th parameters of the neural network ɛ denotes the total number of parameters and λ r e g is a constant coefficient in summary the loss function is written as eq 22 22 l θ m s e p e n a l t y r e g 3 2 bayesian optimization with additive gaussian kernels the fundamental principles of bayesian optimization are introduced in section 2 4 we extend it to high dimensional problems assuming that the objective function f decomposes into the following form 23 f x f v v f u u f t e m p t e m p f s a l t s a l t f z e t a z e t a where v u t e m p s a l t and z e t a denote the five variables that x has see the case study in section 4 in addition to the standard assumption that f is sampled from a gaussian process the assumption is made that each f v a r v a r is also sampled from an independent gaussian process denoted by f v a r gp μ v a r κ v a r where v a r stands for the element of the set v u t e m p s a l t z e t a therefore μ x and κ x x can be rewritten as 24 μ x μ v v μ u u μ t e m p t e m p μ s a l t s a l t μ z e t a z e t a 25 κ x x κ v v v κ u u u κ t e m p t e m p t e m p κ s a l t s a l t s a l t κ z e t a z e t a z e t a taking κ v as an example it only operates on one variable v in small dimensions while the kernel of a standard bayesian optimization works on all variables in large dimensions considering d n the same as d n mentioned in section 2 4 with a variable e g v of new input data the joint distribution of f x and f v v is 26 f x f v v n 0 κ x x κ v v v κ v v v κ v v v where v v 1 v 2 v n then the posterior for f v v can be written as 27 p f v v v x f x n μ v v σ 2 v v where 28 μ v v κ v v v κ x x 1 f x 29 σ 2 v v κ v v v κ v v v κ x x 1 κ v v v as for the acquisition function we modify ucb to an additive version namely ucb add which is a sum of functions on orthogonal domains ucb add is formulated as follows 30 u c b x μ x β 1 2 v a r v u t e m p s a l t z e t a σ v a r v a r u c b v v u c b u u u c b t e m p t e m p u c b s a l t s a l t u c b z e t a z e t a where u c b v v μ v v β 1 2 σ v v and the other four items are expressed in the same way as u c b v v note that for simplicity we write σ v v σ u u σ t e m p t e m p σ s a l t s a l t σ z e t a z e t a as v a r v u t e m p s a l t z e t a σ v a r v a r instead of maximizing u c b x in all dimensions we can maximize u c b x by maximizing each u c b v a r v a r in a few dimensions kandasamy et al 2015 which saves computational resources and achieve the desirable accuracy 3 3 parallelization this section introduces the parallelized version of ucb add ucb add pe which selects several points simultaneously rather than a single point for evaluation via the modified acquisition function we adopt the idea of combining the upper confidence bound strategy and the pure exploration strategy called gp ucb pe in the literature contal et al 2013 and propose ucb add pe to make it applicable to additive structures briefly speaking ucb add pe selects the first query point x 1 via ucb add see eq 31 and the remaining points x 2 x 3 x q via maximizing the updated variance see eq 32 the number of parallelization or the batch size is denoted by q 31 x 1 argmax x x u c b x 32 x m argmax x r v a r v u t e m p s a l t z e t a σ m v a r v a r m 2 3 q in eq 32 σ m v a r v a r represents the updated variance for a specific variable after selecting x 1 x 2 x m 1 points and r represents the relevant region the formal definition of r is illustrated as 33 r x x μ x 2 β t 1 1 2 v a r v u t e m p s a l t z e t a σ v a r v a r l c b where β t 1 denotes the value of the parameter β in the next iteration and the definition of l c b is illustrated below 34 l c b max x x l c b x 35 l c b x μ x β 1 2 v a r v u t e m p s a l t z e t a σ v a r v a r l c b denotes the modified lower confidence bound with additive structures in accordance with u c b the purpose of incorporating β t 1 rather than the current value β is to ensure that current sampling will have an impact on the selection of points in the next iteration as can be seen from the above equations r not only discards regions where the optimum does not fall with high probability but also contains regions where the maximum position of u c b x in the next iteration belongs to with high probability contal et al 2013 literature contal et al 2013 has offered theoretical guarantees for gp ucb pe and algorithm 2 illustrates the pseudocode of peboa 4 case study the roms model for the upstream kuroshio transport in this section we introduce the cnop concepts for the roms model of the upstream kuroshio transport including a brief description of the upstream kuroshio transport the simulation of the roms model and the exact mathematical forms of the objective function and constraint norm kuroshio originates from eastern luzon and has vital impacts on global climate change mid and high latitude sea air interactions marine ecosystems and fisheries zhang 2002 tsukamoto 2006 kwon et al 2010 wu et al 2012 qiu et al 2014 the upstream kuroshio which ranges from the origin to southern taiwan is subject to physical processes such as enso asian monsoon and mesoscale eddies yaremchuk and qu 2004 qiu and lukas 1996 lien et al 2014 kim et al 2004 chiang et al 2015 resulting in a rapid decrease of transport in autumn accurate prediction of this seasonal decrease is essential to improve the forecasting capability of kuroshio transport nevertheless the impact of the above mentioned physical processes causes considerable uncertainty in the forecast so it is necessary to conduct a predictability study to identify the initial error perturbation that has the largest influence on the forecast uncertainty of this seasonal decline by utilizing the cnop method according to the literature zhang et al 2016 roms can be employed to simulate the transport variation of the upstream kuroshio as displayed in fig 6a it models the seasonal transport reduction of upstream kuroshio in a region with a latitude ranging from 0 3 n to 29 n a longitude ranging from 112 e to 162 e a horizontal resolution of 0 125 and 32 sigma levels in this region the state vector consists of 5 parameters v u t e m p s a l t and z e t a the meaning and dimensionality of these parameters are listed in table 2 the reasons for selecting these five physical variables are that upstream kuroshio transport is associated with these factors specifically since kuroshio is a kind of continuous current the transport may be influenced by the meridional velocity and zonal velocity besides sea surface temperature lombard et al 2009 stammer et al 2013 sea level yang et al 2001 and high salinity water wang et al 2015 are significantly correlated with the current shift in the case study the initial state is the same as event 2 in the literature zhang et al 2016 starting from the 12466th day referred to as day 0 to the 12530th day referred to as day 64 fig 6b exhibiting the daily upstream kuroshio transport from day 0 to day 64 indicates a drastic transport reduction of about 8 sv 1 sv 1 0 6 m 3 s the objective function is defined as the square of the transport variation flowing through the target section s at the prediction time t 36 f x 0 m t x 0 x 0 m t x 0 o b j s δ v t d x d y 2 where s is the section from philippine coast to 124 e in the upper 600 meters at 18 n the initial state vector is denoted by x 0 v 0 u 0 t e m p 0 s a l t 0 z e t a 0 and the initial perturbation state vector is denoted by x 0 δ v 0 δ u 0 δ t e m p 0 δ s a l t 0 δ z e t a 0 the error development state vector at the prediction time t can be written as 37 x t δ v t δ u t δ t e m p t δ s a l t t δ z e t a t m t x 0 x 0 m t x 0 note that every variable for example δ v t is determined by the nonlinear propagator m t x 0 and x 0 so δ v t cannot be written as m t v 0 δ v 0 m t v 0 the constraint norm of initial solutions is defined as the total energy in the simulated region 38 x 0 c 2 ρ r e f 2 δ v 0 2 δ u 0 2 g ρ r e f α δ t e m p 0 β δ s a l t 0 ρ r e f n 0 2 d x d y d z ρ r e f 2 g δ z e t a 0 2 d x d y δ 2 where ρ r e f equals to 1027 kg m 3 g is the gravity acceleration α equals to 1 7 1 0 4 k 1 β equals to 7 6 1 0 4 psu 1 n 0 denotes the brunt väisäl a frequency and δ equals to 5 1 0 6 j 1 2 as can be seen from fig 6b after superimposing cnop the transport may be either greater than the reference state a weaker transport decline or less than the reference state a stronger transport decline at the prediction time day 64 we denote the former as cnop1 and the latter as cnop2 the validity of cnop is determined by the magnitude of the absolute value of the transport change after superimposing the cnop compared to the reference state at the prediction time the superimposed cnop is valid if it produces an absolute value of the transport variation greater than 3 sv and vice versa zhang et al 2016 5 experiments and results this section presents the data and experimental settings followed by the assessment of deepfe and the performance evaluation of the proposed method and finally the physical mechanism analysis 5 1 data and experimental settings experimental settings are described in this section the total number of 4000 perturbation samples is obtained via the roms model zhang et al 2016 and hypersphere sampling voelker et al 2017 its theoretical proof yong ming 2008 and experimental validation are presented in the literature zhang et al 2016 wang et al 2012 jiang et al 2008 duan et al 2004 in the process of training the deepfe model 80 percent of samples are for training and 20 percent are for validation tables 3 and 4 list the parameter settings of pso and bayesian optimization most of which refer to empirical values table 5 lists the dimensionality settings of the hidden representation in deepfe and experiments on these dimensionality settings are conducted given that the reconstruction error is the dominant cost the two constant coefficients in deepfe λ p n t and λ r e g should be configured so that these two items account for ten percent of the reconstruction error at the beginning of training see table 6 since running the roms model requires considerable computational resources the maximum number of function evaluations is 600 in the experiments we examine the performance of each method with 600 function evaluations except for the solving efficiency part in section 5 3 2 and the gain analysis part in section 5 3 4 experiments are conducted on computers with 32 amd ryzen 9 5950x 16 core processors and nvidia gpu 5 2 analysis of deepfe in this section we first illustrate the selections of parameters in deepfe since the values of parameters might influence the feature learning performance then comparisons between deepfe and pca are presented in section 5 2 2 to further validate the superiority of deepfe 5 2 1 parameter selections according to the literature smith 2018 the value of batch size and learning rate might impact the performance of the neural network as these two hyperparameters directly determine the update of model weights the value of batch size affects the generalization of models while the learning rate affects the convergence of models and the two can also influence each other therefore it is necessary to make a wise selection of batch size and learning rate the relative error ratio criterion r e r see eq 39 yuan et al 2022 mu et al 2015a is adopted to evaluate the capability of feature learning 39 r e r x x ˆ f x f e where x is the input data x ˆ is the output data x f is frobenius norm and e is a small constant experiments have been conducted with the batch size of 8 16 and 32 and the learning rate of 10 2 10 3 and 10 4 respectively fig 7 illustrates the relative error ratio of the validation data with respect to the number of epochs as can be observed the batch size does not have a noticeable effect on the model compared to the learning rate whereas as the value of the learning rate decreases the degree of overfitting decreases and the epoch to reach the minimum relative error ratio increases the early stopping strategy is adopted during training and the model at the epoch close to the best relative error ratio is selected to further investigate the influence of parameters on optimization we carry out experiments on peboa with the default parallel number and deepfepso with 31 dimensional latent representation termed as peboa 31d and deepfepso 31d 62 dimensional latent representation termed as peboa 62d and deepfepso 62d and 124 dimensional latent representation termed as peboa 124d and deepfepso 124d the quantitative results for five runs are shown in table 7 as shown in table 7 with the optimization method and the dimensionality of latent representation fixed for example peboa 31d models with different values of batch size and learning rate yield similar objective function values accordingly in the following experiments we choose moderate values of batch size and learning rate 16 and 10 3 correspondingly additionally table 7 also indicates the superior performance of our proposed method in comparison with deepfepso as better objective function values are obtained via peboa in each dimensional setting this is further validated in section 5 3 in summary based on the relative error ratio curves of validation data in fig 7 and the optimization results for each combination of parameters the batch size and learning rate of the deepfe model are established 5 2 2 comparison with pca first of all we examine the representation learning performance of deepfe and pca measured in terms of relative error ratio fig 8 displays the relative error ratio of pca with respect to the number of principal components and the relative error ratio given by deepfe using 31 62 and 124 dimensional latent features marked with horizontal dotted lines in terms of pca there is an improvement in the performance of feature learning with the increasing number of principal components however to achieve the performance given by deepfe 31d up to 146 principal components are necessary not to mention to match the performance of deepfe 62d and deepfe 124d specifically deepfe 31d s relative error ratio is approximately 81 6 smaller than that of pca 31d deepfe 62d s relative error ratio is about 68 7 smaller than that of pca 62d deepfe 124d s relative error ratio is around 34 8 smaller than that of pca 124d in brief the relative error ratio of deepfe is on average about 61 7 smaller than that of pca this demonstrates that compared with pca deepfe is capable of achieving even better feature learning performance with fewer feature dimensions in addition to compare deepfe s and pca s reconstruction skills in the optimization process we perform experiments on deepfe incorporated optimization methods called peboa and deepfepso and pca incorporated optimization methods called pcabo and pcapso for solving cnop as summarized in fig 9 in each dimensionality setting the deepfe based optimization not only has a larger average objective function value over five runs but also has a higher probability of obtaining the valid cnop regardless of whether the optimization algorithm is our proposed method or pso taking peboa 31d as an example its average objective function value is larger than that of pcabo 31d and the valid cnop can be gained in two out of five runs while pcabo 31d cannot obtain the valid cnop in all five runs it is the nonlinearity and specialized training of deepfe that results in a smaller relative error ratio and more information retained therefore in solving cnop employing deepfe as the feature extraction component is more expressive than employing the linear pca besides deepfe outperforms pca with respect to identifying feature dimensions according to fig 9 both pca based parallel bayesian optimization with additive gaussian kernels and pca based pso can acquire the valid cnop if the dimensionality of the compressed representation is set to 62 and cannot acquire the valid cnop if it is set to 31 on the contrary deepfe based optimization can obtain the valid cnop except for deepfepso 31d this is because that pca relies on a fixed feature matrix which is not designed for a specific feature dimension in contrast deepfe with specific feature dimensions is purposely trained so mappings between high and low dimensional spaces in the deepfe model cause less information loss this indicates that the determination of the latent feature s dimensionality for deepfe based optimization is more flexible than pca based optimization in solving cnop it should be mentioned that no matter whether the optimization algorithm is bayesian optimization or pso the optimization results with the 62 dimensional hidden feature are better than those with the 31 dimensional hidden feature while the results with the 124 dimensional hidden feature are worse than those with the 62 dimensional hidden feature for instance pcabo 62d outperforms both pcabo 31d and pcabo 124d the theoretical explanation is that even though the relative error ratio decreases as the number of feature dimensions increases the optimization task also becomes tricky due to the growth of the search space hence in solving cnop it is necessary to strike a trade off between feature extraction and optimization for the detailed experimental explanation see the gain analysis on the number of function evaluations and feature dimensions in section 5 3 4 by comparing deepfe and pca itself and assessing them in the optimization process it is concluded that our deepfe excels pca in terms of both feature learning performance and reconstruction capability in solving cnop 5 3 evaluations on performance of our method in sections 5 3 1 and 5 3 2 the proposed method is compared against deepfepso measured by the quality of the obtained cnop and the solving efficiency sections 5 3 3 and 5 3 4 illustrate the acquisition function analysis and the gain analysis on the number of function evaluations and feature dimensions 5 3 1 quality of solutions the assessment of the quality of solutions involves two metrics the magnitude of the objective function values and the likelihood of obtaining valid solutions for every dimensionality setting of latent features we conduct experiments on deepfepso peboa with the default parallel number termed as peboa q4 peboa with the parallel number of 2 termed as peboa q2 peboa without parallelization termed as eboa and peboa without parallelization and additive kernels termed as ebo quantitative results over 20 runs for each method are concluded in figs 10 and 11 among these methods ebo ranks the worst since naïve bayesian optimization is very sensitive to the dimensionality of the search space because ebo cannot obtain comparable objective function values or valid cnop we do not take it into consideration in subsequent experiments fortunately ebo with additive gaussian kernels namely eboa produces significantly better results than ebo specifically compared to ebo ebo 62d to be exact the objective function value of eboa eboa 62d to be exact is 26 5 larger and eboa has a higher probability of obtaining the valid cnop in each dimensional setting this confirms the necessity and effectiveness of incorporating additive gaussian kernels in addition as can be seen from the results of eboa equivalent to peboa q1 peboa q2 and peboa q4 these methods obtain quite similar objective function values and the same likelihood of obtaining the valid cnop which indicates that the parallel numbers have no noticeable effects on either the magnitude of objective function values or the number of valid cnop in fact the major advantage of parallelization is in the aspect of solving efficiency which is analyzed in section 5 3 2 based on the above analysis we select peboa with the best results i e peboa q4 and make a comparison with deepfepso fig 12 visualizes the objective function values over 20 runs for deepfepso and peboa q4 where the darker the color the larger the magnitude of the objective function values it indicates that our proposed method peboa has more dark color blocks in each dimensional setting i e in general it can generate larger objective function values in solving cnop deepfepso on the opposite has more light color blocks especially deepfepso 31d and deepfepso 124d where the lighter color blocks occupy the majority of the 20 blocks this suggests that peboa has merit over deepfepso with regard to the magnitude of objective function values in addition to the qualitative analysis quantitative results are concluded in figs 10 and 11 as summarized in fig 10 for every dimensional setting the average and maximum objective function values of peboa q4 are greater than the values of deepfepso apart from the magnitude of objective function values we also compare peboa q4 and deepfepso in the aspect of the validity of cnop see fig 11 based on the validity of the corresponding cnop the objective function values are divided into three intervals 8 5 1 0 12 denoted by interval a 8 5 1 0 12 9 5 1 0 12 denoted by interval b and 9 5 1 0 12 denoted by interval c cnop with the objective function value in interval c can lead to a significant decrease in upstream kuroshio transport i e the valid cnop cnop with the objective function value in interval b are likely to cause a significant decrease cnop with the objective function value in interval a cannot contribute to significant upstream kuroshio transport decreases over 20 runs the three dimensional settings of deepfepso namely deepfepso 31d deepfepso 62d and deepfepso 124d could not obtain the solution with the objective function value in interval c in comparison peboa q4 except peboa 124d q4 can all acquire the solution in interval c the results of peboa 124d q4 are not in interval c because the search space grows with the increase of feature dimensions moreover based on the 20 runs it can be observed that the probability of obtaining the valid cnop for peboa 31d q4 is 30 which is three times higher than that for deepfepso 31d likewise the probability of obtaining the valid cnop for peboa 62d q4 is 55 which is about 2 2 times higher than that for deepfepso 62d similarly there is a 35 probability of obtaining the valid cnop for peboa 124d q4 versus 20 for deepfepso 124d in short our proposed method has a higher probability of obtaining valid cnop than deepfepso in a nutshell we first compare the bayesian optimization involved methods ebo is considerably inferior in the magnitude of objective function values and the validity of cnop compared to that with additive kernels which verifies the effectiveness of additive kernels secondly we evaluate peboa and deepfepso in the aspects of the magnitude of the objective function values and the validity of cnop experiments confirm that peboa surpasses deepfepso concerning the quality of the obtained cnop 5 3 2 solving efficiency the assessment of solving efficiency includes two metrics time efficiency and convergence speed according to the experiments in section 5 3 1 experiments in this section adopt the feature dimensions with the best results i e peboa 62d q4 peboa 62d q2 eboa 62d and deepfepso 62d conceptually as presented in eq 40 the running time termed as t c n o p is composed of three components the time for searching the promising sampling point denoted by t s e a r c h the time for reconstructing the low dimensional point to the original space denoted by t r e c o n s t r u c t i o n and the time for calculating the objective function value via calling roms denoted by t r o m s fig 13a illustrates the running time of peboa 62d q4 peboa 62d q2 eboa 62d equivalent to peboa 62d q1 and deepfepso 62d over 20 runs overall the running time of eboa 62d is longer than deepfepso 62d because training the gaussian process and maximizing the acquisition function in bayesian optimization i e t s e a r c h in eq 40 cost more time than updating particles velocities and positions in pso 40 t c n o p t s e a r c h t r e c o n s t r u c t i o n t r o m s however the wall clock time does not take into account the validity of cnop thus we introduce t v a l i d to represent the actual time for obtaining the valid cnop in eq 41 t c n o p denotes the average wall clock time over 20 runs n r u n s indicates the number of runs i e 20 and n v a l i d indicates the number of valid cnop obtained refer to fig 11 as illustrated in fig 13b t v a l i d of eboa 62d is 194475 65 s less than deepfepso 62d this authenticates the time efficiency of the proposed method even without parallelization additionally it can be observed that the wall clock time and the actual time for obtaining the valid cnop in peboa decrease as the parallel number increases to be exact peboa 62d q4 and peboa 62d q2 save 71 0 and 44 8 of the computation time correspondingly compared to eboa 62d which confirms that the parallelism implemented by our modified acquisition function can economize computation time 41 t v a l i d t c n o p n r u n s n v a l i d to analyze the convergence performance fig 14 summarizes the quantitative results for 700 function evaluations of peboa 62d q4 peboa 62d q2 eboa 62d equivalent to peboa 62d q1 and deepfepso 62d over 20 runs and fig 15 exhibits the relationship between objective function values and the number of function evaluations divided by the parallel number for the 700 function evaluations the maximum number of iterations for pso and bayesian optimization is set to 35 and 700 respectively quantitative results in figs 14 and 10 in section 5 3 1 confirm that on condition that the maximum number of function evaluations increases from 600 to 700 there is no apparent difference in the mean objective function values in detail the objective function values for peboa 62d q4 peboa 62d q2 eboa 62d and deepfepso 62d increase by 0 303 0 545 0 436 and 0 596 respectively as shown in fig 15 eboa 62d reaches convergence at approximately 520 function evaluations while deepfepso 62d achieves convergence when the number of function evaluations is about 580 it can be demonstrated that in comparison with deepfepso suffering from local stagnation our proposed method even without parallelization can achieve more promising results with fewer function evaluations in addition the objective function value of eboa 62d rises faster in the early stage proving the strength of our proposed method in optimizing computationally expensive functions moreover from the aspect of parallelism our method reaches convergence in fewer function evaluations as the parallel number increases which further corroborates that our parallel implementation can save computational resources in brief compared to deepfepso our proposed method with or without parallelism has a faster convergence rate and spends less time in obtaining the valid cnop 5 3 3 acquisition function analysis for further investigation of the acquisition function we compare the proposed ucb add with two frequently adopted acquisition functions namely ucb termed as eboa ucb 62d and ei termed as eboa ei 62d in 700 function evaluations with 62 feature dimensions figs 14 and 15 summarize quantitative results and the best objective function value with respect to the number of function evaluations over 20 runs respectively it can be seen that eboa 62d outperforms the other two in terms of the magnitude of objective function values the number of valid solutions and the convergence rate besides eboa 62d and eboa ucb 62d has objective function values in interval c while eboa ei 62d does not therefore it can be concluded that the proposed ucb add has the best performance in the roms model of upstream kuroshio transport followed by ucb and finally by ei 5 3 4 gain analysis this section presents the gain analysis on the number of function evaluations and feature dimensions theoretically results of peboa 124d and deepfepso 124d should be better than those of peboa 62d and deepfepso 62d in the absence of a limit on the number of function evaluations since more feature dimensions contain more useful information however more feature dimensions mean the broader search space during optimization and the computational cost increases with the number of function evaluations therefore it is necessary to strike a balance between feature dimensions and optimization in this section we figure out whether the optimization results of peboa 124d q4 and deepfepso 124d deserve additional number of function evaluations we conduct peboa 124d q4 and deepfepso 124d with 900 function evaluations over five runs and make a comparison with peboa 62d q4 and deepfepso 62d with 600 function evaluations over five runs results have shown in fig 9 of section 5 2 comparing fig 9 with fig 16 it can be seen that with the number of function evaluations becoming 1 5 times higher the mean objective function values of peboa 124d q4 and deepfepso 124d only increase by 1 66 and 1 87 compared to those of peboa 62d q4 and deepfepso 62d respectively therefore even though peboa 124d q4 and deepfepso 124d with 900 function evaluations achieve better results this is not a worthwhile effort considering the investment of computational resources and the outcome 5 4 physical mechanism analysis in this section we first probe into prediction errors obtained by peboa q4 and deepfepso then the spatial structure of cnop is analyzed to further validate the efficacy of peboa q4 the adjoint model adj is taken as the benchmark in addition section 5 4 3 presents the analysis regarding the evolution of the obtained cnop over time 5 4 1 prediction errors we analyze the prediction errors induced by cnop for each method where we superimpose the obtained cnop on the reference state at the initial moment and observe the upstream kuroshio transport at the prediction time fig 17 lists the upstream kuroshio transport changes induced by cnop for each method over 20 runs peboa q4 and deepfepso can obtain two types of cnop cnop1 and cnop2 refer to the case study in section 4 considering the validity peboa q4 in every dimensional setting can yield valid cnop1 and cnop2 while only deepfepso 62d and deepfepso 124d can yield valid cnop1 and cnop2 besides cnop obtained by peboa q4 can result in more significant prediction errors for example cnop1 of deepfepso 62d incurs an increase in the upstream kuroshio transport by approximately 3 08 sv only while cnop1 of peboa 62d q4 incurs an increase in the upstream kuroshio transport by about 3 20 sv moreover peboa q4 peboa 62d q4 to be exact is closer to the results of adj i e 3 1445 sv and 3 2773 sv as far as the increase and decrease of transport are concerned in addition considering the number of cnop1 and cnop2 obtained deepfepso prefers to acquire cnop1 while peboa q4 has no such preference i e both cnop1 and cnop2 can be acquired to demonstrate that the cnop acquired from peboa q4 is the optimal perturbation i e the optimally growing initial error we observe whether other types of perturbations can lead to dramatic transport changes fig 18 displays the prediction errors at the forecasted time after random perturbations are superimposed on the initial reference state in experiments of random perturbations the prediction errors become random and there are both increases and decreases in the upstream kuroshio transport the absolute values of these errors are far smaller than 3 sv which further justifies that the perturbations obtained by our method are the optimal initial perturbations 5 4 2 spatial structure of cnop to further examine the performance we investigate the spatial structure of cnop since baroclinic instability contributes to the rapid growth of errors zhang et al 2016 qiu 1999 qiu and chen 2010 chang and oey 2014 figs 19 and 20 depict the velocity and temperature components distribution in the upper 300 meters of the two types of cnop note that we select cnop that results in the most significant transport change for each method tables 8 and 9 display the cosine similarity between the cnop obtained by adj and the cnop obtained by peboa deepfepso as for cnop1 fig 19a b and c indicate that cnop1 obtained by deepfepso and peboa q4 approximates cnop1 obtained by adj fig 19d and e illustrate the exact difference of temperature components and the sum of squared error between deepfepso and adj is 2 92 1 0 5 while the sum of squared error between peboa q4 and adj is smaller 4 01 1 0 6 to be exact as can be seen from table 8 compared to deepfepso the cnop1 obtained by peboa q4 is more similar to the cnop1 of adj as for cnop2 deepfepso yields less satisfactory results than peboa q4 because the large amplitudes of the temperature and velocity components lie in the region of 123 e 127 e 16 n 19 n which is contradictory to the large amplitudes area mentioned in the literature 127 e 130 e 16 n 19 n zhang et al 2016 besides the sum of squared errors for deepfepso is more significant than peboa q4 with values of 1 98 1 0 4 and 3 78 1 0 5 respectively as can be observed in fig 20d and e according to table 9 compared to deepfepso the cnop2 obtained by peboa q4 is more similar to the cnop2 of adj in brief sections 5 4 1 and 5 4 2 substantiate the superiority of our proposed method over aepso in terms of the amplitude of upstream kuroshio transport variability and the structure of obtained cnop 5 4 3 evolution of cnop this section investigates the error development after superimposing cnop to clarify how cnop leads to such prominent prediction errors we first analyze the error fields at the prediction time day 64 after superimposing cnop obtained by our method see figs 21 and 22 figs 21a and 22a exhibit the errors in meridional velocity at the prediction time on the 18 n section after superimposing two types of cnop with cnop1 superimposed anomalous northward meridional currents with a maximum value of approximately 14 cm s are found within the target section s and the related vertical center is located at about 300 m similarly after superimposing cnop2 there are anomalous southward currents in s with a maximal value of about 14 cm s and the vertical center is also near 300 m to pinpoint the reason for these meridional velocity anomalies in the target section we study the mean velocity error fields within the upper 600 m at the prediction moment figs 21b and 22b as for cnop1 an anticyclonic error exists near 18 n with its horizontal center located at 124 e 18 n it is this anticyclonic eddy like structure that leads to the northward meridional velocity error in s and subsequently the anomalous increase of upstream kuroshio transport at day 64 in contrast after superimposing cnop2 a cyclonic eddy like structure exists at approximately the same position as the anticyclonic error described above which accounts for the southward meridional velocity anomaly in the target section and the reduction of transport in sum after superimposing cnop1 cnop2 an anticyclonic cyclonic eddy like structure emerges in the target section at day 64 which causes a northward southward meridional velocity anomaly and an anomalous rise decline in upstream kuroshio transport to explore how the two types of optimal initial errors develop into the anticyclonic or cyclonic eddy like structure we analyze the evolution of cnop from day 0 to day 64 figs 23 and 24 depict error evolutions at 300 m over the prediction period for cnop1 and cnop2 respectively as illustrated in fig 23a a very small anticyclonic eddy like structure appears near 128 e 17 n at day 0 where the maximum amplitude of the anticyclonic velocity error is about 0 2 cm s this anticyclonic eddy like structure propagates westward and grows rapidly with time at day 32 see fig 23c the position of its horizontal center has propagated to 126 e and the maximum velocity error has risen to about 5 0 cm s which is as much as 25 times the amplitude at the initial moment at the end of the prediction see fig 23f its central position reaches the location of 124 e 18 n and the maximum velocity error has reached about 15 0 cm s which is about 75 times the velocity amplitude at the beginning of the prediction similar to fig 23 fig 24 shows that after superimposing cnop2 there is a cyclonic eddy like structure with very small amplitude at the initial moment near 128 e which travels westward reaching 124 e 18 n eventually and intensifies promptly with a final maximum velocity error of about 15 0 cm s it can be concluded that the development of cnop1 and cnop2 during the prediction period is characterized by two features 1 one is the westward propagation of errors specifically from day 0 to day 64 the central position of anticyclonic or cyclonic errors propagates westward by about 4 degrees of longitude and northward by one degree of latitude finally arriving near 124 e 18 n thus triggering changes in the upstream kuroshio transport their main propagation direction is westward 2 the other is the fast growth of anticyclonic or cyclonic errors during the forecast period resulting in an increase in the maximum velocity anomaly by about 75 times which eventually leads to considerable prediction errors these two features are indispensable for optimal initial errors to induce significant prediction errors in the upstream kuroshio transport it should be mentioned that the structures and evolutions of the cnops are similar with those obtained with the adjoint method zhang et al 2016 this suggests the cnops obtained by our proposed peboa are convincible 6 conclusions in this paper we propose a paralleled embedding high dimensional bayesian optimization with additive gaussian kernels for solving high dimensional cnop called peboa to alleviate the problem of intelligent optimization algorithms trapped in local optima and the 20 dimensional search space restriction of ordinary bayesian optimization algorithm and to exploit the computational resources adequately in peboa the deepfe using the convolutional ae with residual connections and customized constraint preserved loss function is designed to compress 10 million dimensional data to relatively low dimensional search spaces then we propose the strategy of additive gaussian kernels exploiting which the bayesian optimization algorithm can optimize in relatively low dimensional search spaces additionally to accelerate the optimization process the acquisition function is modified to sample multiple candidates simultaneously we select the upstream kuroshio transport variation in roms a 10 million dimensional and computationally costly problem as the case study to investigate the performance of peboa experimental results show compared to pca the relative error ratio of deepfe is on average about 61 7 smaller demonstrating that deepfe is capable of achieving better feature learning performance with fewer feature dimensions using deepfe for dimensionality reduction compared to pca peboa and pso both not only get larger mean objective function values but also have higher probabilities of obtaining the valid cnop effectiveness of incorporating the strategy of additive gaussian kernels is also verified which improves the magnitude of objective function values by about 26 5 and obtains valid cnop the modified acquisition function for sampling multiple candidates simultaneously reduces computation time by about 71 0 with four cores compared to deepfepso peboa has 3 4 larger objective function values and 2 2 times greater likelihood of obtaining the valid cnop and it also has a faster convergence rate and takes less time to obtain the valid cnop in a word peboa surpasses deepfepso in terms of the quality of obtained cnop and solving efficiency we also conduct the gain analysis and draw a conclusion that for this case deepfe s compression of data to a 62 dimensional search space is appropriate which makes a trade off between computational cost and quality of solutions compared to deepfepso the cnop obtained by peboa is more consistent with the adjoint model in terms of physical mechanisms including the amplitude of upstream kuroshio transport variability and the structure after superimposing the cnop the anticyclonic cyclonic eddy like structure causes a northward southward meridional velocity anomaly and an anomalous increase decrease in upstream kuroshio transport and the evolution of errors shows two features i e westward propagation and rapid development in the future we intend to apply peboa to other numerical models to validate its generality in solving cnop which includes cnop for double gyre variation in the roms model with 1 0 5 dimensions and cnop for nao in cesm model with 1 0 7 dimensions since the dimensions of these scenarios are smaller than the upstream kuroshio transport in the roms model and our peboa algorithm yields impressive performance in the upstream kuroshio transport problem we believe that the results of these relatively small search space scenarios are also satisfactory besides to further boost the solving efficiency we plan to explore strategies that can accelerate the training of the gaussian process furthermore it is necessary to explore the application of peboa in other problem domains credit authorship contribution statement shijin yuan conceptualization methodology software investigation writing original draft writing review editing yaxuan liu conceptualization methodology software investigation writing original draft writing review editing bo qin conceptualization methodology software writing review editing bin mu conceptualization methodology software writing review editing kun zhang conceptualization methodology software writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study is supported in part by the meteorological joint funds of the national natural science foundation of china under grant u2142211 in part by the key project fund of shanghai 2020 science and technology innovation action plan for social development under grant 20dz1200702 in part by the national key research and development program of china under grant 2020yfa0608000 in part by the national natural science foundation of china under grant 42075141 and in part by the first batch of model interdisciplinary joint research projects of tongji university in 2021 under grant yb 21 202110 appendix joint training process of models in deepfe the pseudocode of training process for deepfe is displayed in algorithm 3 
23762,conditional nonlinear optimal perturbation cnop is widely used in atmospheric and oceanic predictability studies solving cnop is essentially a nonlinear optimization problem with certain constraints one method to solve cnop is the intelligent optimization algorithm but it may not always obtain the satisfactory solution and efficiency because of the local exploitation of individual particles the bayesian optimization algorithm can avoid this local stagnation by building a global probability grasp for the optimization space of cnop nonetheless the limit of approximately 20 dimensional optimization space hampers its application in solving cnop of high dimensional numerical models to overcome this bottleneck we propose a paralleled embedding high dimensional bayesian optimization with additive gaussian kernels peboa algorithm which mainly consists of the feature extraction process and low dimensional optimization process in peboa a feature extraction method deepfe using the convolutional autoencoder with residual connections and customized constraint preserved loss function is designed which compresses 10 million dimensional data to relatively low dimensional search spaces from tens to hundreds of dimensions then we propose the strategy of additive gaussian kernels with which the bayesian optimization algorithm can optimize in relatively low dimensional search spaces concretely a kernel handles a subspace and a subspace can handle dimensions approximately not more than 20 additionally the modified acquisition function can sample multiple candidates simultaneously with the aim of accelerating the optimization process we apply peboa to solve cnop of regional ocean modeling system roms for identifying optimal initial errors of upstream kuroshio transport variation which is a 10 million dimensional and time consuming problem the computational performance of peboa and physical mechanisms of the obtained cnop are analyzed experimental results indicate that deepfe excels principal component analysis pca in terms of relative error ratio the magnitude of objective function values and the obtained cnop pattern besides compared to deepfe based particle swarm optimization peboa has better solving efficiency with 3 4 larger objective function values and 2 2 times greater likelihood of obtaining the valid cnop furthermore the modified acquisition function reduces computation time by about 71 0 with four cores the physical mechanism analysis shows that cnops obtained by peboa are almost identical to the adjoint method which can cause an anomalous increase decrease in upstream kuroshio transport and maintain physics consistency keywords autoencoder based bayesian optimization additive gaussian kernels paralleled acquisition function cnop regional ocean modeling system upstream kuroshio transport data availability data will be made available on request 1 introduction conditional nonlinear optimal perturbation cnop initially proposed by mu et al 2003 stands for the initial perturbation that leads to the largest nonlinear evolution at the prediction time in a given constraint so far it has been broadly adopted to explore the nonlinear dynamics of atmospheric and oceanic systems mu and duan 2005 duan and mu 2009 study the predictability and sensitivity of events dijkstra and viebahn 2015 wang et al 2020 zhang et al 2017b liu et al 2018 tao et al 2017 2019 carry out ensemble forecasts huo et al 2019 huo and duan 2019 duan and huo 2016 and determine targeted observations dai et al 2019 liang et al 2019 mu et al 2015c hu et al 2021 solving cnop is essentially a nonlinear optimization problem with certain constraints there have been some works on methods for solving cnop one of the most popular approaches is the intelligent optimization algorithm a class of optimization algorithms mimicking natural phenomena its advantages lie in that it is adjoint free and is capable of overcoming the discontinuity problem for instance zheng et al 2017 and yuan et al 2015b applied particle swarm optimization pso and some pso variants to solve cnop of a two dimensional ikeda model and a simplified zebiak cane zc model however the intelligent optimization algorithm with local exploitation of individual particles tends to fall into local optima when encountering complex multimodal functions ye et al 2013 larsen et al 2016 xu et al 2015 which may result in obtaining unsatisfactory solutions and efficiency the bayesian optimization algorithm močkus 1975 mockus 1989 consisting of the surrogate model and the acquisition function can avoid this local stagnation by building a global probability grasp for the optimization space of cnop its capability of globally optimizing computationally expensive black box functions makes it outstanding among optimization methods up to now it has been utilized to solve various problems such as robotics nogueira et al 2016 junge et al 2020 martinez cantin 2017 jaquier et al 2020 calandra et al 2016 reinforcement learning barsce et al 2017 imani and ghoreishi 2022 balakrishnan et al 2020 young et al 2020 brochu et al 2010 and the design of engineering systems palar et al 2020 wang et al 2021 hebbal et al 2021 pelamatti et al 2021 lam et al 2018 nonetheless the limit of approximately 20 dimensional optimization space wang et al 2022 li et al 2016 wang et al 2016b nayebi et al 2019 caused by the restricted expression ability of the gaussian process based surrogate model hampers its application in solving cnop of high dimensional numerical models to overcome this bottleneck we propose a paralleled embedding high dimensional bayesian optimization with additive gaussian kernels peboa algorithm which mainly consists of the feature extraction process and low dimensional optimization process specifically in peboa a feature extraction method deepfe using the convolutional autoencoder ae baldi 2012 with residual connections and customized loss function is designed as the embedding component to reduce dimensions nonlinearly with retaining as much information as possible the deepfe allows for searching in low dimensional search spaces and reconstructing solutions to calculate the objective function values unlike the commonly used linear feature extraction method mu et al 2015a yang et al 2020 xu et al 2022 such as principal component analysis pca jolliffe 2005 and its variant candes et al 2009 the deepfe yields less information loss since pca finds a fixed low dimensional hyperplane and only provides linear transformations for data lee 2011 while the deepfe involving neural networks discovers sophisticated relations of hidden layers and learns a compressed representation of complicated data effectively unlike ordinary ae used in the literature yuan et al 2022 dealing with about 100 thousand dimensional double gyre variation problem the deepfe which can handle 10 million dimensional data has more elaborate structures alkhayrat et al 2020 such as more layers and longer widths with residual connections to avoid the vanishing gradients he et al 2015 and presents a customized loss function with the constraint preserved penalty term in this way the deepfe compresses 10 million dimensional data to relatively low dimensional search spaces from tens to hundreds of dimensions then we propose the strategy of additive gaussian kernels with which the bayesian optimization algorithm can optimize in relatively low dimensional search spaces since deepfe is not strong enough to compress the ultra high dimensional bayesian optimization to an effect suitable for optimization i e less than approximately 20 dimensions this strategy is inspired by the studies on structural assumptions for the high dimensional extension of bayesian optimization researchers kandasamy et al 2015 han et al 2020 wang et al 2018a b imposed additive structures on problems assuming that the objective function can be partitioned into functions with small disjoint sets of dimensions experiments revealed that this method outperforms standard bayesian optimization even on non additive functions therefore we propose the strategy of additive gaussian kernels to build the gaussian process for each variable concretely a kernel handles a subspace and a subspace can handle dimensions approximately not more than 20 additionally to speed up the optimization process we modify the acquisition function to sample multiple candidates simultaneously instead of the sequential sampling of the standard bayesian optimization algorithm the modified acquisition function combines the upper confidence bound strategy and the pure exploration strategy contal et al 2013 allowing for evaluating points in parallel in summary we propose the peboa algorithm for solving high dimensional cnop to overcome the problem of intelligent optimization algorithms falling into local optima and to exceed the 20 dimensional limit of ordinary bayesian optimization algorithm we apply peboa to solve cnop of regional ocean modeling system roms for identifying optimal initial errors of upstream kuroshio transport variation zhang et al 2016 which is a 10 million dimensional and time consuming problem our contributions can be summarized as follows we design the deepfe using the convolutional ae with residual connections and customized constraint preserved loss function in peboa the deepfe can compress 10 million dimensional data to relatively low dimensional search spaces from tens to hundreds of dimensions with superior feature learning performance and reconstruction capabilities in peboa we propose the strategy of additive gaussian kernels with which the bayesian optimization algorithm can optimize in relatively low dimensional search spaces concretely a kernel handles a subspace and a subspace can handle dimensions approximately not more than 20 to speed up the optimization process we modify the acquisition function which can sample multiple candidates simultaneously instead of sequential sampling peboa is applied to solve cnop of the roms model for identifying optimal initial errors of upstream kuroshio transport first compared to pca we evaluate deepfe from the relative error ratio the magnitude of objective function values and the obtained cnop pattern second we compare peboa and deepfe based pso deepfepso in terms of the quality of obtained cnop and solving efficiency the modified acquisition function is also assessed cnops obtained by peboa are almost identical to the adjoint method which can cause an anomalous increase decrease of upstream kuroshio transport and maintain physics consistency the rest of this paper is organized as follows section 2 first introduces the concepts of cnop and approaches for solving cnop then briefly summarizes feature extraction methods and pso followed by the theory of bayesian optimization algorithm the components of the proposed peboa are described in section 3 section 4 illustrates the case of upstream kuroshio transport in section 5 we figure out the parameter settings of deepfe compare deepfe with pca compare peboa with deepfepso analyze the performance of our proposed method analyze three acquisition functions conduct the gain analysis and probe into the physical mechanisms section 6 presents the conclusion and future work 2 related works 2 1 cnop cnop represents the initial perturbation that leads to the largest nonlinear evolution at the prediction time in a given constraint the mathematical definition of cnop can be described in eq 1 x 0 denotes the optimal initial perturbation x 0 is the initial state vector m t denotes the nonlinear propagation operator that propagates the evolution equation from the initial moment to moment t which is calculated via numerical models to be precise the roms model is used in this paper x 0 c δ denotes the given constraint of the initial perturbation and o b j denotes the objective function usually the ℓ 2 norm 1 x 0 argmax x 0 c δ f x 0 argmax x 0 c δ m t x 0 x 0 m t x 0 o b j a great deal of previous research has focused on approaches for solving cnop existing approaches can be roughly classified into two categories i e gradient based ones and intelligent optimization algorithm involved ones based on whether or not the gradient information is utilized a detailed and organized overview of existing work on solving cnop is covered to offer a clear structure for the extensive literature reported in this section as illustrated in table 1 gradient based methods considering rewrite f x 0 of eq 1 see eq 2 the gradient of f x 0 can be defined as eq 3 in eq 3 h denotes the tangent linear matrix of the nonlinear model m and h t known as the jacobian matrix of f x 0 is the transpose of matrix h 2 f x 0 min x 0 c δ m t x 0 x 0 m t x 0 2 3 f x 0 2 h t m t x 0 x 0 m t x 0 the gradient can be calculated via adjoint models and then gradient descent optimization algorithms such as spectral projected gradient spg birgin et al 2001 2000 and sequential quadratic programming sqp powell 1983 can be utilized to solve cnop for instance adjoint models with spg have been employed to solve cnop in a roms model of kuroshio intrusion liang et al 2019 a weather research forecasting wrf model of the tropical cyclone wang et al 2011 mu et al 2009 etc tang et al 2012 jiang 2006 sun et al 2010 adjoint models with sqp have been applied in a two dimensional quasigeostrophic model mu and zhang 2006 a theoretical ocean atmosphere coupled model duan et al 2004 etc sun et al 2010 mu and duan 2005 however there are two limits to how far such type of approach can be taken one is that most complicated numerical models do not contain adjoint models on their own and it is challenging and time consuming to develop adjoint models the other is the on off switch problem zheng et al 2012 concretely when the gradient of the objective function with respect to variables does not exist the gradient information will be computed incorrectly which is known as a non smooth optimization problem as for the former limitations some researchers proposed adjoint free methods to calculate the gradient such as the monte carlo ensemble projection algorithm wang and tan 2010 and the singular vector decomposition svd based ensemble projection algorithm yuan et al 2015a chen et al 2015 these methods bypass the construction of adjoint models but the on off switch problem remains intelligent optimization algorithm involved methods intelligent optimization algorithms which do not require gradient information are capable of handling non smooth strongly nonlinear optimization problems they are a class of optimization algorithms mimicking some natural phenomena such as pso genetic algorithm ga simulated annealing sa and wolf search algorithm wsa zheng et al 2012 adopted ga in modified lorenz models to study predictability moreover zheng et al 2017 and yuan et al 2015b applied pso and some pso variants to solve cnop of a two dimensional ikeda model and a simplified zc model similarly ren et al 2016 utilized a modified artificial bee colony algorithm mabc in a simplified zc model these cases confirmed that intelligent optimization algorithms could overcome the discontinuity problem nevertheless such cases were only conducted in low dimensional unimodal or simple multimodal objective functions difficulties arise when applying intelligent optimization algorithms to large scale numerical models and complex multimodal objective functions one popular approach to overcome the curse of dimensionality is combining feature extraction methods such as pca and its variant robust principal component analysis rpca with intelligent optimization algorithms mu et al 2015a put forward ppso to solve cnop in complicated air sea coupled numerical models and the ppso framework has been frequently used yang et al 2020 xu et al 2022 pca rpca based ga has been used in medium complexity zc models and fifth generation mesoscale models mm5 mu et al 2015b zhang et al 2017a wen et al 2015 besides the idea of hybridization emerged mu et al 2019a 2022 applied the pca based ga and pso hybrid algorithm in north atlantic oscillation nao with the community earth system model cesm likewise researchers zhang et al 2018 mu et al 2019c proposed the pca based adaptive cooperative coevolution of pso and wsa acpw to identify sensitive regions of tropical cyclone adaptive observations although this strategy alleviates the difficulty of high dimensional optimization search to some extent the information loss incurred by fixed subspaces of pca exists resulting in unsatisfactory results to deal with this drawback yuan et al 2022 proposed feature extraction based intelligent algorithm framework with neural network fnnia to solve double gyre variation in roms where well trained neural network based feature extraction components including basic ae and generative adversarial network gan are involved experimental results show that the fnnia framework has better objective function value and cnop pattern than the framework without neural networks nevertheless it is necessary to investigate suitable model structures and the applicability of neural network based feature extraction methods in higher dimensional meteorological problems in addition the above methods cannot tackle the problem of intelligent optimization algorithms falling into local optima when encountering complex multimodal functions 2 2 feature extraction methods the objective of feature extraction is to construct feature vectors with dimensionality smaller than the original dataset pca jolliffe 2005 is one of the most frequently used linear feature extraction techniques tadić et al 2019 geng et al 2021 duforet frebourg et al 2016 touzani et al 2020 arivudainambi et al 2019 chen and tian 2022 giuliani 2017 the objective of pca is to find a set of vector bases that maximize the data s projection in the direction of vector bases mathematically pca obtains this set of vector basis via the eigendecomposition of positive semi definite matrixes or the svd of rectangular matrices abdi and williams 2010 as for the implementation researchers have presented detailed tutorials shlens 2014 reris and brooks 2015 rodionova et al 2021 the strengths of pca are that it is easily understood and computationally simple however the drawback lies in its fixed latent subspace that only considers linear correlations that is to say that non principal components with relatively small eigenvalues are likely to contain critical information and discarding them may have a negative impact on the subsequent optimization process the rapid proliferation of deep learning technology gives rise to new ideas chen et al 2014 zhu et al 2014 gensler et al 2016 for feature extraction since some neural network architectures can construct latent subspaces and map efficiently between low dimensional and original spaces ae architecture was first proposed by rumelhart et al 1988 in 1986 with the purpose of learning potential representations of data as described in fig 1 it is composed of two components the encoder and the decoder the former encodes the input data to the low dimensional representation the latter reconstructs the low dimensional representation so that the output is as similar as possible to the input the formal definition baldi 2012 of ae is written as 4 argmin e n c o d e r d e c o d e r l x d e c o d e r e n c o d e r x where e n c o d e r encodes the input x r h i g h d i m to latent representation z r l o w d i m and d e c o d e r decodes z to x ˆ r h i g h d i m h i g h d i m and l o w d i m stand for the dimensionality of the original and compressed space respectively ae learns the e n c o d e r and d e c o d e r by minimizing the loss function denoted by l on the condition that ae has linear activation functions and only has one sigmoid hidden layer it is almost identical to pca therefore researchers bank et al 2021 consider ae as a nonlinear generalization of pca they differ in that pca finds a fixed low dimensional hyperplane and only provides linear transformations for data while ae discovers sophisticated relations of hidden layers and learns a compressed representation of complicated data studies have revealed that compared to pca ae excels in reconstruction due to its nonlinearity for example wang et al 2016a compared ae with state of the art dimensionality reduction methods and the results indicate that ae can learn something different and meaningful on some real datasets like the mnist dataset besides alsenan et al 2020 conducted experiments on qsar dataset and found that ae outperforms pca on the accuracy measure by comparing the merits and drawbacks of pca and ae it can be seen that although pca has fewer parameters and accordingly requires less computation its disadvantage lies in the large amount of information loss in the feature extraction process which is mainly due to two reasons 1 on the one hand pca only considers linear correlation when constructing the low dimensional feature space and ignores the nonlinear correlations of corresponding meteorological data 2 on the other hand non principal components with relatively small eigenvalues may contain key information which is conducive to optimization and directly discarding them by pca may have a negative impact on the subsequent optimization process ae can overcome the above shortcomings by learning the nonlinear relationships of data as well as effectively retaining the critical feature information in brief ae shows superior data compression ability and data reconstruction ability although ae requires higher hardware costs and time costs these shortcomings are no longer the major influencing factors with the improvement of computing power based on the above analysis it can be determined that ae s better feature extraction performance enables it to be a more effective feature extraction component in solving cnop recent years have witnessed a growing academic interest in ae for feature extraction back in 2006 researchers hinton and salakhutdinov 2006 came up with a novel idea of using ae for feature extraction on very large datasets afterward there are studies on the general ae framework for feature extraction wang et al 2014 and ae has proven to be a promising tool for feature extraction in network anomaly detection chen et al 2018 fault diagnosis san martin et al 2019 hyperspectral images ramamurthy et al 2020 and yearly electricity loads ryu et al 2020 nevertheless a suitable network structure such as the number of layers and the widths should be customized for data with different domains and meanings alkhayrat et al 2020 and so far there have been few studies using ae for feature extraction of high dimensional meteorological data such as kuroshio in roms 2 3 pso kennedy and eberhart 1995 proposed pso algorithm for solving nonlinear functions inspired by the social behaviors of animals in pso algorithm each particle in the population has a corresponding position velocity and fitness value objective function value besides the personal best positions and the global best position are also recorded during optimization the first step of pso is to initialize particles positions and velocities randomly after initialization the fitness value of each particle is calculated followed by updating the personal best position of the individual particle and the global best position of the particle swarm the velocities and positions of the particles are then updated according to eqs 5 and 6 where v e l k i denotes the velocity of the k th particle in the i th iteration p o s k i denotes the position of the k th particle in the i th iteration p b e s t k denotes the personal best position of the k th particle g b e s t denotes the global best position the total number of particles are l c 1 and c 2 are acceleration constants r a n d 0 1 represents random numbers in the range 0 to 1 and w denotes the inertia weight according to previous studies the inertia weight is first set high and decreases gradually with iterations shi and eberhart 1998 bansal et al 2011 this process is looped until the termination condition is satisfied the termination condition is usually related to the number of iterations or the objective function value 5 v e l k i w v e l k i 1 c 1 r a n d 0 1 p b e s t k p o s k i 1 c 2 r a n d 0 1 g b e s t p o s k i 1 k 1 2 l 6 pos k i p o s k i 1 v e l k i k 1 2 l it has been found that pso requires few parameters to be tuned and is easy to implement due to this simplicity pso algorithm is widely used in environmental kashani et al 2021 alrashidi and el hawary 2006 vishnu and t k 2020 naderi et al 2019 medical pervaiz et al 2021 and financial pradeepkumar and ravi 2017 fields nonetheless pso is likely to converge prematurely by falling into local optima when encountering complex multi modal functions ye et al 2013 larsen et al 2016 xu et al 2015 to be specific the personal best position of the individual particle and the global best position of the particle swarm may tend to converge to the local optimal position resulting in obtaining wrong optimization results and the failure of trading off between the global exploration and local exploitation even though researchers have introduced some modifications or improvements isiet and gadala 2019 liu et al 2020 tian et al 2019 zeng et al 2022 to pso premature convergence of pso remains a major challenge currently houssein et al 2021 2 4 bayesian optimization bayesian optimization is a technique for expensive black box global optimization mockus 1989 močkus 1975 bayesian optimization has two components the surrogate model and the acquisition function the surrogate model is a probabilistic model that approximates the objective function via a gaussian process regression a gaussian process featured by a prior mean function μ and a covariance function κ offers a distribution over functions f gp μ κ w l o g let μ 0 considering a collection of data pairs d n x f x where x x 1 x 2 x n is a set of input points f x f x 1 f x 2 f x n is the corresponding function values and n is the number of samples with new input data x the joint distribution of f x and f x is 7 f x f x n 0 κ x x κ x x κ x x κ x x then the conditional distribution denoted by p f x x x f x is a multivariant gaussian distribution and can be formulated as 8 p f x x x f x n μ x σ 2 x where 9 μ x κ x x κ x x 1 f x 10 σ 2 x κ x x κ x x κ x x 1 κ x x the squared exponential kernel duvenaud 2014 is frequently adopted as the covariance function and its hyperparameters including the length scale the signal variance and the noise variance are chosen via maximizing the log marginal likelihood the acquisition function denoted by α x is able to guide the search to reach the optimum since it determines the most promising point to sample during iterations see eq 11 and achieves the balance between exploration and exploitation commonly used acquisition functions frazier 2018 are expected improvement ei and upper confidence bound ucb 11 x argmax x x α x ei calculates the expected improvement associated with the current best function value denoted by f 12 α x e i x e max 0 f x f μ x f φ μ x f σ x σ x ϕ μ x f σ x where e φ and ϕ denote the expectation value the gaussian cdf and the gaussian pdf respectively ucb focuses on sampling points with high mean and high standard deviation 13 α x u c b x μ x β 1 2 σ x where β is a coefficient to weigh up exploitation the μ x term and exploration the σ x term β is first set high and then decreases gradually with iterations to enable more exploration in the early stage and more exploitation in the later stage algorithm 1 exhibits the pseudocode of the bayesian optimization algorithm in spite of its pervasive application to low dimensional black box problems less than approximately 20 dimensions bayesian optimization has encountered difficulties when extended to high dimensional domains as the search space grows exponentially with the dimensionality wang et al 2022 to tackle this issue researchers assuming that the objective function only depends on a handful of variables utilized the finite difference sequential likelihood ratio test fdt and the gp sequential likelihood ratio test gpt to determine the most contributing variables in high dimensional bayesian optimization chen et al 2012 this approach is usually summarized as the selection of variables besides the strategy of low dimensional embeddings has emerged such as random embedding bayesian optimization rembo wang et al 2016b its variants qian et al 2016 binois et al 2020 nayebi et al 2019 and so on moriconi et al 2020 dhamala et al 2020 and grosnit et al 2021 assuming that the data has underlying representations these methods can perform bayesian optimization in the low dimensional space and compute the objective function values in the original space moreover there has been a tactic based on certain structural assumptions for instance researchers kandasamy et al 2015 han et al 2020 wang et al 2018a b imposed additive structures on problems assuming that the objective function can be partitioned into functions with small disjoint sets of dimensions experiments revealed that this method outperforms standard bayesian optimization even on non additive functions some researchers oh et al 2019 also proposed bayesian optimization with cylindrical kernels to leverage cylindrical transformations 3 methods as presented in fig 2 the embedding optimization algorithm for solving cnop consists of four components which are data gathering feature extraction optimization in the low dimensional feature space and result acquisition among them the feature extraction step and the optimization step are the core parts the process of each step is explained in detail as follows step 1 obtain data the first step is the data gathering process where datasets with perturbation characteristics are usually constructed based on the specific numerical model constraint norm and physical mechanisms of the meteorological problem to be studied this step is the preparation for the subsequent feature extraction step the obtained samples are denoted by x x 1 x 2 x n where n is the total number of samples x n denotes the n th sample n 1 n and x n r h i g h d i m h i g h d i m is the dimensionality of the original space step 2 conduct feature extraction as for the feature extraction step we design a well structured deepfe model and compare it with pca to demonstrate its superior reconstruction performance step 2 1 preprocess the first procedure in this part is non dimensionalization conesa et al 2016 where pca utilizes standardization to make the mean and standard deviation of all data equal to zero and one respectively and the deepfe utilizes min max scaling of the range 0 1 step 2 2 generate low dimensional representation pca performs mappings between the original and low dimensional space through a fixed feature matrix while the deepfe performs mappings by an adequately trained encoder and decoder c p s and r c s are constructed in this step as shown in eqs 14 and 15 c p s is responsible for compressing the original data to the low dimensional space and r c s is responsible for reconstructing the low dimensional data to the original high dimensional space 14 z n c p s x n 15 x ˆ n r c s z n step 3 perform optimization in the low dimensional feature space regarding the third step we employ the paralleled bayesian optimization with additive kernels and make a comparison with pso after completing the feature extraction process step 2 the optimization of high dimensional cnop see eq 1 can be converted into the low dimensional optimization problem see eq 16 the reduction of the search space can significantly improve the search efficiency 16 x 0 r c s z 0 argmax r c s z 0 c δ f r c s z 0 step 3 1 sample a potential solution pso and our proposed peboa differ in the rules of sampling the potential solution in the low dimensional space specifically peboa relies on the acquisition function to determine the most promising candidate point in each iteration see eq 11 while pso relies on the personal best positions and the global best position to update the velocity and position of each particle see eqs 5 and 6 step 3 2 reconstruct the low dimensional point to the original space this step involves the feature matrix of pca or the decoder of the deepfe to reconstruct the selected low dimensional candidate point to the original space for the calculation of the objective function value step 3 3 compute the objective function value after conducting step 3 2 the selected candidate point denoted by x ˆ is superimposed on the reference state at the initial moment to generate the corresponding initial field and then the roms model is called to obtain the state vector o t at the prediction time shown in eq 17 the objective function value can be calculated using eq 18 17 o t m t x 0 x ˆ 18 f x ˆ o t m t x 0 o b j step 4 output cnop finally the optimal solution in the low dimensional feature space is reconstructed to the original space to obtain the cnop since the concepts of pca and pso have been mentioned in section 2 and the literature mu et al 2015a offers implementation of pca based pso the remainder focuses on peboa including descriptions of its components and the pseudocode 3 1 deepfe section 2 has introduced the basic concepts and applications of ae thus this section elaborates on deepfe s specific structure and details in the proposed peboa 3 1 1 model structures in studying the relations between network structures and performance researchers found that the deeper the network gets the more significant the negative impacts on representation learning performance become he et al 2015 this issue occurs due to the vanishing gradients of deep neural networks and can be addressed by incorporating residual connections zaeemzadeh et al 2020 or skip connections huang et al 2018 srivastava et al 2015 ronneberger et al 2015 thus we employ the residual network as the backbone for both the encoder and the decoder since it allows for a larger network without feature learning performance degradation fig 3 presents an overview of the proposed deepfe we design five models each is composed of the encoder and decoder for the five variables v u t e m p s a l t and z e t a refer to table 2 for the meaning and dimensionality respectively with the aim that components in the encoder and decoder correspond as closely as possible to each other these five models are trained together by the loss function involving five variables see appendix the designed ae structures of two variables z e t a and v are shown in fig 4 because the other three variables structures are nearly the same as v s ae structures we do not depict them graphically z e t a s ae structure is distinguished from others owing to the dimensionality i e the input channels for zeta are 1 while the input channels for the other four variables are 32 in addition z e t a l o w d i m and v l o w d i m in fig 4 indicate the dimensionality of z e t a s and v s hidden representation respectively and the meanings of u l o w d i m t e m p l o w d i m and s a l t l o w d i m are the same as above moreover z e t a h i g h d i m in fig 4 indicates the actual dimensionality of zeta and the meanings of v h i g h d i m u h i g h d i m t e m p h i g h d i m and s a l t h i g h d i m are the same as above the number of repeats of the basic block denoted by r e p e a t n u m in fig 4 determines the complexity of the models considering that the dimensionality of z e t a is smaller than others r e p e a t n u m in the ae structure of z e t a is also set lower and in other variables structures r e p e a t n u m is set higher to achieve satisfactory feature learning note that the last layer uses the sigmoid function as the activation function because in pre processing step the input data is scaled to the range from 0 to 1 this activation function ensures that the output data is also in this range as illustrated in fig 5a the encoder modules consist of three types of blocks the fc block with a series of full connection operations the conv block including two dimensional convolution batch normalization and the activation function relu and the basic encoder block including several conv blocks and residual connections represented by operator where the first connection is used to increase the number of channels in the network and the rest of connection operations do not change the number of channels in the network fig 5b displays the decoder modules they consist of three types of blocks the fc inverse block with a series of full connection operations that correspond to the fc block in the encoder modules the conv transpose block including two dimensional deconvolution batch normalization and the activation function relu and the basic decoder block including several conv transpose blocks and residual connections where the first connection is used to decrease the number of channels in the network and the rest of connection operations do not change the number of channels in the network 3 1 2 loss function since we want the input and output of the deepfe model to be as consistent as possible the reconstruction error the mean squared error is introduced into the loss function 19 m s e 1 d j 1 d x i j x i j 2 where x i j is the j th dimension of the i th input sample x i j is the j th dimension of the i th output sample and d denotes the dimensionality of samples besides to make sure the output namely the initial perturbation satisfies the constraint of cnop refer to eq 1 in section 2 a constraint preserved penalty term is added to the loss function 20 p e n a l t y λ p n t max 0 x ˆ i c 2 δ 2 where x i is the i th output sample c denotes the constraint norm δ 2 is the constraint value and λ p n t is a constant coefficient this item incorporates the physical constraint whose purpose is to keep the reconstructed data within the boundary of the constraint additionally the ℓ 2 regularization is brought into the loss function to prevent overfitting and enhance generalization 21 r e g λ r e g i 1 ɛ θ i 2 where θ i represents the i th parameters of the neural network ɛ denotes the total number of parameters and λ r e g is a constant coefficient in summary the loss function is written as eq 22 22 l θ m s e p e n a l t y r e g 3 2 bayesian optimization with additive gaussian kernels the fundamental principles of bayesian optimization are introduced in section 2 4 we extend it to high dimensional problems assuming that the objective function f decomposes into the following form 23 f x f v v f u u f t e m p t e m p f s a l t s a l t f z e t a z e t a where v u t e m p s a l t and z e t a denote the five variables that x has see the case study in section 4 in addition to the standard assumption that f is sampled from a gaussian process the assumption is made that each f v a r v a r is also sampled from an independent gaussian process denoted by f v a r gp μ v a r κ v a r where v a r stands for the element of the set v u t e m p s a l t z e t a therefore μ x and κ x x can be rewritten as 24 μ x μ v v μ u u μ t e m p t e m p μ s a l t s a l t μ z e t a z e t a 25 κ x x κ v v v κ u u u κ t e m p t e m p t e m p κ s a l t s a l t s a l t κ z e t a z e t a z e t a taking κ v as an example it only operates on one variable v in small dimensions while the kernel of a standard bayesian optimization works on all variables in large dimensions considering d n the same as d n mentioned in section 2 4 with a variable e g v of new input data the joint distribution of f x and f v v is 26 f x f v v n 0 κ x x κ v v v κ v v v κ v v v where v v 1 v 2 v n then the posterior for f v v can be written as 27 p f v v v x f x n μ v v σ 2 v v where 28 μ v v κ v v v κ x x 1 f x 29 σ 2 v v κ v v v κ v v v κ x x 1 κ v v v as for the acquisition function we modify ucb to an additive version namely ucb add which is a sum of functions on orthogonal domains ucb add is formulated as follows 30 u c b x μ x β 1 2 v a r v u t e m p s a l t z e t a σ v a r v a r u c b v v u c b u u u c b t e m p t e m p u c b s a l t s a l t u c b z e t a z e t a where u c b v v μ v v β 1 2 σ v v and the other four items are expressed in the same way as u c b v v note that for simplicity we write σ v v σ u u σ t e m p t e m p σ s a l t s a l t σ z e t a z e t a as v a r v u t e m p s a l t z e t a σ v a r v a r instead of maximizing u c b x in all dimensions we can maximize u c b x by maximizing each u c b v a r v a r in a few dimensions kandasamy et al 2015 which saves computational resources and achieve the desirable accuracy 3 3 parallelization this section introduces the parallelized version of ucb add ucb add pe which selects several points simultaneously rather than a single point for evaluation via the modified acquisition function we adopt the idea of combining the upper confidence bound strategy and the pure exploration strategy called gp ucb pe in the literature contal et al 2013 and propose ucb add pe to make it applicable to additive structures briefly speaking ucb add pe selects the first query point x 1 via ucb add see eq 31 and the remaining points x 2 x 3 x q via maximizing the updated variance see eq 32 the number of parallelization or the batch size is denoted by q 31 x 1 argmax x x u c b x 32 x m argmax x r v a r v u t e m p s a l t z e t a σ m v a r v a r m 2 3 q in eq 32 σ m v a r v a r represents the updated variance for a specific variable after selecting x 1 x 2 x m 1 points and r represents the relevant region the formal definition of r is illustrated as 33 r x x μ x 2 β t 1 1 2 v a r v u t e m p s a l t z e t a σ v a r v a r l c b where β t 1 denotes the value of the parameter β in the next iteration and the definition of l c b is illustrated below 34 l c b max x x l c b x 35 l c b x μ x β 1 2 v a r v u t e m p s a l t z e t a σ v a r v a r l c b denotes the modified lower confidence bound with additive structures in accordance with u c b the purpose of incorporating β t 1 rather than the current value β is to ensure that current sampling will have an impact on the selection of points in the next iteration as can be seen from the above equations r not only discards regions where the optimum does not fall with high probability but also contains regions where the maximum position of u c b x in the next iteration belongs to with high probability contal et al 2013 literature contal et al 2013 has offered theoretical guarantees for gp ucb pe and algorithm 2 illustrates the pseudocode of peboa 4 case study the roms model for the upstream kuroshio transport in this section we introduce the cnop concepts for the roms model of the upstream kuroshio transport including a brief description of the upstream kuroshio transport the simulation of the roms model and the exact mathematical forms of the objective function and constraint norm kuroshio originates from eastern luzon and has vital impacts on global climate change mid and high latitude sea air interactions marine ecosystems and fisheries zhang 2002 tsukamoto 2006 kwon et al 2010 wu et al 2012 qiu et al 2014 the upstream kuroshio which ranges from the origin to southern taiwan is subject to physical processes such as enso asian monsoon and mesoscale eddies yaremchuk and qu 2004 qiu and lukas 1996 lien et al 2014 kim et al 2004 chiang et al 2015 resulting in a rapid decrease of transport in autumn accurate prediction of this seasonal decrease is essential to improve the forecasting capability of kuroshio transport nevertheless the impact of the above mentioned physical processes causes considerable uncertainty in the forecast so it is necessary to conduct a predictability study to identify the initial error perturbation that has the largest influence on the forecast uncertainty of this seasonal decline by utilizing the cnop method according to the literature zhang et al 2016 roms can be employed to simulate the transport variation of the upstream kuroshio as displayed in fig 6a it models the seasonal transport reduction of upstream kuroshio in a region with a latitude ranging from 0 3 n to 29 n a longitude ranging from 112 e to 162 e a horizontal resolution of 0 125 and 32 sigma levels in this region the state vector consists of 5 parameters v u t e m p s a l t and z e t a the meaning and dimensionality of these parameters are listed in table 2 the reasons for selecting these five physical variables are that upstream kuroshio transport is associated with these factors specifically since kuroshio is a kind of continuous current the transport may be influenced by the meridional velocity and zonal velocity besides sea surface temperature lombard et al 2009 stammer et al 2013 sea level yang et al 2001 and high salinity water wang et al 2015 are significantly correlated with the current shift in the case study the initial state is the same as event 2 in the literature zhang et al 2016 starting from the 12466th day referred to as day 0 to the 12530th day referred to as day 64 fig 6b exhibiting the daily upstream kuroshio transport from day 0 to day 64 indicates a drastic transport reduction of about 8 sv 1 sv 1 0 6 m 3 s the objective function is defined as the square of the transport variation flowing through the target section s at the prediction time t 36 f x 0 m t x 0 x 0 m t x 0 o b j s δ v t d x d y 2 where s is the section from philippine coast to 124 e in the upper 600 meters at 18 n the initial state vector is denoted by x 0 v 0 u 0 t e m p 0 s a l t 0 z e t a 0 and the initial perturbation state vector is denoted by x 0 δ v 0 δ u 0 δ t e m p 0 δ s a l t 0 δ z e t a 0 the error development state vector at the prediction time t can be written as 37 x t δ v t δ u t δ t e m p t δ s a l t t δ z e t a t m t x 0 x 0 m t x 0 note that every variable for example δ v t is determined by the nonlinear propagator m t x 0 and x 0 so δ v t cannot be written as m t v 0 δ v 0 m t v 0 the constraint norm of initial solutions is defined as the total energy in the simulated region 38 x 0 c 2 ρ r e f 2 δ v 0 2 δ u 0 2 g ρ r e f α δ t e m p 0 β δ s a l t 0 ρ r e f n 0 2 d x d y d z ρ r e f 2 g δ z e t a 0 2 d x d y δ 2 where ρ r e f equals to 1027 kg m 3 g is the gravity acceleration α equals to 1 7 1 0 4 k 1 β equals to 7 6 1 0 4 psu 1 n 0 denotes the brunt väisäl a frequency and δ equals to 5 1 0 6 j 1 2 as can be seen from fig 6b after superimposing cnop the transport may be either greater than the reference state a weaker transport decline or less than the reference state a stronger transport decline at the prediction time day 64 we denote the former as cnop1 and the latter as cnop2 the validity of cnop is determined by the magnitude of the absolute value of the transport change after superimposing the cnop compared to the reference state at the prediction time the superimposed cnop is valid if it produces an absolute value of the transport variation greater than 3 sv and vice versa zhang et al 2016 5 experiments and results this section presents the data and experimental settings followed by the assessment of deepfe and the performance evaluation of the proposed method and finally the physical mechanism analysis 5 1 data and experimental settings experimental settings are described in this section the total number of 4000 perturbation samples is obtained via the roms model zhang et al 2016 and hypersphere sampling voelker et al 2017 its theoretical proof yong ming 2008 and experimental validation are presented in the literature zhang et al 2016 wang et al 2012 jiang et al 2008 duan et al 2004 in the process of training the deepfe model 80 percent of samples are for training and 20 percent are for validation tables 3 and 4 list the parameter settings of pso and bayesian optimization most of which refer to empirical values table 5 lists the dimensionality settings of the hidden representation in deepfe and experiments on these dimensionality settings are conducted given that the reconstruction error is the dominant cost the two constant coefficients in deepfe λ p n t and λ r e g should be configured so that these two items account for ten percent of the reconstruction error at the beginning of training see table 6 since running the roms model requires considerable computational resources the maximum number of function evaluations is 600 in the experiments we examine the performance of each method with 600 function evaluations except for the solving efficiency part in section 5 3 2 and the gain analysis part in section 5 3 4 experiments are conducted on computers with 32 amd ryzen 9 5950x 16 core processors and nvidia gpu 5 2 analysis of deepfe in this section we first illustrate the selections of parameters in deepfe since the values of parameters might influence the feature learning performance then comparisons between deepfe and pca are presented in section 5 2 2 to further validate the superiority of deepfe 5 2 1 parameter selections according to the literature smith 2018 the value of batch size and learning rate might impact the performance of the neural network as these two hyperparameters directly determine the update of model weights the value of batch size affects the generalization of models while the learning rate affects the convergence of models and the two can also influence each other therefore it is necessary to make a wise selection of batch size and learning rate the relative error ratio criterion r e r see eq 39 yuan et al 2022 mu et al 2015a is adopted to evaluate the capability of feature learning 39 r e r x x ˆ f x f e where x is the input data x ˆ is the output data x f is frobenius norm and e is a small constant experiments have been conducted with the batch size of 8 16 and 32 and the learning rate of 10 2 10 3 and 10 4 respectively fig 7 illustrates the relative error ratio of the validation data with respect to the number of epochs as can be observed the batch size does not have a noticeable effect on the model compared to the learning rate whereas as the value of the learning rate decreases the degree of overfitting decreases and the epoch to reach the minimum relative error ratio increases the early stopping strategy is adopted during training and the model at the epoch close to the best relative error ratio is selected to further investigate the influence of parameters on optimization we carry out experiments on peboa with the default parallel number and deepfepso with 31 dimensional latent representation termed as peboa 31d and deepfepso 31d 62 dimensional latent representation termed as peboa 62d and deepfepso 62d and 124 dimensional latent representation termed as peboa 124d and deepfepso 124d the quantitative results for five runs are shown in table 7 as shown in table 7 with the optimization method and the dimensionality of latent representation fixed for example peboa 31d models with different values of batch size and learning rate yield similar objective function values accordingly in the following experiments we choose moderate values of batch size and learning rate 16 and 10 3 correspondingly additionally table 7 also indicates the superior performance of our proposed method in comparison with deepfepso as better objective function values are obtained via peboa in each dimensional setting this is further validated in section 5 3 in summary based on the relative error ratio curves of validation data in fig 7 and the optimization results for each combination of parameters the batch size and learning rate of the deepfe model are established 5 2 2 comparison with pca first of all we examine the representation learning performance of deepfe and pca measured in terms of relative error ratio fig 8 displays the relative error ratio of pca with respect to the number of principal components and the relative error ratio given by deepfe using 31 62 and 124 dimensional latent features marked with horizontal dotted lines in terms of pca there is an improvement in the performance of feature learning with the increasing number of principal components however to achieve the performance given by deepfe 31d up to 146 principal components are necessary not to mention to match the performance of deepfe 62d and deepfe 124d specifically deepfe 31d s relative error ratio is approximately 81 6 smaller than that of pca 31d deepfe 62d s relative error ratio is about 68 7 smaller than that of pca 62d deepfe 124d s relative error ratio is around 34 8 smaller than that of pca 124d in brief the relative error ratio of deepfe is on average about 61 7 smaller than that of pca this demonstrates that compared with pca deepfe is capable of achieving even better feature learning performance with fewer feature dimensions in addition to compare deepfe s and pca s reconstruction skills in the optimization process we perform experiments on deepfe incorporated optimization methods called peboa and deepfepso and pca incorporated optimization methods called pcabo and pcapso for solving cnop as summarized in fig 9 in each dimensionality setting the deepfe based optimization not only has a larger average objective function value over five runs but also has a higher probability of obtaining the valid cnop regardless of whether the optimization algorithm is our proposed method or pso taking peboa 31d as an example its average objective function value is larger than that of pcabo 31d and the valid cnop can be gained in two out of five runs while pcabo 31d cannot obtain the valid cnop in all five runs it is the nonlinearity and specialized training of deepfe that results in a smaller relative error ratio and more information retained therefore in solving cnop employing deepfe as the feature extraction component is more expressive than employing the linear pca besides deepfe outperforms pca with respect to identifying feature dimensions according to fig 9 both pca based parallel bayesian optimization with additive gaussian kernels and pca based pso can acquire the valid cnop if the dimensionality of the compressed representation is set to 62 and cannot acquire the valid cnop if it is set to 31 on the contrary deepfe based optimization can obtain the valid cnop except for deepfepso 31d this is because that pca relies on a fixed feature matrix which is not designed for a specific feature dimension in contrast deepfe with specific feature dimensions is purposely trained so mappings between high and low dimensional spaces in the deepfe model cause less information loss this indicates that the determination of the latent feature s dimensionality for deepfe based optimization is more flexible than pca based optimization in solving cnop it should be mentioned that no matter whether the optimization algorithm is bayesian optimization or pso the optimization results with the 62 dimensional hidden feature are better than those with the 31 dimensional hidden feature while the results with the 124 dimensional hidden feature are worse than those with the 62 dimensional hidden feature for instance pcabo 62d outperforms both pcabo 31d and pcabo 124d the theoretical explanation is that even though the relative error ratio decreases as the number of feature dimensions increases the optimization task also becomes tricky due to the growth of the search space hence in solving cnop it is necessary to strike a trade off between feature extraction and optimization for the detailed experimental explanation see the gain analysis on the number of function evaluations and feature dimensions in section 5 3 4 by comparing deepfe and pca itself and assessing them in the optimization process it is concluded that our deepfe excels pca in terms of both feature learning performance and reconstruction capability in solving cnop 5 3 evaluations on performance of our method in sections 5 3 1 and 5 3 2 the proposed method is compared against deepfepso measured by the quality of the obtained cnop and the solving efficiency sections 5 3 3 and 5 3 4 illustrate the acquisition function analysis and the gain analysis on the number of function evaluations and feature dimensions 5 3 1 quality of solutions the assessment of the quality of solutions involves two metrics the magnitude of the objective function values and the likelihood of obtaining valid solutions for every dimensionality setting of latent features we conduct experiments on deepfepso peboa with the default parallel number termed as peboa q4 peboa with the parallel number of 2 termed as peboa q2 peboa without parallelization termed as eboa and peboa without parallelization and additive kernels termed as ebo quantitative results over 20 runs for each method are concluded in figs 10 and 11 among these methods ebo ranks the worst since naïve bayesian optimization is very sensitive to the dimensionality of the search space because ebo cannot obtain comparable objective function values or valid cnop we do not take it into consideration in subsequent experiments fortunately ebo with additive gaussian kernels namely eboa produces significantly better results than ebo specifically compared to ebo ebo 62d to be exact the objective function value of eboa eboa 62d to be exact is 26 5 larger and eboa has a higher probability of obtaining the valid cnop in each dimensional setting this confirms the necessity and effectiveness of incorporating additive gaussian kernels in addition as can be seen from the results of eboa equivalent to peboa q1 peboa q2 and peboa q4 these methods obtain quite similar objective function values and the same likelihood of obtaining the valid cnop which indicates that the parallel numbers have no noticeable effects on either the magnitude of objective function values or the number of valid cnop in fact the major advantage of parallelization is in the aspect of solving efficiency which is analyzed in section 5 3 2 based on the above analysis we select peboa with the best results i e peboa q4 and make a comparison with deepfepso fig 12 visualizes the objective function values over 20 runs for deepfepso and peboa q4 where the darker the color the larger the magnitude of the objective function values it indicates that our proposed method peboa has more dark color blocks in each dimensional setting i e in general it can generate larger objective function values in solving cnop deepfepso on the opposite has more light color blocks especially deepfepso 31d and deepfepso 124d where the lighter color blocks occupy the majority of the 20 blocks this suggests that peboa has merit over deepfepso with regard to the magnitude of objective function values in addition to the qualitative analysis quantitative results are concluded in figs 10 and 11 as summarized in fig 10 for every dimensional setting the average and maximum objective function values of peboa q4 are greater than the values of deepfepso apart from the magnitude of objective function values we also compare peboa q4 and deepfepso in the aspect of the validity of cnop see fig 11 based on the validity of the corresponding cnop the objective function values are divided into three intervals 8 5 1 0 12 denoted by interval a 8 5 1 0 12 9 5 1 0 12 denoted by interval b and 9 5 1 0 12 denoted by interval c cnop with the objective function value in interval c can lead to a significant decrease in upstream kuroshio transport i e the valid cnop cnop with the objective function value in interval b are likely to cause a significant decrease cnop with the objective function value in interval a cannot contribute to significant upstream kuroshio transport decreases over 20 runs the three dimensional settings of deepfepso namely deepfepso 31d deepfepso 62d and deepfepso 124d could not obtain the solution with the objective function value in interval c in comparison peboa q4 except peboa 124d q4 can all acquire the solution in interval c the results of peboa 124d q4 are not in interval c because the search space grows with the increase of feature dimensions moreover based on the 20 runs it can be observed that the probability of obtaining the valid cnop for peboa 31d q4 is 30 which is three times higher than that for deepfepso 31d likewise the probability of obtaining the valid cnop for peboa 62d q4 is 55 which is about 2 2 times higher than that for deepfepso 62d similarly there is a 35 probability of obtaining the valid cnop for peboa 124d q4 versus 20 for deepfepso 124d in short our proposed method has a higher probability of obtaining valid cnop than deepfepso in a nutshell we first compare the bayesian optimization involved methods ebo is considerably inferior in the magnitude of objective function values and the validity of cnop compared to that with additive kernels which verifies the effectiveness of additive kernels secondly we evaluate peboa and deepfepso in the aspects of the magnitude of the objective function values and the validity of cnop experiments confirm that peboa surpasses deepfepso concerning the quality of the obtained cnop 5 3 2 solving efficiency the assessment of solving efficiency includes two metrics time efficiency and convergence speed according to the experiments in section 5 3 1 experiments in this section adopt the feature dimensions with the best results i e peboa 62d q4 peboa 62d q2 eboa 62d and deepfepso 62d conceptually as presented in eq 40 the running time termed as t c n o p is composed of three components the time for searching the promising sampling point denoted by t s e a r c h the time for reconstructing the low dimensional point to the original space denoted by t r e c o n s t r u c t i o n and the time for calculating the objective function value via calling roms denoted by t r o m s fig 13a illustrates the running time of peboa 62d q4 peboa 62d q2 eboa 62d equivalent to peboa 62d q1 and deepfepso 62d over 20 runs overall the running time of eboa 62d is longer than deepfepso 62d because training the gaussian process and maximizing the acquisition function in bayesian optimization i e t s e a r c h in eq 40 cost more time than updating particles velocities and positions in pso 40 t c n o p t s e a r c h t r e c o n s t r u c t i o n t r o m s however the wall clock time does not take into account the validity of cnop thus we introduce t v a l i d to represent the actual time for obtaining the valid cnop in eq 41 t c n o p denotes the average wall clock time over 20 runs n r u n s indicates the number of runs i e 20 and n v a l i d indicates the number of valid cnop obtained refer to fig 11 as illustrated in fig 13b t v a l i d of eboa 62d is 194475 65 s less than deepfepso 62d this authenticates the time efficiency of the proposed method even without parallelization additionally it can be observed that the wall clock time and the actual time for obtaining the valid cnop in peboa decrease as the parallel number increases to be exact peboa 62d q4 and peboa 62d q2 save 71 0 and 44 8 of the computation time correspondingly compared to eboa 62d which confirms that the parallelism implemented by our modified acquisition function can economize computation time 41 t v a l i d t c n o p n r u n s n v a l i d to analyze the convergence performance fig 14 summarizes the quantitative results for 700 function evaluations of peboa 62d q4 peboa 62d q2 eboa 62d equivalent to peboa 62d q1 and deepfepso 62d over 20 runs and fig 15 exhibits the relationship between objective function values and the number of function evaluations divided by the parallel number for the 700 function evaluations the maximum number of iterations for pso and bayesian optimization is set to 35 and 700 respectively quantitative results in figs 14 and 10 in section 5 3 1 confirm that on condition that the maximum number of function evaluations increases from 600 to 700 there is no apparent difference in the mean objective function values in detail the objective function values for peboa 62d q4 peboa 62d q2 eboa 62d and deepfepso 62d increase by 0 303 0 545 0 436 and 0 596 respectively as shown in fig 15 eboa 62d reaches convergence at approximately 520 function evaluations while deepfepso 62d achieves convergence when the number of function evaluations is about 580 it can be demonstrated that in comparison with deepfepso suffering from local stagnation our proposed method even without parallelization can achieve more promising results with fewer function evaluations in addition the objective function value of eboa 62d rises faster in the early stage proving the strength of our proposed method in optimizing computationally expensive functions moreover from the aspect of parallelism our method reaches convergence in fewer function evaluations as the parallel number increases which further corroborates that our parallel implementation can save computational resources in brief compared to deepfepso our proposed method with or without parallelism has a faster convergence rate and spends less time in obtaining the valid cnop 5 3 3 acquisition function analysis for further investigation of the acquisition function we compare the proposed ucb add with two frequently adopted acquisition functions namely ucb termed as eboa ucb 62d and ei termed as eboa ei 62d in 700 function evaluations with 62 feature dimensions figs 14 and 15 summarize quantitative results and the best objective function value with respect to the number of function evaluations over 20 runs respectively it can be seen that eboa 62d outperforms the other two in terms of the magnitude of objective function values the number of valid solutions and the convergence rate besides eboa 62d and eboa ucb 62d has objective function values in interval c while eboa ei 62d does not therefore it can be concluded that the proposed ucb add has the best performance in the roms model of upstream kuroshio transport followed by ucb and finally by ei 5 3 4 gain analysis this section presents the gain analysis on the number of function evaluations and feature dimensions theoretically results of peboa 124d and deepfepso 124d should be better than those of peboa 62d and deepfepso 62d in the absence of a limit on the number of function evaluations since more feature dimensions contain more useful information however more feature dimensions mean the broader search space during optimization and the computational cost increases with the number of function evaluations therefore it is necessary to strike a balance between feature dimensions and optimization in this section we figure out whether the optimization results of peboa 124d q4 and deepfepso 124d deserve additional number of function evaluations we conduct peboa 124d q4 and deepfepso 124d with 900 function evaluations over five runs and make a comparison with peboa 62d q4 and deepfepso 62d with 600 function evaluations over five runs results have shown in fig 9 of section 5 2 comparing fig 9 with fig 16 it can be seen that with the number of function evaluations becoming 1 5 times higher the mean objective function values of peboa 124d q4 and deepfepso 124d only increase by 1 66 and 1 87 compared to those of peboa 62d q4 and deepfepso 62d respectively therefore even though peboa 124d q4 and deepfepso 124d with 900 function evaluations achieve better results this is not a worthwhile effort considering the investment of computational resources and the outcome 5 4 physical mechanism analysis in this section we first probe into prediction errors obtained by peboa q4 and deepfepso then the spatial structure of cnop is analyzed to further validate the efficacy of peboa q4 the adjoint model adj is taken as the benchmark in addition section 5 4 3 presents the analysis regarding the evolution of the obtained cnop over time 5 4 1 prediction errors we analyze the prediction errors induced by cnop for each method where we superimpose the obtained cnop on the reference state at the initial moment and observe the upstream kuroshio transport at the prediction time fig 17 lists the upstream kuroshio transport changes induced by cnop for each method over 20 runs peboa q4 and deepfepso can obtain two types of cnop cnop1 and cnop2 refer to the case study in section 4 considering the validity peboa q4 in every dimensional setting can yield valid cnop1 and cnop2 while only deepfepso 62d and deepfepso 124d can yield valid cnop1 and cnop2 besides cnop obtained by peboa q4 can result in more significant prediction errors for example cnop1 of deepfepso 62d incurs an increase in the upstream kuroshio transport by approximately 3 08 sv only while cnop1 of peboa 62d q4 incurs an increase in the upstream kuroshio transport by about 3 20 sv moreover peboa q4 peboa 62d q4 to be exact is closer to the results of adj i e 3 1445 sv and 3 2773 sv as far as the increase and decrease of transport are concerned in addition considering the number of cnop1 and cnop2 obtained deepfepso prefers to acquire cnop1 while peboa q4 has no such preference i e both cnop1 and cnop2 can be acquired to demonstrate that the cnop acquired from peboa q4 is the optimal perturbation i e the optimally growing initial error we observe whether other types of perturbations can lead to dramatic transport changes fig 18 displays the prediction errors at the forecasted time after random perturbations are superimposed on the initial reference state in experiments of random perturbations the prediction errors become random and there are both increases and decreases in the upstream kuroshio transport the absolute values of these errors are far smaller than 3 sv which further justifies that the perturbations obtained by our method are the optimal initial perturbations 5 4 2 spatial structure of cnop to further examine the performance we investigate the spatial structure of cnop since baroclinic instability contributes to the rapid growth of errors zhang et al 2016 qiu 1999 qiu and chen 2010 chang and oey 2014 figs 19 and 20 depict the velocity and temperature components distribution in the upper 300 meters of the two types of cnop note that we select cnop that results in the most significant transport change for each method tables 8 and 9 display the cosine similarity between the cnop obtained by adj and the cnop obtained by peboa deepfepso as for cnop1 fig 19a b and c indicate that cnop1 obtained by deepfepso and peboa q4 approximates cnop1 obtained by adj fig 19d and e illustrate the exact difference of temperature components and the sum of squared error between deepfepso and adj is 2 92 1 0 5 while the sum of squared error between peboa q4 and adj is smaller 4 01 1 0 6 to be exact as can be seen from table 8 compared to deepfepso the cnop1 obtained by peboa q4 is more similar to the cnop1 of adj as for cnop2 deepfepso yields less satisfactory results than peboa q4 because the large amplitudes of the temperature and velocity components lie in the region of 123 e 127 e 16 n 19 n which is contradictory to the large amplitudes area mentioned in the literature 127 e 130 e 16 n 19 n zhang et al 2016 besides the sum of squared errors for deepfepso is more significant than peboa q4 with values of 1 98 1 0 4 and 3 78 1 0 5 respectively as can be observed in fig 20d and e according to table 9 compared to deepfepso the cnop2 obtained by peboa q4 is more similar to the cnop2 of adj in brief sections 5 4 1 and 5 4 2 substantiate the superiority of our proposed method over aepso in terms of the amplitude of upstream kuroshio transport variability and the structure of obtained cnop 5 4 3 evolution of cnop this section investigates the error development after superimposing cnop to clarify how cnop leads to such prominent prediction errors we first analyze the error fields at the prediction time day 64 after superimposing cnop obtained by our method see figs 21 and 22 figs 21a and 22a exhibit the errors in meridional velocity at the prediction time on the 18 n section after superimposing two types of cnop with cnop1 superimposed anomalous northward meridional currents with a maximum value of approximately 14 cm s are found within the target section s and the related vertical center is located at about 300 m similarly after superimposing cnop2 there are anomalous southward currents in s with a maximal value of about 14 cm s and the vertical center is also near 300 m to pinpoint the reason for these meridional velocity anomalies in the target section we study the mean velocity error fields within the upper 600 m at the prediction moment figs 21b and 22b as for cnop1 an anticyclonic error exists near 18 n with its horizontal center located at 124 e 18 n it is this anticyclonic eddy like structure that leads to the northward meridional velocity error in s and subsequently the anomalous increase of upstream kuroshio transport at day 64 in contrast after superimposing cnop2 a cyclonic eddy like structure exists at approximately the same position as the anticyclonic error described above which accounts for the southward meridional velocity anomaly in the target section and the reduction of transport in sum after superimposing cnop1 cnop2 an anticyclonic cyclonic eddy like structure emerges in the target section at day 64 which causes a northward southward meridional velocity anomaly and an anomalous rise decline in upstream kuroshio transport to explore how the two types of optimal initial errors develop into the anticyclonic or cyclonic eddy like structure we analyze the evolution of cnop from day 0 to day 64 figs 23 and 24 depict error evolutions at 300 m over the prediction period for cnop1 and cnop2 respectively as illustrated in fig 23a a very small anticyclonic eddy like structure appears near 128 e 17 n at day 0 where the maximum amplitude of the anticyclonic velocity error is about 0 2 cm s this anticyclonic eddy like structure propagates westward and grows rapidly with time at day 32 see fig 23c the position of its horizontal center has propagated to 126 e and the maximum velocity error has risen to about 5 0 cm s which is as much as 25 times the amplitude at the initial moment at the end of the prediction see fig 23f its central position reaches the location of 124 e 18 n and the maximum velocity error has reached about 15 0 cm s which is about 75 times the velocity amplitude at the beginning of the prediction similar to fig 23 fig 24 shows that after superimposing cnop2 there is a cyclonic eddy like structure with very small amplitude at the initial moment near 128 e which travels westward reaching 124 e 18 n eventually and intensifies promptly with a final maximum velocity error of about 15 0 cm s it can be concluded that the development of cnop1 and cnop2 during the prediction period is characterized by two features 1 one is the westward propagation of errors specifically from day 0 to day 64 the central position of anticyclonic or cyclonic errors propagates westward by about 4 degrees of longitude and northward by one degree of latitude finally arriving near 124 e 18 n thus triggering changes in the upstream kuroshio transport their main propagation direction is westward 2 the other is the fast growth of anticyclonic or cyclonic errors during the forecast period resulting in an increase in the maximum velocity anomaly by about 75 times which eventually leads to considerable prediction errors these two features are indispensable for optimal initial errors to induce significant prediction errors in the upstream kuroshio transport it should be mentioned that the structures and evolutions of the cnops are similar with those obtained with the adjoint method zhang et al 2016 this suggests the cnops obtained by our proposed peboa are convincible 6 conclusions in this paper we propose a paralleled embedding high dimensional bayesian optimization with additive gaussian kernels for solving high dimensional cnop called peboa to alleviate the problem of intelligent optimization algorithms trapped in local optima and the 20 dimensional search space restriction of ordinary bayesian optimization algorithm and to exploit the computational resources adequately in peboa the deepfe using the convolutional ae with residual connections and customized constraint preserved loss function is designed to compress 10 million dimensional data to relatively low dimensional search spaces then we propose the strategy of additive gaussian kernels exploiting which the bayesian optimization algorithm can optimize in relatively low dimensional search spaces additionally to accelerate the optimization process the acquisition function is modified to sample multiple candidates simultaneously we select the upstream kuroshio transport variation in roms a 10 million dimensional and computationally costly problem as the case study to investigate the performance of peboa experimental results show compared to pca the relative error ratio of deepfe is on average about 61 7 smaller demonstrating that deepfe is capable of achieving better feature learning performance with fewer feature dimensions using deepfe for dimensionality reduction compared to pca peboa and pso both not only get larger mean objective function values but also have higher probabilities of obtaining the valid cnop effectiveness of incorporating the strategy of additive gaussian kernels is also verified which improves the magnitude of objective function values by about 26 5 and obtains valid cnop the modified acquisition function for sampling multiple candidates simultaneously reduces computation time by about 71 0 with four cores compared to deepfepso peboa has 3 4 larger objective function values and 2 2 times greater likelihood of obtaining the valid cnop and it also has a faster convergence rate and takes less time to obtain the valid cnop in a word peboa surpasses deepfepso in terms of the quality of obtained cnop and solving efficiency we also conduct the gain analysis and draw a conclusion that for this case deepfe s compression of data to a 62 dimensional search space is appropriate which makes a trade off between computational cost and quality of solutions compared to deepfepso the cnop obtained by peboa is more consistent with the adjoint model in terms of physical mechanisms including the amplitude of upstream kuroshio transport variability and the structure after superimposing the cnop the anticyclonic cyclonic eddy like structure causes a northward southward meridional velocity anomaly and an anomalous increase decrease in upstream kuroshio transport and the evolution of errors shows two features i e westward propagation and rapid development in the future we intend to apply peboa to other numerical models to validate its generality in solving cnop which includes cnop for double gyre variation in the roms model with 1 0 5 dimensions and cnop for nao in cesm model with 1 0 7 dimensions since the dimensions of these scenarios are smaller than the upstream kuroshio transport in the roms model and our peboa algorithm yields impressive performance in the upstream kuroshio transport problem we believe that the results of these relatively small search space scenarios are also satisfactory besides to further boost the solving efficiency we plan to explore strategies that can accelerate the training of the gaussian process furthermore it is necessary to explore the application of peboa in other problem domains credit authorship contribution statement shijin yuan conceptualization methodology software investigation writing original draft writing review editing yaxuan liu conceptualization methodology software investigation writing original draft writing review editing bo qin conceptualization methodology software writing review editing bin mu conceptualization methodology software writing review editing kun zhang conceptualization methodology software writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study is supported in part by the meteorological joint funds of the national natural science foundation of china under grant u2142211 in part by the key project fund of shanghai 2020 science and technology innovation action plan for social development under grant 20dz1200702 in part by the national key research and development program of china under grant 2020yfa0608000 in part by the national natural science foundation of china under grant 42075141 and in part by the first batch of model interdisciplinary joint research projects of tongji university in 2021 under grant yb 21 202110 appendix joint training process of models in deepfe the pseudocode of training process for deepfe is displayed in algorithm 3 
23763,tropical cyclones tcs produce heavy precipitation and provide excessive freshwater flux fwf to the ocean which alters surface salinity and upper ocean stratification leading to changes in ocean physical processes at present roles played by tc induced fwf fwf tc forcing in the upper ocean responses have received less attention than those by tc induced wind stress τ tc forcing an ocean model is used to investigate the role played by fwf tc forcing in upper ocean responses in the pacific several numerical experiments and observation based analyses are performed focusing on typhoon yutu 2018 the fwf tc and τ tc components are explicitly extracted from atmospheric reanalysis fields and are then taken into account or not in model sensitivity experiments as expected the fwf tc forcing acts to freshen the ocean surface enhance the ocean stratification and reduce the mixing and entrainment of subsurface cold water into the mixed layer ml leading to potentially favoring surface warming this fwf effect is opposite to that induced by τ tc forcing i e a cooling effect on sea surface temperature sst the combined effects of fwf tc and τ tc forcings lead to local ocean responses to typhoon yutu that are characterized by surface cooling and salting which are dominated by the τ tc forcing and are also modulated by the fwf tc forcing depending on their relative magnitudes when fwf tc forcing reaches a certain intensity and the τ tc forcing is weak however the warming effect on sst due to fwf tc forcing can even exceed the cooling effect due to τ tc forcing when fwf tc forcing is relatively stronger than τ tc forcing e g tc induced precipitation 10 37 mm h 1 and τ tc 0 14 n m 2 the warming effect on sst due to fwf tc forcing can even exceed the cooling effect due to τ tc forcing meanwhile with the combination of τ tc forcing the fwf tc forcing can also promote sst cooling induced by the τ tc forcing which is dominated by mixing and entrainment in this case the fwf tc forcing increases surface heat loss from the ocean due to enhanced stratification which contributes to τ tc induced vertical mixing and surface cooling further case studies are needed to thoroughly evaluate fwf tc related effects on the upper ocean keywords tropical cyclones upper ocean responses freshwater flux forcing wind stress forcing ocean modeling observations data availability data will be made available on request 1 introduction tropical cyclones tcs also called typhoons or hurricanes are severe synoptic scale phenomena tcs are of climatic and global importance due to their effects on the ocean in association with violent winds and heavy precipitation e g emanuel 2001 sriver and huber 2010 manucharyan et al 2011 da et al 2021 zhang et al 2022a for example tc induced wind stress τ tc forcing contributes to oceanic poleward heat transport suggesting that tcs may play an important role in driving the meridional circulation and thereby regulating climate e g emanuel 2001 besides tc induced freshwater flux fwf tc tc induced precipitation minus tc induced evaporation forcing alters salinity field which in turn exerts a positive feedback to the intensification of tcs balaguru et al 2016 in addition net heat flux hf perturbation is another major atmospheric forcing to the upper ocean which plays a role in the upper ocean responses for example sensible heat flux gain in the wake of tcs slowly restores sea surface temperature sst perturbations induced by tcs which is conducive to the persistence and propagation of sst thermal perturbation signals leading to potentially exerting a basin wide impact on the ocean zhang et al 2013 pei et al 2015 due to interactions among these forcing fields their net effects on the upper ocean responses are complicated and have not been well understood to further explore the roles played by tcs in the climate system one important step is to understand the characteristics of local ocean responses to tcs and related physical processes as previously described e g price 1981 jacob et al 2000 d asaro et al 2007 dare and mcbride 2011 d asaro and coauthors 2014 pun et al 2018 lu et al 2021 the most pronounced signature of upper ocean responses is surface cooling which is mainly due to τ tc induced mixing and entrainment of subsurface cold water into the mixed layer ml price 1981 in addition τ tc induced perturbations to net surface hf and oceanic advection can also modulate the upper ocean responses including sea surface temperature sst price 1981 d asaro et al 2007 chen et al 2010 zhang and coauthors 2018 in turn the resultant sst cooling acts to execute negative feedback on tcs intensity through its effect on air sea enthalpy flux emanuel 1999 schade and emanuel 1999 lin et al 2015 li et al 2021 previous studies have devoted considerable efforts to describing the ocean responses to tcs and related processes most studies have emphasized the effects of τ tc and hf forcing while the effects of fwf tc forcing have received less attention as one of the three major atmospheric forcings to the ocean fwf forcing can play a role in modulating the state of upper ocean through its effects on sea surface salinity sss and buoyancy flux q b busalacchi and zhang 2009 first the fwf forcing directly affects the sss which further alters the ocean stratification the depth of mixed layer mld and the thickness of barrier layer miller 1976 as a result the fwf forcing changes vertical heat transport and related physical processes maes et al 2002 fedorov et al 2004 maes et al 2005 kang et al 2017 second fwf is one part of q b which directly determines the vertical mixing and entrainment of subsurface water into the ml lupton et al 1985 chen et al 1994 since the fwf forcing regulates the dynamical and thermal processes of the ocean by affecting the ocean stratification fwf tc forcing is expected to play a role in the upper ocean responses previous studies have revealed that freshwater input from tc induced precipitation can modify the upper ocean responses e g jacob and koblinsky 2007 heavy precipitation within tcs provides a large fwf to the sea surface that produces freshening and enhances stratification which limits vertical mixing and suppresses ml cooling balaguru et al 2022 however the effects of fwf forcing on the upper ocean responses are compounded with wind stress forcing which are very complicated and have not been well understood for example the extent to which the fwf forcing plays a role in the upper ocean responses to tcs remains elusive due in part to the lack of separable methods for thoroughly deciphering mechanisms of ocean responses to fwf tc forcing based on observations alone numerical model experiments are powerful approaches to exploring essential physical processes involved it is worth noting that different models exhibit considerable differences in representing the ocean responses to atmospheric forcings e g wind stress hf and fwf since model simulations are sensitive to model dynamical formulations and process parameterizations zhang et al 2020 for example model simulations are sensitively dependent on ways in which fwf forcing is represented kang et al 2017 gao et al 2020 to better simulate the upper ocean responses to tcs the ocean model requires well resolved and parameterized vertical mixing processes in this study we use a layer ocean general circulation model ogcm gent and cane 1989 one main feature of this ogcm includes an embedded bulk ml model whose depth is explicitly taken as a predictable field chen et al 1994 representing ml dynamics better besides high resolution in the vertical direction can be achieved right below the ml by adopting this layer ogcm the ogcm has been extensively used in a variety of modeling studies and could well demonstrate the upper ocean responses to atmospheric forcings e g rothstein et al 1998 zhang 2014 pei et al 2015 zhang and coauthors 2018 pei et al 2019 gao et al 2020 zhang et al 2020 in previous studies using the ogcm pei et al 2015 2019 investigate the upper ocean responses to τ tc forcing the ogcm experiments could well display τ tc induced upper ocean responses and clearly separate the related physical processes this work aims to explore the role played by fwf tc forcing in the upper ocean responses with a case study of typhoon yutu 2018 based on numerical model experiments as well as observational analyses in our study fwf tc and τ tc components are explicitly extracted from atmospheric fields and are then taken into account or not in model sensitivity experiments the advantages of using such a strategy enable these related forcings and effects to be represented individually or collectively in ogcm simulations allowing their modulating effects on the upper ocean to be examined in a clean and clear way zhang 2015 some specific questions are going to be investigated what are the characteristics and structures of upper ocean responses to the combined forcings of τ tc and fwf tc what role does the fwf tc forcing play in such effects to what extent can the stabilizing effect due to fwf tc forcing compete with the mixing effect due to τ tc forcing this paper is organized as follows section 2 describes an ogcm observational datasets a method used to extract τ tc and fwf tc forcing fields and model experimental designs section 3 illustrates the upper ocean responses to the combined forcings of τ tc and fwf tc followed by sensitivity experiments in section 4 to demonstrate the roles of fwf tc forcing in the upper ocean responses the summary and discussion are presented in section 5 2 data model and experimental designs 2 1 observational and reanalysis datasets wind stress and precipitation reanalysis datasets are used to extract the τ tc and fwf tc forcings for model experiments respectively wind stress data are from the fifth generation of ecmwf atmospheric reanalysis era5 hersbach et al 2018 which is available from 1959 to the present precipitation data are from the tropical rainfall measuring mission trmm 2011 which is available from december 1997 to january 2020 the grid of these data has a resolution of 0 25 0 25 in space and a six hour sampling in time typhoon yutu 2018 is represented by the best track data from the u s joint typhoon warning center jtwc chu et al 2002 the dataset contains time series of tc s center locations and their intensity sustained maximum 1 min mean 10 m wind speed at 6 hour intervals over the lifetime of each tc observational temperature and salinity datasets are used to depict the characteristics of upper ocean response to typhoon yutu and validate model results sst data are from the national oceanic and atmospheric administration noaa daily optimum interpolation sst version 2 oisst v2 huang and coauthors 2021 argo temperature and salinity profiles used in this study are provided by china argo real time data center available at www argo org cn 2 2 an ocean general circulation model the ogcm used in this work is a reduced gravity primitive equation and sigma coordinate model originally developed by gent and cane 1989 the vertical structure of ogcm consists of an ml the first layer and several layers below which are specified according to a sigma coordinate one main feature of the ogcm includes an embedded bulk ml model whose depth is explicitly predicted chen et al 1994 representing the ml dynamics better several other related efforts have been devoted to improving this ogcm significantly for example an advective atmospheric mixed layer aml model is incorporated into the ogcm to estimate surface hf murtugudde et al 1996 which accounts for a non local effect on sst induced by the atmospheric boundary layer this hf parameterization allows a realistic representation of feedback between the ocean and the atmosphere seager et al 1995 complete hydrology is also added to the model with fwf being treated as a natural boundary condition murtugudde and busalacchi 1998 additionally the effect of penetrative radiation on upper tropical ocean circulation is taken into account with attenuation depths derived from satellite ocean color data murtugudde et al 2002 these studies have improved simulations of ocean circulation and thermohaline structure significantly a high resolution model is developed by zhang et al 2013 and is used for oceanic modeling the model domain covers the tropical pacific basin 25 s 25 n 120 e 76 w with horizontal resolution of 0 33 in longitude and 0 23 0 72 in latitude and 25 layers in the vertical sponge layers are imposed on temperature and salinity fields near the model s southern and northern boundaries poleward of 20 s n the ogcm is integrated for 20 years the spin up run using prescribed climatological monthly mean atmospheric forcing fields as described by zhang et al 2013 and pei et al 2015 fig 1 displays a modeling strategy used in this work consisting of a basin scale ogcm of the tropical pacific forced by the climatological forcing the τ tc and fwf tc forcings respectively the total wind stress τ can be written as τ τ clim α τ tc τ tc in which τ clim is the seasonally varying climatological part and τ tc is the tc part a scalar parameter α τ tc is introduced to represent the intensity of τ tc forcing similarly the fwf can be decomposed into seasonally varying climatological part fwf clim and tc part α fwf tc fwf tc written as fwf fwf clim α fwf tc fwf tc a scalar parameter α fwf tc is introduced to represent the intensity of fwf tc forcing two pathways exist through which the fwf tc forcing can affect the upper ocean one through the sss and the other through the q b zhang and busalacchi 2009 first the fwf tc acts as a source term affecting the sss as described by the salinity conservation equation in the ogcm variation in the sss modifies ocean density and thereby affects stratification and mixing in the upper ocean second fwf tc is one part of q b which acts as a forcing term in the bulk ml model to determine the mld further affecting the vertical mixing and entrainment of subsurface water into the ml using the same modeling system but considering only the τ tc forcing previous studies pei et al 2015 2019 have shown that the τ tc forcing can induce a significant local response which is characterized by a distinct and persistent sst cooling along the tc track accompanied by an ml deepening and other perturbations 2 3 a method to extract τ tc and fwf tc forcings to explicitly extract the τ tc and fwf tc signals from the atmosphere reanalysis datasets we use the locally weighted quadratic least squares regression loess method o neill et al 2010 in addition a threshold value method and the hf parameterization scheme are adopted to represent the tc induced forcing fields here we define the influence area of a tc as a circle centered on its track with a 6 radius and set the τ tc and fwf tc forcing fields to zero outside of this area we use the loess method to extract the τ tc forcing as in zhang et al 2013 and pei et al 2015 for example the smoothed value a of a field a at a grid point is estimated by fitting a regression surface to some subset data locally then a perturbation field is calculated as a a a as shown in o neill et al 2010 half span parameters denoted as α x and α y in x and y directions in the loess method indicate how much subset data are used to fit each quadratic regression locally the half span parameters determine the resulting perturbation fields the larger the values of α x and α y the stronger the perturbation fields a a threshold value method and the hf parameterization are adopted to extract the fwf tc forcing which is defined as tc induced precipitation minus tc induced evaporation the tc induced precipitation rate threshold is fixed at 0 5 mm h 1 as done in lin et al 2015 and the tc induced evaporation is calculated from the hf parameterization in the ogcm seager et al 1995 2 4 model experimental design fig 2 shows the track and intensity of typhoon yutu as indicated during the period october 20 november 3 2018 initially a tropical disturbance on october 20 yutu persists until october 22 before developing into a tropical storm it is upgraded into the typhoon category on october 23 and becomes a category 5 super typhoon on the saffir simpson hurricane wind scale on october 24 with peak winds of 75 m s 1 yutu then passes over the philippine islands on october 29 before finally entering the south china sea on october 30 during its passage yutu induces heavy precipitation e g fig 4a 4c with its peak rate reaching 10 mm h 1 unlike the evolution of wind speeds tc induced precipitation does not change significantly during yutu s passage the precipitation has already reached a high rate in the first few days when winds are weak fig 3 displays the total wind stress field from the era5 on october 24 2018 and the τ tc forcing field extracted using the loess filter the loess filter effectively captures the structure of τ tc forcing which is sensitive to the half span parameters in this study we opt for α x 10 and α y 10 to represent the τ tc forcing field reasonably well fig 3b then we adjust the coefficient α τ tc 1 3 to better rescale the τ tc forcing amplitude to observed intensity fig 3c therefore we use τ τ clim 1 3 τ tc as a control run in addition we can adjust the intensity of τ tc forcing by altering α τ tc value in sensitivity experiments the fwf tc forcing is defined as tc induced precipitation minus tc induced evaporation where tc induced precipitation and evaporation are obtained using a threshold value method and the hf parameterization respectively then we add the fwf tc forcing field to the climatological fwf forcing field i e fwf fwf clim fwf tc α tc fwf 1 0 in the control run fig 4 shows the reanalysis total precipitation fields on october 22 24 2018 and the corresponding fwf tc forcing as expected the fwf forcing fields look much like the reanalysis precipitation fields themselves with only a slight offset due to evaporation contribution unlike the change to τ tc forcing intensity the fwf tc forcing intensity does not change significantly during the evolution of typhoon yutu peaking at 10 mm h 1 in addition we can adjust the intensity of fwf tc forcing by altering α tc fwf value in sensitivity experiments these atmospheric forcing fields are specified for ogcm experiments table 1 shows the designed ogcm experiments performed in this study a climatological reference run does not include tc induced forcings the t c τ run and the t c fwf run indicate that only τ tc or fwf tc forcings are included respectively the control run explicitly incorporates both the τ tc or fwf tc forcings i e α τ tc 1 3 and α x α y 10 α fwf tc 1 0 additional sensitivity experiments e g t c τ 1 3 p 1 5 run are performed with the same τ tc forcing but with different intensities of the fwf tc forcing which can be changed by adjusting α fwf i e α τ tc 1 3 and α x α y 10 1 5 α fwf tc 4 7 the differences among those experiments are used to examine the effects of corresponding atmospheric forcing on the upper ocean with different intensity representations 3 upper ocean responses to typhoon yutu two model experiments are conducted using the ogcm fig 1 to illustrate the upper ocean responses to the combined forcings of τ tc and fwf tc one experiment is called the climatological reference run in which the τ tc and fwf tc parts are not included i e α τ tc 0 0 α fwf tc 0 0 the other is called the control run in which the α τ tc τ tc part i e α τ tc 1 3 and α x α y 10 and α fwf tc fwf tc part i e α fwf tc 1 0 are explicitly incorporated with other model settings being kept the same as the climatological reference run the ogcm is initialized from october 1 2018 and integrated for two months for both model experiments whose differences are calculated as the effects induced by the combined forcings of the prescribed τ tc and fwf tc in addition the observational temperature and salinity datasets from argo are used to depict the characteristics of upper ocean response to typhoon yutu and validate the model results detailed analyses are given in this section 3 1 characteristics of the upper ocean responses the upper ocean responses to tc are characterized by a cold wake price 1981 fig 5a 5c display the observed daily sst responses to typhoon yutu on october 22 24 2018 the observed sst response is calculated as a difference between the daily sst field considered and the pre tc condition on october 17 19 2018 typhoon yutu induces sst cooling along its track with a magnitude as large as 1 06 c on october 22 24 2018 the observed sst response is calculated using a grid based maximum response gmr method with three artificial parameters the 400 km distance the five day response duration and the two day pre typhoon average as done in li et al 2020 typhoon yutu induces sst cooling response along its track with a magnitude as large as 2 33 c in the region where yutu passed by on october 22 24 2018 during yutu s passage the sst perturbations exhibit rapid emergence and persist for a relatively long duration leaving a cold wake after yutu s passage the sst response simulated from the ogcm e g fig 5d 5f is compared with that observed from the satellites for simulations the sst response is calculated as the difference between the control run and the climatological reference run i e ctrl ref consistent with the observed sst response e g fig 5a 5c a cold wake emerges after yutu s passage with a magnitude as large as 1 55 c on october 22 24 2018 it shows that spatial patterns and amplitude of the simulated sst response are similar to those observed by satellites however the simulated sst response exhibits a smoother evolution than observed partly due to pure atmospheric forcing specifications for the ogcm which are extracted from the reanalysis product however the simulated sst cooling exhibits a weaker response than observed partly due to the gmr method defining the sst response as the maximum response during a five day response period li et al 2020 while the simulated sst response is the response of the day the sss change induced by typhoon yutu exhibits different features from the sst change the simulated sss increases during the passage of yutu except for the first few days when τ tc forcing is weak and fwf tc forcing is strong e g fig 6d 6f and the maximum increase and decrease of the sss are 0 16 psu and 0 31 psu on october 22 24 2018 respectively with satellite datasets sun et al 2021 also found the sss response to tcs is initially dominated by tc induced precipitation forcing and subsequently controlled by wind driven vertical mixing and upwelling with a non compensated effect due to the decreased sst and the increased sss the sea surface density ssd increases by up to 0 59 kg m 3 on october 22 24 2018 e g fig 6g 6i during the first few days the decreased temperature and salinity tend to compensate for the density with a magnitude as large as 0 35 kg m 3 on october 22 2018 in addition tc induced sst sss and ssd perturbations can persist in the upper ocean for several days to several weeks not shown suggesting that the local ocean responses may produce significant remote influences the ml deepening is also a commonly observed feature of ocean responses to tcs pan and sun 2013 the mld in the bulk ml model is related to the ratio of wind generation of turbulent kinetic energy to the net positive q b gao et al 2020 fig 7 illustrates the simulated daily mld response to yutu and the related q b on october 24 2018 from fig 7a 7b we can see that the ml deepens by up to 53 m on october 24 when most of the positive q b at the sea surface enhances the stratification and shoals the ml conversely wind driven vertical mixing and upwelling are known to deepen the ml therefore the ml deepening is dominated by the τ tc forcing and compensated by the q b effect away from the center of yutu a shoaling of the ml is dominated by the q b effect furthermore the q b at the sea surface can be expressed as the sum of hf q t and fwf q s busalacchi and zhang 2009 as shown in fig 7c 7d the negative q t and positive q s at the sea surface tend to compensate for the q b during the passage of yutu the q b perturbations are dominated by the q s effect except for the left side of the yutu influence region in addition while the sst sss and ssd perturbations persist for lengthy durations the mld and q b perturbations exhibit a relatively rapid recovery within a few days not shown the ocean responses of the ml to tcs and related processes can further affect thermohaline conditions at the subsurface depth fig 8a illustrates an example of the observed perturbation of thermal structure in response to yutu as measured by two pairs of argo profiles a two layer thermal structure is seen in the vertical direction with a cooling in the surface accompanied by a warming in the thermocline the formation of the cooling warming layers can be partly explained by the τ tc induced vertical mixing which is the essence of the heat pump theory emanuel 2001 for comparison fig 8b displays the simulated vertical temperature perturbation at the same location the observed two layer thermal response is well reproduced in the model besides the salinity structure is more complicated at the same location fig 8c 8d which exhibits a three layer structure in the vertical direction with an increase in surface salinity and a decrease increase in the halocline for the additional two layers the different responses of thermal structure and salinity structure might be due to mixing and vertical advection sun et al 2012 since the argo floats at some distance away from the tc track a compensating downwelling is expected price 1981 zhang and coauthors 2018 zhang 2023 vertical mixing and entrainment of subsurface water contribute to a decrease increase increase decrease in temperature salinity at upper 150 m while the difference below 150 m might be explained by the fact that the contribution of downwelling to the salinity anomaly is somewhat different from that to the temperature lin et al 2017 the perturbations for temperature and salinity persist for days to weeks further affecting the thermohaline conditions and related physical dynamics in the ocean 3 2 ml heat budget analyses to further explore the tc induced processes responsible for sst perturbations we perform heat budget analyses of the ml on october 24 2018 fig 9 1 t t u t x v t y w t z κ h h m h h m h t 2 κ v h m h 2 t e t q net ρ c p h m where t t is the temperature tendency u t x v t y is the horizontal advection w t z is the vertical advection κ h h m h h m h t is the horizontal diffusion 2 κ v h m h 2 t e t is the vertical mixing and entrainment of subsurface water into the ml and q net ρ c p h m is the net hf during the passage of typhoon yutu the enhanced vertical mixing and entrainment act to dominate the surface cooling fig 9b besides further away from the yutu s center the mixing is suppressed which contributes to warming in the ml upwelling and horizontal advection also contribute significantly to sst cooling fig 9c 9d the net hf which mostly attributes to latent hf contributes little to the sst cooling fig 9a after the passage of yutu however the net hf begins to dominate the sst warming which is mainly due to the absorption of downward sensible hf by the ocean the sensible hf gained in the wake of typhoon yutu slowly restores the sst perturbations induced by tc which is conducive to the persistence of thermal perturbation signals potentially exerting a basin wide impact on the ocean therefore τ tc forcing increases vertical mixing and a net surface heat loss from the ocean with the reduced stratification which enhances the entrainment of subsurface cold water into the ml and deepens the ml thus inducing a cold wake along the tc track in contrast fwf tc forcing decreases the surface salinity enhances the stratification and suppresses the mixing possibly leading to a reduction in the sst cooling induced by τ tc forcing however the specific ocean processes induced by fwf tc forcing and the extent to which it contributes to the upper ocean responses need to be understood using sensitivity experiments 3 3 ml salinity budget analyses to explore the tc induced processes responsible for sss perturbations we perform salinity analyses of the ml on october 24 2018 fig 10 2 s t u s x v s y w s z κ h h m h h m h s 2 κ v h m h 2 s e s p e s h m where s t is the salinity tendency u s x v s y is the horizontal advection w s z is the vertical advection κ h h m h h m h s is the horizontal diffusion 2 κ v h m h 2 s e s is the vertical mixing and entrainment of subsurface water into the ml and p e s h m is the fwf related term during the passage of typhoon yutu the τ tc induced vertical mixing and entrainment enhance fig 10b acting to dominate the sss increase on october 24 2018 besides the horizontal and vertical advection contribute little to sss increasing in the ml fig 10c d on the contrary freshwater input from tc induced precipitation contributes significantly to sss decreasing fig 10a in particular when fwf tc forcing reaches a certain intensity and the τ tc forcing is weak the sss decreases which is dominated by the fwf tc forcing fig 6d 4 the role played by fwf tc forcing in the upper ocean responses table 1 shows the designed experiments in this section differences among those experiments are used to reveal the effects of corresponding atmospheric forcing on the upper ocean with different τ tc and fwf tc representations the difference between the control run and the climatological reference run i e ctrl ref represents the total responses to the combined forcings of τ tc and fwf tc ctrl t c fwf represents the τ tc forcing only ctrl t c τ represents the fwf tc forcing only t c τ ref represents the responses to τ tc forcing t c fwf ref represents the responses to fwf tc forcing additional sensitivity experiments different from the climatological reference run e g t c τ 1 3 p 1 5 ref indicate the responses to combined forcings of the same τ tc but with different fwf tc intensities 4 1 differences in response characteristics the fwf tc forcing can modulate the upper ocean responses e g jacob and koblinsky 2007 balaguru et al 2022 therefore in this section we focus on different response characteristics from sensitivity experiments on october 24 2018 as typhoon yutu becomes a category 5 super typhoon on the saffir simpson hurricane wind scale in the case with no τ tc forcing the fwf tc forcing alone increases the sst fig 11b decreases the sss fig 11e and collectively decreases the ssd fig 11h thus temperature and salinity tend to have non compensated effects on density while in the case with the included τ tc forcing the effect of fwf tc forcing on the upper ocean becomes more complicated first the fwf tc forcing reduces the sst cooling or even warms the sst fig 11c meanwhile the sss decreases fig 11f but by a smaller magnitude than in the case with no τ tc forcing fig 11e hence the non compensated effects due to sst and sss decrease the ssd fig 11i second the fwf tc forcing can also increase the sst cooling fig 11c induced by τ tc forcing thus the decreased sst and sss fig 11f tend to compensate for each other leading to the increased ssd fig 11i in this case sst rather than sss dominates the ssd variation miller 1976 found that when precipitation occurs in combination with strong wind forcing the ml is perturbed to be deeper than a case with lower wind speeds suggesting that the ml deepens as wind stress forcing increases fig 12 shows the simulated mld difference and the associated q b difference on october 24 2018 the shoaling of ml in the case with the included τ tc forcing fig 12c is smaller than that in the case with no τ tc forcing fig 12b indicating that not only the τ tc forcing but also the fwf tc forcing contributes to the ml deepening when fwf tc forcing is combined with τ tc forcing in particular different from mld changes the positive q b perturbations fig 12e 12f dominated by the q s effect fig 12k 12l are similar in the two cases which is probably because the nonlinear modulation of mld by the fwf tc forcing is affected by the wind generation of turbulent kinetic energy 4 2 differences in ml heat budget heat budget difference analyses ctrl t c τ of the ml on october 24 2018 fig 13 are conducted to investigate the fwf tc induced specific ocean processes responsible for sst perturbations the net hf contributes mainly to the increase in sst cooling fig 13a on the contrary the vertical mixing fig 13b and upwelling fig 13d dominate the reduction in sst cooling in addition the horizontal advection fig 13c contributes significantly to the warming as well as cooling in the ml consequently fwf tc forcing exhibits multiple signals in the upper ocean responses through different processes fig 17 during the passage of typhoon yutu a positive fwf perturbation at the sea surface freshens the upper ocean and enhances the ocean stratification on the one hand the enhanced stratification stabilizes the upper ocean and thus reduces the mixing and entrainment of subsurface cold water into the ml leading to the reduced cooling or even a warming of sea surface on the other hand the enhanced stratification suppresses downward heat transport which in turn increases surface heat loss from the ocean and increases vertical mixing and thus the sst cooling induced by τ tc forcing additionally it is indicated that the multiple effects on the upper ocean due to fwf tc forcing depend on the intensity of τ tc forcing for example the fwf tc forcing induces the sst warming in the absence of τ tc forcing while the fwf tc forcing can even promote the sst cooling in the presence of τ tc forcing however it remains to see whether fwf tc and τ tc forcings are compensated or non compensated for by each other in their effects on the sst which are clearly sensitive to the specifications of their intensities 4 3 differences in ml salinity budget salinity budget difference analyses ctrl t c τ in the ml on october 24 2018 fig 14 are conducted to investigate the fwf tc induced effects on salinity perturbations as revealed the fwf tc forcing acts to freshen the surface and dominate the sss decreasing fig 14a besides fwf tc induced vertical mixing and entrainment of subsurface water into the ml mainly contributes to sss increasing fig 14b which is due to the transport of salty water in the ml through horizontal advection the fwf tc induced horizontal and vertical advection contributes little to sss perturbations fig 14c d 4 4 quantitative analyses to further examine the effects of fwf tc forcing on the upper ocean a site 155 8 e 9 25 n by which yutu s center passes on october 22 2018 is selected for analyses from sensitivity experiments fig 15 shows the time series of sst differences δ sst which are calculated relative to the reference run except for the t c fwf run different sensitivity experiments are performed with varying intensity of fwf forcing i e the α tc fwf value is adjusted while keeping the wind stress forcing constant i e 0 14 n m 2 as expected the δ sst increases with the increased fwf forcing intensity to investigate the possible existence of a threshold value at which fwf tc induced warming effect can exceed the τ tc induced cooling effect we focus on the δ sst on october 22 2018 compared with the t c τ run the fwf forcing increases the sst cooling when α tc fwf 2 3 i e precipitation 6 99 mm h 1 the fwf forcing reduces the sst cooling when 2 3 α tc fwf 3 4 i e 6 99 mm h 1 precipitation 10 37 mm h 1 the fwf forcing instead increases the sst warming when α tc fwf 3 4 i e precipitation 10 37 mm h 1 the quantitative results are supplemented in fig 17 in particular when taking α tc fwf 3 4 i e precipitation 10 37 mm h 1 there is no apparent change in sst at the selected site on october 22 2018 indicating that the τ tc related cooling effect on sst is offset by the fwf tc related warming effect in addition we investigate the main fields of interest on october 23 2018 fig 16 to understand the characteristics of upper ocean responses in this case t c τ 1 3 p 3 4 run near the center of typhoon yutu both the sst and sss decrease which act to have a compensated effect on ssd the ssd increases which is dominated by the sst effect the mld changes little due to the compensated effect of τ tc and fwf tc forcings in general the upper ocean responses are weaker than those in the control run fig 6 during the first few days however the increased sst and the considerably decreased sss promote the decrease of ssd and the ml shoals due to the specifications of weak τ tc and heavy precipitation forcings generally the upper ocean responses during the first few days are dominated by the fwf tc forcing and are stronger than those in the control run in addition the extent to which the fwf tc forcing contributes to sst responses is quantitatively illustrated in table 2 the fwf tc forcing dominates the sst response when wind speed is smaller than 7 m s 1 and the contribution of fwf tc forcing to the sst response is about 20 40 when wind speed is 7 18 m s 1 when wind speed is greater than 18 m s 1 the contribution of fwf tc forcing to the sst response is about 1 8 or even less according to these analyses of model experiments and observations it is concluded that the upper ocean responses to tcs are dominated by τ tc forcing but can also be modulated by fwf tc and hf forcings besides the nonlinear modulation of ocean responses can happen when τ tc related effects on the upper ocean are compensated for by fwf tc related effects furthermore there are several factors responsible for the difference in the upper ocean response by compositing argo profiles and other remote sensing data wang et al 2016 also show that higher intensity tcs induce stronger cooling than lower intensity besides the sst response is also sensitive to tc translation speed pre tc mld and types of oceanic eddies cyclonic anticyclonic eddies wang et al 2016 lin et al 2017 liu et al 2017 5 conclusion and discussion excessive fwf tc inputs into the ocean play an important role in upper ocean responses but have received less attention than τ tc and hf as expected the fwf tc forcing acts to freshen the ocean surface and enhance the stratification which reduces the vertical mixing and entrainment of cold water leading to surface warming this fwf effect is opposite to that induced by τ tc forcing which induces a cooling effect on surface temperature this work investigates the upper ocean responses to the combined forcings of fwf tc and τ tc through ocean model experiments and observational analyses taking typhoonyutu 2018 as an example the upper ocean responses are characterized by surface cooling surface salting and ml deepening which are dominated by the τ tc forcing and are also modulated by the fwf tc forcing in addition the ocean responses are not limited to the surface but extend to subsurface layer taking the region where the argo buoys are located the simulated ocean responses exhibit a two layer thermal structure and a three layer salinity structure in the vertical both being consistent with the argo profiles analyses of the ml heat and salt budgets are performed to illustrate the related processes responsible for ocean responses during the passage of typhoon yutu the cold wake along the tc track is dominated by vertical mixing and modulated by net hf horizontal advection and vertical advection specifically the τ tc forcing increases mixing and surface heat loss from the ocean both of which act to weaken stratification thus the entrainment of subsurface cold water into the ml enhances and the ml deepens leading to sst cooling on the contrary the fwf tc forcing freshens the ocean surface and enhances the stratification which reduces the vertical mixing and entrainment of cold water from the subsurface leading to reduced cooling or even a warming of sea surface meanwhile the enhanced stratification due to fwf tc forcing can also suppress downward heat transport but increase surface heat loss from the ocean which contributes to vertical mixing and thus surface cooling induced by τ tc forcing by conducting a series of sensitivity experiments it is revealed that fwf tc forcing in combination with τ tc forcing can exhibit multiple signals in the upper ocean responses through different processes on the one hand fwf tc forcing decreases the sss and warms the sst which is counteracted by τ tc forcing in this case the combined effects of sst and sss responses lead to a decrease in ssd indicating non compensated nature by each other on the other hand fwf tc forcing can increase the sst cooling and decrease the sss in this case the increased ssd is dominated by the sst effect and is compensated by the sss effect in particular the extent to which fwf tc forcing contributes to the sst response is quantitatively investigated by modeling experiments and is compared with τ tc forcing when τ tc forcing i e τ tc 0 14 n m 2 is combined with a weak fwf tc forcing i e precipitation 6 99 mm h 1 the fwf tc forcing acts to increase the τ tc related cooling effect on sst as the fwf tc forcing enhances while the τ tc forcing remains constant i e τ tc 0 14 n m 2 6 99 mm h 1 precipitation 10 37 mm h 1 the fwf tc forcing acts to reduce the τ tc related cooling effect on sst although the τ tc forcing dominates the sst response the fwf tc forcing can significantly affect the sst pattern when τ tc forcing effect is weak it is worth noting that when weak τ tc forcing i e τ tc 0 14 n m 2 is combined with a strong fwf tc forcing i e precipitation 10 37 mm h 1 the τ tc related cooling effect on sst is offset by the fwf tc related warming effect furthermore the sst response exerts feedbacks on the intensity of tcs through its effect on the air sea enthalpy flux suggesting that the fwf tc may play an important role in determining the intensity and track of tcs although both τ tc forcing and fwf tc forcing could affect the upper ocean thermohaline structure they might be different in two ways the region and the time interval the radius of inner spiral rainbands is about 2 3 times the radius of maximum wind wang 2008a which means tcs exert the effects on the ocean through fwf tc forcing over a wider region than τ tc forcing and the fwf tc forcing induced by tc precipitation appears intermittent in time while τ tc forcing appears continuous as a result the oceanic response induced by fwf tc forcing might be intermittent over a much wider horizontal region wang et al 2018 besides as previously described the upper ocean responses induced by τ tc forcing are characterized by rightward bias which occur because in the ocean frame the τ tc vector turns clockwise in the time on the right side of tc track and anticlockwise on the left side of the track price 1981 however strong fwf tc forcing due to heavy rainfall dilutes the sss stabilizes the upper ocean and alters the intensity of vertical mixing and entrainment of the subsurface water the fwf tc induced response characteristics do not resemble the rightward bias induced by τ tc forcing the results of this study are considered to be limited in sense that ocean only modeling is taken in which clean individual forcings of τ tc and fwf tc are extracted and specified for sensitivity experiments the upper ocean responses to tcs are more complicated and variable in realistic scenarios for example realistic complicated background conditions of the ocean could also regulate the way in which the upper ocean responds to tcs further case studies are needed to thoroughly evaluate the fwf tc related effects on the upper ocean locally in the western pacific and remotely in the eastern equatorial pacific an important region for el niño southern oscillation zhang et al 2013 2022b credit authorship contribution statement shuoni ye ocean modeling visualization formal analysis investigation validation writing original draft rong hua zhang mentoring methodology writing editing funding acquisition hongna wang writing editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research is supported by the national natural science foundation of china nsfc grant no 42030410 the laoshan laboratory no lskj202202404 and the startup foundation for introducing talent of nuist 
23763,tropical cyclones tcs produce heavy precipitation and provide excessive freshwater flux fwf to the ocean which alters surface salinity and upper ocean stratification leading to changes in ocean physical processes at present roles played by tc induced fwf fwf tc forcing in the upper ocean responses have received less attention than those by tc induced wind stress τ tc forcing an ocean model is used to investigate the role played by fwf tc forcing in upper ocean responses in the pacific several numerical experiments and observation based analyses are performed focusing on typhoon yutu 2018 the fwf tc and τ tc components are explicitly extracted from atmospheric reanalysis fields and are then taken into account or not in model sensitivity experiments as expected the fwf tc forcing acts to freshen the ocean surface enhance the ocean stratification and reduce the mixing and entrainment of subsurface cold water into the mixed layer ml leading to potentially favoring surface warming this fwf effect is opposite to that induced by τ tc forcing i e a cooling effect on sea surface temperature sst the combined effects of fwf tc and τ tc forcings lead to local ocean responses to typhoon yutu that are characterized by surface cooling and salting which are dominated by the τ tc forcing and are also modulated by the fwf tc forcing depending on their relative magnitudes when fwf tc forcing reaches a certain intensity and the τ tc forcing is weak however the warming effect on sst due to fwf tc forcing can even exceed the cooling effect due to τ tc forcing when fwf tc forcing is relatively stronger than τ tc forcing e g tc induced precipitation 10 37 mm h 1 and τ tc 0 14 n m 2 the warming effect on sst due to fwf tc forcing can even exceed the cooling effect due to τ tc forcing meanwhile with the combination of τ tc forcing the fwf tc forcing can also promote sst cooling induced by the τ tc forcing which is dominated by mixing and entrainment in this case the fwf tc forcing increases surface heat loss from the ocean due to enhanced stratification which contributes to τ tc induced vertical mixing and surface cooling further case studies are needed to thoroughly evaluate fwf tc related effects on the upper ocean keywords tropical cyclones upper ocean responses freshwater flux forcing wind stress forcing ocean modeling observations data availability data will be made available on request 1 introduction tropical cyclones tcs also called typhoons or hurricanes are severe synoptic scale phenomena tcs are of climatic and global importance due to their effects on the ocean in association with violent winds and heavy precipitation e g emanuel 2001 sriver and huber 2010 manucharyan et al 2011 da et al 2021 zhang et al 2022a for example tc induced wind stress τ tc forcing contributes to oceanic poleward heat transport suggesting that tcs may play an important role in driving the meridional circulation and thereby regulating climate e g emanuel 2001 besides tc induced freshwater flux fwf tc tc induced precipitation minus tc induced evaporation forcing alters salinity field which in turn exerts a positive feedback to the intensification of tcs balaguru et al 2016 in addition net heat flux hf perturbation is another major atmospheric forcing to the upper ocean which plays a role in the upper ocean responses for example sensible heat flux gain in the wake of tcs slowly restores sea surface temperature sst perturbations induced by tcs which is conducive to the persistence and propagation of sst thermal perturbation signals leading to potentially exerting a basin wide impact on the ocean zhang et al 2013 pei et al 2015 due to interactions among these forcing fields their net effects on the upper ocean responses are complicated and have not been well understood to further explore the roles played by tcs in the climate system one important step is to understand the characteristics of local ocean responses to tcs and related physical processes as previously described e g price 1981 jacob et al 2000 d asaro et al 2007 dare and mcbride 2011 d asaro and coauthors 2014 pun et al 2018 lu et al 2021 the most pronounced signature of upper ocean responses is surface cooling which is mainly due to τ tc induced mixing and entrainment of subsurface cold water into the mixed layer ml price 1981 in addition τ tc induced perturbations to net surface hf and oceanic advection can also modulate the upper ocean responses including sea surface temperature sst price 1981 d asaro et al 2007 chen et al 2010 zhang and coauthors 2018 in turn the resultant sst cooling acts to execute negative feedback on tcs intensity through its effect on air sea enthalpy flux emanuel 1999 schade and emanuel 1999 lin et al 2015 li et al 2021 previous studies have devoted considerable efforts to describing the ocean responses to tcs and related processes most studies have emphasized the effects of τ tc and hf forcing while the effects of fwf tc forcing have received less attention as one of the three major atmospheric forcings to the ocean fwf forcing can play a role in modulating the state of upper ocean through its effects on sea surface salinity sss and buoyancy flux q b busalacchi and zhang 2009 first the fwf forcing directly affects the sss which further alters the ocean stratification the depth of mixed layer mld and the thickness of barrier layer miller 1976 as a result the fwf forcing changes vertical heat transport and related physical processes maes et al 2002 fedorov et al 2004 maes et al 2005 kang et al 2017 second fwf is one part of q b which directly determines the vertical mixing and entrainment of subsurface water into the ml lupton et al 1985 chen et al 1994 since the fwf forcing regulates the dynamical and thermal processes of the ocean by affecting the ocean stratification fwf tc forcing is expected to play a role in the upper ocean responses previous studies have revealed that freshwater input from tc induced precipitation can modify the upper ocean responses e g jacob and koblinsky 2007 heavy precipitation within tcs provides a large fwf to the sea surface that produces freshening and enhances stratification which limits vertical mixing and suppresses ml cooling balaguru et al 2022 however the effects of fwf forcing on the upper ocean responses are compounded with wind stress forcing which are very complicated and have not been well understood for example the extent to which the fwf forcing plays a role in the upper ocean responses to tcs remains elusive due in part to the lack of separable methods for thoroughly deciphering mechanisms of ocean responses to fwf tc forcing based on observations alone numerical model experiments are powerful approaches to exploring essential physical processes involved it is worth noting that different models exhibit considerable differences in representing the ocean responses to atmospheric forcings e g wind stress hf and fwf since model simulations are sensitive to model dynamical formulations and process parameterizations zhang et al 2020 for example model simulations are sensitively dependent on ways in which fwf forcing is represented kang et al 2017 gao et al 2020 to better simulate the upper ocean responses to tcs the ocean model requires well resolved and parameterized vertical mixing processes in this study we use a layer ocean general circulation model ogcm gent and cane 1989 one main feature of this ogcm includes an embedded bulk ml model whose depth is explicitly taken as a predictable field chen et al 1994 representing ml dynamics better besides high resolution in the vertical direction can be achieved right below the ml by adopting this layer ogcm the ogcm has been extensively used in a variety of modeling studies and could well demonstrate the upper ocean responses to atmospheric forcings e g rothstein et al 1998 zhang 2014 pei et al 2015 zhang and coauthors 2018 pei et al 2019 gao et al 2020 zhang et al 2020 in previous studies using the ogcm pei et al 2015 2019 investigate the upper ocean responses to τ tc forcing the ogcm experiments could well display τ tc induced upper ocean responses and clearly separate the related physical processes this work aims to explore the role played by fwf tc forcing in the upper ocean responses with a case study of typhoon yutu 2018 based on numerical model experiments as well as observational analyses in our study fwf tc and τ tc components are explicitly extracted from atmospheric fields and are then taken into account or not in model sensitivity experiments the advantages of using such a strategy enable these related forcings and effects to be represented individually or collectively in ogcm simulations allowing their modulating effects on the upper ocean to be examined in a clean and clear way zhang 2015 some specific questions are going to be investigated what are the characteristics and structures of upper ocean responses to the combined forcings of τ tc and fwf tc what role does the fwf tc forcing play in such effects to what extent can the stabilizing effect due to fwf tc forcing compete with the mixing effect due to τ tc forcing this paper is organized as follows section 2 describes an ogcm observational datasets a method used to extract τ tc and fwf tc forcing fields and model experimental designs section 3 illustrates the upper ocean responses to the combined forcings of τ tc and fwf tc followed by sensitivity experiments in section 4 to demonstrate the roles of fwf tc forcing in the upper ocean responses the summary and discussion are presented in section 5 2 data model and experimental designs 2 1 observational and reanalysis datasets wind stress and precipitation reanalysis datasets are used to extract the τ tc and fwf tc forcings for model experiments respectively wind stress data are from the fifth generation of ecmwf atmospheric reanalysis era5 hersbach et al 2018 which is available from 1959 to the present precipitation data are from the tropical rainfall measuring mission trmm 2011 which is available from december 1997 to january 2020 the grid of these data has a resolution of 0 25 0 25 in space and a six hour sampling in time typhoon yutu 2018 is represented by the best track data from the u s joint typhoon warning center jtwc chu et al 2002 the dataset contains time series of tc s center locations and their intensity sustained maximum 1 min mean 10 m wind speed at 6 hour intervals over the lifetime of each tc observational temperature and salinity datasets are used to depict the characteristics of upper ocean response to typhoon yutu and validate model results sst data are from the national oceanic and atmospheric administration noaa daily optimum interpolation sst version 2 oisst v2 huang and coauthors 2021 argo temperature and salinity profiles used in this study are provided by china argo real time data center available at www argo org cn 2 2 an ocean general circulation model the ogcm used in this work is a reduced gravity primitive equation and sigma coordinate model originally developed by gent and cane 1989 the vertical structure of ogcm consists of an ml the first layer and several layers below which are specified according to a sigma coordinate one main feature of the ogcm includes an embedded bulk ml model whose depth is explicitly predicted chen et al 1994 representing the ml dynamics better several other related efforts have been devoted to improving this ogcm significantly for example an advective atmospheric mixed layer aml model is incorporated into the ogcm to estimate surface hf murtugudde et al 1996 which accounts for a non local effect on sst induced by the atmospheric boundary layer this hf parameterization allows a realistic representation of feedback between the ocean and the atmosphere seager et al 1995 complete hydrology is also added to the model with fwf being treated as a natural boundary condition murtugudde and busalacchi 1998 additionally the effect of penetrative radiation on upper tropical ocean circulation is taken into account with attenuation depths derived from satellite ocean color data murtugudde et al 2002 these studies have improved simulations of ocean circulation and thermohaline structure significantly a high resolution model is developed by zhang et al 2013 and is used for oceanic modeling the model domain covers the tropical pacific basin 25 s 25 n 120 e 76 w with horizontal resolution of 0 33 in longitude and 0 23 0 72 in latitude and 25 layers in the vertical sponge layers are imposed on temperature and salinity fields near the model s southern and northern boundaries poleward of 20 s n the ogcm is integrated for 20 years the spin up run using prescribed climatological monthly mean atmospheric forcing fields as described by zhang et al 2013 and pei et al 2015 fig 1 displays a modeling strategy used in this work consisting of a basin scale ogcm of the tropical pacific forced by the climatological forcing the τ tc and fwf tc forcings respectively the total wind stress τ can be written as τ τ clim α τ tc τ tc in which τ clim is the seasonally varying climatological part and τ tc is the tc part a scalar parameter α τ tc is introduced to represent the intensity of τ tc forcing similarly the fwf can be decomposed into seasonally varying climatological part fwf clim and tc part α fwf tc fwf tc written as fwf fwf clim α fwf tc fwf tc a scalar parameter α fwf tc is introduced to represent the intensity of fwf tc forcing two pathways exist through which the fwf tc forcing can affect the upper ocean one through the sss and the other through the q b zhang and busalacchi 2009 first the fwf tc acts as a source term affecting the sss as described by the salinity conservation equation in the ogcm variation in the sss modifies ocean density and thereby affects stratification and mixing in the upper ocean second fwf tc is one part of q b which acts as a forcing term in the bulk ml model to determine the mld further affecting the vertical mixing and entrainment of subsurface water into the ml using the same modeling system but considering only the τ tc forcing previous studies pei et al 2015 2019 have shown that the τ tc forcing can induce a significant local response which is characterized by a distinct and persistent sst cooling along the tc track accompanied by an ml deepening and other perturbations 2 3 a method to extract τ tc and fwf tc forcings to explicitly extract the τ tc and fwf tc signals from the atmosphere reanalysis datasets we use the locally weighted quadratic least squares regression loess method o neill et al 2010 in addition a threshold value method and the hf parameterization scheme are adopted to represent the tc induced forcing fields here we define the influence area of a tc as a circle centered on its track with a 6 radius and set the τ tc and fwf tc forcing fields to zero outside of this area we use the loess method to extract the τ tc forcing as in zhang et al 2013 and pei et al 2015 for example the smoothed value a of a field a at a grid point is estimated by fitting a regression surface to some subset data locally then a perturbation field is calculated as a a a as shown in o neill et al 2010 half span parameters denoted as α x and α y in x and y directions in the loess method indicate how much subset data are used to fit each quadratic regression locally the half span parameters determine the resulting perturbation fields the larger the values of α x and α y the stronger the perturbation fields a a threshold value method and the hf parameterization are adopted to extract the fwf tc forcing which is defined as tc induced precipitation minus tc induced evaporation the tc induced precipitation rate threshold is fixed at 0 5 mm h 1 as done in lin et al 2015 and the tc induced evaporation is calculated from the hf parameterization in the ogcm seager et al 1995 2 4 model experimental design fig 2 shows the track and intensity of typhoon yutu as indicated during the period october 20 november 3 2018 initially a tropical disturbance on october 20 yutu persists until october 22 before developing into a tropical storm it is upgraded into the typhoon category on october 23 and becomes a category 5 super typhoon on the saffir simpson hurricane wind scale on october 24 with peak winds of 75 m s 1 yutu then passes over the philippine islands on october 29 before finally entering the south china sea on october 30 during its passage yutu induces heavy precipitation e g fig 4a 4c with its peak rate reaching 10 mm h 1 unlike the evolution of wind speeds tc induced precipitation does not change significantly during yutu s passage the precipitation has already reached a high rate in the first few days when winds are weak fig 3 displays the total wind stress field from the era5 on october 24 2018 and the τ tc forcing field extracted using the loess filter the loess filter effectively captures the structure of τ tc forcing which is sensitive to the half span parameters in this study we opt for α x 10 and α y 10 to represent the τ tc forcing field reasonably well fig 3b then we adjust the coefficient α τ tc 1 3 to better rescale the τ tc forcing amplitude to observed intensity fig 3c therefore we use τ τ clim 1 3 τ tc as a control run in addition we can adjust the intensity of τ tc forcing by altering α τ tc value in sensitivity experiments the fwf tc forcing is defined as tc induced precipitation minus tc induced evaporation where tc induced precipitation and evaporation are obtained using a threshold value method and the hf parameterization respectively then we add the fwf tc forcing field to the climatological fwf forcing field i e fwf fwf clim fwf tc α tc fwf 1 0 in the control run fig 4 shows the reanalysis total precipitation fields on october 22 24 2018 and the corresponding fwf tc forcing as expected the fwf forcing fields look much like the reanalysis precipitation fields themselves with only a slight offset due to evaporation contribution unlike the change to τ tc forcing intensity the fwf tc forcing intensity does not change significantly during the evolution of typhoon yutu peaking at 10 mm h 1 in addition we can adjust the intensity of fwf tc forcing by altering α tc fwf value in sensitivity experiments these atmospheric forcing fields are specified for ogcm experiments table 1 shows the designed ogcm experiments performed in this study a climatological reference run does not include tc induced forcings the t c τ run and the t c fwf run indicate that only τ tc or fwf tc forcings are included respectively the control run explicitly incorporates both the τ tc or fwf tc forcings i e α τ tc 1 3 and α x α y 10 α fwf tc 1 0 additional sensitivity experiments e g t c τ 1 3 p 1 5 run are performed with the same τ tc forcing but with different intensities of the fwf tc forcing which can be changed by adjusting α fwf i e α τ tc 1 3 and α x α y 10 1 5 α fwf tc 4 7 the differences among those experiments are used to examine the effects of corresponding atmospheric forcing on the upper ocean with different intensity representations 3 upper ocean responses to typhoon yutu two model experiments are conducted using the ogcm fig 1 to illustrate the upper ocean responses to the combined forcings of τ tc and fwf tc one experiment is called the climatological reference run in which the τ tc and fwf tc parts are not included i e α τ tc 0 0 α fwf tc 0 0 the other is called the control run in which the α τ tc τ tc part i e α τ tc 1 3 and α x α y 10 and α fwf tc fwf tc part i e α fwf tc 1 0 are explicitly incorporated with other model settings being kept the same as the climatological reference run the ogcm is initialized from october 1 2018 and integrated for two months for both model experiments whose differences are calculated as the effects induced by the combined forcings of the prescribed τ tc and fwf tc in addition the observational temperature and salinity datasets from argo are used to depict the characteristics of upper ocean response to typhoon yutu and validate the model results detailed analyses are given in this section 3 1 characteristics of the upper ocean responses the upper ocean responses to tc are characterized by a cold wake price 1981 fig 5a 5c display the observed daily sst responses to typhoon yutu on october 22 24 2018 the observed sst response is calculated as a difference between the daily sst field considered and the pre tc condition on october 17 19 2018 typhoon yutu induces sst cooling along its track with a magnitude as large as 1 06 c on october 22 24 2018 the observed sst response is calculated using a grid based maximum response gmr method with three artificial parameters the 400 km distance the five day response duration and the two day pre typhoon average as done in li et al 2020 typhoon yutu induces sst cooling response along its track with a magnitude as large as 2 33 c in the region where yutu passed by on october 22 24 2018 during yutu s passage the sst perturbations exhibit rapid emergence and persist for a relatively long duration leaving a cold wake after yutu s passage the sst response simulated from the ogcm e g fig 5d 5f is compared with that observed from the satellites for simulations the sst response is calculated as the difference between the control run and the climatological reference run i e ctrl ref consistent with the observed sst response e g fig 5a 5c a cold wake emerges after yutu s passage with a magnitude as large as 1 55 c on october 22 24 2018 it shows that spatial patterns and amplitude of the simulated sst response are similar to those observed by satellites however the simulated sst response exhibits a smoother evolution than observed partly due to pure atmospheric forcing specifications for the ogcm which are extracted from the reanalysis product however the simulated sst cooling exhibits a weaker response than observed partly due to the gmr method defining the sst response as the maximum response during a five day response period li et al 2020 while the simulated sst response is the response of the day the sss change induced by typhoon yutu exhibits different features from the sst change the simulated sss increases during the passage of yutu except for the first few days when τ tc forcing is weak and fwf tc forcing is strong e g fig 6d 6f and the maximum increase and decrease of the sss are 0 16 psu and 0 31 psu on october 22 24 2018 respectively with satellite datasets sun et al 2021 also found the sss response to tcs is initially dominated by tc induced precipitation forcing and subsequently controlled by wind driven vertical mixing and upwelling with a non compensated effect due to the decreased sst and the increased sss the sea surface density ssd increases by up to 0 59 kg m 3 on october 22 24 2018 e g fig 6g 6i during the first few days the decreased temperature and salinity tend to compensate for the density with a magnitude as large as 0 35 kg m 3 on october 22 2018 in addition tc induced sst sss and ssd perturbations can persist in the upper ocean for several days to several weeks not shown suggesting that the local ocean responses may produce significant remote influences the ml deepening is also a commonly observed feature of ocean responses to tcs pan and sun 2013 the mld in the bulk ml model is related to the ratio of wind generation of turbulent kinetic energy to the net positive q b gao et al 2020 fig 7 illustrates the simulated daily mld response to yutu and the related q b on october 24 2018 from fig 7a 7b we can see that the ml deepens by up to 53 m on october 24 when most of the positive q b at the sea surface enhances the stratification and shoals the ml conversely wind driven vertical mixing and upwelling are known to deepen the ml therefore the ml deepening is dominated by the τ tc forcing and compensated by the q b effect away from the center of yutu a shoaling of the ml is dominated by the q b effect furthermore the q b at the sea surface can be expressed as the sum of hf q t and fwf q s busalacchi and zhang 2009 as shown in fig 7c 7d the negative q t and positive q s at the sea surface tend to compensate for the q b during the passage of yutu the q b perturbations are dominated by the q s effect except for the left side of the yutu influence region in addition while the sst sss and ssd perturbations persist for lengthy durations the mld and q b perturbations exhibit a relatively rapid recovery within a few days not shown the ocean responses of the ml to tcs and related processes can further affect thermohaline conditions at the subsurface depth fig 8a illustrates an example of the observed perturbation of thermal structure in response to yutu as measured by two pairs of argo profiles a two layer thermal structure is seen in the vertical direction with a cooling in the surface accompanied by a warming in the thermocline the formation of the cooling warming layers can be partly explained by the τ tc induced vertical mixing which is the essence of the heat pump theory emanuel 2001 for comparison fig 8b displays the simulated vertical temperature perturbation at the same location the observed two layer thermal response is well reproduced in the model besides the salinity structure is more complicated at the same location fig 8c 8d which exhibits a three layer structure in the vertical direction with an increase in surface salinity and a decrease increase in the halocline for the additional two layers the different responses of thermal structure and salinity structure might be due to mixing and vertical advection sun et al 2012 since the argo floats at some distance away from the tc track a compensating downwelling is expected price 1981 zhang and coauthors 2018 zhang 2023 vertical mixing and entrainment of subsurface water contribute to a decrease increase increase decrease in temperature salinity at upper 150 m while the difference below 150 m might be explained by the fact that the contribution of downwelling to the salinity anomaly is somewhat different from that to the temperature lin et al 2017 the perturbations for temperature and salinity persist for days to weeks further affecting the thermohaline conditions and related physical dynamics in the ocean 3 2 ml heat budget analyses to further explore the tc induced processes responsible for sst perturbations we perform heat budget analyses of the ml on october 24 2018 fig 9 1 t t u t x v t y w t z κ h h m h h m h t 2 κ v h m h 2 t e t q net ρ c p h m where t t is the temperature tendency u t x v t y is the horizontal advection w t z is the vertical advection κ h h m h h m h t is the horizontal diffusion 2 κ v h m h 2 t e t is the vertical mixing and entrainment of subsurface water into the ml and q net ρ c p h m is the net hf during the passage of typhoon yutu the enhanced vertical mixing and entrainment act to dominate the surface cooling fig 9b besides further away from the yutu s center the mixing is suppressed which contributes to warming in the ml upwelling and horizontal advection also contribute significantly to sst cooling fig 9c 9d the net hf which mostly attributes to latent hf contributes little to the sst cooling fig 9a after the passage of yutu however the net hf begins to dominate the sst warming which is mainly due to the absorption of downward sensible hf by the ocean the sensible hf gained in the wake of typhoon yutu slowly restores the sst perturbations induced by tc which is conducive to the persistence of thermal perturbation signals potentially exerting a basin wide impact on the ocean therefore τ tc forcing increases vertical mixing and a net surface heat loss from the ocean with the reduced stratification which enhances the entrainment of subsurface cold water into the ml and deepens the ml thus inducing a cold wake along the tc track in contrast fwf tc forcing decreases the surface salinity enhances the stratification and suppresses the mixing possibly leading to a reduction in the sst cooling induced by τ tc forcing however the specific ocean processes induced by fwf tc forcing and the extent to which it contributes to the upper ocean responses need to be understood using sensitivity experiments 3 3 ml salinity budget analyses to explore the tc induced processes responsible for sss perturbations we perform salinity analyses of the ml on october 24 2018 fig 10 2 s t u s x v s y w s z κ h h m h h m h s 2 κ v h m h 2 s e s p e s h m where s t is the salinity tendency u s x v s y is the horizontal advection w s z is the vertical advection κ h h m h h m h s is the horizontal diffusion 2 κ v h m h 2 s e s is the vertical mixing and entrainment of subsurface water into the ml and p e s h m is the fwf related term during the passage of typhoon yutu the τ tc induced vertical mixing and entrainment enhance fig 10b acting to dominate the sss increase on october 24 2018 besides the horizontal and vertical advection contribute little to sss increasing in the ml fig 10c d on the contrary freshwater input from tc induced precipitation contributes significantly to sss decreasing fig 10a in particular when fwf tc forcing reaches a certain intensity and the τ tc forcing is weak the sss decreases which is dominated by the fwf tc forcing fig 6d 4 the role played by fwf tc forcing in the upper ocean responses table 1 shows the designed experiments in this section differences among those experiments are used to reveal the effects of corresponding atmospheric forcing on the upper ocean with different τ tc and fwf tc representations the difference between the control run and the climatological reference run i e ctrl ref represents the total responses to the combined forcings of τ tc and fwf tc ctrl t c fwf represents the τ tc forcing only ctrl t c τ represents the fwf tc forcing only t c τ ref represents the responses to τ tc forcing t c fwf ref represents the responses to fwf tc forcing additional sensitivity experiments different from the climatological reference run e g t c τ 1 3 p 1 5 ref indicate the responses to combined forcings of the same τ tc but with different fwf tc intensities 4 1 differences in response characteristics the fwf tc forcing can modulate the upper ocean responses e g jacob and koblinsky 2007 balaguru et al 2022 therefore in this section we focus on different response characteristics from sensitivity experiments on october 24 2018 as typhoon yutu becomes a category 5 super typhoon on the saffir simpson hurricane wind scale in the case with no τ tc forcing the fwf tc forcing alone increases the sst fig 11b decreases the sss fig 11e and collectively decreases the ssd fig 11h thus temperature and salinity tend to have non compensated effects on density while in the case with the included τ tc forcing the effect of fwf tc forcing on the upper ocean becomes more complicated first the fwf tc forcing reduces the sst cooling or even warms the sst fig 11c meanwhile the sss decreases fig 11f but by a smaller magnitude than in the case with no τ tc forcing fig 11e hence the non compensated effects due to sst and sss decrease the ssd fig 11i second the fwf tc forcing can also increase the sst cooling fig 11c induced by τ tc forcing thus the decreased sst and sss fig 11f tend to compensate for each other leading to the increased ssd fig 11i in this case sst rather than sss dominates the ssd variation miller 1976 found that when precipitation occurs in combination with strong wind forcing the ml is perturbed to be deeper than a case with lower wind speeds suggesting that the ml deepens as wind stress forcing increases fig 12 shows the simulated mld difference and the associated q b difference on october 24 2018 the shoaling of ml in the case with the included τ tc forcing fig 12c is smaller than that in the case with no τ tc forcing fig 12b indicating that not only the τ tc forcing but also the fwf tc forcing contributes to the ml deepening when fwf tc forcing is combined with τ tc forcing in particular different from mld changes the positive q b perturbations fig 12e 12f dominated by the q s effect fig 12k 12l are similar in the two cases which is probably because the nonlinear modulation of mld by the fwf tc forcing is affected by the wind generation of turbulent kinetic energy 4 2 differences in ml heat budget heat budget difference analyses ctrl t c τ of the ml on october 24 2018 fig 13 are conducted to investigate the fwf tc induced specific ocean processes responsible for sst perturbations the net hf contributes mainly to the increase in sst cooling fig 13a on the contrary the vertical mixing fig 13b and upwelling fig 13d dominate the reduction in sst cooling in addition the horizontal advection fig 13c contributes significantly to the warming as well as cooling in the ml consequently fwf tc forcing exhibits multiple signals in the upper ocean responses through different processes fig 17 during the passage of typhoon yutu a positive fwf perturbation at the sea surface freshens the upper ocean and enhances the ocean stratification on the one hand the enhanced stratification stabilizes the upper ocean and thus reduces the mixing and entrainment of subsurface cold water into the ml leading to the reduced cooling or even a warming of sea surface on the other hand the enhanced stratification suppresses downward heat transport which in turn increases surface heat loss from the ocean and increases vertical mixing and thus the sst cooling induced by τ tc forcing additionally it is indicated that the multiple effects on the upper ocean due to fwf tc forcing depend on the intensity of τ tc forcing for example the fwf tc forcing induces the sst warming in the absence of τ tc forcing while the fwf tc forcing can even promote the sst cooling in the presence of τ tc forcing however it remains to see whether fwf tc and τ tc forcings are compensated or non compensated for by each other in their effects on the sst which are clearly sensitive to the specifications of their intensities 4 3 differences in ml salinity budget salinity budget difference analyses ctrl t c τ in the ml on october 24 2018 fig 14 are conducted to investigate the fwf tc induced effects on salinity perturbations as revealed the fwf tc forcing acts to freshen the surface and dominate the sss decreasing fig 14a besides fwf tc induced vertical mixing and entrainment of subsurface water into the ml mainly contributes to sss increasing fig 14b which is due to the transport of salty water in the ml through horizontal advection the fwf tc induced horizontal and vertical advection contributes little to sss perturbations fig 14c d 4 4 quantitative analyses to further examine the effects of fwf tc forcing on the upper ocean a site 155 8 e 9 25 n by which yutu s center passes on october 22 2018 is selected for analyses from sensitivity experiments fig 15 shows the time series of sst differences δ sst which are calculated relative to the reference run except for the t c fwf run different sensitivity experiments are performed with varying intensity of fwf forcing i e the α tc fwf value is adjusted while keeping the wind stress forcing constant i e 0 14 n m 2 as expected the δ sst increases with the increased fwf forcing intensity to investigate the possible existence of a threshold value at which fwf tc induced warming effect can exceed the τ tc induced cooling effect we focus on the δ sst on october 22 2018 compared with the t c τ run the fwf forcing increases the sst cooling when α tc fwf 2 3 i e precipitation 6 99 mm h 1 the fwf forcing reduces the sst cooling when 2 3 α tc fwf 3 4 i e 6 99 mm h 1 precipitation 10 37 mm h 1 the fwf forcing instead increases the sst warming when α tc fwf 3 4 i e precipitation 10 37 mm h 1 the quantitative results are supplemented in fig 17 in particular when taking α tc fwf 3 4 i e precipitation 10 37 mm h 1 there is no apparent change in sst at the selected site on october 22 2018 indicating that the τ tc related cooling effect on sst is offset by the fwf tc related warming effect in addition we investigate the main fields of interest on october 23 2018 fig 16 to understand the characteristics of upper ocean responses in this case t c τ 1 3 p 3 4 run near the center of typhoon yutu both the sst and sss decrease which act to have a compensated effect on ssd the ssd increases which is dominated by the sst effect the mld changes little due to the compensated effect of τ tc and fwf tc forcings in general the upper ocean responses are weaker than those in the control run fig 6 during the first few days however the increased sst and the considerably decreased sss promote the decrease of ssd and the ml shoals due to the specifications of weak τ tc and heavy precipitation forcings generally the upper ocean responses during the first few days are dominated by the fwf tc forcing and are stronger than those in the control run in addition the extent to which the fwf tc forcing contributes to sst responses is quantitatively illustrated in table 2 the fwf tc forcing dominates the sst response when wind speed is smaller than 7 m s 1 and the contribution of fwf tc forcing to the sst response is about 20 40 when wind speed is 7 18 m s 1 when wind speed is greater than 18 m s 1 the contribution of fwf tc forcing to the sst response is about 1 8 or even less according to these analyses of model experiments and observations it is concluded that the upper ocean responses to tcs are dominated by τ tc forcing but can also be modulated by fwf tc and hf forcings besides the nonlinear modulation of ocean responses can happen when τ tc related effects on the upper ocean are compensated for by fwf tc related effects furthermore there are several factors responsible for the difference in the upper ocean response by compositing argo profiles and other remote sensing data wang et al 2016 also show that higher intensity tcs induce stronger cooling than lower intensity besides the sst response is also sensitive to tc translation speed pre tc mld and types of oceanic eddies cyclonic anticyclonic eddies wang et al 2016 lin et al 2017 liu et al 2017 5 conclusion and discussion excessive fwf tc inputs into the ocean play an important role in upper ocean responses but have received less attention than τ tc and hf as expected the fwf tc forcing acts to freshen the ocean surface and enhance the stratification which reduces the vertical mixing and entrainment of cold water leading to surface warming this fwf effect is opposite to that induced by τ tc forcing which induces a cooling effect on surface temperature this work investigates the upper ocean responses to the combined forcings of fwf tc and τ tc through ocean model experiments and observational analyses taking typhoonyutu 2018 as an example the upper ocean responses are characterized by surface cooling surface salting and ml deepening which are dominated by the τ tc forcing and are also modulated by the fwf tc forcing in addition the ocean responses are not limited to the surface but extend to subsurface layer taking the region where the argo buoys are located the simulated ocean responses exhibit a two layer thermal structure and a three layer salinity structure in the vertical both being consistent with the argo profiles analyses of the ml heat and salt budgets are performed to illustrate the related processes responsible for ocean responses during the passage of typhoon yutu the cold wake along the tc track is dominated by vertical mixing and modulated by net hf horizontal advection and vertical advection specifically the τ tc forcing increases mixing and surface heat loss from the ocean both of which act to weaken stratification thus the entrainment of subsurface cold water into the ml enhances and the ml deepens leading to sst cooling on the contrary the fwf tc forcing freshens the ocean surface and enhances the stratification which reduces the vertical mixing and entrainment of cold water from the subsurface leading to reduced cooling or even a warming of sea surface meanwhile the enhanced stratification due to fwf tc forcing can also suppress downward heat transport but increase surface heat loss from the ocean which contributes to vertical mixing and thus surface cooling induced by τ tc forcing by conducting a series of sensitivity experiments it is revealed that fwf tc forcing in combination with τ tc forcing can exhibit multiple signals in the upper ocean responses through different processes on the one hand fwf tc forcing decreases the sss and warms the sst which is counteracted by τ tc forcing in this case the combined effects of sst and sss responses lead to a decrease in ssd indicating non compensated nature by each other on the other hand fwf tc forcing can increase the sst cooling and decrease the sss in this case the increased ssd is dominated by the sst effect and is compensated by the sss effect in particular the extent to which fwf tc forcing contributes to the sst response is quantitatively investigated by modeling experiments and is compared with τ tc forcing when τ tc forcing i e τ tc 0 14 n m 2 is combined with a weak fwf tc forcing i e precipitation 6 99 mm h 1 the fwf tc forcing acts to increase the τ tc related cooling effect on sst as the fwf tc forcing enhances while the τ tc forcing remains constant i e τ tc 0 14 n m 2 6 99 mm h 1 precipitation 10 37 mm h 1 the fwf tc forcing acts to reduce the τ tc related cooling effect on sst although the τ tc forcing dominates the sst response the fwf tc forcing can significantly affect the sst pattern when τ tc forcing effect is weak it is worth noting that when weak τ tc forcing i e τ tc 0 14 n m 2 is combined with a strong fwf tc forcing i e precipitation 10 37 mm h 1 the τ tc related cooling effect on sst is offset by the fwf tc related warming effect furthermore the sst response exerts feedbacks on the intensity of tcs through its effect on the air sea enthalpy flux suggesting that the fwf tc may play an important role in determining the intensity and track of tcs although both τ tc forcing and fwf tc forcing could affect the upper ocean thermohaline structure they might be different in two ways the region and the time interval the radius of inner spiral rainbands is about 2 3 times the radius of maximum wind wang 2008a which means tcs exert the effects on the ocean through fwf tc forcing over a wider region than τ tc forcing and the fwf tc forcing induced by tc precipitation appears intermittent in time while τ tc forcing appears continuous as a result the oceanic response induced by fwf tc forcing might be intermittent over a much wider horizontal region wang et al 2018 besides as previously described the upper ocean responses induced by τ tc forcing are characterized by rightward bias which occur because in the ocean frame the τ tc vector turns clockwise in the time on the right side of tc track and anticlockwise on the left side of the track price 1981 however strong fwf tc forcing due to heavy rainfall dilutes the sss stabilizes the upper ocean and alters the intensity of vertical mixing and entrainment of the subsurface water the fwf tc induced response characteristics do not resemble the rightward bias induced by τ tc forcing the results of this study are considered to be limited in sense that ocean only modeling is taken in which clean individual forcings of τ tc and fwf tc are extracted and specified for sensitivity experiments the upper ocean responses to tcs are more complicated and variable in realistic scenarios for example realistic complicated background conditions of the ocean could also regulate the way in which the upper ocean responds to tcs further case studies are needed to thoroughly evaluate the fwf tc related effects on the upper ocean locally in the western pacific and remotely in the eastern equatorial pacific an important region for el niño southern oscillation zhang et al 2013 2022b credit authorship contribution statement shuoni ye ocean modeling visualization formal analysis investigation validation writing original draft rong hua zhang mentoring methodology writing editing funding acquisition hongna wang writing editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research is supported by the national natural science foundation of china nsfc grant no 42030410 the laoshan laboratory no lskj202202404 and the startup foundation for introducing talent of nuist 
23764,we evaluate the skills of ocean sea ice general circulation models involved in the ocean modeling intercomparison project in simulating the ocean mixed layer depth and its seasonal cycle in the arctic region during summer months all models consistently underestimate the mixed layer depth compared to observational data from the monthly isopycnal mixed layer ocean climatology and the ice tethered profilers in fall and winter the models exhibit great variability compared to observational data and inter model comparison reveals differences up to several tens of meters we analyze the origin of the fall and winter model biases in ice covered regions where the seasonal cycle of the surface salinity and mixed layer depth is strongly influenced by brine rejection resulting from ocean sea ice interactions focusing first on the central arctic ocean defined here as the region north of 80 n we show that all models simulate more or less the same vertical sea ice mass balance and thus similar salt fluxes into the ocean during sea ice freezing furthermore the model ensemble features a strong relationship between the stratification profile in september and the mixed layer depth at the end of winter the models whose stratification compares the best to observational data also display the most realistic values of the mixed layer depth at the end of winter we argue that the discrepancies between models are therefore not so much linked to the surface salt balance but rather to the accuracy with which those models reproduce the vertical salinity profile in short a weakly stratified ocean tends to create a deep mixed layer while strong stratification leads to a shallow mixed layer to substantiate this conclusion we apply a simple conceptual model which simulates the month to month evolution of the mixed layer depth using as input the vertical salinity gradients and the surface salt fluxes from general circulation models quite surprisingly this simplified dynamics captures very well the behavior of the general circulation models emphasizing the role of the different vertical stratification in the control of the mixed layer depth furthermore this interplay may also significantly account for the large mixed layer biases observed in other ice covered regions of the pan arctic seas even though sea ice ocean interaction is not the only driver of mixed layer variability in fall and winter there keywords mixed layer depth arctic ocean sea ice ocean stratification omip models data availability data will be made available on request 1 introduction the arctic mixed layer ml is the upper layer of the arctic ocean that controls the exchanges between the deeper ocean sea ice and the atmosphere those transfers are influenced by complex thermodynamical and dynamical processes likely to create strong heterogeneities in ocean surface properties such as discontinuous and dynamic sea ice cover ocean eddies or salinity fronts and filaments at the kilometre scale rippeth and fine 2022 goosse et al 2018 horvat et al 2016 the ml is characterized by a homogeneous density profile which goes from the oceanic surface to the beginning of the pycnocline an accurate characterization of the mixed layer depth mld is relevant to a large number of physical and biological processes from a physical point of view the ml mediates the transfer of heat between ocean sea ice and atmosphere and therefore plays a key role in the global energy budget and oceanic circulation mcphee 2008 gettelman and rood 2016 in the last decades global climate change has strongly affected the arctic region in particular leading to a fast decrease in sea ice extent timmermans and marshall 2020 nummelin et al 2016 perovich and richter menge 2009 this retreat of the sea ice affects the dynamics of the ml from a biological point of view a major spatial expansion of under ice phytoplankton blooms at high latitudes has been observed by arrigo et al 2012 boles et al 2020 and horvat et al 2017 these organisms benefit from the mixing in the upper layer of the ocean ardyna et al 2020 while crucial an accurate modeling of the mld remains a challenge for global climate models in particular large discrepancies are found among the climate models that performed climate projections for the assessment reports of the intergovernmental panel on climate change ipcc cassotta et al 2022 meredith et al 2019 previous studies by ilıcak et al 2016 and tsujino et al 2020 highlighted the poor skills of general circulation models gcm in simulating the mld in arctic regions with large biases between the models and the observational data in the present study we aim to substantiate those discrepancies by assessing the skills of the ocean sea ice gcm that participated in the ocean model intercomparison project omip we study the ability of these models to reproduce the seasonal cycle of the mld in the ice covered regions of the pan arctic seas specifically we focus on the central arctic ocean beaufort sea chukchi sea east siberian sea laptev sea kara sea and barents sea in these regions the mld varies seasonally from 20 to 80 m in winter to 5 to 30 m in summer peralta ferriz and woodgate 2015 showed that in these areas the mld is strongly correlated to the ocean stratification and the wind mainly affects the ml during ice free periods our goal here is to study the ability of omip models to reproduce the fall and winter deepening of the mld compared to the monthly isopycnal mixed layer ocean climatology mimoc schmidtko et al 2013 and ice tethered profilers itp observations toole et al 2011 krishfield et al 2008 we describe and quantify their biases and we give some insights about the origin of the differences by using a simplified surface model inspired from the work of martinson 1990 we focus on the fall and winter seasons because we aim to identify the origins of inter model differences which are much larger during these seasons the paper is organized as follows section 2 gives a brief description of the omip dataset mimoc climatology and itp observational data section 3 presents a diagnosis of the mld in the central arctic ocean simulated by the omip models and analyzes other variables relevant to understand ml seasonal changes such as the sea ice concentration the surface fluxes the salinity profiles and the ocean stratification we also describe the simplified surface model and assess its skills with respect to the omip models in the central arctic as well as in the other pan arctic seas finally section 4 presents concluding remarks and discusses implications of our work 2 method in this section we briefly present the selected omip models and the observational data mimoc provides us with monthly observational data of the mld schmidtko et al 2013 mimoc climatology is available at the national oceanic and atmospheric administration and it contains fields of ocean physical properties such as density temperature and salinity as a function of depth such values are obtained by conductivity temperature depth ctd instruments from shipboard data of the world ocean database ice tethered profilers itp and argo program due to data availability the climatology is set up between 2007 and 2011 mimoc has a spatial resolution of 0 5 from 80 s to 90 n and vertical resolution of 81 levels the mld is calculated using the algorithm of holte and talley 2009 which performs a statistical optimization based on traditional threshold and gradient methods over temperature salinity and density individual profiles thereby improving the accuracy of the depth between homogeneous mixed layer and turbulent mixing as previously discussed by schmidtko et al 2013 this methodology yields a good agreement with the common threshold density criteria δ ρ ρ z ρ z r e f 0 03 kg m 3 used in the omip framework also known as the sigma t criterion griffies et al 2016 this criterion has been introduced by levitus 1982 and defines the mld as the position from the shallowest depth down to the first depth for which the relative difference of density exceeds 0 03 kg m 3 note that the surface level is only indicative in the model simulations it is defined in an ad hoc manner and could vary from one model to another treguier et al 2023 in order to estimate the impact of averaging different datasets and observations in mimoc we also computed directly the mld from individual ice tethered profilers itp toole et al 2011 krishfield et al 2008 using the completed missions available at woods hole oceanographic institution whoi we calculate the vertical potential density profiles using the teos 10 gsw gibbs sea water python library from the conservative temperature and the absolute salinity profiles we applied the sigma t criterion to compare itp observational data with omip models where the reference surface depth for itp is more or less 5 m the itp includes data from 2004 until 2019 with the majority of the observations between the years 2007 to 2015 we use here the itp data from 2004 until 2011 finally we use the osi 450 observational dataset for sea ice concentration lavergne et al 2019 the osi 450 dataset is available at the eumetsat data services and it contains the sea ice concentration calculated from swath observations the period covered by osi 450 observational data goes from january 1979 to december 2015 and its grid spacing is about 25 km we have used the mean over the period covered by mimoc data for consistency 2007 2011 for the gcm models we use models participating in the omip project we work with models that contributed to both phases of the project omip i using as forcing the coordinated ocean ice reference experiments version 2 griffies et al 2016 core ii and omip ii using as forcing the updated japanese 55 year atmospheric reanalysis tsujino et al 2020 jra55 do in order to compare models with observational data we set up two climatologies between 2007 and 2009 for omip i and between 2007 and 2011 for omip ii both using the last cycle sixth of the protocol the difference between the two periods is due to omip i experiments ending in 2009 our results and conclusions are not affected by this set up our study uses the variables ocean mixed layer depth mlotst for which the models follow the previously explained sigma t criterion sea ice mass change from thermodynamics sidmassth defined as the ice mass balance due to surface and basal heat fluxes i e melting sublimation and freezing sea water salinity so defined as the salt content of sea water it is a dimensionless variable expressed in parts per thousand sea water potential temperature thetao defined as the mean potential temperature in c using as reference the ocean surface sea ice concentration sicon defined as the percentage of the grid cell covered by sea ice the definitions of these variables follow the omip protocol griffies et al 2016 omip models do not generically produce the sidmassth variable hence we restrict our analysis to those which do we thus work with a subsample of 10 models with different ocean and sea ice components as well as vertical resolution and for which the nominal resolution ranges between 1 and 0 25 see table 1 it is worth mentioning that all variables are interpolated to the mimoc nominal spatial resolution before analyzing them 3 results 3 1 mixed layer in permanent ice covered regions our analysis focuses first on the central arctic ocean geographically defined here as the region from 80 n to the north pole we first single out this area because it contains the largest sea ice extent in the arctic region and is likely the region where vertical mass exchanges between sea ice and the mld are most dominant in addition to the wind driven mixing in the upper part of the ocean and horizontal advection exchanges that potentially play a major role in all regions figs 1 and 2 show the pan arctic mld spatial distributions from mimoc itp profiles and omip models with the ensemble average of the models and its standard deviation in september and march respectively in september mimoc and omip models have a shallow and quite homogeneous mixed layer the omip mld ensemble average has a similar behavior to individual models and the ensemble standard deviation reaches only a few meters the itp profiles display larger spatial variability with some deeper spots compared to mimoc and omip models the disagreement between models and observations is higher in march many models tend to systematically overestimate the mld by several tens of meters compared to the mimoc and the itp observational data the ensemble average also displays deeper ml compare to observational data with a large standard deviation in the central arctic ocean north of svalbard and in the barents sea core ii forced models studied by ilıcak et al 2016 display a similar behavior with strong biases of the march mld compared with the mimoc dataset we now analyze the seasonal cycle of the spatially averaged mld this cycle is shown in fig 3 for all omip models mimoc climatology and itp observations we observe that the ml from mimoc and itp observational data remains shallow during the whole year both seasonal cycles exhibit less than 10 meters of amplitude only varying between 25 meters at the low summer value and 35 meters at the peak winter value for mimoc and between 22 and 28 meters for itp observations please note that the seasonal cycle from itp observational data displays a different behavior than in peralta ferriz and woodgate 2015 where they show a larger amplitude of the mld seasonal cycle in the central arctic ocean the differences in the mld criterion explain it in peralta ferriz and woodgate 2015 they calculate the mld using the threshold criterion δ ρ 0 1 kg m 3 in contrast in our study we use the sigma t criterion δ ρ 0 03 kg m 3 several criteria are used to compute the mld in the arctic region cole and stadler 2019 stranne et al 2018 polyakov et al 2013 timmermans et al 2012 mizobata and shimada 2012 jackson et al 2012 and there is still no clear consensus as to which criterion is the best however our aim here is to compare omip models and then we employ the threshold 0 03 kg m 3 criterion to obtain the mld it explains our choice of criterion additionally fig 3 shows that most omip models exhibit a large dispersion compared with the observational data mimoc and itp all the models display two clear seasonal phases i in spring almost all the modeled ml are deeper than the observational data and in summer all the models underestimate it by about 15 m ii in fall and winter the simulated ml becomes deeper and discrepancies with observations reach up to several tens of meters in some models the differences are also large between the models the standard deviation of the omip models ensemble average reaches about 20 m for instance cmcc models generate too deep mixed layers and the cas esm2 0 model a too shallow mld comparing omip i to omip ii protocols a systematic decrease exists for the amplitude of the mld seasonal cycle in the latter case this effect was previously observed by tsujino et al 2020 presumably due to the more significant freshwater discharge from greenland in the omip ii models for cmcc cm2 sr5 this trend is hardly significant and may as well be due to statistical biases in such cases one could not rule out that omip ii protocols may produce larger mld at a very local level as for instance reported by shu et al 2022 comparing the ensemble average spatial distribution of omip i and omip ii in the central arctic ocean for cesm2 and mri esm2 0 the difference is clearly visible with differences in the march mld of 17 and 11 m respectively however even in this case switching from omip i to omip ii hardly compensates the biases with the observed mld additionally increasing the resolution does not seem to correct the biases either the model with the highest resolution is cmcc cm2 hr4 omip ii and it simulates close to the exact same cycle as its low resolution counterpart cmcc cm2 sr5 omip ii to quantify the mld spatial variability from omip models and observational data we show their spatial standard deviation for each month in the left panel of fig 4 this quantity measures the dispersion of the mld around the central arctic ocean for most omip models and mimoc dataset the standard deviation remains lower than 10 meters over the entire year while the standard deviation for itp observational is slightly higher with values between 10 and 15 m the cmcc and ipsl models display a more significant standard deviation during fall and winter where the ipsl model reaches more than 100 meters in march april and may the right panel of fig 4 shows the relative standard deviation rsd defined as the percentage of the ratio between the standard deviation and the mean value during the year most omip models display a rsd smaller than 50 it means that the spatial variability is not too large however cmcc cm2 hr4 omip ii and cesm2 omip ii models reach large rsd of more or less 100 and the ipsl cm6a 0 omip i model even has values close to 400 these models have an important spatial variability with standard deviation values larger than their mean it is also observed in fig 2 where these models show important spatial differences in this region some studies distinguish between eurasian and makarov basins to have more homogeneous conditions in the central arctic ocean see for instance peralta ferriz and woodgate 2015 we do not make this choice because the spatial variations are not too large in most omip models and mimoc observational data 3 2 sea ice and ocean physical properties sea ice concentration a key feature of the central arctic ocean is the presence of sea ice during the whole year we refer to this area as a permanent ice covered regions the left panel of fig 5 illustrates the seasonal cycle of sea ice concentration at the surface of the ocean a 100 value means that the surface is fully covered by sea ice while a lower value means that there exist uncovered sectors observational data shows that sea ice concentration in the central arctic is approximately 100 during fall winter and spring only decreasing to 80 in summer hence exhibiting a variation of about 20 in magnitude at a qualitative level this decrease is correctly reproduced by most of the models at a quantitative level the omip ii models perform slightly better than their omip i counterparts in the former models the sea ice concentration typically varies between 15 and 40 from summer to winter this is more faithful than the variation of 60 simulated by cas esm2 0 cesm2 cmcc cm2 sr5 and cmcc esm2 part of the omip i experiment this effect was previously noticed by tsujino et al 2020 and thoroughly explained by lin et al 2022 due to the change of shortwave radiation fluxes from omip i to omip ii simulations salt transfer from a physical point of view the permanent presence of a sea ice layer reduces the interactions between the upper ocean and the atmosphere for instance limiting the shear produced by winds and waves the ml characteristics are then largely determined by the interactions between the ml and the sea ice the rough physical picture is then the following in fall and winter the growth of sea ice is associated with brine rejection i e salt is rejected from crystal structures of water ice increasing salinity in the upper layer of the ocean in spring and summer sea ice melts and freshwater goes to the ocean decreasing the salt concentration the details of the mass transfer may depend on the dynamics of each model and its parameterization for instance barthélemy et al 2015 studied the impact of this process in the ocean using the nemo lim3 global ocean sea ice model we can estimate this transfer of mass associated with sea ice formation and melting directly from the sidmassth output in the omip protocol we convert it into an equivalent salt flux measured as the meters of salt transferred in each month the right panel of fig 5 shows the seasonal cycle of the corresponding salt flux φ s for omip models during fall and winter when the salt flux is positive inter model comparison displays small variations for instance the mean value in january is 6 13 ppt meters month with a standard deviation of 0 6 ppt meters month fig a 1 shown in appendix reveals that all the models simulates a similar amount for salt transfers towards the ocean totaled over the winter months without any clear link with the mld in winter this suggests that the biases observed in the omip mld are not due to discrepancies related to the sea ice mass budget but rather to other processes involved in the mld seasonal evolution such as wind driven and horizontal exchanges vertical salinity profile and stratification the fluxes from sea ice affect the physical properties of the ocean in particular the vertical density profile this in turn causes variations in the ocean stratification we recall that the mld in omip models is determined by applying a density criterion which is by construction sensitive upon the underlying vertical density profile while ocean density is in general a non linear function of temperature and salinity the density in permanent ice covered regions is mostly controlled by ocean salinity and temperature variations are relatively small in the top layers of the ocean see for instance gettelman and rood chapter 6 2016 this behavior is indeed observed in omip models when monitoring the spatially averaged ocean salinity and temperature vertical profiles hereafter simply referred to as the mean salinity profile and mean temperature profile fig 6 shows that in most of the omip models and in the mimoc dataset while the temperature is not vertically changing much less than 1 c the first abrupt change of the mean salinity profile indicates the bottom of the mixed layer and the beginning of the halocline this suggests that in ice covered regions including but not limited to the central arctic the halocline shape provides a reliable indicator to estimate the ocean stratification the vertical stratification below the mixed layer at the month m is estimated here as the mean vertical gradient of the mean salinity profile s m between the mld h m and the current depth z that is 1 γ m z s m z s m h m z h m the vertical salinity profile is not linear as shown in fig 6 hence the stratification varies with depth the vertical salinity profile also varies with time to monitor its evolution we compute the ocean stratification of the current month m using eq 1 see fig 7 please note that the stratification is calculated over the halocline profile using as an approximation for z the depth corresponding to the ml of the following month as expected all omip models and mimoc climatology display a decrease in ocean stratification as winter progresses as a consequence of the transfer of salt from sea ice among the omip models the largest variations between models for the stratification occur in september and decreases over fall and winter the fall winter evolution of omip models ocean stratification is far more pronounced than mimoc observational data focusing on september we note that omip models with ocean stratification values closer to mimoc also show a seasonal cycle similar to observational data 3 3 a surface model for the salt balance surface model we propose to use a simple framework to reproduces the fall and winter deepening of the ml in terms of the salt balance and mixed layer dynamics this framework is inspired by the work of martinson 1990 and is illustrated in fig 8 the model neglects the non linearity of the vertical salinity profile as well as the effect of the wind stress and horizontal exchanges it links the salt flux φ s flowing into the ocean between september and month m as 2 φ s m s m 1 h m 1 s m h m h m 1 h m 2 where h m and h m 1 represent the mld at month m and m 1 and s m h m and s m 1 h m 1 the salinity values at the corresponding mld in principle the salt flux the mld and the salinity depend on the latitude and longitude coordinates for the sake of clarity we later omit to explicitly feature this spatial dependence eq 2 can be interpreted as a midpoint approximation as illustrated in the right panel of fig 8 it relies on the observation that essentially in the omip models the vertical salinity profiles stay piecewise linear during fall and winter months as shown in the left panel of fig 8 to estimate the salinity at the mld we use the data driven approximation 3 s m 1 h m 1 s m h m γ s e p t h m a r h m 1 h m involving the september stratification until the mld in march γ s e p t h m a r which is obtained by eq 1 using eq 3 the salt flux of eq 2 becomes 4 φ s m γ s e p t h m a r 2 h m 1 2 h m 2 from this formula one can now explicitly relate the mld at month m to the mld at month m 1 as 5 h m 1 h m 1 2 φ s m h m 2 γ s e p t h m a r we use eq 5 to reproduce the fall and winter ml deepening for each omip model the formula depends on two main inputs namely the salt flux ϕ s m and the september stratification between the basis of the ml and a depth corresponding to the ml in march γ s e p t h m a r to prescribe those values we use the outputs of the omip models values for the salt flux ϕ s correspond to the ones previously shown in the right panel of fig 5 the september stratifications for the various models are calculated from the omip salinity profiles we later discuss two series of simulations obtained either with i a local methodology in which eq 5 is applied at each grid point x y using the local value for the september stratification γ s e p t h m a r and salt fluxes φ s m the local mld h m 1 x y obtained from the surface model is then averaged in space for the plots ii an average methodology in which eq 5 is applied to the salinity gradients and salt fluxes averaged in space the first methodology should in principle be able to capture spatial fluctuations but may be more sensitive to the horizontal transport that is neglected in the present framework in contrast to the second one which is only driven by averaged omip output quantities surface model vs omip models we here analyze the skills of both methodologies to reproduce the fall and winter ml deepening fig 9 displays the full fall and winter mld seasonal cycle simulated by the surface model for each omip model and mimoc observational data the fall and winter deepening from mimoc observational data is only shown in the average methodology because we do not have access to salt flux observations we use as an approximation the ensemble average salt flux from omip models at a qualitative level the surface model provides a correct representation of the mld growth in the first three months october november and december for both methodologies but the local one displays remarkable quantitative agreement with most of the omip seasonal cycles we relate this feature to the fact that averaged inputs loose tracks of the spatial fluctuations which are present in omip simulations specifically we observe strong spatial variations in the ocean stratification measured here by using the salinity profile see fig a 2 and fig a 3 in appendix besides we notice that the models cas esm2 0 omip i and ipsl cm6a 0 omip i have most disagreement with the surface model to quantify this behavior we have calculated the mld relative error 6 relative error m l d o m i p m i m o c m l d s u r f a c e m l d o m i p m i m o c where m l d o m i p m i m o c corresponds to the mld from omip models or mimoc observational data and m l d s u r f a c e is the estimated mld from the surface model the march mld error is shown in the bottom right panel of fig 9 almost all omip models display less than 15 error and mimoc more or less 25 of error among the omip models whose mld in march is quasi perfectly reproduced by the simple model we find the models cesm2 omip i and cmcc omip i for those two models the surface model displays less of 10 error we however recall that compared to observational data both cesm2 omip i and cmcc omip i largely overestimate the amplitude of mld seasonal cycle and in particular the mld in march the fact that a simplified model which only considers the vertical salt flux captures much of the mld evolution when those values are biased compared with observational data suggests that the mld modeling could improve by including other processes responsible for changes in mld such as wind driven and horizontal exchanges neglected in the simple model conversely cas esm2 0 omip i and ipsl cm6a 0 omip i display the largest mismatch with the surface model while those models are the omip models which most consistently reproduce the mld seasonal cycle in comparison to observational data this suggests that those models represent better the ml dynamics and that this dynamics is more complex that the one of the simple surface model we conclude this section by noticing that there exists a strong relationship between the september ocean stratification and the march mld in omip models this relationship is shown in the left panel of fig 10 where we observe that deep mixed layers relate to weakly stratified oceans while shallow mixed layers relate to strongly stratified oceans as expected furthermore compared to mimoc dataset the omip models with the best representation of the stratification in september are also the ones with the best mixed layer in winter this suggests that the representation of the stratification is a strong signature of the mld biases in omip models 3 4 adjacent seas of the central arctic ocean we now analyze if a similar behavior for the mld dynamics is present in neighboring seas of the central arctic ocean specifically we look into the beaufort chukchi east siberian laptev kara and barents seas their boundaries are shown in the right panel of fig 10 their mld seasonal cycle from mimoc itp and omip models are displayed in fig 11 itp observations are only available during the whole year for the beaufort and chukchi seas in the other regions we only compare with mimoc climatology the seasonal cycle of the mimoc mld in the beaufort chukchi east siberian laptev and kara seas exhibits a similar behavior than in the central arctic ocean characterized by a small seasonal amplitude and a shallow ml throughout the year about 20 meters over the year the behavior is quite different in the barents sea where the mimoc mld seasonal cycle shows a high seasonal amplitude it is characterized by a 60 meter difference between january and august with a maximum mld of 80 meters in january in each region the omip models differ strongly with each other some having mld close to the mimoc one while others overestimate it in winter by tens of meters regarding itp observations the mld seasonal cycles in the beaufort and chukchi seas are a few meters shallower compared to mimoc as already noticed for the central arctic ocean the magnitude of the inter model variations differs depending on the sea under consideration in the beaufort chukchi kara east siberian and laptev seas discrepancies between models reach up to 30 meters on average in the barents sea the mld inter model variations reach more than 100 m with the exception of the barents sea the arctic adjacent seas have more than 80 of sea ice concentration during winter months see fig a 5 in appendix this suggests that in these almost fully ice covered regions brine rejection has a large impact on the ml fall and winter dynamics in addition to other processes such as wind driven mixing and horizontal advection similarly to the central arctic ocean in these regions the fall and winter salt flux shows small variations between models see fig a 4 in appendix to get more quantitative insights we also applied the surface model in its averaged version for all omip models in each region the relative errors of the march mld are shown in fig 12 in the beaufort and chukchi seas the surface model display a similar behavior than the central arctic ocean almost all omip models reach less than 15 error in the reproduction of the fall and winter mld deepening in the east siberian and laptev seas a more subtle feature is observed the small relative errors are due to the shallow ml and not to the good prediction of the surface model in the kara sea the relative errors are larger than in the other almost fully ice covered regions and then the surface model is not pertinent to explain the mld variations this could be due to the lower concentration of sea ice in winter compared to other regions as well as exchanges with the barents sea the surface model displays a poor ability to reproduce the fall and winter deepening of the ml in the barents sea as expected in this area which is partly covered by sea ice see fig a 5 in appendix the fall and winter ml deepening is not dominated by the salt balance associated with the exchanges with sea ice but is controlled by surface cooling wind driven mixing and horizontal advection those mechanisms are not considered by the surface model explaining its poor performance at reproducing the results of the gcm models those results suggest that only the beaufort and chukchi seas display similar behavior compared to the central arctic ocean omip models with larger biases compared with observational data are the ones with the best representation using the simplest model suggesting that this missing the impact of more complex processes leads to overestimating the mld seasonal cycle as a side remark large errors are obtained with the cas esm2 0 omip i model in some almost fully ice covered regions fig 11 shows that mismatches come from the model itself with a poor representation of the mld seasonal cycle in the east siberian laptev and kara seas this is confirmed by looking at its mld spatial distribution during the whole year see fig a 6 in appendix we observe that during april may and june the cas esm2 0 model simulates large mld especially on the east siberian and laptev coasts shu et al 2022 suggest that one possible reason for these discrepancies is that the cas esm2 0 model has the canadian arctic archipelago passes closed finally in all the regions apart from the kara sea omip models whose ocean stratification compares the best to the mimoc observational data also show the closest ml deepening at the end of the winter see fig 13 besides only the beaufort and chukchi seas display a strong relation between the stratification in september and the mld at the end of winter also observed in the central arctic ocean this is compatible with the general idea that strongly stratified oceans lead to shallow ml and weakly stratified oceans lead to deep ml however for the others regions the relation is not so clear 4 discussion and conclusion we have studied the ability of omip models to reproduce the fall and winter deepening of the ml in pan arctic seas central arctic ocean beaufort chukchi east siberian laptev kara and barents seas we have shown that in all these regions omip models poorly represent the mld seasonal cycle in summer a large part of omip models underestimates the mld by about 15 meters compared to mimoc climatology during fall and winter large biases appear with some models with very deep ml and others with values closer to observational data in particular we have observed that the cas esm2 0 ipsl cm6a 0 and mri esm2 0 models from omip i protocol are consistent enough in simulating the fall and winter deepening in more than one region of the pan arctic seas however cmcc models at low and high resolution and from omip i and omip ii protocols display too depth ml in almost all the seas during these seasons discrepancies between models reach up to 30 meters on average in all the regions except in the barents sea where the mld inter model variations reach more than 100 m we showed that omip models provide consistent sea ice concentrations and ice ocean salt fluxes at the same time discrepancies have been observed in the ocean stratification at the beginning of the sea ice growth season in the central arctic ocean beaufort and chukchi seas we have shown a strong relationship between the ocean stratification in september and the mld at the end of winter weakly stratified oceans lead to large mld and strongly stratified oceans lead to small mld it should be noted that this is not a causal relation and then we can also reverse the relationship for instance shallow ml in winter leads to strongly stratified oceans at the beginning of the fall furthermore omip models with similar ocean stratification compared to mimoc observational data perform better in the reproduction of the mld at the end of the winter we use the mimoc climatology and the itp observational data to compare omip models we have found that both observational data have similar mld seasonal cycles in the central arctic ocean beaufort and chukchi seas using the omip recommended density threshold of 0 03 kg m 3 to compute the mld the depth of the mixed layer varies depending on the criterion used for instance the ml is deeper using a criterion of 0 1 kg m 3 instead of 0 03 kg m 3 in pan arctic regions peralta ferriz and woodgate 2015 the different choices of criterion by different authors make model data comparisons more difficult additionally due to itp data availability the east siberian laptev kara and barents seas are only compared with mimoc climatology in principle mimoc climatology is strongly based on argo itp observation during this period the spatial distribution of argo float data is very sparse in laptev and kara seas fournier et al 2020 it suggests that mimoc may not represent the full reality regarding mld in these regions we were able to reproduce the fall and winter deepening of the ml simulated by the omip models using a simple surface model based on the vertical salt balance dynamics this model uses as inputs the vertical salinity gradient in september and the salinity flux from omip models in the central arctic ocean beaufort and chukchi seas we have noted that omip models with the largest relative errors from the reproduction of the fall and winter ml deepening using the surface model are the ones that display more realistic values of the mld seasonal cycle compared with observational data it suggests that these models accurately reproduce the ml dynamics and that these dynamics is more complex that the one of the simple surface model in the other regions the mld dynamics is different for instance in the barents sea the retreat of ice cover during summer is larger than in the central arctic hence favoring exchanges with the atmosphere this feature is likely to foster deeper ml the barents sea displays a larger mld seasonal cycle than the other regions still and despite these different behaviors the poor modeling of gcm in pan arctic seas seems to be linked to a very simple modeling of the processes involved in its dynamics here we focused on the vertical mass exchanges associated with salt balance a natural perspective is to study the impact of the other mechanisms such as surface cooling wind driven mixing and horizontal advection furthermore an appealing perspective to our work concerns the study of the mld inter annual variability particularly the effect of the feedback between the sea ice and the mixed layer in this prospect we expect to analyze the gcm of the coupled part of the cmip6 protocol from the historical simulations in the same line as previous works by watts et al 2021 and keen et al 2021 in the inter model analysis of sea ice and mass budget in the arctic credit authorship contribution statement s allende conceptualization methodology formal analysis writing original draft visualization t fichefet conceptualization writing review editing h goosse conceptualization writing review editing a m treguier conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was conducted within the jpi oceans and climate project medley mixed layer heterogeneity which is partly funded by the belgian science policy office under contract be 20e p1 medley and by the french agence nationale pour la recherche under contract 19 jpoc 0001 01 appendix we here compile additional figures related to the relation between the mld in march and the cumulative salt flux fig a 1 the sea surface salinity maps from the mimoc dataset and omip models in september fig a 2 and march fig a 3 the salinity flux in the pan arctic seas fig a 4 the sea ice concentration in the pan arctic seas fig a 5 and the spatial distribution of the sea surface salinity from the cas esm2 0 omip i model fig a 6 
23764,we evaluate the skills of ocean sea ice general circulation models involved in the ocean modeling intercomparison project in simulating the ocean mixed layer depth and its seasonal cycle in the arctic region during summer months all models consistently underestimate the mixed layer depth compared to observational data from the monthly isopycnal mixed layer ocean climatology and the ice tethered profilers in fall and winter the models exhibit great variability compared to observational data and inter model comparison reveals differences up to several tens of meters we analyze the origin of the fall and winter model biases in ice covered regions where the seasonal cycle of the surface salinity and mixed layer depth is strongly influenced by brine rejection resulting from ocean sea ice interactions focusing first on the central arctic ocean defined here as the region north of 80 n we show that all models simulate more or less the same vertical sea ice mass balance and thus similar salt fluxes into the ocean during sea ice freezing furthermore the model ensemble features a strong relationship between the stratification profile in september and the mixed layer depth at the end of winter the models whose stratification compares the best to observational data also display the most realistic values of the mixed layer depth at the end of winter we argue that the discrepancies between models are therefore not so much linked to the surface salt balance but rather to the accuracy with which those models reproduce the vertical salinity profile in short a weakly stratified ocean tends to create a deep mixed layer while strong stratification leads to a shallow mixed layer to substantiate this conclusion we apply a simple conceptual model which simulates the month to month evolution of the mixed layer depth using as input the vertical salinity gradients and the surface salt fluxes from general circulation models quite surprisingly this simplified dynamics captures very well the behavior of the general circulation models emphasizing the role of the different vertical stratification in the control of the mixed layer depth furthermore this interplay may also significantly account for the large mixed layer biases observed in other ice covered regions of the pan arctic seas even though sea ice ocean interaction is not the only driver of mixed layer variability in fall and winter there keywords mixed layer depth arctic ocean sea ice ocean stratification omip models data availability data will be made available on request 1 introduction the arctic mixed layer ml is the upper layer of the arctic ocean that controls the exchanges between the deeper ocean sea ice and the atmosphere those transfers are influenced by complex thermodynamical and dynamical processes likely to create strong heterogeneities in ocean surface properties such as discontinuous and dynamic sea ice cover ocean eddies or salinity fronts and filaments at the kilometre scale rippeth and fine 2022 goosse et al 2018 horvat et al 2016 the ml is characterized by a homogeneous density profile which goes from the oceanic surface to the beginning of the pycnocline an accurate characterization of the mixed layer depth mld is relevant to a large number of physical and biological processes from a physical point of view the ml mediates the transfer of heat between ocean sea ice and atmosphere and therefore plays a key role in the global energy budget and oceanic circulation mcphee 2008 gettelman and rood 2016 in the last decades global climate change has strongly affected the arctic region in particular leading to a fast decrease in sea ice extent timmermans and marshall 2020 nummelin et al 2016 perovich and richter menge 2009 this retreat of the sea ice affects the dynamics of the ml from a biological point of view a major spatial expansion of under ice phytoplankton blooms at high latitudes has been observed by arrigo et al 2012 boles et al 2020 and horvat et al 2017 these organisms benefit from the mixing in the upper layer of the ocean ardyna et al 2020 while crucial an accurate modeling of the mld remains a challenge for global climate models in particular large discrepancies are found among the climate models that performed climate projections for the assessment reports of the intergovernmental panel on climate change ipcc cassotta et al 2022 meredith et al 2019 previous studies by ilıcak et al 2016 and tsujino et al 2020 highlighted the poor skills of general circulation models gcm in simulating the mld in arctic regions with large biases between the models and the observational data in the present study we aim to substantiate those discrepancies by assessing the skills of the ocean sea ice gcm that participated in the ocean model intercomparison project omip we study the ability of these models to reproduce the seasonal cycle of the mld in the ice covered regions of the pan arctic seas specifically we focus on the central arctic ocean beaufort sea chukchi sea east siberian sea laptev sea kara sea and barents sea in these regions the mld varies seasonally from 20 to 80 m in winter to 5 to 30 m in summer peralta ferriz and woodgate 2015 showed that in these areas the mld is strongly correlated to the ocean stratification and the wind mainly affects the ml during ice free periods our goal here is to study the ability of omip models to reproduce the fall and winter deepening of the mld compared to the monthly isopycnal mixed layer ocean climatology mimoc schmidtko et al 2013 and ice tethered profilers itp observations toole et al 2011 krishfield et al 2008 we describe and quantify their biases and we give some insights about the origin of the differences by using a simplified surface model inspired from the work of martinson 1990 we focus on the fall and winter seasons because we aim to identify the origins of inter model differences which are much larger during these seasons the paper is organized as follows section 2 gives a brief description of the omip dataset mimoc climatology and itp observational data section 3 presents a diagnosis of the mld in the central arctic ocean simulated by the omip models and analyzes other variables relevant to understand ml seasonal changes such as the sea ice concentration the surface fluxes the salinity profiles and the ocean stratification we also describe the simplified surface model and assess its skills with respect to the omip models in the central arctic as well as in the other pan arctic seas finally section 4 presents concluding remarks and discusses implications of our work 2 method in this section we briefly present the selected omip models and the observational data mimoc provides us with monthly observational data of the mld schmidtko et al 2013 mimoc climatology is available at the national oceanic and atmospheric administration and it contains fields of ocean physical properties such as density temperature and salinity as a function of depth such values are obtained by conductivity temperature depth ctd instruments from shipboard data of the world ocean database ice tethered profilers itp and argo program due to data availability the climatology is set up between 2007 and 2011 mimoc has a spatial resolution of 0 5 from 80 s to 90 n and vertical resolution of 81 levels the mld is calculated using the algorithm of holte and talley 2009 which performs a statistical optimization based on traditional threshold and gradient methods over temperature salinity and density individual profiles thereby improving the accuracy of the depth between homogeneous mixed layer and turbulent mixing as previously discussed by schmidtko et al 2013 this methodology yields a good agreement with the common threshold density criteria δ ρ ρ z ρ z r e f 0 03 kg m 3 used in the omip framework also known as the sigma t criterion griffies et al 2016 this criterion has been introduced by levitus 1982 and defines the mld as the position from the shallowest depth down to the first depth for which the relative difference of density exceeds 0 03 kg m 3 note that the surface level is only indicative in the model simulations it is defined in an ad hoc manner and could vary from one model to another treguier et al 2023 in order to estimate the impact of averaging different datasets and observations in mimoc we also computed directly the mld from individual ice tethered profilers itp toole et al 2011 krishfield et al 2008 using the completed missions available at woods hole oceanographic institution whoi we calculate the vertical potential density profiles using the teos 10 gsw gibbs sea water python library from the conservative temperature and the absolute salinity profiles we applied the sigma t criterion to compare itp observational data with omip models where the reference surface depth for itp is more or less 5 m the itp includes data from 2004 until 2019 with the majority of the observations between the years 2007 to 2015 we use here the itp data from 2004 until 2011 finally we use the osi 450 observational dataset for sea ice concentration lavergne et al 2019 the osi 450 dataset is available at the eumetsat data services and it contains the sea ice concentration calculated from swath observations the period covered by osi 450 observational data goes from january 1979 to december 2015 and its grid spacing is about 25 km we have used the mean over the period covered by mimoc data for consistency 2007 2011 for the gcm models we use models participating in the omip project we work with models that contributed to both phases of the project omip i using as forcing the coordinated ocean ice reference experiments version 2 griffies et al 2016 core ii and omip ii using as forcing the updated japanese 55 year atmospheric reanalysis tsujino et al 2020 jra55 do in order to compare models with observational data we set up two climatologies between 2007 and 2009 for omip i and between 2007 and 2011 for omip ii both using the last cycle sixth of the protocol the difference between the two periods is due to omip i experiments ending in 2009 our results and conclusions are not affected by this set up our study uses the variables ocean mixed layer depth mlotst for which the models follow the previously explained sigma t criterion sea ice mass change from thermodynamics sidmassth defined as the ice mass balance due to surface and basal heat fluxes i e melting sublimation and freezing sea water salinity so defined as the salt content of sea water it is a dimensionless variable expressed in parts per thousand sea water potential temperature thetao defined as the mean potential temperature in c using as reference the ocean surface sea ice concentration sicon defined as the percentage of the grid cell covered by sea ice the definitions of these variables follow the omip protocol griffies et al 2016 omip models do not generically produce the sidmassth variable hence we restrict our analysis to those which do we thus work with a subsample of 10 models with different ocean and sea ice components as well as vertical resolution and for which the nominal resolution ranges between 1 and 0 25 see table 1 it is worth mentioning that all variables are interpolated to the mimoc nominal spatial resolution before analyzing them 3 results 3 1 mixed layer in permanent ice covered regions our analysis focuses first on the central arctic ocean geographically defined here as the region from 80 n to the north pole we first single out this area because it contains the largest sea ice extent in the arctic region and is likely the region where vertical mass exchanges between sea ice and the mld are most dominant in addition to the wind driven mixing in the upper part of the ocean and horizontal advection exchanges that potentially play a major role in all regions figs 1 and 2 show the pan arctic mld spatial distributions from mimoc itp profiles and omip models with the ensemble average of the models and its standard deviation in september and march respectively in september mimoc and omip models have a shallow and quite homogeneous mixed layer the omip mld ensemble average has a similar behavior to individual models and the ensemble standard deviation reaches only a few meters the itp profiles display larger spatial variability with some deeper spots compared to mimoc and omip models the disagreement between models and observations is higher in march many models tend to systematically overestimate the mld by several tens of meters compared to the mimoc and the itp observational data the ensemble average also displays deeper ml compare to observational data with a large standard deviation in the central arctic ocean north of svalbard and in the barents sea core ii forced models studied by ilıcak et al 2016 display a similar behavior with strong biases of the march mld compared with the mimoc dataset we now analyze the seasonal cycle of the spatially averaged mld this cycle is shown in fig 3 for all omip models mimoc climatology and itp observations we observe that the ml from mimoc and itp observational data remains shallow during the whole year both seasonal cycles exhibit less than 10 meters of amplitude only varying between 25 meters at the low summer value and 35 meters at the peak winter value for mimoc and between 22 and 28 meters for itp observations please note that the seasonal cycle from itp observational data displays a different behavior than in peralta ferriz and woodgate 2015 where they show a larger amplitude of the mld seasonal cycle in the central arctic ocean the differences in the mld criterion explain it in peralta ferriz and woodgate 2015 they calculate the mld using the threshold criterion δ ρ 0 1 kg m 3 in contrast in our study we use the sigma t criterion δ ρ 0 03 kg m 3 several criteria are used to compute the mld in the arctic region cole and stadler 2019 stranne et al 2018 polyakov et al 2013 timmermans et al 2012 mizobata and shimada 2012 jackson et al 2012 and there is still no clear consensus as to which criterion is the best however our aim here is to compare omip models and then we employ the threshold 0 03 kg m 3 criterion to obtain the mld it explains our choice of criterion additionally fig 3 shows that most omip models exhibit a large dispersion compared with the observational data mimoc and itp all the models display two clear seasonal phases i in spring almost all the modeled ml are deeper than the observational data and in summer all the models underestimate it by about 15 m ii in fall and winter the simulated ml becomes deeper and discrepancies with observations reach up to several tens of meters in some models the differences are also large between the models the standard deviation of the omip models ensemble average reaches about 20 m for instance cmcc models generate too deep mixed layers and the cas esm2 0 model a too shallow mld comparing omip i to omip ii protocols a systematic decrease exists for the amplitude of the mld seasonal cycle in the latter case this effect was previously observed by tsujino et al 2020 presumably due to the more significant freshwater discharge from greenland in the omip ii models for cmcc cm2 sr5 this trend is hardly significant and may as well be due to statistical biases in such cases one could not rule out that omip ii protocols may produce larger mld at a very local level as for instance reported by shu et al 2022 comparing the ensemble average spatial distribution of omip i and omip ii in the central arctic ocean for cesm2 and mri esm2 0 the difference is clearly visible with differences in the march mld of 17 and 11 m respectively however even in this case switching from omip i to omip ii hardly compensates the biases with the observed mld additionally increasing the resolution does not seem to correct the biases either the model with the highest resolution is cmcc cm2 hr4 omip ii and it simulates close to the exact same cycle as its low resolution counterpart cmcc cm2 sr5 omip ii to quantify the mld spatial variability from omip models and observational data we show their spatial standard deviation for each month in the left panel of fig 4 this quantity measures the dispersion of the mld around the central arctic ocean for most omip models and mimoc dataset the standard deviation remains lower than 10 meters over the entire year while the standard deviation for itp observational is slightly higher with values between 10 and 15 m the cmcc and ipsl models display a more significant standard deviation during fall and winter where the ipsl model reaches more than 100 meters in march april and may the right panel of fig 4 shows the relative standard deviation rsd defined as the percentage of the ratio between the standard deviation and the mean value during the year most omip models display a rsd smaller than 50 it means that the spatial variability is not too large however cmcc cm2 hr4 omip ii and cesm2 omip ii models reach large rsd of more or less 100 and the ipsl cm6a 0 omip i model even has values close to 400 these models have an important spatial variability with standard deviation values larger than their mean it is also observed in fig 2 where these models show important spatial differences in this region some studies distinguish between eurasian and makarov basins to have more homogeneous conditions in the central arctic ocean see for instance peralta ferriz and woodgate 2015 we do not make this choice because the spatial variations are not too large in most omip models and mimoc observational data 3 2 sea ice and ocean physical properties sea ice concentration a key feature of the central arctic ocean is the presence of sea ice during the whole year we refer to this area as a permanent ice covered regions the left panel of fig 5 illustrates the seasonal cycle of sea ice concentration at the surface of the ocean a 100 value means that the surface is fully covered by sea ice while a lower value means that there exist uncovered sectors observational data shows that sea ice concentration in the central arctic is approximately 100 during fall winter and spring only decreasing to 80 in summer hence exhibiting a variation of about 20 in magnitude at a qualitative level this decrease is correctly reproduced by most of the models at a quantitative level the omip ii models perform slightly better than their omip i counterparts in the former models the sea ice concentration typically varies between 15 and 40 from summer to winter this is more faithful than the variation of 60 simulated by cas esm2 0 cesm2 cmcc cm2 sr5 and cmcc esm2 part of the omip i experiment this effect was previously noticed by tsujino et al 2020 and thoroughly explained by lin et al 2022 due to the change of shortwave radiation fluxes from omip i to omip ii simulations salt transfer from a physical point of view the permanent presence of a sea ice layer reduces the interactions between the upper ocean and the atmosphere for instance limiting the shear produced by winds and waves the ml characteristics are then largely determined by the interactions between the ml and the sea ice the rough physical picture is then the following in fall and winter the growth of sea ice is associated with brine rejection i e salt is rejected from crystal structures of water ice increasing salinity in the upper layer of the ocean in spring and summer sea ice melts and freshwater goes to the ocean decreasing the salt concentration the details of the mass transfer may depend on the dynamics of each model and its parameterization for instance barthélemy et al 2015 studied the impact of this process in the ocean using the nemo lim3 global ocean sea ice model we can estimate this transfer of mass associated with sea ice formation and melting directly from the sidmassth output in the omip protocol we convert it into an equivalent salt flux measured as the meters of salt transferred in each month the right panel of fig 5 shows the seasonal cycle of the corresponding salt flux φ s for omip models during fall and winter when the salt flux is positive inter model comparison displays small variations for instance the mean value in january is 6 13 ppt meters month with a standard deviation of 0 6 ppt meters month fig a 1 shown in appendix reveals that all the models simulates a similar amount for salt transfers towards the ocean totaled over the winter months without any clear link with the mld in winter this suggests that the biases observed in the omip mld are not due to discrepancies related to the sea ice mass budget but rather to other processes involved in the mld seasonal evolution such as wind driven and horizontal exchanges vertical salinity profile and stratification the fluxes from sea ice affect the physical properties of the ocean in particular the vertical density profile this in turn causes variations in the ocean stratification we recall that the mld in omip models is determined by applying a density criterion which is by construction sensitive upon the underlying vertical density profile while ocean density is in general a non linear function of temperature and salinity the density in permanent ice covered regions is mostly controlled by ocean salinity and temperature variations are relatively small in the top layers of the ocean see for instance gettelman and rood chapter 6 2016 this behavior is indeed observed in omip models when monitoring the spatially averaged ocean salinity and temperature vertical profiles hereafter simply referred to as the mean salinity profile and mean temperature profile fig 6 shows that in most of the omip models and in the mimoc dataset while the temperature is not vertically changing much less than 1 c the first abrupt change of the mean salinity profile indicates the bottom of the mixed layer and the beginning of the halocline this suggests that in ice covered regions including but not limited to the central arctic the halocline shape provides a reliable indicator to estimate the ocean stratification the vertical stratification below the mixed layer at the month m is estimated here as the mean vertical gradient of the mean salinity profile s m between the mld h m and the current depth z that is 1 γ m z s m z s m h m z h m the vertical salinity profile is not linear as shown in fig 6 hence the stratification varies with depth the vertical salinity profile also varies with time to monitor its evolution we compute the ocean stratification of the current month m using eq 1 see fig 7 please note that the stratification is calculated over the halocline profile using as an approximation for z the depth corresponding to the ml of the following month as expected all omip models and mimoc climatology display a decrease in ocean stratification as winter progresses as a consequence of the transfer of salt from sea ice among the omip models the largest variations between models for the stratification occur in september and decreases over fall and winter the fall winter evolution of omip models ocean stratification is far more pronounced than mimoc observational data focusing on september we note that omip models with ocean stratification values closer to mimoc also show a seasonal cycle similar to observational data 3 3 a surface model for the salt balance surface model we propose to use a simple framework to reproduces the fall and winter deepening of the ml in terms of the salt balance and mixed layer dynamics this framework is inspired by the work of martinson 1990 and is illustrated in fig 8 the model neglects the non linearity of the vertical salinity profile as well as the effect of the wind stress and horizontal exchanges it links the salt flux φ s flowing into the ocean between september and month m as 2 φ s m s m 1 h m 1 s m h m h m 1 h m 2 where h m and h m 1 represent the mld at month m and m 1 and s m h m and s m 1 h m 1 the salinity values at the corresponding mld in principle the salt flux the mld and the salinity depend on the latitude and longitude coordinates for the sake of clarity we later omit to explicitly feature this spatial dependence eq 2 can be interpreted as a midpoint approximation as illustrated in the right panel of fig 8 it relies on the observation that essentially in the omip models the vertical salinity profiles stay piecewise linear during fall and winter months as shown in the left panel of fig 8 to estimate the salinity at the mld we use the data driven approximation 3 s m 1 h m 1 s m h m γ s e p t h m a r h m 1 h m involving the september stratification until the mld in march γ s e p t h m a r which is obtained by eq 1 using eq 3 the salt flux of eq 2 becomes 4 φ s m γ s e p t h m a r 2 h m 1 2 h m 2 from this formula one can now explicitly relate the mld at month m to the mld at month m 1 as 5 h m 1 h m 1 2 φ s m h m 2 γ s e p t h m a r we use eq 5 to reproduce the fall and winter ml deepening for each omip model the formula depends on two main inputs namely the salt flux ϕ s m and the september stratification between the basis of the ml and a depth corresponding to the ml in march γ s e p t h m a r to prescribe those values we use the outputs of the omip models values for the salt flux ϕ s correspond to the ones previously shown in the right panel of fig 5 the september stratifications for the various models are calculated from the omip salinity profiles we later discuss two series of simulations obtained either with i a local methodology in which eq 5 is applied at each grid point x y using the local value for the september stratification γ s e p t h m a r and salt fluxes φ s m the local mld h m 1 x y obtained from the surface model is then averaged in space for the plots ii an average methodology in which eq 5 is applied to the salinity gradients and salt fluxes averaged in space the first methodology should in principle be able to capture spatial fluctuations but may be more sensitive to the horizontal transport that is neglected in the present framework in contrast to the second one which is only driven by averaged omip output quantities surface model vs omip models we here analyze the skills of both methodologies to reproduce the fall and winter ml deepening fig 9 displays the full fall and winter mld seasonal cycle simulated by the surface model for each omip model and mimoc observational data the fall and winter deepening from mimoc observational data is only shown in the average methodology because we do not have access to salt flux observations we use as an approximation the ensemble average salt flux from omip models at a qualitative level the surface model provides a correct representation of the mld growth in the first three months october november and december for both methodologies but the local one displays remarkable quantitative agreement with most of the omip seasonal cycles we relate this feature to the fact that averaged inputs loose tracks of the spatial fluctuations which are present in omip simulations specifically we observe strong spatial variations in the ocean stratification measured here by using the salinity profile see fig a 2 and fig a 3 in appendix besides we notice that the models cas esm2 0 omip i and ipsl cm6a 0 omip i have most disagreement with the surface model to quantify this behavior we have calculated the mld relative error 6 relative error m l d o m i p m i m o c m l d s u r f a c e m l d o m i p m i m o c where m l d o m i p m i m o c corresponds to the mld from omip models or mimoc observational data and m l d s u r f a c e is the estimated mld from the surface model the march mld error is shown in the bottom right panel of fig 9 almost all omip models display less than 15 error and mimoc more or less 25 of error among the omip models whose mld in march is quasi perfectly reproduced by the simple model we find the models cesm2 omip i and cmcc omip i for those two models the surface model displays less of 10 error we however recall that compared to observational data both cesm2 omip i and cmcc omip i largely overestimate the amplitude of mld seasonal cycle and in particular the mld in march the fact that a simplified model which only considers the vertical salt flux captures much of the mld evolution when those values are biased compared with observational data suggests that the mld modeling could improve by including other processes responsible for changes in mld such as wind driven and horizontal exchanges neglected in the simple model conversely cas esm2 0 omip i and ipsl cm6a 0 omip i display the largest mismatch with the surface model while those models are the omip models which most consistently reproduce the mld seasonal cycle in comparison to observational data this suggests that those models represent better the ml dynamics and that this dynamics is more complex that the one of the simple surface model we conclude this section by noticing that there exists a strong relationship between the september ocean stratification and the march mld in omip models this relationship is shown in the left panel of fig 10 where we observe that deep mixed layers relate to weakly stratified oceans while shallow mixed layers relate to strongly stratified oceans as expected furthermore compared to mimoc dataset the omip models with the best representation of the stratification in september are also the ones with the best mixed layer in winter this suggests that the representation of the stratification is a strong signature of the mld biases in omip models 3 4 adjacent seas of the central arctic ocean we now analyze if a similar behavior for the mld dynamics is present in neighboring seas of the central arctic ocean specifically we look into the beaufort chukchi east siberian laptev kara and barents seas their boundaries are shown in the right panel of fig 10 their mld seasonal cycle from mimoc itp and omip models are displayed in fig 11 itp observations are only available during the whole year for the beaufort and chukchi seas in the other regions we only compare with mimoc climatology the seasonal cycle of the mimoc mld in the beaufort chukchi east siberian laptev and kara seas exhibits a similar behavior than in the central arctic ocean characterized by a small seasonal amplitude and a shallow ml throughout the year about 20 meters over the year the behavior is quite different in the barents sea where the mimoc mld seasonal cycle shows a high seasonal amplitude it is characterized by a 60 meter difference between january and august with a maximum mld of 80 meters in january in each region the omip models differ strongly with each other some having mld close to the mimoc one while others overestimate it in winter by tens of meters regarding itp observations the mld seasonal cycles in the beaufort and chukchi seas are a few meters shallower compared to mimoc as already noticed for the central arctic ocean the magnitude of the inter model variations differs depending on the sea under consideration in the beaufort chukchi kara east siberian and laptev seas discrepancies between models reach up to 30 meters on average in the barents sea the mld inter model variations reach more than 100 m with the exception of the barents sea the arctic adjacent seas have more than 80 of sea ice concentration during winter months see fig a 5 in appendix this suggests that in these almost fully ice covered regions brine rejection has a large impact on the ml fall and winter dynamics in addition to other processes such as wind driven mixing and horizontal advection similarly to the central arctic ocean in these regions the fall and winter salt flux shows small variations between models see fig a 4 in appendix to get more quantitative insights we also applied the surface model in its averaged version for all omip models in each region the relative errors of the march mld are shown in fig 12 in the beaufort and chukchi seas the surface model display a similar behavior than the central arctic ocean almost all omip models reach less than 15 error in the reproduction of the fall and winter mld deepening in the east siberian and laptev seas a more subtle feature is observed the small relative errors are due to the shallow ml and not to the good prediction of the surface model in the kara sea the relative errors are larger than in the other almost fully ice covered regions and then the surface model is not pertinent to explain the mld variations this could be due to the lower concentration of sea ice in winter compared to other regions as well as exchanges with the barents sea the surface model displays a poor ability to reproduce the fall and winter deepening of the ml in the barents sea as expected in this area which is partly covered by sea ice see fig a 5 in appendix the fall and winter ml deepening is not dominated by the salt balance associated with the exchanges with sea ice but is controlled by surface cooling wind driven mixing and horizontal advection those mechanisms are not considered by the surface model explaining its poor performance at reproducing the results of the gcm models those results suggest that only the beaufort and chukchi seas display similar behavior compared to the central arctic ocean omip models with larger biases compared with observational data are the ones with the best representation using the simplest model suggesting that this missing the impact of more complex processes leads to overestimating the mld seasonal cycle as a side remark large errors are obtained with the cas esm2 0 omip i model in some almost fully ice covered regions fig 11 shows that mismatches come from the model itself with a poor representation of the mld seasonal cycle in the east siberian laptev and kara seas this is confirmed by looking at its mld spatial distribution during the whole year see fig a 6 in appendix we observe that during april may and june the cas esm2 0 model simulates large mld especially on the east siberian and laptev coasts shu et al 2022 suggest that one possible reason for these discrepancies is that the cas esm2 0 model has the canadian arctic archipelago passes closed finally in all the regions apart from the kara sea omip models whose ocean stratification compares the best to the mimoc observational data also show the closest ml deepening at the end of the winter see fig 13 besides only the beaufort and chukchi seas display a strong relation between the stratification in september and the mld at the end of winter also observed in the central arctic ocean this is compatible with the general idea that strongly stratified oceans lead to shallow ml and weakly stratified oceans lead to deep ml however for the others regions the relation is not so clear 4 discussion and conclusion we have studied the ability of omip models to reproduce the fall and winter deepening of the ml in pan arctic seas central arctic ocean beaufort chukchi east siberian laptev kara and barents seas we have shown that in all these regions omip models poorly represent the mld seasonal cycle in summer a large part of omip models underestimates the mld by about 15 meters compared to mimoc climatology during fall and winter large biases appear with some models with very deep ml and others with values closer to observational data in particular we have observed that the cas esm2 0 ipsl cm6a 0 and mri esm2 0 models from omip i protocol are consistent enough in simulating the fall and winter deepening in more than one region of the pan arctic seas however cmcc models at low and high resolution and from omip i and omip ii protocols display too depth ml in almost all the seas during these seasons discrepancies between models reach up to 30 meters on average in all the regions except in the barents sea where the mld inter model variations reach more than 100 m we showed that omip models provide consistent sea ice concentrations and ice ocean salt fluxes at the same time discrepancies have been observed in the ocean stratification at the beginning of the sea ice growth season in the central arctic ocean beaufort and chukchi seas we have shown a strong relationship between the ocean stratification in september and the mld at the end of winter weakly stratified oceans lead to large mld and strongly stratified oceans lead to small mld it should be noted that this is not a causal relation and then we can also reverse the relationship for instance shallow ml in winter leads to strongly stratified oceans at the beginning of the fall furthermore omip models with similar ocean stratification compared to mimoc observational data perform better in the reproduction of the mld at the end of the winter we use the mimoc climatology and the itp observational data to compare omip models we have found that both observational data have similar mld seasonal cycles in the central arctic ocean beaufort and chukchi seas using the omip recommended density threshold of 0 03 kg m 3 to compute the mld the depth of the mixed layer varies depending on the criterion used for instance the ml is deeper using a criterion of 0 1 kg m 3 instead of 0 03 kg m 3 in pan arctic regions peralta ferriz and woodgate 2015 the different choices of criterion by different authors make model data comparisons more difficult additionally due to itp data availability the east siberian laptev kara and barents seas are only compared with mimoc climatology in principle mimoc climatology is strongly based on argo itp observation during this period the spatial distribution of argo float data is very sparse in laptev and kara seas fournier et al 2020 it suggests that mimoc may not represent the full reality regarding mld in these regions we were able to reproduce the fall and winter deepening of the ml simulated by the omip models using a simple surface model based on the vertical salt balance dynamics this model uses as inputs the vertical salinity gradient in september and the salinity flux from omip models in the central arctic ocean beaufort and chukchi seas we have noted that omip models with the largest relative errors from the reproduction of the fall and winter ml deepening using the surface model are the ones that display more realistic values of the mld seasonal cycle compared with observational data it suggests that these models accurately reproduce the ml dynamics and that these dynamics is more complex that the one of the simple surface model in the other regions the mld dynamics is different for instance in the barents sea the retreat of ice cover during summer is larger than in the central arctic hence favoring exchanges with the atmosphere this feature is likely to foster deeper ml the barents sea displays a larger mld seasonal cycle than the other regions still and despite these different behaviors the poor modeling of gcm in pan arctic seas seems to be linked to a very simple modeling of the processes involved in its dynamics here we focused on the vertical mass exchanges associated with salt balance a natural perspective is to study the impact of the other mechanisms such as surface cooling wind driven mixing and horizontal advection furthermore an appealing perspective to our work concerns the study of the mld inter annual variability particularly the effect of the feedback between the sea ice and the mixed layer in this prospect we expect to analyze the gcm of the coupled part of the cmip6 protocol from the historical simulations in the same line as previous works by watts et al 2021 and keen et al 2021 in the inter model analysis of sea ice and mass budget in the arctic credit authorship contribution statement s allende conceptualization methodology formal analysis writing original draft visualization t fichefet conceptualization writing review editing h goosse conceptualization writing review editing a m treguier conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was conducted within the jpi oceans and climate project medley mixed layer heterogeneity which is partly funded by the belgian science policy office under contract be 20e p1 medley and by the french agence nationale pour la recherche under contract 19 jpoc 0001 01 appendix we here compile additional figures related to the relation between the mld in march and the cumulative salt flux fig a 1 the sea surface salinity maps from the mimoc dataset and omip models in september fig a 2 and march fig a 3 the salinity flux in the pan arctic seas fig a 4 the sea ice concentration in the pan arctic seas fig a 5 and the spatial distribution of the sea surface salinity from the cas esm2 0 omip i model fig a 6 
